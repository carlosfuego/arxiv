[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2510.25695v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25695v1",
                "updated": "2025-10-29T17:00:16Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    17,
                    0,
                    16,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T17:00:16Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    17,
                    0,
                    16,
                    2,
                    302,
                    0
                ],
                "title": "Over 3 kV and Ultra-Low leakage Vertical (011) \\b{eta}-Ga2O3 Power\n  Diodes with Engineered Schottky Contact and High-permittivity Dielectric\n  Field Plate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Over 3 kV and Ultra-Low leakage Vertical (011) \\b{eta}-Ga2O3 Power\n  Diodes with Engineered Schottky Contact and High-permittivity Dielectric\n  Field Plate"
                },
                "summary": "We report over 3 kV breakdown voltage and ultra-low leakage (011)\n\\b{eta}-Ga2O3 power devices utilizing Schottky barrier engineering and\nhigh-permittivity (\\k{appa}) dielectric (ZrO2) field plate. The (011)\norientation of \\b{eta}-Ga2O3 enabled low background doping and thick drift\nlayers which are promising to support kV-class vertical \\b{eta}-Ga2O3 power\nswitches. The Schottky barrier engineering was performed with a composite Pt\ncap/PtOx/Pt (1.5 nm) anode contact to take advantage of the enhanced reverse\nblocking capabilities enabled by PtOx while allowing low turn-on voltage by the\ninterfacing thin Pt layer. We also performed a systematic study using a\nco-processed Pt/(011) \\b{eta}-Ga2O3 Schottky barrier diodes (SBDs) on the same\nwafer. The bare SBDs revealed a breakdown voltage of ~1.5 kV, while the\nfield-plate Pt/(011) \\b{eta}-Ga2O3 SBDs achieved an increased breakdown voltage\nof 2.75 kV owing to the edge field management. Further enhancement of the\nbreakdown voltage was achieved by tunneling leakage management using composite\nPt cap/PtOx/Pt (1.5 nm) Schottky contacts that ultimately enabled breakdown\nvoltage of 3.7 kV for the field-plate diodes. Remarkably, the Pt cap/PtOx/Pt\n(1.5 nm) Schottky contacts maintained similar turn-on voltage as the Pt/(011)\n\\b{eta}-Ga2O3 SBDs. The combination of efficient tunneling leakage management\nby composite Pt cap/PtOx/Pt (1.5 nm) contacts with similar turn-on voltage,\nedge field reduction by high-\\k{appa} dielectric ZrO2 field plate, as well as\nthe advantageous material properties offered by (011) \\b{eta}-Ga2O3 demonstrate\na promising strategy for developing ultra-low leakage and multi-kV class\nvertical (011) \\b{eta}-Ga2O3 power devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report over 3 kV breakdown voltage and ultra-low leakage (011)\n\\b{eta}-Ga2O3 power devices utilizing Schottky barrier engineering and\nhigh-permittivity (\\k{appa}) dielectric (ZrO2) field plate. The (011)\norientation of \\b{eta}-Ga2O3 enabled low background doping and thick drift\nlayers which are promising to support kV-class vertical \\b{eta}-Ga2O3 power\nswitches. The Schottky barrier engineering was performed with a composite Pt\ncap/PtOx/Pt (1.5 nm) anode contact to take advantage of the enhanced reverse\nblocking capabilities enabled by PtOx while allowing low turn-on voltage by the\ninterfacing thin Pt layer. We also performed a systematic study using a\nco-processed Pt/(011) \\b{eta}-Ga2O3 Schottky barrier diodes (SBDs) on the same\nwafer. The bare SBDs revealed a breakdown voltage of ~1.5 kV, while the\nfield-plate Pt/(011) \\b{eta}-Ga2O3 SBDs achieved an increased breakdown voltage\nof 2.75 kV owing to the edge field management. Further enhancement of the\nbreakdown voltage was achieved by tunneling leakage management using composite\nPt cap/PtOx/Pt (1.5 nm) Schottky contacts that ultimately enabled breakdown\nvoltage of 3.7 kV for the field-plate diodes. Remarkably, the Pt cap/PtOx/Pt\n(1.5 nm) Schottky contacts maintained similar turn-on voltage as the Pt/(011)\n\\b{eta}-Ga2O3 SBDs. The combination of efficient tunneling leakage management\nby composite Pt cap/PtOx/Pt (1.5 nm) contacts with similar turn-on voltage,\nedge field reduction by high-\\k{appa} dielectric ZrO2 field plate, as well as\nthe advantageous material properties offered by (011) \\b{eta}-Ga2O3 demonstrate\na promising strategy for developing ultra-low leakage and multi-kV class\nvertical (011) \\b{eta}-Ga2O3 power devices."
                },
                "authors": [
                    {
                        "name": "Emerson J. Hollar"
                    },
                    {
                        "name": "Esmat Farzana"
                    }
                ],
                "author_detail": {
                    "name": "Esmat Farzana"
                },
                "author": "Esmat Farzana",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25695v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25695v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25604v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25604v1",
                "updated": "2025-10-29T15:12:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    15,
                    12,
                    35,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T15:12:35Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    15,
                    12,
                    35,
                    2,
                    302,
                    0
                ],
                "title": "Quickest Change Point Detection with Measurements over a Lossy Link",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quickest Change Point Detection with Measurements over a Lossy Link"
                },
                "summary": "Motivated by Industry 4.0 applications, we consider quickest change detection\n(QCD) of an abrupt change in a process when its measurements are transmitted by\na sensor over a lossy wireless link to a decision maker (DM). The sensor node\nsamples measurements using a Bernoulli sampling process, and places the\nmeasurement samples in the transmit queue of its transmitter. The transmitter\nuses a retransmit-until-success transmission strategy to deliver packets to the\nDM over the lossy link, in which the packet losses are modeled as a Bernoulli\nprocess, with different loss probabilities before and after the change. We pose\nthe QCD problem in the non-Bayesian setting under Lorden's framework, and\npropose a CUSUM algorithm. By defining a suitable Markov process, involving the\nDM measurements and the queue length process, we show that the problem reduces\nto QCD in a Markov process. Characterizing the information measure per\nmeasurement sample at the DM, we establish the asymptotic optimality of our\nalgorithm when the false alarm rate tends to zero. Further, when the DM\nreceives incomplete data due to channel loss, we present asymptotically optimal\nQCD algorithms by suitably modifying the CUSUM algorithm. We then explore the\nlast-come-first-served (LCFS) queuing discipline at the sensor transmit queue\nto lower detection delay in the non-asymptotic case. Next, we consider the case\nof multiple sensors, each with its own wireless transmitter queue, and show\nthat our analysis extends to the case of multiple homogeneous sensors. When the\nsensors are heterogeneous, we present a sensor scheduling algorithm that\nminimizes detection delay by balancing the trade-off between the age of the\nobservations and their information content. Numerical analysis demonstrate\ntrade-offs that can be used to optimize system design parameters in the\nnon-asymptotic regime.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Motivated by Industry 4.0 applications, we consider quickest change detection\n(QCD) of an abrupt change in a process when its measurements are transmitted by\na sensor over a lossy wireless link to a decision maker (DM). The sensor node\nsamples measurements using a Bernoulli sampling process, and places the\nmeasurement samples in the transmit queue of its transmitter. The transmitter\nuses a retransmit-until-success transmission strategy to deliver packets to the\nDM over the lossy link, in which the packet losses are modeled as a Bernoulli\nprocess, with different loss probabilities before and after the change. We pose\nthe QCD problem in the non-Bayesian setting under Lorden's framework, and\npropose a CUSUM algorithm. By defining a suitable Markov process, involving the\nDM measurements and the queue length process, we show that the problem reduces\nto QCD in a Markov process. Characterizing the information measure per\nmeasurement sample at the DM, we establish the asymptotic optimality of our\nalgorithm when the false alarm rate tends to zero. Further, when the DM\nreceives incomplete data due to channel loss, we present asymptotically optimal\nQCD algorithms by suitably modifying the CUSUM algorithm. We then explore the\nlast-come-first-served (LCFS) queuing discipline at the sensor transmit queue\nto lower detection delay in the non-asymptotic case. Next, we consider the case\nof multiple sensors, each with its own wireless transmitter queue, and show\nthat our analysis extends to the case of multiple homogeneous sensors. When the\nsensors are heterogeneous, we present a sensor scheduling algorithm that\nminimizes detection delay by balancing the trade-off between the age of the\nobservations and their information content. Numerical analysis demonstrate\ntrade-offs that can be used to optimize system design parameters in the\nnon-asymptotic regime."
                },
                "authors": [
                    {
                        "name": "Krishna Chaythanya KV"
                    },
                    {
                        "name": "Saqib Abbas Baba"
                    },
                    {
                        "name": "Anurag Kumar"
                    },
                    {
                        "name": "Arpan Chattopadhyay"
                    },
                    {
                        "name": "Rajesh Sundaresan"
                    }
                ],
                "author_detail": {
                    "name": "Rajesh Sundaresan"
                },
                "author": "Rajesh Sundaresan",
                "arxiv_comment": "17 pages, 6 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25604v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25604v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25600v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25600v2",
                "updated": "2025-10-30T03:43:02Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    3,
                    43,
                    2,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-29T15:10:17Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    15,
                    10,
                    17,
                    2,
                    302,
                    0
                ],
                "title": "PureKV: Plug-and-Play KV Cache Optimization with Spatial-Temporal Sparse\n  Attention for Vision-Language Large Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PureKV: Plug-and-Play KV Cache Optimization with Spatial-Temporal Sparse\n  Attention for Vision-Language Large Models"
                },
                "summary": "Vision-Language Large Models (VLLMs) face significant efficiency challenges\nwhen processing high-resolution inputs. The quadratic complexity in attention\nand autoregressive generation, as well as the constantly growing key value (KV)\ncache size, severely hinder the prefilling and decoding stages. Recent efforts\nhave attempted to compress KV cache by identifying and pruning KV cache of less\nimportant tokens, but these methods typically rely on attention scores to\nestimate token importance, making them incompatible with efficient attention\nmechanisms such as FlashAttention and Sparse Attention, which do not explicitly\ncompute attention matrices. Moreover, existing methods overlook how sparse\nattention, while accelerating the prefilling stage, alters the information\nstructure of the KV cache, thereby compromising the effectiveness of downstream\nKV cache compression strategies. To address this issue, we propose PureKV, a\nplug-and-play framework for joint optimization of sparse attention and KV cache\ncompression. We first introduce a KV cache compression strategy that is fully\ncompatible with efficient attention accelerators. Our method utilizes lower\nlayer attention scores to estimate the importance of high layers' KV cache,\nenabling active pruning without compromising accuracy. In addition, we have\ndesigned a Spatial-Temporal Sparse Attention (ST-SpAttn) module specifically\ntailored for video KV cache compression algorithms. This module combines\nspatial and temporal attention sparsity to improve the compression efficiency\nof KV cache optimization algorithms by purifying spatial noise and temporal\nredundancy in KV cache. At the same time, ST-SpAttn also accelerated the\nprefilling stage of VLLMs. Extensive experiments on VLLMs (VideoLLaMA2,\nQwen2.5-VL) have shown that PureKV achieves 5.0 times KV cache compression and\n3.16 times prefill acceleration, with negligible quality degradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Large Models (VLLMs) face significant efficiency challenges\nwhen processing high-resolution inputs. The quadratic complexity in attention\nand autoregressive generation, as well as the constantly growing key value (KV)\ncache size, severely hinder the prefilling and decoding stages. Recent efforts\nhave attempted to compress KV cache by identifying and pruning KV cache of less\nimportant tokens, but these methods typically rely on attention scores to\nestimate token importance, making them incompatible with efficient attention\nmechanisms such as FlashAttention and Sparse Attention, which do not explicitly\ncompute attention matrices. Moreover, existing methods overlook how sparse\nattention, while accelerating the prefilling stage, alters the information\nstructure of the KV cache, thereby compromising the effectiveness of downstream\nKV cache compression strategies. To address this issue, we propose PureKV, a\nplug-and-play framework for joint optimization of sparse attention and KV cache\ncompression. We first introduce a KV cache compression strategy that is fully\ncompatible with efficient attention accelerators. Our method utilizes lower\nlayer attention scores to estimate the importance of high layers' KV cache,\nenabling active pruning without compromising accuracy. In addition, we have\ndesigned a Spatial-Temporal Sparse Attention (ST-SpAttn) module specifically\ntailored for video KV cache compression algorithms. This module combines\nspatial and temporal attention sparsity to improve the compression efficiency\nof KV cache optimization algorithms by purifying spatial noise and temporal\nredundancy in KV cache. At the same time, ST-SpAttn also accelerated the\nprefilling stage of VLLMs. Extensive experiments on VLLMs (VideoLLaMA2,\nQwen2.5-VL) have shown that PureKV achieves 5.0 times KV cache compression and\n3.16 times prefill acceleration, with negligible quality degradation."
                },
                "authors": [
                    {
                        "name": "Zhonghua Jiang"
                    },
                    {
                        "name": "Kunxi Li"
                    },
                    {
                        "name": "Yiyun Zhou"
                    },
                    {
                        "name": "Sihao Liu"
                    },
                    {
                        "name": "Zhaode Wang"
                    },
                    {
                        "name": "Chengfei lv"
                    },
                    {
                        "name": "Shengyu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Shengyu Zhang"
                },
                "author": "Shengyu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25600v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25600v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25590v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25590v1",
                "updated": "2025-10-29T14:58:37Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    14,
                    58,
                    37,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T14:58:37Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    14,
                    58,
                    37,
                    2,
                    302,
                    0
                ],
                "title": "RegionE: Adaptive Region-Aware Generation for Efficient Image Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RegionE: Adaptive Region-Aware Generation for Efficient Image Editing"
                },
                "summary": "Recently, instruction-based image editing (IIE) has received widespread\nattention. In practice, IIE often modifies only specific regions of an image,\nwhile the remaining areas largely remain unchanged. Although these two types of\nregions differ significantly in generation difficulty and computational\nredundancy, existing IIE models do not account for this distinction, instead\napplying a uniform generation process across the entire image. This motivates\nus to propose RegionE, an adaptive, region-aware generation framework that\naccelerates IIE tasks without additional training. Specifically, the RegionE\nframework consists of three main components: 1) Adaptive Region Partition. We\nobserved that the trajectory of unedited regions is straight, allowing for\nmulti-step denoised predictions to be inferred in a single step. Therefore, in\nthe early denoising stages, we partition the image into edited and unedited\nregions based on the difference between the final estimated result and the\nreference image. 2) Region-Aware Generation. After distinguishing the regions,\nwe replace multi-step denoising with one-step prediction for unedited areas.\nFor edited regions, the trajectory is curved, requiring local iterative\ndenoising. To improve the efficiency and quality of local iterative generation,\nwe propose the Region-Instruction KV Cache, which reduces computational cost\nwhile incorporating global information. 3) Adaptive Velocity Decay Cache.\nObserving that adjacent timesteps in edited regions exhibit strong velocity\nsimilarity, we further propose an adaptive velocity decay cache to accelerate\nthe local denoising process. We applied RegionE to state-of-the-art IIE base\nmodels, including Step1X-Edit, FLUX.1 Kontext, and Qwen-Image-Edit. RegionE\nachieved acceleration factors of 2.57, 2.41, and 2.06. Evaluations by GPT-4o\nconfirmed that semantic and perceptual fidelity were well preserved.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, instruction-based image editing (IIE) has received widespread\nattention. In practice, IIE often modifies only specific regions of an image,\nwhile the remaining areas largely remain unchanged. Although these two types of\nregions differ significantly in generation difficulty and computational\nredundancy, existing IIE models do not account for this distinction, instead\napplying a uniform generation process across the entire image. This motivates\nus to propose RegionE, an adaptive, region-aware generation framework that\naccelerates IIE tasks without additional training. Specifically, the RegionE\nframework consists of three main components: 1) Adaptive Region Partition. We\nobserved that the trajectory of unedited regions is straight, allowing for\nmulti-step denoised predictions to be inferred in a single step. Therefore, in\nthe early denoising stages, we partition the image into edited and unedited\nregions based on the difference between the final estimated result and the\nreference image. 2) Region-Aware Generation. After distinguishing the regions,\nwe replace multi-step denoising with one-step prediction for unedited areas.\nFor edited regions, the trajectory is curved, requiring local iterative\ndenoising. To improve the efficiency and quality of local iterative generation,\nwe propose the Region-Instruction KV Cache, which reduces computational cost\nwhile incorporating global information. 3) Adaptive Velocity Decay Cache.\nObserving that adjacent timesteps in edited regions exhibit strong velocity\nsimilarity, we further propose an adaptive velocity decay cache to accelerate\nthe local denoising process. We applied RegionE to state-of-the-art IIE base\nmodels, including Step1X-Edit, FLUX.1 Kontext, and Qwen-Image-Edit. RegionE\nachieved acceleration factors of 2.57, 2.41, and 2.06. Evaluations by GPT-4o\nconfirmed that semantic and perceptual fidelity were well preserved."
                },
                "authors": [
                    {
                        "name": "Pengtao Chen"
                    },
                    {
                        "name": "Xianfang Zeng"
                    },
                    {
                        "name": "Maosen Zhao"
                    },
                    {
                        "name": "Mingzhu Shen"
                    },
                    {
                        "name": "Peng Ye"
                    },
                    {
                        "name": "Bangyin Xiang"
                    },
                    {
                        "name": "Zhibo Wang"
                    },
                    {
                        "name": "Wei Cheng"
                    },
                    {
                        "name": "Gang Yu"
                    },
                    {
                        "name": "Tao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tao Chen"
                },
                "author": "Tao Chen",
                "arxiv_comment": "26 pages, 10 figures, 18 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25590v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25590v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21710v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21710v2",
                "updated": "2025-10-29T14:46:17Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    14,
                    46,
                    17,
                    2,
                    302,
                    0
                ],
                "published": "2025-06-26T18:51:04Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    18,
                    51,
                    4,
                    3,
                    177,
                    0
                ],
                "title": "FOCUS: Internal MLLM Representations for Efficient Fine-Grained Visual\n  Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FOCUS: Internal MLLM Representations for Efficient Fine-Grained Visual\n  Question Answering"
                },
                "summary": "While Multimodal Large Language Models (MLLMs) offer strong perception and\nreasoning capabilities for image-text input, Visual Question Answering (VQA)\nfocusing on small image details still remains a challenge. Although visual\ncropping techniques seem promising, recent approaches have several limitations:\nthe need for task-specific fine-tuning, low efficiency due to uninformed\nexhaustive search, or incompatibility with efficient attention implementations.\nWe address these shortcomings by proposing a training-free visual cropping\nmethod, dubbed FOCUS, that leverages MLLM-internal representations to guide the\nsearch for the most relevant image region. This is accomplished in four steps:\nfirst, we identify the target object(s) in the VQA prompt; second, we compute\nan object relevance map using the key-value (KV) cache; third, we propose and\nrank relevant image regions based on the map; and finally, we perform the\nfine-grained VQA task using the top-ranked region. As a result of this informed\nsearch strategy, FOCUS achieves strong performance across four fine-grained VQA\ndatasets and three types of MLLMs. It outperforms three popular visual cropping\nmethods in both accuracy and efficiency, and matches the best-performing\nbaseline, ZoomEye, while requiring 3 - 6.5 x less compute.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Multimodal Large Language Models (MLLMs) offer strong perception and\nreasoning capabilities for image-text input, Visual Question Answering (VQA)\nfocusing on small image details still remains a challenge. Although visual\ncropping techniques seem promising, recent approaches have several limitations:\nthe need for task-specific fine-tuning, low efficiency due to uninformed\nexhaustive search, or incompatibility with efficient attention implementations.\nWe address these shortcomings by proposing a training-free visual cropping\nmethod, dubbed FOCUS, that leverages MLLM-internal representations to guide the\nsearch for the most relevant image region. This is accomplished in four steps:\nfirst, we identify the target object(s) in the VQA prompt; second, we compute\nan object relevance map using the key-value (KV) cache; third, we propose and\nrank relevant image regions based on the map; and finally, we perform the\nfine-grained VQA task using the top-ranked region. As a result of this informed\nsearch strategy, FOCUS achieves strong performance across four fine-grained VQA\ndatasets and three types of MLLMs. It outperforms three popular visual cropping\nmethods in both accuracy and efficiency, and matches the best-performing\nbaseline, ZoomEye, while requiring 3 - 6.5 x less compute."
                },
                "authors": [
                    {
                        "name": "Liangyu Zhong"
                    },
                    {
                        "name": "Fabio Rosenthal"
                    },
                    {
                        "name": "Joachim Sicking"
                    },
                    {
                        "name": "Fabian Hüger"
                    },
                    {
                        "name": "Thorsten Bagdonat"
                    },
                    {
                        "name": "Hanno Gottschalk"
                    },
                    {
                        "name": "Leo Schwinn"
                    }
                ],
                "author_detail": {
                    "name": "Leo Schwinn"
                },
                "author": "Leo Schwinn",
                "arxiv_comment": "Accepted by NeurIPS 2025 - main track. Project page:\n  https://focus-mllm-vqa.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21710v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21710v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25412v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25412v1",
                "updated": "2025-10-29T11:29:03Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    11,
                    29,
                    3,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T11:29:03Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    11,
                    29,
                    3,
                    2,
                    302,
                    0
                ],
                "title": "Serve Programs, Not Prompts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serve Programs, Not Prompts"
                },
                "summary": "Current large language model (LLM) serving systems, primarily designed for\ntext completion, are neither efficient nor adaptable for increasingly complex\nLLM applications due to their inflexible design. We propose a new LLM serving\nsystem architecture that serves programs instead of prompts to address this\nproblem. These programs, called LLM Inference Programs (LIPs), allow users to\ncustomize token prediction and KV cache management at runtime and to offload\nparts of their application logic, such as tool execution, to the server. We\ndescribe an example of this architecture through a system named Symphony, which\nfunctions as an operating system for LIPs. Symphony exposes LLM model\ncomputations via system calls and virtualizes KV cache with a dedicated file\nsystem, while ensuring GPU efficiency with a two-level process scheduling\nscheme. Symphony has the potential to open the door to a more efficient and\nextensible ecosystem for LLM applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current large language model (LLM) serving systems, primarily designed for\ntext completion, are neither efficient nor adaptable for increasingly complex\nLLM applications due to their inflexible design. We propose a new LLM serving\nsystem architecture that serves programs instead of prompts to address this\nproblem. These programs, called LLM Inference Programs (LIPs), allow users to\ncustomize token prediction and KV cache management at runtime and to offload\nparts of their application logic, such as tool execution, to the server. We\ndescribe an example of this architecture through a system named Symphony, which\nfunctions as an operating system for LIPs. Symphony exposes LLM model\ncomputations via system calls and virtualizes KV cache with a dedicated file\nsystem, while ensuring GPU efficiency with a two-level process scheduling\nscheme. Symphony has the potential to open the door to a more efficient and\nextensible ecosystem for LLM applications."
                },
                "authors": [
                    {
                        "name": "In Gim"
                    },
                    {
                        "name": "Lin Zhong"
                    }
                ],
                "author_detail": {
                    "name": "Lin Zhong"
                },
                "author": "Lin Zhong",
                "arxiv_doi": "10.1145/3713082.3730398",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3713082.3730398",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.25412v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25412v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "HotOS 2025. Follow-up implementation work (SOSP 2025) is available at\n  arXiv:2510.24051",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25160v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25160v2",
                "updated": "2025-10-30T08:52:17Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    8,
                    52,
                    17,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-29T04:29:17Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    4,
                    29,
                    17,
                    2,
                    302,
                    0
                ],
                "title": "Model-Document Protocol for AI Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model-Document Protocol for AI Search"
                },
                "summary": "AI search depends on linking large language models (LLMs) with vast external\nknowledge sources. Yet web pages, PDF files, and other raw documents are not\ninherently LLM-ready: they are long, noisy, and unstructured. Conventional\nretrieval methods treat these documents as verbatim text and return raw\npassages, leaving the burden of fragment assembly and contextual reasoning to\nthe LLM. This gap underscores the need for a new retrieval paradigm that\nredefines how models interact with documents.\n  We introduce the Model-Document Protocol (MDP), a general framework that\nformalizes how raw text is bridged to LLMs through consumable knowledge\nrepresentations. Rather than treating retrieval as passage fetching, MDP\ndefines multiple pathways that transform unstructured documents into\ntask-specific, LLM-ready inputs. These include agentic reasoning, which curates\nraw evidence into coherent context; memory grounding, which accumulates\nreusable notes to enrich reasoning; and structured leveraging, which encodes\ndocuments into formal representations such as graphs or key-value caches. All\nthree pathways share the same goal: ensuring that what reaches the LLM is not\nraw fragments but compact, structured knowledge directly consumable for\nreasoning.\n  As an instantiation, we present MDP-Agent, which realizes the protocol\nthrough an agentic process: constructing document-level gist memories for\nglobal coverage, performing diffusion-based exploration with vertical\nexploitation to uncover layered dependencies, and applying map-reduce style\nsynthesis to integrate large-scale evidence into compact yet sufficient\ncontext. Experiments on information-seeking benchmarks demonstrate that\nMDP-Agent outperforms baselines, validating both the soundness of the MDP\nframework and the effectiveness of its agentic instantiation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI search depends on linking large language models (LLMs) with vast external\nknowledge sources. Yet web pages, PDF files, and other raw documents are not\ninherently LLM-ready: they are long, noisy, and unstructured. Conventional\nretrieval methods treat these documents as verbatim text and return raw\npassages, leaving the burden of fragment assembly and contextual reasoning to\nthe LLM. This gap underscores the need for a new retrieval paradigm that\nredefines how models interact with documents.\n  We introduce the Model-Document Protocol (MDP), a general framework that\nformalizes how raw text is bridged to LLMs through consumable knowledge\nrepresentations. Rather than treating retrieval as passage fetching, MDP\ndefines multiple pathways that transform unstructured documents into\ntask-specific, LLM-ready inputs. These include agentic reasoning, which curates\nraw evidence into coherent context; memory grounding, which accumulates\nreusable notes to enrich reasoning; and structured leveraging, which encodes\ndocuments into formal representations such as graphs or key-value caches. All\nthree pathways share the same goal: ensuring that what reaches the LLM is not\nraw fragments but compact, structured knowledge directly consumable for\nreasoning.\n  As an instantiation, we present MDP-Agent, which realizes the protocol\nthrough an agentic process: constructing document-level gist memories for\nglobal coverage, performing diffusion-based exploration with vertical\nexploitation to uncover layered dependencies, and applying map-reduce style\nsynthesis to integrate large-scale evidence into compact yet sufficient\ncontext. Experiments on information-seeking benchmarks demonstrate that\nMDP-Agent outperforms baselines, validating both the soundness of the MDP\nframework and the effectiveness of its agentic instantiation."
                },
                "authors": [
                    {
                        "name": "Hongjin Qian"
                    },
                    {
                        "name": "Zheng Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Liu"
                },
                "author": "Zheng Liu",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25160v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25160v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25152v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25152v1",
                "updated": "2025-10-29T04:09:50Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    4,
                    9,
                    50,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T04:09:50Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    4,
                    9,
                    50,
                    2,
                    302,
                    0
                ],
                "title": "Off-Centered WoS-Type Solvers with Statistical Weighting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Off-Centered WoS-Type Solvers with Statistical Weighting"
                },
                "summary": "Stochastic PDE solvers have emerged as a powerful alternative to traditional\ndiscretization-based methods for solving partial differential equations (PDEs),\nespecially in geometry processing and graphics. While off-centered estimators\nenhance sample reuse in WoS-type Monte Carlo solvers, they introduce\ncorrelation artifacts and bias when Green's functions are approximated. In this\npaper, we propose a statistically weighted off-centered WoS-type estimator that\nleverages local similarity filtering to selectively combine samples across\nneighboring evaluation points. Our method balances bias and variance through a\nprincipled weighting strategy that suppresses unreliable estimators. We\ndemonstrate our approach's effectiveness on various PDEs,including screened\nPoisson equations and boundary conditions, achieving consistent improvements\nover existing solvers such as vanilla Walk on Spheres, mean value caching, and\nboundary value caching. Our method also naturally extends to gradient field\nestimation and mixed boundary problems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stochastic PDE solvers have emerged as a powerful alternative to traditional\ndiscretization-based methods for solving partial differential equations (PDEs),\nespecially in geometry processing and graphics. While off-centered estimators\nenhance sample reuse in WoS-type Monte Carlo solvers, they introduce\ncorrelation artifacts and bias when Green's functions are approximated. In this\npaper, we propose a statistically weighted off-centered WoS-type estimator that\nleverages local similarity filtering to selectively combine samples across\nneighboring evaluation points. Our method balances bias and variance through a\nprincipled weighting strategy that suppresses unreliable estimators. We\ndemonstrate our approach's effectiveness on various PDEs,including screened\nPoisson equations and boundary conditions, achieving consistent improvements\nover existing solvers such as vanilla Walk on Spheres, mean value caching, and\nboundary value caching. Our method also naturally extends to gradient field\nestimation and mixed boundary problems."
                },
                "authors": [
                    {
                        "name": "Anchang Bao"
                    },
                    {
                        "name": "Jie Xu"
                    },
                    {
                        "name": "Enya Shen"
                    },
                    {
                        "name": "Jianmin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jianmin Wang"
                },
                "author": "Jianmin Wang",
                "arxiv_comment": "SIGGRAPH Asia 2025 conference paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25152v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25152v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25122v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25122v1",
                "updated": "2025-10-29T03:00:36Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    3,
                    0,
                    36,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T03:00:36Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    3,
                    0,
                    36,
                    2,
                    302,
                    0
                ],
                "title": "NanoVLA: Routing Decoupled Vision-Language Understanding for Nano-sized\n  Generalist Robotic Policies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NanoVLA: Routing Decoupled Vision-Language Understanding for Nano-sized\n  Generalist Robotic Policies"
                },
                "summary": "Vision-language-action (VLA) models have significantly advanced robotic\nmanipulation by integrating vision-language models (VLMs), and action decoders\ninto a unified architecture. However, their deployment on resource-constrained\nedge devices, such as mobile robots or embedded systems (e.g., Jetson Orin\nNano), remains challenging due to high computational demands, especially in\nreal-world scenarios where power, latency, and computational resources are\ncritical. To close this gap, we introduce Nano-scale Vision-Language Action\n(NanoVLA), a family of lightweight VLA architectures that achieve high\nperformance with minimal resources. Our core innovations include: (1)\nvision-language decoupling that moves conventional early vision and language\ninputs fusion in VLM to late stage, achieving better performance while enabling\ncaching and reduce inference overhead and latency; (2) long-short action\nchunking to ensure smooth, coherent multi-step planning without sacrificing\nreal-time responsiveness; (3) dynamic routing that adaptively assigns\nlightweight or heavy backbones based on task complexity, further optimizing\ninference efficiency. Experimental results on several benchmarks, as well as\nreal-world deployments, demonstrate that NanoVLA achieves up to 52x faster\ninference on edge devices compared to previous state-of-the-art VLA models,\nwith 98% less parameters while maintaining or surpassing their task accuracy\nand generalization. Ablation studies confirm that our decoupling strategy\npreserves cross-task transferability, and the routing module enhances\ncost-performance trade-offs, enabling practical, high-precision robotic\nmanipulation on resource-constrained hardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language-action (VLA) models have significantly advanced robotic\nmanipulation by integrating vision-language models (VLMs), and action decoders\ninto a unified architecture. However, their deployment on resource-constrained\nedge devices, such as mobile robots or embedded systems (e.g., Jetson Orin\nNano), remains challenging due to high computational demands, especially in\nreal-world scenarios where power, latency, and computational resources are\ncritical. To close this gap, we introduce Nano-scale Vision-Language Action\n(NanoVLA), a family of lightweight VLA architectures that achieve high\nperformance with minimal resources. Our core innovations include: (1)\nvision-language decoupling that moves conventional early vision and language\ninputs fusion in VLM to late stage, achieving better performance while enabling\ncaching and reduce inference overhead and latency; (2) long-short action\nchunking to ensure smooth, coherent multi-step planning without sacrificing\nreal-time responsiveness; (3) dynamic routing that adaptively assigns\nlightweight or heavy backbones based on task complexity, further optimizing\ninference efficiency. Experimental results on several benchmarks, as well as\nreal-world deployments, demonstrate that NanoVLA achieves up to 52x faster\ninference on edge devices compared to previous state-of-the-art VLA models,\nwith 98% less parameters while maintaining or surpassing their task accuracy\nand generalization. Ablation studies confirm that our decoupling strategy\npreserves cross-task transferability, and the routing module enhances\ncost-performance trade-offs, enabling practical, high-precision robotic\nmanipulation on resource-constrained hardware."
                },
                "authors": [
                    {
                        "name": "Jiahong Chen"
                    },
                    {
                        "name": "Jing Wang"
                    },
                    {
                        "name": "Long Chen"
                    },
                    {
                        "name": "Chuwei Cai"
                    },
                    {
                        "name": "Jinghui Lu"
                    }
                ],
                "author_detail": {
                    "name": "Jinghui Lu"
                },
                "author": "Jinghui Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25122v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25122v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24824v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24824v1",
                "updated": "2025-10-28T15:35:50Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    15,
                    35,
                    50,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T15:35:50Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    15,
                    35,
                    50,
                    1,
                    301,
                    0
                ],
                "title": "Parallel Loop Transformer for Efficient Test-Time Computation Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parallel Loop Transformer for Efficient Test-Time Computation Scaling"
                },
                "summary": "Large Language Models (LLMs) are powerful but often too slow and costly for\nreal-world use during inference. Looped transformers save on parameters by\nreusing the same weights for multiple computational steps, or \"loops.\" However,\nthis approach has a major flaw: the loops run one after another, causing\ninference latency and memory requirements to increase with each added loop.\nThis makes them impractical for fast applications. To solve this problem, we\nintroduce the Parallel Loop Transformer (PLT). PLT is a new architecture that\ndelivers the performance benefits of a deep, looped model but with the low\nlatency of a standard, non-looped model. PLT works using two key techniques.\nFirst, Cross-Loop Parallelism (CLP) breaks the sequential dependency by\ncomputing different loops for different tokens at the same time, all within a\nsingle pass. Second, to prevent memory costs from growing, we use an Efficient\nRepresentation Enhancement strategy. This method shares the memory (KV cache)\nfrom the first loop with all other loops. It then uses a Gated Sliding-Window\nAttention (G-SWA) to combine this shared global information with local\ninformation, maintaining high accuracy. Our experiments show that PLT achieves\nthe high accuracy of a traditional looped model but with almost no extra\nlatency or memory cost compared to a standard transformer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are powerful but often too slow and costly for\nreal-world use during inference. Looped transformers save on parameters by\nreusing the same weights for multiple computational steps, or \"loops.\" However,\nthis approach has a major flaw: the loops run one after another, causing\ninference latency and memory requirements to increase with each added loop.\nThis makes them impractical for fast applications. To solve this problem, we\nintroduce the Parallel Loop Transformer (PLT). PLT is a new architecture that\ndelivers the performance benefits of a deep, looped model but with the low\nlatency of a standard, non-looped model. PLT works using two key techniques.\nFirst, Cross-Loop Parallelism (CLP) breaks the sequential dependency by\ncomputing different loops for different tokens at the same time, all within a\nsingle pass. Second, to prevent memory costs from growing, we use an Efficient\nRepresentation Enhancement strategy. This method shares the memory (KV cache)\nfrom the first loop with all other loops. It then uses a Gated Sliding-Window\nAttention (G-SWA) to combine this shared global information with local\ninformation, maintaining high accuracy. Our experiments show that PLT achieves\nthe high accuracy of a traditional looped model but with almost no extra\nlatency or memory cost compared to a standard transformer."
                },
                "authors": [
                    {
                        "name": "Bohong Wu"
                    },
                    {
                        "name": "Mengzhao Chen"
                    },
                    {
                        "name": "Xiang Luo"
                    },
                    {
                        "name": "Shen Yan"
                    },
                    {
                        "name": "Qifan Yu"
                    },
                    {
                        "name": "Fan Xia"
                    },
                    {
                        "name": "Tianqi Zhang"
                    },
                    {
                        "name": "Hongrui Zhan"
                    },
                    {
                        "name": "Zheng Zhong"
                    },
                    {
                        "name": "Xun Zhou"
                    },
                    {
                        "name": "Siyuan Qiao"
                    },
                    {
                        "name": "Xingyan Bin"
                    }
                ],
                "author_detail": {
                    "name": "Xingyan Bin"
                },
                "author": "Xingyan Bin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24824v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24824v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24359v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24359v1",
                "updated": "2025-10-28T12:28:02Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    12,
                    28,
                    2,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T12:28:02Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    12,
                    28,
                    2,
                    1,
                    301,
                    0
                ],
                "title": "An N-of-1 Artificial Intelligence Ecosystem for Precision Medicine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An N-of-1 Artificial Intelligence Ecosystem for Precision Medicine"
                },
                "summary": "Artificial intelligence in medicine is built to serve the average patient. By\nminimizing error across large datasets, most systems deliver strong aggregate\naccuracy yet falter at the margins: patients with rare variants,\nmultimorbidity, or underrepresented demographics. This average patient fallacy\nerodes both equity and trust. We propose a different design: a multi-agent\necosystem for N-of-1 decision support. In this environment, agents clustered by\norgan systems, patient populations, and analytic modalities draw on a shared\nlibrary of models and evidence synthesis tools. Their results converge in a\ncoordination layer that weighs reliability, uncertainty, and data density\nbefore presenting the clinician with a decision-support packet: risk estimates\nbounded by confidence ranges, outlier flags, and linked evidence. Validation\nshifts from population averages to individual reliability, measured by error in\nlow-density regions, calibration in the small, and risk--coverage trade-offs.\nAnticipated challenges include computational demands, automation bias, and\nregulatory fit, addressed through caching strategies, consensus checks, and\nadaptive trial frameworks. By moving from monolithic models to orchestrated\nintelligence, this approach seeks to align medical AI with the first principle\nof medicine: care that is transparent, equitable, and centered on the\nindividual.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial intelligence in medicine is built to serve the average patient. By\nminimizing error across large datasets, most systems deliver strong aggregate\naccuracy yet falter at the margins: patients with rare variants,\nmultimorbidity, or underrepresented demographics. This average patient fallacy\nerodes both equity and trust. We propose a different design: a multi-agent\necosystem for N-of-1 decision support. In this environment, agents clustered by\norgan systems, patient populations, and analytic modalities draw on a shared\nlibrary of models and evidence synthesis tools. Their results converge in a\ncoordination layer that weighs reliability, uncertainty, and data density\nbefore presenting the clinician with a decision-support packet: risk estimates\nbounded by confidence ranges, outlier flags, and linked evidence. Validation\nshifts from population averages to individual reliability, measured by error in\nlow-density regions, calibration in the small, and risk--coverage trade-offs.\nAnticipated challenges include computational demands, automation bias, and\nregulatory fit, addressed through caching strategies, consensus checks, and\nadaptive trial frameworks. By moving from monolithic models to orchestrated\nintelligence, this approach seeks to align medical AI with the first principle\nof medicine: care that is transparent, equitable, and centered on the\nindividual."
                },
                "authors": [
                    {
                        "name": "Pedram Fard"
                    },
                    {
                        "name": "Alaleh Azhir"
                    },
                    {
                        "name": "Neguine Rezaii"
                    },
                    {
                        "name": "Jiazi Tian"
                    },
                    {
                        "name": "Hossein Estiri"
                    }
                ],
                "author_detail": {
                    "name": "Hossein Estiri"
                },
                "author": "Hossein Estiri",
                "arxiv_comment": "This study has been supported by grants from the National Institutes\n  of Health: The National Institute on Aging R01AG074372 and The National\n  Institute of Allergy and Infectious Diseases R01AI165535",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24359v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24359v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24273v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24273v1",
                "updated": "2025-10-28T10:32:52Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    10,
                    32,
                    52,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T10:32:52Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    10,
                    32,
                    52,
                    1,
                    301,
                    0
                ],
                "title": "SALS: Sparse Attention in Latent Space for KV cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SALS: Sparse Attention in Latent Space for KV cache Compression"
                },
                "summary": "Large Language Models capable of handling extended contexts are in high\ndemand, yet their inference remains challenging due to substantial Key-Value\ncache size and high memory bandwidth requirements. Previous research has\ndemonstrated that KV cache exhibits low-rank characteristics within the hidden\ndimension, suggesting the potential for effective compression. However, due to\nthe widely adopted Rotary Position Embedding mechanism in modern LLMs, naive\nlow-rank compression suffers severe accuracy degradation or creates a new speed\nbottleneck, as the low-rank cache must first be reconstructed in order to apply\nRoPE. In this paper, we introduce two key insights: first, the application of\nRoPE to the key vectors increases their variance, which in turn results in a\nhigher rank; second, after the key vectors are transformed into the latent\nspace, they largely maintain their representation across most layers. Based on\nthese insights, we propose the Sparse Attention in Latent Space framework. SALS\nprojects the KV cache into a compact latent space via low-rank projection, and\nperforms sparse token selection using RoPE-free query-key interactions in this\nspace. By reconstructing only a small subset of important tokens, it avoids the\noverhead of full KV cache reconstruction. We comprehensively evaluate SALS on\nvarious tasks using two large-scale models: LLaMA2-7b-chat and Mistral-7b, and\nadditionally verify its scalability on the RULER-128k benchmark with\nLLaMA3.1-8B-Instruct. Experimental results demonstrate that SALS achieves SOTA\nperformance by maintaining competitive accuracy. Under different settings, SALS\nachieves 6.4-fold KV cache compression and 5.7-fold speed-up in the attention\noperator compared to FlashAttention2 on the 4K sequence. For the end-to-end\nthroughput performance, we achieves 1.4-fold and 4.5-fold improvement compared\nto GPT-fast on 4k and 32K sequences, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models capable of handling extended contexts are in high\ndemand, yet their inference remains challenging due to substantial Key-Value\ncache size and high memory bandwidth requirements. Previous research has\ndemonstrated that KV cache exhibits low-rank characteristics within the hidden\ndimension, suggesting the potential for effective compression. However, due to\nthe widely adopted Rotary Position Embedding mechanism in modern LLMs, naive\nlow-rank compression suffers severe accuracy degradation or creates a new speed\nbottleneck, as the low-rank cache must first be reconstructed in order to apply\nRoPE. In this paper, we introduce two key insights: first, the application of\nRoPE to the key vectors increases their variance, which in turn results in a\nhigher rank; second, after the key vectors are transformed into the latent\nspace, they largely maintain their representation across most layers. Based on\nthese insights, we propose the Sparse Attention in Latent Space framework. SALS\nprojects the KV cache into a compact latent space via low-rank projection, and\nperforms sparse token selection using RoPE-free query-key interactions in this\nspace. By reconstructing only a small subset of important tokens, it avoids the\noverhead of full KV cache reconstruction. We comprehensively evaluate SALS on\nvarious tasks using two large-scale models: LLaMA2-7b-chat and Mistral-7b, and\nadditionally verify its scalability on the RULER-128k benchmark with\nLLaMA3.1-8B-Instruct. Experimental results demonstrate that SALS achieves SOTA\nperformance by maintaining competitive accuracy. Under different settings, SALS\nachieves 6.4-fold KV cache compression and 5.7-fold speed-up in the attention\noperator compared to FlashAttention2 on the 4K sequence. For the end-to-end\nthroughput performance, we achieves 1.4-fold and 4.5-fold improvement compared\nto GPT-fast on 4k and 32K sequences, respectively."
                },
                "authors": [
                    {
                        "name": "Junlin Mu"
                    },
                    {
                        "name": "Hantao Huang"
                    },
                    {
                        "name": "Jihang Zhang"
                    },
                    {
                        "name": "Minghui Yu"
                    },
                    {
                        "name": "Tao Wang"
                    },
                    {
                        "name": "Yidong Li"
                    }
                ],
                "author_detail": {
                    "name": "Yidong Li"
                },
                "author": "Yidong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24273v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24273v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24051v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24051v1",
                "updated": "2025-10-28T04:17:55Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    4,
                    17,
                    55,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T04:17:55Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    4,
                    17,
                    55,
                    1,
                    301,
                    0
                ],
                "title": "Pie: A Programmable Serving System for Emerging LLM Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pie: A Programmable Serving System for Emerging LLM Applications"
                },
                "summary": "Emerging large language model (LLM) applications involve diverse reasoning\nstrategies and agentic workflows, straining the capabilities of existing\nserving systems built on a monolithic token generation loop. This paper\nintroduces Pie, a programmable LLM serving system designed for flexibility and\nefficiency. Pie decomposes the traditional generation loop into fine-grained\nservice handlers exposed via an API and delegates control of the generation\nprocess to user-provided programs, called inferlets. This enables applications\nto implement new KV cache strategies, bespoke generation logic, and seamlessly\nintegrate computation and I/O-entirely within the application, without\nrequiring modifications to the serving system. Pie executes inferlets using\nWebAssembly, benefiting from its lightweight sandboxing. Our evaluation shows\nPie matches state-of-the-art performance on standard tasks (3-12% latency\noverhead) while significantly improving latency and throughput (1.3x-3.4x\nhigher) on agentic workflows by enabling application-specific optimizations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emerging large language model (LLM) applications involve diverse reasoning\nstrategies and agentic workflows, straining the capabilities of existing\nserving systems built on a monolithic token generation loop. This paper\nintroduces Pie, a programmable LLM serving system designed for flexibility and\nefficiency. Pie decomposes the traditional generation loop into fine-grained\nservice handlers exposed via an API and delegates control of the generation\nprocess to user-provided programs, called inferlets. This enables applications\nto implement new KV cache strategies, bespoke generation logic, and seamlessly\nintegrate computation and I/O-entirely within the application, without\nrequiring modifications to the serving system. Pie executes inferlets using\nWebAssembly, benefiting from its lightweight sandboxing. Our evaluation shows\nPie matches state-of-the-art performance on standard tasks (3-12% latency\noverhead) while significantly improving latency and throughput (1.3x-3.4x\nhigher) on agentic workflows by enabling application-specific optimizations."
                },
                "authors": [
                    {
                        "name": "In Gim"
                    },
                    {
                        "name": "Zhiyao Ma"
                    },
                    {
                        "name": "Seung-seob Lee"
                    },
                    {
                        "name": "Lin Zhong"
                    }
                ],
                "author_detail": {
                    "name": "Lin Zhong"
                },
                "author": "Lin Zhong",
                "arxiv_doi": "10.1145/3731569.3764814",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3731569.3764814",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.24051v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24051v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "SOSP 2025. Source code available at\n  https://github.com/pie-project/pie",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01068v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01068v4",
                "updated": "2025-10-28T04:00:18Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    4,
                    0,
                    18,
                    1,
                    301,
                    0
                ],
                "published": "2025-02-03T05:25:09Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    5,
                    25,
                    9,
                    0,
                    34,
                    0
                ],
                "title": "FastKV: KV Cache Compression for Fast Long-Context Processing with\n  Token-Selective Propagation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastKV: KV Cache Compression for Fast Long-Context Processing with\n  Token-Selective Propagation"
                },
                "summary": "While large language models (LLMs) excel at handling long-context sequences,\nthey require substantial prefill computation and key-value (KV) cache, which\ncan heavily burden computational efficiency and memory usage in both prefill\nand decoding stages. Recent works that compress KV caches with prefill\nacceleration reduce this cost but inadvertently tie the prefill compute\nreduction to the decoding KV budget. This coupling arises from overlooking the\nlayer-dependent variation of critical context, often leading to accuracy\ndegradation. To address this issue, we introduce FastKV, a KV cache compression\nframework designed to reduce latency in both prefill and decoding by leveraging\nthe stabilization of token importance in later layers. FastKV performs\nfull-context computation until a Token-Selective Propagation (TSP) layer, which\nforwards only the most informative tokens to subsequent layers. From these\npropagated tokens, FastKV independently selects salient KV entries for caching,\nthereby decoupling KV budget from the prefill compute reduction based on the\nTSP decision. This independent control of the TSP rate and KV retention rate\nenables flexible optimization of efficiency and accuracy. Experimental results\nshow that FastKV achieves speedups of up to 1.82$\\times$ in prefill and\n2.87$\\times$ in decoding compared to the full-context baseline, while matching\nthe accuracy of the baselines that only accelerate the decoding stage. Our code\nis available at https://github.com/dongwonjo/FastKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) excel at handling long-context sequences,\nthey require substantial prefill computation and key-value (KV) cache, which\ncan heavily burden computational efficiency and memory usage in both prefill\nand decoding stages. Recent works that compress KV caches with prefill\nacceleration reduce this cost but inadvertently tie the prefill compute\nreduction to the decoding KV budget. This coupling arises from overlooking the\nlayer-dependent variation of critical context, often leading to accuracy\ndegradation. To address this issue, we introduce FastKV, a KV cache compression\nframework designed to reduce latency in both prefill and decoding by leveraging\nthe stabilization of token importance in later layers. FastKV performs\nfull-context computation until a Token-Selective Propagation (TSP) layer, which\nforwards only the most informative tokens to subsequent layers. From these\npropagated tokens, FastKV independently selects salient KV entries for caching,\nthereby decoupling KV budget from the prefill compute reduction based on the\nTSP decision. This independent control of the TSP rate and KV retention rate\nenables flexible optimization of efficiency and accuracy. Experimental results\nshow that FastKV achieves speedups of up to 1.82$\\times$ in prefill and\n2.87$\\times$ in decoding compared to the full-context baseline, while matching\nthe accuracy of the baselines that only accelerate the decoding stage. Our code\nis available at https://github.com/dongwonjo/FastKV."
                },
                "authors": [
                    {
                        "name": "Dongwon Jo"
                    },
                    {
                        "name": "Jiwon Song"
                    },
                    {
                        "name": "Yulhwa Kim"
                    },
                    {
                        "name": "Jae-Joon Kim"
                    }
                ],
                "author_detail": {
                    "name": "Jae-Joon Kim"
                },
                "author": "Jae-Joon Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01068v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01068v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14969v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14969v2",
                "updated": "2025-10-27T21:48:48Z",
                "updated_parsed": [
                    2025,
                    10,
                    27,
                    21,
                    48,
                    48,
                    0,
                    300,
                    0
                ],
                "published": "2025-05-20T23:12:16Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    23,
                    12,
                    16,
                    1,
                    140,
                    0
                ],
                "title": "STree: Speculative Tree Decoding for Hybrid State-Space Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STree: Speculative Tree Decoding for Hybrid State-Space Models"
                },
                "summary": "Speculative decoding is a technique to leverage hardware concurrency in order\nto enable multiple steps of token generation in a single forward pass, thus\nimproving the efficiency of large-scale autoregressive (AR) Transformer models.\nState-space models (SSMs) are already more efficient than AR Transformers,\nsince their state summarizes all past data with no need to cache or re-process\ntokens in the sliding window context. However, their state can also comprise\nthousands of tokens; so, speculative decoding has recently been extended to\nSSMs. Existing approaches, however, do not leverage the tree-based verification\nmethods, since current SSMs lack the means to compute a token tree efficiently.\nWe propose the first scalable algorithm to perform tree-based speculative\ndecoding in state-space models (SSMs) and hybrid architectures of SSMs and\nTransformer layers. We exploit the structure of accumulated state transition\nmatrices to facilitate tree-based speculative decoding with minimal overhead\nrelative to current SSM implementations. Along with the algorithm, we describe\na hardware-aware implementation that improves naive application of AR\nTransformer tree-based speculative decoding methods to SSMs. Furthermore, we\noutperform vanilla speculative decoding with SSMs even with a baseline drafting\nmodel and tree structure on three different benchmarks, opening up\nopportunities for further speed up with SSM and hybrid model inference. Code\ncan be found at: https://github.com/wyc1997/stree.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding is a technique to leverage hardware concurrency in order\nto enable multiple steps of token generation in a single forward pass, thus\nimproving the efficiency of large-scale autoregressive (AR) Transformer models.\nState-space models (SSMs) are already more efficient than AR Transformers,\nsince their state summarizes all past data with no need to cache or re-process\ntokens in the sliding window context. However, their state can also comprise\nthousands of tokens; so, speculative decoding has recently been extended to\nSSMs. Existing approaches, however, do not leverage the tree-based verification\nmethods, since current SSMs lack the means to compute a token tree efficiently.\nWe propose the first scalable algorithm to perform tree-based speculative\ndecoding in state-space models (SSMs) and hybrid architectures of SSMs and\nTransformer layers. We exploit the structure of accumulated state transition\nmatrices to facilitate tree-based speculative decoding with minimal overhead\nrelative to current SSM implementations. Along with the algorithm, we describe\na hardware-aware implementation that improves naive application of AR\nTransformer tree-based speculative decoding methods to SSMs. Furthermore, we\noutperform vanilla speculative decoding with SSMs even with a baseline drafting\nmodel and tree structure on three different benchmarks, opening up\nopportunities for further speed up with SSM and hybrid model inference. Code\ncan be found at: https://github.com/wyc1997/stree."
                },
                "authors": [
                    {
                        "name": "Yangchao Wu"
                    },
                    {
                        "name": "Zongyue Qin"
                    },
                    {
                        "name": "Alex Wong"
                    },
                    {
                        "name": "Stefano Soatto"
                    }
                ],
                "author_detail": {
                    "name": "Stefano Soatto"
                },
                "author": "Stefano Soatto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14969v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14969v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.12362v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.12362v2",
                "updated": "2025-10-27T17:31:15Z",
                "updated_parsed": [
                    2025,
                    10,
                    27,
                    17,
                    31,
                    15,
                    0,
                    300,
                    0
                ],
                "published": "2024-04-18T17:45:19Z",
                "published_parsed": [
                    2024,
                    4,
                    18,
                    17,
                    45,
                    19,
                    3,
                    109,
                    0
                ],
                "title": "KV-weights are all you need for skipless transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-weights are all you need for skipless transformers"
                },
                "summary": "He and Hofmann (arXiv:2311.01906) detailed a skipless transformer without the\nV and P (post-attention projection) linear layers, which reduces the total\nnumber of weights. However, this scheme is only applicable to MHA (multi-head\nattention), but not for MQA (multi-query attention) and GQA (grouped-query\nattention). The latter schemes are used by many popular LLMs such as Llama 2,\nMistral, Mixtral, PaLM, and Gemma. Therefore, this micro-paper proposes\nmathematically equivalent versions that are suitable for MQA and GQA. For\nexample, removing Q and P from a skipless version of Mistral-7B would remove\n15% of its weights (and thus reduce its compute and memory complexity). Watch\nour explainer video https://youtu.be/Tx_lMpphd2g and see\nhttps://github.com/OpenMachine-ai/transformer-tricks for code and more\ntransformer tricks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "He and Hofmann (arXiv:2311.01906) detailed a skipless transformer without the\nV and P (post-attention projection) linear layers, which reduces the total\nnumber of weights. However, this scheme is only applicable to MHA (multi-head\nattention), but not for MQA (multi-query attention) and GQA (grouped-query\nattention). The latter schemes are used by many popular LLMs such as Llama 2,\nMistral, Mixtral, PaLM, and Gemma. Therefore, this micro-paper proposes\nmathematically equivalent versions that are suitable for MQA and GQA. For\nexample, removing Q and P from a skipless version of Mistral-7B would remove\n15% of its weights (and thus reduce its compute and memory complexity). Watch\nour explainer video https://youtu.be/Tx_lMpphd2g and see\nhttps://github.com/OpenMachine-ai/transformer-tricks for code and more\ntransformer tricks."
                },
                "authors": [
                    {
                        "name": "Nils Graef"
                    }
                ],
                "author_detail": {
                    "name": "Nils Graef"
                },
                "author": "Nils Graef",
                "arxiv_comment": "6 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.12362v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.12362v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05530v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05530v3",
                "updated": "2025-10-27T16:20:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    27,
                    16,
                    20,
                    28,
                    0,
                    300,
                    0
                ],
                "published": "2025-03-07T15:54:04Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    15,
                    54,
                    4,
                    4,
                    66,
                    0
                ],
                "title": "Leveraging Approximate Caching for Faster Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Approximate Caching for Faster Retrieval-Augmented Generation"
                },
                "summary": "Retrieval-augmented generation (RAG) improves the reliability of large\nlanguage model (LLM) answers by integrating external knowledge. However, RAG\nincreases the end-to-end inference time since looking for relevant documents\nfrom large vector databases is computationally expensive. To address this, we\nintroduce Proximity, an approximate key-value cache that optimizes the RAG\nworkflow by leveraging similarities in user queries. Instead of treating each\nquery independently, Proximity reuses previously retrieved documents when\nsimilar queries appear, substantially reducing the reliance on expensive vector\ndatabase lookups. To efficiently scale, Proximity employs a locality-sensitive\nhashing (LSH) scheme that enables fast cache lookups while preserving retrieval\naccuracy. We evaluate Proximity using the MMLU and MedRAG question-answering\nbenchmarks. Our experiments demonstrate that Proximity with our LSH scheme and\na realistically-skewed MedRAG workload reduces database calls by 77.2% while\nmaintaining database recall and test accuracy. We experiment with different\nsimilarity tolerances and cache capacities, and show that the time spent within\nthe Proximity cache remains low and constant (4.8 microseconds) even as the\ncache grows substantially in size. Our results demonstrate that approximate\ncaching is a practical and effective strategy for optimizing RAG-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) improves the reliability of large\nlanguage model (LLM) answers by integrating external knowledge. However, RAG\nincreases the end-to-end inference time since looking for relevant documents\nfrom large vector databases is computationally expensive. To address this, we\nintroduce Proximity, an approximate key-value cache that optimizes the RAG\nworkflow by leveraging similarities in user queries. Instead of treating each\nquery independently, Proximity reuses previously retrieved documents when\nsimilar queries appear, substantially reducing the reliance on expensive vector\ndatabase lookups. To efficiently scale, Proximity employs a locality-sensitive\nhashing (LSH) scheme that enables fast cache lookups while preserving retrieval\naccuracy. We evaluate Proximity using the MMLU and MedRAG question-answering\nbenchmarks. Our experiments demonstrate that Proximity with our LSH scheme and\na realistically-skewed MedRAG workload reduces database calls by 77.2% while\nmaintaining database recall and test accuracy. We experiment with different\nsimilarity tolerances and cache capacities, and show that the time spent within\nthe Proximity cache remains low and constant (4.8 microseconds) even as the\ncache grows substantially in size. Our results demonstrate that approximate\ncaching is a practical and effective strategy for optimizing RAG-based systems."
                },
                "authors": [
                    {
                        "name": "Shai Bergman"
                    },
                    {
                        "name": "Anne-Marie Kermarrec"
                    },
                    {
                        "name": "Diana Petrescu"
                    },
                    {
                        "name": "Rafael Pires"
                    },
                    {
                        "name": "Mathis Randl"
                    },
                    {
                        "name": "Martijn de Vos"
                    },
                    {
                        "name": "Ji Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ji Zhang"
                },
                "author": "Ji Zhang",
                "arxiv_doi": "10.1145/3721462.3770776",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3721462.3770776",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.05530v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05530v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted at Middleware '25",
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08343v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08343v2",
                "updated": "2025-10-27T14:59:46Z",
                "updated_parsed": [
                    2025,
                    10,
                    27,
                    14,
                    59,
                    46,
                    0,
                    300,
                    0
                ],
                "published": "2025-08-11T10:47:35Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    10,
                    47,
                    35,
                    0,
                    223,
                    0
                ],
                "title": "A Data-driven ML Approach for Maximizing Performance in LLM-Adapter\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Data-driven ML Approach for Maximizing Performance in LLM-Adapter\n  Serving"
                },
                "summary": "With the rapid adoption of Large Language Models (LLMs), LLM-adapters have\nbecome increasingly common, providing lightweight specialization of large-scale\nmodels. Serving hundreds or thousands of these adapters on a single GPU allows\nrequest aggregation, increasing throughput, but may also cause request\nstarvation if GPU memory limits are exceeded. To address this issue, this study\nfocuses on determining the joint configuration of concurrent and parallel\nadapters that maximizes GPU throughput without inducing starvation, given\nheterogeneous adapter and traffic properties. We propose a data-driven ML\napproach leveraging interpretable models to tackle this caching problem and\nintroduce the first Digital Twin capable of reproducing an LLM-adapter serving\nsystem, enabling efficient training data generation. Experiments with the vLLM\nframework and LoRA adapters show that the Digital Twin reproduces throughput\nwithin 5.1% of real results, while the ML approach predicts optimal numbers of\nconcurrent and parallel adapters with an error of at most 7.2% under\nheterogeneous, real-world workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid adoption of Large Language Models (LLMs), LLM-adapters have\nbecome increasingly common, providing lightweight specialization of large-scale\nmodels. Serving hundreds or thousands of these adapters on a single GPU allows\nrequest aggregation, increasing throughput, but may also cause request\nstarvation if GPU memory limits are exceeded. To address this issue, this study\nfocuses on determining the joint configuration of concurrent and parallel\nadapters that maximizes GPU throughput without inducing starvation, given\nheterogeneous adapter and traffic properties. We propose a data-driven ML\napproach leveraging interpretable models to tackle this caching problem and\nintroduce the first Digital Twin capable of reproducing an LLM-adapter serving\nsystem, enabling efficient training data generation. Experiments with the vLLM\nframework and LoRA adapters show that the Digital Twin reproduces throughput\nwithin 5.1% of real results, while the ML approach predicts optimal numbers of\nconcurrent and parallel adapters with an error of at most 7.2% under\nheterogeneous, real-world workloads."
                },
                "authors": [
                    {
                        "name": "Ferran Agullo"
                    },
                    {
                        "name": "Joan Oliveras"
                    },
                    {
                        "name": "Chen Wang"
                    },
                    {
                        "name": "Alberto Gutierrez-Torre"
                    },
                    {
                        "name": "Olivier Tardieu"
                    },
                    {
                        "name": "Alaa Youssef"
                    },
                    {
                        "name": "Jordi Torres"
                    },
                    {
                        "name": "Josep Ll. Berral"
                    }
                ],
                "author_detail": {
                    "name": "Josep Ll. Berral"
                },
                "author": "Josep Ll. Berral",
                "arxiv_comment": "Accepted in a computer science workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08343v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08343v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01488v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01488v2",
                "updated": "2025-10-27T11:55:07Z",
                "updated_parsed": [
                    2025,
                    10,
                    27,
                    11,
                    55,
                    7,
                    0,
                    300,
                    0
                ],
                "published": "2025-08-02T21:00:55Z",
                "published_parsed": [
                    2025,
                    8,
                    2,
                    21,
                    0,
                    55,
                    5,
                    214,
                    0
                ],
                "title": "PESTO: Real-Time Pitch Estimation with Self-supervised\n  Transposition-equivariant Objective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PESTO: Real-Time Pitch Estimation with Self-supervised\n  Transposition-equivariant Objective"
                },
                "summary": "In this paper, we introduce PESTO, a self-supervised learning approach for\nsingle-pitch estimation using a Siamese architecture. Our model processes\nindividual frames of a Variable-$Q$ Transform (VQT) and predicts pitch\ndistributions. The neural network is designed to be equivariant to\ntranslations, notably thanks to a Toeplitz fully-connected layer. In addition,\nwe construct pitch-shifted pairs by translating and cropping the VQT frames and\ntrain our model with a novel class-based transposition-equivariant objective,\neliminating the need for annotated data. Thanks to this architecture and\ntraining objective, our model achieves remarkable performances while being very\nlightweight ($130$k parameters). Evaluations on music and speech datasets\n(MIR-1K, MDB-stem-synth, and PTDB) demonstrate that PESTO not only outperforms\nself-supervised baselines but also competes with supervised methods, exhibiting\nsuperior cross-dataset generalization. Finally, we enhance PESTO's practical\nutility by developing a streamable VQT implementation using cached\nconvolutions. Combined with our model's low latency (less than 10 ms) and\nminimal parameter count, this makes PESTO particularly suitable for real-time\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce PESTO, a self-supervised learning approach for\nsingle-pitch estimation using a Siamese architecture. Our model processes\nindividual frames of a Variable-$Q$ Transform (VQT) and predicts pitch\ndistributions. The neural network is designed to be equivariant to\ntranslations, notably thanks to a Toeplitz fully-connected layer. In addition,\nwe construct pitch-shifted pairs by translating and cropping the VQT frames and\ntrain our model with a novel class-based transposition-equivariant objective,\neliminating the need for annotated data. Thanks to this architecture and\ntraining objective, our model achieves remarkable performances while being very\nlightweight ($130$k parameters). Evaluations on music and speech datasets\n(MIR-1K, MDB-stem-synth, and PTDB) demonstrate that PESTO not only outperforms\nself-supervised baselines but also competes with supervised methods, exhibiting\nsuperior cross-dataset generalization. Finally, we enhance PESTO's practical\nutility by developing a streamable VQT implementation using cached\nconvolutions. Combined with our model's low latency (less than 10 ms) and\nminimal parameter count, this makes PESTO particularly suitable for real-time\napplications."
                },
                "authors": [
                    {
                        "name": "Alain Riou"
                    },
                    {
                        "name": "Bernardo Torres"
                    },
                    {
                        "name": "Ben Hayes"
                    },
                    {
                        "name": "Stefan Lattner"
                    },
                    {
                        "name": "Gaëtan Hadjeres"
                    },
                    {
                        "name": "Gaël Richard"
                    },
                    {
                        "name": "Geoffroy Peeters"
                    }
                ],
                "author_detail": {
                    "name": "Geoffroy Peeters"
                },
                "author": "Geoffroy Peeters",
                "arxiv_doi": "10.5334/TISMIR.251",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.5334/TISMIR.251",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.01488v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01488v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Transactions of the International Society for Music Information\n  Retrieval, 8(1): 334-352 (2025)",
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.22876v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.22876v1",
                "updated": "2025-10-26T23:59:23Z",
                "updated_parsed": [
                    2025,
                    10,
                    26,
                    23,
                    59,
                    23,
                    6,
                    299,
                    0
                ],
                "published": "2025-10-26T23:59:23Z",
                "published_parsed": [
                    2025,
                    10,
                    26,
                    23,
                    59,
                    23,
                    6,
                    299,
                    0
                ],
                "title": "Batch Speculative Decoding Done Right",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Batch Speculative Decoding Done Right"
                },
                "summary": "Speculative decoding speeds up LLM inference by using a small draft model to\npropose multiple tokens that a target model verifies in parallel. Extending\nthis idea to batches is essential for production serving, but it introduces the\nragged tensor problem: sequences in the same batch accept different numbers of\ndraft tokens, breaking right-alignment and corrupting position IDs, attention\nmasks, and KV-cache state. We show that several existing batch implementations\nviolate output equivalence-the fundamental requirement that speculative\ndecoding must produce identical token sequences to standard autoregressive\ngeneration. These violations occur precisely due to improper handling of the\nragged tensor problem. In response, we (1) characterize the synchronization\nrequirements that guarantee correctness, (2) present a correctness-first batch\nspeculative decoding EQSPEC that exposes realignment as consuming 40% of\noverhead, and (3) introduce EXSPEC, which maintains a sliding pool of sequences\nand dynamically forms same-length groups, to reduce the realignment overhead\nwhile preserving per-sequence speculative speedups. On the SpecBench dataset,\nacross Vicuna-7B/68M, Qwen3-8B/0.6B, and GLM-4-9B/0.6B target/draft pairs, our\napproach achieves up to 3$\\times$ throughput improvement at batch size 8\ncompared to batch size 1, with efficient scaling through batch size 8, while\nmaintaining 95% output equivalence. Our method requires no custom kernels and\nintegrates cleanly with existing inference stacks. Our code is available at\nhttps://github.com/eBay/spec_dec.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding speeds up LLM inference by using a small draft model to\npropose multiple tokens that a target model verifies in parallel. Extending\nthis idea to batches is essential for production serving, but it introduces the\nragged tensor problem: sequences in the same batch accept different numbers of\ndraft tokens, breaking right-alignment and corrupting position IDs, attention\nmasks, and KV-cache state. We show that several existing batch implementations\nviolate output equivalence-the fundamental requirement that speculative\ndecoding must produce identical token sequences to standard autoregressive\ngeneration. These violations occur precisely due to improper handling of the\nragged tensor problem. In response, we (1) characterize the synchronization\nrequirements that guarantee correctness, (2) present a correctness-first batch\nspeculative decoding EQSPEC that exposes realignment as consuming 40% of\noverhead, and (3) introduce EXSPEC, which maintains a sliding pool of sequences\nand dynamically forms same-length groups, to reduce the realignment overhead\nwhile preserving per-sequence speculative speedups. On the SpecBench dataset,\nacross Vicuna-7B/68M, Qwen3-8B/0.6B, and GLM-4-9B/0.6B target/draft pairs, our\napproach achieves up to 3$\\times$ throughput improvement at batch size 8\ncompared to batch size 1, with efficient scaling through batch size 8, while\nmaintaining 95% output equivalence. Our method requires no custom kernels and\nintegrates cleanly with existing inference stacks. Our code is available at\nhttps://github.com/eBay/spec_dec."
                },
                "authors": [
                    {
                        "name": "Ranran Haoran Zhang"
                    },
                    {
                        "name": "Soumik Dey"
                    },
                    {
                        "name": "Ashirbad Mishra"
                    },
                    {
                        "name": "Hansi Wu"
                    },
                    {
                        "name": "Binbin Li"
                    },
                    {
                        "name": "Rui Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Rui Zhang"
                },
                "author": "Rui Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.22876v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.22876v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.22765v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.22765v1",
                "updated": "2025-10-26T17:28:05Z",
                "updated_parsed": [
                    2025,
                    10,
                    26,
                    17,
                    28,
                    5,
                    6,
                    299,
                    0
                ],
                "published": "2025-10-26T17:28:05Z",
                "published_parsed": [
                    2025,
                    10,
                    26,
                    17,
                    28,
                    5,
                    6,
                    299,
                    0
                ],
                "title": "Jarvis: Towards Personalized AI Assistant via Personal KV-Cache\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jarvis: Towards Personalized AI Assistant via Personal KV-Cache\n  Retrieval"
                },
                "summary": "The rapid development of Vision-language models (VLMs) enables open-ended\nperception and reasoning. Recent works have started to investigate how to adapt\ngeneral-purpose VLMs into personalized assistants. Even commercial models such\nas ChatGPT now support model personalization by incorporating user-specific\ninformation. However, existing methods either learn a set of concept tokens or\ntrain a VLM to utilize user-specific information. However, both pipelines\nstruggle to generate accurate answers as personalized assistants. We introduce\nJarvis, an innovative framework for a personalized AI assistant through\npersonal KV-Cache retrieval, which stores user-specific information in the\nKV-Caches of both textual and visual tokens. The textual tokens are created by\nsummarizing user information into metadata, while the visual tokens are\nproduced by extracting distinct image patches from the user's images. When\nanswering a question, Jarvis first retrieves related KV-Caches from personal\nstorage and uses them to ensure accuracy in responses. We also introduce a\nfine-grained benchmark built with the same distinct image patch mining\npipeline, emphasizing accurate question answering based on fine-grained\nuser-specific information. Jarvis is capable of providing more accurate\nresponses, particularly when they depend on specific local details. Jarvis\nachieves state-of-the-art results in both visual question answering and\ntext-only tasks across multiple datasets, indicating a practical path toward\npersonalized AI assistants. The code and dataset will be released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of Vision-language models (VLMs) enables open-ended\nperception and reasoning. Recent works have started to investigate how to adapt\ngeneral-purpose VLMs into personalized assistants. Even commercial models such\nas ChatGPT now support model personalization by incorporating user-specific\ninformation. However, existing methods either learn a set of concept tokens or\ntrain a VLM to utilize user-specific information. However, both pipelines\nstruggle to generate accurate answers as personalized assistants. We introduce\nJarvis, an innovative framework for a personalized AI assistant through\npersonal KV-Cache retrieval, which stores user-specific information in the\nKV-Caches of both textual and visual tokens. The textual tokens are created by\nsummarizing user information into metadata, while the visual tokens are\nproduced by extracting distinct image patches from the user's images. When\nanswering a question, Jarvis first retrieves related KV-Caches from personal\nstorage and uses them to ensure accuracy in responses. We also introduce a\nfine-grained benchmark built with the same distinct image patch mining\npipeline, emphasizing accurate question answering based on fine-grained\nuser-specific information. Jarvis is capable of providing more accurate\nresponses, particularly when they depend on specific local details. Jarvis\nachieves state-of-the-art results in both visual question answering and\ntext-only tasks across multiple datasets, indicating a practical path toward\npersonalized AI assistants. The code and dataset will be released."
                },
                "authors": [
                    {
                        "name": "Binxiao Xu"
                    },
                    {
                        "name": "Junyu Feng"
                    },
                    {
                        "name": "Ruichuan An"
                    },
                    {
                        "name": "Yulin Luo"
                    },
                    {
                        "name": "Shilin Yan"
                    },
                    {
                        "name": "Hao Liang"
                    },
                    {
                        "name": "Ming Lu"
                    },
                    {
                        "name": "Wentao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wentao Zhang"
                },
                "author": "Wentao Zhang",
                "arxiv_comment": "19 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.22765v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.22765v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10367v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10367v3",
                "updated": "2025-10-26T13:31:41Z",
                "updated_parsed": [
                    2025,
                    10,
                    26,
                    13,
                    31,
                    41,
                    6,
                    299,
                    0
                ],
                "published": "2025-07-14T15:09:01Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    15,
                    9,
                    1,
                    0,
                    195,
                    0
                ],
                "title": "FalconFS: Distributed File System for Large-Scale Deep Learning Pipeline",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FalconFS: Distributed File System for Large-Scale Deep Learning Pipeline"
                },
                "summary": "Client-side metadata caching has long been considered an effective method for\naccelerating metadata operations in distributed file systems (DFSs). However,\nwe have found that client-side state (e.g., caching) is not only ineffective\nbut also consumes valuable memory resources in the deep learning pipelines. We\nthus propose FalconFS, a DFS optimized for deep learning pipelines with the\nstateless-client architecture. Specifically, instead of performing client-side\npath resolution and caching, FalconFS efficiently resolves paths on the server\nside using hybrid metadata indexing and lazy namespace replication. FalconFS\nalso boosts server concurrency with concurrent request merging and provides\neasy deployment with VFS shortcut. Evaluations against CephFS and Lustre show\nthat FalconFS achieves up to 5.72$\\times$ throughput for small file read/write\nand up to 12.81$\\times$ throughput for deep learning model training. FalconFS\nhas been running in Huawei autonomous driving system's production environment\nwith 10,000 NPUs for one year and has been open-sourced.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Client-side metadata caching has long been considered an effective method for\naccelerating metadata operations in distributed file systems (DFSs). However,\nwe have found that client-side state (e.g., caching) is not only ineffective\nbut also consumes valuable memory resources in the deep learning pipelines. We\nthus propose FalconFS, a DFS optimized for deep learning pipelines with the\nstateless-client architecture. Specifically, instead of performing client-side\npath resolution and caching, FalconFS efficiently resolves paths on the server\nside using hybrid metadata indexing and lazy namespace replication. FalconFS\nalso boosts server concurrency with concurrent request merging and provides\neasy deployment with VFS shortcut. Evaluations against CephFS and Lustre show\nthat FalconFS achieves up to 5.72$\\times$ throughput for small file read/write\nand up to 12.81$\\times$ throughput for deep learning model training. FalconFS\nhas been running in Huawei autonomous driving system's production environment\nwith 10,000 NPUs for one year and has been open-sourced."
                },
                "authors": [
                    {
                        "name": "Jingwei Xu"
                    },
                    {
                        "name": "Junbin Kang"
                    },
                    {
                        "name": "Mingkai Dong"
                    },
                    {
                        "name": "Mingyu Liu"
                    },
                    {
                        "name": "Lu Zhang"
                    },
                    {
                        "name": "Shaohong Guo"
                    },
                    {
                        "name": "Ziyan Qiu"
                    },
                    {
                        "name": "Mingzhen You"
                    },
                    {
                        "name": "Ziyi Tian"
                    },
                    {
                        "name": "Anqi Yu"
                    },
                    {
                        "name": "Tianhong Ding"
                    },
                    {
                        "name": "Xinwei Hu"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "arxiv_comment": "Accepted by NSDI'26",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10367v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10367v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.22556v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.22556v1",
                "updated": "2025-10-26T07:17:10Z",
                "updated_parsed": [
                    2025,
                    10,
                    26,
                    7,
                    17,
                    10,
                    6,
                    299,
                    0
                ],
                "published": "2025-10-26T07:17:10Z",
                "published_parsed": [
                    2025,
                    10,
                    26,
                    7,
                    17,
                    10,
                    6,
                    299,
                    0
                ],
                "title": "SABlock: Semantic-Aware KV Cache Eviction with Adaptive Compression\n  Block Size",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SABlock: Semantic-Aware KV Cache Eviction with Adaptive Compression\n  Block Size"
                },
                "summary": "The growing memory footprint of the Key-Value (KV) cache poses a severe\nscalability bottleneck for long-context Large Language Model (LLM) inference.\nWhile KV cache eviction has emerged as an effective solution by discarding less\ncritical tokens, existing token-, block-, and sentence-level compression\nmethods struggle to balance semantic coherence and memory efficiency. To this\nend, we introduce SABlock, a \\underline{s}emantic-aware KV cache eviction\nframework with \\underline{a}daptive \\underline{block} sizes. Specifically,\nSABlock first performs semantic segmentation to align compression boundaries\nwith linguistic structures, then applies segment-guided token scoring to refine\ntoken importance estimation. Finally, for each segment, a budget-driven search\nstrategy adaptively determines the optimal block size that preserves semantic\nintegrity while improving compression efficiency under a given cache budget.\nExtensive experiments on long-context benchmarks demonstrate that SABlock\nconsistently outperforms state-of-the-art baselines under the same memory\nbudgets. For instance, on Needle-in-a-Haystack (NIAH), SABlock achieves 99.9%\nretrieval accuracy with only 96 KV entries, nearly matching the performance of\nthe full-cache baseline that retains up to 8K entries. Under a fixed cache\nbudget of 1,024, SABlock further reduces peak memory usage by 46.28% and\nachieves up to 9.5x faster decoding on a 128K context length.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing memory footprint of the Key-Value (KV) cache poses a severe\nscalability bottleneck for long-context Large Language Model (LLM) inference.\nWhile KV cache eviction has emerged as an effective solution by discarding less\ncritical tokens, existing token-, block-, and sentence-level compression\nmethods struggle to balance semantic coherence and memory efficiency. To this\nend, we introduce SABlock, a \\underline{s}emantic-aware KV cache eviction\nframework with \\underline{a}daptive \\underline{block} sizes. Specifically,\nSABlock first performs semantic segmentation to align compression boundaries\nwith linguistic structures, then applies segment-guided token scoring to refine\ntoken importance estimation. Finally, for each segment, a budget-driven search\nstrategy adaptively determines the optimal block size that preserves semantic\nintegrity while improving compression efficiency under a given cache budget.\nExtensive experiments on long-context benchmarks demonstrate that SABlock\nconsistently outperforms state-of-the-art baselines under the same memory\nbudgets. For instance, on Needle-in-a-Haystack (NIAH), SABlock achieves 99.9%\nretrieval accuracy with only 96 KV entries, nearly matching the performance of\nthe full-cache baseline that retains up to 8K entries. Under a fixed cache\nbudget of 1,024, SABlock further reduces peak memory usage by 46.28% and\nachieves up to 9.5x faster decoding on a 128K context length."
                },
                "authors": [
                    {
                        "name": "Jinhan Chen"
                    },
                    {
                        "name": "Jianchun Liu"
                    },
                    {
                        "name": "Hongli Xu"
                    },
                    {
                        "name": "Xianjun Gao"
                    },
                    {
                        "name": "Shilong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Shilong Wang"
                },
                "author": "Shilong Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.22556v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.22556v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04077v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04077v3",
                "updated": "2025-10-26T04:25:10Z",
                "updated_parsed": [
                    2025,
                    10,
                    26,
                    4,
                    25,
                    10,
                    6,
                    299,
                    0
                ],
                "published": "2025-02-06T13:41:46Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    13,
                    41,
                    46,
                    3,
                    37,
                    0
                ],
                "title": "AttentionPredictor: Temporal Patterns Matter for KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AttentionPredictor: Temporal Patterns Matter for KV Cache Compression"
                },
                "summary": "With the development of large language models (LLMs), efficient inference\nthrough Key-Value (KV) cache compression has attracted considerable attention,\nespecially for long-context generation. To compress the KV cache, recent\nmethods identify critical KV tokens through static modeling of attention\nscores. However, these methods often struggle to accurately determine critical\ntokens as they neglect the temporal patterns in attention scores, resulting in\na noticeable degradation in LLM performance. To address this challenge, we\npropose AttentionPredictor, which is the first learning-based method to\ndirectly predict attention patterns for KV cache compression and critical token\nidentification. Specifically, AttentionPredictor learns a lightweight, unified\nconvolution model to dynamically capture spatiotemporal patterns and predict\nthe next-token attention scores. An appealing feature of AttentionPredictor is\nthat it accurately predicts the attention score and shares the unified\nprediction model, which consumes negligible memory, among all transformer\nlayers. Moreover, we propose a cross-token critical cache prefetching framework\nthat hides the token estimation time overhead to accelerate the decoding stage.\nBy retaining most of the attention information, AttentionPredictor achieves\n13$\\times$ KV cache compression and 5.6$\\times$ speedup in a cache offloading\nscenario with comparable LLM performance, significantly outperforming the\nstate-of-the-arts. The code is available at\nhttps://github.com/MIRALab-USTC/LLM-AttentionPredictor.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the development of large language models (LLMs), efficient inference\nthrough Key-Value (KV) cache compression has attracted considerable attention,\nespecially for long-context generation. To compress the KV cache, recent\nmethods identify critical KV tokens through static modeling of attention\nscores. However, these methods often struggle to accurately determine critical\ntokens as they neglect the temporal patterns in attention scores, resulting in\na noticeable degradation in LLM performance. To address this challenge, we\npropose AttentionPredictor, which is the first learning-based method to\ndirectly predict attention patterns for KV cache compression and critical token\nidentification. Specifically, AttentionPredictor learns a lightweight, unified\nconvolution model to dynamically capture spatiotemporal patterns and predict\nthe next-token attention scores. An appealing feature of AttentionPredictor is\nthat it accurately predicts the attention score and shares the unified\nprediction model, which consumes negligible memory, among all transformer\nlayers. Moreover, we propose a cross-token critical cache prefetching framework\nthat hides the token estimation time overhead to accelerate the decoding stage.\nBy retaining most of the attention information, AttentionPredictor achieves\n13$\\times$ KV cache compression and 5.6$\\times$ speedup in a cache offloading\nscenario with comparable LLM performance, significantly outperforming the\nstate-of-the-arts. The code is available at\nhttps://github.com/MIRALab-USTC/LLM-AttentionPredictor."
                },
                "authors": [
                    {
                        "name": "Qingyue Yang"
                    },
                    {
                        "name": "Jie Wang"
                    },
                    {
                        "name": "Xing Li"
                    },
                    {
                        "name": "Zhihai Wang"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Xianzhi Yu"
                    },
                    {
                        "name": "Wulong Liu"
                    },
                    {
                        "name": "Jianye Hao"
                    },
                    {
                        "name": "Mingxuan Yuan"
                    },
                    {
                        "name": "Bin Li"
                    }
                ],
                "author_detail": {
                    "name": "Bin Li"
                },
                "author": "Bin Li",
                "arxiv_comment": "NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04077v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04077v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.23657v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.23657v1",
                "updated": "2025-10-26T01:25:24Z",
                "updated_parsed": [
                    2025,
                    10,
                    26,
                    1,
                    25,
                    24,
                    6,
                    299,
                    0
                ],
                "published": "2025-10-26T01:25:24Z",
                "published_parsed": [
                    2025,
                    10,
                    26,
                    1,
                    25,
                    24,
                    6,
                    299,
                    0
                ],
                "title": "A machine learning framework integrating seed traits and plasma\n  parameters for predicting germination uplift in crops",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A machine learning framework integrating seed traits and plasma\n  parameters for predicting germination uplift in crops"
                },
                "summary": "Cold plasma (CP) is an eco-friendly method to enhance seed germination, yet\noutcomes remain difficult to predict due to complex seed--plasma--environment\ninteractions. This study introduces the first machine learning framework to\nforecast germination uplift in soybean, barley, sunflower, radish, and tomato\nunder dielectric barrier discharge (DBD) plasma. Among the models tested (GB,\nXGB, ET, and hybrids), Extra Trees (ET) performed best (R\\textsuperscript{2} =\n0.919; RMSE = 3.21; MAE = 2.62), improving to R\\textsuperscript{2} = 0.925\nafter feature reduction. Engineering analysis revealed a hormetic response:\nnegligible effects at $<$7 kV or $<$200 s, maximum germination at 7--15 kV for\n200--500 s, and reduced germination beyond 20 kV or prolonged exposures.\nDischarge power was also a dominant factor, with germination rate maximizing at\n$\\geq$100 W with low exposure time. Species and cultivar-level predictions\nshowed radish (MAE = 1.46) and soybean (MAE = 2.05) were modeled with high\nconsistency, while sunflower remained slightly higher variable (MAE = 3.80).\nAmong cultivars, Williams (MAE = 1.23) and Sari (1.33) were well predicted,\nwhile Arian (2.86) and Ny\\'{\\i}rs\\'{e}gi fekete (3.74) were comparatively\npoorly captured. This framework was also embedded into MLflow, providing a\ndecision-support tool for optimizing CP seed germination in precision\nagriculture.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cold plasma (CP) is an eco-friendly method to enhance seed germination, yet\noutcomes remain difficult to predict due to complex seed--plasma--environment\ninteractions. This study introduces the first machine learning framework to\nforecast germination uplift in soybean, barley, sunflower, radish, and tomato\nunder dielectric barrier discharge (DBD) plasma. Among the models tested (GB,\nXGB, ET, and hybrids), Extra Trees (ET) performed best (R\\textsuperscript{2} =\n0.919; RMSE = 3.21; MAE = 2.62), improving to R\\textsuperscript{2} = 0.925\nafter feature reduction. Engineering analysis revealed a hormetic response:\nnegligible effects at $<$7 kV or $<$200 s, maximum germination at 7--15 kV for\n200--500 s, and reduced germination beyond 20 kV or prolonged exposures.\nDischarge power was also a dominant factor, with germination rate maximizing at\n$\\geq$100 W with low exposure time. Species and cultivar-level predictions\nshowed radish (MAE = 1.46) and soybean (MAE = 2.05) were modeled with high\nconsistency, while sunflower remained slightly higher variable (MAE = 3.80).\nAmong cultivars, Williams (MAE = 1.23) and Sari (1.33) were well predicted,\nwhile Arian (2.86) and Ny\\'{\\i}rs\\'{e}gi fekete (3.74) were comparatively\npoorly captured. This framework was also embedded into MLflow, providing a\ndecision-support tool for optimizing CP seed germination in precision\nagriculture."
                },
                "authors": [
                    {
                        "name": "Saklain Niam"
                    },
                    {
                        "name": "Tashfiqur Rahman"
                    },
                    {
                        "name": "Md. Amjad Patwary"
                    },
                    {
                        "name": "Mukarram Hossain"
                    }
                ],
                "author_detail": {
                    "name": "Mukarram Hossain"
                },
                "author": "Mukarram Hossain",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.23657v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.23657v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.22467v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.22467v1",
                "updated": "2025-10-26T00:50:12Z",
                "updated_parsed": [
                    2025,
                    10,
                    26,
                    0,
                    50,
                    12,
                    6,
                    299,
                    0
                ],
                "published": "2025-10-26T00:50:12Z",
                "published_parsed": [
                    2025,
                    10,
                    26,
                    0,
                    50,
                    12,
                    6,
                    299,
                    0
                ],
                "title": "Backward-Friendly Optimization: Training Large Language Models with\n  Approximate Gradients under Memory Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Backward-Friendly Optimization: Training Large Language Models with\n  Approximate Gradients under Memory Constraints"
                },
                "summary": "Full fine-tuning of Large Language Models (LLMs) is notoriously\nmemory-intensive, primarily because conventional optimizers such as SGD or Adam\nassume access to exact gradients derived from cached activations. Existing\nsolutions either alter the model architecture (e.g., reversible networks) or\ntrade memory for computation (e.g., activation checkpointing), but the\noptimizer itself remains untouched. In this work, we introduce GradLite, a\nbackward-friendly optimizer that relaxes the requirement of exact gradients,\nenabling efficient training even when intermediate activations are aggressively\ndiscarded or approximated. GradLite leverages two key techniques: (i) low-rank\nJacobian approximation, which reduces the dimensionality of backpropagated\nerror signals, and (ii) error-feedback correction, which accumulates and\ncompensates approximation errors across iterations to preserve convergence\nguarantees. We provide a theoretical analysis showing that GradLite maintains\nunbiased gradient estimates with bounded variance, ensuring convergence rates\ncomparable to Adam. Empirically, GradLite reduces optimizer-state and\nactivation memory consumption by up to 50\\% without architectural changes, and\nachieves on-par or superior downstream performance on reasoning (MMLU, GSM8K),\nmultilingual, and dialogue benchmarks compared to checkpointing and\noptimizer-centric baselines (LoMo, GaLore).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Full fine-tuning of Large Language Models (LLMs) is notoriously\nmemory-intensive, primarily because conventional optimizers such as SGD or Adam\nassume access to exact gradients derived from cached activations. Existing\nsolutions either alter the model architecture (e.g., reversible networks) or\ntrade memory for computation (e.g., activation checkpointing), but the\noptimizer itself remains untouched. In this work, we introduce GradLite, a\nbackward-friendly optimizer that relaxes the requirement of exact gradients,\nenabling efficient training even when intermediate activations are aggressively\ndiscarded or approximated. GradLite leverages two key techniques: (i) low-rank\nJacobian approximation, which reduces the dimensionality of backpropagated\nerror signals, and (ii) error-feedback correction, which accumulates and\ncompensates approximation errors across iterations to preserve convergence\nguarantees. We provide a theoretical analysis showing that GradLite maintains\nunbiased gradient estimates with bounded variance, ensuring convergence rates\ncomparable to Adam. Empirically, GradLite reduces optimizer-state and\nactivation memory consumption by up to 50\\% without architectural changes, and\nachieves on-par or superior downstream performance on reasoning (MMLU, GSM8K),\nmultilingual, and dialogue benchmarks compared to checkpointing and\noptimizer-centric baselines (LoMo, GaLore)."
                },
                "authors": [
                    {
                        "name": "Jing Yang"
                    },
                    {
                        "name": "Kaitong Cai"
                    },
                    {
                        "name": "Yijia Fan"
                    },
                    {
                        "name": "Yufeng Yang"
                    },
                    {
                        "name": "Keze Wang"
                    }
                ],
                "author_detail": {
                    "name": "Keze Wang"
                },
                "author": "Keze Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.22467v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.22467v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10524v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10524v3",
                "updated": "2025-10-25T14:12:56Z",
                "updated_parsed": [
                    2025,
                    10,
                    25,
                    14,
                    12,
                    56,
                    5,
                    298,
                    0
                ],
                "published": "2025-07-14T17:49:00Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    17,
                    49,
                    0,
                    0,
                    195,
                    0
                ],
                "title": "Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive\n  Token-Level Computation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive\n  Token-Level Computation"
                },
                "summary": "Scaling language models unlocks impressive capabilities, but the accompanying\ncomputational and memory demands make both training and deployment expensive.\nExisting efficiency efforts typically target either parameter sharing or\nadaptive computation, leaving open the question of how to attain both\nsimultaneously. We introduce Mixture-of-Recursions (MoR), a unified framework\nthat combines the two axes of efficiency inside a single Recursive Transformer.\nMoR reuses a shared stack of layers across recursion steps to achieve parameter\nefficiency, while lightweight routers enable adaptive token-level thinking by\ndynamically assigning different recursion depths to individual tokens. This\nallows MoR to focus quadratic attention computation only among tokens still\nactive at a given recursion depth, further improving memory access efficiency\nby selectively caching only their key-value pairs. Beyond these core\nmechanisms, we also propose a KV sharing variant that reuses KV pairs from the\nfirst recursion, specifically designed to further decrease memory footprint.\nAcross model scales ranging from 135M to 1.7B parameters, MoR forms a new\nPareto frontier: at equal training FLOPs and smaller model sizes, it\nsignificantly lowers validation perplexity and improves few-shot accuracy,\nwhile delivering higher throughput compared with vanilla and existing recursive\nbaselines. These gains demonstrate that MoR is an effective path towards\nlarge-model quality without incurring large-model cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling language models unlocks impressive capabilities, but the accompanying\ncomputational and memory demands make both training and deployment expensive.\nExisting efficiency efforts typically target either parameter sharing or\nadaptive computation, leaving open the question of how to attain both\nsimultaneously. We introduce Mixture-of-Recursions (MoR), a unified framework\nthat combines the two axes of efficiency inside a single Recursive Transformer.\nMoR reuses a shared stack of layers across recursion steps to achieve parameter\nefficiency, while lightweight routers enable adaptive token-level thinking by\ndynamically assigning different recursion depths to individual tokens. This\nallows MoR to focus quadratic attention computation only among tokens still\nactive at a given recursion depth, further improving memory access efficiency\nby selectively caching only their key-value pairs. Beyond these core\nmechanisms, we also propose a KV sharing variant that reuses KV pairs from the\nfirst recursion, specifically designed to further decrease memory footprint.\nAcross model scales ranging from 135M to 1.7B parameters, MoR forms a new\nPareto frontier: at equal training FLOPs and smaller model sizes, it\nsignificantly lowers validation perplexity and improves few-shot accuracy,\nwhile delivering higher throughput compared with vanilla and existing recursive\nbaselines. These gains demonstrate that MoR is an effective path towards\nlarge-model quality without incurring large-model cost."
                },
                "authors": [
                    {
                        "name": "Sangmin Bae"
                    },
                    {
                        "name": "Yujin Kim"
                    },
                    {
                        "name": "Reza Bayat"
                    },
                    {
                        "name": "Sungnyun Kim"
                    },
                    {
                        "name": "Jiyoun Ha"
                    },
                    {
                        "name": "Tal Schuster"
                    },
                    {
                        "name": "Adam Fisch"
                    },
                    {
                        "name": "Hrayr Harutyunyan"
                    },
                    {
                        "name": "Ziwei Ji"
                    },
                    {
                        "name": "Aaron Courville"
                    },
                    {
                        "name": "Se-Young Yun"
                    }
                ],
                "author_detail": {
                    "name": "Se-Young Yun"
                },
                "author": "Se-Young Yun",
                "arxiv_comment": "38 pages, 9 figures, 17 tables, codes at\n  https://github.com/raymin0223/mixture_of_recursions",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10524v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10524v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.23649v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.23649v1",
                "updated": "2025-10-25T11:43:27Z",
                "updated_parsed": [
                    2025,
                    10,
                    25,
                    11,
                    43,
                    27,
                    5,
                    298,
                    0
                ],
                "published": "2025-10-25T11:43:27Z",
                "published_parsed": [
                    2025,
                    10,
                    25,
                    11,
                    43,
                    27,
                    5,
                    298,
                    0
                ],
                "title": "Efficient Low Rank Attention for Long-Context Inference in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Low Rank Attention for Long-Context Inference in Large\n  Language Models"
                },
                "summary": "As the length of input text grows, the key-value (KV) cache in LLMs imposes\nprohibitive GPU memory costs and limits long-context inference on resource\nconstrained devices. Existing approaches, such as KV quantization and pruning,\nreduce memory usage but suffer from numerical precision loss or suboptimal\nretention of key-value pairs. We introduce Low Rank Query and Key attention\n(LRQK), a two-stage framework that jointly decomposes the full-precision query\nand key matrices into compact rank-\\(r\\) factors during the prefill stage, and\nthen uses these low-dimensional projections to compute proxy attention scores\nin \\(\\mathcal{O}(lr)\\) time at each decode step. By selecting only the\ntop-\\(k\\) tokens and a small fixed set of recent tokens, LRQK employs a mixed\nGPU-CPU cache with a hit-and-miss mechanism that transfers only missing\nfull-precision KV pairs, thereby preserving exact attention outputs while\nreducing CPU-GPU data movement. Extensive experiments on the RULER and\nLongBench benchmarks with LLaMA-3-8B and Qwen2.5-7B demonstrate that LRQK\nmatches or surpasses leading sparse-attention methods in long context settings,\nwhile delivering significant memory savings with minimal loss in accuracy. Our\ncode is available at https://github.com/tenghuilee/LRQK.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the length of input text grows, the key-value (KV) cache in LLMs imposes\nprohibitive GPU memory costs and limits long-context inference on resource\nconstrained devices. Existing approaches, such as KV quantization and pruning,\nreduce memory usage but suffer from numerical precision loss or suboptimal\nretention of key-value pairs. We introduce Low Rank Query and Key attention\n(LRQK), a two-stage framework that jointly decomposes the full-precision query\nand key matrices into compact rank-\\(r\\) factors during the prefill stage, and\nthen uses these low-dimensional projections to compute proxy attention scores\nin \\(\\mathcal{O}(lr)\\) time at each decode step. By selecting only the\ntop-\\(k\\) tokens and a small fixed set of recent tokens, LRQK employs a mixed\nGPU-CPU cache with a hit-and-miss mechanism that transfers only missing\nfull-precision KV pairs, thereby preserving exact attention outputs while\nreducing CPU-GPU data movement. Extensive experiments on the RULER and\nLongBench benchmarks with LLaMA-3-8B and Qwen2.5-7B demonstrate that LRQK\nmatches or surpasses leading sparse-attention methods in long context settings,\nwhile delivering significant memory savings with minimal loss in accuracy. Our\ncode is available at https://github.com/tenghuilee/LRQK."
                },
                "authors": [
                    {
                        "name": "Tenghui Li"
                    },
                    {
                        "name": "Guoxu Zhou"
                    },
                    {
                        "name": "Xuyang Zhao"
                    },
                    {
                        "name": "Yuning Qiu"
                    },
                    {
                        "name": "Qibin Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Qibin Zhao"
                },
                "author": "Qibin Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.23649v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.23649v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.22145v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.22145v1",
                "updated": "2025-10-25T03:34:34Z",
                "updated_parsed": [
                    2025,
                    10,
                    25,
                    3,
                    34,
                    34,
                    5,
                    298,
                    0
                ],
                "published": "2025-10-25T03:34:34Z",
                "published_parsed": [
                    2025,
                    10,
                    25,
                    3,
                    34,
                    34,
                    5,
                    298,
                    0
                ],
                "title": "Fundamental Limits of Coded Caching with Fixed Subpacketization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fundamental Limits of Coded Caching with Fixed Subpacketization"
                },
                "summary": "Coded caching is a promising technique to create coded multicast\nopportunities for cache-aided networks. By splitting each file into $F$ equal\npackets (i.e., the subpacketization level $F$) and letting each user cache a\nset of packets, the transmission load can be significantly reduced via coded\nmulticasting. It has been shown that a higher subpacketization level could\npotentially lead to a lower transmission load, as more packets can be combined\nfor efficient transmission. On the other hand, a larger $F$ indicates a higher\ncoding complexity and is problematic from a practical perspective when $F$ is\nextremely large. Despite many works attempting to design coded caching schemes\nwith low subpacketization levels, a fundamental problem remains open: What is\nthe minimum transmission load given any fixed subpacketization level? In this\npaper, we consider the classical cache-aided networks with identically uncoded\nplacement and one-shot delivery strategy, and investigate the fundamental\ntrade-off between the transmission load and the subpacketization level. We\npropose a \\emph{general} lower bound on the transmission load for any fixed\nsubpacketization by reformulating the centralized coded caching schemes via the\ncombinatorial structure of the corresponding placement delivery array. The\nlower bound also recovers existing optimality results for the bipartite graph\nscheme (including the well-known Maddah-Ali and Niesen (MN) scheme and the\nconjugate MN scheme) as well as the grouping bipartite graph scheme.\nFurthermore, by carefully exploiting the combinatorial structure and computing\nthe union size of sorted sets, we establish a new optimality result, i.e., the\npartition scheme can achieve the optimal rate-subpacketization trade-off.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coded caching is a promising technique to create coded multicast\nopportunities for cache-aided networks. By splitting each file into $F$ equal\npackets (i.e., the subpacketization level $F$) and letting each user cache a\nset of packets, the transmission load can be significantly reduced via coded\nmulticasting. It has been shown that a higher subpacketization level could\npotentially lead to a lower transmission load, as more packets can be combined\nfor efficient transmission. On the other hand, a larger $F$ indicates a higher\ncoding complexity and is problematic from a practical perspective when $F$ is\nextremely large. Despite many works attempting to design coded caching schemes\nwith low subpacketization levels, a fundamental problem remains open: What is\nthe minimum transmission load given any fixed subpacketization level? In this\npaper, we consider the classical cache-aided networks with identically uncoded\nplacement and one-shot delivery strategy, and investigate the fundamental\ntrade-off between the transmission load and the subpacketization level. We\npropose a \\emph{general} lower bound on the transmission load for any fixed\nsubpacketization by reformulating the centralized coded caching schemes via the\ncombinatorial structure of the corresponding placement delivery array. The\nlower bound also recovers existing optimality results for the bipartite graph\nscheme (including the well-known Maddah-Ali and Niesen (MN) scheme and the\nconjugate MN scheme) as well as the grouping bipartite graph scheme.\nFurthermore, by carefully exploiting the combinatorial structure and computing\nthe union size of sorted sets, we establish a new optimality result, i.e., the\npartition scheme can achieve the optimal rate-subpacketization trade-off."
                },
                "authors": [
                    {
                        "name": "Minquan Cheng"
                    },
                    {
                        "name": "Yifei Huang"
                    },
                    {
                        "name": "Youlong Wu"
                    },
                    {
                        "name": "Jinyan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jinyan Wang"
                },
                "author": "Jinyan Wang",
                "arxiv_comment": "19 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.22145v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.22145v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10270v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10270v3",
                "updated": "2025-10-25T02:29:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    25,
                    2,
                    29,
                    47,
                    5,
                    298,
                    0
                ],
                "published": "2025-03-13T11:26:45Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    11,
                    26,
                    45,
                    3,
                    72,
                    0
                ],
                "title": "EEdit: Rethinking the Spatial and Temporal Redundancy for Efficient\n  Image Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EEdit: Rethinking the Spatial and Temporal Redundancy for Efficient\n  Image Editing"
                },
                "summary": "Inversion-based image editing is rapidly gaining momentum while suffering\nfrom significant computation overhead, hindering its application in real-time\ninteractive scenarios. In this paper, we rethink that the redundancy in\ninversion-based image editing exists in both the spatial and temporal\ndimensions, such as the unnecessary computation in unedited regions and the\nredundancy in the inversion progress. To tackle these challenges, we propose a\npractical framework, named EEdit, to achieve efficient image editing.\nSpecifically, we introduce three techniques to solve them one by one. For\nspatial redundancy, spatial locality caching is introduced to compute the\nedited region and its neighboring regions while skipping the unedited regions,\nand token indexing preprocessing is designed to further accelerate the caching.\nFor temporal redundancy, inversion step skipping is proposed to reuse the\nlatent for efficient editing. Our experiments demonstrate an average of 2.46\n$\\times$ acceleration without performance drop in a wide range of editing tasks\nincluding prompt-guided image editing, dragging and image composition. Our\ncodes are available at https://github.com/yuriYanZeXuan/EEdit",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inversion-based image editing is rapidly gaining momentum while suffering\nfrom significant computation overhead, hindering its application in real-time\ninteractive scenarios. In this paper, we rethink that the redundancy in\ninversion-based image editing exists in both the spatial and temporal\ndimensions, such as the unnecessary computation in unedited regions and the\nredundancy in the inversion progress. To tackle these challenges, we propose a\npractical framework, named EEdit, to achieve efficient image editing.\nSpecifically, we introduce three techniques to solve them one by one. For\nspatial redundancy, spatial locality caching is introduced to compute the\nedited region and its neighboring regions while skipping the unedited regions,\nand token indexing preprocessing is designed to further accelerate the caching.\nFor temporal redundancy, inversion step skipping is proposed to reuse the\nlatent for efficient editing. Our experiments demonstrate an average of 2.46\n$\\times$ acceleration without performance drop in a wide range of editing tasks\nincluding prompt-guided image editing, dragging and image composition. Our\ncodes are available at https://github.com/yuriYanZeXuan/EEdit"
                },
                "authors": [
                    {
                        "name": "Zexuan Yan"
                    },
                    {
                        "name": "Yue Ma"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Wenteng Chen"
                    },
                    {
                        "name": "Qifeng Chen"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "accepted by ICCV2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10270v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10270v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02770v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02770v4",
                "updated": "2025-10-25T00:33:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    25,
                    0,
                    33,
                    14,
                    5,
                    298,
                    0
                ],
                "published": "2025-02-04T23:26:10Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    23,
                    26,
                    10,
                    1,
                    35,
                    0
                ],
                "title": "Twilight: Adaptive Attention Sparsity with Hierarchical Top-$p$ Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Twilight: Adaptive Attention Sparsity with Hierarchical Top-$p$ Pruning"
                },
                "summary": "Leveraging attention sparsity to accelerate long-context large language\nmodels (LLMs) has been a hot research topic. However, current algorithms such\nas sparse attention or key-value (KV) cache compression tend to use a fixed\nbudget, which presents a significant challenge during deployment because it\nfails to account for the dynamic nature of real-world scenarios, where the\noptimal balance between accuracy and efficiency can vary greatly. In this\npaper, we find that borrowing top-$p$ sampling (nucleus sampling) to sparse\nattention can surprisingly achieve adaptive budgeting. Based on this, we\npropose Twilight, a framework to bring adaptive sparsity to any existing sparse\nattention algorithm without sacrificing their accuracy. Empirical results show\nthat Twilight can adaptively prune at most 98% of redundant tokens, leading to\n$15.4\\times$ acceleration in self-attention operations and $3.9\\times$\nacceleration in end-to-end per token latency in long context LLM decoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging attention sparsity to accelerate long-context large language\nmodels (LLMs) has been a hot research topic. However, current algorithms such\nas sparse attention or key-value (KV) cache compression tend to use a fixed\nbudget, which presents a significant challenge during deployment because it\nfails to account for the dynamic nature of real-world scenarios, where the\noptimal balance between accuracy and efficiency can vary greatly. In this\npaper, we find that borrowing top-$p$ sampling (nucleus sampling) to sparse\nattention can surprisingly achieve adaptive budgeting. Based on this, we\npropose Twilight, a framework to bring adaptive sparsity to any existing sparse\nattention algorithm without sacrificing their accuracy. Empirical results show\nthat Twilight can adaptively prune at most 98% of redundant tokens, leading to\n$15.4\\times$ acceleration in self-attention operations and $3.9\\times$\nacceleration in end-to-end per token latency in long context LLM decoding."
                },
                "authors": [
                    {
                        "name": "Chaofan Lin"
                    },
                    {
                        "name": "Jiaming Tang"
                    },
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Hanshuo Wang"
                    },
                    {
                        "name": "Tian Tang"
                    },
                    {
                        "name": "Boyu Tian"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Mingyu Gao"
                    }
                ],
                "author_detail": {
                    "name": "Mingyu Gao"
                },
                "author": "Mingyu Gao",
                "arxiv_comment": "To appear on NeurIPS 2025 (spotlight)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02770v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02770v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.22049v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.22049v1",
                "updated": "2025-10-24T22:17:49Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    22,
                    17,
                    49,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T22:17:49Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    22,
                    17,
                    49,
                    4,
                    297,
                    0
                ],
                "title": "Massive Memorization with Hundreds of Trillions of Parameters for\n  Sequential Transducer Generative Recommenders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Massive Memorization with Hundreds of Trillions of Parameters for\n  Sequential Transducer Generative Recommenders"
                },
                "summary": "Modern large-scale recommendation systems rely heavily on user interaction\nhistory sequences to enhance the model performance. The advent of large\nlanguage models and sequential modeling techniques, particularly\ntransformer-like architectures, has led to significant advancements recently\n(e.g., HSTU, SIM, and TWIN models). While scaling to ultra-long user histories\n(10k to 100k items) generally improves model performance, it also creates\nsignificant challenges on latency, queries per second (QPS) and GPU cost in\nindustry-scale recommendation systems. Existing models do not adequately\naddress these industrial scalability issues. In this paper, we propose a novel\ntwo-stage modeling framework, namely VIrtual Sequential Target Attention\n(VISTA), which decomposes traditional target attention from a candidate item to\nuser history items into two distinct stages: (1) user history summarization\ninto a few hundred tokens; followed by (2) candidate item attention to those\ntokens. These summarization token embeddings are then cached in storage system\nand then utilized as sequence features for downstream model training and\ninference. This novel design for scalability enables VISTA to scale to lifelong\nuser histories (up to one million items) while keeping downstream training and\ninference costs fixed, which is essential in industry. Our approach achieves\nsignificant improvements in offline and online metrics and has been\nsuccessfully deployed on an industry leading recommendation platform serving\nbillions of users.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern large-scale recommendation systems rely heavily on user interaction\nhistory sequences to enhance the model performance. The advent of large\nlanguage models and sequential modeling techniques, particularly\ntransformer-like architectures, has led to significant advancements recently\n(e.g., HSTU, SIM, and TWIN models). While scaling to ultra-long user histories\n(10k to 100k items) generally improves model performance, it also creates\nsignificant challenges on latency, queries per second (QPS) and GPU cost in\nindustry-scale recommendation systems. Existing models do not adequately\naddress these industrial scalability issues. In this paper, we propose a novel\ntwo-stage modeling framework, namely VIrtual Sequential Target Attention\n(VISTA), which decomposes traditional target attention from a candidate item to\nuser history items into two distinct stages: (1) user history summarization\ninto a few hundred tokens; followed by (2) candidate item attention to those\ntokens. These summarization token embeddings are then cached in storage system\nand then utilized as sequence features for downstream model training and\ninference. This novel design for scalability enables VISTA to scale to lifelong\nuser histories (up to one million items) while keeping downstream training and\ninference costs fixed, which is essential in industry. Our approach achieves\nsignificant improvements in offline and online metrics and has been\nsuccessfully deployed on an industry leading recommendation platform serving\nbillions of users."
                },
                "authors": [
                    {
                        "name": "Zhimin Chen"
                    },
                    {
                        "name": "Chenyu Zhao"
                    },
                    {
                        "name": "Ka Chun Mo"
                    },
                    {
                        "name": "Yunjiang Jiang"
                    },
                    {
                        "name": "Jane H. Lee"
                    },
                    {
                        "name": "Shouwei Chen"
                    },
                    {
                        "name": "Khushhall Chandra Mahajan"
                    },
                    {
                        "name": "Ning Jiang"
                    },
                    {
                        "name": "Kai Ren"
                    },
                    {
                        "name": "Jinhui Li"
                    },
                    {
                        "name": "Wen-Yun Yang"
                    }
                ],
                "author_detail": {
                    "name": "Wen-Yun Yang"
                },
                "author": "Wen-Yun Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.22049v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.22049v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21696v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21696v1",
                "updated": "2025-10-24T17:56:37Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    17,
                    56,
                    37,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T17:56:37Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    17,
                    56,
                    37,
                    4,
                    297,
                    0
                ],
                "title": "BachVid: Training-Free Video Generation with Consistent Background and\n  Character",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BachVid: Training-Free Video Generation with Consistent Background and\n  Character"
                },
                "summary": "Diffusion Transformers (DiTs) have recently driven significant progress in\ntext-to-video (T2V) generation. However, generating multiple videos with\nconsistent characters and backgrounds remains a significant challenge. Existing\nmethods typically rely on reference images or extensive training, and often\nonly address character consistency, leaving background consistency to\nimage-to-video models. We introduce BachVid, the first training-free method\nthat achieves consistent video generation without needing any reference images.\nOur approach is based on a systematic analysis of DiT's attention mechanism and\nintermediate features, revealing its ability to extract foreground masks and\nidentify matching points during the denoising process. Our method leverages\nthis finding by first generating an identity video and caching the intermediate\nvariables, and then inject these cached variables into corresponding positions\nin newly generated videos, ensuring both foreground and background consistency\nacross multiple videos. Experimental results demonstrate that BachVid achieves\nrobust consistency in generated videos without requiring additional training,\noffering a novel and efficient solution for consistent video generation without\nrelying on reference images or additional training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) have recently driven significant progress in\ntext-to-video (T2V) generation. However, generating multiple videos with\nconsistent characters and backgrounds remains a significant challenge. Existing\nmethods typically rely on reference images or extensive training, and often\nonly address character consistency, leaving background consistency to\nimage-to-video models. We introduce BachVid, the first training-free method\nthat achieves consistent video generation without needing any reference images.\nOur approach is based on a systematic analysis of DiT's attention mechanism and\nintermediate features, revealing its ability to extract foreground masks and\nidentify matching points during the denoising process. Our method leverages\nthis finding by first generating an identity video and caching the intermediate\nvariables, and then inject these cached variables into corresponding positions\nin newly generated videos, ensuring both foreground and background consistency\nacross multiple videos. Experimental results demonstrate that BachVid achieves\nrobust consistency in generated videos without requiring additional training,\noffering a novel and efficient solution for consistent video generation without\nrelying on reference images or additional training."
                },
                "authors": [
                    {
                        "name": "Han Yan"
                    },
                    {
                        "name": "Xibin Song"
                    },
                    {
                        "name": "Yifu Wang"
                    },
                    {
                        "name": "Hongdong Li"
                    },
                    {
                        "name": "Pan Ji"
                    },
                    {
                        "name": "Chao Ma"
                    }
                ],
                "author_detail": {
                    "name": "Chao Ma"
                },
                "author": "Chao Ma",
                "arxiv_comment": "Project page: https://wolfball.github.io/bachvid",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21696v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21696v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.20787v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.20787v2",
                "updated": "2025-10-24T16:56:22Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    16,
                    56,
                    22,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-23T17:53:03Z",
                "published_parsed": [
                    2025,
                    10,
                    23,
                    17,
                    53,
                    3,
                    3,
                    296,
                    0
                ],
                "title": "Alleviating Forgetfulness of Linear Attention by Hybrid Sparse Attention\n  and Contextualized Learnable Token Eviction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Alleviating Forgetfulness of Linear Attention by Hybrid Sparse Attention\n  and Contextualized Learnable Token Eviction"
                },
                "summary": "Linear-attention models that compress the entire input sequence into a\nfixed-size recurrent state offer an efficient alternative to Transformers, but\ntheir finite memory induces forgetfulness that harms retrieval-intensive tasks.\nTo mitigate the issue, we explore a series of hybrid models that restore direct\naccess to past tokens. We interleave token mixers with intermediate time and\nspace complexity between linear and full attention, including sparse attention\nwith token eviction, and the query-aware native sparse attention. Particularly,\nwe propose a novel learnable token eviction approach. Combined with\nsliding-window attention, an end-to-end trainable lightweight CNN aggregates\ninformation from both past and future adjacent tokens to adaptively retain a\nlimited set of critical KV-pairs per head, maintaining linear attention's\nconstant time and space complexity. Efficient Triton kernels for the sparse\nattention mechanisms are provided. Empirical evaluations on retrieval-intensive\nbenchmarks support the effectiveness of our approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linear-attention models that compress the entire input sequence into a\nfixed-size recurrent state offer an efficient alternative to Transformers, but\ntheir finite memory induces forgetfulness that harms retrieval-intensive tasks.\nTo mitigate the issue, we explore a series of hybrid models that restore direct\naccess to past tokens. We interleave token mixers with intermediate time and\nspace complexity between linear and full attention, including sparse attention\nwith token eviction, and the query-aware native sparse attention. Particularly,\nwe propose a novel learnable token eviction approach. Combined with\nsliding-window attention, an end-to-end trainable lightweight CNN aggregates\ninformation from both past and future adjacent tokens to adaptively retain a\nlimited set of critical KV-pairs per head, maintaining linear attention's\nconstant time and space complexity. Efficient Triton kernels for the sparse\nattention mechanisms are provided. Empirical evaluations on retrieval-intensive\nbenchmarks support the effectiveness of our approaches."
                },
                "authors": [
                    {
                        "name": "Mutian He"
                    },
                    {
                        "name": "Philip N. Garner"
                    }
                ],
                "author_detail": {
                    "name": "Philip N. Garner"
                },
                "author": "Philip N. Garner",
                "arxiv_comment": "19 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.20787v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.20787v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17388v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17388v2",
                "updated": "2025-10-24T14:55:42Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    14,
                    55,
                    42,
                    4,
                    297,
                    0
                ],
                "published": "2025-09-22T06:52:35Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    6,
                    52,
                    35,
                    0,
                    265,
                    0
                ],
                "title": "Prefetching in Deep Memory Hierarchies with NVRAM as Main Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prefetching in Deep Memory Hierarchies with NVRAM as Main Memory"
                },
                "summary": "Emerging applications, such as big data analytics and machine learning,\nrequire increasingly large amounts of main memory, often exceeding the capacity\nof current commodity processors built on DRAM technology. To address this,\nrecent research has focused on off-chip memory controllers that facilitate\naccess to diverse memory media, each with unique density and latency\ncharacteristics. While these solutions improve memory system performance, they\nalso exacerbate the already significant memory latency. As a result,\nmulti-level prefetching techniques are essential to mitigate these extended\nlatencies.\n  This paper investigates the advantages of prefetching across both sides of\nthe memory system: the off-chip memory and the on-chip cache hierarchy. Our\nprimary objective is to assess the impact of a multi-level prefetching engine\non overall system performance. Additionally, we analyze the individual\ncontribution of each prefetching level to system efficiency. To achieve this,\nthe study evaluates two key prefetching approaches: HMC (Hybrid Memory\nController) and HMC+L1, both of which employ prefetching mechanisms commonly\nused by processor vendors. The HMC approach integrates a prefetcher within the\noff-chip hybrid memory controller, while the HMC+L1 approach combines this with\nadditional L1 on-chip prefetchers.\n  Experimental results on an out-of-order execution processor show that on-chip\ncache prefetchers are crucial for maximizing the benefits of off-chip\nprefetching, which in turn further enhances performance. Specifically, the\noff-chip HMC prefetcher achieves coverage and accuracy rates exceeding 60% and\nup to 80%, while the combined HMC+L1 approach boosts off-chip prefetcher\ncoverage to as much as 92%. Consequently, overall performance increases from 9%\nwith the HMC approach to 12% when L1 prefetching is also employed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emerging applications, such as big data analytics and machine learning,\nrequire increasingly large amounts of main memory, often exceeding the capacity\nof current commodity processors built on DRAM technology. To address this,\nrecent research has focused on off-chip memory controllers that facilitate\naccess to diverse memory media, each with unique density and latency\ncharacteristics. While these solutions improve memory system performance, they\nalso exacerbate the already significant memory latency. As a result,\nmulti-level prefetching techniques are essential to mitigate these extended\nlatencies.\n  This paper investigates the advantages of prefetching across both sides of\nthe memory system: the off-chip memory and the on-chip cache hierarchy. Our\nprimary objective is to assess the impact of a multi-level prefetching engine\non overall system performance. Additionally, we analyze the individual\ncontribution of each prefetching level to system efficiency. To achieve this,\nthe study evaluates two key prefetching approaches: HMC (Hybrid Memory\nController) and HMC+L1, both of which employ prefetching mechanisms commonly\nused by processor vendors. The HMC approach integrates a prefetcher within the\noff-chip hybrid memory controller, while the HMC+L1 approach combines this with\nadditional L1 on-chip prefetchers.\n  Experimental results on an out-of-order execution processor show that on-chip\ncache prefetchers are crucial for maximizing the benefits of off-chip\nprefetching, which in turn further enhances performance. Specifically, the\noff-chip HMC prefetcher achieves coverage and accuracy rates exceeding 60% and\nup to 80%, while the combined HMC+L1 approach boosts off-chip prefetcher\ncoverage to as much as 92%. Consequently, overall performance increases from 9%\nwith the HMC approach to 12% when L1 prefetching is also employed."
                },
                "authors": [
                    {
                        "name": "Manel Lurbe"
                    },
                    {
                        "name": "Miguel Avargues"
                    },
                    {
                        "name": "Salvador Petit"
                    },
                    {
                        "name": "Maria E. Gomez"
                    },
                    {
                        "name": "Rui Yang"
                    },
                    {
                        "name": "Guanhao Wang"
                    },
                    {
                        "name": "Julio Sahuquillo"
                    }
                ],
                "author_detail": {
                    "name": "Julio Sahuquillo"
                },
                "author": "Julio Sahuquillo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17388v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17388v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22922v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22922v2",
                "updated": "2025-10-24T11:53:34Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    11,
                    53,
                    34,
                    4,
                    297,
                    0
                ],
                "published": "2025-06-28T15:15:31Z",
                "published_parsed": [
                    2025,
                    6,
                    28,
                    15,
                    15,
                    31,
                    5,
                    179,
                    0
                ],
                "title": "Global Predecessor Indexing: Avoiding Binary Search in Weighted Job\n  Scheduling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Global Predecessor Indexing: Avoiding Binary Search in Weighted Job\n  Scheduling"
                },
                "summary": "We present an improved solution to the Weighted Job Scheduling (WJS) problem.\nWhile the classical dynamic programming (DP) solution for $n$ jobs runs in $O(n\n\\log(n))$ time due to comparison-based sorting and per-job binary search, we\neliminate the binary search bottleneck. In its place, we introduce a novel\nmulti-phase preprocessing technique called \\emph{Global Predecessor Indexing\n(GPI)}, which computes the latest non-overlapping job (i.e., the predecessor)\nfor all jobs via a two-pointer linear-time pass after sorting. This yields a\ntime complexity of $O(S(n) + n)$ where $S(n)$ is the time to sort all jobs. GPI\nenables direct use in the classical DP recurrence. When combined with\nlinear-time sorting, GPI yields a complete $O(n)$ solution. Even with\ncomparison-based sorting, GPI significantly outperforms the classical solution\nin practice by avoiding repeated binary searches in favor of the more\ncache-efficient extra sort and two-pointer pass.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present an improved solution to the Weighted Job Scheduling (WJS) problem.\nWhile the classical dynamic programming (DP) solution for $n$ jobs runs in $O(n\n\\log(n))$ time due to comparison-based sorting and per-job binary search, we\neliminate the binary search bottleneck. In its place, we introduce a novel\nmulti-phase preprocessing technique called \\emph{Global Predecessor Indexing\n(GPI)}, which computes the latest non-overlapping job (i.e., the predecessor)\nfor all jobs via a two-pointer linear-time pass after sorting. This yields a\ntime complexity of $O(S(n) + n)$ where $S(n)$ is the time to sort all jobs. GPI\nenables direct use in the classical DP recurrence. When combined with\nlinear-time sorting, GPI yields a complete $O(n)$ solution. Even with\ncomparison-based sorting, GPI significantly outperforms the classical solution\nin practice by avoiding repeated binary searches in favor of the more\ncache-efficient extra sort and two-pointer pass."
                },
                "authors": [
                    {
                        "name": "Amit Joshi"
                    }
                ],
                "author_detail": {
                    "name": "Amit Joshi"
                },
                "author": "Amit Joshi",
                "arxiv_comment": "6 pages, 9 figures including tables. Short theoretical and practical\n  paper on improved dynamic programming for weighted job scheduling with\n  linear-time preprocessing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22922v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22922v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21361v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21361v1",
                "updated": "2025-10-24T11:42:38Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    11,
                    42,
                    38,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T11:42:38Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    11,
                    42,
                    38,
                    4,
                    297,
                    0
                ],
                "title": "Compositional Monte Carlo Tree Diffusion for Extendable Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compositional Monte Carlo Tree Diffusion for Extendable Planning"
                },
                "summary": "Monte Carlo Tree Diffusion (MCTD) integrates diffusion models with structured\ntree search to enable effective trajectory exploration through stepwise\nreasoning. However, MCTD remains fundamentally limited by training trajectory\nlengths. While periodic replanning allows plan concatenation for longer plan\ngeneration, the planning process remains locally confined, as MCTD searches\nwithin individual trajectories without access to global context. We propose\nCompositional Monte Carlo Tree Diffusion (C-MCTD), a framework that elevates\nplanning from individual trajectory optimization to reasoning over complete\nplan compositions. C-MCTD introduces three complementary components: (1) Online\nComposer, which performs globally-aware planning by searching across entire\nplan compositions; (2) Distributed Composer, which reduces search complexity\nthrough parallel exploration from multiple starting points; and (3) Preplan\nComposer, which accelerates inference by leveraging cached plan graphs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Monte Carlo Tree Diffusion (MCTD) integrates diffusion models with structured\ntree search to enable effective trajectory exploration through stepwise\nreasoning. However, MCTD remains fundamentally limited by training trajectory\nlengths. While periodic replanning allows plan concatenation for longer plan\ngeneration, the planning process remains locally confined, as MCTD searches\nwithin individual trajectories without access to global context. We propose\nCompositional Monte Carlo Tree Diffusion (C-MCTD), a framework that elevates\nplanning from individual trajectory optimization to reasoning over complete\nplan compositions. C-MCTD introduces three complementary components: (1) Online\nComposer, which performs globally-aware planning by searching across entire\nplan compositions; (2) Distributed Composer, which reduces search complexity\nthrough parallel exploration from multiple starting points; and (3) Preplan\nComposer, which accelerates inference by leveraging cached plan graphs."
                },
                "authors": [
                    {
                        "name": "Jaesik Yoon"
                    },
                    {
                        "name": "Hyeonseo Cho"
                    },
                    {
                        "name": "Sungjin Ahn"
                    }
                ],
                "author_detail": {
                    "name": "Sungjin Ahn"
                },
                "author": "Sungjin Ahn",
                "arxiv_comment": "24 pages, 4 figures, NeurIPS 25 Spotlight",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21361v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21361v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16242v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16242v6",
                "updated": "2025-10-24T08:41:38Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    8,
                    41,
                    38,
                    4,
                    297,
                    0
                ],
                "published": "2025-07-22T05:26:28Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    5,
                    26,
                    28,
                    1,
                    203,
                    0
                ],
                "title": "Robustifying Learning-Augmented Caching Efficiently without Compromising\n  1-Consistency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robustifying Learning-Augmented Caching Efficiently without Compromising\n  1-Consistency"
                },
                "summary": "The online caching problem aims to minimize cache misses when serving a\nsequence of requests under a limited cache size. While naive learning-augmented\ncaching algorithms achieve ideal $1$-consistency, they lack robustness\nguarantees. Existing robustification methods either sacrifice $1$-consistency\nor introduce excessive computational overhead. In this paper, we introduce\nGuard, a lightweight robustification framework that enhances the robustness of\na broad class of learning-augmented caching algorithms to $2H_k + 2$, while\npreserving their $1$-consistency. Guard achieves the current best-known\ntrade-off between consistency and robustness, with only $O(1)$ additional\nper-request overhead, thereby maintaining the original time complexity of the\nbase algorithm. Extensive experiments across multiple real-world datasets and\nprediction models validate the effectiveness of Guard in practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The online caching problem aims to minimize cache misses when serving a\nsequence of requests under a limited cache size. While naive learning-augmented\ncaching algorithms achieve ideal $1$-consistency, they lack robustness\nguarantees. Existing robustification methods either sacrifice $1$-consistency\nor introduce excessive computational overhead. In this paper, we introduce\nGuard, a lightweight robustification framework that enhances the robustness of\na broad class of learning-augmented caching algorithms to $2H_k + 2$, while\npreserving their $1$-consistency. Guard achieves the current best-known\ntrade-off between consistency and robustness, with only $O(1)$ additional\nper-request overhead, thereby maintaining the original time complexity of the\nbase algorithm. Extensive experiments across multiple real-world datasets and\nprediction models validate the effectiveness of Guard in practice."
                },
                "authors": [
                    {
                        "name": "Peng Chen"
                    },
                    {
                        "name": "Hailiang Zhao"
                    },
                    {
                        "name": "Jiaji Zhang"
                    },
                    {
                        "name": "Xueyan Tang"
                    },
                    {
                        "name": "Yixuan Wang"
                    },
                    {
                        "name": "Shuiguang Deng"
                    }
                ],
                "author_detail": {
                    "name": "Shuiguang Deng"
                },
                "author": "Shuiguang Deng",
                "arxiv_comment": "Accepted to NeurIPS 2025.\n  https://neurips.cc/virtual/2025/poster/116615",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16242v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16242v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19240v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19240v2",
                "updated": "2025-10-24T08:35:21Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    8,
                    35,
                    21,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-22T04:48:41Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    4,
                    48,
                    41,
                    2,
                    295,
                    0
                ],
                "title": "A General Solution for the Implementation of CI/CD in Embedded Linux\n  Development",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A General Solution for the Implementation of CI/CD in Embedded Linux\n  Development"
                },
                "summary": "With the growing use of embedded systems in various industries, the need for\nautomated platforms for the development and deployment of customized\nLinux-based operating systems has become more important. This research was\nconducted with the aim of designing and implementing an integrated and\nreproducible infrastructure for the development, building, and testing of a\nLinux-based operating system using the Yocto Project. The proposed structure\nwas implemented based on a three-layer architecture consisting of the main\nYocto repositories, a custom layer (meta-custom), and a coordinating manifest\nlayer to ensure version synchronization, scalability, and reproducibility.\nThree sample projects, including libhelloworld, helloworld, and the kernel\nmodule hello mod, were developed and integrated into the build process.\nContinuous Integration and Continuous Deployment pipelines were implemented\nwith GitLab CI and combined with an isolated Docker environment to automate and\nstreamline the build and testing workflows. Using a local cache server\ncontaining hashserv, downloads and sstate cache significantly reduced the build\ntime. The functionality and stability of the system were verified through six\nboot test scenarios in the QEMU simulator. The results show that the proposed\ndesign not only ensures reproducibility but also can be extended to advanced\napplications such as continuous deployment of real-time Linux versions. Future\nrecommendations include expanding automated tests, implementing system\nmonitoring with Prometheus and Grafana, using distributed builds, optimizing\nwith Docker multi-stage builds, and enabling continuous deployment of real-time\nLinux changes to provide a stable and scalable model for industrial and\nresearch projects in embedded systems with a rapid and reliable development\ncycle.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the growing use of embedded systems in various industries, the need for\nautomated platforms for the development and deployment of customized\nLinux-based operating systems has become more important. This research was\nconducted with the aim of designing and implementing an integrated and\nreproducible infrastructure for the development, building, and testing of a\nLinux-based operating system using the Yocto Project. The proposed structure\nwas implemented based on a three-layer architecture consisting of the main\nYocto repositories, a custom layer (meta-custom), and a coordinating manifest\nlayer to ensure version synchronization, scalability, and reproducibility.\nThree sample projects, including libhelloworld, helloworld, and the kernel\nmodule hello mod, were developed and integrated into the build process.\nContinuous Integration and Continuous Deployment pipelines were implemented\nwith GitLab CI and combined with an isolated Docker environment to automate and\nstreamline the build and testing workflows. Using a local cache server\ncontaining hashserv, downloads and sstate cache significantly reduced the build\ntime. The functionality and stability of the system were verified through six\nboot test scenarios in the QEMU simulator. The results show that the proposed\ndesign not only ensures reproducibility but also can be extended to advanced\napplications such as continuous deployment of real-time Linux versions. Future\nrecommendations include expanding automated tests, implementing system\nmonitoring with Prometheus and Grafana, using distributed builds, optimizing\nwith Docker multi-stage builds, and enabling continuous deployment of real-time\nLinux changes to provide a stable and scalable model for industrial and\nresearch projects in embedded systems with a rapid and reliable development\ncycle."
                },
                "authors": [
                    {
                        "name": "Behnam Agahi"
                    },
                    {
                        "name": "Hamed Farbeh"
                    }
                ],
                "author_detail": {
                    "name": "Hamed Farbeh"
                },
                "author": "Hamed Farbeh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19240v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19240v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15745v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15745v2",
                "updated": "2025-10-24T05:39:03Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    5,
                    39,
                    3,
                    4,
                    297,
                    0
                ],
                "published": "2025-06-18T02:22:14Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    2,
                    22,
                    14,
                    2,
                    169,
                    0
                ],
                "title": "InfiniPot-V: Memory-Constrained KV Cache Compression for Streaming Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InfiniPot-V: Memory-Constrained KV Cache Compression for Streaming Video\n  Understanding"
                },
                "summary": "Modern multimodal large language models (MLLMs) can reason over hour-long\nvideo, yet their key-value (KV) cache grows linearly with time-quickly\nexceeding the fixed memory of phones, AR glasses, and edge robots. Prior\ncompression schemes either assume the whole video and user query are available\noffline or must first build the full cache, so memory still scales with stream\nlength. InfiniPot-V is the first training-free, query-agnostic framework that\nenforces a hard, length-independent memory cap for streaming video\nunderstanding. During video encoding it monitors the cache and, once a user-set\nthreshold is reached, runs a lightweight compression pass that (i) removes\ntemporally redundant tokens via Temporal-axis Redundancy (TaR) metric and (ii)\nkeeps semantically significant tokens via Value-Norm (VaN) ranking. Across four\nopen-source MLLMs and four long-video and streaming-video benchmarks,\nInfiniPot-V cuts peak GPU memory by up to 94%, sustains real-time generation,\nand matches or surpasses full-cache accuracy-even in multi-turn dialogues. By\ndissolving the KV cache bottleneck without retraining or query knowledge,\nInfiniPot-V closes the gap for on-device streaming video assistants.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern multimodal large language models (MLLMs) can reason over hour-long\nvideo, yet their key-value (KV) cache grows linearly with time-quickly\nexceeding the fixed memory of phones, AR glasses, and edge robots. Prior\ncompression schemes either assume the whole video and user query are available\noffline or must first build the full cache, so memory still scales with stream\nlength. InfiniPot-V is the first training-free, query-agnostic framework that\nenforces a hard, length-independent memory cap for streaming video\nunderstanding. During video encoding it monitors the cache and, once a user-set\nthreshold is reached, runs a lightweight compression pass that (i) removes\ntemporally redundant tokens via Temporal-axis Redundancy (TaR) metric and (ii)\nkeeps semantically significant tokens via Value-Norm (VaN) ranking. Across four\nopen-source MLLMs and four long-video and streaming-video benchmarks,\nInfiniPot-V cuts peak GPU memory by up to 94%, sustains real-time generation,\nand matches or surpasses full-cache accuracy-even in multi-turn dialogues. By\ndissolving the KV cache bottleneck without retraining or query knowledge,\nInfiniPot-V closes the gap for on-device streaming video assistants."
                },
                "authors": [
                    {
                        "name": "Minsoo Kim"
                    },
                    {
                        "name": "Kyuhong Shim"
                    },
                    {
                        "name": "Jungwook Choi"
                    },
                    {
                        "name": "Simyung Chang"
                    }
                ],
                "author_detail": {
                    "name": "Simyung Chang"
                },
                "author": "Simyung Chang",
                "arxiv_comment": "NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15745v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15745v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13866v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13866v2",
                "updated": "2025-10-24T04:48:06Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    4,
                    48,
                    6,
                    4,
                    297,
                    0
                ],
                "published": "2025-05-20T03:21:52Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    3,
                    21,
                    52,
                    1,
                    140,
                    0
                ],
                "title": "Reasoning Path Compression: Compressing Generation Trajectories for\n  Efficient LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning Path Compression: Compressing Generation Trajectories for\n  Efficient LLM Reasoning"
                },
                "summary": "Recent reasoning-focused language models achieve high accuracy by generating\nlengthy intermediate reasoning paths before producing final answers. While this\napproach is effective in solving problems that require logical thinking, long\nreasoning paths significantly increase memory usage and reduce throughput of\ntoken generation, limiting the practical deployment of such models. We propose\nReasoning Path Compression (RPC), a training-free method that accelerates\ninference by leveraging the semantic sparsity of reasoning paths. RPC\nperiodically compresses the KV cache by retaining cache entries that receive\nhigh importance score, which are computed using a selector window composed of\nrecently generated queries. Experiments show that RPC improves generation\nthroughput of QwQ-32B by up to 1.60$\\times$ compared to the inference with full\nKV cache, with an accuracy drop of 1.2\\% on the AIME 2024 benchmark. Our\nfindings demonstrate that semantic sparsity in reasoning traces can be\neffectively exploited for compression, offering a practical path toward\nefficient deployment of reasoning LLMs. Our code is available at\nhttps://github.com/jiwonsong-dev/ReasoningPathCompression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent reasoning-focused language models achieve high accuracy by generating\nlengthy intermediate reasoning paths before producing final answers. While this\napproach is effective in solving problems that require logical thinking, long\nreasoning paths significantly increase memory usage and reduce throughput of\ntoken generation, limiting the practical deployment of such models. We propose\nReasoning Path Compression (RPC), a training-free method that accelerates\ninference by leveraging the semantic sparsity of reasoning paths. RPC\nperiodically compresses the KV cache by retaining cache entries that receive\nhigh importance score, which are computed using a selector window composed of\nrecently generated queries. Experiments show that RPC improves generation\nthroughput of QwQ-32B by up to 1.60$\\times$ compared to the inference with full\nKV cache, with an accuracy drop of 1.2\\% on the AIME 2024 benchmark. Our\nfindings demonstrate that semantic sparsity in reasoning traces can be\neffectively exploited for compression, offering a practical path toward\nefficient deployment of reasoning LLMs. Our code is available at\nhttps://github.com/jiwonsong-dev/ReasoningPathCompression."
                },
                "authors": [
                    {
                        "name": "Jiwon Song"
                    },
                    {
                        "name": "Dongwon Jo"
                    },
                    {
                        "name": "Yulhwa Kim"
                    },
                    {
                        "name": "Jae-Joon Kim"
                    }
                ],
                "author_detail": {
                    "name": "Jae-Joon Kim"
                },
                "author": "Jae-Joon Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13866v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13866v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06425v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06425v5",
                "updated": "2025-10-23T23:35:32Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    23,
                    35,
                    32,
                    3,
                    296,
                    0
                ],
                "published": "2025-01-11T03:37:10Z",
                "published_parsed": [
                    2025,
                    1,
                    11,
                    3,
                    37,
                    10,
                    5,
                    11,
                    0
                ],
                "title": "Tensor Product Attention Is All You Need",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tensor Product Attention Is All You Need"
                },
                "summary": "Scaling language models to handle longer input sequences typically\nnecessitates large key-value (KV) caches, resulting in substantial memory\noverhead during inference. In this paper, we propose Tensor Product Attention\n(TPA), a novel attention mechanism that uses tensor decompositions to represent\nqueries, keys, and values compactly, substantially shrinking the KV cache size\nat inference time. By factorizing these representations into contextual\nlow-rank components and seamlessly integrating with Rotary Position Embedding\n(RoPE), TPA achieves improved model quality alongside memory efficiency. Based\non TPA, we introduce the Tensor ProducT ATTenTion Transformer (T6), a new model\narchitecture for sequence modeling. Through extensive empirical evaluation on\nlanguage modeling tasks, we demonstrate that T6 surpasses or matches the\nperformance of standard Transformer baselines including Multi-Head Attention\n(MHA), Multi-Query Attention (MQA), Grouped-Query Attention (GQA), and\nMulti-Head Latent Attention (MLA) across various metrics, including perplexity\nand a range of established evaluation benchmarks. Notably, TPA's memory\nefficiency and computational efficiency at decoding stage enables processing\nlonger sequences under fixed resource constraints, addressing a critical\nscalability challenge in modern language models. Project Page:\nhttps://github.com/tensorgi/TPA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling language models to handle longer input sequences typically\nnecessitates large key-value (KV) caches, resulting in substantial memory\noverhead during inference. In this paper, we propose Tensor Product Attention\n(TPA), a novel attention mechanism that uses tensor decompositions to represent\nqueries, keys, and values compactly, substantially shrinking the KV cache size\nat inference time. By factorizing these representations into contextual\nlow-rank components and seamlessly integrating with Rotary Position Embedding\n(RoPE), TPA achieves improved model quality alongside memory efficiency. Based\non TPA, we introduce the Tensor ProducT ATTenTion Transformer (T6), a new model\narchitecture for sequence modeling. Through extensive empirical evaluation on\nlanguage modeling tasks, we demonstrate that T6 surpasses or matches the\nperformance of standard Transformer baselines including Multi-Head Attention\n(MHA), Multi-Query Attention (MQA), Grouped-Query Attention (GQA), and\nMulti-Head Latent Attention (MLA) across various metrics, including perplexity\nand a range of established evaluation benchmarks. Notably, TPA's memory\nefficiency and computational efficiency at decoding stage enables processing\nlonger sequences under fixed resource constraints, addressing a critical\nscalability challenge in modern language models. Project Page:\nhttps://github.com/tensorgi/TPA."
                },
                "authors": [
                    {
                        "name": "Yifan Zhang"
                    },
                    {
                        "name": "Yifeng Liu"
                    },
                    {
                        "name": "Huizhuo Yuan"
                    },
                    {
                        "name": "Zhen Qin"
                    },
                    {
                        "name": "Yang Yuan"
                    },
                    {
                        "name": "Quanquan Gu"
                    },
                    {
                        "name": "Andrew Chi-Chih Yao"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Chi-Chih Yao"
                },
                "author": "Andrew Chi-Chih Yao",
                "arxiv_comment": "Published in NeurIPS 2025 (Spotlight); Project Page:\n  https://github.com/tensorgi/TPA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06425v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06425v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16986v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16986v2",
                "updated": "2025-10-23T21:31:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    21,
                    31,
                    35,
                    3,
                    296,
                    0
                ],
                "published": "2025-05-22T17:54:32Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    54,
                    32,
                    3,
                    142,
                    0
                ],
                "title": "T1: A Tool-Oriented Conversational Dataset for Multi-Turn Agentic\n  Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "T1: A Tool-Oriented Conversational Dataset for Multi-Turn Agentic\n  Planning"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities as\nintelligent agents capable of solving complex problems. However, effective\nplanning in scenarios involving dependencies between API or tool\ncalls-particularly in multi-turn conversations-remains a significant challenge.\nTo address this, we introduce T1, a tool-augmented, multi-domain, multi-turn\nconversational dataset specifically designed to capture and manage inter-tool\ndependencies across diverse domains. T1 enables rigorous evaluation of agents'\nability to coordinate tool use across nine distinct domains (4 single domain\nand 5 multi-domain) with the help of an integrated caching mechanism for both\nshort- and long-term memory, while supporting dynamic replanning-such as\ndeciding whether to recompute or reuse cached results. Beyond facilitating\nresearch on tool use and planning, T1 also serves as a benchmark for evaluating\nthe performance of open-weight and proprietary large language models. We\npresent results powered by T1-Agent, highlighting their ability to plan and\nreason in complex, tool-dependent scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive capabilities as\nintelligent agents capable of solving complex problems. However, effective\nplanning in scenarios involving dependencies between API or tool\ncalls-particularly in multi-turn conversations-remains a significant challenge.\nTo address this, we introduce T1, a tool-augmented, multi-domain, multi-turn\nconversational dataset specifically designed to capture and manage inter-tool\ndependencies across diverse domains. T1 enables rigorous evaluation of agents'\nability to coordinate tool use across nine distinct domains (4 single domain\nand 5 multi-domain) with the help of an integrated caching mechanism for both\nshort- and long-term memory, while supporting dynamic replanning-such as\ndeciding whether to recompute or reuse cached results. Beyond facilitating\nresearch on tool use and planning, T1 also serves as a benchmark for evaluating\nthe performance of open-weight and proprietary large language models. We\npresent results powered by T1-Agent, highlighting their ability to plan and\nreason in complex, tool-dependent scenarios."
                },
                "authors": [
                    {
                        "name": "Amartya Chakraborty"
                    },
                    {
                        "name": "Paresh Dashore"
                    },
                    {
                        "name": "Nadia Bathaee"
                    },
                    {
                        "name": "Anmol Jain"
                    },
                    {
                        "name": "Anirban Das"
                    },
                    {
                        "name": "Shi-Xiong Zhang"
                    },
                    {
                        "name": "Sambit Sahu"
                    },
                    {
                        "name": "Milind Naphade"
                    },
                    {
                        "name": "Genta Indra Winata"
                    }
                ],
                "author_detail": {
                    "name": "Genta Indra Winata"
                },
                "author": "Genta Indra Winata",
                "arxiv_comment": "Accepted by NeurIPS 2025 Datasets and Benchmarks Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16986v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16986v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.00566v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.00566v3",
                "updated": "2025-10-23T19:45:39Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    19,
                    45,
                    39,
                    3,
                    296,
                    0
                ],
                "published": "2025-10-01T06:38:45Z",
                "published_parsed": [
                    2025,
                    10,
                    1,
                    6,
                    38,
                    45,
                    2,
                    274,
                    0
                ],
                "title": "Panorama: Fast-Track Nearest Neighbors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Panorama: Fast-Track Nearest Neighbors"
                },
                "summary": "Approximate Nearest-Neighbor Search (ANNS) efficiently finds data items whose\nembeddings are close to that of a given query in a high-dimensional space,\naiming to balance accuracy with speed. Used in recommendation systems, image\nand video retrieval, natural language processing, and retrieval-augmented\ngeneration (RAG), ANNS algorithms such as IVFPQ, HNSW graphs, Annoy, and MRPT\nutilize graph, tree, clustering, and quantization techniques to navigate large\nvector spaces. Despite this progress, ANNS systems spend up to 99% of query\ntime to compute distances in their final refinement phase. In this paper, we\npresent PANORAMA, a machine learning-driven approach that tackles the ANNS\nverification bottleneck through data-adaptive learned orthogonal transforms\nthat facilitate the accretive refinement of distance bounds. Such transforms\ncompact over 90% of signal energy into the first half of dimensions, enabling\nearly candidate pruning with partial distance computations. We integrate\nPANORAMA into state-of-the-art ANNS methods, namely IVFPQ/Flat, HNSW, MRPT, and\nAnnoy, without index modification, using level-major memory layouts,\nSIMD-vectorized partial distance computations, and cache-aware access patterns.\nExperiments across diverse datasets -- from image-based CIFAR-10 and GIST to\nmodern embedding spaces including OpenAI's Ada 2 and Large 3 -- demonstrate\nthat PANORAMA affords a 2--30$\\times$ end-to-end speedup with no recall loss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Approximate Nearest-Neighbor Search (ANNS) efficiently finds data items whose\nembeddings are close to that of a given query in a high-dimensional space,\naiming to balance accuracy with speed. Used in recommendation systems, image\nand video retrieval, natural language processing, and retrieval-augmented\ngeneration (RAG), ANNS algorithms such as IVFPQ, HNSW graphs, Annoy, and MRPT\nutilize graph, tree, clustering, and quantization techniques to navigate large\nvector spaces. Despite this progress, ANNS systems spend up to 99% of query\ntime to compute distances in their final refinement phase. In this paper, we\npresent PANORAMA, a machine learning-driven approach that tackles the ANNS\nverification bottleneck through data-adaptive learned orthogonal transforms\nthat facilitate the accretive refinement of distance bounds. Such transforms\ncompact over 90% of signal energy into the first half of dimensions, enabling\nearly candidate pruning with partial distance computations. We integrate\nPANORAMA into state-of-the-art ANNS methods, namely IVFPQ/Flat, HNSW, MRPT, and\nAnnoy, without index modification, using level-major memory layouts,\nSIMD-vectorized partial distance computations, and cache-aware access patterns.\nExperiments across diverse datasets -- from image-based CIFAR-10 and GIST to\nmodern embedding spaces including OpenAI's Ada 2 and Large 3 -- demonstrate\nthat PANORAMA affords a 2--30$\\times$ end-to-end speedup with no recall loss."
                },
                "authors": [
                    {
                        "name": "Vansh Ramani"
                    },
                    {
                        "name": "Alexis Schlomer"
                    },
                    {
                        "name": "Akash Nayar"
                    },
                    {
                        "name": "Sayan Ranu"
                    },
                    {
                        "name": "Jignesh M. Patel"
                    },
                    {
                        "name": "Panagiotis Karras"
                    }
                ],
                "author_detail": {
                    "name": "Panagiotis Karras"
                },
                "author": "Panagiotis Karras",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.00566v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.00566v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.07254v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.07254v3",
                "updated": "2025-10-23T18:52:25Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    18,
                    52,
                    25,
                    3,
                    296,
                    0
                ],
                "published": "2025-06-08T18:43:31Z",
                "published_parsed": [
                    2025,
                    6,
                    8,
                    18,
                    43,
                    31,
                    6,
                    159,
                    0
                ],
                "title": "A Stable Whitening Optimizer for Efficient Neural Network Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Stable Whitening Optimizer for Efficient Neural Network Training"
                },
                "summary": "In this work, we take an experimentally grounded look at neural network\noptimization. Building on the Shampoo family of algorithms, we identify and\nalleviate three key issues, resulting in the proposed SPlus method. First, we\nfind that naive Shampoo is prone to divergence when matrix-inverses are cached\nfor long periods. We introduce an alternate bounded update combining a\nhistorical eigenbasis with instantaneous normalization, resulting in\nacross-the-board stability and significantly lower computational requirements.\nSecond, we adapt a shape-aware scaling to enable learning rate transfer across\nnetwork width. Third, we find that high learning rates result in large\nparameter noise, and propose a simple iterate-averaging scheme which unblocks\nfaster learning. To properly confirm these findings, we introduce a pointed\nTransformer training benchmark, considering three objectives (language\nmodelling, image classification, and diffusion modelling) across different\nstages of training. On average, SPlus is able to reach the validation\nperformance of Adam within 44-58% of the gradient steps and 62-83% of the\nwallclock time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we take an experimentally grounded look at neural network\noptimization. Building on the Shampoo family of algorithms, we identify and\nalleviate three key issues, resulting in the proposed SPlus method. First, we\nfind that naive Shampoo is prone to divergence when matrix-inverses are cached\nfor long periods. We introduce an alternate bounded update combining a\nhistorical eigenbasis with instantaneous normalization, resulting in\nacross-the-board stability and significantly lower computational requirements.\nSecond, we adapt a shape-aware scaling to enable learning rate transfer across\nnetwork width. Third, we find that high learning rates result in large\nparameter noise, and propose a simple iterate-averaging scheme which unblocks\nfaster learning. To properly confirm these findings, we introduce a pointed\nTransformer training benchmark, considering three objectives (language\nmodelling, image classification, and diffusion modelling) across different\nstages of training. On average, SPlus is able to reach the validation\nperformance of Adam within 44-58% of the gradient steps and 62-83% of the\nwallclock time."
                },
                "authors": [
                    {
                        "name": "Kevin Frans"
                    },
                    {
                        "name": "Sergey Levine"
                    },
                    {
                        "name": "Pieter Abbeel"
                    }
                ],
                "author_detail": {
                    "name": "Pieter Abbeel"
                },
                "author": "Pieter Abbeel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.07254v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.07254v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.20707v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.20707v1",
                "updated": "2025-10-23T16:17:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    16,
                    17,
                    47,
                    3,
                    296,
                    0
                ],
                "published": "2025-10-23T16:17:47Z",
                "published_parsed": [
                    2025,
                    10,
                    23,
                    16,
                    17,
                    47,
                    3,
                    296,
                    0
                ],
                "title": "Mixing Importance with Diversity: Joint Optimization for KV Cache\n  Compression in Large Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixing Importance with Diversity: Joint Optimization for KV Cache\n  Compression in Large Vision-Language Models"
                },
                "summary": "Recent large vision-language models (LVLMs) demonstrate remarkable\ncapabilities in processing extended multi-modal sequences, yet the resulting\nkey-value (KV) cache expansion creates a critical memory bottleneck that\nfundamentally limits deployment scalability. While existing KV cache\ncompression methods focus on retaining high-importance KV pairs to minimize\nstorage, they often overlook the modality-specific semantic redundancy patterns\nthat emerge distinctively in multi-modal KV caches. In this work, we first\nanalyze how, beyond simple importance, the KV cache in LVLMs exhibits varying\nlevels of redundancy across attention heads. We show that relying solely on\nimportance can only cover a subset of the full KV cache information\ndistribution, leading to potential loss of semantic coverage. To address this,\nwe propose \\texttt{MixKV}, a novel method that mixes importance with diversity\nfor optimized KV cache compression in LVLMs. \\texttt{MixKV} adapts to head-wise\nsemantic redundancy, selectively balancing diversity and importance when\ncompressing KV pairs. Extensive experiments demonstrate that \\texttt{MixKV}\nconsistently enhances existing methods across multiple LVLMs. Under extreme\ncompression (budget=64), \\texttt{MixKV} improves baseline methods by an average\nof \\textbf{5.1\\%} across five multi-modal understanding benchmarks and achieves\nremarkable gains of \\textbf{8.0\\%} and \\textbf{9.0\\%} for SnapKV and AdaKV on\nGUI grounding tasks, all while maintaining comparable inference efficiency.\nFurthermore, \\texttt{MixKV} extends seamlessly to LLMs with comparable\nperformance gains. Our code is available at\n\\href{https://github.com/xuyang-liu16/MixKV}{\\textcolor{citeblue}{https://github.com/xuyang-liu16/MixKV}}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large vision-language models (LVLMs) demonstrate remarkable\ncapabilities in processing extended multi-modal sequences, yet the resulting\nkey-value (KV) cache expansion creates a critical memory bottleneck that\nfundamentally limits deployment scalability. While existing KV cache\ncompression methods focus on retaining high-importance KV pairs to minimize\nstorage, they often overlook the modality-specific semantic redundancy patterns\nthat emerge distinctively in multi-modal KV caches. In this work, we first\nanalyze how, beyond simple importance, the KV cache in LVLMs exhibits varying\nlevels of redundancy across attention heads. We show that relying solely on\nimportance can only cover a subset of the full KV cache information\ndistribution, leading to potential loss of semantic coverage. To address this,\nwe propose \\texttt{MixKV}, a novel method that mixes importance with diversity\nfor optimized KV cache compression in LVLMs. \\texttt{MixKV} adapts to head-wise\nsemantic redundancy, selectively balancing diversity and importance when\ncompressing KV pairs. Extensive experiments demonstrate that \\texttt{MixKV}\nconsistently enhances existing methods across multiple LVLMs. Under extreme\ncompression (budget=64), \\texttt{MixKV} improves baseline methods by an average\nof \\textbf{5.1\\%} across five multi-modal understanding benchmarks and achieves\nremarkable gains of \\textbf{8.0\\%} and \\textbf{9.0\\%} for SnapKV and AdaKV on\nGUI grounding tasks, all while maintaining comparable inference efficiency.\nFurthermore, \\texttt{MixKV} extends seamlessly to LLMs with comparable\nperformance gains. Our code is available at\n\\href{https://github.com/xuyang-liu16/MixKV}{\\textcolor{citeblue}{https://github.com/xuyang-liu16/MixKV}}."
                },
                "authors": [
                    {
                        "name": "Xuyang Liu"
                    },
                    {
                        "name": "Xiyan Gui"
                    },
                    {
                        "name": "Yuchao Zhang"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "Our code is available at https://github.com/xuyang-liu16/MixKV",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.20707v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.20707v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16407v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16407v2",
                "updated": "2025-10-23T15:26:38Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    15,
                    26,
                    38,
                    3,
                    296,
                    0
                ],
                "published": "2025-09-19T20:31:38Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    20,
                    31,
                    38,
                    4,
                    262,
                    0
                ],
                "title": "WarpSpeed: A High-Performance Library for Concurrent GPU Hash Tables",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WarpSpeed: A High-Performance Library for Concurrent GPU Hash Tables"
                },
                "summary": "GPU hash tables are increasingly used to accelerate data processing, but\ntheir limited functionality restricts adoption in large-scale data processing\napplications. Current limitations include incomplete concurrency support and\nmissing compound operations such as upserts.\n  This paper presents WarpSpeed, a library of high-performance concurrent GPU\nhash tables with a unified benchmarking framework for performance analysis.\nWarpSpeed implements eight state-of-the-art Nvidia GPU hash table designs and\nprovides a rich API designed for modern GPU applications. Our evaluation uses\ndiverse benchmarks to assess both correctness and scalability, and we\ndemonstrate real-world impact by integrating these hash tables into three\ndownstream applications.\n  We propose several optimization techniques to reduce concurrency overhead,\nincluding fingerprint-based metadata to minimize cache line probes and\nspecialized Nvidia GPU instructions for lock-free queries. Our findings provide\nnew insights into concurrent GPU hash table design and offer practical guidance\nfor developing efficient, scalable data structures on modern GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPU hash tables are increasingly used to accelerate data processing, but\ntheir limited functionality restricts adoption in large-scale data processing\napplications. Current limitations include incomplete concurrency support and\nmissing compound operations such as upserts.\n  This paper presents WarpSpeed, a library of high-performance concurrent GPU\nhash tables with a unified benchmarking framework for performance analysis.\nWarpSpeed implements eight state-of-the-art Nvidia GPU hash table designs and\nprovides a rich API designed for modern GPU applications. Our evaluation uses\ndiverse benchmarks to assess both correctness and scalability, and we\ndemonstrate real-world impact by integrating these hash tables into three\ndownstream applications.\n  We propose several optimization techniques to reduce concurrency overhead,\nincluding fingerprint-based metadata to minimize cache line probes and\nspecialized Nvidia GPU instructions for lock-free queries. Our findings provide\nnew insights into concurrent GPU hash table design and offer practical guidance\nfor developing efficient, scalable data structures on modern GPUs."
                },
                "authors": [
                    {
                        "name": "Hunter McCoy"
                    },
                    {
                        "name": "Prashant Pandey"
                    }
                ],
                "author_detail": {
                    "name": "Prashant Pandey"
                },
                "author": "Prashant Pandey",
                "arxiv_comment": "Accepted to ALENEX`26",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16407v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16407v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13251v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13251v4",
                "updated": "2025-10-23T14:23:24Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    14,
                    23,
                    24,
                    3,
                    296,
                    0
                ],
                "published": "2025-02-18T19:22:44Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    19,
                    22,
                    44,
                    1,
                    49,
                    0
                ],
                "title": "Neural Attention Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Attention Search"
                },
                "summary": "We present Neural Attention Search (NAtS), a framework that automatically\nevaluates the importance of each token within a sequence and determines if the\ncorresponding token can be dropped after several steps. This approach can\nefficiently reduce the KV cache sizes required by transformer-based models\nduring inference and thus reduce inference costs. In this paper, we design a\nsearch space that contains three token types: (i) Global Tokens will be\npreserved and queried by all the following tokens. (ii) Local Tokens survive\nuntil the next global token appears. (iii) Sliding Window Tokens have an impact\non the inference of a fixed size of the next following tokens. Similar to the\nOne-Shot Neural Architecture Search approach, this token-type information can\nbe learned jointly with the architecture weights via a learnable attention\nmask. Experiments on both training a new transformer from scratch and\nfine-tuning existing large language models show that NAtS can efficiently\nreduce the KV cache size required for the models while maintaining the models'\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Neural Attention Search (NAtS), a framework that automatically\nevaluates the importance of each token within a sequence and determines if the\ncorresponding token can be dropped after several steps. This approach can\nefficiently reduce the KV cache sizes required by transformer-based models\nduring inference and thus reduce inference costs. In this paper, we design a\nsearch space that contains three token types: (i) Global Tokens will be\npreserved and queried by all the following tokens. (ii) Local Tokens survive\nuntil the next global token appears. (iii) Sliding Window Tokens have an impact\non the inference of a fixed size of the next following tokens. Similar to the\nOne-Shot Neural Architecture Search approach, this token-type information can\nbe learned jointly with the architecture weights via a learnable attention\nmask. Experiments on both training a new transformer from scratch and\nfine-tuning existing large language models show that NAtS can efficiently\nreduce the KV cache size required for the models while maintaining the models'\nperformance."
                },
                "authors": [
                    {
                        "name": "Difan Deng"
                    },
                    {
                        "name": "Marius Lindauer"
                    }
                ],
                "author_detail": {
                    "name": "Marius Lindauer"
                },
                "author": "Marius Lindauer",
                "arxiv_comment": "35 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13251v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13251v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.20499v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.20499v1",
                "updated": "2025-10-23T12:39:59Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    12,
                    39,
                    59,
                    3,
                    296,
                    0
                ],
                "published": "2025-10-23T12:39:59Z",
                "published_parsed": [
                    2025,
                    10,
                    23,
                    12,
                    39,
                    59,
                    3,
                    296,
                    0
                ],
                "title": "GPU-Accelerated Primal Heuristics for Mixed Integer Programming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPU-Accelerated Primal Heuristics for Mixed Integer Programming"
                },
                "summary": "We introduce a fusion of GPU accelerated primal heuristics for Mixed Integer\nProgramming. Leveraging GPU acceleration enables exploration of larger search\nregions and faster iterations. A GPU-accelerated PDLP serves as an approximate\nLP solver, while a new probing cache facilitates rapid roundings and early\ninfeasibility detection. Several state-of-the-art heuristics, including\nFeasibility Pump, Feasibility Jump, and Fix-and-Propagate, are further\naccelerated and enhanced. The combined approach of these GPU-driven algorithms\nyields significant improvements over existing methods, both in the number of\nfeasible solutions and the quality of objectives by achieving 221 feasible\nsolutions and 22% objective gap in the MIPLIB2017 benchmark on a presolved\ndataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a fusion of GPU accelerated primal heuristics for Mixed Integer\nProgramming. Leveraging GPU acceleration enables exploration of larger search\nregions and faster iterations. A GPU-accelerated PDLP serves as an approximate\nLP solver, while a new probing cache facilitates rapid roundings and early\ninfeasibility detection. Several state-of-the-art heuristics, including\nFeasibility Pump, Feasibility Jump, and Fix-and-Propagate, are further\naccelerated and enhanced. The combined approach of these GPU-driven algorithms\nyields significant improvements over existing methods, both in the number of\nfeasible solutions and the quality of objectives by achieving 221 feasible\nsolutions and 22% objective gap in the MIPLIB2017 benchmark on a presolved\ndataset."
                },
                "authors": [
                    {
                        "name": "Akif Çördük"
                    },
                    {
                        "name": "Piotr Sielski"
                    },
                    {
                        "name": "Alice Boucher"
                    },
                    {
                        "name": "Kumar Aatish"
                    }
                ],
                "author_detail": {
                    "name": "Kumar Aatish"
                },
                "author": "Kumar Aatish",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.20499v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.20499v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.20878v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.20878v1",
                "updated": "2025-10-23T12:28:58Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    12,
                    28,
                    58,
                    3,
                    296,
                    0
                ],
                "published": "2025-10-23T12:28:58Z",
                "published_parsed": [
                    2025,
                    10,
                    23,
                    12,
                    28,
                    58,
                    3,
                    296,
                    0
                ],
                "title": "HA-RAG: Hotness-Aware RAG Acceleration via Mixed Precision and Data\n  Placement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HA-RAG: Hotness-Aware RAG Acceleration via Mixed Precision and Data\n  Placement"
                },
                "summary": "Retrieval-Augmented Generation (RAG) improves model output accuracy by\nleveraging external knowledge bases, serving as an effective solution to\naddress hallucination issues and knowledge-update delays in Large Language\nModels (LLMs). However, the introduction of external knowledge bases presents\nRAG with challenges in long-context processing, significantly increasing memory\nconsumption and inference latency. Existing research accelerates inference by\nprecomputing Key and Value (KV) of the knowledge base and loading them\non-demand during inference. Based on the access frequency of different KV\nchunks within the external knowledge base, this paper proposes a hotness-aware\nRAG (HA-RAG) inference optimization system. First, leveraging the numerical\ndistribution of KV chunks, we introduce a hotness-aware mixed-precision\ncompressing and loading method to reduce disk I/O and memory access overhead.\nSecond, we design a hotness-aware data placement strategy that prioritizes\nstoring frequently accessed KV chunks in high-speed memory to improve data\naccess efficiency. Experimental results demonstrate that, compared with\nTurboRAG, the proposed HA-RAG achieves an average speedup of 2.10x and maximum\nspeedup of 10.49x in Time-To-First-Token (TTFT) with negligible accuracy loss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) improves model output accuracy by\nleveraging external knowledge bases, serving as an effective solution to\naddress hallucination issues and knowledge-update delays in Large Language\nModels (LLMs). However, the introduction of external knowledge bases presents\nRAG with challenges in long-context processing, significantly increasing memory\nconsumption and inference latency. Existing research accelerates inference by\nprecomputing Key and Value (KV) of the knowledge base and loading them\non-demand during inference. Based on the access frequency of different KV\nchunks within the external knowledge base, this paper proposes a hotness-aware\nRAG (HA-RAG) inference optimization system. First, leveraging the numerical\ndistribution of KV chunks, we introduce a hotness-aware mixed-precision\ncompressing and loading method to reduce disk I/O and memory access overhead.\nSecond, we design a hotness-aware data placement strategy that prioritizes\nstoring frequently accessed KV chunks in high-speed memory to improve data\naccess efficiency. Experimental results demonstrate that, compared with\nTurboRAG, the proposed HA-RAG achieves an average speedup of 2.10x and maximum\nspeedup of 10.49x in Time-To-First-Token (TTFT) with negligible accuracy loss."
                },
                "authors": [
                    {
                        "name": "Danying Ge"
                    },
                    {
                        "name": "Jianhua Gao"
                    },
                    {
                        "name": "Yixue Yang"
                    },
                    {
                        "name": "Weixing Ji"
                    }
                ],
                "author_detail": {
                    "name": "Weixing Ji"
                },
                "author": "Weixing Ji",
                "arxiv_comment": "13 pages,16 figures,2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.20878v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.20878v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.4; E.4; I.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21865v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21865v1",
                "updated": "2025-10-23T10:35:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    10,
                    35,
                    35,
                    3,
                    296,
                    0
                ],
                "published": "2025-10-23T10:35:35Z",
                "published_parsed": [
                    2025,
                    10,
                    23,
                    10,
                    35,
                    35,
                    3,
                    296,
                    0
                ],
                "title": "Prefetching Cache Optimization Using Graph Neural Networks: A Modular\n  Framework and Conceptual Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prefetching Cache Optimization Using Graph Neural Networks: A Modular\n  Framework and Conceptual Analysis"
                },
                "summary": "Caching and prefetching techniques are fundamental to modern computing,\nserving to bridge the growing performance gap between processors and memory.\nTraditional prefetching strategies are often limited by their reliance on\npredefined heuristics or simplified statistical models, which fail to capture\nthe complex, non-linear dependencies in modern data access patterns. This paper\nintroduces a modular framework leveraging Graph Neural Networks (GNNs) to model\nand predict access patterns within graph-structured data, focusing on web\nnavigation and hierarchical file systems. The toolchain consists of: a route\nmapper for extracting structural information, a graph constructor for creating\ngraph representations, a walk session generator for simulating user behaviors,\nand a gnn prefetch module for training and inference. We provide a detailed\nconceptual analysis showing how GNN-based approaches can outperform\nconventional methods by learning intricate dependencies. This work offers both\ntheoretical foundations and a practical, replicable pipeline for future\nresearch in graph-driven systems optimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching and prefetching techniques are fundamental to modern computing,\nserving to bridge the growing performance gap between processors and memory.\nTraditional prefetching strategies are often limited by their reliance on\npredefined heuristics or simplified statistical models, which fail to capture\nthe complex, non-linear dependencies in modern data access patterns. This paper\nintroduces a modular framework leveraging Graph Neural Networks (GNNs) to model\nand predict access patterns within graph-structured data, focusing on web\nnavigation and hierarchical file systems. The toolchain consists of: a route\nmapper for extracting structural information, a graph constructor for creating\ngraph representations, a walk session generator for simulating user behaviors,\nand a gnn prefetch module for training and inference. We provide a detailed\nconceptual analysis showing how GNN-based approaches can outperform\nconventional methods by learning intricate dependencies. This work offers both\ntheoretical foundations and a practical, replicable pipeline for future\nresearch in graph-driven systems optimization."
                },
                "authors": [
                    {
                        "name": "F. I. Qowy"
                    }
                ],
                "author_detail": {
                    "name": "F. I. Qowy"
                },
                "author": "F. I. Qowy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21865v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21865v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.20400v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.20400v1",
                "updated": "2025-10-23T10:06:48Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    10,
                    6,
                    48,
                    3,
                    296,
                    0
                ],
                "published": "2025-10-23T10:06:48Z",
                "published_parsed": [
                    2025,
                    10,
                    23,
                    10,
                    6,
                    48,
                    3,
                    296,
                    0
                ],
                "title": "Squire: A General-Purpose Accelerator to Exploit Fine-Grain Parallelism\n  on Dependency-Bound Kernels",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Squire: A General-Purpose Accelerator to Exploit Fine-Grain Parallelism\n  on Dependency-Bound Kernels"
                },
                "summary": "Multiple HPC applications are often bottlenecked by compute-intensive kernels\nimplementing complex dependency patterns (data-dependency bound). Traditional\ngeneral-purpose accelerators struggle to effectively exploit fine-grain\nparallelism due to limitations in implementing convoluted data-dependency\npatterns (like SIMD) and overheads due to synchronization and data transfers\n(like GPGPUs). In contrast, custom FPGA and ASIC designs offer improved\nperformance and energy efficiency at a high cost in hardware design and\nprogramming complexity and often lack the flexibility to process different\nworkloads. We propose Squire, a general-purpose accelerator designed to exploit\nfine-grain parallelism effectively on dependency-bound kernels. Each Squire\naccelerator has a set of general-purpose low-power in-order cores that can\nrapidly communicate among themselves and directly access data from the L2\ncache. Our proposal integrates one Squire accelerator per core in a typical\nmulticore system, allowing the acceleration of dependency-bound kernels within\nparallel tasks with minimal software changes. As a case study, we evaluate\nSquire's effectiveness by accelerating five kernels that implement complex\ndependency patterns. We use three of these kernels to build an end-to-end\nread-mapping tool that will be used to evaluate Squire. Squire obtains speedups\nup to 7.64$\\times$ in dynamic programming kernels. Overall, Squire provides an\nacceleration for an end-to-end application of 3.66$\\times$. In addition, Squire\nreduces energy consumption by up to 56% with a minimal area overhead of 10.5%\ncompared to a Neoverse-N1 baseline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multiple HPC applications are often bottlenecked by compute-intensive kernels\nimplementing complex dependency patterns (data-dependency bound). Traditional\ngeneral-purpose accelerators struggle to effectively exploit fine-grain\nparallelism due to limitations in implementing convoluted data-dependency\npatterns (like SIMD) and overheads due to synchronization and data transfers\n(like GPGPUs). In contrast, custom FPGA and ASIC designs offer improved\nperformance and energy efficiency at a high cost in hardware design and\nprogramming complexity and often lack the flexibility to process different\nworkloads. We propose Squire, a general-purpose accelerator designed to exploit\nfine-grain parallelism effectively on dependency-bound kernels. Each Squire\naccelerator has a set of general-purpose low-power in-order cores that can\nrapidly communicate among themselves and directly access data from the L2\ncache. Our proposal integrates one Squire accelerator per core in a typical\nmulticore system, allowing the acceleration of dependency-bound kernels within\nparallel tasks with minimal software changes. As a case study, we evaluate\nSquire's effectiveness by accelerating five kernels that implement complex\ndependency patterns. We use three of these kernels to build an end-to-end\nread-mapping tool that will be used to evaluate Squire. Squire obtains speedups\nup to 7.64$\\times$ in dynamic programming kernels. Overall, Squire provides an\nacceleration for an end-to-end application of 3.66$\\times$. In addition, Squire\nreduces energy consumption by up to 56% with a minimal area overhead of 10.5%\ncompared to a Neoverse-N1 baseline."
                },
                "authors": [
                    {
                        "name": "Rubén Langarita"
                    },
                    {
                        "name": "Jesús Alastruey-Benedé"
                    },
                    {
                        "name": "Pablo Ibáñez-Marín"
                    },
                    {
                        "name": "Santiago Marco-Sola"
                    },
                    {
                        "name": "Miquel Moretó"
                    },
                    {
                        "name": "Adrià Armejach"
                    }
                ],
                "author_detail": {
                    "name": "Adrià Armejach"
                },
                "author": "Adrià Armejach",
                "arxiv_comment": "11 pages, 10 figures, 5 tables, 4 algorithms, accepted on PACT25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.20400v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.20400v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11843v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11843v2",
                "updated": "2025-10-23T09:55:50Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    9,
                    55,
                    50,
                    3,
                    296,
                    0
                ],
                "published": "2024-11-18T18:59:15Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    18,
                    59,
                    15,
                    0,
                    323,
                    0
                ],
                "title": "Bi-Mamba: Towards Accurate 1-Bit State Space Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bi-Mamba: Towards Accurate 1-Bit State Space Models"
                },
                "summary": "The typical Selective State-Space Model (SSM) used in Mamba addresses several\nlimitations of Transformers, such as the quadratic computational complexity\nwith respect to sequence length and the significant memory requirements during\ninference due to the key-value (KV) cache. However, the increasing size of\nMamba models continues to pose challenges for training and deployment,\nparticularly due to their substantial computational demands during both\ntraining and inference. In this work, we introduce $\\texttt{Bi-Mamba}$, a\nscalable and powerful 1-bit Mamba architecture designed to enable more\nefficient large language models (LLMs), with model sizes of 780M, 1.3B, and\n2.7B parameters. $\\texttt{Bi-Mamba}$ models are trained from scratch on a\nstandard LLM-scale dataset using an autoregressive distillation loss. Extensive\nexperiments on language modeling benchmarks demonstrate that\n$\\texttt{Bi-Mamba}$ achieves performance comparable to its full-precision (FP16\nor BF16) counterparts, while outperforming post-training binarization (PTB)\nMamba and binarization-aware training (BAT) Transformer baselines. Moreover,\n$\\texttt{Bi-Mamba}$ drastically reduces memory usage and computational cost\ncompared to the original Mamba. Our work pioneers a new line of\nlinear-complexity LLMs under low-bit representation and provides the way for\nthe design of specialized hardware optimized for efficient 1-bit Mamba-based\nmodels. Code and the pre-trained weights are available at\nhttps://github.com/Tangshengku/Bi-Mamba.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The typical Selective State-Space Model (SSM) used in Mamba addresses several\nlimitations of Transformers, such as the quadratic computational complexity\nwith respect to sequence length and the significant memory requirements during\ninference due to the key-value (KV) cache. However, the increasing size of\nMamba models continues to pose challenges for training and deployment,\nparticularly due to their substantial computational demands during both\ntraining and inference. In this work, we introduce $\\texttt{Bi-Mamba}$, a\nscalable and powerful 1-bit Mamba architecture designed to enable more\nefficient large language models (LLMs), with model sizes of 780M, 1.3B, and\n2.7B parameters. $\\texttt{Bi-Mamba}$ models are trained from scratch on a\nstandard LLM-scale dataset using an autoregressive distillation loss. Extensive\nexperiments on language modeling benchmarks demonstrate that\n$\\texttt{Bi-Mamba}$ achieves performance comparable to its full-precision (FP16\nor BF16) counterparts, while outperforming post-training binarization (PTB)\nMamba and binarization-aware training (BAT) Transformer baselines. Moreover,\n$\\texttt{Bi-Mamba}$ drastically reduces memory usage and computational cost\ncompared to the original Mamba. Our work pioneers a new line of\nlinear-complexity LLMs under low-bit representation and provides the way for\nthe design of specialized hardware optimized for efficient 1-bit Mamba-based\nmodels. Code and the pre-trained weights are available at\nhttps://github.com/Tangshengku/Bi-Mamba."
                },
                "authors": [
                    {
                        "name": "Shengkun Tang"
                    },
                    {
                        "name": "Liqun Ma"
                    },
                    {
                        "name": "Haonan Li"
                    },
                    {
                        "name": "Mingjie Sun"
                    },
                    {
                        "name": "Zhiqiang Shen"
                    }
                ],
                "author_detail": {
                    "name": "Zhiqiang Shen"
                },
                "author": "Zhiqiang Shen",
                "arxiv_comment": "Accepted in TMLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11843v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11843v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19755v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19755v2",
                "updated": "2025-10-23T09:09:15Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    9,
                    9,
                    15,
                    3,
                    296,
                    0
                ],
                "published": "2025-10-22T16:46:05Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    16,
                    46,
                    5,
                    2,
                    295,
                    0
                ],
                "title": "A Survey on Cache Methods in Diffusion Models: Toward Efficient\n  Multi-Modal Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Cache Methods in Diffusion Models: Toward Efficient\n  Multi-Modal Generation"
                },
                "summary": "Diffusion Models have become a cornerstone of modern generative AI for their\nexceptional generation quality and controllability. However, their inherent\n\\textit{multi-step iterations} and \\textit{complex backbone networks} lead to\nprohibitive computational overhead and generation latency, forming a major\nbottleneck for real-time applications. Although existing acceleration\ntechniques have made progress, they still face challenges such as limited\napplicability, high training costs, or quality degradation.\n  Against this backdrop, \\textbf{Diffusion Caching} offers a promising\ntraining-free, architecture-agnostic, and efficient inference paradigm. Its\ncore mechanism identifies and reuses intrinsic computational redundancies in\nthe diffusion process. By enabling feature-level cross-step reuse and\ninter-layer scheduling, it reduces computation without modifying model\nparameters. This paper systematically reviews the theoretical foundations and\nevolution of Diffusion Caching and proposes a unified framework for its\nclassification and analysis.\n  Through comparative analysis of representative methods, we show that\nDiffusion Caching evolves from \\textit{static reuse} to \\textit{dynamic\nprediction}. This trend enhances caching flexibility across diverse tasks and\nenables integration with other acceleration techniques such as sampling\noptimization and model distillation, paving the way for a unified, efficient\ninference framework for future multimodal and interactive applications. We\nargue that this paradigm will become a key enabler of real-time and efficient\ngenerative AI, injecting new vitality into both theory and practice of\n\\textit{Efficient Generative Intelligence}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Models have become a cornerstone of modern generative AI for their\nexceptional generation quality and controllability. However, their inherent\n\\textit{multi-step iterations} and \\textit{complex backbone networks} lead to\nprohibitive computational overhead and generation latency, forming a major\nbottleneck for real-time applications. Although existing acceleration\ntechniques have made progress, they still face challenges such as limited\napplicability, high training costs, or quality degradation.\n  Against this backdrop, \\textbf{Diffusion Caching} offers a promising\ntraining-free, architecture-agnostic, and efficient inference paradigm. Its\ncore mechanism identifies and reuses intrinsic computational redundancies in\nthe diffusion process. By enabling feature-level cross-step reuse and\ninter-layer scheduling, it reduces computation without modifying model\nparameters. This paper systematically reviews the theoretical foundations and\nevolution of Diffusion Caching and proposes a unified framework for its\nclassification and analysis.\n  Through comparative analysis of representative methods, we show that\nDiffusion Caching evolves from \\textit{static reuse} to \\textit{dynamic\nprediction}. This trend enhances caching flexibility across diverse tasks and\nenables integration with other acceleration techniques such as sampling\noptimization and model distillation, paving the way for a unified, efficient\ninference framework for future multimodal and interactive applications. We\nargue that this paradigm will become a key enabler of real-time and efficient\ngenerative AI, injecting new vitality into both theory and practice of\n\\textit{Efficient Generative Intelligence}."
                },
                "authors": [
                    {
                        "name": "Jiacheng Liu"
                    },
                    {
                        "name": "Xinyu Wang"
                    },
                    {
                        "name": "Yuqi Lin"
                    },
                    {
                        "name": "Zhikai Wang"
                    },
                    {
                        "name": "Peiru Wang"
                    },
                    {
                        "name": "Peiliang Cai"
                    },
                    {
                        "name": "Qinming Zhou"
                    },
                    {
                        "name": "Zhengan Yan"
                    },
                    {
                        "name": "Zexuan Yan"
                    },
                    {
                        "name": "Zhengyi Shi"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Yue Ma"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "22 pages,2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19755v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19755v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.16807v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.16807v2",
                "updated": "2025-10-23T08:29:11Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    8,
                    29,
                    11,
                    3,
                    296,
                    0
                ],
                "published": "2025-10-19T12:17:42Z",
                "published_parsed": [
                    2025,
                    10,
                    19,
                    12,
                    17,
                    42,
                    6,
                    292,
                    0
                ],
                "title": "Improving Model Representation and Reducing KV Cache via Skip\n  Connections with First Value Heads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Model Representation and Reducing KV Cache via Skip\n  Connections with First Value Heads"
                },
                "summary": "Transformer models have driven breakthroughs across various language tasks by\ntheir strong capability to learn rich contextual representations. Scaling them\nto improve representation, however, often demands substantial memory and\ncompute costs, such as the Key-Value (KV) cache used during auto-regressive\ndecoding. Skip connections offer a promising way to improve representation\nwithout bloating resource usage, yet most prior works either improve\nexpressivity while leaving KV costs unchanged, or reduce memory at the cost of\nweaker representation. In this work, we propose SkipV1Former, a Transformer\nvariant that uses skip connections from the first layer's Value heads to\nstrengthen model representation and reduce KV cache. Specifically, from the\nsecond block onward, each layer reuses half of its Value heads from the very\nfirst layer, while computing the other half as usual-cutting Value projections\nand V cache by nearly 50 \\%. Theoretically, we show that routing uncompressed\nfirst-layer Values into deeper layers restores information lost to compression\nand accelerates the model's implicit mesa-optimization-a key pattern of\nTransformer in auto-regressive tasks. Empirically, across different model\nscales, SkipV1Former delivers consistent reductions of approximately 25 \\% in\nKV cache while improving perplexity relative to standard Multi-Head Attention\n(MHA) Transformers and some advanced variants. Moreover, we propose a recipe\nfor uptraining existing MHA Transformer checkpoints to SkipV1Former with only\n10-15\\% additional compute. Finally, SkipV1Former can seamlessly combine\nadvanced methods like Group-Query Attention and Multi-Latent Attention to\nachieve further KV cache savings and performance improvement. When combined\nwith YOCO, it cuts KV cache size by nearly 50 \\% while still improving\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer models have driven breakthroughs across various language tasks by\ntheir strong capability to learn rich contextual representations. Scaling them\nto improve representation, however, often demands substantial memory and\ncompute costs, such as the Key-Value (KV) cache used during auto-regressive\ndecoding. Skip connections offer a promising way to improve representation\nwithout bloating resource usage, yet most prior works either improve\nexpressivity while leaving KV costs unchanged, or reduce memory at the cost of\nweaker representation. In this work, we propose SkipV1Former, a Transformer\nvariant that uses skip connections from the first layer's Value heads to\nstrengthen model representation and reduce KV cache. Specifically, from the\nsecond block onward, each layer reuses half of its Value heads from the very\nfirst layer, while computing the other half as usual-cutting Value projections\nand V cache by nearly 50 \\%. Theoretically, we show that routing uncompressed\nfirst-layer Values into deeper layers restores information lost to compression\nand accelerates the model's implicit mesa-optimization-a key pattern of\nTransformer in auto-regressive tasks. Empirically, across different model\nscales, SkipV1Former delivers consistent reductions of approximately 25 \\% in\nKV cache while improving perplexity relative to standard Multi-Head Attention\n(MHA) Transformers and some advanced variants. Moreover, we propose a recipe\nfor uptraining existing MHA Transformer checkpoints to SkipV1Former with only\n10-15\\% additional compute. Finally, SkipV1Former can seamlessly combine\nadvanced methods like Group-Query Attention and Multi-Latent Attention to\nachieve further KV cache savings and performance improvement. When combined\nwith YOCO, it cuts KV cache size by nearly 50 \\% while still improving\nperformance."
                },
                "authors": [
                    {
                        "name": "Zhoutong Wu"
                    },
                    {
                        "name": "Yuan Zhang"
                    },
                    {
                        "name": "Yiming Dong"
                    },
                    {
                        "name": "Chenheng Zhang"
                    },
                    {
                        "name": "Cong Fang"
                    },
                    {
                        "name": "Kun Yuan"
                    },
                    {
                        "name": "Zhouchen Lin"
                    }
                ],
                "author_detail": {
                    "name": "Zhouchen Lin"
                },
                "author": "Zhouchen Lin",
                "arxiv_comment": "The code is available at:\n  \\url{https://github.com/Zhoutong-Wu/SkipV1Former}",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.16807v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.16807v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.20230v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.20230v1",
                "updated": "2025-10-23T05:22:09Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    5,
                    22,
                    9,
                    3,
                    296,
                    0
                ],
                "published": "2025-10-23T05:22:09Z",
                "published_parsed": [
                    2025,
                    10,
                    23,
                    5,
                    22,
                    9,
                    3,
                    296,
                    0
                ],
                "title": "Soft Phonon Charge-Density Wave Formation in the Kagome Metal\n  KV$_3$Sb$_5$",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Soft Phonon Charge-Density Wave Formation in the Kagome Metal\n  KV$_3$Sb$_5$"
                },
                "summary": "A range of of unusual emergent behaviors have been reported in the\ncharge-density wave (CDW) state of the $A$V$_3$Sb$_5$ ($A=~$K, Rb, Cs) kagome\nmetals, including a CDW formation process without soft phonons, which points to\nan unconventional CDW mechanism. Here, we use inelastic x-ray scattering to\nshow that the CDW in KV$_3$Sb$_5$ forms via phonons that soften to zero energy\nat the CDW ordering vector ($L$-point) around $T_{\\rm CDW}=78$~K. These soft\nphonons exhibit a remarkable in-plane anisotropy, extending over a much larger\nmomentum range along $L$-$A$ relative to $L$-$H$, which leads to diffuse\nscattering common among $A$V$_3$Sb$_5$. Using first-principles calculations, we\nfind that the momentum-dependent electron-phonon coupling (EPC) is peaked at\n$L$ and exhibits the same in-plane anisotropy as the phonon softening.\nConversely, the electronic susceptibility is not peaked at $L$ and shows the\nopposite in-plane anisotropy. Our findings favor momentum-dependent EPC as the\ndriving mechanism of the CDW in KV$_3$Sb$_5$, with a CDW formation process\nsimilar to that of transition metal dichalcogenides.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A range of of unusual emergent behaviors have been reported in the\ncharge-density wave (CDW) state of the $A$V$_3$Sb$_5$ ($A=~$K, Rb, Cs) kagome\nmetals, including a CDW formation process without soft phonons, which points to\nan unconventional CDW mechanism. Here, we use inelastic x-ray scattering to\nshow that the CDW in KV$_3$Sb$_5$ forms via phonons that soften to zero energy\nat the CDW ordering vector ($L$-point) around $T_{\\rm CDW}=78$~K. These soft\nphonons exhibit a remarkable in-plane anisotropy, extending over a much larger\nmomentum range along $L$-$A$ relative to $L$-$H$, which leads to diffuse\nscattering common among $A$V$_3$Sb$_5$. Using first-principles calculations, we\nfind that the momentum-dependent electron-phonon coupling (EPC) is peaked at\n$L$ and exhibits the same in-plane anisotropy as the phonon softening.\nConversely, the electronic susceptibility is not peaked at $L$ and shows the\nopposite in-plane anisotropy. Our findings favor momentum-dependent EPC as the\ndriving mechanism of the CDW in KV$_3$Sb$_5$, with a CDW formation process\nsimilar to that of transition metal dichalcogenides."
                },
                "authors": [
                    {
                        "name": "Yifan Wang"
                    },
                    {
                        "name": "Chenchao Xu"
                    },
                    {
                        "name": "Zhimian Wu"
                    },
                    {
                        "name": "Huachen Rao"
                    },
                    {
                        "name": "Zhaoyang Shan"
                    },
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Guanghan Cao"
                    },
                    {
                        "name": "Michael Smidman"
                    },
                    {
                        "name": "Ming Shi"
                    },
                    {
                        "name": "Huiqiu Yuan"
                    },
                    {
                        "name": "Tao Wu"
                    },
                    {
                        "name": "Xianhui Chen"
                    },
                    {
                        "name": "Chao Cao"
                    },
                    {
                        "name": "Yu Song"
                    }
                ],
                "author_detail": {
                    "name": "Yu Song"
                },
                "author": "Yu Song",
                "arxiv_comment": "submitted to journal in July 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.20230v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.20230v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.supr-con",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.supr-con",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19258v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19258v4",
                "updated": "2025-10-23T00:47:24Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    0,
                    47,
                    24,
                    3,
                    296,
                    0
                ],
                "published": "2024-10-25T02:22:00Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    2,
                    22,
                    0,
                    4,
                    299,
                    0
                ],
                "title": "Not All Heads Matter: A Head-Level KV Cache Compression Method with\n  Integrated Retrieval and Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not All Heads Matter: A Head-Level KV Cache Compression Method with\n  Integrated Retrieval and Reasoning"
                },
                "summary": "Key-Value (KV) caching is a common technique to enhance the computational\nefficiency of Large Language Models (LLMs), but its memory overhead grows\nrapidly with input length. Prior work has shown that not all tokens are equally\nimportant for text generation, proposing layer-level KV cache compression to\nselectively retain key information. Recognizing the distinct roles of attention\nheads in generation, we propose HeadKV, a head-level KV cache compression\nmethod, and HeadKV-R2, which leverages a novel contextual reasoning ability\nestimation for compression. Our approach operates at the level of individual\nheads, estimating their importance for contextual QA tasks that require both\nretrieval and reasoning capabilities. Extensive experiments across diverse\nbenchmarks (LongBench, LooGLE), model architectures (e.g., Llama-3-8B-Instruct,\nMistral-7B-Instruct), and long-context abilities tests demonstrate that our\nhead-level KV cache compression significantly outperforms strong baselines,\nparticularly in low-resource settings (KV size = 64 & 128). Notably, our method\nretains just 1.5% of the KV cache while achieving 97% of the performance of the\nfull KV cache on the contextual question answering benchmark. Codes are\navailable at https://github.com/FYYFU/HeadKV",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value (KV) caching is a common technique to enhance the computational\nefficiency of Large Language Models (LLMs), but its memory overhead grows\nrapidly with input length. Prior work has shown that not all tokens are equally\nimportant for text generation, proposing layer-level KV cache compression to\nselectively retain key information. Recognizing the distinct roles of attention\nheads in generation, we propose HeadKV, a head-level KV cache compression\nmethod, and HeadKV-R2, which leverages a novel contextual reasoning ability\nestimation for compression. Our approach operates at the level of individual\nheads, estimating their importance for contextual QA tasks that require both\nretrieval and reasoning capabilities. Extensive experiments across diverse\nbenchmarks (LongBench, LooGLE), model architectures (e.g., Llama-3-8B-Instruct,\nMistral-7B-Instruct), and long-context abilities tests demonstrate that our\nhead-level KV cache compression significantly outperforms strong baselines,\nparticularly in low-resource settings (KV size = 64 & 128). Notably, our method\nretains just 1.5% of the KV cache while achieving 97% of the performance of the\nfull KV cache on the contextual question answering benchmark. Codes are\navailable at https://github.com/FYYFU/HeadKV"
                },
                "authors": [
                    {
                        "name": "Yu Fu"
                    },
                    {
                        "name": "Zefan Cai"
                    },
                    {
                        "name": "Abedelkadir Asi"
                    },
                    {
                        "name": "Wayne Xiong"
                    },
                    {
                        "name": "Yue Dong"
                    },
                    {
                        "name": "Wen Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Wen Xiao"
                },
                "author": "Wen Xiao",
                "arxiv_comment": "Accepted to ICLR2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19258v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19258v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.00744v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.00744v2",
                "updated": "2025-10-23T00:40:38Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    0,
                    40,
                    38,
                    3,
                    296,
                    0
                ],
                "published": "2025-05-31T23:16:53Z",
                "published_parsed": [
                    2025,
                    5,
                    31,
                    23,
                    16,
                    53,
                    5,
                    151,
                    0
                ],
                "title": "Blending Complementary Memory Systems in Hybrid Quadratic-Linear\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Blending Complementary Memory Systems in Hybrid Quadratic-Linear\n  Transformers"
                },
                "summary": "We develop hybrid memory architectures for general-purpose sequence\nprocessing neural networks, that combine key-value memory using softmax\nattention (KV-memory) with fast weight memory through dynamic synaptic\nmodulation (FW-memory) -- the core principles of quadratic and linear\ntransformers, respectively. These two memory systems have complementary but\nindividually limited properties: KV-memory offers precise retrieval but is\nconstrained by quadratic complexity in sequence length, while FW-memory\nsupports arbitrarily long sequences and enables more expressive computation but\nsacrifices precise recall. We propose and compare three methods to blend these\ntwo systems into a single memory system, differing in how and when input\ninformation is delivered to each system, to leverage the strengths of both. We\nconduct experiments on general language modeling and retrieval tasks by\ntraining 340M- and 1.3B-parameter models from scratch, as well as on synthetic\nalgorithmic tasks designed to precisely illustrate the benefits of certain\nhybrid methods over others. We also evaluate our hybrid memory systems on\nreinforcement learning in partially observable environments. Overall, we\ndemonstrate how a well-designed hybrid can overcome the limitations of its\nindividual components, offering new insights into the design principle of\nneural memory systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We develop hybrid memory architectures for general-purpose sequence\nprocessing neural networks, that combine key-value memory using softmax\nattention (KV-memory) with fast weight memory through dynamic synaptic\nmodulation (FW-memory) -- the core principles of quadratic and linear\ntransformers, respectively. These two memory systems have complementary but\nindividually limited properties: KV-memory offers precise retrieval but is\nconstrained by quadratic complexity in sequence length, while FW-memory\nsupports arbitrarily long sequences and enables more expressive computation but\nsacrifices precise recall. We propose and compare three methods to blend these\ntwo systems into a single memory system, differing in how and when input\ninformation is delivered to each system, to leverage the strengths of both. We\nconduct experiments on general language modeling and retrieval tasks by\ntraining 340M- and 1.3B-parameter models from scratch, as well as on synthetic\nalgorithmic tasks designed to precisely illustrate the benefits of certain\nhybrid methods over others. We also evaluate our hybrid memory systems on\nreinforcement learning in partially observable environments. Overall, we\ndemonstrate how a well-designed hybrid can overcome the limitations of its\nindividual components, offering new insights into the design principle of\nneural memory systems."
                },
                "authors": [
                    {
                        "name": "Kazuki Irie"
                    },
                    {
                        "name": "Morris Yau"
                    },
                    {
                        "name": "Samuel J. Gershman"
                    }
                ],
                "author_detail": {
                    "name": "Samuel J. Gershman"
                },
                "author": "Samuel J. Gershman",
                "arxiv_comment": "Accepted to NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.00744v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.00744v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.03712v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.03712v2",
                "updated": "2025-10-22T23:56:45Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    23,
                    56,
                    45,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-04T07:22:39Z",
                "published_parsed": [
                    2025,
                    10,
                    4,
                    7,
                    22,
                    39,
                    5,
                    277,
                    0
                ],
                "title": "Detecting and Preventing Latent Risk Accumulation in High-Performance\n  Software Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting and Preventing Latent Risk Accumulation in High-Performance\n  Software Systems"
                },
                "summary": "Modern distributed systems employ aggressive optimization strategies that\ncreate latent risks - hidden vulnerabilities where exceptional performance\nmasks catastrophic fragility when optimizations fail. Cache layers achieving\n99% hit rates can obscure database bottlenecks until cache failures trigger\n100x load amplification and cascading collapse. Current reliability engineering\nfocuses on reactive incident response rather than proactive detection of\noptimization-induced vulnerabilities. This paper presents the first\ncomprehensive framework for systematic latent risk detection, prevention, and\noptimization through integrated mathematical modeling, intelligent perturbation\ntesting, and risk-aware performance optimization. We introduce the Latent Risk\nIndex (LRI) that correlates strongly with incident severity (r=0.863, p<0.001),\nenabling predictive risk assessment. Our framework integrates three systems:\nHYDRA employing six optimization-aware perturbation strategies achieving 89.7%\nrisk discovery rates, RAVEN providing continuous production monitoring with\n92.9% precision and 93.8% recall across 1,748 scenarios, and APEX enabling\nrisk-aware optimization maintaining 96.6% baseline performance while reducing\nlatent risks by 59.2%. Evaluation across three testbed environments\ndemonstrates strong statistical validation with large effect sizes (Cohen\nd>2.0) and exceptional reproducibility (r>0.92). Production deployment over 24\nweeks shows 69.1% mean time to recovery reduction, 78.6% incident severity\nreduction, and 81 prevented incidents generating 1.44M USD average annual\nbenefits with 3.2-month ROI. Our approach transforms reliability engineering\nfrom reactive incident management to proactive risk-aware optimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern distributed systems employ aggressive optimization strategies that\ncreate latent risks - hidden vulnerabilities where exceptional performance\nmasks catastrophic fragility when optimizations fail. Cache layers achieving\n99% hit rates can obscure database bottlenecks until cache failures trigger\n100x load amplification and cascading collapse. Current reliability engineering\nfocuses on reactive incident response rather than proactive detection of\noptimization-induced vulnerabilities. This paper presents the first\ncomprehensive framework for systematic latent risk detection, prevention, and\noptimization through integrated mathematical modeling, intelligent perturbation\ntesting, and risk-aware performance optimization. We introduce the Latent Risk\nIndex (LRI) that correlates strongly with incident severity (r=0.863, p<0.001),\nenabling predictive risk assessment. Our framework integrates three systems:\nHYDRA employing six optimization-aware perturbation strategies achieving 89.7%\nrisk discovery rates, RAVEN providing continuous production monitoring with\n92.9% precision and 93.8% recall across 1,748 scenarios, and APEX enabling\nrisk-aware optimization maintaining 96.6% baseline performance while reducing\nlatent risks by 59.2%. Evaluation across three testbed environments\ndemonstrates strong statistical validation with large effect sizes (Cohen\nd>2.0) and exceptional reproducibility (r>0.92). Production deployment over 24\nweeks shows 69.1% mean time to recovery reduction, 78.6% incident severity\nreduction, and 81 prevented incidents generating 1.44M USD average annual\nbenefits with 3.2-month ROI. Our approach transforms reliability engineering\nfrom reactive incident management to proactive risk-aware optimization."
                },
                "authors": [
                    {
                        "name": "Jahidul Arafat"
                    },
                    {
                        "name": "Kh. M. Moniruzzaman"
                    },
                    {
                        "name": "Shamim Hossain"
                    },
                    {
                        "name": "Fariha Tasmin"
                    }
                ],
                "author_detail": {
                    "name": "Fariha Tasmin"
                },
                "author": "Fariha Tasmin",
                "arxiv_comment": "26 pages, 12 tables, 4 figures. Academic-industry collaboration.\n  Framework (HYDRA, RAVEN, APEX) for optimization-induced vulnerabilities.\n  Evaluated: 2,160 configs, 12.7TB data, 1,748 scenarios",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.03712v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.03712v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68M15, 90B25, 68T05, 90C29",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.4; C.2.4; D.2.5; D.4.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19670v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19670v1",
                "updated": "2025-10-22T15:16:56Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    15,
                    16,
                    56,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T15:16:56Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    15,
                    16,
                    56,
                    2,
                    295,
                    0
                ],
                "title": "CoSense-LLM: Semantics at the Edge with Cost- and Uncertainty-Aware\n  Cloud-Edge Cooperation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoSense-LLM: Semantics at the Edge with Cost- and Uncertainty-Aware\n  Cloud-Edge Cooperation"
                },
                "summary": "We present CoSense-LLM, an edge-first framework that turns continuous\nmultimodal sensor streams (for example Wi-Fi CSI, IMU, audio, RFID, and\nlightweight vision) into compact, verifiable semantic tokens and coordinates\nwith large language models under explicit latency, energy, bandwidth, and\nprivacy constraints. CoSense-LLM has four parts: (i) SenseFusion, a lightweight\nencoder that aligns sensor embeddings with language and compresses them into\nshort discrete code sequences; (ii) Edge-RAG, a local hybrid retrieval layer\nthat grounds generation in site specific policies and notes; (iii)\nPromptRouter, a cost and uncertainty aware policy that selects edge only\ngeneration, edge plus retrieval, or compact cloud escalation; and (iv) Secure\nExecution, an auditable redaction path that enforces data minimization so raw\nwaveforms never leave the device. The system works with modern serving\noptimizations, including paged or streaming KV caches, FlashAttention style\nkernels, speculative decoding, and quantized LoRA adapters, and supports on\ndevice personalization and federated updates under non IID drift. Across home,\noffice, and clinic deployments, CoSense-LLM delivers grounded explanations\nwhile meeting tight service level objectives: it sustains sub second (p95) end\nto end latency on edge dominant paths, reduces inter tier token and bandwidth\ncosts by preferring local retrieval grounded responses, and preserves privacy\nby transmitting only discrete codes and redacted metadata. Ablations show that\nEdge-RAG improves factual consistency and reduces contradictions, calibrated\nuncertainty enables selective abstention and controlled escalations, and KV\nplus decoding accelerators lower energy per decision. The results support an\nedge first design that treats semantics, privacy, and predictable latency as co\nequal goals for large model deployments in interference prone environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present CoSense-LLM, an edge-first framework that turns continuous\nmultimodal sensor streams (for example Wi-Fi CSI, IMU, audio, RFID, and\nlightweight vision) into compact, verifiable semantic tokens and coordinates\nwith large language models under explicit latency, energy, bandwidth, and\nprivacy constraints. CoSense-LLM has four parts: (i) SenseFusion, a lightweight\nencoder that aligns sensor embeddings with language and compresses them into\nshort discrete code sequences; (ii) Edge-RAG, a local hybrid retrieval layer\nthat grounds generation in site specific policies and notes; (iii)\nPromptRouter, a cost and uncertainty aware policy that selects edge only\ngeneration, edge plus retrieval, or compact cloud escalation; and (iv) Secure\nExecution, an auditable redaction path that enforces data minimization so raw\nwaveforms never leave the device. The system works with modern serving\noptimizations, including paged or streaming KV caches, FlashAttention style\nkernels, speculative decoding, and quantized LoRA adapters, and supports on\ndevice personalization and federated updates under non IID drift. Across home,\noffice, and clinic deployments, CoSense-LLM delivers grounded explanations\nwhile meeting tight service level objectives: it sustains sub second (p95) end\nto end latency on edge dominant paths, reduces inter tier token and bandwidth\ncosts by preferring local retrieval grounded responses, and preserves privacy\nby transmitting only discrete codes and redacted metadata. Ablations show that\nEdge-RAG improves factual consistency and reduces contradictions, calibrated\nuncertainty enables selective abstention and controlled escalations, and KV\nplus decoding accelerators lower energy per decision. The results support an\nedge first design that treats semantics, privacy, and predictable latency as co\nequal goals for large model deployments in interference prone environments."
                },
                "authors": [
                    {
                        "name": "Hasan Akgul"
                    },
                    {
                        "name": "Mari Eplik"
                    },
                    {
                        "name": "Javier Rojas"
                    },
                    {
                        "name": "Aina Binti Abdullah"
                    },
                    {
                        "name": "Pieter van der Merwe"
                    }
                ],
                "author_detail": {
                    "name": "Pieter van der Merwe"
                },
                "author": "Pieter van der Merwe",
                "arxiv_comment": "19 pages,8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19670v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19670v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6; C.2.4; C.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08666v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08666v3",
                "updated": "2025-10-22T14:33:49Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    14,
                    33,
                    49,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-09T16:19:42Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    16,
                    19,
                    42,
                    3,
                    282,
                    0
                ],
                "title": "dInfer: An Efficient Inference Framework for Diffusion Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "dInfer: An Efficient Inference Framework for Diffusion Language Models"
                },
                "summary": "Diffusion-based large language models (dLLMs) have emerged as a promising\nalternative to autoregressive (AR) LLMs, leveraging denoising-based generation\nto enable inherent parallelism. Even more and more open-sourced dLLM models\nemerge, yet their widespread adoption remains constrained by the lack of a\nstandardized and efficient inference framework. We present dInfer, an efficient\nand extensible framework for dLLM inference. dInfer decomposes the inference\npipeline into four modular components--model, diffusion iteration manager,\ndecoding strategy, and KV-cache manager--and integrates novel algorithms for\neach component alongside system-level optimizations. Through this combination\nof algorithmic innovations and system enhancements, dInfer achieves substantial\nefficiency gains without compromising output quality on LLaDA-MoE. At batch\nsize 1, it surpasses 1,100 tokens per second on HumanEval and averages over 800\ntokens per second across six benchmarks on $8\\times$ H800 GPUs. Compared to\nprior systems, dInfer delivers a $10\\times$ speedup over Fast-dLLM while\nmaintaining similar model performance. Even compared to the AR model (with a\ncomparable number of activation parameters and performance) QWen2.5-3B, which\nis highly optimized with the latest vLLM inference engine, dInfer still\ndelivers a $2$-$3\\times$ speedup. The implementation of dInfer is open-sourced\nat https://github.com/inclusionAI/dInfer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based large language models (dLLMs) have emerged as a promising\nalternative to autoregressive (AR) LLMs, leveraging denoising-based generation\nto enable inherent parallelism. Even more and more open-sourced dLLM models\nemerge, yet their widespread adoption remains constrained by the lack of a\nstandardized and efficient inference framework. We present dInfer, an efficient\nand extensible framework for dLLM inference. dInfer decomposes the inference\npipeline into four modular components--model, diffusion iteration manager,\ndecoding strategy, and KV-cache manager--and integrates novel algorithms for\neach component alongside system-level optimizations. Through this combination\nof algorithmic innovations and system enhancements, dInfer achieves substantial\nefficiency gains without compromising output quality on LLaDA-MoE. At batch\nsize 1, it surpasses 1,100 tokens per second on HumanEval and averages over 800\ntokens per second across six benchmarks on $8\\times$ H800 GPUs. Compared to\nprior systems, dInfer delivers a $10\\times$ speedup over Fast-dLLM while\nmaintaining similar model performance. Even compared to the AR model (with a\ncomparable number of activation parameters and performance) QWen2.5-3B, which\nis highly optimized with the latest vLLM inference engine, dInfer still\ndelivers a $2$-$3\\times$ speedup. The implementation of dInfer is open-sourced\nat https://github.com/inclusionAI/dInfer."
                },
                "authors": [
                    {
                        "name": "Yuxin Ma"
                    },
                    {
                        "name": "Lun Du"
                    },
                    {
                        "name": "Lanning Wei"
                    },
                    {
                        "name": "Kun Chen"
                    },
                    {
                        "name": "Qian Xu"
                    },
                    {
                        "name": "Kangyu Wang"
                    },
                    {
                        "name": "Guofeng Feng"
                    },
                    {
                        "name": "Guoshan Lu"
                    },
                    {
                        "name": "Lin Liu"
                    },
                    {
                        "name": "Xiaojing Qi"
                    },
                    {
                        "name": "Xinyuan Zhang"
                    },
                    {
                        "name": "Zhen Tao"
                    },
                    {
                        "name": "Haibo Feng"
                    },
                    {
                        "name": "Ziyun Jiang"
                    },
                    {
                        "name": "Ying Xu"
                    },
                    {
                        "name": "Zenan Huang"
                    },
                    {
                        "name": "Yihong Zhuang"
                    },
                    {
                        "name": "Haokai Xu"
                    },
                    {
                        "name": "Jiaqi Hu"
                    },
                    {
                        "name": "Zhenzhong Lan"
                    },
                    {
                        "name": "Junbo Zhao"
                    },
                    {
                        "name": "Jianguo Li"
                    },
                    {
                        "name": "Da Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Da Zheng"
                },
                "author": "Da Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08666v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08666v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19875v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19875v1",
                "updated": "2025-10-22T09:42:29Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    9,
                    42,
                    29,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T09:42:29Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    9,
                    42,
                    29,
                    2,
                    295,
                    0
                ],
                "title": "Stream: Scaling up Mechanistic Interpretability to Long Context in LLMs\n  via Sparse Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stream: Scaling up Mechanistic Interpretability to Long Context in LLMs\n  via Sparse Attention"
                },
                "summary": "As Large Language Models (LLMs) scale to million-token contexts, traditional\nMechanistic Interpretability techniques for analyzing attention scale\nquadratically with context length, demanding terabytes of memory beyond 100,000\ntokens. We introduce Sparse Tracing, a novel technique that leverages dynamic\nsparse attention to efficiently analyze long context attention patterns. We\npresent Stream, a compilable hierarchical pruning algorithm that estimates\nper-head sparse attention masks in near-linear time $O(T \\log T)$ and linear\nspace $O(T)$, enabling one-pass interpretability at scale. Stream performs a\nbinary-search-style refinement to retain only the top-$k$ key blocks per query\nwhile preserving the model's next-token behavior. We apply Stream to long\nchain-of-thought reasoning traces and identify thought anchors while pruning\n97-99\\% of token interactions. On the RULER benchmark, Stream preserves\ncritical retrieval paths while discarding 90-96\\% of interactions and exposes\nlayer-wise routes from the needle to output. Our method offers a practical\ndrop-in tool for analyzing attention patterns and tracing information flow\nwithout terabytes of caches. By making long context interpretability feasible\non consumer GPUs, Sparse Tracing helps democratize chain-of-thought monitoring.\nCode is available at https://anonymous.4open.science/r/stream-03B8/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) scale to million-token contexts, traditional\nMechanistic Interpretability techniques for analyzing attention scale\nquadratically with context length, demanding terabytes of memory beyond 100,000\ntokens. We introduce Sparse Tracing, a novel technique that leverages dynamic\nsparse attention to efficiently analyze long context attention patterns. We\npresent Stream, a compilable hierarchical pruning algorithm that estimates\nper-head sparse attention masks in near-linear time $O(T \\log T)$ and linear\nspace $O(T)$, enabling one-pass interpretability at scale. Stream performs a\nbinary-search-style refinement to retain only the top-$k$ key blocks per query\nwhile preserving the model's next-token behavior. We apply Stream to long\nchain-of-thought reasoning traces and identify thought anchors while pruning\n97-99\\% of token interactions. On the RULER benchmark, Stream preserves\ncritical retrieval paths while discarding 90-96\\% of interactions and exposes\nlayer-wise routes from the needle to output. Our method offers a practical\ndrop-in tool for analyzing attention patterns and tracing information flow\nwithout terabytes of caches. By making long context interpretability feasible\non consumer GPUs, Sparse Tracing helps democratize chain-of-thought monitoring.\nCode is available at https://anonymous.4open.science/r/stream-03B8/."
                },
                "authors": [
                    {
                        "name": "J Rosser"
                    },
                    {
                        "name": "José Luis Redondo García"
                    },
                    {
                        "name": "Gustavo Penha"
                    },
                    {
                        "name": "Konstantina Palla"
                    },
                    {
                        "name": "Hugues Bouchard"
                    }
                ],
                "author_detail": {
                    "name": "Hugues Bouchard"
                },
                "author": "Hugues Bouchard",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19875v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19875v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T40",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24761v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24761v1",
                "updated": "2025-10-22T07:50:06Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    7,
                    50,
                    6,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T07:50:06Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    7,
                    50,
                    6,
                    2,
                    295,
                    0
                ],
                "title": "ODataX: A Progressive Evolution of the Open Data Protocol",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ODataX: A Progressive Evolution of the Open Data Protocol"
                },
                "summary": "The Open Data Protocol (OData) provides a standardized approach for building\nand consuming RESTful APIs with rich query capabilities. Despite its power and\nmaturity, OData adoption remains confined primarily to enterprise environments,\nparticularly within Microsoft and SAP ecosystems. This paper analyzes the key\nbarriers preventing wider OData adoption and introduces ODataX, an evolved\nversion of the protocol designed to address these limitations. ODataX maintains\nbackward compatibility with OData v4 while introducing progressive complexity\ndisclosure through simplified query syntax, built-in performance guardrails via\nquery cost estimation, and enhanced caching mechanisms. This work aims to\nbridge the gap between enterprise-grade query standardization and the\nsimplicity demanded by modern web development practices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Open Data Protocol (OData) provides a standardized approach for building\nand consuming RESTful APIs with rich query capabilities. Despite its power and\nmaturity, OData adoption remains confined primarily to enterprise environments,\nparticularly within Microsoft and SAP ecosystems. This paper analyzes the key\nbarriers preventing wider OData adoption and introduces ODataX, an evolved\nversion of the protocol designed to address these limitations. ODataX maintains\nbackward compatibility with OData v4 while introducing progressive complexity\ndisclosure through simplified query syntax, built-in performance guardrails via\nquery cost estimation, and enhanced caching mechanisms. This work aims to\nbridge the gap between enterprise-grade query standardization and the\nsimplicity demanded by modern web development practices."
                },
                "authors": [
                    {
                        "name": "Anirudh Ganesh"
                    },
                    {
                        "name": "Nitin Sood"
                    }
                ],
                "author_detail": {
                    "name": "Nitin Sood"
                },
                "author": "Nitin Sood",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24761v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24761v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19264v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19264v1",
                "updated": "2025-10-22T05:47:41Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    5,
                    47,
                    41,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T05:47:41Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    5,
                    47,
                    41,
                    2,
                    295,
                    0
                ],
                "title": "LAPRAD: LLM-Assisted PRotocol Attack Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LAPRAD: LLM-Assisted PRotocol Attack Discovery"
                },
                "summary": "With the goal of improving the security of Internet protocols, we seek\nfaster, semi-automatic methods to discover new vulnerabilities in protocols\nsuch as DNS, BGP, and others. To this end, we introduce the LLM-Assisted\nProtocol Attack Discovery (LAPRAD) methodology, enabling security researchers\nwith some DNS knowledge to efficiently uncover vulnerabilities that would\notherwise be hard to detect.\n  LAPRAD follows a three-stage process. In the first, we consult an LLM\n(GPT-o1) that has been trained on a broad corpus of DNS-related sources and\nprevious DDoS attacks to identify potential exploits. In the second stage, a\ndifferent LLM automatically constructs the corresponding attack configurations\nusing the ReACT approach implemented via LangChain (DNS zone file generation).\nFinally, in the third stage, we validate the attack's functionality and\neffectiveness.\n  Using LAPRAD, we uncovered three new DDoS attacks on the DNS protocol and\nrediscovered two recently reported ones that were not included in the LLM's\ntraining data. The first new attack employs a bait-and-switch technique to\ntrick resolvers into caching large, bogus DNSSEC RRSIGs, reducing their serving\ncapacity to as little as 6%. The second exploits large DNSSEC encryption\nalgorithms (RSA-4096) with multiple keys, thereby bypassing a recently\nimplemented default RRSet limit. The third leverages ANY-type responses to\nproduce a similar effect.\n  These variations of a cache-flushing DDoS attack, called SigCacheFlush,\ncircumvent existing patches, severely degrade resolver query capacity, and\nimpact the latest versions of major DNS resolver implementations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the goal of improving the security of Internet protocols, we seek\nfaster, semi-automatic methods to discover new vulnerabilities in protocols\nsuch as DNS, BGP, and others. To this end, we introduce the LLM-Assisted\nProtocol Attack Discovery (LAPRAD) methodology, enabling security researchers\nwith some DNS knowledge to efficiently uncover vulnerabilities that would\notherwise be hard to detect.\n  LAPRAD follows a three-stage process. In the first, we consult an LLM\n(GPT-o1) that has been trained on a broad corpus of DNS-related sources and\nprevious DDoS attacks to identify potential exploits. In the second stage, a\ndifferent LLM automatically constructs the corresponding attack configurations\nusing the ReACT approach implemented via LangChain (DNS zone file generation).\nFinally, in the third stage, we validate the attack's functionality and\neffectiveness.\n  Using LAPRAD, we uncovered three new DDoS attacks on the DNS protocol and\nrediscovered two recently reported ones that were not included in the LLM's\ntraining data. The first new attack employs a bait-and-switch technique to\ntrick resolvers into caching large, bogus DNSSEC RRSIGs, reducing their serving\ncapacity to as little as 6%. The second exploits large DNSSEC encryption\nalgorithms (RSA-4096) with multiple keys, thereby bypassing a recently\nimplemented default RRSet limit. The third leverages ANY-type responses to\nproduce a similar effect.\n  These variations of a cache-flushing DDoS attack, called SigCacheFlush,\ncircumvent existing patches, severely degrade resolver query capacity, and\nimpact the latest versions of major DNS resolver implementations."
                },
                "authors": [
                    {
                        "name": "R. Can Aygun"
                    },
                    {
                        "name": "Yehuda Afek"
                    },
                    {
                        "name": "Anat Bremler-Barr"
                    },
                    {
                        "name": "Leonard Kleinrock"
                    }
                ],
                "author_detail": {
                    "name": "Leonard Kleinrock"
                },
                "arxiv_affiliation": "UCLA",
                "author": "Leonard Kleinrock",
                "arxiv_comment": "IFIP Networking 2025 Proceedings (Accepted on 05.05.2025)",
                "arxiv_journal_ref": "Published in IFIP Networking 2025 Proceedings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19264v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19264v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19183v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19183v1",
                "updated": "2025-10-22T02:41:07Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    2,
                    41,
                    7,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T02:41:07Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    2,
                    41,
                    7,
                    2,
                    295,
                    0
                ],
                "title": "PruneHal: Reducing Hallucinations in Multi-modal Large Language Models\n  through Adaptive KV Cache Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PruneHal: Reducing Hallucinations in Multi-modal Large Language Models\n  through Adaptive KV Cache Pruning"
                },
                "summary": "While multi-modal large language models (MLLMs) have made significant\nprogress in recent years, the issue of hallucinations remains a major\nchallenge. To mitigate this phenomenon, existing solutions either introduce\nadditional data for further training or incorporate external or internal\ninformation during inference. However, these approaches inevitably introduce\nextra computational costs. In this paper, we observe that hallucinations in\nMLLMs are strongly associated with insufficient attention allocated to visual\ntokens. In particular, the presence of redundant visual tokens disperses the\nmodel's attention, preventing it from focusing on the most informative ones. As\na result, critical visual cues are often under-attended, which in turn\nexacerbates the occurrence of hallucinations. Building on this observation, we\npropose \\textbf{PruneHal}, a training-free, simple yet effective method that\nleverages adaptive KV cache pruning to enhance the model's focus on critical\nvisual information, thereby mitigating hallucinations. To the best of our\nknowledge, we are the first to apply token pruning for hallucination mitigation\nin MLLMs. Notably, our method don't require additional training and incurs\nnearly no extra inference cost. Moreover, PruneHal is model-agnostic and can be\nseamlessly integrated with different decoding strategies, including those\nspecifically designed for hallucination mitigation. We evaluate PruneHal on\nseveral widely used hallucination evaluation benchmarks using four mainstream\nMLLMs, achieving robust and outstanding results that highlight the\neffectiveness and superiority of our method. Our code will be publicly\navailable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While multi-modal large language models (MLLMs) have made significant\nprogress in recent years, the issue of hallucinations remains a major\nchallenge. To mitigate this phenomenon, existing solutions either introduce\nadditional data for further training or incorporate external or internal\ninformation during inference. However, these approaches inevitably introduce\nextra computational costs. In this paper, we observe that hallucinations in\nMLLMs are strongly associated with insufficient attention allocated to visual\ntokens. In particular, the presence of redundant visual tokens disperses the\nmodel's attention, preventing it from focusing on the most informative ones. As\na result, critical visual cues are often under-attended, which in turn\nexacerbates the occurrence of hallucinations. Building on this observation, we\npropose \\textbf{PruneHal}, a training-free, simple yet effective method that\nleverages adaptive KV cache pruning to enhance the model's focus on critical\nvisual information, thereby mitigating hallucinations. To the best of our\nknowledge, we are the first to apply token pruning for hallucination mitigation\nin MLLMs. Notably, our method don't require additional training and incurs\nnearly no extra inference cost. Moreover, PruneHal is model-agnostic and can be\nseamlessly integrated with different decoding strategies, including those\nspecifically designed for hallucination mitigation. We evaluate PruneHal on\nseveral widely used hallucination evaluation benchmarks using four mainstream\nMLLMs, achieving robust and outstanding results that highlight the\neffectiveness and superiority of our method. Our code will be publicly\navailable."
                },
                "authors": [
                    {
                        "name": "Fengyuan Sun"
                    },
                    {
                        "name": "Hui Chen"
                    },
                    {
                        "name": "Xinhao Xu"
                    },
                    {
                        "name": "Dandan Zheng"
                    },
                    {
                        "name": "Jingdong Chen"
                    },
                    {
                        "name": "Jun Zhou"
                    },
                    {
                        "name": "Jungong Han"
                    },
                    {
                        "name": "Guiguang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Guiguang Ding"
                },
                "author": "Guiguang Ding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19183v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19183v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19171v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19171v1",
                "updated": "2025-10-22T02:09:23Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    2,
                    9,
                    23,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T02:09:23Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    2,
                    9,
                    23,
                    2,
                    295,
                    0
                ],
                "title": "Think Straight, Stop Smart: Structured Reasoning for Efficient Multi-Hop\n  RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Think Straight, Stop Smart: Structured Reasoning for Efficient Multi-Hop\n  RAG"
                },
                "summary": "Multi-hop retrieval-augmented generation (RAG) is a promising strategy for\ncomplex reasoning, yet existing iterative prompting approaches remain\ninefficient. They often regenerate predictable token sequences at every step\nand rely on stochastic stopping, leading to excessive token usage and unstable\ntermination. We propose TSSS (Think Straight, Stop Smart), a structured\nmulti-hop RAG framework designed for efficiency. TSSS introduces (i) a\ntemplate-based reasoning that caches recurring prefixes and anchors sub-queries\nto the main question, reducing token generation cost while promoting stable\nreasoning, and (ii) a retriever-based terminator, which deterministically halts\nreasoning once additional sub-queries collapse into repetition. This separation\nof structured reasoning and termination control enables both faster inference\nand more reliable answers. On HotpotQA, 2WikiMultiHop, and MuSiQue, TSSS\nachieves state-of-the-art accuracy and competitive efficiency among RAG-CoT\napproaches, highlighting its effectiveness in efficiency-constrained scenarios\nsuch as on-device inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-hop retrieval-augmented generation (RAG) is a promising strategy for\ncomplex reasoning, yet existing iterative prompting approaches remain\ninefficient. They often regenerate predictable token sequences at every step\nand rely on stochastic stopping, leading to excessive token usage and unstable\ntermination. We propose TSSS (Think Straight, Stop Smart), a structured\nmulti-hop RAG framework designed for efficiency. TSSS introduces (i) a\ntemplate-based reasoning that caches recurring prefixes and anchors sub-queries\nto the main question, reducing token generation cost while promoting stable\nreasoning, and (ii) a retriever-based terminator, which deterministically halts\nreasoning once additional sub-queries collapse into repetition. This separation\nof structured reasoning and termination control enables both faster inference\nand more reliable answers. On HotpotQA, 2WikiMultiHop, and MuSiQue, TSSS\nachieves state-of-the-art accuracy and competitive efficiency among RAG-CoT\napproaches, highlighting its effectiveness in efficiency-constrained scenarios\nsuch as on-device inference."
                },
                "authors": [
                    {
                        "name": "Jihwan Bang"
                    },
                    {
                        "name": "Juntae Lee"
                    },
                    {
                        "name": "Seunghan Yang"
                    },
                    {
                        "name": "Sungha Choi"
                    }
                ],
                "author_detail": {
                    "name": "Sungha Choi"
                },
                "author": "Sungha Choi",
                "arxiv_comment": "Accepted at NeurIPS 2025 Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19171v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19171v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11857v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11857v3",
                "updated": "2025-10-21T22:37:11Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    22,
                    37,
                    11,
                    1,
                    294,
                    0
                ],
                "published": "2024-10-04T15:23:28Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    15,
                    23,
                    28,
                    4,
                    278,
                    0
                ],
                "title": "LLMBridge: Reducing Costs to Access LLMs in a Prompt-Centric Internet",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMBridge: Reducing Costs to Access LLMs in a Prompt-Centric Internet"
                },
                "summary": "Today's Internet infrastructure is centered around content retrieval over\nHTTP, with middleboxes (e.g., HTTP proxies) playing a crucial role in\nperformance, security, and cost-effectiveness. We envision a future where\nInternet communication will be dominated by \"prompts\" sent to generative AI\nmodels. For this, we will need proxies that provide similar functions to HTTP\nproxies (e.g., caching, routing, compression) while dealing with unique\nchallenges and opportunities of prompt-based communication. As a first step\ntoward supporting prompt-based communication, we present LLMBridge, an LLM\nproxy designed for cost-conscious users, such as those in developing regions\nand education (e.g., students, instructors). LLMBridge supports three key\noptimizations: model selection (routing prompts to the most suitable model),\ncontext management (intelligently reducing the amount of context), and semantic\ncaching (serving prompts using local models and vector databases). These\noptimizations introduce trade-offs between cost and quality, which applications\nnavigate through a high-level, bidirectional interface. As case studies, we\ndeploy LLMBridge in two cost-sensitive settings: a WhatsApp-based Q&A service\nand a university classroom environment. The WhatsApp service has been live for\nover twelve months, serving 100+ users and handling more than 14.7K requests.\nIn parallel, we exposed LLMBridge to students across three computer science\ncourses over a semester, where it supported diverse LLM-powered applications -\nsuch as reasoning agents and chatbots - and handled an average of 500 requests\nper day. We report on deployment experiences across both settings and use the\ncollected workloads to benchmark the effectiveness of various cost-optimization\nstrategies, analyzing their trade-offs in cost, latency, and response quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Today's Internet infrastructure is centered around content retrieval over\nHTTP, with middleboxes (e.g., HTTP proxies) playing a crucial role in\nperformance, security, and cost-effectiveness. We envision a future where\nInternet communication will be dominated by \"prompts\" sent to generative AI\nmodels. For this, we will need proxies that provide similar functions to HTTP\nproxies (e.g., caching, routing, compression) while dealing with unique\nchallenges and opportunities of prompt-based communication. As a first step\ntoward supporting prompt-based communication, we present LLMBridge, an LLM\nproxy designed for cost-conscious users, such as those in developing regions\nand education (e.g., students, instructors). LLMBridge supports three key\noptimizations: model selection (routing prompts to the most suitable model),\ncontext management (intelligently reducing the amount of context), and semantic\ncaching (serving prompts using local models and vector databases). These\noptimizations introduce trade-offs between cost and quality, which applications\nnavigate through a high-level, bidirectional interface. As case studies, we\ndeploy LLMBridge in two cost-sensitive settings: a WhatsApp-based Q&A service\nand a university classroom environment. The WhatsApp service has been live for\nover twelve months, serving 100+ users and handling more than 14.7K requests.\nIn parallel, we exposed LLMBridge to students across three computer science\ncourses over a semester, where it supported diverse LLM-powered applications -\nsuch as reasoning agents and chatbots - and handled an average of 500 requests\nper day. We report on deployment experiences across both settings and use the\ncollected workloads to benchmark the effectiveness of various cost-optimization\nstrategies, analyzing their trade-offs in cost, latency, and response quality."
                },
                "authors": [
                    {
                        "name": "Noah Martin"
                    },
                    {
                        "name": "Abdullah Bin Faisal"
                    },
                    {
                        "name": "Hiba Eltigani"
                    },
                    {
                        "name": "Rukhshan Haroon"
                    },
                    {
                        "name": "Swaminathan Lamelas"
                    },
                    {
                        "name": "Fahad Dogar"
                    }
                ],
                "author_detail": {
                    "name": "Fahad Dogar"
                },
                "author": "Fahad Dogar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11857v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11857v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18191v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18191v3",
                "updated": "2025-10-21T21:07:17Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    21,
                    7,
                    17,
                    1,
                    294,
                    0
                ],
                "published": "2025-03-23T20:18:16Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    20,
                    18,
                    16,
                    6,
                    82,
                    0
                ],
                "title": "DFUSE: Strongly Consistent Write-Back Kernel Caching for Distributed\n  Userspace File Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DFUSE: Strongly Consistent Write-Back Kernel Caching for Distributed\n  Userspace File Systems"
                },
                "summary": "Cloud platforms host thousands of tenants that demand POSIX semantics, high\nthroughput, and rapid evolution from their storage layer. Kernel-native\ndistributed file systems supply raw speed, but their privileged code base\ncouples every release to the kernel, widens the blast radius of crashes, and\nslows innovation. FUSE-based distributed file systems flip those trade-offs:\nthey run in user space for fast deployment and strong fault isolation, yet the\nFUSE interface disables the kernel's write-back page cache whenever strong\nconsistency is required. Practitioners must therefore choose between (i) weak\nconsistency with fast write-back caching or (ii) strong consistency with slow\nwrite-through I/O, a limitation that has kept FUSE distributed file systems out\nof write-intensive cloud workloads.\n  To this end, we present DFUSE, the first distributed FUSE file system that\ndelivers write-back kernel caching and strong consistency. DFUSE achieves this\nby offloading userspace consistency control to the kernel driver, allowing\ncoordinated access to the kernel's page cache across nodes. This design\neliminates blind local cache updates and ensures cluster-wide strong\nconsistency without compromising performance. In our evaluation, DFUSE achieves\nup to 68.0% higher throughput and 40.4% lower latency than the existing\nwrite-through design of FUSE-based distributed file systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cloud platforms host thousands of tenants that demand POSIX semantics, high\nthroughput, and rapid evolution from their storage layer. Kernel-native\ndistributed file systems supply raw speed, but their privileged code base\ncouples every release to the kernel, widens the blast radius of crashes, and\nslows innovation. FUSE-based distributed file systems flip those trade-offs:\nthey run in user space for fast deployment and strong fault isolation, yet the\nFUSE interface disables the kernel's write-back page cache whenever strong\nconsistency is required. Practitioners must therefore choose between (i) weak\nconsistency with fast write-back caching or (ii) strong consistency with slow\nwrite-through I/O, a limitation that has kept FUSE distributed file systems out\nof write-intensive cloud workloads.\n  To this end, we present DFUSE, the first distributed FUSE file system that\ndelivers write-back kernel caching and strong consistency. DFUSE achieves this\nby offloading userspace consistency control to the kernel driver, allowing\ncoordinated access to the kernel's page cache across nodes. This design\neliminates blind local cache updates and ensures cluster-wide strong\nconsistency without compromising performance. In our evaluation, DFUSE achieves\nup to 68.0% higher throughput and 40.4% lower latency than the existing\nwrite-through design of FUSE-based distributed file systems."
                },
                "authors": [
                    {
                        "name": "Haoyu Li"
                    },
                    {
                        "name": "Jingkai Fu"
                    },
                    {
                        "name": "Qing Li"
                    },
                    {
                        "name": "Windsor Hsu"
                    },
                    {
                        "name": "Asaf Cidon"
                    }
                ],
                "author_detail": {
                    "name": "Asaf Cidon"
                },
                "author": "Asaf Cidon",
                "arxiv_doi": "10.1145/3772052.3772208",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3772052.3772208",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.18191v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18191v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "This is the version accepted to ACM SoCC 2025. The title has been\n  updated to match the published version",
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.15878v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.15878v2",
                "updated": "2025-10-21T16:32:50Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    16,
                    32,
                    50,
                    1,
                    294,
                    0
                ],
                "published": "2025-08-21T16:10:26Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    16,
                    10,
                    26,
                    3,
                    233,
                    0
                ],
                "title": "Putting the Context back into Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Putting the Context back into Memory"
                },
                "summary": "Requests arriving at main memory are often different from what programmers\ncan observe or estimate by using CPU-based monitoring. Hardware cache\nprefetching, memory request scheduling and interleaving cause a loss of\nobservability that limits potential data movement and tiering optimizations. In\nresponse, memory-side telemetry hardware like page access heat map units (HMU)\nand page prefetchers were proposed to inform Operating Systems with accurate\nusage data. However, it is still hard to map memory activity to software\nprogram functions and objects because of the decoupled nature of host\nprocessors and memory devices. Valuable program context is stripped out from\nthe memory bus, leaving only commands, addresses and data. Programmers have\nexpert knowledge of future data accesses, priorities, and access to processor\nstate, which could be useful hints for runtime memory device optimization. This\npaper makes context visible at memory devices by encoding any user-visible\nstate as detectable packets in the memory read address stream, in a\nnondestructive manner without significant capacity overhead, drivers or special\naccess privileges. We prototyped an end-to-end system with metadata injection\nthat can be reliably detected and decoded from a memory address trace, either\nby a host processor, or a memory module. We illustrate a use case with precise\ncode execution markers and object address range tracking. In the future, real\ntime metadata decoding with near-memory computing (NMC) could provide\ncustomized telemetry and statistics to users, or act on application hints to\nperform functions like prioritizing requests, remapping data and reconfiguring\ndevices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Requests arriving at main memory are often different from what programmers\ncan observe or estimate by using CPU-based monitoring. Hardware cache\nprefetching, memory request scheduling and interleaving cause a loss of\nobservability that limits potential data movement and tiering optimizations. In\nresponse, memory-side telemetry hardware like page access heat map units (HMU)\nand page prefetchers were proposed to inform Operating Systems with accurate\nusage data. However, it is still hard to map memory activity to software\nprogram functions and objects because of the decoupled nature of host\nprocessors and memory devices. Valuable program context is stripped out from\nthe memory bus, leaving only commands, addresses and data. Programmers have\nexpert knowledge of future data accesses, priorities, and access to processor\nstate, which could be useful hints for runtime memory device optimization. This\npaper makes context visible at memory devices by encoding any user-visible\nstate as detectable packets in the memory read address stream, in a\nnondestructive manner without significant capacity overhead, drivers or special\naccess privileges. We prototyped an end-to-end system with metadata injection\nthat can be reliably detected and decoded from a memory address trace, either\nby a host processor, or a memory module. We illustrate a use case with precise\ncode execution markers and object address range tracking. In the future, real\ntime metadata decoding with near-memory computing (NMC) could provide\ncustomized telemetry and statistics to users, or act on application hints to\nperform functions like prioritizing requests, remapping data and reconfiguring\ndevices."
                },
                "authors": [
                    {
                        "name": "David A. Roberts"
                    }
                ],
                "author_detail": {
                    "name": "David A. Roberts"
                },
                "author": "David A. Roberts",
                "arxiv_comment": "Fixed errors in paragraph numbering",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.15878v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.15878v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18716v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18716v1",
                "updated": "2025-10-21T15:17:37Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    15,
                    17,
                    37,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T15:17:37Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    15,
                    17,
                    37,
                    1,
                    294,
                    0
                ],
                "title": "SSD: Spatial-Semantic Head Decoupling for Efficient Autoregressive Image\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SSD: Spatial-Semantic Head Decoupling for Efficient Autoregressive Image\n  Generation"
                },
                "summary": "Autoregressive image generation models like Janus-Pro produce high-quality\nimages, but at the significant cost of high memory and ever-growing\ncomputational demands due to the large number of visual tokens. While KV cache\ncompression has been extensively studied in language modeling, it still remains\nlargely unexplored for the image generation domain. In this work, we begin by\nidentifying a distinct and prominent attention phenomenon, which we term\nspatial locality and emergent semantic sink. To leverage this key insight, we\nintroduce a novel KV cache compression framework. Specifically, we compress the\nKV cache for all visual tokens by adaptively decoupling attention heads into\ntwo separate types: for spatial-locality heads, our method maintains a short\nrecent token window; for semantic-sink heads, it strategically preserves a\ncompact set of highly-attended tokens. Our extensive experiments demonstrate\nthat the proposed method achieves a 5$\\times$ reduction in memory usage and a\nnotable 6.6$\\times$ speedup in overall throughput with only minimal visual\nquality loss, thereby enabling highly efficient native autoregressive image\ngeneration on resource-constrained hardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive image generation models like Janus-Pro produce high-quality\nimages, but at the significant cost of high memory and ever-growing\ncomputational demands due to the large number of visual tokens. While KV cache\ncompression has been extensively studied in language modeling, it still remains\nlargely unexplored for the image generation domain. In this work, we begin by\nidentifying a distinct and prominent attention phenomenon, which we term\nspatial locality and emergent semantic sink. To leverage this key insight, we\nintroduce a novel KV cache compression framework. Specifically, we compress the\nKV cache for all visual tokens by adaptively decoupling attention heads into\ntwo separate types: for spatial-locality heads, our method maintains a short\nrecent token window; for semantic-sink heads, it strategically preserves a\ncompact set of highly-attended tokens. Our extensive experiments demonstrate\nthat the proposed method achieves a 5$\\times$ reduction in memory usage and a\nnotable 6.6$\\times$ speedup in overall throughput with only minimal visual\nquality loss, thereby enabling highly efficient native autoregressive image\ngeneration on resource-constrained hardware."
                },
                "authors": [
                    {
                        "name": "Siyong Jian"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18716v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18716v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.14576v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.14576v3",
                "updated": "2025-10-21T15:13:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    15,
                    13,
                    47,
                    1,
                    294,
                    0
                ],
                "published": "2024-01-26T00:27:00Z",
                "published_parsed": [
                    2024,
                    1,
                    26,
                    0,
                    27,
                    0,
                    4,
                    26,
                    0
                ],
                "title": "ParaLog: Consistent Host-side Logging for Parallel Checkpoints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ParaLog: Consistent Host-side Logging for Parallel Checkpoints"
                },
                "summary": "Output-intensive scientific applications are highly sensitive to low storage\nthroughput. While existing scientific application stacks are optimized for\ntraditional High-Performance Computing (HPC) environments with high remote\nstorage and network bandwidth, these assumptions often fail in modern settings\nlike cloud deployment. This is because the existing scientific application I/O\nstack fails to leverage the available resources. At the same time, scientific\napplications exhibit special synchronization and data output requirements that\nare difficult to satisfy using traditional approaches such as block-level or\nfilesystem-level caching. We introduce ParaLog, a distributed host-side logging\napproach designed to accelerate scientific applications transparently. ParaLog\nemphasizes deployability, enabling support for unmodified message passing\ninterface (MPI) applications and implementations while preserving crash\nconsistency semantics. We evaluate ParaLog across traditional HPC, cloud HPC,\nlocal clusters, and hybrid environments, demonstrating its capability to reduce\nend-to-end execution time by 13-26% for popular scientific applications in\ncloud settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Output-intensive scientific applications are highly sensitive to low storage\nthroughput. While existing scientific application stacks are optimized for\ntraditional High-Performance Computing (HPC) environments with high remote\nstorage and network bandwidth, these assumptions often fail in modern settings\nlike cloud deployment. This is because the existing scientific application I/O\nstack fails to leverage the available resources. At the same time, scientific\napplications exhibit special synchronization and data output requirements that\nare difficult to satisfy using traditional approaches such as block-level or\nfilesystem-level caching. We introduce ParaLog, a distributed host-side logging\napproach designed to accelerate scientific applications transparently. ParaLog\nemphasizes deployability, enabling support for unmodified message passing\ninterface (MPI) applications and implementations while preserving crash\nconsistency semantics. We evaluate ParaLog across traditional HPC, cloud HPC,\nlocal clusters, and hybrid environments, demonstrating its capability to reduce\nend-to-end execution time by 13-26% for popular scientific applications in\ncloud settings."
                },
                "authors": [
                    {
                        "name": "Steven W. D. Chien"
                    },
                    {
                        "name": "Kento Sato"
                    },
                    {
                        "name": "Artur Podobas"
                    },
                    {
                        "name": "Niclas Jansson"
                    },
                    {
                        "name": "Stefano Markidis"
                    },
                    {
                        "name": "Michio Honda"
                    }
                ],
                "author_detail": {
                    "name": "Michio Honda"
                },
                "author": "Michio Honda",
                "arxiv_doi": "10.1145/3772052.3772212",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3772052.3772212",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2401.14576v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.14576v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted to SoCC 2025",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18672v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18672v1",
                "updated": "2025-10-21T14:25:51Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    14,
                    25,
                    51,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T14:25:51Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    14,
                    25,
                    51,
                    1,
                    294,
                    0
                ],
                "title": "Reasoning Language Model Inference Serving Unveiled: An Empirical Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning Language Model Inference Serving Unveiled: An Empirical Study"
                },
                "summary": "The reasoning large language model (RLLM) has been proven competitive in\nsolving complex reasoning tasks such as mathematics, coding, compared to\ngeneral LLM. However, the serving performance and behavior of RLLM remains\nunexplored, which may undermine the deployment and utilization of RLLM in\nreal-world scenario. To close this gap, in this paper, we conduct a\ncomprehensive study of RLLM service. We first perform a pilot study on\ncomparing the serving performance between RLLM and traditional LLM and reveal\nthat there are several distinct differences regarding serving behavior: (1)\nsignificant memory usage and fluctuations; (2) straggler requests; (3) adaptive\nrunning time; (4) domain preference. Then we further investigate whether\nexisting inference optimization techniques are valid for RLLM. Our main\ntakeaways are that model quantization methods and speculative decoding can\nimprove service system efficiency with small compromise to RLLM accuracy, while\nprefix caching, KV cache quantization may even degrade accuracy or serving\nperformance for small RLLM. Lastly, we conduct evaluation under real world\nworkload modeled by Gamma distribution to verify our findings. Empirical\nresults of real world workload evaluation across different dataset are aligned\nwith our main findings regarding RLLM serving. We hope our work can provide the\nresearch community and industry with insights to advance RLLM inference\nserving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The reasoning large language model (RLLM) has been proven competitive in\nsolving complex reasoning tasks such as mathematics, coding, compared to\ngeneral LLM. However, the serving performance and behavior of RLLM remains\nunexplored, which may undermine the deployment and utilization of RLLM in\nreal-world scenario. To close this gap, in this paper, we conduct a\ncomprehensive study of RLLM service. We first perform a pilot study on\ncomparing the serving performance between RLLM and traditional LLM and reveal\nthat there are several distinct differences regarding serving behavior: (1)\nsignificant memory usage and fluctuations; (2) straggler requests; (3) adaptive\nrunning time; (4) domain preference. Then we further investigate whether\nexisting inference optimization techniques are valid for RLLM. Our main\ntakeaways are that model quantization methods and speculative decoding can\nimprove service system efficiency with small compromise to RLLM accuracy, while\nprefix caching, KV cache quantization may even degrade accuracy or serving\nperformance for small RLLM. Lastly, we conduct evaluation under real world\nworkload modeled by Gamma distribution to verify our findings. Empirical\nresults of real world workload evaluation across different dataset are aligned\nwith our main findings regarding RLLM serving. We hope our work can provide the\nresearch community and industry with insights to advance RLLM inference\nserving."
                },
                "authors": [
                    {
                        "name": "Qi Li"
                    },
                    {
                        "name": "Junpan Wu"
                    },
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Yuxin Wang"
                    },
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Yuhan Chen"
                    },
                    {
                        "name": "Shaohuai Shi"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18672v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18672v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18586v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18586v1",
                "updated": "2025-10-21T12:39:32Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    12,
                    39,
                    32,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T12:39:32Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    12,
                    39,
                    32,
                    1,
                    294,
                    0
                ],
                "title": "Tokencake: A KV-Cache-centric Serving Framework for LLM-based\n  Multi-Agent Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tokencake: A KV-Cache-centric Serving Framework for LLM-based\n  Multi-Agent Applications"
                },
                "summary": "Large Language Models (LLMs) are increasingly deployed in complex multi-agent\napplications that use external function calls. This workload creates severe\nperformance challenges for the KV Cache: space contention leads to the eviction\nof critical agents' caches and time underutilization leaves the cache of agents\nstalled on long-running tool calls idling in GPU memory. We present Tokencake,\na KV-Cache-centric serving framework that co-optimizes scheduling and memory\nmanagement with an agent-aware design. Tokencake's Space Scheduler uses dynamic\nmemory partitioning to shield critical agents from contention, while its Time\nScheduler employs a proactive offload and predictive upload mechanism to\nrepurpose GPU memory during function call stalls. Our evaluation on\nrepresentative multi-agent benchmarks shows that Tokencake can reduce\nend-to-end latency by over 47.06%, improve effective GPU memory utilization by\nup to 16.9% compared to vLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly deployed in complex multi-agent\napplications that use external function calls. This workload creates severe\nperformance challenges for the KV Cache: space contention leads to the eviction\nof critical agents' caches and time underutilization leaves the cache of agents\nstalled on long-running tool calls idling in GPU memory. We present Tokencake,\na KV-Cache-centric serving framework that co-optimizes scheduling and memory\nmanagement with an agent-aware design. Tokencake's Space Scheduler uses dynamic\nmemory partitioning to shield critical agents from contention, while its Time\nScheduler employs a proactive offload and predictive upload mechanism to\nrepurpose GPU memory during function call stalls. Our evaluation on\nrepresentative multi-agent benchmarks shows that Tokencake can reduce\nend-to-end latency by over 47.06%, improve effective GPU memory utilization by\nup to 16.9% compared to vLLM."
                },
                "authors": [
                    {
                        "name": "Zhuohang Bian"
                    },
                    {
                        "name": "Feiyang Wu"
                    },
                    {
                        "name": "Teng Ma"
                    },
                    {
                        "name": "Youwei Zhuo"
                    }
                ],
                "author_detail": {
                    "name": "Youwei Zhuo"
                },
                "author": "Youwei Zhuo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18586v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18586v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18546v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18546v1",
                "updated": "2025-10-21T11:52:44Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    11,
                    52,
                    44,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T11:52:44Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    11,
                    52,
                    44,
                    1,
                    294,
                    0
                ],
                "title": "EfficientNav: Towards On-Device Object-Goal Navigation with Navigation\n  Map Caching and Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EfficientNav: Towards On-Device Object-Goal Navigation with Navigation\n  Map Caching and Retrieval"
                },
                "summary": "Object-goal navigation (ObjNav) tasks an agent with navigating to the\nlocation of a specific object in an unseen environment. Embodied agents\nequipped with large language models (LLMs) and online constructed navigation\nmaps can perform ObjNav in a zero-shot manner. However, existing agents heavily\nrely on giant LLMs on the cloud, e.g., GPT-4, while directly switching to small\nLLMs, e.g., LLaMA3.2-11b, suffer from significant success rate drops due to\nlimited model capacity for understanding complex navigation maps, which\nprevents deploying ObjNav on local devices. At the same time, the long prompt\nintroduced by the navigation map description will cause high planning latency\non local devices. In this paper, we propose EfficientNav to enable on-device\nefficient LLM-based zero-shot ObjNav. To help the smaller LLMs better\nunderstand the environment, we propose semantics-aware memory retrieval to\nprune redundant information in navigation maps. To reduce planning latency, we\npropose discrete memory caching and attention-based memory clustering to\nefficiently save and re-use the KV cache. Extensive experimental results\ndemonstrate that EfficientNav achieves 11.1% improvement in success rate on\nHM3D benchmark over GPT-4-based baselines, and demonstrates 6.7x real-time\nlatency reduction and 4.7x end-to-end latency reduction over GPT-4 planner. Our\ncode will be released soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Object-goal navigation (ObjNav) tasks an agent with navigating to the\nlocation of a specific object in an unseen environment. Embodied agents\nequipped with large language models (LLMs) and online constructed navigation\nmaps can perform ObjNav in a zero-shot manner. However, existing agents heavily\nrely on giant LLMs on the cloud, e.g., GPT-4, while directly switching to small\nLLMs, e.g., LLaMA3.2-11b, suffer from significant success rate drops due to\nlimited model capacity for understanding complex navigation maps, which\nprevents deploying ObjNav on local devices. At the same time, the long prompt\nintroduced by the navigation map description will cause high planning latency\non local devices. In this paper, we propose EfficientNav to enable on-device\nefficient LLM-based zero-shot ObjNav. To help the smaller LLMs better\nunderstand the environment, we propose semantics-aware memory retrieval to\nprune redundant information in navigation maps. To reduce planning latency, we\npropose discrete memory caching and attention-based memory clustering to\nefficiently save and re-use the KV cache. Extensive experimental results\ndemonstrate that EfficientNav achieves 11.1% improvement in success rate on\nHM3D benchmark over GPT-4-based baselines, and demonstrates 6.7x real-time\nlatency reduction and 4.7x end-to-end latency reduction over GPT-4 planner. Our\ncode will be released soon."
                },
                "authors": [
                    {
                        "name": "Zebin Yang"
                    },
                    {
                        "name": "Sunjian Zheng"
                    },
                    {
                        "name": "Tong Xie"
                    },
                    {
                        "name": "Tianshi Xu"
                    },
                    {
                        "name": "Bo Yu"
                    },
                    {
                        "name": "Fan Wang"
                    },
                    {
                        "name": "Jie Tang"
                    },
                    {
                        "name": "Shaoshan Liu"
                    },
                    {
                        "name": "Meng Li"
                    }
                ],
                "author_detail": {
                    "name": "Meng Li"
                },
                "author": "Meng Li",
                "arxiv_comment": "NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18546v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18546v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02175v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02175v2",
                "updated": "2025-10-21T10:33:29Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    10,
                    33,
                    29,
                    1,
                    294,
                    0
                ],
                "published": "2025-02-04T09:48:14Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    9,
                    48,
                    14,
                    1,
                    35,
                    0
                ],
                "title": "VLA-Cache: Efficient Vision-Language-Action Manipulation via Adaptive\n  Token Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VLA-Cache: Efficient Vision-Language-Action Manipulation via Adaptive\n  Token Caching"
                },
                "summary": "Vision-Language-Action (VLA) models have demonstrated strong multi-modal\nreasoning capabilities, enabling direct action generation from visual\nperception and language instructions in an end-to-end manner. However, their\nsubstantial computational cost poses a challenge for real-time robotic control,\nwhere rapid decision-making is essential. This paper introduces VLA-Cache, a\ntraining-free inference acceleration method that reduces computational overhead\nby adaptively caching and reusing static visual tokens across frames.\nExploiting the temporal continuity in robotic manipulation, VLA-Cache\nidentifies minimally changed tokens between adjacent frames and reuses their\ncached key-value representations, thereby circumventing redundant computations.\nAdditionally, to maintain action precision, VLA-Cache selectively re-computes\ntask-relevant tokens that are environmentally sensitive, ensuring the fidelity\nof critical visual information. To further optimize efficiency, we introduce a\nlayer adaptive token reusing strategy that dynamically adjusts the reuse ratio\nbased on attention concentration across decoder layers, prioritizing critical\ntokens for recomputation. Extensive experiments on two simulation platforms\n(LIBERO and SIMPLER) and a real-world robotic system demonstrate that VLA-Cache\nachieves up to 1.7x speedup in CUDA latency and a 15% increase in control\nfrequency, with negligible loss on task success rate. The code and videos can\nbe found at our project page: https://vla-cache.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language-Action (VLA) models have demonstrated strong multi-modal\nreasoning capabilities, enabling direct action generation from visual\nperception and language instructions in an end-to-end manner. However, their\nsubstantial computational cost poses a challenge for real-time robotic control,\nwhere rapid decision-making is essential. This paper introduces VLA-Cache, a\ntraining-free inference acceleration method that reduces computational overhead\nby adaptively caching and reusing static visual tokens across frames.\nExploiting the temporal continuity in robotic manipulation, VLA-Cache\nidentifies minimally changed tokens between adjacent frames and reuses their\ncached key-value representations, thereby circumventing redundant computations.\nAdditionally, to maintain action precision, VLA-Cache selectively re-computes\ntask-relevant tokens that are environmentally sensitive, ensuring the fidelity\nof critical visual information. To further optimize efficiency, we introduce a\nlayer adaptive token reusing strategy that dynamically adjusts the reuse ratio\nbased on attention concentration across decoder layers, prioritizing critical\ntokens for recomputation. Extensive experiments on two simulation platforms\n(LIBERO and SIMPLER) and a real-world robotic system demonstrate that VLA-Cache\nachieves up to 1.7x speedup in CUDA latency and a 15% increase in control\nfrequency, with negligible loss on task success rate. The code and videos can\nbe found at our project page: https://vla-cache.github.io."
                },
                "authors": [
                    {
                        "name": "Siyu Xu"
                    },
                    {
                        "name": "Yunke Wang"
                    },
                    {
                        "name": "Chenghao Xia"
                    },
                    {
                        "name": "Dihao Zhu"
                    },
                    {
                        "name": "Tao Huang"
                    },
                    {
                        "name": "Chang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Chang Xu"
                },
                "author": "Chang Xu",
                "arxiv_comment": "Accepted to NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02175v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02175v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06436v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06436v2",
                "updated": "2025-10-21T10:08:33Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    10,
                    8,
                    33,
                    1,
                    294,
                    0
                ],
                "published": "2025-09-08T08:34:02Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    8,
                    34,
                    2,
                    0,
                    251,
                    0
                ],
                "title": "Tree of Agents: Improving Long-Context Capabilities of Large Language\n  Models through Multi-Perspective Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tree of Agents: Improving Long-Context Capabilities of Large Language\n  Models through Multi-Perspective Reasoning"
                },
                "summary": "Large language models (LLMs) face persistent challenges when handling\nlong-context tasks, most notably the lost in the middle issue, where\ninformation located in the middle of a long input tends to be underutilized.\nSome existing methods that reduce input have the risk of discarding key\ninformation, while others that extend context windows often lead to attention\ndispersion. To address these limitations, we propose Tree of Agents (TOA), a\nmulti-agent reasoning framework that segments the input into chunks processed\nby independent agents. Each agent generates its local cognition, then agents\ndynamically exchange information for collaborative reasoning along\ntree-structured paths. TOA enables agents to probe different reasoning orders\nfor multi-perspective understanding, effectively mitigating position bias and\nreducing hallucinations. To improve processing efficiency, we incorporate\nprefix-hash caching and adaptive pruning strategies, achieving significant\nperformance improvements with comparable API overhead. Experiments show that\nTOA, powered by compact LLaMA3.1-8B, significantly outperforms multiple\nbaselines and demonstrates comparable performance to the latest and much larger\ncommercial models, such as Gemini1.5-pro, on various long-context tasks. Code\nis available at https://github.com/Aireduce952/Tree-of-Agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) face persistent challenges when handling\nlong-context tasks, most notably the lost in the middle issue, where\ninformation located in the middle of a long input tends to be underutilized.\nSome existing methods that reduce input have the risk of discarding key\ninformation, while others that extend context windows often lead to attention\ndispersion. To address these limitations, we propose Tree of Agents (TOA), a\nmulti-agent reasoning framework that segments the input into chunks processed\nby independent agents. Each agent generates its local cognition, then agents\ndynamically exchange information for collaborative reasoning along\ntree-structured paths. TOA enables agents to probe different reasoning orders\nfor multi-perspective understanding, effectively mitigating position bias and\nreducing hallucinations. To improve processing efficiency, we incorporate\nprefix-hash caching and adaptive pruning strategies, achieving significant\nperformance improvements with comparable API overhead. Experiments show that\nTOA, powered by compact LLaMA3.1-8B, significantly outperforms multiple\nbaselines and demonstrates comparable performance to the latest and much larger\ncommercial models, such as Gemini1.5-pro, on various long-context tasks. Code\nis available at https://github.com/Aireduce952/Tree-of-Agents."
                },
                "authors": [
                    {
                        "name": "Song Yu"
                    },
                    {
                        "name": "Xiaofei Xu"
                    },
                    {
                        "name": "Ke Deng"
                    },
                    {
                        "name": "Li Li"
                    },
                    {
                        "name": "Lin Tian"
                    }
                ],
                "author_detail": {
                    "name": "Lin Tian"
                },
                "author": "Lin Tian",
                "arxiv_comment": "19 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06436v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06436v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18480v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18480v2",
                "updated": "2025-10-30T08:46:37Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    8,
                    46,
                    37,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-21T10:00:32Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    10,
                    0,
                    32,
                    1,
                    294,
                    0
                ],
                "title": "How Efficient Are Diffusion Language Models? A Critical Examination of\n  Efficiency Evaluation Practices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Efficient Are Diffusion Language Models? A Critical Examination of\n  Efficiency Evaluation Practices"
                },
                "summary": "Diffusion language models (DLMs) have emerged as a promising alternative to\nthe long-dominant autoregressive (AR) paradigm, offering a parallelable\ndecoding process that could yield greater efficiency. Yet, in practice, current\nopen-source DLMs often underperform their AR counterparts in speed, limiting\ntheir real-world utility. This work presents a systematic study of DLM\nefficiency, identifying key issues in prior evaluation methods. Through\nempirical benchmarking and a roofline-based theoretical analysis, we\ndemonstrate that AR models generally achieve higher throughput, while DLMs\nconsistently lag. We also investigate acceleration strategies, finding that\ntechniques like dual cache and parallel decoding mainly offer gains at small\nbatch sizes, with their benefits diminishing upon scaling. Our findings\nunderscore the necessity of robust evaluation methods and improved acceleration\nstrategies to advance research on DLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion language models (DLMs) have emerged as a promising alternative to\nthe long-dominant autoregressive (AR) paradigm, offering a parallelable\ndecoding process that could yield greater efficiency. Yet, in practice, current\nopen-source DLMs often underperform their AR counterparts in speed, limiting\ntheir real-world utility. This work presents a systematic study of DLM\nefficiency, identifying key issues in prior evaluation methods. Through\nempirical benchmarking and a roofline-based theoretical analysis, we\ndemonstrate that AR models generally achieve higher throughput, while DLMs\nconsistently lag. We also investigate acceleration strategies, finding that\ntechniques like dual cache and parallel decoding mainly offer gains at small\nbatch sizes, with their benefits diminishing upon scaling. Our findings\nunderscore the necessity of robust evaluation methods and improved acceleration\nstrategies to advance research on DLMs."
                },
                "authors": [
                    {
                        "name": "Han Peng"
                    },
                    {
                        "name": "Peiyu Liu"
                    },
                    {
                        "name": "Zican Dong"
                    },
                    {
                        "name": "Daixuan Cheng"
                    },
                    {
                        "name": "Junyi Li"
                    },
                    {
                        "name": "Yiru Tang"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Wayne Xin Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Wayne Xin Zhao"
                },
                "author": "Wayne Xin Zhao",
                "arxiv_comment": "Withdrawn by the authors to better delineate the related work from\n  the paper's original contributions",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18480v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18480v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18413v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18413v1",
                "updated": "2025-10-21T08:44:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    8,
                    44,
                    47,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T08:44:47Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    8,
                    44,
                    47,
                    1,
                    294,
                    0
                ],
                "title": "Adamas: Hadamard Sparse Attention for Efficient Long-Context Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adamas: Hadamard Sparse Attention for Efficient Long-Context Inference"
                },
                "summary": "Large language models (LLMs) now support context windows of hundreds of\nthousands to millions of tokens, enabling applications such as long-document\nsummarization, large-scale code synthesis, multi-document question answering\nand persistent multi-turn dialogue. However, such extended contexts exacerbate\nthe quadratic cost of self-attention, leading to severe latency in\nautoregressive decoding. Existing sparse attention methods alleviate these\ncosts but rely on heuristic patterns that struggle to recall critical key-value\n(KV) pairs for each query, resulting in accuracy degradation. We introduce\nAdamas, a lightweight yet highly accurate sparse attention mechanism designed\nfor long-context inference. Adamas applies the Hadamard transform,\nbucketization and 2-bit compression to produce compact representations, and\nleverages Manhattan-distance estimation for efficient top-k selections.\nExperiments show that Adamas matches the accuracy of full attention with only a\n64-token budget, achieves near-lossless performance at 128, and supports up to\n8x higher sparsity than prior state-of-the-art (SOTA) methods while delivering\nup to 4.4x self-attention and 1.5x end-to-end speedups on 32K-length sequences.\nRemarkably, Adamas attains comparable or even lower perplexity than full\nattention, underscoring its effectiveness in maintaining accuracy under\naggressive sparsity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) now support context windows of hundreds of\nthousands to millions of tokens, enabling applications such as long-document\nsummarization, large-scale code synthesis, multi-document question answering\nand persistent multi-turn dialogue. However, such extended contexts exacerbate\nthe quadratic cost of self-attention, leading to severe latency in\nautoregressive decoding. Existing sparse attention methods alleviate these\ncosts but rely on heuristic patterns that struggle to recall critical key-value\n(KV) pairs for each query, resulting in accuracy degradation. We introduce\nAdamas, a lightweight yet highly accurate sparse attention mechanism designed\nfor long-context inference. Adamas applies the Hadamard transform,\nbucketization and 2-bit compression to produce compact representations, and\nleverages Manhattan-distance estimation for efficient top-k selections.\nExperiments show that Adamas matches the accuracy of full attention with only a\n64-token budget, achieves near-lossless performance at 128, and supports up to\n8x higher sparsity than prior state-of-the-art (SOTA) methods while delivering\nup to 4.4x self-attention and 1.5x end-to-end speedups on 32K-length sequences.\nRemarkably, Adamas attains comparable or even lower perplexity than full\nattention, underscoring its effectiveness in maintaining accuracy under\naggressive sparsity."
                },
                "authors": [
                    {
                        "name": "Siyuan Yan"
                    },
                    {
                        "name": "Guo-Qing Jiang"
                    },
                    {
                        "name": "Yuchen Zhang"
                    },
                    {
                        "name": "Xiaoxing Ma"
                    },
                    {
                        "name": "Ran Zhu"
                    },
                    {
                        "name": "Chun Cao"
                    },
                    {
                        "name": "Jingwei Xu"
                    }
                ],
                "author_detail": {
                    "name": "Jingwei Xu"
                },
                "author": "Jingwei Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18413v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18413v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20002v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20002v5",
                "updated": "2025-10-21T06:47:29Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    6,
                    47,
                    29,
                    1,
                    294,
                    0
                ],
                "published": "2024-09-30T06:55:00Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    6,
                    55,
                    0,
                    0,
                    274,
                    0
                ],
                "title": "The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM\n  Serving Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM\n  Serving Systems"
                },
                "summary": "The wide deployment of Large Language Models (LLMs) has given rise to strong\ndemands for optimizing their inference performance. Today's techniques serving\nthis purpose primarily focus on reducing latency and improving throughput\nthrough algorithmic and hardware enhancements, while largely overlooking their\nprivacy side effects, particularly in a multi-user environment. In our\nresearch, for the first time, we discovered a set of new timing side channels\nin LLM systems, arising from shared caches and GPU memory allocations, which\ncan be exploited to infer both confidential system prompts and those issued by\nother users. These vulnerabilities echo security challenges observed in\ntraditional computing systems, highlighting an urgent need to address potential\ninformation leakage in LLM serving infrastructures. In this paper, we report\nnovel attack strategies designed to exploit such timing side channels inherent\nin LLM deployments, specifically targeting the Key-Value (KV) cache and\nsemantic cache widely used to enhance LLM inference performance. Our approach\nleverages timing measurements and classification models to detect cache hits,\nallowing an adversary to infer private prompts with high accuracy. We also\npropose a token-by-token search algorithm to efficiently recover shared prompt\nprefixes in the caches, showing the feasibility of stealing system prompts and\nthose produced by peer users. Our experimental studies on black-box testing of\npopular online LLM services demonstrate that such privacy risks are completely\nrealistic, with significant consequences. Our findings underscore the need for\nrobust mitigation to protect LLM systems against such emerging threats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The wide deployment of Large Language Models (LLMs) has given rise to strong\ndemands for optimizing their inference performance. Today's techniques serving\nthis purpose primarily focus on reducing latency and improving throughput\nthrough algorithmic and hardware enhancements, while largely overlooking their\nprivacy side effects, particularly in a multi-user environment. In our\nresearch, for the first time, we discovered a set of new timing side channels\nin LLM systems, arising from shared caches and GPU memory allocations, which\ncan be exploited to infer both confidential system prompts and those issued by\nother users. These vulnerabilities echo security challenges observed in\ntraditional computing systems, highlighting an urgent need to address potential\ninformation leakage in LLM serving infrastructures. In this paper, we report\nnovel attack strategies designed to exploit such timing side channels inherent\nin LLM deployments, specifically targeting the Key-Value (KV) cache and\nsemantic cache widely used to enhance LLM inference performance. Our approach\nleverages timing measurements and classification models to detect cache hits,\nallowing an adversary to infer private prompts with high accuracy. We also\npropose a token-by-token search algorithm to efficiently recover shared prompt\nprefixes in the caches, showing the feasibility of stealing system prompts and\nthose produced by peer users. Our experimental studies on black-box testing of\npopular online LLM services demonstrate that such privacy risks are completely\nrealistic, with significant consequences. Our findings underscore the need for\nrobust mitigation to protect LLM systems against such emerging threats."
                },
                "authors": [
                    {
                        "name": "Linke Song"
                    },
                    {
                        "name": "Zixuan Pang"
                    },
                    {
                        "name": "Wenhao Wang"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "XiaoFeng Wang"
                    },
                    {
                        "name": "Hongbo Chen"
                    },
                    {
                        "name": "Wei Song"
                    },
                    {
                        "name": "Yier Jin"
                    },
                    {
                        "name": "Dan Meng"
                    },
                    {
                        "name": "Rui Hou"
                    }
                ],
                "author_detail": {
                    "name": "Rui Hou"
                },
                "author": "Rui Hou",
                "arxiv_comment": "This work was first submitted for review on Sept. 5, 2024, and the\n  initial version was uploaded to Arxiv on Sept. 30, 2024. The latest version\n  has accepted for publication by IEEE Transactions on Information Forensics\n  and Security (TIFS)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20002v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20002v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14374v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14374v3",
                "updated": "2025-10-21T06:30:21Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    6,
                    30,
                    21,
                    1,
                    294,
                    0
                ],
                "published": "2025-04-19T18:25:20Z",
                "published_parsed": [
                    2025,
                    4,
                    19,
                    18,
                    25,
                    20,
                    5,
                    109,
                    0
                ],
                "title": "A fast MPI-based Distributed Hash-Table as Surrogate Model demonstrated\n  in a coupled reactive transport HPC simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A fast MPI-based Distributed Hash-Table as Surrogate Model demonstrated\n  in a coupled reactive transport HPC simulation"
                },
                "summary": "Surrogate models can play a pivotal role in enhancing performance in\ncontemporary High-Performance Computing applications. Cache-based surrogates\nuse already calculated simulation results to interpolate or extrapolate further\nsimulation output values. But this approach only pays off if the access time to\nretrieve the needed values is much faster than the actual simulation. While the\nmost existing key-value stores use a Client-Server architecture with dedicated\nstorage nodes, this is not the most suitable architecture for HPC applications.\nInstead, we propose a distributed architecture where the parallel processes\noffer a part of their available memory to build a shared distributed hash table\nbased on MPI. This paper presents three DHT approaches with the special\nrequirements of HPC applications in mind. The presented lock-free design\noutperforms both DHT versions which use explicit synchronization by\ncoarse-grained resp. fine-grained locking. The lock-free DHT shows very good\nscaling regarding read and write performance. The runtime of a coupled reactive\ntransport simulation was improved between 14% and 42% using the lock-free DHT\nas a surrogate model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surrogate models can play a pivotal role in enhancing performance in\ncontemporary High-Performance Computing applications. Cache-based surrogates\nuse already calculated simulation results to interpolate or extrapolate further\nsimulation output values. But this approach only pays off if the access time to\nretrieve the needed values is much faster than the actual simulation. While the\nmost existing key-value stores use a Client-Server architecture with dedicated\nstorage nodes, this is not the most suitable architecture for HPC applications.\nInstead, we propose a distributed architecture where the parallel processes\noffer a part of their available memory to build a shared distributed hash table\nbased on MPI. This paper presents three DHT approaches with the special\nrequirements of HPC applications in mind. The presented lock-free design\noutperforms both DHT versions which use explicit synchronization by\ncoarse-grained resp. fine-grained locking. The lock-free DHT shows very good\nscaling regarding read and write performance. The runtime of a coupled reactive\ntransport simulation was improved between 14% and 42% using the lock-free DHT\nas a surrogate model."
                },
                "authors": [
                    {
                        "name": "Max Lübke"
                    },
                    {
                        "name": "Marco De Lucia"
                    },
                    {
                        "name": "Steffen Christgau"
                    },
                    {
                        "name": "Stefan Petri"
                    },
                    {
                        "name": "Bettina Schnor"
                    }
                ],
                "author_detail": {
                    "name": "Bettina Schnor"
                },
                "author": "Bettina Schnor",
                "arxiv_doi": "10.1007/978-3-031-97635-3_28",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-031-97635-3_28",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.14374v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14374v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Long version, 15 pages, 6 figures; Short version (8 pages) included\n  in the proceedings of \"25th International Conference on Computational\n  Science\" (ICCS25)",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18269v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18269v1",
                "updated": "2025-10-21T03:39:41Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    3,
                    39,
                    41,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T03:39:41Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    3,
                    39,
                    41,
                    1,
                    294,
                    0
                ],
                "title": "StreamingTOM: Streaming Token Compression for Efficient Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StreamingTOM: Streaming Token Compression for Efficient Video\n  Understanding"
                },
                "summary": "Unlike offline processing, streaming video vision-language models face two\nfundamental constraints: causality and accumulation. Causality prevents access\nto future frames that offline methods exploit, while accumulation causes tokens\nto grow unbounded, creating efficiency bottlenecks. However, existing\napproaches only regulate post-LLM kv-cache, leaving costly pre-LLM prefill\nunchanged. We introduce StreamingTOM, a training-free, plug-and-play two-stage\nframework that addresses both pre-LLM and post-LLM bottlenecks with predictable\nlatency. Causal Temporal Reduction imposes a fixed per-frame budget and selects\ntokens based on adjacent-frame changes and token saliency, drastically reducing\nper-frame prefill cost by processing only a compact subset of visual tokens per\nframe instead of all visual tokens. Online Quantized Memory stores tokens in\n4-bit format, retrieves relevant groups on demand, and dequantizes them,\nkeeping the active kv-cache bounded regardless of stream length. Experiments\ndemonstrate our method achieves $15.7\\times$ kv-cache compression, $1.2\\times$\nlower peak memory and $2\\times$ faster TTFT compared to prior SOTA.\nStreamingTOM maintains state-of-the-art accuracy among training-free methods\nwith an average of $63.8\\%$ on offline benchmarks and $55.8\\%/3.7$ on RVS.\nThese results highlight the practical benefits of our two-stage approach for\nefficient streaming video understanding with bounded growth.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlike offline processing, streaming video vision-language models face two\nfundamental constraints: causality and accumulation. Causality prevents access\nto future frames that offline methods exploit, while accumulation causes tokens\nto grow unbounded, creating efficiency bottlenecks. However, existing\napproaches only regulate post-LLM kv-cache, leaving costly pre-LLM prefill\nunchanged. We introduce StreamingTOM, a training-free, plug-and-play two-stage\nframework that addresses both pre-LLM and post-LLM bottlenecks with predictable\nlatency. Causal Temporal Reduction imposes a fixed per-frame budget and selects\ntokens based on adjacent-frame changes and token saliency, drastically reducing\nper-frame prefill cost by processing only a compact subset of visual tokens per\nframe instead of all visual tokens. Online Quantized Memory stores tokens in\n4-bit format, retrieves relevant groups on demand, and dequantizes them,\nkeeping the active kv-cache bounded regardless of stream length. Experiments\ndemonstrate our method achieves $15.7\\times$ kv-cache compression, $1.2\\times$\nlower peak memory and $2\\times$ faster TTFT compared to prior SOTA.\nStreamingTOM maintains state-of-the-art accuracy among training-free methods\nwith an average of $63.8\\%$ on offline benchmarks and $55.8\\%/3.7$ on RVS.\nThese results highlight the practical benefits of our two-stage approach for\nefficient streaming video understanding with bounded growth."
                },
                "authors": [
                    {
                        "name": "Xueyi Chen"
                    },
                    {
                        "name": "Keda Tao"
                    },
                    {
                        "name": "Kele Shao"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18269v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18269v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17777v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17777v1",
                "updated": "2025-10-20T17:35:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    17,
                    35,
                    47,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T17:35:47Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    17,
                    35,
                    47,
                    0,
                    293,
                    0
                ],
                "title": "SparseVILA: Decoupling Visual Sparsity for Efficient VLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparseVILA: Decoupling Visual Sparsity for Efficient VLM Inference"
                },
                "summary": "Vision Language Models (VLMs) have rapidly advanced in integrating visual and\ntextual reasoning, powering applications across high-resolution image\nunderstanding, long-video analysis, and multi-turn conversation. However, their\nscalability remains limited by the growing number of visual tokens that\ndominate inference latency. We present SparseVILA, a new paradigm for efficient\nVLM inference that decouples visual sparsity across the prefilling and decoding\nstages. SparseVILA distributes sparsity across stages by pruning redundant\nvisual tokens during prefill and retrieving only query-relevant tokens during\ndecoding. This decoupled design matches leading prefill pruning methods while\npreserving multi-turn fidelity by retaining most of the visual cache so that\nquery-aware tokens can be retrieved at each conversation round. Built on an\nAWQ-optimized inference pipeline, SparseVILA achieves up to 4.0 times faster\nprefilling, 2.5 times faster decoding, and an overall 2.6 times end-to-end\nspeedup on long-context video tasks -- while improving accuracy on\ndocument-understanding and reasoning tasks. By decoupling query-agnostic\npruning and query-aware retrieval, SparseVILA establishes a new direction for\nefficient multimodal inference, offering a training-free, architecture-agnostic\nframework for accelerating large VLMs without sacrificing capability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision Language Models (VLMs) have rapidly advanced in integrating visual and\ntextual reasoning, powering applications across high-resolution image\nunderstanding, long-video analysis, and multi-turn conversation. However, their\nscalability remains limited by the growing number of visual tokens that\ndominate inference latency. We present SparseVILA, a new paradigm for efficient\nVLM inference that decouples visual sparsity across the prefilling and decoding\nstages. SparseVILA distributes sparsity across stages by pruning redundant\nvisual tokens during prefill and retrieving only query-relevant tokens during\ndecoding. This decoupled design matches leading prefill pruning methods while\npreserving multi-turn fidelity by retaining most of the visual cache so that\nquery-aware tokens can be retrieved at each conversation round. Built on an\nAWQ-optimized inference pipeline, SparseVILA achieves up to 4.0 times faster\nprefilling, 2.5 times faster decoding, and an overall 2.6 times end-to-end\nspeedup on long-context video tasks -- while improving accuracy on\ndocument-understanding and reasoning tasks. By decoupling query-agnostic\npruning and query-aware retrieval, SparseVILA establishes a new direction for\nefficient multimodal inference, offering a training-free, architecture-agnostic\nframework for accelerating large VLMs without sacrificing capability."
                },
                "authors": [
                    {
                        "name": "Samir Khaki"
                    },
                    {
                        "name": "Junxian Guo"
                    },
                    {
                        "name": "Jiaming Tang"
                    },
                    {
                        "name": "Shang Yang"
                    },
                    {
                        "name": "Yukang Chen"
                    },
                    {
                        "name": "Konstantinos N. Plataniotis"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Zhijian Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zhijian Liu"
                },
                "author": "Zhijian Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17777v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17777v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17238v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17238v1",
                "updated": "2025-10-20T07:27:37Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    7,
                    27,
                    37,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T07:27:37Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    7,
                    27,
                    37,
                    0,
                    293,
                    0
                ],
                "title": "StreamingThinker: Large Language Models Can Think While Reading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StreamingThinker: Large Language Models Can Think While Reading"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in\nchain of thought (CoT) reasoning. However, the current LLM reasoning paradigm\ninitiates thinking only after the entire input is available, which introduces\nunnecessary latency and weakens attention to earlier information in dynamic\nscenarios. Inspired by human cognition of thinking while reading, we first\ndesign a \\textit{\\textbf{streaming thinking}} paradigm for LLMs, where\nreasoning unfolds in the order of input and further adjusts its depth once\nreading is complete. We instantiate this paradigm with\n\\textit{StreamingThinker}, a framework that enables LLMs to think while reading\nthrough the integration of streaming CoT generation, streaming-constraint\ntraining, and streaming parallel inference. Specifically, StreamingThinker\nemploys streaming reasoning units with quality control for CoT generation,\nenforces order-preserving reasoning through streaming attention masks and\nposition encoding, and leverages parallel KV caches that decouple input\nencoding from reasoning generation, thereby ensuring alignment and enabling\ntrue concurrency. We evaluate StreamingThinker on the Qwen3 model family across\nmath reasoning, logical reasoning, and context-based QA reasoning tasks.\nExperimental results show that the StreamingThinker preserves performance\ncomparable to batch thinking, while yielding an 80\\% reduction in token waiting\nbefore the onset of reasoning and a more than 60\\% reduction in time-level\nlatency for producing the final answer, demonstrating the effectiveness of the\nstreaming paradigm for LLM reasoning. Code will be released at\n\\href{https://github.com/EIT-NLP/StreamingLLM/tree/main/StreamingThinker}{this\nrepository.}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable capabilities in\nchain of thought (CoT) reasoning. However, the current LLM reasoning paradigm\ninitiates thinking only after the entire input is available, which introduces\nunnecessary latency and weakens attention to earlier information in dynamic\nscenarios. Inspired by human cognition of thinking while reading, we first\ndesign a \\textit{\\textbf{streaming thinking}} paradigm for LLMs, where\nreasoning unfolds in the order of input and further adjusts its depth once\nreading is complete. We instantiate this paradigm with\n\\textit{StreamingThinker}, a framework that enables LLMs to think while reading\nthrough the integration of streaming CoT generation, streaming-constraint\ntraining, and streaming parallel inference. Specifically, StreamingThinker\nemploys streaming reasoning units with quality control for CoT generation,\nenforces order-preserving reasoning through streaming attention masks and\nposition encoding, and leverages parallel KV caches that decouple input\nencoding from reasoning generation, thereby ensuring alignment and enabling\ntrue concurrency. We evaluate StreamingThinker on the Qwen3 model family across\nmath reasoning, logical reasoning, and context-based QA reasoning tasks.\nExperimental results show that the StreamingThinker preserves performance\ncomparable to batch thinking, while yielding an 80\\% reduction in token waiting\nbefore the onset of reasoning and a more than 60\\% reduction in time-level\nlatency for producing the final answer, demonstrating the effectiveness of the\nstreaming paradigm for LLM reasoning. Code will be released at\n\\href{https://github.com/EIT-NLP/StreamingLLM/tree/main/StreamingThinker}{this\nrepository.}"
                },
                "authors": [
                    {
                        "name": "Junlong Tong"
                    },
                    {
                        "name": "Yingqi Fan"
                    },
                    {
                        "name": "Anhao Zhao"
                    },
                    {
                        "name": "Yunpu Ma"
                    },
                    {
                        "name": "Xiaoyu Shen"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoyu Shen"
                },
                "author": "Xiaoyu Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17238v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17238v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17098v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17098v1",
                "updated": "2025-10-20T02:04:18Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    2,
                    4,
                    18,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T02:04:18Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    2,
                    4,
                    18,
                    0,
                    293,
                    0
                ],
                "title": "Can Transformer Memory Be Corrupted? Investigating Cache-Side\n  Vulnerabilities in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Transformer Memory Be Corrupted? Investigating Cache-Side\n  Vulnerabilities in Large Language Models"
                },
                "summary": "Even when prompts and parameters are secured, transformer language models\nremain vulnerable because their key-value (KV) cache during inference\nconstitutes an overlooked attack surface. This paper introduces Malicious Token\nInjection (MTI), a modular framework that systematically perturbs cached key\nvectors at selected layers and timesteps through controlled magnitude and\nfrequency, using additive Gaussian noise, zeroing, and orthogonal rotations. A\ntheoretical analysis quantifies how these perturbations propagate through\nattention, linking logit deviations to the Frobenius norm of corruption and\nsoftmax Lipschitz dynamics. Empirical results show that MTI significantly\nalters next-token distributions and downstream task performance across GPT-2\nand LLaMA-2/7B, as well as destabilizes retrieval-augmented and agentic\nreasoning pipelines. These findings identify cache integrity as a critical yet\nunderexplored vulnerability in current LLM deployments, positioning cache\ncorruption as a reproducible and theoretically grounded threat model for future\nrobustness and security research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Even when prompts and parameters are secured, transformer language models\nremain vulnerable because their key-value (KV) cache during inference\nconstitutes an overlooked attack surface. This paper introduces Malicious Token\nInjection (MTI), a modular framework that systematically perturbs cached key\nvectors at selected layers and timesteps through controlled magnitude and\nfrequency, using additive Gaussian noise, zeroing, and orthogonal rotations. A\ntheoretical analysis quantifies how these perturbations propagate through\nattention, linking logit deviations to the Frobenius norm of corruption and\nsoftmax Lipschitz dynamics. Empirical results show that MTI significantly\nalters next-token distributions and downstream task performance across GPT-2\nand LLaMA-2/7B, as well as destabilizes retrieval-augmented and agentic\nreasoning pipelines. These findings identify cache integrity as a critical yet\nunderexplored vulnerability in current LLM deployments, positioning cache\ncorruption as a reproducible and theoretically grounded threat model for future\nrobustness and security research."
                },
                "authors": [
                    {
                        "name": "Elias Hossain"
                    },
                    {
                        "name": "Swayamjit Saha"
                    },
                    {
                        "name": "Somshubhra Roy"
                    },
                    {
                        "name": "Ravi Prasad"
                    }
                ],
                "author_detail": {
                    "name": "Ravi Prasad"
                },
                "author": "Ravi Prasad",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17098v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17098v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17045v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17045v1",
                "updated": "2025-10-19T23:17:13Z",
                "updated_parsed": [
                    2025,
                    10,
                    19,
                    23,
                    17,
                    13,
                    6,
                    292,
                    0
                ],
                "published": "2025-10-19T23:17:13Z",
                "published_parsed": [
                    2025,
                    10,
                    19,
                    23,
                    17,
                    13,
                    6,
                    292,
                    0
                ],
                "title": "Video Reasoning without Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Reasoning without Training"
                },
                "summary": "Video reasoning using Large Multimodal Models (LMMs) relies on costly\nreinforcement learning (RL) and verbose chain-of-thought, resulting in\nsubstantial computational overhead during both training and inference.\nMoreover, the mechanisms that control the thinking process in these reasoning\nmodels are very limited. In this paper, using entropy of the model's output as\na signal, we discover that the high-quality models go through a series of\nmicro-explorations and micro-exploitations which keep the reasoning process\ngrounded (i.e., avoid excessive randomness while the model is exploring or\nthinking through an answer). We further observe that once this \"thinking\"\nprocess is over, more accurate models demonstrate a better convergence by\nreducing the entropy significantly via a final exploitation phase (i.e., a more\ncertain convergence towards a solution trajectory). We then use these novel,\ntheoretically-grounded insights to tune the model's behavior directly at\ninference, without using any RL or supervised fine-tuning. Specifically, during\ninference, our proposed approach called V-Reason (Video-Reason) adapts the\nvalue cache of the LMM via a few optimization steps on a small, trainable\ncontroller using an entropy-based objective, i.e., no supervision from any\ndataset or RL is necessary. This tuning improves the model's micro-exploration\nand exploitation behavior during inference. Our experiments show that our\nproposed method achieves significant improvements over the base\ninstruction-tuned models across several video reasoning datasets, narrowing the\ngap with RL-trained models to within 0.6% average accuracy without any\ntraining, while offering massive efficiency benefits: output tokens are reduced\nby 58.6% compared to the RL model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video reasoning using Large Multimodal Models (LMMs) relies on costly\nreinforcement learning (RL) and verbose chain-of-thought, resulting in\nsubstantial computational overhead during both training and inference.\nMoreover, the mechanisms that control the thinking process in these reasoning\nmodels are very limited. In this paper, using entropy of the model's output as\na signal, we discover that the high-quality models go through a series of\nmicro-explorations and micro-exploitations which keep the reasoning process\ngrounded (i.e., avoid excessive randomness while the model is exploring or\nthinking through an answer). We further observe that once this \"thinking\"\nprocess is over, more accurate models demonstrate a better convergence by\nreducing the entropy significantly via a final exploitation phase (i.e., a more\ncertain convergence towards a solution trajectory). We then use these novel,\ntheoretically-grounded insights to tune the model's behavior directly at\ninference, without using any RL or supervised fine-tuning. Specifically, during\ninference, our proposed approach called V-Reason (Video-Reason) adapts the\nvalue cache of the LMM via a few optimization steps on a small, trainable\ncontroller using an entropy-based objective, i.e., no supervision from any\ndataset or RL is necessary. This tuning improves the model's micro-exploration\nand exploitation behavior during inference. Our experiments show that our\nproposed method achieves significant improvements over the base\ninstruction-tuned models across several video reasoning datasets, narrowing the\ngap with RL-trained models to within 0.6% average accuracy without any\ntraining, while offering massive efficiency benefits: output tokens are reduced\nby 58.6% compared to the RL model."
                },
                "authors": [
                    {
                        "name": "Deepak Sridhar"
                    },
                    {
                        "name": "Kartikeya Bhardwaj"
                    },
                    {
                        "name": "Jeya Pradha Jeyaraj"
                    },
                    {
                        "name": "Nuno Vasconcelos"
                    },
                    {
                        "name": "Ankita Nayak"
                    },
                    {
                        "name": "Harris Teague"
                    }
                ],
                "author_detail": {
                    "name": "Harris Teague"
                },
                "author": "Harris Teague",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17045v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17045v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.16871v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.16871v1",
                "updated": "2025-10-19T15:13:25Z",
                "updated_parsed": [
                    2025,
                    10,
                    19,
                    15,
                    13,
                    25,
                    6,
                    292,
                    0
                ],
                "published": "2025-10-19T15:13:25Z",
                "published_parsed": [
                    2025,
                    10,
                    19,
                    15,
                    13,
                    25,
                    6,
                    292,
                    0
                ],
                "title": "Addendum: Systematic Evaluation of Randomized Cache Designs against\n  Cache Occupancy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Addendum: Systematic Evaluation of Randomized Cache Designs against\n  Cache Occupancy"
                },
                "summary": "In the main text published at USENIX Security 2025, we presented a systematic\nanalysis of the role of cache occupancy in the design considerations for\nrandomized caches (from the perspectives of performance and security). On the\nperformance front, we presented a uniform benchmarking strategy that allows for\na fair comparison among different randomized cache designs. Likewise, from the\nsecurity perspective, we presented three threat assumptions: (1) covert\nchannels; (2) process fingerprinting side-channel; and (3) AES key recovery.\nThe main takeaway of our work is an open problem of designing a randomized\ncache of comparable efficiency with modern set-associative LLCs, while still\nresisting both contention-based and occupancy-based attacks. This note is meant\nas an addendum to the main text in light of the observations made in [2]. To\nsummarize, the authors in [2] argue that (1) L1d cache size plays a role in\nadversarial success, and that (2) a patched version of MIRAGE with randomized\ninitial seeding of global eviction map prevents leakage of AES key. We discuss\nthe same in this addendum.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the main text published at USENIX Security 2025, we presented a systematic\nanalysis of the role of cache occupancy in the design considerations for\nrandomized caches (from the perspectives of performance and security). On the\nperformance front, we presented a uniform benchmarking strategy that allows for\na fair comparison among different randomized cache designs. Likewise, from the\nsecurity perspective, we presented three threat assumptions: (1) covert\nchannels; (2) process fingerprinting side-channel; and (3) AES key recovery.\nThe main takeaway of our work is an open problem of designing a randomized\ncache of comparable efficiency with modern set-associative LLCs, while still\nresisting both contention-based and occupancy-based attacks. This note is meant\nas an addendum to the main text in light of the observations made in [2]. To\nsummarize, the authors in [2] argue that (1) L1d cache size plays a role in\nadversarial success, and that (2) a patched version of MIRAGE with randomized\ninitial seeding of global eviction map prevents leakage of AES key. We discuss\nthe same in this addendum."
                },
                "authors": [
                    {
                        "name": "Anirban Chakraborty"
                    },
                    {
                        "name": "Nimish Mishra"
                    },
                    {
                        "name": "Sayandeep Saha"
                    },
                    {
                        "name": "Sarani Bhattacharya"
                    },
                    {
                        "name": "Debdeep Mukhopadhyay"
                    }
                ],
                "author_detail": {
                    "name": "Debdeep Mukhopadhyay"
                },
                "author": "Debdeep Mukhopadhyay",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.16871v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.16871v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.16805v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.16805v1",
                "updated": "2025-10-19T12:16:40Z",
                "updated_parsed": [
                    2025,
                    10,
                    19,
                    12,
                    16,
                    40,
                    6,
                    292,
                    0
                ],
                "published": "2025-10-19T12:16:40Z",
                "published_parsed": [
                    2025,
                    10,
                    19,
                    12,
                    16,
                    40,
                    6,
                    292,
                    0
                ],
                "title": "Mixed-Precision Quantization for Language Models: Techniques and\n  Prospects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixed-Precision Quantization for Language Models: Techniques and\n  Prospects"
                },
                "summary": "The rapid scaling of language models (LMs) has resulted in unprecedented\ncomputational, memory, and energy requirements, making their training and\ndeployment increasingly unsustainable. Quantization has emerged as an essential\ncompression technique to reduce model size, alleviate memory bottlenecks, and\naccelerate inference. However, while uniform low-bit quantization (e.g., INT8,\nINT4) provides significant efficiency gains, it can degrade accuracy in\nsensitive components of transformer-based LMs. Mixed-precision quantization\noffers a promising alternative by selectively allocating precision across\nlayers or within tensors to balance efficiency and accuracy. This survey\nprovides a comprehensive overview of Mixed-Precision quantization frameworks\nfor LMs (MXPLMs). We first review quantization fundamentals, including uniform\nand non-uniform quantizers, quantization granularity, and methods widely used\nin post-training quantization. We then categorize and compare recent MXPLM\nframeworks according to their bit allocation strategies and precision\nconfigurations across weights, activations, and key-value caches. A comparative\nanalysis highlights differences in perplexity, zero-shot task performance, and\ndeployment trade-offs. Furthermore, we contrast MXPLMs with earlier\nmixed-precision quantization methods for deep neural networks, identifying\nstrategies that transfer and those that face challenges in the LM setting.\nFinally, we summarize open issues and future directions, including\nhardware-aware design, activation quantization, and scalable optimization\nmethods for billion-parameter models. By consolidating recent advances, this\nwork serves as a reference for understanding the current landscape and research\nprospects of mixed-precision quantization for large-scale language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid scaling of language models (LMs) has resulted in unprecedented\ncomputational, memory, and energy requirements, making their training and\ndeployment increasingly unsustainable. Quantization has emerged as an essential\ncompression technique to reduce model size, alleviate memory bottlenecks, and\naccelerate inference. However, while uniform low-bit quantization (e.g., INT8,\nINT4) provides significant efficiency gains, it can degrade accuracy in\nsensitive components of transformer-based LMs. Mixed-precision quantization\noffers a promising alternative by selectively allocating precision across\nlayers or within tensors to balance efficiency and accuracy. This survey\nprovides a comprehensive overview of Mixed-Precision quantization frameworks\nfor LMs (MXPLMs). We first review quantization fundamentals, including uniform\nand non-uniform quantizers, quantization granularity, and methods widely used\nin post-training quantization. We then categorize and compare recent MXPLM\nframeworks according to their bit allocation strategies and precision\nconfigurations across weights, activations, and key-value caches. A comparative\nanalysis highlights differences in perplexity, zero-shot task performance, and\ndeployment trade-offs. Furthermore, we contrast MXPLMs with earlier\nmixed-precision quantization methods for deep neural networks, identifying\nstrategies that transfer and those that face challenges in the LM setting.\nFinally, we summarize open issues and future directions, including\nhardware-aware design, activation quantization, and scalable optimization\nmethods for billion-parameter models. By consolidating recent advances, this\nwork serves as a reference for understanding the current landscape and research\nprospects of mixed-precision quantization for large-scale language models."
                },
                "authors": [
                    {
                        "name": "Mariam Rakka"
                    },
                    {
                        "name": "Marios Fournarakis"
                    },
                    {
                        "name": "Olga Krestinskaya"
                    },
                    {
                        "name": "Jinane Bazzi"
                    },
                    {
                        "name": "Khaled N. Salama"
                    },
                    {
                        "name": "Fadi Kurdahi"
                    },
                    {
                        "name": "Ahmed M. Eltawil"
                    },
                    {
                        "name": "Mohammed E. Fouda"
                    }
                ],
                "author_detail": {
                    "name": "Mohammed E. Fouda"
                },
                "author": "Mohammed E. Fouda",
                "arxiv_comment": "46 pages, 6 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.16805v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.16805v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19505v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19505v2",
                "updated": "2025-10-18T11:29:52Z",
                "updated_parsed": [
                    2025,
                    10,
                    18,
                    11,
                    29,
                    52,
                    5,
                    291,
                    0
                ],
                "published": "2025-06-24T10:45:48Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    10,
                    45,
                    48,
                    1,
                    175,
                    0
                ],
                "title": "AnTKV: Anchor Token-Aware Sub-Bit Vector Quantization for KV Cache in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AnTKV: Anchor Token-Aware Sub-Bit Vector Quantization for KV Cache in\n  Large Language Models"
                },
                "summary": "Quantization has emerged as an effective and lightweight solution to reduce\nthe memory footprint of the KV cache in Large Language Models. Nevertheless,\nminimizing the accuracy degradation caused by ultra-low-bit KV cache\nquantization remains a significant challenge. While scalar quantization is\nconstrained by 1-bit bound, vector quantization exploits intra-vector\ncorrelations and enables sub-bit regimes, making it more suitable for\nultra-low-bit quantization. To further mitigate quantization-induced\ndegradation, we reveal that the degradation is highly uneven across tokens in\nattention quality. To investigate this unevenness, we introduce anchor score to\nmeasure each token's sensitivity to quantization. Our analysis and experiments\nshow that preserving a small subset (1\\%) of tokens with the highest Anchor\nScore significantly mitigates accuracy loss under aggressive quantization.\n  We propose AnTKV, a dual-stage framework that leverages anchor token-aware\nvector quantization to compress the KV cache. It combines offline token-aware\ncentroids learning and online anchor token selection to balance compression and\naccuracy. To enable efficient deployment, we design an online anchor token\nselection kernel compatible with FlashAttention. It allows LLaMA3-8B to scale\nto 840K tokens on a single 80GB A100, while delivering up to $3.5\\times$ higher\ndecoding throughput over the FP16 baseline. Experiments demonstrate that AnTKV\nmatches or surpasses prior methods at 4-bit, and significantly reduce\nperplexity under ultra-low-bit quantization, achieving 6.32 at 1-bit on\nMistral-7B, compared to 7.25 for CQ and 15.36 for KVQuant.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization has emerged as an effective and lightweight solution to reduce\nthe memory footprint of the KV cache in Large Language Models. Nevertheless,\nminimizing the accuracy degradation caused by ultra-low-bit KV cache\nquantization remains a significant challenge. While scalar quantization is\nconstrained by 1-bit bound, vector quantization exploits intra-vector\ncorrelations and enables sub-bit regimes, making it more suitable for\nultra-low-bit quantization. To further mitigate quantization-induced\ndegradation, we reveal that the degradation is highly uneven across tokens in\nattention quality. To investigate this unevenness, we introduce anchor score to\nmeasure each token's sensitivity to quantization. Our analysis and experiments\nshow that preserving a small subset (1\\%) of tokens with the highest Anchor\nScore significantly mitigates accuracy loss under aggressive quantization.\n  We propose AnTKV, a dual-stage framework that leverages anchor token-aware\nvector quantization to compress the KV cache. It combines offline token-aware\ncentroids learning and online anchor token selection to balance compression and\naccuracy. To enable efficient deployment, we design an online anchor token\nselection kernel compatible with FlashAttention. It allows LLaMA3-8B to scale\nto 840K tokens on a single 80GB A100, while delivering up to $3.5\\times$ higher\ndecoding throughput over the FP16 baseline. Experiments demonstrate that AnTKV\nmatches or surpasses prior methods at 4-bit, and significantly reduce\nperplexity under ultra-low-bit quantization, achieving 6.32 at 1-bit on\nMistral-7B, compared to 7.25 for CQ and 15.36 for KVQuant."
                },
                "authors": [
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Chuanfu Xiao"
                    },
                    {
                        "name": "Yang Wang"
                    },
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Baotong Lu"
                    },
                    {
                        "name": "Mao Yang"
                    },
                    {
                        "name": "Xinyu Chen"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19505v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19505v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06807v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06807v2",
                "updated": "2025-10-18T06:04:53Z",
                "updated_parsed": [
                    2025,
                    10,
                    18,
                    6,
                    4,
                    53,
                    5,
                    291,
                    0
                ],
                "published": "2025-01-12T13:18:04Z",
                "published_parsed": [
                    2025,
                    1,
                    12,
                    13,
                    18,
                    4,
                    6,
                    12,
                    0
                ],
                "title": "MPCache: MPC-Friendly KV Cache Eviction for Efficient Private LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MPCache: MPC-Friendly KV Cache Eviction for Efficient Private LLM\n  Inference"
                },
                "summary": "Private large language model (LLM) inference based on secure multi-party\ncomputation (MPC) achieves formal data privacy protection but suffers from\nsignificant latency overhead, especially for long input sequences. While\nkey-value (KV) cache eviction and sparse attention algorithms have been\nproposed for efficient LLM inference in plaintext, they are not designed for\nMPC and cannot benefit private LLM inference directly. In this paper, we\npropose an accurate and MPC-friendly KV cache eviction framework, dubbed\nMPCache, building on the observation that historical tokens in a long sequence\nmay have different effects on the downstream decoding. Hence, MPCache combines\na look-once static eviction algorithm to discard unimportant KV cache and a\nquery-aware dynamic selection algorithm to activate only a small subset of KV\ncache for attention computation. MPCache further incorporates a series of\noptimizations for efficient dynamic KV cache selection, including MPC-friendly\nsimilarity approximation, hierarchical KV cache clustering, and cross-layer\nindex-sharing strategy. Extensive experiments demonstrate that MPCache\nconsistently outperforms prior-art KV cache eviction baselines across different\ngeneration tasks and achieves 1.8 ~ 2.01x and 3.39 ~ 8.37x decoding latency and\ncommunication reduction on different sequence lengths, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Private large language model (LLM) inference based on secure multi-party\ncomputation (MPC) achieves formal data privacy protection but suffers from\nsignificant latency overhead, especially for long input sequences. While\nkey-value (KV) cache eviction and sparse attention algorithms have been\nproposed for efficient LLM inference in plaintext, they are not designed for\nMPC and cannot benefit private LLM inference directly. In this paper, we\npropose an accurate and MPC-friendly KV cache eviction framework, dubbed\nMPCache, building on the observation that historical tokens in a long sequence\nmay have different effects on the downstream decoding. Hence, MPCache combines\na look-once static eviction algorithm to discard unimportant KV cache and a\nquery-aware dynamic selection algorithm to activate only a small subset of KV\ncache for attention computation. MPCache further incorporates a series of\noptimizations for efficient dynamic KV cache selection, including MPC-friendly\nsimilarity approximation, hierarchical KV cache clustering, and cross-layer\nindex-sharing strategy. Extensive experiments demonstrate that MPCache\nconsistently outperforms prior-art KV cache eviction baselines across different\ngeneration tasks and achieves 1.8 ~ 2.01x and 3.39 ~ 8.37x decoding latency and\ncommunication reduction on different sequence lengths, respectively."
                },
                "authors": [
                    {
                        "name": "Wenxuan Zeng"
                    },
                    {
                        "name": "Ye Dong"
                    },
                    {
                        "name": "Jinjin Zhou"
                    },
                    {
                        "name": "Jin Tan"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Tao Wei"
                    },
                    {
                        "name": "Runsheng Wang"
                    },
                    {
                        "name": "Meng Li"
                    }
                ],
                "author_detail": {
                    "name": "Meng Li"
                },
                "author": "Meng Li",
                "arxiv_comment": "Accepted by NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06807v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06807v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08907v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08907v3",
                "updated": "2025-10-18T02:48:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    18,
                    2,
                    48,
                    35,
                    5,
                    291,
                    0
                ],
                "published": "2025-10-10T01:42:14Z",
                "published_parsed": [
                    2025,
                    10,
                    10,
                    1,
                    42,
                    14,
                    4,
                    283,
                    0
                ],
                "title": "Autoencoding-Free Context Compression for LLMs via Contextual Semantic\n  Anchors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoencoding-Free Context Compression for LLMs via Contextual Semantic\n  Anchors"
                },
                "summary": "Context compression presents a promising approach for accelerating large\nlanguage model (LLM) inference by compressing long contexts into compact\nrepresentations. Current context compression methods predominantly rely on\nautoencoding tasks to train context-agnostic compression tokens to compress\ncontextual semantics. While autoencoding tasks enable compression tokens to\nacquire compression capabilities, compression via autoencoding tasks creates a\nfundamental mismatch: the models are optimized for reconstruction that diverge\nfrom actual downstream tasks, thereby weakening the features more beneficial\nfor real-world usage. We propose Semantic-Anchor Compression (SAC), a novel\nmethod that shifts from autoencoding task based compression to an architecture\nthat is equipped with this compression capability \\textit{a priori}. Instead of\ntraining models to compress contexts through autoencoding tasks, SAC directly\nselects so-called anchor tokens from the original context and aggregates\ncontextual information into their key-value (KV) representations. By deriving\nrepresentations directly from the contextual tokens, SAC eliminates the need\nfor autoencoding training. To ensure compression performance while directly\nleveraging anchor tokens, SAC incorporates two key designs: (1) anchor\nembeddings that enable the compressor to identify critical tokens, and (2)\nbidirectional attention modification that allows anchor tokens to capture\ninformation from the entire context. Experimental results demonstrate that SAC\nconsistently outperforms existing context compression methods across various\ncompression ratios. On out-of-distribution evaluation using MRQA, SAC achieves\n1 EM improvement at 5x compression over strong baselines, with increasing\nadvantages at higher compression ratios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context compression presents a promising approach for accelerating large\nlanguage model (LLM) inference by compressing long contexts into compact\nrepresentations. Current context compression methods predominantly rely on\nautoencoding tasks to train context-agnostic compression tokens to compress\ncontextual semantics. While autoencoding tasks enable compression tokens to\nacquire compression capabilities, compression via autoencoding tasks creates a\nfundamental mismatch: the models are optimized for reconstruction that diverge\nfrom actual downstream tasks, thereby weakening the features more beneficial\nfor real-world usage. We propose Semantic-Anchor Compression (SAC), a novel\nmethod that shifts from autoencoding task based compression to an architecture\nthat is equipped with this compression capability \\textit{a priori}. Instead of\ntraining models to compress contexts through autoencoding tasks, SAC directly\nselects so-called anchor tokens from the original context and aggregates\ncontextual information into their key-value (KV) representations. By deriving\nrepresentations directly from the contextual tokens, SAC eliminates the need\nfor autoencoding training. To ensure compression performance while directly\nleveraging anchor tokens, SAC incorporates two key designs: (1) anchor\nembeddings that enable the compressor to identify critical tokens, and (2)\nbidirectional attention modification that allows anchor tokens to capture\ninformation from the entire context. Experimental results demonstrate that SAC\nconsistently outperforms existing context compression methods across various\ncompression ratios. On out-of-distribution evaluation using MRQA, SAC achieves\n1 EM improvement at 5x compression over strong baselines, with increasing\nadvantages at higher compression ratios."
                },
                "authors": [
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Runsong Zhao"
                    },
                    {
                        "name": "Pengcheng Huang"
                    },
                    {
                        "name": "Xinyu Liu"
                    },
                    {
                        "name": "Junyi Xiao"
                    },
                    {
                        "name": "Chunyang Xiao"
                    },
                    {
                        "name": "Tong Xiao"
                    },
                    {
                        "name": "Shengxiang Gao"
                    },
                    {
                        "name": "Zhengtao Yu"
                    },
                    {
                        "name": "Jingbo Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jingbo Zhu"
                },
                "author": "Jingbo Zhu",
                "arxiv_comment": "18 pages,9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08907v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08907v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.16292v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.16292v1",
                "updated": "2025-10-18T01:31:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    18,
                    1,
                    31,
                    14,
                    5,
                    291,
                    0
                ],
                "published": "2025-10-18T01:31:14Z",
                "published_parsed": [
                    2025,
                    10,
                    18,
                    1,
                    31,
                    14,
                    5,
                    291,
                    0
                ],
                "title": "QSVD: Efficient Low-rank Approximation for Unified Query-Key-Value\n  Weight Compression in Low-Precision Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QSVD: Efficient Low-rank Approximation for Unified Query-Key-Value\n  Weight Compression in Low-Precision Vision-Language Models"
                },
                "summary": "Vision-Language Models (VLMs) are integral to tasks such as image captioning\nand visual question answering, but their high computational cost, driven by\nlarge memory footprints and processing time, limits their scalability and\nreal-time applicability. In this work, we propose leveraging Singular-Value\nDecomposition (SVD) over the joint query (Q), key (K), and value (V) weight\nmatrices to reduce KV cache size and computational overhead. We in addition\nintroduce an efficient rank allocation strategy that dynamically adjusts the\nSVD rank based on its impact on VLM accuracy, achieving a significant reduction\nin both memory usage and computational cost. Finally, we extend this approach\nby applying quantization to both VLM weights and activations, resulting in a\nhighly efficient VLM. Our method outperforms previous approaches that rely\nsolely on quantization or SVD by achieving more than $10\\%$ accuracy\nimprovement while consuming less hardware cost, making it better for real-time\ndeployment on resource-constrained devices. We open source our code at\n\\href{https://github.com/SAI-Lab-NYU/QSVD}{\\texttt{https://github.com/SAI-Lab-NYU/QSVD}}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Models (VLMs) are integral to tasks such as image captioning\nand visual question answering, but their high computational cost, driven by\nlarge memory footprints and processing time, limits their scalability and\nreal-time applicability. In this work, we propose leveraging Singular-Value\nDecomposition (SVD) over the joint query (Q), key (K), and value (V) weight\nmatrices to reduce KV cache size and computational overhead. We in addition\nintroduce an efficient rank allocation strategy that dynamically adjusts the\nSVD rank based on its impact on VLM accuracy, achieving a significant reduction\nin both memory usage and computational cost. Finally, we extend this approach\nby applying quantization to both VLM weights and activations, resulting in a\nhighly efficient VLM. Our method outperforms previous approaches that rely\nsolely on quantization or SVD by achieving more than $10\\%$ accuracy\nimprovement while consuming less hardware cost, making it better for real-time\ndeployment on resource-constrained devices. We open source our code at\n\\href{https://github.com/SAI-Lab-NYU/QSVD}{\\texttt{https://github.com/SAI-Lab-NYU/QSVD}}."
                },
                "authors": [
                    {
                        "name": "Yutong Wang"
                    },
                    {
                        "name": "Haiyu Wang"
                    },
                    {
                        "name": "Sai Qian Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Sai Qian Zhang"
                },
                "author": "Sai Qian Zhang",
                "arxiv_comment": "Accepted as Spotlight paper by NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.16292v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.16292v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.16276v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.16276v1",
                "updated": "2025-10-18T00:21:45Z",
                "updated_parsed": [
                    2025,
                    10,
                    18,
                    0,
                    21,
                    45,
                    5,
                    291,
                    0
                ],
                "published": "2025-10-18T00:21:45Z",
                "published_parsed": [
                    2025,
                    10,
                    18,
                    0,
                    21,
                    45,
                    5,
                    291,
                    0
                ],
                "title": "What Limits Agentic Systems Efficiency?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What Limits Agentic Systems Efficiency?"
                },
                "summary": "Large Language Models (LLMs), such as OpenAI-o1 and DeepSeek-R1, have\ndemonstrated strong reasoning capabilities. To further enhance LLM\ncapabilities, recent agentic systems, such as Deep Research, incorporate web\ninteractions into LLM reasoning to mitigate uncertainties and reduce potential\nerrors. However, existing research predominantly focuses on reasoning\nperformance, often neglecting the efficiency of agentic systems. In this work,\nwe present a comprehensive empirical study that identifies efficiency\nbottlenecks in web-interactive agentic systems. We decompose end-to-end latency\ninto two primary components: LLM API latency and web environment latency. We\nconduct a comprehensive empirical study across 15 models and 5 providers to\ndemonstrate high variability in API-based agentic systems. We observe that web\nenvironment latency can contribute as much as 53.7% to the overall latency in a\nweb-based agentic system. To improve latency, we propose SpecCache, a caching\nframework augmented with speculative execution that can reduce web environment\noverhead. Extensive evaluations on two standard benchmarks show that our\napproach improves the cache hit rate by up to 58x compared to a random caching\nstrategy, while reducing web environment overhead by up to 3.2x, without\ndegrading agentic system performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), such as OpenAI-o1 and DeepSeek-R1, have\ndemonstrated strong reasoning capabilities. To further enhance LLM\ncapabilities, recent agentic systems, such as Deep Research, incorporate web\ninteractions into LLM reasoning to mitigate uncertainties and reduce potential\nerrors. However, existing research predominantly focuses on reasoning\nperformance, often neglecting the efficiency of agentic systems. In this work,\nwe present a comprehensive empirical study that identifies efficiency\nbottlenecks in web-interactive agentic systems. We decompose end-to-end latency\ninto two primary components: LLM API latency and web environment latency. We\nconduct a comprehensive empirical study across 15 models and 5 providers to\ndemonstrate high variability in API-based agentic systems. We observe that web\nenvironment latency can contribute as much as 53.7% to the overall latency in a\nweb-based agentic system. To improve latency, we propose SpecCache, a caching\nframework augmented with speculative execution that can reduce web environment\noverhead. Extensive evaluations on two standard benchmarks show that our\napproach improves the cache hit rate by up to 58x compared to a random caching\nstrategy, while reducing web environment overhead by up to 3.2x, without\ndegrading agentic system performance."
                },
                "authors": [
                    {
                        "name": "Song Bian"
                    },
                    {
                        "name": "Minghao Yan"
                    },
                    {
                        "name": "Anand Jayarajan"
                    },
                    {
                        "name": "Gennady Pekhimenko"
                    },
                    {
                        "name": "Shivaram Venkataraman"
                    }
                ],
                "author_detail": {
                    "name": "Shivaram Venkataraman"
                },
                "author": "Shivaram Venkataraman",
                "arxiv_comment": "27 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.16276v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.16276v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.15590v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.15590v1",
                "updated": "2025-10-17T12:38:25Z",
                "updated_parsed": [
                    2025,
                    10,
                    17,
                    12,
                    38,
                    25,
                    4,
                    290,
                    0
                ],
                "published": "2025-10-17T12:38:25Z",
                "published_parsed": [
                    2025,
                    10,
                    17,
                    12,
                    38,
                    25,
                    4,
                    290,
                    0
                ],
                "title": "A single optically detectable tumbling spin in silicon",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A single optically detectable tumbling spin in silicon"
                },
                "summary": "Electron spin resonance spectroscopy is a widely used technique for analyzing\nthe microscopic structure, local environment and reorientation of atomic and\nmolecular systems. Conventional inductive detection methods typically require\nto probe more than a billion of electron spins such that single atom motion is\nhidden through ensemble averaging. While several single spin spectroscopy\nmethods are currently available, they have been so far limited to static\nsystems. Here we demonstrate single spin spectroscopy of a fluorescent tumbling\ndefect in silicon called the G center, behaving as a pseudo-molecule randomly\nreorienting itself in the crystalline matrix. Using high-resolution spin\nspectroscopy, we reveal a fine magnetic structure resulting from the spin\nprincipal axes jumping between discrete orientations in the crystal. By\nmodeling the atomic reorientation of the defect, we demonstrate that spin\ntumbling induces variations in the coupling to the microwave magnetic field,\nenabling position-dependent Rabi frequencies to be detected in coherent spin\ncontrol experiments. By virtue of its pseudo-molecule configuration, the G\ncenter in silicon is a unique quantum system to investigate the mutual\ninteraction between optical, spin and rotation properties in a highly versatile\nmaterial.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electron spin resonance spectroscopy is a widely used technique for analyzing\nthe microscopic structure, local environment and reorientation of atomic and\nmolecular systems. Conventional inductive detection methods typically require\nto probe more than a billion of electron spins such that single atom motion is\nhidden through ensemble averaging. While several single spin spectroscopy\nmethods are currently available, they have been so far limited to static\nsystems. Here we demonstrate single spin spectroscopy of a fluorescent tumbling\ndefect in silicon called the G center, behaving as a pseudo-molecule randomly\nreorienting itself in the crystalline matrix. Using high-resolution spin\nspectroscopy, we reveal a fine magnetic structure resulting from the spin\nprincipal axes jumping between discrete orientations in the crystal. By\nmodeling the atomic reorientation of the defect, we demonstrate that spin\ntumbling induces variations in the coupling to the microwave magnetic field,\nenabling position-dependent Rabi frequencies to be detected in coherent spin\ncontrol experiments. By virtue of its pseudo-molecule configuration, the G\ncenter in silicon is a unique quantum system to investigate the mutual\ninteraction between optical, spin and rotation properties in a highly versatile\nmaterial."
                },
                "authors": [
                    {
                        "name": "Félix Cache"
                    },
                    {
                        "name": "Yoann Baron"
                    },
                    {
                        "name": "Baptiste Lefaucher"
                    },
                    {
                        "name": "Jean-Baptiste Jager"
                    },
                    {
                        "name": "Frédéric Mazen"
                    },
                    {
                        "name": "Frédéric Milési"
                    },
                    {
                        "name": "Sébastien Kerdilès"
                    },
                    {
                        "name": "Isabelle Robert-Philip"
                    },
                    {
                        "name": "Jean-Michel Gérard"
                    },
                    {
                        "name": "Guillaume Cassabois"
                    },
                    {
                        "name": "Vincent Jacques"
                    },
                    {
                        "name": "Anaïs Dréau"
                    }
                ],
                "author_detail": {
                    "name": "Anaïs Dréau"
                },
                "author": "Anaïs Dréau",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.15590v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.15590v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.15413v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.15413v1",
                "updated": "2025-10-17T08:07:27Z",
                "updated_parsed": [
                    2025,
                    10,
                    17,
                    8,
                    7,
                    27,
                    4,
                    290,
                    0
                ],
                "published": "2025-10-17T08:07:27Z",
                "published_parsed": [
                    2025,
                    10,
                    17,
                    8,
                    7,
                    27,
                    4,
                    290,
                    0
                ],
                "title": "FHE-SQL: Fully Homomorphic Encrypted SQL Database",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FHE-SQL: Fully Homomorphic Encrypted SQL Database"
                },
                "summary": "FHE-SQL is a privacy-preserving database system that enables secure query\nprocessing on encrypted data using Fully Homomorphic Encryption (FHE),\nproviding privacy guaranties where an untrusted server can execute encrypted\nqueries without learning either the query contents or the underlying data.\nUnlike property-preserving encryption-based systems such as CryptDB, which rely\non deterministic or order-preserving encryption and are vulnerable to\nfrequency, order, and equality-pattern inference attacks, FHE-SQL performs\ncomputations entirely under encryption, eliminating these leakage channels.\nCompared to trusted-hardware approaches such as TrustedDB, which depend on a\nhardware security module and thus inherit its trust and side-channel\nlimitations, our design achieves end-to-end cryptographic protection without\nrequiring trusted execution environments. In contrast to high-performance\nFHE-based engines-Hermes, which target specialized workloads such as vector\nsearch, FHE-SQL supports general SQL query semantics with schema-aware,\ntype-safe definitions suitable for relational data management. FHE-SQL\nmitigates the high cost of ciphertext space by using an indirection\narchitecture that separates metadata in RocksDB from large ciphertexts in blob\nstorage. It supports oblivious selection via homomorphic boolean masks,\nmulti-tier caching, and garbage collection, with security proven under the\nUniversal Composability framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FHE-SQL is a privacy-preserving database system that enables secure query\nprocessing on encrypted data using Fully Homomorphic Encryption (FHE),\nproviding privacy guaranties where an untrusted server can execute encrypted\nqueries without learning either the query contents or the underlying data.\nUnlike property-preserving encryption-based systems such as CryptDB, which rely\non deterministic or order-preserving encryption and are vulnerable to\nfrequency, order, and equality-pattern inference attacks, FHE-SQL performs\ncomputations entirely under encryption, eliminating these leakage channels.\nCompared to trusted-hardware approaches such as TrustedDB, which depend on a\nhardware security module and thus inherit its trust and side-channel\nlimitations, our design achieves end-to-end cryptographic protection without\nrequiring trusted execution environments. In contrast to high-performance\nFHE-based engines-Hermes, which target specialized workloads such as vector\nsearch, FHE-SQL supports general SQL query semantics with schema-aware,\ntype-safe definitions suitable for relational data management. FHE-SQL\nmitigates the high cost of ciphertext space by using an indirection\narchitecture that separates metadata in RocksDB from large ciphertexts in blob\nstorage. It supports oblivious selection via homomorphic boolean masks,\nmulti-tier caching, and garbage collection, with security proven under the\nUniversal Composability framework."
                },
                "authors": [
                    {
                        "name": "Po-Yu Tseng"
                    },
                    {
                        "name": "Po-Chu Hsu"
                    },
                    {
                        "name": "Shih-Wei Liao"
                    }
                ],
                "author_detail": {
                    "name": "Shih-Wei Liao"
                },
                "author": "Shih-Wei Liao",
                "arxiv_comment": "12 pages, 1 figures, Keywords: Fully Homomorphic Encryption, Private\n  Information Retrieval, Encrypted Databases, Privacy-Preserving Systems",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.15413v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.15413v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03409v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03409v4",
                "updated": "2025-10-17T06:54:10Z",
                "updated_parsed": [
                    2025,
                    10,
                    17,
                    6,
                    54,
                    10,
                    4,
                    290,
                    0
                ],
                "published": "2024-12-04T15:48:59Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    15,
                    48,
                    59,
                    2,
                    339,
                    0
                ],
                "title": "PrefixKV: Adaptive Prefix KV Cache is What Vision Instruction-Following\n  Models Need for Efficient Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PrefixKV: Adaptive Prefix KV Cache is What Vision Instruction-Following\n  Models Need for Efficient Generation"
                },
                "summary": "Recently, large vision-language models (LVLMs) have rapidly gained popularity\nfor their strong generation and reasoning capabilities given diverse multimodal\ninputs. However, these models incur significant computational and memory\noverhead during inference, which greatly hinders the efficient deployment in\npractical scenarios. The extensive key-value (KV) cache, necessitated by the\nlengthy input and output sequences, notably contributes to the high inference\ncost. Based on this, recent works have investigated ways to reduce the KV cache\nsize for higher efficiency. Although effective, they generally overlook the\ndistinct importance distributions of KV vectors across layers and maintain the\nsame cache size for each layer during the next token prediction. This results\nin the significant contextual information loss for certain layers, leading to\nnotable performance decline. To address this, we present PrefixKV, where\n\"Prefix\" means the top-ranked KV based on importance rather than position in\nthe original sequence. It reframes the challenge of determining KV cache sizes\nfor all layers into the task of searching for the optimal global prefix\nconfiguration. With an adaptive layer-wise KV retention recipe based on binary\nsearch, the maximum contextual information can thus be preserved in each layer,\nfacilitating the generation. Extensive experiments demonstrate that our method\nachieves the state-of-the-art performance compared with others. It exhibits\nsuperior inference efficiency and generation quality trade-offs, showing\npromising potential for practical applications. Code is available at\nhttps://github.com/THU-MIG/PrefixKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large vision-language models (LVLMs) have rapidly gained popularity\nfor their strong generation and reasoning capabilities given diverse multimodal\ninputs. However, these models incur significant computational and memory\noverhead during inference, which greatly hinders the efficient deployment in\npractical scenarios. The extensive key-value (KV) cache, necessitated by the\nlengthy input and output sequences, notably contributes to the high inference\ncost. Based on this, recent works have investigated ways to reduce the KV cache\nsize for higher efficiency. Although effective, they generally overlook the\ndistinct importance distributions of KV vectors across layers and maintain the\nsame cache size for each layer during the next token prediction. This results\nin the significant contextual information loss for certain layers, leading to\nnotable performance decline. To address this, we present PrefixKV, where\n\"Prefix\" means the top-ranked KV based on importance rather than position in\nthe original sequence. It reframes the challenge of determining KV cache sizes\nfor all layers into the task of searching for the optimal global prefix\nconfiguration. With an adaptive layer-wise KV retention recipe based on binary\nsearch, the maximum contextual information can thus be preserved in each layer,\nfacilitating the generation. Extensive experiments demonstrate that our method\nachieves the state-of-the-art performance compared with others. It exhibits\nsuperior inference efficiency and generation quality trade-offs, showing\npromising potential for practical applications. Code is available at\nhttps://github.com/THU-MIG/PrefixKV."
                },
                "authors": [
                    {
                        "name": "Ao Wang"
                    },
                    {
                        "name": "Hui Chen"
                    },
                    {
                        "name": "Jiaxin Li"
                    },
                    {
                        "name": "Jianchao Tan"
                    },
                    {
                        "name": "Kefeng Zhang"
                    },
                    {
                        "name": "Xunliang Cai"
                    },
                    {
                        "name": "Zijia Lin"
                    },
                    {
                        "name": "Jungong Han"
                    },
                    {
                        "name": "Guiguang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Guiguang Ding"
                },
                "author": "Guiguang Ding",
                "arxiv_comment": "NeurIPS 2025 Camera-ready Version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03409v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03409v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.01890v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.01890v2",
                "updated": "2025-10-17T06:45:17Z",
                "updated_parsed": [
                    2025,
                    10,
                    17,
                    6,
                    45,
                    17,
                    4,
                    290,
                    0
                ],
                "published": "2024-08-04T00:38:34Z",
                "published_parsed": [
                    2024,
                    8,
                    4,
                    0,
                    38,
                    34,
                    6,
                    217,
                    0
                ],
                "title": "Cross-layer Attention Sharing for Pre-trained Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-layer Attention Sharing for Pre-trained Large Language Models"
                },
                "summary": "To enhance the efficiency of the attention mechanism within large language\nmodels (LLMs), previous works primarily compress the KV cache or group\nattention heads, while largely overlooking redundancy between layers. Our\ncomprehensive analyses across various LLMs show that highly similar attention\npatterns persist within most layers. It's intuitive to reduce the redundancy by\nsharing attention weights across layers. However, further analysis reveals two\nchallenges: (1) Directly sharing the weight matrix without carefully\nrearranging the attention heads proves to be ineffective; (2) Shallow layers\nare vulnerable to small deviations in attention weights.\n  Driven by these insights, we introduce LISA, a lightweight substitute for\nself-attention in well-trained LLMs. LISA employs tiny feed-forward networks to\nalign attention heads between adjacent layers and low-rank matrices to\napproximate differences in layer-wise attention weights. Evaluations\nencompassing 13 typical benchmarks demonstrate that LISA maintains high\nresponse quality in terms of accuracy and perplexity while reducing redundant\nattention calculations within 53%-84% of the total layers. Our implementations\nof LISA achieve a 6x compression of Q and K matrices within the attention\nmechanism, with maximum throughput improvements 19.5%, 32.3%, and 40.1% for\nLLaMA3-8B, LLaMA2-7B, and LLaMA2-13B, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To enhance the efficiency of the attention mechanism within large language\nmodels (LLMs), previous works primarily compress the KV cache or group\nattention heads, while largely overlooking redundancy between layers. Our\ncomprehensive analyses across various LLMs show that highly similar attention\npatterns persist within most layers. It's intuitive to reduce the redundancy by\nsharing attention weights across layers. However, further analysis reveals two\nchallenges: (1) Directly sharing the weight matrix without carefully\nrearranging the attention heads proves to be ineffective; (2) Shallow layers\nare vulnerable to small deviations in attention weights.\n  Driven by these insights, we introduce LISA, a lightweight substitute for\nself-attention in well-trained LLMs. LISA employs tiny feed-forward networks to\nalign attention heads between adjacent layers and low-rank matrices to\napproximate differences in layer-wise attention weights. Evaluations\nencompassing 13 typical benchmarks demonstrate that LISA maintains high\nresponse quality in terms of accuracy and perplexity while reducing redundant\nattention calculations within 53%-84% of the total layers. Our implementations\nof LISA achieve a 6x compression of Q and K matrices within the attention\nmechanism, with maximum throughput improvements 19.5%, 32.3%, and 40.1% for\nLLaMA3-8B, LLaMA2-7B, and LLaMA2-13B, respectively."
                },
                "authors": [
                    {
                        "name": "Yongyu Mu"
                    },
                    {
                        "name": "Yuzhang Wu"
                    },
                    {
                        "name": "Yuchun Fan"
                    },
                    {
                        "name": "Chenglong Wang"
                    },
                    {
                        "name": "Hengyu Li"
                    },
                    {
                        "name": "Jiali Zeng"
                    },
                    {
                        "name": "Qiaozhi He"
                    },
                    {
                        "name": "Murun Yang"
                    },
                    {
                        "name": "Fandong Meng"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Tong Xiao"
                    },
                    {
                        "name": "Jingbo Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jingbo Zhu"
                },
                "author": "Jingbo Zhu",
                "arxiv_comment": "A version accepted by TACL, prior to its publication by MIT Press",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.01890v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.01890v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.15152v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.15152v1",
                "updated": "2025-10-16T21:22:16Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    21,
                    22,
                    16,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T21:22:16Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    21,
                    22,
                    16,
                    3,
                    289,
                    0
                ],
                "title": "Tail-Optimized Caching for LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tail-Optimized Caching for LLM Inference"
                },
                "summary": "Prompt caching is critical for reducing latency and cost in LLM inference:\nOpenAI and Anthropic report up to 50-90% cost savings through prompt reuse.\nDespite its widespread success, little is known about what constitutes an\noptimal prompt caching policy, particularly when optimizing tail latency, a\nmetric of central importance to practitioners. The widely used Least Recently\nUsed (LRU) policy can perform arbitrarily poor on this metric, as it is\noblivious to the heterogeneity of conversation lengths. To address this gap, we\npropose Tail-Optimized LRU, a simple two-line modification that reallocates KV\ncache capacity to prioritize high-latency conversations by evicting cache\nentries that are unlikely to affect future turns. Though the implementation is\nsimple, we prove its optimality under a natural stochastic model of\nconversation dynamics, providing the first theoretical justification for LRU in\nthis setting, a result that may be of independent interest to the caching\ncommunity. Experimentally, on real conversation data WildChat, Tail-Optimized\nLRU achieves up to 27.5% reduction in P90 tail Time to First Token latency and\n23.9% in P95 tail latency compared to LRU, along with up to 38.9% decrease in\nSLO violations of 200ms. We believe this provides a practical and theoretically\ngrounded option for practitioners seeking to optimize tail latency in\nreal-world LLM deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt caching is critical for reducing latency and cost in LLM inference:\nOpenAI and Anthropic report up to 50-90% cost savings through prompt reuse.\nDespite its widespread success, little is known about what constitutes an\noptimal prompt caching policy, particularly when optimizing tail latency, a\nmetric of central importance to practitioners. The widely used Least Recently\nUsed (LRU) policy can perform arbitrarily poor on this metric, as it is\noblivious to the heterogeneity of conversation lengths. To address this gap, we\npropose Tail-Optimized LRU, a simple two-line modification that reallocates KV\ncache capacity to prioritize high-latency conversations by evicting cache\nentries that are unlikely to affect future turns. Though the implementation is\nsimple, we prove its optimality under a natural stochastic model of\nconversation dynamics, providing the first theoretical justification for LRU in\nthis setting, a result that may be of independent interest to the caching\ncommunity. Experimentally, on real conversation data WildChat, Tail-Optimized\nLRU achieves up to 27.5% reduction in P90 tail Time to First Token latency and\n23.9% in P95 tail latency compared to LRU, along with up to 38.9% decrease in\nSLO violations of 200ms. We believe this provides a practical and theoretically\ngrounded option for practitioners seeking to optimize tail latency in\nreal-world LLM deployments."
                },
                "authors": [
                    {
                        "name": "Wenxin Zhang"
                    },
                    {
                        "name": "Yueying Li"
                    },
                    {
                        "name": "Ciamac C. Moallemi"
                    },
                    {
                        "name": "Tianyi Peng"
                    }
                ],
                "author_detail": {
                    "name": "Tianyi Peng"
                },
                "author": "Tianyi Peng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.15152v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.15152v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.15095v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.15095v1",
                "updated": "2025-10-16T19:28:30Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    19,
                    28,
                    30,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T19:28:30Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    19,
                    28,
                    30,
                    3,
                    289,
                    0
                ],
                "title": "Hive Hash Table: A Warp-Cooperative, Dynamically Resizable Hash Table\n  for GPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hive Hash Table: A Warp-Cooperative, Dynamically Resizable Hash Table\n  for GPUs"
                },
                "summary": "Hash tables are essential building blocks in data-intensive applications, yet\nexisting GPU implementations often struggle with concurrent updates, high load\nfactors, and irregular memory access patterns. We present Hive hash table, a\nhigh-performance, warp-cooperative and dynamically resizable GPU hash table\nthat adapts to varying workloads without global rehashing.\n  Hive hash table makes three key contributions. First, a cache-aligned packed\nbucket layout stores key-value pairs as 64-bit words, enabling coalesced memory\naccess and atomic updates via single-CAS operations. Second, warp-synchronous\nconcurrency protocols - Warp-Aggregated-Bitmask-Claim (WABC) and\nWarp-Cooperative Match-and-Elect (WCME) - reduce contention to one atomic\noperation per warp while ensuring lock-free progress. Third, a\nload-factor-aware dynamic resizing strategy expands or contracts capacity in\nwarp-parallel K-bucket batches using linear hashing, maintaining balanced\noccupancy. To handle insertions under heavy contention, Hive hash table employs\na four-step strategy: replace, claim-and-commit, bounded cuckoo eviction, and\noverflow-stash fallback. This design provides lock-free fast paths and bounded\nrecovery cost under contention determined by a fixed eviction depth, while\neliminating ABA hazards during concurrent updates.\n  Experimental evaluation on an NVIDIA RTX 4090 shows Hive hash table sustains\nload factors up to 95% while delivering 1.5-2x higher throughput than\nstate-of-the-art GPU hash tables (Slab-Hash, DyCuckoo, WarpCore) under mixed\ninsert-delete-lookup workloads. On balanced workload, Hive hash table reaches\n3.5 billion updates/s and nearly 4 billion lookups/s, demonstrating scalability\nand efficiency for GPU-accelerated data processing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hash tables are essential building blocks in data-intensive applications, yet\nexisting GPU implementations often struggle with concurrent updates, high load\nfactors, and irregular memory access patterns. We present Hive hash table, a\nhigh-performance, warp-cooperative and dynamically resizable GPU hash table\nthat adapts to varying workloads without global rehashing.\n  Hive hash table makes three key contributions. First, a cache-aligned packed\nbucket layout stores key-value pairs as 64-bit words, enabling coalesced memory\naccess and atomic updates via single-CAS operations. Second, warp-synchronous\nconcurrency protocols - Warp-Aggregated-Bitmask-Claim (WABC) and\nWarp-Cooperative Match-and-Elect (WCME) - reduce contention to one atomic\noperation per warp while ensuring lock-free progress. Third, a\nload-factor-aware dynamic resizing strategy expands or contracts capacity in\nwarp-parallel K-bucket batches using linear hashing, maintaining balanced\noccupancy. To handle insertions under heavy contention, Hive hash table employs\na four-step strategy: replace, claim-and-commit, bounded cuckoo eviction, and\noverflow-stash fallback. This design provides lock-free fast paths and bounded\nrecovery cost under contention determined by a fixed eviction depth, while\neliminating ABA hazards during concurrent updates.\n  Experimental evaluation on an NVIDIA RTX 4090 shows Hive hash table sustains\nload factors up to 95% while delivering 1.5-2x higher throughput than\nstate-of-the-art GPU hash tables (Slab-Hash, DyCuckoo, WarpCore) under mixed\ninsert-delete-lookup workloads. On balanced workload, Hive hash table reaches\n3.5 billion updates/s and nearly 4 billion lookups/s, demonstrating scalability\nand efficiency for GPU-accelerated data processing."
                },
                "authors": [
                    {
                        "name": "Md Sabbir Hossain Polak"
                    },
                    {
                        "name": "David Troendle"
                    },
                    {
                        "name": "Byunghyun Jang"
                    }
                ],
                "author_detail": {
                    "name": "Byunghyun Jang"
                },
                "author": "Byunghyun Jang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.15095v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.15095v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14973v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14973v1",
                "updated": "2025-10-16T17:59:48Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    59,
                    48,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T17:59:48Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    59,
                    48,
                    3,
                    289,
                    0
                ],
                "title": "Attention Is All You Need for KV Cache in Diffusion LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention Is All You Need for KV Cache in Diffusion LLMs"
                },
                "summary": "This work studies how to adaptively recompute key-value (KV) caches for\ndiffusion large language models (DLMs) to maximize prediction accuracy while\nminimizing decoding latency. Prior methods' decoders recompute QKV for all\ntokens at every denoising step and layer, despite KV states changing little\nacross most steps, especially in shallow layers, leading to substantial\nredundancy. We make three observations: (1) distant ${\\bf MASK}$ tokens\nprimarily act as a length-bias and can be cached block-wise beyond the active\nprediction window; (2) KV dynamics increase with depth, suggesting that\nselective refresh starting from deeper layers is sufficient; and (3) the\nmost-attended token exhibits the smallest KV drift, providing a conservative\nlower bound on cache change for other tokens. Building on these, we propose\n${\\bf Elastic-Cache}$, a training-free, architecture-agnostic strategy that\njointly decides ${when}$ to refresh (via an attention-aware drift test on the\nmost-attended token) and ${where}$ to refresh (via a depth-aware schedule that\nrecomputes from a chosen layer onward while reusing shallow-layer caches and\noff-window MASK caches). Unlike fixed-period schemes, Elastic-Cache performs\nadaptive, layer-aware cache updates for diffusion LLMs, reducing redundant\ncomputation and accelerating decoding with negligible loss in generation\nquality. Experiments on LLaDA-Instruct, LLaDA-1.5, and LLaDA-V across\nmathematical reasoning and code generation tasks demonstrate consistent\nspeedups: $8.7\\times$ on GSM8K (256 tokens), $45.1\\times$ on longer sequences,\nand $4.8\\times$ on HumanEval, while consistently maintaining higher accuracy\nthan the baseline. Our method achieves significantly higher throughput\n($6.8\\times$ on GSM8K) than existing confidence-based approaches while\npreserving generation quality, enabling practical deployment of diffusion LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work studies how to adaptively recompute key-value (KV) caches for\ndiffusion large language models (DLMs) to maximize prediction accuracy while\nminimizing decoding latency. Prior methods' decoders recompute QKV for all\ntokens at every denoising step and layer, despite KV states changing little\nacross most steps, especially in shallow layers, leading to substantial\nredundancy. We make three observations: (1) distant ${\\bf MASK}$ tokens\nprimarily act as a length-bias and can be cached block-wise beyond the active\nprediction window; (2) KV dynamics increase with depth, suggesting that\nselective refresh starting from deeper layers is sufficient; and (3) the\nmost-attended token exhibits the smallest KV drift, providing a conservative\nlower bound on cache change for other tokens. Building on these, we propose\n${\\bf Elastic-Cache}$, a training-free, architecture-agnostic strategy that\njointly decides ${when}$ to refresh (via an attention-aware drift test on the\nmost-attended token) and ${where}$ to refresh (via a depth-aware schedule that\nrecomputes from a chosen layer onward while reusing shallow-layer caches and\noff-window MASK caches). Unlike fixed-period schemes, Elastic-Cache performs\nadaptive, layer-aware cache updates for diffusion LLMs, reducing redundant\ncomputation and accelerating decoding with negligible loss in generation\nquality. Experiments on LLaDA-Instruct, LLaDA-1.5, and LLaDA-V across\nmathematical reasoning and code generation tasks demonstrate consistent\nspeedups: $8.7\\times$ on GSM8K (256 tokens), $45.1\\times$ on longer sequences,\nand $4.8\\times$ on HumanEval, while consistently maintaining higher accuracy\nthan the baseline. Our method achieves significantly higher throughput\n($6.8\\times$ on GSM8K) than existing confidence-based approaches while\npreserving generation quality, enabling practical deployment of diffusion LLMs."
                },
                "authors": [
                    {
                        "name": "Quan Nguyen-Tri"
                    },
                    {
                        "name": "Mukul Ranjan"
                    },
                    {
                        "name": "Zhiqiang Shen"
                    }
                ],
                "author_detail": {
                    "name": "Zhiqiang Shen"
                },
                "author": "Zhiqiang Shen",
                "arxiv_comment": "https://vila-lab.github.io/elastic-cache-webpage/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14973v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14973v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14891v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14891v1",
                "updated": "2025-10-16T17:10:03Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    10,
                    3,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T17:10:03Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    10,
                    3,
                    3,
                    289,
                    0
                ],
                "title": "A Performance Portable Matrix Free Dense MTTKRP in GenTen",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Performance Portable Matrix Free Dense MTTKRP in GenTen"
                },
                "summary": "We extend the GenTen tensor decomposition package by introducing an\naccelerated dense matricized tensor times Khatri-Rao product (MTTKRP), the\nworkhorse kernel for canonical polyadic (CP) tensor decompositions, that is\nportable and performant on modern CPU and GPU architectures. In contrast to the\nstate-of-the-art matrix multiply based MTTKRP kernels used by Tensor Toolbox,\nTensorLy, etc., that explicitly form Khatri-Rao matrices, we develop a\nmatrix-free element-wise parallelization approach whose memory cost grows with\nthe rank R like the sum of the tensor shape O(R(n+m+k)), compared to\nmatrix-based methods whose memory cost grows like the product of the tensor\nshape O(R(mnk)). For the largest problem we study, a rank 2000 MTTKRP, the\nsmaller growth rate yields a matrix-free memory cost of just 2% of the\nmatrix-based methods, a 50x improvement. In practice, the reduced memory impact\nmeans our matrix-free MTTKRP can compute a rank 2000 tensor decomposition on a\nsingle NVIDIA H100 instead of six H100s using a matrix-based MTTKRP. We also\ncompare our optimized matrix-free MTTKRP to baseline matrix-free\nimplementations on different devices, showing a 3x single-device speedup on an\nIntel 8480+ CPU and an 11x speedup on a H100 GPU. In addition to numerical\nresults, we provide fine grained performance models for an ideal multi-level\ncache machine, compare analytical performance predictions to empirical results,\nand provide a motivated heuristic selection for selecting an algorithmic\nhyperparameter.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We extend the GenTen tensor decomposition package by introducing an\naccelerated dense matricized tensor times Khatri-Rao product (MTTKRP), the\nworkhorse kernel for canonical polyadic (CP) tensor decompositions, that is\nportable and performant on modern CPU and GPU architectures. In contrast to the\nstate-of-the-art matrix multiply based MTTKRP kernels used by Tensor Toolbox,\nTensorLy, etc., that explicitly form Khatri-Rao matrices, we develop a\nmatrix-free element-wise parallelization approach whose memory cost grows with\nthe rank R like the sum of the tensor shape O(R(n+m+k)), compared to\nmatrix-based methods whose memory cost grows like the product of the tensor\nshape O(R(mnk)). For the largest problem we study, a rank 2000 MTTKRP, the\nsmaller growth rate yields a matrix-free memory cost of just 2% of the\nmatrix-based methods, a 50x improvement. In practice, the reduced memory impact\nmeans our matrix-free MTTKRP can compute a rank 2000 tensor decomposition on a\nsingle NVIDIA H100 instead of six H100s using a matrix-based MTTKRP. We also\ncompare our optimized matrix-free MTTKRP to baseline matrix-free\nimplementations on different devices, showing a 3x single-device speedup on an\nIntel 8480+ CPU and an 11x speedup on a H100 GPU. In addition to numerical\nresults, we provide fine grained performance models for an ideal multi-level\ncache machine, compare analytical performance predictions to empirical results,\nand provide a motivated heuristic selection for selecting an algorithmic\nhyperparameter."
                },
                "authors": [
                    {
                        "name": "Gabriel Kosmacher"
                    },
                    {
                        "name": "Eric T. Phipps"
                    },
                    {
                        "name": "Sivasankaran Rajamanickam"
                    }
                ],
                "author_detail": {
                    "name": "Sivasankaran Rajamanickam"
                },
                "author": "Sivasankaran Rajamanickam",
                "arxiv_comment": "10 pages, 5 figures, 4 tables, for implementation see\n  https://github.com/sandialabs/GenTen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14891v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14891v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "G.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2510.25770v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25770v1",
                "updated": "2025-10-29T17:59:16Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    17,
                    59,
                    16,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T17:59:16Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    17,
                    59,
                    16,
                    2,
                    302,
                    0
                ],
                "title": "E-Scores for (In)Correctness Assessment of Generative Model Outputs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "E-Scores for (In)Correctness Assessment of Generative Model Outputs"
                },
                "summary": "While generative models, especially large language models (LLMs), are\nubiquitous in today's world, principled mechanisms to assess their\n(in)correctness are limited. Using the conformal prediction framework, previous\nworks construct sets of LLM responses where the probability of including an\nincorrect response, or error, is capped at a desired user-defined tolerance\nlevel. However, since these methods are based on p-values, they are susceptible\nto p-hacking, i.e., choosing the tolerance level post-hoc can invalidate the\nguarantees. We therefore leverage e-values to complement generative model\noutputs with e-scores as a measure of incorrectness. In addition to achieving\nthe same statistical guarantees as before, e-scores provide users flexibility\nin adaptively choosing tolerance levels after observing the e-scores\nthemselves, by upper bounding a post-hoc notion of error called size\ndistortion. We experimentally demonstrate their efficacy in assessing LLM\noutputs for different correctness types: mathematical factuality and property\nconstraints satisfaction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While generative models, especially large language models (LLMs), are\nubiquitous in today's world, principled mechanisms to assess their\n(in)correctness are limited. Using the conformal prediction framework, previous\nworks construct sets of LLM responses where the probability of including an\nincorrect response, or error, is capped at a desired user-defined tolerance\nlevel. However, since these methods are based on p-values, they are susceptible\nto p-hacking, i.e., choosing the tolerance level post-hoc can invalidate the\nguarantees. We therefore leverage e-values to complement generative model\noutputs with e-scores as a measure of incorrectness. In addition to achieving\nthe same statistical guarantees as before, e-scores provide users flexibility\nin adaptively choosing tolerance levels after observing the e-scores\nthemselves, by upper bounding a post-hoc notion of error called size\ndistortion. We experimentally demonstrate their efficacy in assessing LLM\noutputs for different correctness types: mathematical factuality and property\nconstraints satisfaction."
                },
                "authors": [
                    {
                        "name": "Guneet S. Dhillon"
                    },
                    {
                        "name": "Javier González"
                    },
                    {
                        "name": "Teodora Pandeva"
                    },
                    {
                        "name": "Alicia Curth"
                    }
                ],
                "author_detail": {
                    "name": "Alicia Curth"
                },
                "author": "Alicia Curth",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25770v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25770v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25769v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25769v1",
                "updated": "2025-10-29T17:59:06Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    17,
                    59,
                    6,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T17:59:06Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    17,
                    59,
                    6,
                    2,
                    302,
                    0
                ],
                "title": "Neural Stochastic Flows: Solver-Free Modelling and Inference for SDE\n  Solutions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Stochastic Flows: Solver-Free Modelling and Inference for SDE\n  Solutions"
                },
                "summary": "Stochastic differential equations (SDEs) are well suited to modelling noisy\nand irregularly sampled time series found in finance, physics, and machine\nlearning. Traditional approaches require costly numerical solvers to sample\nbetween arbitrary time points. We introduce Neural Stochastic Flows (NSFs) and\ntheir latent variants, which directly learn (latent) SDE transition laws using\nconditional normalising flows with architectural constraints that preserve\nproperties inherited from stochastic flows. This enables one-shot sampling\nbetween arbitrary states and yields up to two orders of magnitude speed-ups at\nlarge time gaps. Experiments on synthetic SDE simulations and on real-world\ntracking and video data show that NSFs maintain distributional accuracy\ncomparable to numerical approaches while dramatically reducing computation for\narbitrary time-point sampling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stochastic differential equations (SDEs) are well suited to modelling noisy\nand irregularly sampled time series found in finance, physics, and machine\nlearning. Traditional approaches require costly numerical solvers to sample\nbetween arbitrary time points. We introduce Neural Stochastic Flows (NSFs) and\ntheir latent variants, which directly learn (latent) SDE transition laws using\nconditional normalising flows with architectural constraints that preserve\nproperties inherited from stochastic flows. This enables one-shot sampling\nbetween arbitrary states and yields up to two orders of magnitude speed-ups at\nlarge time gaps. Experiments on synthetic SDE simulations and on real-world\ntracking and video data show that NSFs maintain distributional accuracy\ncomparable to numerical approaches while dramatically reducing computation for\narbitrary time-point sampling."
                },
                "authors": [
                    {
                        "name": "Naoki Kiyohara"
                    },
                    {
                        "name": "Edward Johns"
                    },
                    {
                        "name": "Yingzhen Li"
                    }
                ],
                "author_detail": {
                    "name": "Yingzhen Li"
                },
                "author": "Yingzhen Li",
                "arxiv_comment": "NeurIPS 2025 (poster). Project page:\n  https://nkiyohara.github.io/nsf-neurips2025/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25769v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25769v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25766v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25766v1",
                "updated": "2025-10-29T17:58:59Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    17,
                    58,
                    59,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T17:58:59Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    17,
                    58,
                    59,
                    2,
                    302,
                    0
                ],
                "title": "Decomposition-Enhanced Training for Post-Hoc Attributions In Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decomposition-Enhanced Training for Post-Hoc Attributions In Language\n  Models"
                },
                "summary": "Large language models (LLMs) are increasingly used for long-document question\nanswering, where reliable attribution to sources is critical for trust.\nExisting post-hoc attribution methods work well for extractive QA but struggle\nin multi-hop, abstractive, and semi-extractive settings, where answers\nsynthesize information across passages. To address these challenges, we argue\nthat post-hoc attribution can be reframed as a reasoning problem, where answers\nare decomposed into constituent units, each tied to specific context. We first\nshow that prompting models to generate such decompositions alongside\nattributions improves performance. Building on this, we introduce DecompTune, a\npost-training method that teaches models to produce answer decompositions as\nintermediate reasoning steps. We curate a diverse dataset of complex QA tasks,\nannotated with decompositions by a strong LLM, and post-train Qwen-2.5 (7B and\n14B) using a two-stage SFT + GRPO pipeline with task-specific curated rewards.\nAcross extensive experiments and ablations, DecompTune substantially improves\nattribution quality, outperforming prior methods and matching or exceeding\nstate-of-the-art frontier models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly used for long-document question\nanswering, where reliable attribution to sources is critical for trust.\nExisting post-hoc attribution methods work well for extractive QA but struggle\nin multi-hop, abstractive, and semi-extractive settings, where answers\nsynthesize information across passages. To address these challenges, we argue\nthat post-hoc attribution can be reframed as a reasoning problem, where answers\nare decomposed into constituent units, each tied to specific context. We first\nshow that prompting models to generate such decompositions alongside\nattributions improves performance. Building on this, we introduce DecompTune, a\npost-training method that teaches models to produce answer decompositions as\nintermediate reasoning steps. We curate a diverse dataset of complex QA tasks,\nannotated with decompositions by a strong LLM, and post-train Qwen-2.5 (7B and\n14B) using a two-stage SFT + GRPO pipeline with task-specific curated rewards.\nAcross extensive experiments and ablations, DecompTune substantially improves\nattribution quality, outperforming prior methods and matching or exceeding\nstate-of-the-art frontier models."
                },
                "authors": [
                    {
                        "name": "Sriram Balasubramaniam"
                    },
                    {
                        "name": "Samyadeep Basu"
                    },
                    {
                        "name": "Koustava Goswami"
                    },
                    {
                        "name": "Ryan Rossi"
                    },
                    {
                        "name": "Varun Manjunatha"
                    },
                    {
                        "name": "Roshan Santhosh"
                    },
                    {
                        "name": "Ruiyi Zhang"
                    },
                    {
                        "name": "Soheil Feizi"
                    },
                    {
                        "name": "Nedim Lipka"
                    }
                ],
                "author_detail": {
                    "name": "Nedim Lipka"
                },
                "author": "Nedim Lipka",
                "arxiv_comment": "Post-hoc attribution",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25766v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25766v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18905v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18905v2",
                "updated": "2025-10-29T17:57:23Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    17,
                    57,
                    23,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-21T01:03:46Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    1,
                    3,
                    46,
                    1,
                    294,
                    0
                ],
                "title": "3D Optimization for AI Inference Scaling: Balancing Accuracy, Cost, and\n  Latency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D Optimization for AI Inference Scaling: Balancing Accuracy, Cost, and\n  Latency"
                },
                "summary": "AI inference scaling is often tuned through 1D heuristics (a fixed reasoning\npasses) or 2D bivariate trade-offs (e.g., performance vs. compute), which fail\nto consider cost and latency constraints. We introduce a 3D optimization\nframework that jointly calibrates accuracy, cost, and latency within a unified\ndecision space, enabling constraints-aware inference scaling. Using Monte Carlo\nsimulations across three representative scenarios and nine simulated large\nlanguage models, we evaluate four optimization methods to address the 3D\nmulti-objective optimization (MOO) problem. Framing inference scaling in MOO\nshapes a feasible space that 1D and 2D optimizations fail to capture, enabling\nenvironmentadaptive selection of the inference scaling k. Results show that\nknee-point optimization achieves the best balance, while accuracy-maximization\nremains favorable when precision is prioritized. The framework establishes a\ntheoretical foundation for deployment-aware inference scaling across diverse\noperational contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI inference scaling is often tuned through 1D heuristics (a fixed reasoning\npasses) or 2D bivariate trade-offs (e.g., performance vs. compute), which fail\nto consider cost and latency constraints. We introduce a 3D optimization\nframework that jointly calibrates accuracy, cost, and latency within a unified\ndecision space, enabling constraints-aware inference scaling. Using Monte Carlo\nsimulations across three representative scenarios and nine simulated large\nlanguage models, we evaluate four optimization methods to address the 3D\nmulti-objective optimization (MOO) problem. Framing inference scaling in MOO\nshapes a feasible space that 1D and 2D optimizations fail to capture, enabling\nenvironmentadaptive selection of the inference scaling k. Results show that\nknee-point optimization achieves the best balance, while accuracy-maximization\nremains favorable when precision is prioritized. The framework establishes a\ntheoretical foundation for deployment-aware inference scaling across diverse\noperational contexts."
                },
                "authors": [
                    {
                        "name": "Minseok Jung"
                    },
                    {
                        "name": "Abhas Ricky"
                    },
                    {
                        "name": "Muhammad Rameez Chatni"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Rameez Chatni"
                },
                "author": "Muhammad Rameez Chatni",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18905v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18905v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01939v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01939v3",
                "updated": "2025-10-29T17:57:03Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    17,
                    57,
                    3,
                    2,
                    302,
                    0
                ],
                "published": "2025-07-02T17:49:52Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    17,
                    49,
                    52,
                    2,
                    183,
                    0
                ],
                "title": "SpecCLIP: Aligning and Translating Spectroscopic Measurements for Stars",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpecCLIP: Aligning and Translating Spectroscopic Measurements for Stars"
                },
                "summary": "In recent years, large language models (LLMs) have transformed natural\nlanguage understanding through vast datasets and large-scale parameterization.\nInspired by this success, we present SpecCLIP, a foundation model framework\nthat extends LLM-inspired methodologies to stellar spectral analysis. Stellar\nspectra, akin to structured language, encode rich physical and chemical\ninformation about stars. By training foundation models on large-scale spectral\ndatasets, our goal is to learn robust and informative embeddings that support\ndiverse downstream applications. As a proof of concept, SpecCLIP involves\npre-training on two spectral types--LAMOST low-resolution and Gaia XP--followed\nby contrastive alignment using the CLIP (Contrastive Language-Image\nPre-training) framework, adapted to associate spectra from different\ninstruments. This alignment is complemented by auxiliary decoders that preserve\nspectrum-specific information and enable translation (prediction) between\nspectral types, with the former achieved by maximizing mutual information\nbetween embeddings and input spectra. The result is a cross-spectrum framework\nenabling intrinsic calibration and flexible applications across instruments. We\ndemonstrate that fine-tuning these models on moderate-sized labeled datasets\nimproves adaptability to tasks such as stellar-parameter estimation and\nchemical-abundance determination. SpecCLIP also enhances the accuracy and\nprecision of parameter estimates benchmarked against external survey data.\nAdditionally, its similarity search and cross-spectrum prediction capabilities\noffer potential for anomaly detection. Our results suggest that contrastively\ntrained foundation models enriched with spectrum-aware decoders can advance\nprecision stellar spectroscopy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, large language models (LLMs) have transformed natural\nlanguage understanding through vast datasets and large-scale parameterization.\nInspired by this success, we present SpecCLIP, a foundation model framework\nthat extends LLM-inspired methodologies to stellar spectral analysis. Stellar\nspectra, akin to structured language, encode rich physical and chemical\ninformation about stars. By training foundation models on large-scale spectral\ndatasets, our goal is to learn robust and informative embeddings that support\ndiverse downstream applications. As a proof of concept, SpecCLIP involves\npre-training on two spectral types--LAMOST low-resolution and Gaia XP--followed\nby contrastive alignment using the CLIP (Contrastive Language-Image\nPre-training) framework, adapted to associate spectra from different\ninstruments. This alignment is complemented by auxiliary decoders that preserve\nspectrum-specific information and enable translation (prediction) between\nspectral types, with the former achieved by maximizing mutual information\nbetween embeddings and input spectra. The result is a cross-spectrum framework\nenabling intrinsic calibration and flexible applications across instruments. We\ndemonstrate that fine-tuning these models on moderate-sized labeled datasets\nimproves adaptability to tasks such as stellar-parameter estimation and\nchemical-abundance determination. SpecCLIP also enhances the accuracy and\nprecision of parameter estimates benchmarked against external survey data.\nAdditionally, its similarity search and cross-spectrum prediction capabilities\noffer potential for anomaly detection. Our results suggest that contrastively\ntrained foundation models enriched with spectrum-aware decoders can advance\nprecision stellar spectroscopy."
                },
                "authors": [
                    {
                        "name": "Xiaosheng Zhao"
                    },
                    {
                        "name": "Yang Huang"
                    },
                    {
                        "name": "Guirong Xue"
                    },
                    {
                        "name": "Xiao Kong"
                    },
                    {
                        "name": "Jifeng Liu"
                    },
                    {
                        "name": "Xiaoyu Tang"
                    },
                    {
                        "name": "Timothy C. Beers"
                    },
                    {
                        "name": "Yuan-Sen Ting"
                    },
                    {
                        "name": "A-Li Luo"
                    }
                ],
                "author_detail": {
                    "name": "A-Li Luo"
                },
                "author": "A-Li Luo",
                "arxiv_comment": "27 pages, 8 figures, 5 tables. Minor update: added corrected\n  acknowledgments and corrected a misstated hyperparameter value (noted in\n  footnote) for reproducibility. Submitted to AAS Journals. Comments welcome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01939v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01939v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25761v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25761v1",
                "updated": "2025-10-29T17:56:17Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    17,
                    56,
                    17,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T17:56:17Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    17,
                    56,
                    17,
                    2,
                    302,
                    0
                ],
                "title": "DiagramEval: Evaluating LLM-Generated Diagrams via Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiagramEval: Evaluating LLM-Generated Diagrams via Graphs"
                },
                "summary": "Diagrams play a central role in research papers for conveying ideas, yet they\nare often notoriously complex and labor-intensive to create. Although diagrams\nare presented as images, standard image generative models struggle to produce\nclear diagrams with well-defined structure. We argue that a promising direction\nis to generate demonstration diagrams directly in textual form as SVGs, which\ncan leverage recent advances in large language models (LLMs). However, due to\nthe complexity of components and the multimodal nature of diagrams,\nsufficiently discriminative and explainable metrics for evaluating the quality\nof LLM-generated diagrams remain lacking. In this paper, we propose\nDiagramEval, a novel evaluation metric designed to assess demonstration\ndiagrams generated by LLMs. Specifically, DiagramEval conceptualizes diagrams\nas graphs, treating text elements as nodes and their connections as directed\nedges, and evaluates diagram quality using two new groups of metrics: node\nalignment and path alignment. For the first time, we effectively evaluate\ndiagrams produced by state-of-the-art LLMs on recent research literature,\nquantitatively demonstrating the validity of our metrics. Furthermore, we show\nhow the enhanced explainability of our proposed metrics offers valuable\ninsights into the characteristics of LLM-generated diagrams. Code:\nhttps://github.com/ulab-uiuc/diagram-eval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diagrams play a central role in research papers for conveying ideas, yet they\nare often notoriously complex and labor-intensive to create. Although diagrams\nare presented as images, standard image generative models struggle to produce\nclear diagrams with well-defined structure. We argue that a promising direction\nis to generate demonstration diagrams directly in textual form as SVGs, which\ncan leverage recent advances in large language models (LLMs). However, due to\nthe complexity of components and the multimodal nature of diagrams,\nsufficiently discriminative and explainable metrics for evaluating the quality\nof LLM-generated diagrams remain lacking. In this paper, we propose\nDiagramEval, a novel evaluation metric designed to assess demonstration\ndiagrams generated by LLMs. Specifically, DiagramEval conceptualizes diagrams\nas graphs, treating text elements as nodes and their connections as directed\nedges, and evaluates diagram quality using two new groups of metrics: node\nalignment and path alignment. For the first time, we effectively evaluate\ndiagrams produced by state-of-the-art LLMs on recent research literature,\nquantitatively demonstrating the validity of our metrics. Furthermore, we show\nhow the enhanced explainability of our proposed metrics offers valuable\ninsights into the characteristics of LLM-generated diagrams. Code:\nhttps://github.com/ulab-uiuc/diagram-eval."
                },
                "authors": [
                    {
                        "name": "Chumeng Liang"
                    },
                    {
                        "name": "Jiaxuan You"
                    }
                ],
                "author_detail": {
                    "name": "Jiaxuan You"
                },
                "author": "Jiaxuan You",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25761v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25761v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25758v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25758v1",
                "updated": "2025-10-29T17:54:20Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    17,
                    54,
                    20,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T17:54:20Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    17,
                    54,
                    20,
                    2,
                    302,
                    0
                ],
                "title": "TheraMind: A Strategic and Adaptive Agent for Longitudinal Psychological\n  Counseling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TheraMind: A Strategic and Adaptive Agent for Longitudinal Psychological\n  Counseling"
                },
                "summary": "Large language models (LLMs) in psychological counseling have attracted\nincreasing attention. However, existing approaches often lack emotional\nunderstanding, adaptive strategies, and the use of therapeutic methods across\nmultiple sessions with long-term memory, leaving them far from real clinical\npractice. To address these critical gaps, we introduce TheraMind, a strategic\nand adaptive agent for longitudinal psychological counseling. The cornerstone\nof TheraMind is a novel dual-loop architecture that decouples the complex\ncounseling process into an Intra-Session Loop for tactical dialogue management\nand a Cross-Session Loop for strategic therapeutic planning. The Intra-Session\nLoop perceives the patient's emotional state to dynamically select response\nstrategies while leveraging cross-session memory to ensure continuity.\nCrucially, the Cross-Session Loop empowers the agent with long-term\nadaptability by evaluating the efficacy of the applied therapy after each\nsession and adjusting the method for subsequent interactions. We validate our\napproach in a high-fidelity simulation environment grounded in real clinical\ncases. Extensive evaluations show that TheraMind outperforms other methods,\nespecially on multi-session metrics like Coherence, Flexibility, and\nTherapeutic Attunement, validating the effectiveness of its dual-loop design in\nemulating strategic, adaptive, and longitudinal therapeutic behavior. The code\nis publicly available at https://0mwwm0.github.io/TheraMind/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) in psychological counseling have attracted\nincreasing attention. However, existing approaches often lack emotional\nunderstanding, adaptive strategies, and the use of therapeutic methods across\nmultiple sessions with long-term memory, leaving them far from real clinical\npractice. To address these critical gaps, we introduce TheraMind, a strategic\nand adaptive agent for longitudinal psychological counseling. The cornerstone\nof TheraMind is a novel dual-loop architecture that decouples the complex\ncounseling process into an Intra-Session Loop for tactical dialogue management\nand a Cross-Session Loop for strategic therapeutic planning. The Intra-Session\nLoop perceives the patient's emotional state to dynamically select response\nstrategies while leveraging cross-session memory to ensure continuity.\nCrucially, the Cross-Session Loop empowers the agent with long-term\nadaptability by evaluating the efficacy of the applied therapy after each\nsession and adjusting the method for subsequent interactions. We validate our\napproach in a high-fidelity simulation environment grounded in real clinical\ncases. Extensive evaluations show that TheraMind outperforms other methods,\nespecially on multi-session metrics like Coherence, Flexibility, and\nTherapeutic Attunement, validating the effectiveness of its dual-loop design in\nemulating strategic, adaptive, and longitudinal therapeutic behavior. The code\nis publicly available at https://0mwwm0.github.io/TheraMind/."
                },
                "authors": [
                    {
                        "name": "He Hu"
                    },
                    {
                        "name": "Yucheng Zhou"
                    },
                    {
                        "name": "Chiyuan Ma"
                    },
                    {
                        "name": "Qianning Wang"
                    },
                    {
                        "name": "Zheng Zhang"
                    },
                    {
                        "name": "Fei Ma"
                    },
                    {
                        "name": "Laizhong Cui"
                    },
                    {
                        "name": "Qi Tian"
                    }
                ],
                "author_detail": {
                    "name": "Qi Tian"
                },
                "author": "Qi Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25758v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25758v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25743v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25743v1",
                "updated": "2025-10-29T17:46:07Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    17,
                    46,
                    7,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T17:46:07Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    17,
                    46,
                    7,
                    2,
                    302,
                    0
                ],
                "title": "Agentic Economic Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic Economic Modeling"
                },
                "summary": "We introduce Agentic Economic Modeling (AEM), a framework that aligns\nsynthetic LLM choices with small-sample human evidence for reliable econometric\ninference. AEM first generates task-conditioned synthetic choices via LLMs,\nthen learns a bias-correction mapping from task features and raw LLM choices to\nhuman-aligned choices, upon which standard econometric estimators perform\ninference to recover demand elasticities and treatment effects.We validate AEM\nin two experiments. In a large scale conjoint study with millions of\nobservations, using only 10% of the original data to fit the correction model\nlowers the error of the demand-parameter estimates, while uncorrected LLM\nchoices even increase the errors. In a regional field experiment, a mixture\nmodel calibrated on 10% of geographic regions estimates an out-of-domain\ntreatment effect of -65\\pm10 bps, closely matching the full human experiment\n(-60\\pm8 bps).Under time-wise extrapolation, training with only day-one human\ndata yields -24 bps (95% CI: [-26, -22], p<1e-5),improving over the human-only\nday-one baseline (-17 bps, 95% CI: [-43, +9], p=0.2049).These results\ndemonstrate AEM's potential to improve RCT efficiency and establish a\nfoundation method for LLM-based counterfactual generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Agentic Economic Modeling (AEM), a framework that aligns\nsynthetic LLM choices with small-sample human evidence for reliable econometric\ninference. AEM first generates task-conditioned synthetic choices via LLMs,\nthen learns a bias-correction mapping from task features and raw LLM choices to\nhuman-aligned choices, upon which standard econometric estimators perform\ninference to recover demand elasticities and treatment effects.We validate AEM\nin two experiments. In a large scale conjoint study with millions of\nobservations, using only 10% of the original data to fit the correction model\nlowers the error of the demand-parameter estimates, while uncorrected LLM\nchoices even increase the errors. In a regional field experiment, a mixture\nmodel calibrated on 10% of geographic regions estimates an out-of-domain\ntreatment effect of -65\\pm10 bps, closely matching the full human experiment\n(-60\\pm8 bps).Under time-wise extrapolation, training with only day-one human\ndata yields -24 bps (95% CI: [-26, -22], p<1e-5),improving over the human-only\nday-one baseline (-17 bps, 95% CI: [-43, +9], p=0.2049).These results\ndemonstrate AEM's potential to improve RCT efficiency and establish a\nfoundation method for LLM-based counterfactual generation."
                },
                "authors": [
                    {
                        "name": "Bohan Zhang"
                    },
                    {
                        "name": "Jiaxuan Li"
                    },
                    {
                        "name": "Ali Hortaçsu"
                    },
                    {
                        "name": "Xiaoyang Ye"
                    },
                    {
                        "name": "Victor Chernozhukov"
                    },
                    {
                        "name": "Angelo Ni"
                    },
                    {
                        "name": "Edward Huang"
                    }
                ],
                "author_detail": {
                    "name": "Edward Huang"
                },
                "author": "Edward Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25743v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25743v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25741v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25741v1",
                "updated": "2025-10-29T17:45:42Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    17,
                    45,
                    42,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T17:45:42Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    17,
                    45,
                    42,
                    2,
                    302,
                    0
                ],
                "title": "Scaling Latent Reasoning via Looped Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Latent Reasoning via Looped Language Models"
                },
                "summary": "Modern LLMs are trained to \"think\" primarily via explicit text generation,\nsuch as chain-of-thought (CoT), which defers reasoning to post-training and\nunder-leverages pre-training data. We present and open-source Ouro, named after\nthe recursive Ouroboros, a family of pre-trained Looped Language Models\n(LoopLM) that instead build reasoning into the pre-training phase through (i)\niterative computation in latent space, (ii) an entropy-regularized objective\nfor learned depth allocation, and (iii) scaling to 7.7T tokens. Ouro 1.4B and\n2.6B models enjoy superior performance that match the results of up to 12B SOTA\nLLMs across a wide range of benchmarks. Through controlled experiments, we show\nthis advantage stems not from increased knowledge capacity, but from superior\nknowledge manipulation capabilities. We also show that LoopLM yields reasoning\ntraces more aligned with final outputs than explicit CoT. We hope our results\nshow the potential of LoopLM as a novel scaling direction in the reasoning era.\nOur model could be found in: http://ouro-llm.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern LLMs are trained to \"think\" primarily via explicit text generation,\nsuch as chain-of-thought (CoT), which defers reasoning to post-training and\nunder-leverages pre-training data. We present and open-source Ouro, named after\nthe recursive Ouroboros, a family of pre-trained Looped Language Models\n(LoopLM) that instead build reasoning into the pre-training phase through (i)\niterative computation in latent space, (ii) an entropy-regularized objective\nfor learned depth allocation, and (iii) scaling to 7.7T tokens. Ouro 1.4B and\n2.6B models enjoy superior performance that match the results of up to 12B SOTA\nLLMs across a wide range of benchmarks. Through controlled experiments, we show\nthis advantage stems not from increased knowledge capacity, but from superior\nknowledge manipulation capabilities. We also show that LoopLM yields reasoning\ntraces more aligned with final outputs than explicit CoT. We hope our results\nshow the potential of LoopLM as a novel scaling direction in the reasoning era.\nOur model could be found in: http://ouro-llm.github.io."
                },
                "authors": [
                    {
                        "name": "Rui-Jie Zhu"
                    },
                    {
                        "name": "Zixuan Wang"
                    },
                    {
                        "name": "Kai Hua"
                    },
                    {
                        "name": "Tianyu Zhang"
                    },
                    {
                        "name": "Ziniu Li"
                    },
                    {
                        "name": "Haoran Que"
                    },
                    {
                        "name": "Boyi Wei"
                    },
                    {
                        "name": "Zixin Wen"
                    },
                    {
                        "name": "Fan Yin"
                    },
                    {
                        "name": "He Xing"
                    },
                    {
                        "name": "Lu Li"
                    },
                    {
                        "name": "Jiajun Shi"
                    },
                    {
                        "name": "Kaijing Ma"
                    },
                    {
                        "name": "Shanda Li"
                    },
                    {
                        "name": "Taylor Kergan"
                    },
                    {
                        "name": "Andrew Smith"
                    },
                    {
                        "name": "Xingwei Qu"
                    },
                    {
                        "name": "Mude Hui"
                    },
                    {
                        "name": "Bohong Wu"
                    },
                    {
                        "name": "Qiyang Min"
                    },
                    {
                        "name": "Hongzhi Huang"
                    },
                    {
                        "name": "Xun Zhou"
                    },
                    {
                        "name": "Wei Ye"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Jian Yang"
                    },
                    {
                        "name": "Yunfeng Shi"
                    },
                    {
                        "name": "Chenghua Lin"
                    },
                    {
                        "name": "Enduo Zhao"
                    },
                    {
                        "name": "Tianle Cai"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Wenhao Huang"
                    },
                    {
                        "name": "Yoshua Bengio"
                    },
                    {
                        "name": "Jason Eshraghian"
                    }
                ],
                "author_detail": {
                    "name": "Jason Eshraghian"
                },
                "author": "Jason Eshraghian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25741v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25741v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25739v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25739v1",
                "updated": "2025-10-29T17:43:31Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    17,
                    43,
                    31,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T17:43:31Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    17,
                    43,
                    31,
                    2,
                    302,
                    0
                ],
                "title": "Hawk: Leveraging Spatial Context for Faster Autoregressive Text-to-Image\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hawk: Leveraging Spatial Context for Faster Autoregressive Text-to-Image\n  Generation"
                },
                "summary": "Autoregressive (AR) image generation models are capable of producing\nhigh-fidelity images but often suffer from slow inference due to their\ninherently sequential, token-by-token decoding process. Speculative decoding,\nwhich employs a lightweight draft model to approximate the output of a larger\nAR model, has shown promise in accelerating text generation without\ncompromising quality. However, its application to image generation remains\nlargely underexplored. The challenges stem from a significantly larger sampling\nspace, which complicates the alignment between the draft and target model\noutputs, coupled with the inadequate use of the two-dimensional spatial\nstructure inherent in images, thereby limiting the modeling of local\ndependencies. To overcome these challenges, we introduce Hawk, a new approach\nthat harnesses the spatial structure of images to guide the speculative model\ntoward more accurate and efficient predictions. Experimental results on\nmultiple text-to-image benchmarks demonstrate a 1.71x speedup over standard AR\nmodels, while preserving both image fidelity and diversity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive (AR) image generation models are capable of producing\nhigh-fidelity images but often suffer from slow inference due to their\ninherently sequential, token-by-token decoding process. Speculative decoding,\nwhich employs a lightweight draft model to approximate the output of a larger\nAR model, has shown promise in accelerating text generation without\ncompromising quality. However, its application to image generation remains\nlargely underexplored. The challenges stem from a significantly larger sampling\nspace, which complicates the alignment between the draft and target model\noutputs, coupled with the inadequate use of the two-dimensional spatial\nstructure inherent in images, thereby limiting the modeling of local\ndependencies. To overcome these challenges, we introduce Hawk, a new approach\nthat harnesses the spatial structure of images to guide the speculative model\ntoward more accurate and efficient predictions. Experimental results on\nmultiple text-to-image benchmarks demonstrate a 1.71x speedup over standard AR\nmodels, while preserving both image fidelity and diversity."
                },
                "authors": [
                    {
                        "name": "Zhi-Kai Chen"
                    },
                    {
                        "name": "Jun-Peng Jiang"
                    },
                    {
                        "name": "Han-Jia Ye"
                    },
                    {
                        "name": "De-Chuan Zhan"
                    }
                ],
                "author_detail": {
                    "name": "De-Chuan Zhan"
                },
                "author": "De-Chuan Zhan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25739v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25739v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25732v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25732v1",
                "updated": "2025-10-29T17:37:50Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    17,
                    37,
                    50,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T17:37:50Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    17,
                    37,
                    50,
                    2,
                    302,
                    0
                ],
                "title": "The Limits of Obliviate: Evaluating Unlearning in LLMs via\n  Stimulus-Knowledge Entanglement-Behavior Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Limits of Obliviate: Evaluating Unlearning in LLMs via\n  Stimulus-Knowledge Entanglement-Behavior Framework"
                },
                "summary": "Unlearning in large language models (LLMs) is crucial for managing sensitive\ndata and correcting misinformation, yet evaluating its effectiveness remains an\nopen problem. We investigate whether persuasive prompting can recall factual\nknowledge from deliberately unlearned LLMs across models ranging from 2.7B to\n13B parameters (OPT-2.7B, LLaMA-2-7B, LLaMA-3.1-8B, LLaMA-2-13B). Drawing from\nACT-R and Hebbian theory (spreading activation theories), as well as\ncommunication principles, we introduce Stimulus-Knowledge Entanglement-Behavior\nFramework (SKeB), which models information entanglement via domain graphs and\ntests whether factual recall in unlearned models is correlated with persuasive\nframing. We develop entanglement metrics to quantify knowledge activation\npatterns and evaluate factuality, non-factuality, and hallucination in outputs.\nOur results show persuasive prompts substantially enhance factual knowledge\nrecall (14.8% baseline vs. 24.5% with authority framing), with effectiveness\ninversely correlated to model size (128% recovery in 2.7B vs. 15% in 13B). SKeB\nprovides a foundation for assessing unlearning completeness, robustness, and\noverall behavior in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlearning in large language models (LLMs) is crucial for managing sensitive\ndata and correcting misinformation, yet evaluating its effectiveness remains an\nopen problem. We investigate whether persuasive prompting can recall factual\nknowledge from deliberately unlearned LLMs across models ranging from 2.7B to\n13B parameters (OPT-2.7B, LLaMA-2-7B, LLaMA-3.1-8B, LLaMA-2-13B). Drawing from\nACT-R and Hebbian theory (spreading activation theories), as well as\ncommunication principles, we introduce Stimulus-Knowledge Entanglement-Behavior\nFramework (SKeB), which models information entanglement via domain graphs and\ntests whether factual recall in unlearned models is correlated with persuasive\nframing. We develop entanglement metrics to quantify knowledge activation\npatterns and evaluate factuality, non-factuality, and hallucination in outputs.\nOur results show persuasive prompts substantially enhance factual knowledge\nrecall (14.8% baseline vs. 24.5% with authority framing), with effectiveness\ninversely correlated to model size (128% recovery in 2.7B vs. 15% in 13B). SKeB\nprovides a foundation for assessing unlearning completeness, robustness, and\noverall behavior in LLMs."
                },
                "authors": [
                    {
                        "name": "Aakriti Shah"
                    },
                    {
                        "name": "Thai Le"
                    }
                ],
                "author_detail": {
                    "name": "Thai Le"
                },
                "author": "Thai Le",
                "arxiv_comment": "14 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25732v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25732v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.6; I.2.4; G.2.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25724v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25724v1",
                "updated": "2025-10-29T17:31:27Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    17,
                    31,
                    27,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T17:31:27Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    17,
                    31,
                    27,
                    2,
                    302,
                    0
                ],
                "title": "BambooKG: A Neurobiologically-inspired Frequency-Weight Knowledge Graph",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BambooKG: A Neurobiologically-inspired Frequency-Weight Knowledge Graph"
                },
                "summary": "Retrieval-Augmented Generation allows LLMs to access external knowledge,\nreducing hallucinations and ageing-data issues. However, it treats retrieved\nchunks independently and struggles with multi-hop or relational reasoning,\nespecially across documents. Knowledge graphs enhance this by capturing the\nrelationships between entities using triplets, enabling structured, multi-chunk\nreasoning. However, these tend to miss information that fails to conform to the\ntriplet structure. We introduce BambooKG, a knowledge graph with\nfrequency-based weights on non-triplet edges which reflect link strength,\ndrawing on the Hebbian principle of \"fire together, wire together\". This\ndecreases information loss and results in improved performance on single- and\nmulti-hop reasoning, outperforming the existing solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation allows LLMs to access external knowledge,\nreducing hallucinations and ageing-data issues. However, it treats retrieved\nchunks independently and struggles with multi-hop or relational reasoning,\nespecially across documents. Knowledge graphs enhance this by capturing the\nrelationships between entities using triplets, enabling structured, multi-chunk\nreasoning. However, these tend to miss information that fails to conform to the\ntriplet structure. We introduce BambooKG, a knowledge graph with\nfrequency-based weights on non-triplet edges which reflect link strength,\ndrawing on the Hebbian principle of \"fire together, wire together\". This\ndecreases information loss and results in improved performance on single- and\nmulti-hop reasoning, outperforming the existing solutions."
                },
                "authors": [
                    {
                        "name": "Vanya Arikutharam"
                    },
                    {
                        "name": "Arkadiy Ukolov"
                    }
                ],
                "author_detail": {
                    "name": "Arkadiy Ukolov"
                },
                "author": "Arkadiy Ukolov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25724v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25724v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09888v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09888v3",
                "updated": "2025-10-29T17:29:51Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    17,
                    29,
                    51,
                    2,
                    302,
                    0
                ],
                "published": "2024-12-13T06:06:33Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    6,
                    6,
                    33,
                    4,
                    348,
                    0
                ],
                "title": "gwsnr: A python package for efficient signal-to-noise calculation of\n  gravitational-waves",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "gwsnr: A python package for efficient signal-to-noise calculation of\n  gravitational-waves"
                },
                "summary": "Gravitational wave astrophysics requires accurate evaluation of the\nSignal-to-Noise Ratio (SNR) and the Probability of Detection (Pdet) for\napplications such as population simulations and hierarchical Bayesian inference\nwith selection effects. Traditional approaches for computing SNR are often\ncomputationally demanding and inefficient for large-scale analyses. The gwsnr\nPython package addresses this challenge by providing efficient and flexible\ntools for calculating the optimal SNR. It features a user-friendly interface\nand employs techniques such as a partial-scaling interpolation method for\nnon-precessing binaries, a multiprocessing inner-product routine for\nfrequency-domain waveforms that include spin precession and subdominant modes,\namong others. High computational performance is achieved through NumPy\nvectorization and Just-in-Time compilation with Numba, with optional GPU\nacceleration using JAX and MLX. By combining efficiency, scalability, and ease\nof use, gwsnr enables large-scale simulations of detectable compact binary\nmergers and facilitates robust modeling of selection effects through Pdet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gravitational wave astrophysics requires accurate evaluation of the\nSignal-to-Noise Ratio (SNR) and the Probability of Detection (Pdet) for\napplications such as population simulations and hierarchical Bayesian inference\nwith selection effects. Traditional approaches for computing SNR are often\ncomputationally demanding and inefficient for large-scale analyses. The gwsnr\nPython package addresses this challenge by providing efficient and flexible\ntools for calculating the optimal SNR. It features a user-friendly interface\nand employs techniques such as a partial-scaling interpolation method for\nnon-precessing binaries, a multiprocessing inner-product routine for\nfrequency-domain waveforms that include spin precession and subdominant modes,\namong others. High computational performance is achieved through NumPy\nvectorization and Just-in-Time compilation with Numba, with optional GPU\nacceleration using JAX and MLX. By combining efficiency, scalability, and ease\nof use, gwsnr enables large-scale simulations of detectable compact binary\nmergers and facilitates robust modeling of selection effects through Pdet."
                },
                "authors": [
                    {
                        "name": "Hemantakumar Phurailatpam"
                    },
                    {
                        "name": "Otto Akseli Hannuksela"
                    }
                ],
                "author_detail": {
                    "name": "Otto Akseli Hannuksela"
                },
                "author": "Otto Akseli Hannuksela",
                "arxiv_comment": "6 pages, 0 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09888v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09888v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23722v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23722v2",
                "updated": "2025-10-29T17:27:45Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    17,
                    27,
                    45,
                    2,
                    302,
                    0
                ],
                "published": "2025-05-29T17:54:32Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    54,
                    32,
                    3,
                    149,
                    0
                ],
                "title": "LLMs are Better Than You Think: Label-Guided In-Context Learning for\n  Named Entity Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs are Better Than You Think: Label-Guided In-Context Learning for\n  Named Entity Recognition"
                },
                "summary": "In-context learning (ICL) enables large language models (LLMs) to perform new\ntasks using only a few demonstrations. However, in Named Entity Recognition\n(NER), existing ICL methods typically rely on task-agnostic semantic similarity\nfor demonstration retrieval, which often yields less relevant examples and\nleads to inferior results. We introduce DEER, a training-free ICL approach that\nenables LLMs to make more informed entity predictions through the use of\nlabel-grounded statistics. DEER leverages token-level statistics from training\nlabels to identify tokens most informative for entity recognition, enabling\nentity-focused demonstrations. It further uses these statistics to detect and\nrefine error-prone tokens through a targeted reflection step. Evaluated on five\nNER datasets across four LLMs, DEER consistently outperforms existing ICL\nmethods and achieves performance comparable to supervised fine-tuning. Further\nanalyses demonstrate that DEER improves example retrieval, remains effective on\nboth seen and unseen entities, and exhibits strong robustness in low-resource\nsettings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) enables large language models (LLMs) to perform new\ntasks using only a few demonstrations. However, in Named Entity Recognition\n(NER), existing ICL methods typically rely on task-agnostic semantic similarity\nfor demonstration retrieval, which often yields less relevant examples and\nleads to inferior results. We introduce DEER, a training-free ICL approach that\nenables LLMs to make more informed entity predictions through the use of\nlabel-grounded statistics. DEER leverages token-level statistics from training\nlabels to identify tokens most informative for entity recognition, enabling\nentity-focused demonstrations. It further uses these statistics to detect and\nrefine error-prone tokens through a targeted reflection step. Evaluated on five\nNER datasets across four LLMs, DEER consistently outperforms existing ICL\nmethods and achieves performance comparable to supervised fine-tuning. Further\nanalyses demonstrate that DEER improves example retrieval, remains effective on\nboth seen and unseen entities, and exhibits strong robustness in low-resource\nsettings."
                },
                "authors": [
                    {
                        "name": "Fan Bai"
                    },
                    {
                        "name": "Hamid Hassanzadeh"
                    },
                    {
                        "name": "Ardavan Saeedi"
                    },
                    {
                        "name": "Mark Dredze"
                    }
                ],
                "author_detail": {
                    "name": "Mark Dredze"
                },
                "author": "Mark Dredze",
                "arxiv_comment": "Accepted to EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23722v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23722v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25706v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25706v1",
                "updated": "2025-10-29T17:17:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    17,
                    17,
                    14,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T17:17:14Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    17,
                    17,
                    14,
                    2,
                    302,
                    0
                ],
                "title": "Cosmological Constraints from Dark Energy Survey Year 1 Cluster Lensing\n  and Abundances with Simulation-based Forward-Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cosmological Constraints from Dark Energy Survey Year 1 Cluster Lensing\n  and Abundances with Simulation-based Forward-Modeling"
                },
                "summary": "We present a simulation-based forward-modeling framework for cosmological\ninference from optical galaxy-cluster samples, and apply it to the abundance\nand weak-lensing signals of DES-Y1 redMaPPer clusters. The model embeds\ncosmology-dependent optical selection using a counts-in-cylinders approach,\nwhile also accounting for cluster miscentering and baryonic feedback in\nlensing. Applied to DES-Y1, and assuming a flat $\\Lambda$CDM cosmology, we\nobtain $\\Omega_m=0.254^{+0.026}_{-0.020}$ and\n$\\sigma_8=0.826^{+0.030}_{-0.034}$, consistent with a broad suite of\nlow-redshift structure measurements, including recent full-shape analyses, the\nDES/KiDS/HSC 3$\\times$2 results, and most cluster-abundance studies. Our\nresults are also consistent with \\textit{Planck}, with the difference being\nsignificant at $2.58\\sigma$. These results establish simulation-based\nforward-modeling of cluster abundances as a promising new tool for precision\ncosmology with Stage~IV survey data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a simulation-based forward-modeling framework for cosmological\ninference from optical galaxy-cluster samples, and apply it to the abundance\nand weak-lensing signals of DES-Y1 redMaPPer clusters. The model embeds\ncosmology-dependent optical selection using a counts-in-cylinders approach,\nwhile also accounting for cluster miscentering and baryonic feedback in\nlensing. Applied to DES-Y1, and assuming a flat $\\Lambda$CDM cosmology, we\nobtain $\\Omega_m=0.254^{+0.026}_{-0.020}$ and\n$\\sigma_8=0.826^{+0.030}_{-0.034}$, consistent with a broad suite of\nlow-redshift structure measurements, including recent full-shape analyses, the\nDES/KiDS/HSC 3$\\times$2 results, and most cluster-abundance studies. Our\nresults are also consistent with \\textit{Planck}, with the difference being\nsignificant at $2.58\\sigma$. These results establish simulation-based\nforward-modeling of cluster abundances as a promising new tool for precision\ncosmology with Stage~IV survey data."
                },
                "authors": [
                    {
                        "name": "Andrés N. Salcedo"
                    },
                    {
                        "name": "Eduardo Rozo"
                    },
                    {
                        "name": "Hao-Yi Wu"
                    },
                    {
                        "name": "David H. Weinberg"
                    },
                    {
                        "name": "Pranav Chiploonkar"
                    },
                    {
                        "name": "Chun-Hao To"
                    },
                    {
                        "name": "Shulei Cao"
                    },
                    {
                        "name": "Eli S. Rykoff"
                    },
                    {
                        "name": "Nicole Marcelina Gountanis"
                    },
                    {
                        "name": "Conghao Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Conghao Zhou"
                },
                "author": "Conghao Zhou",
                "arxiv_comment": "21 pages, 8 main text figures and 1 table, 2 appendix figures and 1\n  table, submitted to PRD",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25706v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25706v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17720v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17720v4",
                "updated": "2025-10-29T17:15:43Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    17,
                    15,
                    43,
                    2,
                    302,
                    0
                ],
                "published": "2025-02-24T23:23:27Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    23,
                    23,
                    27,
                    0,
                    55,
                    0
                ],
                "title": "Spontaneous Giving and Calculated Greed in Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spontaneous Giving and Calculated Greed in Language Models"
                },
                "summary": "Large language models demonstrate strong problem-solving abilities through\nreasoning techniques such as chain-of-thought prompting and reflection.\nHowever, it remains unclear whether these reasoning capabilities extend to a\nform of social intelligence: making effective decisions in cooperative\ncontexts. We examine this question using economic games that simulate social\ndilemmas. First, we apply chain-of-thought and reflection prompting to GPT-4o\nin a Public Goods Game. We then evaluate multiple off-the-shelf models across\nsix cooperation and punishment games, comparing those with and without explicit\nreasoning mechanisms. We find that reasoning models consistently reduce\ncooperation and norm enforcement, favoring individual rationality. In repeated\ninteractions, groups with more reasoning agents exhibit lower collective gains.\nThese behaviors mirror human patterns of \"spontaneous giving and calculated\ngreed.\" Our findings underscore the need for LLM architectures that incorporate\nsocial intelligence alongside reasoning, to help address--rather than\nreinforce--the challenges of collective action.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models demonstrate strong problem-solving abilities through\nreasoning techniques such as chain-of-thought prompting and reflection.\nHowever, it remains unclear whether these reasoning capabilities extend to a\nform of social intelligence: making effective decisions in cooperative\ncontexts. We examine this question using economic games that simulate social\ndilemmas. First, we apply chain-of-thought and reflection prompting to GPT-4o\nin a Public Goods Game. We then evaluate multiple off-the-shelf models across\nsix cooperation and punishment games, comparing those with and without explicit\nreasoning mechanisms. We find that reasoning models consistently reduce\ncooperation and norm enforcement, favoring individual rationality. In repeated\ninteractions, groups with more reasoning agents exhibit lower collective gains.\nThese behaviors mirror human patterns of \"spontaneous giving and calculated\ngreed.\" Our findings underscore the need for LLM architectures that incorporate\nsocial intelligence alongside reasoning, to help address--rather than\nreinforce--the challenges of collective action."
                },
                "authors": [
                    {
                        "name": "Yuxuan Li"
                    },
                    {
                        "name": "Hirokazu Shirado"
                    }
                ],
                "author_detail": {
                    "name": "Hirokazu Shirado"
                },
                "author": "Hirokazu Shirado",
                "arxiv_comment": "Accepted to EMNLP 2025 main conference and selected as an Oral\n  Presentation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17720v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17720v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15584v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15584v3",
                "updated": "2025-10-29T17:10:45Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    17,
                    10,
                    45,
                    2,
                    302,
                    0
                ],
                "published": "2024-12-20T05:40:32Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    5,
                    40,
                    32,
                    4,
                    355,
                    0
                ],
                "title": "To Rely or Not to Rely? Evaluating Interventions for Appropriate\n  Reliance on Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To Rely or Not to Rely? Evaluating Interventions for Appropriate\n  Reliance on Large Language Models"
                },
                "summary": "As Large Language Models become integral to decision-making, optimism about\ntheir power is tempered with concern over their errors. Users may over-rely on\nLLM advice that is confidently stated but wrong, or under-rely due to mistrust.\nReliance interventions have been developed to help users of LLMs, but they lack\nrigorous evaluation for appropriate reliance. We benchmark the performance of\nthree relevant interventions by conducting a randomized online experiment with\n400 participants attempting two challenging tasks: LSAT logical reasoning and\nimage-based numerical estimation. For each question, participants first\nanswered independently, then received LLM advice modified by one of three\nreliance interventions and answered the question again. Our findings indicate\nthat while interventions reduce over-reliance, they generally fail to improve\nappropriate reliance. Furthermore, people became more confident after making\nwrong reliance decisions in certain contexts, demonstrating poor calibration.\nBased on our findings, we discuss implications for designing effective reliance\ninterventions in human-LLM collaboration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models become integral to decision-making, optimism about\ntheir power is tempered with concern over their errors. Users may over-rely on\nLLM advice that is confidently stated but wrong, or under-rely due to mistrust.\nReliance interventions have been developed to help users of LLMs, but they lack\nrigorous evaluation for appropriate reliance. We benchmark the performance of\nthree relevant interventions by conducting a randomized online experiment with\n400 participants attempting two challenging tasks: LSAT logical reasoning and\nimage-based numerical estimation. For each question, participants first\nanswered independently, then received LLM advice modified by one of three\nreliance interventions and answered the question again. Our findings indicate\nthat while interventions reduce over-reliance, they generally fail to improve\nappropriate reliance. Furthermore, people became more confident after making\nwrong reliance decisions in certain contexts, demonstrating poor calibration.\nBased on our findings, we discuss implications for designing effective reliance\ninterventions in human-LLM collaboration."
                },
                "authors": [
                    {
                        "name": "Jessica Y. Bo"
                    },
                    {
                        "name": "Sophia Wan"
                    },
                    {
                        "name": "Ashton Anderson"
                    }
                ],
                "author_detail": {
                    "name": "Ashton Anderson"
                },
                "author": "Ashton Anderson",
                "arxiv_journal_ref": "Proceedings of the 2025 CHI Conference on Human Factors in\n  Computing Systems",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15584v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15584v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22586v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22586v2",
                "updated": "2025-10-29T17:09:56Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    17,
                    9,
                    56,
                    2,
                    302,
                    0
                ],
                "published": "2025-05-28T16:58:23Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    16,
                    58,
                    23,
                    2,
                    148,
                    0
                ],
                "title": "Precise In-Parameter Concept Erasure in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Precise In-Parameter Concept Erasure in Large Language Models"
                },
                "summary": "Large language models (LLMs) often acquire knowledge during pretraining that\nis undesirable in downstream deployments, e.g., sensitive information or\ncopyrighted content. Existing approaches for removing such knowledge rely on\nfine-tuning, training low-rank adapters or fact-level editing, but these are\neither too coarse, too shallow, or ineffective. In this work, we propose PISCES\n(Precise In-parameter Suppression for Concept EraSure), a novel framework for\nprecisely erasing entire concepts from model parameters by directly editing\ndirections that encode them in parameter space. PISCES uses a disentangler\nmodel to decompose MLP vectors into interpretable features, identifies those\nassociated with a target concept using automated interpretability techniques,\nand removes them from model parameters. Experiments on Gemma 2 and Llama 3.1\nover various concepts show that PISCES achieves modest gains in efficacy over\nleading erasure methods, reducing accuracy on the target concept to as low as\n7.7%, while dramatically improving erasure specificity (by up to 31%) and\nrobustness (by up to 38%). Overall, these results demonstrate that\nfeature-based in-parameter editing enables a more precise and reliable approach\nfor removing conceptual knowledge in language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) often acquire knowledge during pretraining that\nis undesirable in downstream deployments, e.g., sensitive information or\ncopyrighted content. Existing approaches for removing such knowledge rely on\nfine-tuning, training low-rank adapters or fact-level editing, but these are\neither too coarse, too shallow, or ineffective. In this work, we propose PISCES\n(Precise In-parameter Suppression for Concept EraSure), a novel framework for\nprecisely erasing entire concepts from model parameters by directly editing\ndirections that encode them in parameter space. PISCES uses a disentangler\nmodel to decompose MLP vectors into interpretable features, identifies those\nassociated with a target concept using automated interpretability techniques,\nand removes them from model parameters. Experiments on Gemma 2 and Llama 3.1\nover various concepts show that PISCES achieves modest gains in efficacy over\nleading erasure methods, reducing accuracy on the target concept to as low as\n7.7%, while dramatically improving erasure specificity (by up to 31%) and\nrobustness (by up to 38%). Overall, these results demonstrate that\nfeature-based in-parameter editing enables a more precise and reliable approach\nfor removing conceptual knowledge in language models."
                },
                "authors": [
                    {
                        "name": "Yoav Gur-Arieh"
                    },
                    {
                        "name": "Clara Suslik"
                    },
                    {
                        "name": "Yihuai Hong"
                    },
                    {
                        "name": "Fazl Barez"
                    },
                    {
                        "name": "Mor Geva"
                    }
                ],
                "author_detail": {
                    "name": "Mor Geva"
                },
                "author": "Mor Geva",
                "arxiv_comment": "Accepted to EMNLP 2025 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22586v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22586v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25701v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25701v1",
                "updated": "2025-10-29T17:05:00Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    17,
                    5,
                    0,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T17:05:00Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    17,
                    5,
                    0,
                    2,
                    302,
                    0
                ],
                "title": "Interpreting LLMs as Credit Risk Classifiers: Do Their Feature\n  Explanations Align with Classical ML?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interpreting LLMs as Credit Risk Classifiers: Do Their Feature\n  Explanations Align with Classical ML?"
                },
                "summary": "Large Language Models (LLMs) are increasingly explored as flexible\nalternatives to classical machine learning models for classification tasks\nthrough zero-shot prompting. However, their suitability for structured tabular\ndata remains underexplored, especially in high-stakes financial applications\nsuch as financial risk assessment. This study conducts a systematic comparison\nbetween zero-shot LLM-based classifiers and LightGBM, a state-of-the-art\ngradient-boosting model, on a real-world loan default prediction task. We\nevaluate their predictive performance, analyze feature attributions using SHAP,\nand assess the reliability of LLM-generated self-explanations. While LLMs are\nable to identify key financial risk indicators, their feature importance\nrankings diverge notably from LightGBM, and their self-explanations often fail\nto align with empirical SHAP attributions. These findings highlight the\nlimitations of LLMs as standalone models for structured financial risk\nprediction and raise concerns about the trustworthiness of their self-generated\nexplanations. Our results underscore the need for explainability audits,\nbaseline comparisons with interpretable models, and human-in-the-loop oversight\nwhen deploying LLMs in risk-sensitive financial environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly explored as flexible\nalternatives to classical machine learning models for classification tasks\nthrough zero-shot prompting. However, their suitability for structured tabular\ndata remains underexplored, especially in high-stakes financial applications\nsuch as financial risk assessment. This study conducts a systematic comparison\nbetween zero-shot LLM-based classifiers and LightGBM, a state-of-the-art\ngradient-boosting model, on a real-world loan default prediction task. We\nevaluate their predictive performance, analyze feature attributions using SHAP,\nand assess the reliability of LLM-generated self-explanations. While LLMs are\nable to identify key financial risk indicators, their feature importance\nrankings diverge notably from LightGBM, and their self-explanations often fail\nto align with empirical SHAP attributions. These findings highlight the\nlimitations of LLMs as standalone models for structured financial risk\nprediction and raise concerns about the trustworthiness of their self-generated\nexplanations. Our results underscore the need for explainability audits,\nbaseline comparisons with interpretable models, and human-in-the-loop oversight\nwhen deploying LLMs in risk-sensitive financial environments."
                },
                "authors": [
                    {
                        "name": "Saeed AlMarri"
                    },
                    {
                        "name": "Kristof Juhasz"
                    },
                    {
                        "name": "Mathieu Ravaut"
                    },
                    {
                        "name": "Gautier Marti"
                    },
                    {
                        "name": "Hamdan Al Ahbabi"
                    },
                    {
                        "name": "Ibrahim Elfadel"
                    }
                ],
                "author_detail": {
                    "name": "Ibrahim Elfadel"
                },
                "author": "Ibrahim Elfadel",
                "arxiv_comment": "8 pages, 6 figures, 3 tables, CIKM 2025 FinFAI workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25701v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25701v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.01200v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.01200v2",
                "updated": "2025-10-29T17:02:41Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    17,
                    2,
                    41,
                    2,
                    302,
                    0
                ],
                "published": "2025-09-01T07:34:50Z",
                "published_parsed": [
                    2025,
                    9,
                    1,
                    7,
                    34,
                    50,
                    0,
                    244,
                    0
                ],
                "title": "SimulMEGA: MoE Routers are Advanced Policy Makers for Simultaneous\n  Speech Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SimulMEGA: MoE Routers are Advanced Policy Makers for Simultaneous\n  Speech Translation"
                },
                "summary": "Simultaneous Speech Translation (SimulST) enables real-time cross-lingual\ncommunication by jointly optimizing speech recognition and machine translation\nunder strict latency constraints. Existing systems struggle to balance\ntranslation quality, latency, and semantic coherence, particularly in\nmultilingual many-to-many scenarios where divergent read and write policies\nhinder unified strategy learning. In this paper, we present SimulMEGA\n(Simultaneous Generation by Mixture-of-Experts Gating), an unsupervised policy\nlearning framework that combines prefix-based training with a\nMixture-of-Experts refiner to learn effective read and write decisions in an\nimplicit manner, without adding inference-time overhead. Our design requires\nonly minimal modifications to standard transformer architectures and\ngeneralizes across both speech-to-text and text-to-speech streaming tasks.\nThrough comprehensive evaluation on six language pairs, our 500M parameter\nspeech-to-text model outperforms the Seamless baseline, achieving under 7\npercent BLEU degradation at 1.5 seconds average lag and under 3 percent at 3\nseconds. We further demonstrate the versatility of SimulMEGA by extending it to\nstreaming TTS with a unidirectional backbone, yielding superior latency quality\ntradeoffs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simultaneous Speech Translation (SimulST) enables real-time cross-lingual\ncommunication by jointly optimizing speech recognition and machine translation\nunder strict latency constraints. Existing systems struggle to balance\ntranslation quality, latency, and semantic coherence, particularly in\nmultilingual many-to-many scenarios where divergent read and write policies\nhinder unified strategy learning. In this paper, we present SimulMEGA\n(Simultaneous Generation by Mixture-of-Experts Gating), an unsupervised policy\nlearning framework that combines prefix-based training with a\nMixture-of-Experts refiner to learn effective read and write decisions in an\nimplicit manner, without adding inference-time overhead. Our design requires\nonly minimal modifications to standard transformer architectures and\ngeneralizes across both speech-to-text and text-to-speech streaming tasks.\nThrough comprehensive evaluation on six language pairs, our 500M parameter\nspeech-to-text model outperforms the Seamless baseline, achieving under 7\npercent BLEU degradation at 1.5 seconds average lag and under 3 percent at 3\nseconds. We further demonstrate the versatility of SimulMEGA by extending it to\nstreaming TTS with a unidirectional backbone, yielding superior latency quality\ntradeoffs."
                },
                "authors": [
                    {
                        "name": "Chenyang Le"
                    },
                    {
                        "name": "Bing Han"
                    },
                    {
                        "name": "Jinshun Li"
                    },
                    {
                        "name": "Songyong Chen"
                    },
                    {
                        "name": "Yanmin Qian"
                    }
                ],
                "author_detail": {
                    "name": "Yanmin Qian"
                },
                "author": "Yanmin Qian",
                "arxiv_comment": "NeurIPS 2025 poster",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.01200v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.01200v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25697v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25697v2",
                "updated": "2025-10-30T12:22:50Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    12,
                    22,
                    50,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-29T17:01:51Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    17,
                    1,
                    51,
                    2,
                    302,
                    0
                ],
                "title": "Fourier Neural Operators for Two-Phase, 2D Mold-Filling Problems Related\n  to Metal Casting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fourier Neural Operators for Two-Phase, 2D Mold-Filling Problems Related\n  to Metal Casting"
                },
                "summary": "We formulate mold filling in metal casting as a 2D neural operator learning\nproblem that maps geometry and boundary data on an unstructured mesh to time\nresolved flow quantities, replacing expensive transient CFD. In the proposed\nmethod, a graph based encoder aggregates local neighborhood information on the\ninput mesh and encodes geometry and boundary data, a Fourier spectral core\noperates on a regular latent grid to capture global interactions across the\ndomain, and a graph based decoder projects the latent fields to a target mesh.\nThe model is trained to jointly predict velocity components, pressure, and\nliquid volume fraction over a fixed rollout horizon and generalizes across\ndifferent ingate locations and process settings. On held out geometries and\ninlet conditions, it reproduces large scale advection and the fluid-air\ninterface evolution with localized errors near steep gradients. The mean\nrelative L2 error is about 5% across all fields, and inference is two to three\norders of magnitude faster than conventional CFD, enabling design in the loop\nexploration. Ablation studies show monotonic accuracy degradation under\nstronger spatial subsampling of input vertices and a smoother decline under\ntemporal subsampling. Halving the training set yields only a small increase in\nerror. These results establish neural operators as accurate and data efficient\nsurrogates for 2D mold filling and enable rapid optimization of gating systems\nin casting workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We formulate mold filling in metal casting as a 2D neural operator learning\nproblem that maps geometry and boundary data on an unstructured mesh to time\nresolved flow quantities, replacing expensive transient CFD. In the proposed\nmethod, a graph based encoder aggregates local neighborhood information on the\ninput mesh and encodes geometry and boundary data, a Fourier spectral core\noperates on a regular latent grid to capture global interactions across the\ndomain, and a graph based decoder projects the latent fields to a target mesh.\nThe model is trained to jointly predict velocity components, pressure, and\nliquid volume fraction over a fixed rollout horizon and generalizes across\ndifferent ingate locations and process settings. On held out geometries and\ninlet conditions, it reproduces large scale advection and the fluid-air\ninterface evolution with localized errors near steep gradients. The mean\nrelative L2 error is about 5% across all fields, and inference is two to three\norders of magnitude faster than conventional CFD, enabling design in the loop\nexploration. Ablation studies show monotonic accuracy degradation under\nstronger spatial subsampling of input vertices and a smoother decline under\ntemporal subsampling. Halving the training set yields only a small increase in\nerror. These results establish neural operators as accurate and data efficient\nsurrogates for 2D mold filling and enable rapid optimization of gating systems\nin casting workflows."
                },
                "authors": [
                    {
                        "name": "Edgard Moreira Minete"
                    },
                    {
                        "name": "Mathis Immertreu"
                    },
                    {
                        "name": "Fabian Teichmann"
                    },
                    {
                        "name": "Sebastian Müller"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Müller"
                },
                "author": "Sebastian Müller",
                "arxiv_comment": "21 pages, 7 figures, 5 tables, 63 references",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25697v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25697v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.6.3, J.2, I.2.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.23323v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.23323v2",
                "updated": "2025-10-29T16:59:39Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    16,
                    59,
                    39,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-24T14:47:49Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    14,
                    47,
                    49,
                    4,
                    297,
                    0
                ],
                "title": "Towards Scaling Deep Neural Networks with Predictive Coding: Theory and\n  Practice",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Scaling Deep Neural Networks with Predictive Coding: Theory and\n  Practice"
                },
                "summary": "Backpropagation (BP) is the standard algorithm for training the deep neural\nnetworks that power modern artificial intelligence including large language\nmodels. However, BP is energy inefficient and unlikely to be implemented by the\nbrain. This thesis studies an alternative, potentially more efficient\nbrain-inspired algorithm called predictive coding (PC). Unlike BP, PC networks\n(PCNs) perform inference by iterative equilibration of neuron activities before\nlearning or weight updates. Recent work has suggested that this iterative\ninference procedure provides a range of benefits over BP, such as faster\ntraining. However, these advantages have not been consistently observed, the\ninference and learning dynamics of PCNs are still poorly understood, and deep\nPCNs remain practically untrainable. Here, we make significant progress towards\nscaling PCNs by taking a theoretical approach grounded in optimisation theory.\nFirst, we show that the learning dynamics of PC can be understood as an\napproximate trust-region method using second-order information, despite\nexplicitly using only first-order local updates. Second, going beyond this\napproximation, we show that PC can in principle make use of arbitrarily\nhigher-order information, such that for feedforward networks the effective\nlandscape on which PC learns is far more benign and robust to vanishing\ngradients than the (mean squared error) loss landscape. Third, motivated by a\nstudy of the inference dynamics of PCNs, we propose a new parameterisation\ncalled \"$\\mu$PC\", which for the first time allows stable training of 100+ layer\nnetworks with little tuning and competitive performance on simple tasks.\nOverall, this thesis significantly advances our fundamental understanding of\nthe inference and learning dynamics of PCNs, while highlighting the need for\nfuture research to focus on hardware co-design if PC is to compete with BP at\nscale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Backpropagation (BP) is the standard algorithm for training the deep neural\nnetworks that power modern artificial intelligence including large language\nmodels. However, BP is energy inefficient and unlikely to be implemented by the\nbrain. This thesis studies an alternative, potentially more efficient\nbrain-inspired algorithm called predictive coding (PC). Unlike BP, PC networks\n(PCNs) perform inference by iterative equilibration of neuron activities before\nlearning or weight updates. Recent work has suggested that this iterative\ninference procedure provides a range of benefits over BP, such as faster\ntraining. However, these advantages have not been consistently observed, the\ninference and learning dynamics of PCNs are still poorly understood, and deep\nPCNs remain practically untrainable. Here, we make significant progress towards\nscaling PCNs by taking a theoretical approach grounded in optimisation theory.\nFirst, we show that the learning dynamics of PC can be understood as an\napproximate trust-region method using second-order information, despite\nexplicitly using only first-order local updates. Second, going beyond this\napproximation, we show that PC can in principle make use of arbitrarily\nhigher-order information, such that for feedforward networks the effective\nlandscape on which PC learns is far more benign and robust to vanishing\ngradients than the (mean squared error) loss landscape. Third, motivated by a\nstudy of the inference dynamics of PCNs, we propose a new parameterisation\ncalled \"$\\mu$PC\", which for the first time allows stable training of 100+ layer\nnetworks with little tuning and competitive performance on simple tasks.\nOverall, this thesis significantly advances our fundamental understanding of\nthe inference and learning dynamics of PCNs, while highlighting the need for\nfuture research to focus on hardware co-design if PC is to compete with BP at\nscale."
                },
                "authors": [
                    {
                        "name": "Francesco Innocenti"
                    }
                ],
                "author_detail": {
                    "name": "Francesco Innocenti"
                },
                "author": "Francesco Innocenti",
                "arxiv_comment": "PhD thesis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.23323v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.23323v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25694v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25694v1",
                "updated": "2025-10-29T16:59:07Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    16,
                    59,
                    7,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T16:59:07Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    16,
                    59,
                    7,
                    2,
                    302,
                    0
                ],
                "title": "Process-Level Trajectory Evaluation for Environment Configuration in\n  Software Engineering Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Process-Level Trajectory Evaluation for Environment Configuration in\n  Software Engineering Agents"
                },
                "summary": "Large language model-based agents show promise for software engineering, but\nenvironment configuration remains a bottleneck due to heavy manual effort and\nscarce large-scale, high-quality datasets. Existing benchmarks assess only\nend-to-end build/test success, obscuring where and why agents succeed or fail.\nWe introduce the Environment Configuration Diagnosis Benchmark, Enconda-bench,\nwhich provides process-level trajectory assessment of fine-grained agent\ncapabilities during environment setup-planning, perception-driven error\ndiagnosis, feedback-driven repair, and action to execute final environment\nconfiguration. Our task instances are automatically constructed by injecting\nrealistic README errors and are validated in Docker for scalable, high-quality\nevaluation. Enconda-bench combines process-level analysis with end-to-end\nexecutability to enable capability assessments beyond aggregate success rates.\nEvaluations across state-of-the-art LLMs and agent frameworks show that while\nagents can localize errors, they struggle to translate feedback into effective\ncorrections, limiting end-to-end performance. To our knowledge, Enconda-bench\nis the first framework to provide process-level internal capability assessment\nfor environment configuration, offering actionable insights for improving\nsoftware engineering agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model-based agents show promise for software engineering, but\nenvironment configuration remains a bottleneck due to heavy manual effort and\nscarce large-scale, high-quality datasets. Existing benchmarks assess only\nend-to-end build/test success, obscuring where and why agents succeed or fail.\nWe introduce the Environment Configuration Diagnosis Benchmark, Enconda-bench,\nwhich provides process-level trajectory assessment of fine-grained agent\ncapabilities during environment setup-planning, perception-driven error\ndiagnosis, feedback-driven repair, and action to execute final environment\nconfiguration. Our task instances are automatically constructed by injecting\nrealistic README errors and are validated in Docker for scalable, high-quality\nevaluation. Enconda-bench combines process-level analysis with end-to-end\nexecutability to enable capability assessments beyond aggregate success rates.\nEvaluations across state-of-the-art LLMs and agent frameworks show that while\nagents can localize errors, they struggle to translate feedback into effective\ncorrections, limiting end-to-end performance. To our knowledge, Enconda-bench\nis the first framework to provide process-level internal capability assessment\nfor environment configuration, offering actionable insights for improving\nsoftware engineering agents."
                },
                "authors": [
                    {
                        "name": "Jiayi Kuang"
                    },
                    {
                        "name": "Yinghui Li"
                    },
                    {
                        "name": "Xin Zhang"
                    },
                    {
                        "name": "Yangning Li"
                    },
                    {
                        "name": "Di Yin"
                    },
                    {
                        "name": "Xing Sun"
                    },
                    {
                        "name": "Ying Shen"
                    },
                    {
                        "name": "Philip S. Yu"
                    }
                ],
                "author_detail": {
                    "name": "Philip S. Yu"
                },
                "author": "Philip S. Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25694v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25694v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08599v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08599v2",
                "updated": "2025-10-29T16:51:36Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    16,
                    51,
                    36,
                    2,
                    302,
                    0
                ],
                "published": "2025-08-12T03:28:59Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    3,
                    28,
                    59,
                    1,
                    224,
                    0
                ],
                "title": "Squeezed Light Generation in Periodically Poled Thin-Film Lithium\n  Niobate Waveguides",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Squeezed Light Generation in Periodically Poled Thin-Film Lithium\n  Niobate Waveguides"
                },
                "summary": "Squeezed states of light play a key role in quantum-enhanced sensing and\ncontinuous-variable quantum information processing. Realizing integrated\nsqueezed light sources is crucial for developing compact and scalable photonic\nquantum systems. In this work, we demonstrate on-chip broadband vacuum\nsqueezing at telecommunication wavelengths on the thin-film lithium niobate\n(TFLN) platform. Our device integrates periodically poled lithium niobate\n(PPLN) nanophotonic waveguides with low-loss edge couplers, comprising bilayer\ninverse tapers and an SU-8 polymer waveguide. This configuration achieves a\nfiber-to-chip coupling loss of 1.4 dB and a total homodyne detection loss of 4\ndB, enabling a measured squeezing level of 1.4 dB. Additional measurements in a\nmore efficient PPLN waveguide (without low-loss couplers) infer an on-chip\nsqueezing level of over 10 dB at a pump power of 62 mW. These results\nunderscore the potential of TFLN platform for efficient and scalable squeezed\nlight generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Squeezed states of light play a key role in quantum-enhanced sensing and\ncontinuous-variable quantum information processing. Realizing integrated\nsqueezed light sources is crucial for developing compact and scalable photonic\nquantum systems. In this work, we demonstrate on-chip broadband vacuum\nsqueezing at telecommunication wavelengths on the thin-film lithium niobate\n(TFLN) platform. Our device integrates periodically poled lithium niobate\n(PPLN) nanophotonic waveguides with low-loss edge couplers, comprising bilayer\ninverse tapers and an SU-8 polymer waveguide. This configuration achieves a\nfiber-to-chip coupling loss of 1.4 dB and a total homodyne detection loss of 4\ndB, enabling a measured squeezing level of 1.4 dB. Additional measurements in a\nmore efficient PPLN waveguide (without low-loss couplers) infer an on-chip\nsqueezing level of over 10 dB at a pump power of 62 mW. These results\nunderscore the potential of TFLN platform for efficient and scalable squeezed\nlight generation."
                },
                "authors": [
                    {
                        "name": "Xiaodong Shi"
                    },
                    {
                        "name": "Angela Anna Baiju"
                    },
                    {
                        "name": "Xu Chen"
                    },
                    {
                        "name": "Sakthi Sanjeev Mohanraj"
                    },
                    {
                        "name": "Sihao Wang"
                    },
                    {
                        "name": "Veerendra Dhyani"
                    },
                    {
                        "name": "Biveen Shajilal"
                    },
                    {
                        "name": "Mengyao Zhao"
                    },
                    {
                        "name": "Ran Yang"
                    },
                    {
                        "name": "Yue Li"
                    },
                    {
                        "name": "Guangxing Wu"
                    },
                    {
                        "name": "Hao Hao"
                    },
                    {
                        "name": "Victor Leong"
                    },
                    {
                        "name": "Ping Koy Lam"
                    },
                    {
                        "name": "Di Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Di Zhu"
                },
                "author": "Di Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08599v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08599v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25683v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25683v1",
                "updated": "2025-10-29T16:47:24Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    16,
                    47,
                    24,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T16:47:24Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    16,
                    47,
                    24,
                    2,
                    302,
                    0
                ],
                "title": "Graph Network-based Structural Simulator: Graph Neural Networks for\n  Structural Dynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Network-based Structural Simulator: Graph Neural Networks for\n  Structural Dynamics"
                },
                "summary": "Graph Neural Networks (GNNs) have recently been explored as surrogate models\nfor numerical simulations. While their applications in computational fluid\ndynamics have been investigated, little attention has been given to structural\nproblems, especially for dynamic cases. To address this gap, we introduce the\nGraph Network-based Structural Simulator (GNSS), a GNN framework for surrogate\nmodeling of dynamic structural problems.\n  GNSS follows the encode-process-decode paradigm typical of GNN-based machine\nlearning models, and its design makes it particularly suited for dynamic\nsimulations thanks to three key features: (i) expressing node kinematics in\nnode-fixed local frames, which avoids catastrophic cancellation in\nfinite-difference velocities; (ii) employing a sign-aware regression loss,\nwhich reduces phase errors in long rollouts; and (iii) using a\nwavelength-informed connectivity radius, which optimizes graph construction.\n  We evaluate GNSS on a case study involving a beam excited by a 50kHz\nHanning-modulated pulse. The results show that GNSS accurately reproduces the\nphysics of the problem over hundreds of timesteps and generalizes to unseen\nloading conditions, where existing GNNs fail to converge or deliver meaningful\npredictions.\n  Compared with explicit finite element baselines, GNSS achieves substantial\ninference speedups while preserving spatial and temporal fidelity. These\nfindings demonstrate that locality-preserving GNNs with physics-consistent\nupdate rules are a competitive alternative for dynamic, wave-dominated\nstructural simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNNs) have recently been explored as surrogate models\nfor numerical simulations. While their applications in computational fluid\ndynamics have been investigated, little attention has been given to structural\nproblems, especially for dynamic cases. To address this gap, we introduce the\nGraph Network-based Structural Simulator (GNSS), a GNN framework for surrogate\nmodeling of dynamic structural problems.\n  GNSS follows the encode-process-decode paradigm typical of GNN-based machine\nlearning models, and its design makes it particularly suited for dynamic\nsimulations thanks to three key features: (i) expressing node kinematics in\nnode-fixed local frames, which avoids catastrophic cancellation in\nfinite-difference velocities; (ii) employing a sign-aware regression loss,\nwhich reduces phase errors in long rollouts; and (iii) using a\nwavelength-informed connectivity radius, which optimizes graph construction.\n  We evaluate GNSS on a case study involving a beam excited by a 50kHz\nHanning-modulated pulse. The results show that GNSS accurately reproduces the\nphysics of the problem over hundreds of timesteps and generalizes to unseen\nloading conditions, where existing GNNs fail to converge or deliver meaningful\npredictions.\n  Compared with explicit finite element baselines, GNSS achieves substantial\ninference speedups while preserving spatial and temporal fidelity. These\nfindings demonstrate that locality-preserving GNNs with physics-consistent\nupdate rules are a competitive alternative for dynamic, wave-dominated\nstructural simulations."
                },
                "authors": [
                    {
                        "name": "Alessandro Lucchetti"
                    },
                    {
                        "name": "Francesco Cadini"
                    },
                    {
                        "name": "Marco Giglio"
                    },
                    {
                        "name": "Luca Lomazzi"
                    }
                ],
                "author_detail": {
                    "name": "Luca Lomazzi"
                },
                "arxiv_affiliation": "Politecnico di Milano, Department of Mechanical Engineering, Milano, Italy",
                "author": "Luca Lomazzi",
                "arxiv_comment": "16 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25683v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25683v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25677v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25677v1",
                "updated": "2025-10-29T16:43:07Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    16,
                    43,
                    7,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T16:43:07Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    16,
                    43,
                    7,
                    2,
                    302,
                    0
                ],
                "title": "ZK-SenseLM: Verifiable Large-Model Wireless Sensing with Selective\n  Abstention and Zero-Knowledge Attestation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZK-SenseLM: Verifiable Large-Model Wireless Sensing with Selective\n  Abstention and Zero-Knowledge Attestation"
                },
                "summary": "ZK-SenseLM is a secure and auditable wireless sensing framework that pairs a\nlarge-model encoder for Wi-Fi channel state information (and optionally mmWave\nradar or RFID) with a policy-grounded decision layer and end-to-end\nzero-knowledge proofs of inference. The encoder uses masked spectral\npretraining with phase-consistency regularization, plus a light cross-modal\nalignment that ties RF features to compact, human-interpretable policy tokens.\nTo reduce unsafe actions under distribution shift, we add a calibrated\nselective-abstention head; the chosen risk-coverage operating point is\nregistered and bound into the proof. We implement a four-stage proving\npipeline: (C1) feature sanity and commitment, (C2) threshold and version\nbinding, (C3) time-window binding, and (C4) PLONK-style proofs that the\nquantized network, given the committed window, produced the logged action and\nconfidence. Micro-batched proving amortizes cost across adjacent windows, and a\ngateway option offloads proofs from low-power devices. The system integrates\nwith differentially private federated learning and on-device personalization\nwithout weakening verifiability: model hashes and the registered threshold are\npart of each public statement. Across activity, presence or intrusion,\nrespiratory proxy, and RF fingerprinting tasks, ZK-SenseLM improves macro-F1\nand calibration, yields favorable coverage-risk curves under perturbations, and\nrejects tamper and replay with compact proofs and fast verification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZK-SenseLM is a secure and auditable wireless sensing framework that pairs a\nlarge-model encoder for Wi-Fi channel state information (and optionally mmWave\nradar or RFID) with a policy-grounded decision layer and end-to-end\nzero-knowledge proofs of inference. The encoder uses masked spectral\npretraining with phase-consistency regularization, plus a light cross-modal\nalignment that ties RF features to compact, human-interpretable policy tokens.\nTo reduce unsafe actions under distribution shift, we add a calibrated\nselective-abstention head; the chosen risk-coverage operating point is\nregistered and bound into the proof. We implement a four-stage proving\npipeline: (C1) feature sanity and commitment, (C2) threshold and version\nbinding, (C3) time-window binding, and (C4) PLONK-style proofs that the\nquantized network, given the committed window, produced the logged action and\nconfidence. Micro-batched proving amortizes cost across adjacent windows, and a\ngateway option offloads proofs from low-power devices. The system integrates\nwith differentially private federated learning and on-device personalization\nwithout weakening verifiability: model hashes and the registered threshold are\npart of each public statement. Across activity, presence or intrusion,\nrespiratory proxy, and RF fingerprinting tasks, ZK-SenseLM improves macro-F1\nand calibration, yields favorable coverage-risk curves under perturbations, and\nrejects tamper and replay with compact proofs and fast verification."
                },
                "authors": [
                    {
                        "name": "Hasan Akgul"
                    },
                    {
                        "name": "Mari Eplik"
                    },
                    {
                        "name": "Javier Rojas"
                    },
                    {
                        "name": "Aina Binti Abdullah"
                    },
                    {
                        "name": "Pieter van der Merwe"
                    }
                ],
                "author_detail": {
                    "name": "Pieter van der Merwe"
                },
                "author": "Pieter van der Merwe",
                "arxiv_comment": "45 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25677v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25677v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.2.1; D.4.6; E.3; I.2.6; I.5.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25674v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25674v1",
                "updated": "2025-10-29T16:42:07Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    16,
                    42,
                    7,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T16:42:07Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    16,
                    42,
                    7,
                    2,
                    302,
                    0
                ],
                "title": "Mechanistic Interpretability of RNNs emulating Hidden Markov Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mechanistic Interpretability of RNNs emulating Hidden Markov Models"
                },
                "summary": "Recurrent neural networks (RNNs) provide a powerful approach in neuroscience\nto infer latent dynamics in neural populations and to generate hypotheses about\nthe neural computations underlying behavior. However, past work has focused on\nrelatively simple, input-driven, and largely deterministic behaviors - little\nis known about the mechanisms that would allow RNNs to generate the richer,\nspontaneous, and potentially stochastic behaviors observed in natural settings.\nModeling with Hidden Markov Models (HMMs) has revealed a segmentation of\nnatural behaviors into discrete latent states with stochastic transitions\nbetween them, a type of dynamics that may appear at odds with the continuous\nstate spaces implemented by RNNs. Here we first show that RNNs can replicate\nHMM emission statistics and then reverse-engineer the trained networks to\nuncover the mechanisms they implement. In the absence of inputs, the activity\nof trained RNNs collapses towards a single fixed point. When driven by\nstochastic input, trajectories instead exhibit noise-sustained dynamics along\nclosed orbits. Rotation along these orbits modulates the emission probabilities\nand is governed by transitions between regions of slow, noise-driven dynamics\nconnected by fast, deterministic transitions. The trained RNNs develop highly\nstructured connectivity, with a small set of \"kick neurons\" initiating\ntransitions between these regions. This mechanism emerges during training as\nthe network shifts into a regime of stochastic resonance, enabling it to\nperform probabilistic computations. Analyses across multiple HMM architectures\n- fully connected, cyclic, and linear-chain - reveal that this solution\ngeneralizes through the modular reuse of the same dynamical motif, suggesting a\ncompositional principle by which RNNs can emulate complex discrete latent\ndynamics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recurrent neural networks (RNNs) provide a powerful approach in neuroscience\nto infer latent dynamics in neural populations and to generate hypotheses about\nthe neural computations underlying behavior. However, past work has focused on\nrelatively simple, input-driven, and largely deterministic behaviors - little\nis known about the mechanisms that would allow RNNs to generate the richer,\nspontaneous, and potentially stochastic behaviors observed in natural settings.\nModeling with Hidden Markov Models (HMMs) has revealed a segmentation of\nnatural behaviors into discrete latent states with stochastic transitions\nbetween them, a type of dynamics that may appear at odds with the continuous\nstate spaces implemented by RNNs. Here we first show that RNNs can replicate\nHMM emission statistics and then reverse-engineer the trained networks to\nuncover the mechanisms they implement. In the absence of inputs, the activity\nof trained RNNs collapses towards a single fixed point. When driven by\nstochastic input, trajectories instead exhibit noise-sustained dynamics along\nclosed orbits. Rotation along these orbits modulates the emission probabilities\nand is governed by transitions between regions of slow, noise-driven dynamics\nconnected by fast, deterministic transitions. The trained RNNs develop highly\nstructured connectivity, with a small set of \"kick neurons\" initiating\ntransitions between these regions. This mechanism emerges during training as\nthe network shifts into a regime of stochastic resonance, enabling it to\nperform probabilistic computations. Analyses across multiple HMM architectures\n- fully connected, cyclic, and linear-chain - reveal that this solution\ngeneralizes through the modular reuse of the same dynamical motif, suggesting a\ncompositional principle by which RNNs can emulate complex discrete latent\ndynamics."
                },
                "authors": [
                    {
                        "name": "Elia Torre"
                    },
                    {
                        "name": "Michele Viscione"
                    },
                    {
                        "name": "Lucas Pompe"
                    },
                    {
                        "name": "Benjamin F Grewe"
                    },
                    {
                        "name": "Valerio Mante"
                    }
                ],
                "author_detail": {
                    "name": "Valerio Mante"
                },
                "author": "Valerio Mante",
                "arxiv_comment": "Accepted at NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25674v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25674v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25662v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25662v1",
                "updated": "2025-10-29T16:23:46Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    16,
                    23,
                    46,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T16:23:46Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    16,
                    23,
                    46,
                    2,
                    302,
                    0
                ],
                "title": "User Misconceptions of LLM-Based Conversational Programming Assistants",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "User Misconceptions of LLM-Based Conversational Programming Assistants"
                },
                "summary": "Programming assistants powered by large language models (LLMs) have become\nwidely available, with conversational assistants like ChatGPT proving\nparticularly accessible to less experienced programmers. However, the varied\ncapabilities of these tools across model versions and the mixed availability of\nextensions that enable web search, code execution, or retrieval-augmented\ngeneration create opportunities for user misconceptions about what systems can\nand cannot do. Such misconceptions may lead to over-reliance, unproductive\npractices, or insufficient quality control in LLM-assisted programming. Here,\nwe aim to characterize misconceptions that users of conversational LLM-based\nassistants may have in programming contexts. Using a two-phase approach, we\nfirst brainstorm and catalog user misconceptions that may occur, and then\nconduct a qualitative analysis to examine whether these conceptual issues\nsurface in naturalistic Python-programming conversations with an LLM-based\nchatbot drawn from an openly available dataset. Indeed, we see evidence that\nsome users have misplaced expectations about the availability of LLM-based\nchatbot features like web access, code execution, or non-text output\ngeneration. We also see potential evidence for deeper conceptual issues around\nthe scope of information required to debug, validate, and optimize programs.\nOur findings reinforce the need for designing LLM-based tools that more clearly\ncommunicate their programming capabilities to users.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Programming assistants powered by large language models (LLMs) have become\nwidely available, with conversational assistants like ChatGPT proving\nparticularly accessible to less experienced programmers. However, the varied\ncapabilities of these tools across model versions and the mixed availability of\nextensions that enable web search, code execution, or retrieval-augmented\ngeneration create opportunities for user misconceptions about what systems can\nand cannot do. Such misconceptions may lead to over-reliance, unproductive\npractices, or insufficient quality control in LLM-assisted programming. Here,\nwe aim to characterize misconceptions that users of conversational LLM-based\nassistants may have in programming contexts. Using a two-phase approach, we\nfirst brainstorm and catalog user misconceptions that may occur, and then\nconduct a qualitative analysis to examine whether these conceptual issues\nsurface in naturalistic Python-programming conversations with an LLM-based\nchatbot drawn from an openly available dataset. Indeed, we see evidence that\nsome users have misplaced expectations about the availability of LLM-based\nchatbot features like web access, code execution, or non-text output\ngeneration. We also see potential evidence for deeper conceptual issues around\nthe scope of information required to debug, validate, and optimize programs.\nOur findings reinforce the need for designing LLM-based tools that more clearly\ncommunicate their programming capabilities to users."
                },
                "authors": [
                    {
                        "name": "Gabrielle O'Brien"
                    },
                    {
                        "name": "Antonio Pedro Santos Alves"
                    },
                    {
                        "name": "Sebastian Baltes"
                    },
                    {
                        "name": "Grischa Liebel"
                    },
                    {
                        "name": "Mircea Lungu"
                    },
                    {
                        "name": "Marcos Kalinowski"
                    }
                ],
                "author_detail": {
                    "name": "Marcos Kalinowski"
                },
                "author": "Marcos Kalinowski",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25662v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25662v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25661v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25661v1",
                "updated": "2025-10-29T16:23:27Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    16,
                    23,
                    27,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T16:23:27Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    16,
                    23,
                    27,
                    2,
                    302,
                    0
                ],
                "title": "Accurate Leakage Speculation for Quantum Error Correction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate Leakage Speculation for Quantum Error Correction"
                },
                "summary": "Quantum Error Correction (QEC) protects qubits against bit- and phase-flip\nerrors in the |0> or |1> subspace, but physical qubits can also leak into\nhigher energy levels (e.g., |2>). Leakage is especially harmful, as it corrupts\nall subsequent syndrome measurements and can spread to neighboring qubits.\nDetecting leakage on data qubits is particularly challenging, since they are\nnever measured directly during QEC cycles. Prior work, such as eraser,\naddresses this by inferring leakage from syndrome patterns using a fixed\nheuristic. However, this approach often misclassifies benign syndromes,\ntriggering excessive leakage-reduction circuits (LRCs). Because LRCs are\nthemselves noisy and slow, these false triggers lengthen QEC cycles and inflate\nlogical error rates.\n  We propose gladiator, a general and adaptable leakage speculation framework\nthat works across surface code, color code, and qLDPC codes. Offline, gladiator\nbuilds a code-aware error-propagation graph calibrated to device data. Online,\nit classifies each syndrome in a few nanoseconds and schedules LRC only when\nthe observed pattern is provably leakage-dominated. This precise speculation\neliminates up to 3x (and on average 2x) unnecessary LRCs, shortens QEC cycles,\nand suppresses false positives at their source. Evaluated on standard\nfault-tolerant benchmarks, gladiator delivers 1.7x-3.9x speedups and 16%\nreduction in logical error rate, advancing the efficiency of fault-tolerant\nquantum computing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum Error Correction (QEC) protects qubits against bit- and phase-flip\nerrors in the |0> or |1> subspace, but physical qubits can also leak into\nhigher energy levels (e.g., |2>). Leakage is especially harmful, as it corrupts\nall subsequent syndrome measurements and can spread to neighboring qubits.\nDetecting leakage on data qubits is particularly challenging, since they are\nnever measured directly during QEC cycles. Prior work, such as eraser,\naddresses this by inferring leakage from syndrome patterns using a fixed\nheuristic. However, this approach often misclassifies benign syndromes,\ntriggering excessive leakage-reduction circuits (LRCs). Because LRCs are\nthemselves noisy and slow, these false triggers lengthen QEC cycles and inflate\nlogical error rates.\n  We propose gladiator, a general and adaptable leakage speculation framework\nthat works across surface code, color code, and qLDPC codes. Offline, gladiator\nbuilds a code-aware error-propagation graph calibrated to device data. Online,\nit classifies each syndrome in a few nanoseconds and schedules LRC only when\nthe observed pattern is provably leakage-dominated. This precise speculation\neliminates up to 3x (and on average 2x) unnecessary LRCs, shortens QEC cycles,\nand suppresses false positives at their source. Evaluated on standard\nfault-tolerant benchmarks, gladiator delivers 1.7x-3.9x speedups and 16%\nreduction in logical error rate, advancing the efficiency of fault-tolerant\nquantum computing."
                },
                "authors": [
                    {
                        "name": "Chaithanya Naik Mude"
                    },
                    {
                        "name": "Swamit Tannu"
                    }
                ],
                "author_detail": {
                    "name": "Swamit Tannu"
                },
                "author": "Swamit Tannu",
                "arxiv_doi": "10.1145/3725843.3756053",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3725843.3756053",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.25661v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25661v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "11 pages,14 figures, MICRO '25: Proceedings of the 58th IEEE/ACM\n  International Symposium on Microarchitecture",
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25647v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25647v1",
                "updated": "2025-10-29T16:07:51Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    16,
                    7,
                    51,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T16:07:51Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    16,
                    7,
                    51,
                    2,
                    302,
                    0
                ],
                "title": "Extreme equivalent width-selected low-mass starbursts at $z=4-9$:\n  insights into their role in cosmic reionization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extreme equivalent width-selected low-mass starbursts at $z=4-9$:\n  insights into their role in cosmic reionization"
                },
                "summary": "We investigate the properties of extreme emission line galaxies (EELGs) at\n$z=4-9$ and their role in reionization. Compact, low-mass galaxies with intense\noptical emission lines are linked to elevated specific star formation rates\n(sSFRs) and recent bursts of star formation. Feedback in these systems may\nenable the leakage of ionizing radiation into the intergalactic medium. Using\nJWST/NIRSpec spectroscopy from the CAPERS, CEERS, and RUBIES surveys, we\ncompile 160 NIRCam-selected EELGs in the EGS field. These galaxies show extreme\nrest-frame equivalent widths (EWs), with a median EW([O\nIII]+H${\\beta}$)=1616\\r{A} and EW(H${\\alpha}$)=763\\r{A}. They are low-mass\n(median log(M$_{\\star}$/M$_{\\odot}$)=8.26) with high sSFRs (median 43\nGyr$^{-1}$), above the $z\\sim6$ main sequence. UV slopes are diverse, with a\nmean $\\beta=-2.0$, and only 7% have extremely blue continua ($\\beta<-2.6$).\nEmission-line diagnostics suggest stellar populations as the primary ionizing\nsource, although an AGN fraction of 14% is found. These galaxies are efficient\nionizing photon producers, with mean log($\\xi_{\\rm ion}$ [Hz\nerg$^{-1}$])=25.34, exceeding typical values at similar redshifts. Escape\nfractions, however, are heterogeneous: 9% of EELGs show escape fractions $>$10%\nfor both Ly${\\alpha}$ and LyC photons, while 82% lack detectable Ly${\\alpha}$\nemission. The median inferred LyC escape fraction is modest (4.4%) but enhanced\nin super-Eddington systems with sSFR >25 Gyr$^{-1}$. The galaxies are extremely\ncompact, with a median effective radius of 0.49 kpc, and exhibit a recent\nstar-formation burst. Our analysis indicates that sSFR and star-formation rate\nsurface density are the primary drivers of their extreme emission line\nstrengths.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate the properties of extreme emission line galaxies (EELGs) at\n$z=4-9$ and their role in reionization. Compact, low-mass galaxies with intense\noptical emission lines are linked to elevated specific star formation rates\n(sSFRs) and recent bursts of star formation. Feedback in these systems may\nenable the leakage of ionizing radiation into the intergalactic medium. Using\nJWST/NIRSpec spectroscopy from the CAPERS, CEERS, and RUBIES surveys, we\ncompile 160 NIRCam-selected EELGs in the EGS field. These galaxies show extreme\nrest-frame equivalent widths (EWs), with a median EW([O\nIII]+H${\\beta}$)=1616\\r{A} and EW(H${\\alpha}$)=763\\r{A}. They are low-mass\n(median log(M$_{\\star}$/M$_{\\odot}$)=8.26) with high sSFRs (median 43\nGyr$^{-1}$), above the $z\\sim6$ main sequence. UV slopes are diverse, with a\nmean $\\beta=-2.0$, and only 7% have extremely blue continua ($\\beta<-2.6$).\nEmission-line diagnostics suggest stellar populations as the primary ionizing\nsource, although an AGN fraction of 14% is found. These galaxies are efficient\nionizing photon producers, with mean log($\\xi_{\\rm ion}$ [Hz\nerg$^{-1}$])=25.34, exceeding typical values at similar redshifts. Escape\nfractions, however, are heterogeneous: 9% of EELGs show escape fractions $>$10%\nfor both Ly${\\alpha}$ and LyC photons, while 82% lack detectable Ly${\\alpha}$\nemission. The median inferred LyC escape fraction is modest (4.4%) but enhanced\nin super-Eddington systems with sSFR >25 Gyr$^{-1}$. The galaxies are extremely\ncompact, with a median effective radius of 0.49 kpc, and exhibit a recent\nstar-formation burst. Our analysis indicates that sSFR and star-formation rate\nsurface density are the primary drivers of their extreme emission line\nstrengths."
                },
                "authors": [
                    {
                        "name": "M. Llerena"
                    },
                    {
                        "name": "L. Pentericci"
                    },
                    {
                        "name": "R. Amorín"
                    },
                    {
                        "name": "A. Ferrara"
                    },
                    {
                        "name": "M. Dickinson"
                    },
                    {
                        "name": "F. Arevalo"
                    },
                    {
                        "name": "A. Calabrò"
                    },
                    {
                        "name": "L. Napolitano"
                    },
                    {
                        "name": "S. Mascia"
                    },
                    {
                        "name": "P. Arrabal Haro"
                    },
                    {
                        "name": "R. Begley"
                    },
                    {
                        "name": "N. J. Cleri"
                    },
                    {
                        "name": "K. Davis"
                    },
                    {
                        "name": "W. Hu"
                    },
                    {
                        "name": "J. S. Kartaltepe"
                    },
                    {
                        "name": "A. M. Koekemoer"
                    },
                    {
                        "name": "R. A. Lucas"
                    },
                    {
                        "name": "E. McGrath"
                    },
                    {
                        "name": "D. J. McLeod"
                    },
                    {
                        "name": "C. Papovich"
                    },
                    {
                        "name": "T. M. Stanton"
                    },
                    {
                        "name": "A. J. Taylor"
                    },
                    {
                        "name": "R. Tripodi"
                    },
                    {
                        "name": "X. Wang"
                    },
                    {
                        "name": "L. Y. A. Yung"
                    }
                ],
                "author_detail": {
                    "name": "L. Y. A. Yung"
                },
                "author": "L. Y. A. Yung",
                "arxiv_comment": "Submitted to A&A. Comments are welcome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25647v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25647v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24636v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24636v2",
                "updated": "2025-10-29T16:06:18Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    16,
                    6,
                    18,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-28T17:02:46Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    17,
                    2,
                    46,
                    1,
                    301,
                    0
                ],
                "title": "OpenReward: Learning to Reward Long-form Agentic Tasks via Reinforcement\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OpenReward: Learning to Reward Long-form Agentic Tasks via Reinforcement\n  Learning"
                },
                "summary": "Reward models (RMs) have become essential for aligning large language models\n(LLMs), serving as scalable proxies for human evaluation in both training and\ninference. However, existing RMs struggle on knowledge-intensive and long-form\ntasks, where evaluating correctness requires grounding beyond the model's\ninternal knowledge. This limitation hinders them from reliably discriminating\nsubtle quality differences, especially when external evidence is necessary. To\naddress this, we introduce OpenRM, a tool-augmented long-form reward model that\nsystematically judges open-ended responses by invoking external tools to gather\nrelevant evidence. We train OpenRM with Group Relative Policy Optimization\n(GRPO) on over 27K synthesized pairwise examples generated through a\ncontrollable data synthesis framework. The training objective jointly\nsupervises intermediate tool usage and final outcome accuracy, incentivizing\nour reward model to learn effective evidence-based judgment strategies.\nExtensive experiments on three newly-collected datasets and two widely-used\nbenchmarks demonstrate that OpenRM substantially outperforms existing reward\nmodeling approaches. As a further step, we integrate OpenRM into both\ninference-time response selection and training-time data selection. This yields\nconsistent gains in downstream LLM alignment tasks, highlighting the potential\nof tool-augmented reward models for scaling reliable long-form evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reward models (RMs) have become essential for aligning large language models\n(LLMs), serving as scalable proxies for human evaluation in both training and\ninference. However, existing RMs struggle on knowledge-intensive and long-form\ntasks, where evaluating correctness requires grounding beyond the model's\ninternal knowledge. This limitation hinders them from reliably discriminating\nsubtle quality differences, especially when external evidence is necessary. To\naddress this, we introduce OpenRM, a tool-augmented long-form reward model that\nsystematically judges open-ended responses by invoking external tools to gather\nrelevant evidence. We train OpenRM with Group Relative Policy Optimization\n(GRPO) on over 27K synthesized pairwise examples generated through a\ncontrollable data synthesis framework. The training objective jointly\nsupervises intermediate tool usage and final outcome accuracy, incentivizing\nour reward model to learn effective evidence-based judgment strategies.\nExtensive experiments on three newly-collected datasets and two widely-used\nbenchmarks demonstrate that OpenRM substantially outperforms existing reward\nmodeling approaches. As a further step, we integrate OpenRM into both\ninference-time response selection and training-time data selection. This yields\nconsistent gains in downstream LLM alignment tasks, highlighting the potential\nof tool-augmented reward models for scaling reliable long-form evaluation."
                },
                "authors": [
                    {
                        "name": "Ziyou Hu"
                    },
                    {
                        "name": "Zhengliang Shi"
                    },
                    {
                        "name": "Minghang Zhu"
                    },
                    {
                        "name": "Haitao Li"
                    },
                    {
                        "name": "Teng Sun"
                    },
                    {
                        "name": "Pengjie Ren"
                    },
                    {
                        "name": "Suzan Verberne"
                    },
                    {
                        "name": "Zhaochun Ren"
                    }
                ],
                "author_detail": {
                    "name": "Zhaochun Ren"
                },
                "author": "Zhaochun Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24636v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24636v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.07464v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.07464v3",
                "updated": "2025-10-29T15:59:41Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    15,
                    59,
                    41,
                    2,
                    302,
                    0
                ],
                "published": "2025-06-09T06:15:54Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    6,
                    15,
                    54,
                    0,
                    160,
                    0
                ],
                "title": "DeepVideo-R1: Video Reinforcement Fine-Tuning via Difficulty-aware\n  Regressive GRPO",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepVideo-R1: Video Reinforcement Fine-Tuning via Difficulty-aware\n  Regressive GRPO"
                },
                "summary": "Recent works have demonstrated the effectiveness of reinforcement learning\n(RL)-based post-training for enhancing the reasoning capabilities of large\nlanguage models (LLMs). In particular, Group Relative Policy Optimization\n(GRPO) has shown impressive success using a PPO-style reinforcement algorithm\nwith group-normalized rewards. However, the effectiveness of GRPO in Video\nLarge Language Models (VideoLLMs) has still been less studyed. In this paper,\nwe explore GRPO and identify two problems that deteriorate the effective\nlearning: (1) reliance on safeguards, and (2) vanishing advantage. To mitigate\nthese challenges, we propose DeepVideo-R1, a video large language model trained\nwith Reg-GRPO (Regressive GRPO) and difficulty-aware data augmentation.\nReg-GRPO reformulates the GRPO loss function into a regression task that\ndirectly predicts the advantage in GRPO, eliminating the need for safeguards\nsuch as the clipping and min functions. It directly aligns the model with\nadvantages, providing guidance to prefer better ones. The difficulty-aware data\naugmentation strategy augments input prompts/videos to locate the difficulty of\nsamples at solvable difficulty levels, enabling diverse reward signals. Our\nexperimental results show that our approach significantly improves video\nreasoning performance across multiple benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent works have demonstrated the effectiveness of reinforcement learning\n(RL)-based post-training for enhancing the reasoning capabilities of large\nlanguage models (LLMs). In particular, Group Relative Policy Optimization\n(GRPO) has shown impressive success using a PPO-style reinforcement algorithm\nwith group-normalized rewards. However, the effectiveness of GRPO in Video\nLarge Language Models (VideoLLMs) has still been less studyed. In this paper,\nwe explore GRPO and identify two problems that deteriorate the effective\nlearning: (1) reliance on safeguards, and (2) vanishing advantage. To mitigate\nthese challenges, we propose DeepVideo-R1, a video large language model trained\nwith Reg-GRPO (Regressive GRPO) and difficulty-aware data augmentation.\nReg-GRPO reformulates the GRPO loss function into a regression task that\ndirectly predicts the advantage in GRPO, eliminating the need for safeguards\nsuch as the clipping and min functions. It directly aligns the model with\nadvantages, providing guidance to prefer better ones. The difficulty-aware data\naugmentation strategy augments input prompts/videos to locate the difficulty of\nsamples at solvable difficulty levels, enabling diverse reward signals. Our\nexperimental results show that our approach significantly improves video\nreasoning performance across multiple benchmarks."
                },
                "authors": [
                    {
                        "name": "Jinyoung Park"
                    },
                    {
                        "name": "Jeehye Na"
                    },
                    {
                        "name": "Jinyoung Kim"
                    },
                    {
                        "name": "Hyunwoo J. Kim"
                    }
                ],
                "author_detail": {
                    "name": "Hyunwoo J. Kim"
                },
                "author": "Hyunwoo J. Kim",
                "arxiv_comment": "NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.07464v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.07464v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.14785v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.14785v2",
                "updated": "2025-10-29T15:56:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    15,
                    56,
                    28,
                    2,
                    302,
                    0
                ],
                "published": "2025-07-20T02:00:21Z",
                "published_parsed": [
                    2025,
                    7,
                    20,
                    2,
                    0,
                    21,
                    6,
                    201,
                    0
                ],
                "title": "Exploring the In-Context Learning Capabilities of LLMs for Money\n  Laundering Detection in Financial Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the In-Context Learning Capabilities of LLMs for Money\n  Laundering Detection in Financial Graphs"
                },
                "summary": "The complexity and interconnectivity of entities involved in money laundering\ndemand investigative reasoning over graph-structured data. This paper explores\nthe use of large language models (LLMs) as reasoning engines over localized\nsubgraphs extracted from a financial knowledge graph. We propose a lightweight\npipeline that retrieves k-hop neighborhoods around entities of interest,\nserializes them into structured text, and prompts an LLM via few-shot\nin-context learning to assess suspiciousness and generate justifications. Using\nsynthetic anti-money laundering (AML) scenarios that reflect common laundering\nbehaviors, we show that LLMs can emulate analyst-style logic, highlight red\nflags, and provide coherent explanations. While this study is exploratory, it\nillustrates the potential of LLM-based graph reasoning in AML and lays\ngroundwork for explainable, language-driven financial crime analytics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The complexity and interconnectivity of entities involved in money laundering\ndemand investigative reasoning over graph-structured data. This paper explores\nthe use of large language models (LLMs) as reasoning engines over localized\nsubgraphs extracted from a financial knowledge graph. We propose a lightweight\npipeline that retrieves k-hop neighborhoods around entities of interest,\nserializes them into structured text, and prompts an LLM via few-shot\nin-context learning to assess suspiciousness and generate justifications. Using\nsynthetic anti-money laundering (AML) scenarios that reflect common laundering\nbehaviors, we show that LLMs can emulate analyst-style logic, highlight red\nflags, and provide coherent explanations. While this study is exploratory, it\nillustrates the potential of LLM-based graph reasoning in AML and lays\ngroundwork for explainable, language-driven financial crime analytics."
                },
                "authors": [
                    {
                        "name": "Erfan Pirmorad"
                    }
                ],
                "author_detail": {
                    "name": "Erfan Pirmorad"
                },
                "author": "Erfan Pirmorad",
                "arxiv_comment": "Accepted at AI4FCF-ICDM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.14785v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.14785v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.23965v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.23965v2",
                "updated": "2025-10-29T15:51:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    15,
                    51,
                    35,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-28T00:42:38Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    0,
                    42,
                    38,
                    1,
                    301,
                    0
                ],
                "title": "The Sign Estimator: LLM Alignment in the Face of Choice Heterogeneity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Sign Estimator: LLM Alignment in the Face of Choice Heterogeneity"
                },
                "summary": "Traditional LLM alignment methods are vulnerable to heterogeneity in human\npreferences. Fitting a na\\\"ive probabilistic model to pairwise comparison data\n(say over prompt-completion pairs) yields an inconsistent estimate of the\npopulation-average utility -a canonical measure of social welfare. We propose a\nnew method, dubbed the sign estimator, that provides a simple, provably\nconsistent, and efficient estimator by replacing cross-entropy with binary\nclassification loss in the aggregation step. This simple modification recovers\nconsistent ordinal alignment under mild assumptions and achieves the first\npolynomial finite-sample error bounds in this setting. In realistic simulations\nof LLM alignment using digital twins, the sign estimator substantially reduces\npreference distortion over a panel of simulated personas, cutting (angular)\nestimation error by nearly 35% and decreasing disagreement with true population\npreferences from 12% to 8% compared to standard RLHF. Our method also compares\nfavorably to panel data heuristics that explicitly model user heterogeneity and\nrequire tracking individual-level preference data-all while maintaining the\nimplementation simplicity of existing LLM alignment pipelines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional LLM alignment methods are vulnerable to heterogeneity in human\npreferences. Fitting a na\\\"ive probabilistic model to pairwise comparison data\n(say over prompt-completion pairs) yields an inconsistent estimate of the\npopulation-average utility -a canonical measure of social welfare. We propose a\nnew method, dubbed the sign estimator, that provides a simple, provably\nconsistent, and efficient estimator by replacing cross-entropy with binary\nclassification loss in the aggregation step. This simple modification recovers\nconsistent ordinal alignment under mild assumptions and achieves the first\npolynomial finite-sample error bounds in this setting. In realistic simulations\nof LLM alignment using digital twins, the sign estimator substantially reduces\npreference distortion over a panel of simulated personas, cutting (angular)\nestimation error by nearly 35% and decreasing disagreement with true population\npreferences from 12% to 8% compared to standard RLHF. Our method also compares\nfavorably to panel data heuristics that explicitly model user heterogeneity and\nrequire tracking individual-level preference data-all while maintaining the\nimplementation simplicity of existing LLM alignment pipelines."
                },
                "authors": [
                    {
                        "name": "Ali Aouad"
                    },
                    {
                        "name": "Aymane El Gadarri"
                    },
                    {
                        "name": "Vivek F. Farias"
                    }
                ],
                "author_detail": {
                    "name": "Vivek F. Farias"
                },
                "author": "Vivek F. Farias",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.23965v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.23965v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25628v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25628v1",
                "updated": "2025-10-29T15:32:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    15,
                    32,
                    47,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T15:32:47Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    15,
                    32,
                    47,
                    2,
                    302,
                    0
                ],
                "title": "EHR-R1: A Reasoning-Enhanced Foundational Language Model for Electronic\n  Health Record Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EHR-R1: A Reasoning-Enhanced Foundational Language Model for Electronic\n  Health Record Analysis"
                },
                "summary": "Electronic Health Records (EHRs) contain rich yet complex information, and\ntheir automated analysis is critical for clinical decision-making. Despite\nrecent advances of large language models (LLMs) in clinical workflows, their\nability to analyze EHRs remains limited due to narrow task coverage and lack of\nEHR-oriented reasoning capabilities. This paper aims to bridge the gap,\nspecifically, we present EHR-Ins, a large-scale, comprehensive EHR reasoning\ninstruction dataset, comprising 300k high-quality reasoning cases and 4M\nnon-reasoning cases across 42 distinct EHR tasks. Its core innovation is a\nthinking-graph-driven framework that enables to generate high-quality reasoning\ndata at scale. Based on it, we develop EHR-R1, a series of reasoning-enhanced\nLLMs with up to 72B parameters tailored for EHR analysis. Through a multi-stage\ntraining paradigm, including domain adaptation, reasoning enhancement, and\nreinforcement learning, EHR-R1 systematically acquires domain knowledge and\ndiverse reasoning capabilities, enabling accurate and robust EHR analysis.\nLastly, we introduce EHR-Bench, a new benchmark curated from MIMIC-IV, spanning\n42 tasks, to comprehensively assess reasoning and prediction across EHR\nscenarios. In experiments, we show that the resulting EHR-R1 consistently\noutperforms state-of-the-art commercial and open-source LLMs (including\nDeepSeek-V3 and GPT-4o), surpassing GPT-4o by over 30 points on MIMIC-Bench and\nachieving a 10\\% higher zero-shot AUROC on EHRSHOT. Collectively, EHR-Ins,\nEHR-R1, and EHR-Bench have significantly advanced the development for more\nreliable and clinically relevant EHR analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electronic Health Records (EHRs) contain rich yet complex information, and\ntheir automated analysis is critical for clinical decision-making. Despite\nrecent advances of large language models (LLMs) in clinical workflows, their\nability to analyze EHRs remains limited due to narrow task coverage and lack of\nEHR-oriented reasoning capabilities. This paper aims to bridge the gap,\nspecifically, we present EHR-Ins, a large-scale, comprehensive EHR reasoning\ninstruction dataset, comprising 300k high-quality reasoning cases and 4M\nnon-reasoning cases across 42 distinct EHR tasks. Its core innovation is a\nthinking-graph-driven framework that enables to generate high-quality reasoning\ndata at scale. Based on it, we develop EHR-R1, a series of reasoning-enhanced\nLLMs with up to 72B parameters tailored for EHR analysis. Through a multi-stage\ntraining paradigm, including domain adaptation, reasoning enhancement, and\nreinforcement learning, EHR-R1 systematically acquires domain knowledge and\ndiverse reasoning capabilities, enabling accurate and robust EHR analysis.\nLastly, we introduce EHR-Bench, a new benchmark curated from MIMIC-IV, spanning\n42 tasks, to comprehensively assess reasoning and prediction across EHR\nscenarios. In experiments, we show that the resulting EHR-R1 consistently\noutperforms state-of-the-art commercial and open-source LLMs (including\nDeepSeek-V3 and GPT-4o), surpassing GPT-4o by over 30 points on MIMIC-Bench and\nachieving a 10\\% higher zero-shot AUROC on EHRSHOT. Collectively, EHR-Ins,\nEHR-R1, and EHR-Bench have significantly advanced the development for more\nreliable and clinically relevant EHR analysis."
                },
                "authors": [
                    {
                        "name": "Yusheng Liao"
                    },
                    {
                        "name": "Chaoyi Wu"
                    },
                    {
                        "name": "Junwei Liu"
                    },
                    {
                        "name": "Shuyang Jiang"
                    },
                    {
                        "name": "Pengcheng Qiu"
                    },
                    {
                        "name": "Haowen Wang"
                    },
                    {
                        "name": "Yun Yue"
                    },
                    {
                        "name": "Shuai Zhen"
                    },
                    {
                        "name": "Jian Wang"
                    },
                    {
                        "name": "Qianrui Fan"
                    },
                    {
                        "name": "Jinjie Gu"
                    },
                    {
                        "name": "Ya Zhang"
                    },
                    {
                        "name": "Yanfeng Wang"
                    },
                    {
                        "name": "Yu Wang"
                    },
                    {
                        "name": "Weidi Xie"
                    }
                ],
                "author_detail": {
                    "name": "Weidi Xie"
                },
                "author": "Weidi Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25628v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25628v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25626v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25626v1",
                "updated": "2025-10-29T15:30:31Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    15,
                    30,
                    31,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T15:30:31Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    15,
                    30,
                    31,
                    2,
                    302,
                    0
                ],
                "title": "Are Language Models Efficient Reasoners? A Perspective from Logic\n  Programming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are Language Models Efficient Reasoners? A Perspective from Logic\n  Programming"
                },
                "summary": "Modern language models (LMs) exhibit strong deductive reasoning capabilities,\nyet standard evaluations emphasize correctness while overlooking a key aspect\nof human-like reasoning: efficiency. In real-world reasoning scenarios, much of\nthe available information is irrelevant, and effective deductive inference\nrequires identifying and ignoring such distractions. We propose a framework for\nassessing LM reasoning efficiency through the lens of logic programming,\nintroducing a simple method to align proofs written in natural language -- as\ngenerated by an LM -- with shortest proofs found by executing the logic\nprogram. Efficiency is quantified by measuring how well a model avoids\nunnecessary inference. Empirically, we construct a dataset of math word\nproblems injected with various number of irrelevant axioms that vary in\nsemantic overlap with the goal theorem. We find that current LMs show marked\naccuracy declines under such conditions -- even with minimal, domain-consistent\ndistractions -- and the proofs they generate frequently exhibit detours through\nirrelevant inferences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern language models (LMs) exhibit strong deductive reasoning capabilities,\nyet standard evaluations emphasize correctness while overlooking a key aspect\nof human-like reasoning: efficiency. In real-world reasoning scenarios, much of\nthe available information is irrelevant, and effective deductive inference\nrequires identifying and ignoring such distractions. We propose a framework for\nassessing LM reasoning efficiency through the lens of logic programming,\nintroducing a simple method to align proofs written in natural language -- as\ngenerated by an LM -- with shortest proofs found by executing the logic\nprogram. Efficiency is quantified by measuring how well a model avoids\nunnecessary inference. Empirically, we construct a dataset of math word\nproblems injected with various number of irrelevant axioms that vary in\nsemantic overlap with the goal theorem. We find that current LMs show marked\naccuracy declines under such conditions -- even with minimal, domain-consistent\ndistractions -- and the proofs they generate frequently exhibit detours through\nirrelevant inferences."
                },
                "authors": [
                    {
                        "name": "Andreas Opedal"
                    },
                    {
                        "name": "Yanick Zengaffinen"
                    },
                    {
                        "name": "Haruki Shirakami"
                    },
                    {
                        "name": "Clemente Pasti"
                    },
                    {
                        "name": "Mrinmaya Sachan"
                    },
                    {
                        "name": "Abulhair Saparov"
                    },
                    {
                        "name": "Ryan Cotterell"
                    },
                    {
                        "name": "Bernhard Schölkopf"
                    }
                ],
                "author_detail": {
                    "name": "Bernhard Schölkopf"
                },
                "author": "Bernhard Schölkopf",
                "arxiv_comment": "Accepted to NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25626v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25626v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25623v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25623v1",
                "updated": "2025-10-29T15:27:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    15,
                    27,
                    47,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T15:27:47Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    15,
                    27,
                    47,
                    2,
                    302,
                    0
                ],
                "title": "Evaluating the Role of Verifiers in Test-Time Scaling for Legal\n  Reasoning Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the Role of Verifiers in Test-Time Scaling for Legal\n  Reasoning Tasks"
                },
                "summary": "Test-time scaling (TTS) techniques can improve the performance of large\nlanguage models (LLMs) at the expense of additional computation and latency.\nWhile TTS has proven effective in formal domains such as mathematics and\nprogramming \\citep{snell2024scaling, chen2024more}, its value in argumentative\ndomains such as law remains underexplored. We present an empirical study of\nverifier-based TTS methods for legal multiple-choice QA (MCQA) across five\nbenchmarks. Using a family of 7 reward models, we evaluate both outcome-level\n(Best-of-$N$) and process-level (tree search) verification under realistic\nlow-$N$ budgets. Our analysis systematically investigates how verifier utility\nis affected by key properties such as domain specialization, model size, and\nsupervision type (process-supervised PRMs vs. outcome-only ORMs), even when\napplied across different roles.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time scaling (TTS) techniques can improve the performance of large\nlanguage models (LLMs) at the expense of additional computation and latency.\nWhile TTS has proven effective in formal domains such as mathematics and\nprogramming \\citep{snell2024scaling, chen2024more}, its value in argumentative\ndomains such as law remains underexplored. We present an empirical study of\nverifier-based TTS methods for legal multiple-choice QA (MCQA) across five\nbenchmarks. Using a family of 7 reward models, we evaluate both outcome-level\n(Best-of-$N$) and process-level (tree search) verification under realistic\nlow-$N$ budgets. Our analysis systematically investigates how verifier utility\nis affected by key properties such as domain specialization, model size, and\nsupervision type (process-supervised PRMs vs. outcome-only ORMs), even when\napplied across different roles."
                },
                "authors": [
                    {
                        "name": "Davide Romano"
                    },
                    {
                        "name": "Jonathan Schwarz"
                    },
                    {
                        "name": "Daniele Giofré"
                    }
                ],
                "author_detail": {
                    "name": "Daniele Giofré"
                },
                "author": "Daniele Giofré",
                "arxiv_comment": "Accepted to EMNLP - NLLP Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25623v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25623v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09810v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09810v2",
                "updated": "2025-10-29T15:27:12Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    15,
                    27,
                    12,
                    2,
                    302,
                    0
                ],
                "published": "2025-09-11T19:28:56Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    19,
                    28,
                    56,
                    3,
                    254,
                    0
                ],
                "title": "Towards a Common Framework for Autoformalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards a Common Framework for Autoformalization"
                },
                "summary": "Autoformalization has emerged as a term referring to the automation of\nformalization - specifically, the formalization of mathematics using\ninteractive theorem provers (proof assistants). Its rapid development has been\ndriven by progress in deep learning, especially large language models (LLMs).\nMore recently, the term has expanded beyond mathematics to describe the broader\ntask of translating informal input into formal logical representations. At the\nsame time, a growing body of research explores using LLMs to translate informal\nlanguage into formal representations for reasoning, planning, and knowledge\nrepresentation - often without explicitly referring to this process as\nautoformalization. As a result, despite addressing similar tasks, the largely\nindependent development of these research areas has limited opportunities for\nshared methodologies, benchmarks, and theoretical frameworks that could\naccelerate progress. The goal of this paper is to review - explicit or implicit\n- instances of what can be considered autoformalization and to propose a\nunified framework, encouraging cross-pollination between different fields to\nadvance the development of next generation AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoformalization has emerged as a term referring to the automation of\nformalization - specifically, the formalization of mathematics using\ninteractive theorem provers (proof assistants). Its rapid development has been\ndriven by progress in deep learning, especially large language models (LLMs).\nMore recently, the term has expanded beyond mathematics to describe the broader\ntask of translating informal input into formal logical representations. At the\nsame time, a growing body of research explores using LLMs to translate informal\nlanguage into formal representations for reasoning, planning, and knowledge\nrepresentation - often without explicitly referring to this process as\nautoformalization. As a result, despite addressing similar tasks, the largely\nindependent development of these research areas has limited opportunities for\nshared methodologies, benchmarks, and theoretical frameworks that could\naccelerate progress. The goal of this paper is to review - explicit or implicit\n- instances of what can be considered autoformalization and to propose a\nunified framework, encouraging cross-pollination between different fields to\nadvance the development of next generation AI systems."
                },
                "authors": [
                    {
                        "name": "Agnieszka Mensfelt"
                    },
                    {
                        "name": "David Tena Cucala"
                    },
                    {
                        "name": "Santiago Franco"
                    },
                    {
                        "name": "Angeliki Koutsoukou-Argyraki"
                    },
                    {
                        "name": "Vince Trencsenyi"
                    },
                    {
                        "name": "Kostas Stathis"
                    }
                ],
                "author_detail": {
                    "name": "Kostas Stathis"
                },
                "author": "Kostas Stathis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09810v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09810v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25621v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25621v1",
                "updated": "2025-10-29T15:25:34Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    15,
                    25,
                    34,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T15:25:34Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    15,
                    25,
                    34,
                    2,
                    302,
                    0
                ],
                "title": "FARSIQA: Faithful and Advanced RAG System for Islamic Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FARSIQA: Faithful and Advanced RAG System for Islamic Question Answering"
                },
                "summary": "The advent of Large Language Models (LLMs) has revolutionized Natural\nLanguage Processing, yet their application in high-stakes, specialized domains\nlike religious question answering is hindered by challenges like hallucination\nand unfaithfulness to authoritative sources. This issue is particularly\ncritical for the Persian-speaking Muslim community, where accuracy and\ntrustworthiness are paramount. Existing Retrieval-Augmented Generation (RAG)\nsystems, relying on simplistic single-pass pipelines, fall short on complex,\nmulti-hop queries requiring multi-step reasoning and evidence aggregation. To\naddress this gap, we introduce FARSIQA, a novel, end-to-end system for Faithful\nAdvanced Question Answering in the Persian Islamic domain. FARSIQA is built\nupon our innovative FAIR-RAG architecture: a Faithful, Adaptive, Iterative\nRefinement framework for RAG. FAIR-RAG employs a dynamic, self-correcting\nprocess: it adaptively decomposes complex queries, assesses evidence\nsufficiency, and enters an iterative loop to generate sub-queries,\nprogressively filling information gaps. Operating on a curated knowledge base\nof over one million authoritative Islamic documents, FARSIQA demonstrates\nsuperior performance. Rigorous evaluation on the challenging IslamicPCQA\nbenchmark shows state-of-the-art performance: the system achieves a remarkable\n97.0% in Negative Rejection - a 40-point improvement over baselines - and a\nhigh Answer Correctness score of 74.3%. Our work establishes a new standard for\nPersian Islamic QA and validates that our iterative, adaptive architecture is\ncrucial for building faithful, reliable AI systems in sensitive domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of Large Language Models (LLMs) has revolutionized Natural\nLanguage Processing, yet their application in high-stakes, specialized domains\nlike religious question answering is hindered by challenges like hallucination\nand unfaithfulness to authoritative sources. This issue is particularly\ncritical for the Persian-speaking Muslim community, where accuracy and\ntrustworthiness are paramount. Existing Retrieval-Augmented Generation (RAG)\nsystems, relying on simplistic single-pass pipelines, fall short on complex,\nmulti-hop queries requiring multi-step reasoning and evidence aggregation. To\naddress this gap, we introduce FARSIQA, a novel, end-to-end system for Faithful\nAdvanced Question Answering in the Persian Islamic domain. FARSIQA is built\nupon our innovative FAIR-RAG architecture: a Faithful, Adaptive, Iterative\nRefinement framework for RAG. FAIR-RAG employs a dynamic, self-correcting\nprocess: it adaptively decomposes complex queries, assesses evidence\nsufficiency, and enters an iterative loop to generate sub-queries,\nprogressively filling information gaps. Operating on a curated knowledge base\nof over one million authoritative Islamic documents, FARSIQA demonstrates\nsuperior performance. Rigorous evaluation on the challenging IslamicPCQA\nbenchmark shows state-of-the-art performance: the system achieves a remarkable\n97.0% in Negative Rejection - a 40-point improvement over baselines - and a\nhigh Answer Correctness score of 74.3%. Our work establishes a new standard for\nPersian Islamic QA and validates that our iterative, adaptive architecture is\ncrucial for building faithful, reliable AI systems in sensitive domains."
                },
                "authors": [
                    {
                        "name": "Mohammad Aghajani Asl"
                    },
                    {
                        "name": "Behrooz Minaei Bidgoli"
                    }
                ],
                "author_detail": {
                    "name": "Behrooz Minaei Bidgoli"
                },
                "author": "Behrooz Minaei Bidgoli",
                "arxiv_comment": "37 pages, 5 figures, 10 tables. Keywords: Retrieval-Augmented\n  Generation (RAG), Question Answering (QA), Islamic Knowledge Base, Faithful\n  AI, Persian NLP, Multi-hop Reasoning, Large Language Models (LLMs)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25621v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25621v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50, 68T05, 68T30",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; H.3.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.17220v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.17220v3",
                "updated": "2025-10-29T15:19:12Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    15,
                    19,
                    12,
                    2,
                    302,
                    0
                ],
                "published": "2024-05-27T14:37:01Z",
                "published_parsed": [
                    2024,
                    5,
                    27,
                    14,
                    37,
                    1,
                    0,
                    148,
                    0
                ],
                "title": "RLAIF-V: Open-Source AI Feedback Leads to Super GPT-4V Trustworthiness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RLAIF-V: Open-Source AI Feedback Leads to Super GPT-4V Trustworthiness"
                },
                "summary": "Traditional feedback learning for hallucination reduction relies on\nlabor-intensive manual labeling or expensive proprietary models. This leaves\nthe community without foundational knowledge about how to build high-quality\nfeedback with open-source MLLMs. In this work, we introduce RLAIF-V, a novel\nframework that aligns MLLMs in a fully open-source paradigm. RLAIF-V maximally\nexplores open-source MLLMs from two perspectives, including high-quality\nfeedback data generation for preference learning and self-feedback guidance for\ninference-time scaling. Extensive experiments on six benchmarks in both\nautomatic and human evaluation show that RLAIF-V substantially enhances the\ntrustworthiness of models at both preference learning and inference time.\nRLAIF-V 7B reduces object hallucination by 80.7\\% and overall hallucination by\n33.7\\%. Remarkably, RLAIF-V 12B further reveals the self-alignment potential of\nopen-source MLLMs, where the model can learn from feedback of itself to achieve\nsuper GPT-4V trustworthiness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional feedback learning for hallucination reduction relies on\nlabor-intensive manual labeling or expensive proprietary models. This leaves\nthe community without foundational knowledge about how to build high-quality\nfeedback with open-source MLLMs. In this work, we introduce RLAIF-V, a novel\nframework that aligns MLLMs in a fully open-source paradigm. RLAIF-V maximally\nexplores open-source MLLMs from two perspectives, including high-quality\nfeedback data generation for preference learning and self-feedback guidance for\ninference-time scaling. Extensive experiments on six benchmarks in both\nautomatic and human evaluation show that RLAIF-V substantially enhances the\ntrustworthiness of models at both preference learning and inference time.\nRLAIF-V 7B reduces object hallucination by 80.7\\% and overall hallucination by\n33.7\\%. Remarkably, RLAIF-V 12B further reveals the self-alignment potential of\nopen-source MLLMs, where the model can learn from feedback of itself to achieve\nsuper GPT-4V trustworthiness."
                },
                "authors": [
                    {
                        "name": "Tianyu Yu"
                    },
                    {
                        "name": "Haoye Zhang"
                    },
                    {
                        "name": "Qiming Li"
                    },
                    {
                        "name": "Qixin Xu"
                    },
                    {
                        "name": "Yuan Yao"
                    },
                    {
                        "name": "Da Chen"
                    },
                    {
                        "name": "Xiaoman Lu"
                    },
                    {
                        "name": "Ganqu Cui"
                    },
                    {
                        "name": "Yunkai Dang"
                    },
                    {
                        "name": "Taiwen He"
                    },
                    {
                        "name": "Xiaocheng Feng"
                    },
                    {
                        "name": "Jun Song"
                    },
                    {
                        "name": "Bo Zheng"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Tat-Seng Chua"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "arxiv_comment": "Project Website: https://github.com/RLHF-V/RLAIF-V",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.17220v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.17220v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25612v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25612v1",
                "updated": "2025-10-29T15:17:31Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    15,
                    17,
                    31,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T15:17:31Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    15,
                    17,
                    31,
                    2,
                    302,
                    0
                ],
                "title": "Counterfactual-based Agent Influence Ranker for Agentic AI Workflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Counterfactual-based Agent Influence Ranker for Agentic AI Workflows"
                },
                "summary": "An Agentic AI Workflow (AAW), also known as an LLM-based multi-agent system,\nis an autonomous system that assembles several LLM-based agents to work\ncollaboratively towards a shared goal. The high autonomy, widespread adoption,\nand growing interest in such AAWs highlight the need for a deeper understanding\nof their operations, from both quality and security aspects. To this day, there\nare no existing methods to assess the influence of each agent on the AAW's\nfinal output. Adopting techniques from related fields is not feasible since\nexisting methods perform only static structural analysis, which is unsuitable\nfor inference time execution. We present Counterfactual-based Agent Influence\nRanker (CAIR) - the first method for assessing the influence level of each\nagent on the AAW's output and determining which agents are the most\ninfluential. By performing counterfactual analysis, CAIR provides a\ntask-agnostic analysis that can be used both offline and at inference time. We\nevaluate CAIR using an AAWs dataset of our creation, containing 30 different\nuse cases with 230 different functionalities. Our evaluation showed that CAIR\nproduces consistent rankings, outperforms baseline methods, and can easily\nenhance the effectiveness and relevancy of downstream tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Agentic AI Workflow (AAW), also known as an LLM-based multi-agent system,\nis an autonomous system that assembles several LLM-based agents to work\ncollaboratively towards a shared goal. The high autonomy, widespread adoption,\nand growing interest in such AAWs highlight the need for a deeper understanding\nof their operations, from both quality and security aspects. To this day, there\nare no existing methods to assess the influence of each agent on the AAW's\nfinal output. Adopting techniques from related fields is not feasible since\nexisting methods perform only static structural analysis, which is unsuitable\nfor inference time execution. We present Counterfactual-based Agent Influence\nRanker (CAIR) - the first method for assessing the influence level of each\nagent on the AAW's output and determining which agents are the most\ninfluential. By performing counterfactual analysis, CAIR provides a\ntask-agnostic analysis that can be used both offline and at inference time. We\nevaluate CAIR using an AAWs dataset of our creation, containing 30 different\nuse cases with 230 different functionalities. Our evaluation showed that CAIR\nproduces consistent rankings, outperforms baseline methods, and can easily\nenhance the effectiveness and relevancy of downstream tasks."
                },
                "authors": [
                    {
                        "name": "Amit Giloni"
                    },
                    {
                        "name": "Chiara Picardi"
                    },
                    {
                        "name": "Roy Betser"
                    },
                    {
                        "name": "Shamik Bose"
                    },
                    {
                        "name": "Aishvariya Priya Rathina Sabapathy"
                    },
                    {
                        "name": "Roman Vainshtein"
                    }
                ],
                "author_detail": {
                    "name": "Roman Vainshtein"
                },
                "author": "Roman Vainshtein",
                "arxiv_comment": "Accepted to EMNLP 2025, 27 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25612v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25612v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25607v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25607v1",
                "updated": "2025-10-29T15:16:11Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    15,
                    16,
                    11,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T15:16:11Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    15,
                    16,
                    11,
                    2,
                    302,
                    0
                ],
                "title": "Inference on Welfare and Value Functionals under Optimal Treatment\n  Assignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference on Welfare and Value Functionals under Optimal Treatment\n  Assignment"
                },
                "summary": "We provide theoretical results for the estimation and inference of a class of\nwelfare and value functionals of the nonparametric conditional average\ntreatment effect (CATE) function under optimal treatment assignment, i.e.,\ntreatment is assigned to an observed type if and only if its CATE is\nnonnegative. For the optimal welfare functional defined as the average value of\nCATE on the subpopulation with nonnegative CATE, we establish the $\\sqrt{n}$\nasymptotic normality of the semiparametric plug-in estimators and provide an\nanalytical asymptotic variance formula. For more general value functionals, we\nshow that the plug-in estimators are typically asymptotically normal at the\n1-dimensional nonparametric estimation rate, and we provide a consistent\nvariance estimator based on the sieve Riesz representer, as well as a proposed\ncomputational procedure for numerical integration on submanifolds. The key\nreason underlying the different convergence rates for the welfare functional\nversus the general value functional lies in that, on the boundary subpopulation\nfor whom CATE is zero, the integrand vanishes for the welfare functional but\ndoes not for general value functionals. We demonstrate in Monte Carlo\nsimulations the good finite-sample performance of our estimation and inference\nprocedures, and conduct an empirical application of our methods on the\neffectiveness of job training programs on earnings using the JTPA data set.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We provide theoretical results for the estimation and inference of a class of\nwelfare and value functionals of the nonparametric conditional average\ntreatment effect (CATE) function under optimal treatment assignment, i.e.,\ntreatment is assigned to an observed type if and only if its CATE is\nnonnegative. For the optimal welfare functional defined as the average value of\nCATE on the subpopulation with nonnegative CATE, we establish the $\\sqrt{n}$\nasymptotic normality of the semiparametric plug-in estimators and provide an\nanalytical asymptotic variance formula. For more general value functionals, we\nshow that the plug-in estimators are typically asymptotically normal at the\n1-dimensional nonparametric estimation rate, and we provide a consistent\nvariance estimator based on the sieve Riesz representer, as well as a proposed\ncomputational procedure for numerical integration on submanifolds. The key\nreason underlying the different convergence rates for the welfare functional\nversus the general value functional lies in that, on the boundary subpopulation\nfor whom CATE is zero, the integrand vanishes for the welfare functional but\ndoes not for general value functionals. We demonstrate in Monte Carlo\nsimulations the good finite-sample performance of our estimation and inference\nprocedures, and conduct an empirical application of our methods on the\neffectiveness of job training programs on earnings using the JTPA data set."
                },
                "authors": [
                    {
                        "name": "Xiaohong Chen"
                    },
                    {
                        "name": "Zhenxiao Chen"
                    },
                    {
                        "name": "Wayne Yuan Gao"
                    }
                ],
                "author_detail": {
                    "name": "Wayne Yuan Gao"
                },
                "author": "Wayne Yuan Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25607v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25607v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25602v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25602v1",
                "updated": "2025-10-29T15:11:53Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    15,
                    11,
                    53,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T15:11:53Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    15,
                    11,
                    53,
                    2,
                    302,
                    0
                ],
                "title": "INT v.s. FP: A Comprehensive Study of Fine-Grained Low-bit Quantization\n  Formats",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "INT v.s. FP: A Comprehensive Study of Fine-Grained Low-bit Quantization\n  Formats"
                },
                "summary": "Modern AI hardware, such as Nvidia's Blackwell architecture, is increasingly\nembracing low-precision floating-point (FP) formats to handle the pervasive\nactivation outliers in Large Language Models (LLMs). Despite this industry\ntrend, a unified comparison of FP and integer (INT) quantization across varying\ngranularities has been missing, leaving algorithm and hardware co-design\nwithout clear guidance. This paper fills that gap by systematically\ninvestigating the trade-offs between FP and INT formats. We reveal a critical\nperformance crossover: while FP excels in coarse-grained quantization, the\ncomparison at fine-grained (block-wise) levels is more nuanced. Our\ncomprehensive comparison demonstrates that for popular 8-bit fine-grained\nformats (e.g., MX with block size 32), MXINT8 is superior to its FP counterpart\nin both algorithmic accuracy and hardware efficiency. However, for 4-bit\nformats, FP (e.g., MXFP4, NVFP4) often holds an accuracy advantage , though we\nshow that NVINT4 can surpass NVFP4 when outlier-mitigation techniques like\nHadamard rotation are applied. We also introduce a symmetric clipping method\nthat resolves gradient bias in fine-grained low-bit INT training, enabling\nnearly lossless performance for MXINT8 training. These findings challenge the\ncurrent hardware trajectory, demonstrating that a one-size-fits-all FP approach\nis suboptimal and advocating that fine-grained INT formats, particularly\nMXINT8, offer a better balance of accuracy, power, and efficiency for future AI\naccelerators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern AI hardware, such as Nvidia's Blackwell architecture, is increasingly\nembracing low-precision floating-point (FP) formats to handle the pervasive\nactivation outliers in Large Language Models (LLMs). Despite this industry\ntrend, a unified comparison of FP and integer (INT) quantization across varying\ngranularities has been missing, leaving algorithm and hardware co-design\nwithout clear guidance. This paper fills that gap by systematically\ninvestigating the trade-offs between FP and INT formats. We reveal a critical\nperformance crossover: while FP excels in coarse-grained quantization, the\ncomparison at fine-grained (block-wise) levels is more nuanced. Our\ncomprehensive comparison demonstrates that for popular 8-bit fine-grained\nformats (e.g., MX with block size 32), MXINT8 is superior to its FP counterpart\nin both algorithmic accuracy and hardware efficiency. However, for 4-bit\nformats, FP (e.g., MXFP4, NVFP4) often holds an accuracy advantage , though we\nshow that NVINT4 can surpass NVFP4 when outlier-mitigation techniques like\nHadamard rotation are applied. We also introduce a symmetric clipping method\nthat resolves gradient bias in fine-grained low-bit INT training, enabling\nnearly lossless performance for MXINT8 training. These findings challenge the\ncurrent hardware trajectory, demonstrating that a one-size-fits-all FP approach\nis suboptimal and advocating that fine-grained INT formats, particularly\nMXINT8, offer a better balance of accuracy, power, and efficiency for future AI\naccelerators."
                },
                "authors": [
                    {
                        "name": "Mengzhao Chen"
                    },
                    {
                        "name": "Meng Wu"
                    },
                    {
                        "name": "Hui Jin"
                    },
                    {
                        "name": "Zhihang Yuan"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Chaoyi Zhang"
                    },
                    {
                        "name": "Yunshui Li"
                    },
                    {
                        "name": "Jie Huang"
                    },
                    {
                        "name": "Jin Ma"
                    },
                    {
                        "name": "Zeyue Xue"
                    },
                    {
                        "name": "Zhiheng Liu"
                    },
                    {
                        "name": "Xingyan Bin"
                    },
                    {
                        "name": "Ping Luo"
                    }
                ],
                "author_detail": {
                    "name": "Ping Luo"
                },
                "author": "Ping Luo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25602v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25602v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25595v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25595v1",
                "updated": "2025-10-29T15:03:53Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    15,
                    3,
                    53,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T15:03:53Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    15,
                    3,
                    53,
                    2,
                    302,
                    0
                ],
                "title": "Communication and Verification in LLM Agents towards Collaboration under\n  Information Asymmetry",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Communication and Verification in LLM Agents towards Collaboration under\n  Information Asymmetry"
                },
                "summary": "While Large Language Model (LLM) agents are often approached from the angle\nof action planning/generation to accomplish a goal (e.g., given by language\ndescriptions), their abilities to collaborate with each other to achieve a\njoint goal are not well explored. To address this limitation, this paper\nstudies LLM agents in task collaboration, particularly under the condition of\ninformation asymmetry, where agents have disparities in their knowledge and\nskills and need to work together to complete a shared task. We extend Einstein\nPuzzles, a classical symbolic puzzle, to a table-top game. In this game, two\nLLM agents must reason, communicate, and act to satisfy spatial and relational\nconstraints required to solve the puzzle. We apply a fine-tuning-plus-verifier\nframework in which LLM agents are equipped with various communication\nstrategies and verification signals from the environment. Empirical results\nhighlight the critical importance of aligned communication, especially when\nagents possess both information-seeking and -providing capabilities.\nInterestingly, agents without communication can still achieve high task\nperformance; however, further analysis reveals a lack of true rule\nunderstanding and lower trust from human evaluators. Instead, by integrating an\nenvironment-based verifier, we enhance agents' ability to comprehend task rules\nand complete tasks, promoting both safer and more interpretable collaboration\nin AI systems. https://github.com/Roihn/EinsteinPuzzles",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Large Language Model (LLM) agents are often approached from the angle\nof action planning/generation to accomplish a goal (e.g., given by language\ndescriptions), their abilities to collaborate with each other to achieve a\njoint goal are not well explored. To address this limitation, this paper\nstudies LLM agents in task collaboration, particularly under the condition of\ninformation asymmetry, where agents have disparities in their knowledge and\nskills and need to work together to complete a shared task. We extend Einstein\nPuzzles, a classical symbolic puzzle, to a table-top game. In this game, two\nLLM agents must reason, communicate, and act to satisfy spatial and relational\nconstraints required to solve the puzzle. We apply a fine-tuning-plus-verifier\nframework in which LLM agents are equipped with various communication\nstrategies and verification signals from the environment. Empirical results\nhighlight the critical importance of aligned communication, especially when\nagents possess both information-seeking and -providing capabilities.\nInterestingly, agents without communication can still achieve high task\nperformance; however, further analysis reveals a lack of true rule\nunderstanding and lower trust from human evaluators. Instead, by integrating an\nenvironment-based verifier, we enhance agents' ability to comprehend task rules\nand complete tasks, promoting both safer and more interpretable collaboration\nin AI systems. https://github.com/Roihn/EinsteinPuzzles"
                },
                "authors": [
                    {
                        "name": "Run Peng"
                    },
                    {
                        "name": "Ziqiao Ma"
                    },
                    {
                        "name": "Amy Pang"
                    },
                    {
                        "name": "Sikai Li"
                    },
                    {
                        "name": "Zhang Xi-Jia"
                    },
                    {
                        "name": "Yingzhuo Yu"
                    },
                    {
                        "name": "Cristian-Paul Bara"
                    },
                    {
                        "name": "Joyce Chai"
                    }
                ],
                "author_detail": {
                    "name": "Joyce Chai"
                },
                "author": "Joyce Chai",
                "arxiv_comment": "Workshop on Multi-Agent System @ ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25595v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25595v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13738v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13738v2",
                "updated": "2025-10-29T15:00:42Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    15,
                    0,
                    42,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-15T16:45:59Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    16,
                    45,
                    59,
                    2,
                    288,
                    0
                ],
                "title": "HyMiRec: A Hybrid Multi-interest Learning Framework for LLM-based\n  Sequential Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HyMiRec: A Hybrid Multi-interest Learning Framework for LLM-based\n  Sequential Recommendation"
                },
                "summary": "Large language models (LLMs) have recently demonstrated strong potential for\nsequential recommendation. However, current LLM-based approaches face critical\nlimitations in modeling users' long-term and diverse interests. First, due to\ninference latency and feature fetching bandwidth constraints, existing methods\ntypically truncate user behavior sequences to include only the most recent\ninteractions, resulting in the loss of valuable long-range preference signals.\nSecond, most current methods rely on next-item prediction with a single\npredicted embedding, overlooking the multifaceted nature of user interests and\nlimiting recommendation diversity. To address these challenges, we propose\nHyMiRec, a hybrid multi-interest sequential recommendation framework, which\nleverages a lightweight recommender to extracts coarse interest embeddings from\nlong user sequences and an LLM-based recommender to captures refined interest\nembeddings. To alleviate the overhead of fetching features, we introduce a\nresidual codebook based on cosine similarity, enabling efficient compression\nand reuse of user history embeddings. To model the diverse preferences of\nusers, we design a disentangled multi-interest learning module, which leverages\nmultiple interest queries to learn disentangles multiple interest signals\nadaptively, allowing the model to capture different facets of user intent.\nExtensive experiments are conducted on both benchmark datasets and a collected\nindustrial dataset, demonstrating our effectiveness over existing\nstate-of-the-art methods. Furthermore, online A/B testing shows that HyMiRec\nbrings consistent improvements in real-world recommendation systems. Code is\navailable at https://github.com/FireRedTeam/FireRedSeqRec.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have recently demonstrated strong potential for\nsequential recommendation. However, current LLM-based approaches face critical\nlimitations in modeling users' long-term and diverse interests. First, due to\ninference latency and feature fetching bandwidth constraints, existing methods\ntypically truncate user behavior sequences to include only the most recent\ninteractions, resulting in the loss of valuable long-range preference signals.\nSecond, most current methods rely on next-item prediction with a single\npredicted embedding, overlooking the multifaceted nature of user interests and\nlimiting recommendation diversity. To address these challenges, we propose\nHyMiRec, a hybrid multi-interest sequential recommendation framework, which\nleverages a lightweight recommender to extracts coarse interest embeddings from\nlong user sequences and an LLM-based recommender to captures refined interest\nembeddings. To alleviate the overhead of fetching features, we introduce a\nresidual codebook based on cosine similarity, enabling efficient compression\nand reuse of user history embeddings. To model the diverse preferences of\nusers, we design a disentangled multi-interest learning module, which leverages\nmultiple interest queries to learn disentangles multiple interest signals\nadaptively, allowing the model to capture different facets of user intent.\nExtensive experiments are conducted on both benchmark datasets and a collected\nindustrial dataset, demonstrating our effectiveness over existing\nstate-of-the-art methods. Furthermore, online A/B testing shows that HyMiRec\nbrings consistent improvements in real-world recommendation systems. Code is\navailable at https://github.com/FireRedTeam/FireRedSeqRec."
                },
                "authors": [
                    {
                        "name": "Jingyi Zhou"
                    },
                    {
                        "name": "Cheng Chen"
                    },
                    {
                        "name": "Kai Zuo"
                    },
                    {
                        "name": "Manjie Xu"
                    },
                    {
                        "name": "Zhendong Fu"
                    },
                    {
                        "name": "Yibo Chen"
                    },
                    {
                        "name": "Xu Tang"
                    },
                    {
                        "name": "Yao Hu"
                    }
                ],
                "author_detail": {
                    "name": "Yao Hu"
                },
                "author": "Yao Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13738v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13738v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25590v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25590v1",
                "updated": "2025-10-29T14:58:37Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    14,
                    58,
                    37,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T14:58:37Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    14,
                    58,
                    37,
                    2,
                    302,
                    0
                ],
                "title": "RegionE: Adaptive Region-Aware Generation for Efficient Image Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RegionE: Adaptive Region-Aware Generation for Efficient Image Editing"
                },
                "summary": "Recently, instruction-based image editing (IIE) has received widespread\nattention. In practice, IIE often modifies only specific regions of an image,\nwhile the remaining areas largely remain unchanged. Although these two types of\nregions differ significantly in generation difficulty and computational\nredundancy, existing IIE models do not account for this distinction, instead\napplying a uniform generation process across the entire image. This motivates\nus to propose RegionE, an adaptive, region-aware generation framework that\naccelerates IIE tasks without additional training. Specifically, the RegionE\nframework consists of three main components: 1) Adaptive Region Partition. We\nobserved that the trajectory of unedited regions is straight, allowing for\nmulti-step denoised predictions to be inferred in a single step. Therefore, in\nthe early denoising stages, we partition the image into edited and unedited\nregions based on the difference between the final estimated result and the\nreference image. 2) Region-Aware Generation. After distinguishing the regions,\nwe replace multi-step denoising with one-step prediction for unedited areas.\nFor edited regions, the trajectory is curved, requiring local iterative\ndenoising. To improve the efficiency and quality of local iterative generation,\nwe propose the Region-Instruction KV Cache, which reduces computational cost\nwhile incorporating global information. 3) Adaptive Velocity Decay Cache.\nObserving that adjacent timesteps in edited regions exhibit strong velocity\nsimilarity, we further propose an adaptive velocity decay cache to accelerate\nthe local denoising process. We applied RegionE to state-of-the-art IIE base\nmodels, including Step1X-Edit, FLUX.1 Kontext, and Qwen-Image-Edit. RegionE\nachieved acceleration factors of 2.57, 2.41, and 2.06. Evaluations by GPT-4o\nconfirmed that semantic and perceptual fidelity were well preserved.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, instruction-based image editing (IIE) has received widespread\nattention. In practice, IIE often modifies only specific regions of an image,\nwhile the remaining areas largely remain unchanged. Although these two types of\nregions differ significantly in generation difficulty and computational\nredundancy, existing IIE models do not account for this distinction, instead\napplying a uniform generation process across the entire image. This motivates\nus to propose RegionE, an adaptive, region-aware generation framework that\naccelerates IIE tasks without additional training. Specifically, the RegionE\nframework consists of three main components: 1) Adaptive Region Partition. We\nobserved that the trajectory of unedited regions is straight, allowing for\nmulti-step denoised predictions to be inferred in a single step. Therefore, in\nthe early denoising stages, we partition the image into edited and unedited\nregions based on the difference between the final estimated result and the\nreference image. 2) Region-Aware Generation. After distinguishing the regions,\nwe replace multi-step denoising with one-step prediction for unedited areas.\nFor edited regions, the trajectory is curved, requiring local iterative\ndenoising. To improve the efficiency and quality of local iterative generation,\nwe propose the Region-Instruction KV Cache, which reduces computational cost\nwhile incorporating global information. 3) Adaptive Velocity Decay Cache.\nObserving that adjacent timesteps in edited regions exhibit strong velocity\nsimilarity, we further propose an adaptive velocity decay cache to accelerate\nthe local denoising process. We applied RegionE to state-of-the-art IIE base\nmodels, including Step1X-Edit, FLUX.1 Kontext, and Qwen-Image-Edit. RegionE\nachieved acceleration factors of 2.57, 2.41, and 2.06. Evaluations by GPT-4o\nconfirmed that semantic and perceptual fidelity were well preserved."
                },
                "authors": [
                    {
                        "name": "Pengtao Chen"
                    },
                    {
                        "name": "Xianfang Zeng"
                    },
                    {
                        "name": "Maosen Zhao"
                    },
                    {
                        "name": "Mingzhu Shen"
                    },
                    {
                        "name": "Peng Ye"
                    },
                    {
                        "name": "Bangyin Xiang"
                    },
                    {
                        "name": "Zhibo Wang"
                    },
                    {
                        "name": "Wei Cheng"
                    },
                    {
                        "name": "Gang Yu"
                    },
                    {
                        "name": "Tao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tao Chen"
                },
                "author": "Tao Chen",
                "arxiv_comment": "26 pages, 10 figures, 18 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25590v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25590v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25588v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25588v1",
                "updated": "2025-10-29T14:54:22Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    14,
                    54,
                    22,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T14:54:22Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    14,
                    54,
                    22,
                    2,
                    302,
                    0
                ],
                "title": "Standardization of Psychiatric Diagnoses -- Role of Fine-tuned LLM\n  Consortium and OpenAI-gpt-oss Reasoning LLM Enabled Decision Support System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Standardization of Psychiatric Diagnoses -- Role of Fine-tuned LLM\n  Consortium and OpenAI-gpt-oss Reasoning LLM Enabled Decision Support System"
                },
                "summary": "The diagnosis of most mental disorders, including psychiatric evaluations,\nprimarily depends on dialogues between psychiatrists and patients. This\nsubjective process can lead to variability in diagnoses across clinicians and\npatients, resulting in inconsistencies and challenges in achieving reliable\noutcomes. To address these issues and standardize psychiatric diagnoses, we\npropose a Fine-Tuned Large Language Model (LLM) Consortium and OpenAI-gpt-oss\nReasoning LLM-enabled Decision Support System for the clinical diagnosis of\nmental disorders. Our approach leverages fine-tuned LLMs trained on\nconversational datasets involving psychiatrist-patient interactions focused on\nmental health conditions (e.g., depression). The diagnostic predictions from\nindividual models are aggregated through a consensus-based decision-making\nprocess, refined by the OpenAI-gpt-oss reasoning LLM. We propose a novel method\nfor deploying LLM agents that orchestrate communication between the LLM\nconsortium and the reasoning LLM, ensuring transparency, reliability, and\nresponsible AI across the entire diagnostic workflow. Experimental results\ndemonstrate the transformative potential of combining fine-tuned LLMs with a\nreasoning model to create a robust and highly accurate diagnostic system for\nmental health assessment. A prototype of the proposed platform, integrating\nthree fine-tuned LLMs with the OpenAI-gpt-oss reasoning LLM, was developed in\ncollaboration with the U.S. Army Medical Research Team in Norfolk, Virginia,\nUSA. To the best of our knowledge, this work represents the first application\nof a fine-tuned LLM consortium integrated with a reasoning LLM for clinical\nmental health diagnosis paving the way for next-generation AI-powered eHealth\nsystems aimed at standardizing psychiatric diagnoses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The diagnosis of most mental disorders, including psychiatric evaluations,\nprimarily depends on dialogues between psychiatrists and patients. This\nsubjective process can lead to variability in diagnoses across clinicians and\npatients, resulting in inconsistencies and challenges in achieving reliable\noutcomes. To address these issues and standardize psychiatric diagnoses, we\npropose a Fine-Tuned Large Language Model (LLM) Consortium and OpenAI-gpt-oss\nReasoning LLM-enabled Decision Support System for the clinical diagnosis of\nmental disorders. Our approach leverages fine-tuned LLMs trained on\nconversational datasets involving psychiatrist-patient interactions focused on\nmental health conditions (e.g., depression). The diagnostic predictions from\nindividual models are aggregated through a consensus-based decision-making\nprocess, refined by the OpenAI-gpt-oss reasoning LLM. We propose a novel method\nfor deploying LLM agents that orchestrate communication between the LLM\nconsortium and the reasoning LLM, ensuring transparency, reliability, and\nresponsible AI across the entire diagnostic workflow. Experimental results\ndemonstrate the transformative potential of combining fine-tuned LLMs with a\nreasoning model to create a robust and highly accurate diagnostic system for\nmental health assessment. A prototype of the proposed platform, integrating\nthree fine-tuned LLMs with the OpenAI-gpt-oss reasoning LLM, was developed in\ncollaboration with the U.S. Army Medical Research Team in Norfolk, Virginia,\nUSA. To the best of our knowledge, this work represents the first application\nof a fine-tuned LLM consortium integrated with a reasoning LLM for clinical\nmental health diagnosis paving the way for next-generation AI-powered eHealth\nsystems aimed at standardizing psychiatric diagnoses."
                },
                "authors": [
                    {
                        "name": "Eranga Bandara"
                    },
                    {
                        "name": "Ross Gore"
                    },
                    {
                        "name": "Atmaram Yarlagadda"
                    },
                    {
                        "name": "Anita H. Clayton"
                    },
                    {
                        "name": "Preston Samuel"
                    },
                    {
                        "name": "Christopher K. Rhea"
                    },
                    {
                        "name": "Sachin Shetty"
                    }
                ],
                "author_detail": {
                    "name": "Sachin Shetty"
                },
                "author": "Sachin Shetty",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25588v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25588v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.12484v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.12484v4",
                "updated": "2025-10-29T14:52:49Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    14,
                    52,
                    49,
                    2,
                    302,
                    0
                ],
                "published": "2025-06-14T12:49:51Z",
                "published_parsed": [
                    2025,
                    6,
                    14,
                    12,
                    49,
                    51,
                    5,
                    165,
                    0
                ],
                "title": "Robust LLM Unlearning with MUDMAN: Meta-Unlearning with Disruption\n  Masking And Normalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust LLM Unlearning with MUDMAN: Meta-Unlearning with Disruption\n  Masking And Normalization"
                },
                "summary": "Language models can retain dangerous knowledge and skills even after\nextensive safety fine-tuning, posing both misuse and misalignment risks. Recent\nstudies show that even specialized unlearning methods can be easily reversed.\nTo address this, we systematically evaluate many existing and novel components\nof unlearning methods and identify ones crucial for irreversible unlearning.\n  We introduce Disruption Masking, a technique in which we only allow updating\nweights, where the signs of the unlearning gradient and the retaining gradient\nare the same. This ensures all updates are non-disruptive.\n  Additionally, we identify the need for normalizing the unlearning gradients,\nand also confirm the usefulness of meta-learning. We combine these insights\ninto MUDMAN (Meta-Unlearning with Disruption Masking and Normalization) and\nvalidate its effectiveness at preventing the recovery of dangerous\ncapabilities. MUDMAN outperforms the prior TAR method by 40%, setting a new\nstate-of-the-art for robust unlearning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language models can retain dangerous knowledge and skills even after\nextensive safety fine-tuning, posing both misuse and misalignment risks. Recent\nstudies show that even specialized unlearning methods can be easily reversed.\nTo address this, we systematically evaluate many existing and novel components\nof unlearning methods and identify ones crucial for irreversible unlearning.\n  We introduce Disruption Masking, a technique in which we only allow updating\nweights, where the signs of the unlearning gradient and the retaining gradient\nare the same. This ensures all updates are non-disruptive.\n  Additionally, we identify the need for normalizing the unlearning gradients,\nand also confirm the usefulness of meta-learning. We combine these insights\ninto MUDMAN (Meta-Unlearning with Disruption Masking and Normalization) and\nvalidate its effectiveness at preventing the recovery of dangerous\ncapabilities. MUDMAN outperforms the prior TAR method by 40%, setting a new\nstate-of-the-art for robust unlearning."
                },
                "authors": [
                    {
                        "name": "Filip Sondej"
                    },
                    {
                        "name": "Yushi Yang"
                    },
                    {
                        "name": "Mikołaj Kniejski"
                    },
                    {
                        "name": "Marcel Windys"
                    }
                ],
                "author_detail": {
                    "name": "Marcel Windys"
                },
                "author": "Marcel Windys",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.12484v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.12484v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.16714v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.16714v3",
                "updated": "2025-10-29T14:48:21Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    14,
                    48,
                    21,
                    2,
                    302,
                    0
                ],
                "published": "2024-02-26T16:31:28Z",
                "published_parsed": [
                    2024,
                    2,
                    26,
                    16,
                    31,
                    28,
                    0,
                    57,
                    0
                ],
                "title": "Quantum Transformer: Accelerating model inference via quantum linear\n  algebra",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum Transformer: Accelerating model inference via quantum linear\n  algebra"
                },
                "summary": "Powerful generative artificial intelligence from large language models (LLMs)\nharnesses extensive computational resources for inference. In this work, we\ninvestigate the transformer architecture, a key component of these models,\nunder the lens of fault-tolerant quantum computing. We develop quantum\nsubroutines to construct the building blocks in the transformer, including the\nself-attention, residual connection with layer normalization, and feed-forward\nnetwork. As an important subroutine, we show how to efficiently implement the\nHadamard product and element-wise functions of matrices on quantum computers.\nOur algorithm prepares an amplitude encoding of the transformer output, which\ncan be measured for prediction or use in the next layer. We find that the\nmatrix norm of the input sequence plays a dominant role in the quantum\ncomplexity. With numerical experiments on open-source LLMs, including for\nbio-informatics applications, we demonstrate the potential of a quantum speedup\nfor transformer inference in practical regimes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Powerful generative artificial intelligence from large language models (LLMs)\nharnesses extensive computational resources for inference. In this work, we\ninvestigate the transformer architecture, a key component of these models,\nunder the lens of fault-tolerant quantum computing. We develop quantum\nsubroutines to construct the building blocks in the transformer, including the\nself-attention, residual connection with layer normalization, and feed-forward\nnetwork. As an important subroutine, we show how to efficiently implement the\nHadamard product and element-wise functions of matrices on quantum computers.\nOur algorithm prepares an amplitude encoding of the transformer output, which\ncan be measured for prediction or use in the next layer. We find that the\nmatrix norm of the input sequence plays a dominant role in the quantum\ncomplexity. With numerical experiments on open-source LLMs, including for\nbio-informatics applications, we demonstrate the potential of a quantum speedup\nfor transformer inference in practical regimes."
                },
                "authors": [
                    {
                        "name": "Naixu Guo"
                    },
                    {
                        "name": "Zhan Yu"
                    },
                    {
                        "name": "Matthew Choi"
                    },
                    {
                        "name": "Yizhan Han"
                    },
                    {
                        "name": "Aman Agrawal"
                    },
                    {
                        "name": "Kouhei Nakaji"
                    },
                    {
                        "name": "Alán Aspuru-Guzik"
                    },
                    {
                        "name": "Patrick Rebentrost"
                    }
                ],
                "author_detail": {
                    "name": "Patrick Rebentrost"
                },
                "author": "Patrick Rebentrost",
                "arxiv_comment": "45 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.16714v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.16714v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25579v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25579v2",
                "updated": "2025-10-30T11:19:36Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    11,
                    19,
                    36,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-29T14:45:45Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    14,
                    45,
                    45,
                    2,
                    302,
                    0
                ],
                "title": "Population of Binary Black Holes Inferred from One Hundred and Fifty\n  Gravitational Wave Signals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Population of Binary Black Holes Inferred from One Hundred and Fifty\n  Gravitational Wave Signals"
                },
                "summary": "The LIGO-Virgo-KAGRA collaborations have reported gravitational wave signals\nfrom more than 150 binary black holes in the fourth catalog (GWTC-4). Here, we\ninvestigate the population properties of these binary black holes using the\nmixture-model framework Vamana. We present one-dimensional distributions of\nmasses and spins, explore their correlations, and examine their evolution with\nredshift. These features may reflect astrophysical processes associated with\nbinary black hole formation channels, although most remain poorly constrained.\nA notable feature is a peak near $10M_\\odot$ in the primary mass and $8M_\\odot$\nin the chirp mass. Additionally, the primary and secondary masses correlate\nuniquely, producing pronounced peaks in the chirp mass around $14M_\\odot$ and\n$27M_\\odot$. The three peaks are roughly separated by a factor of two. A simple\nexplanation for such well-placed peaks is a hierarchical merger scenario, in\nwhich the first peak arises from mergers of black holes of stellar origin, and\nhigher-mass peaks arise from repeated mergers of black holes from lower-mass\npeaks. Although most binaries do not exhibit the high spins and characteristic\nmass ratios expected from hierarchical mergers, those that do are associated\nwith the peaks observed in the chirp mass distribution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The LIGO-Virgo-KAGRA collaborations have reported gravitational wave signals\nfrom more than 150 binary black holes in the fourth catalog (GWTC-4). Here, we\ninvestigate the population properties of these binary black holes using the\nmixture-model framework Vamana. We present one-dimensional distributions of\nmasses and spins, explore their correlations, and examine their evolution with\nredshift. These features may reflect astrophysical processes associated with\nbinary black hole formation channels, although most remain poorly constrained.\nA notable feature is a peak near $10M_\\odot$ in the primary mass and $8M_\\odot$\nin the chirp mass. Additionally, the primary and secondary masses correlate\nuniquely, producing pronounced peaks in the chirp mass around $14M_\\odot$ and\n$27M_\\odot$. The three peaks are roughly separated by a factor of two. A simple\nexplanation for such well-placed peaks is a hierarchical merger scenario, in\nwhich the first peak arises from mergers of black holes of stellar origin, and\nhigher-mass peaks arise from repeated mergers of black holes from lower-mass\npeaks. Although most binaries do not exhibit the high spins and characteristic\nmass ratios expected from hierarchical mergers, those that do are associated\nwith the peaks observed in the chirp mass distribution."
                },
                "authors": [
                    {
                        "name": "Vaibhav Tiwari"
                    }
                ],
                "author_detail": {
                    "name": "Vaibhav Tiwari"
                },
                "author": "Vaibhav Tiwari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25579v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25579v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25577v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25577v1",
                "updated": "2025-10-29T14:44:44Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    14,
                    44,
                    44,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T14:44:44Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    14,
                    44,
                    44,
                    2,
                    302,
                    0
                ],
                "title": "Lost in Phonation: Voice Quality Variation as an Evaluation Dimension\n  for Speech Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lost in Phonation: Voice Quality Variation as an Evaluation Dimension\n  for Speech Foundation Models"
                },
                "summary": "Recent advances in speech foundation models (SFMs) have enabled the direct\nprocessing of spoken language from raw audio, bypassing intermediate textual\nrepresentations. This capability allows SFMs to be exposed to, and potentially\nrespond to, rich paralinguistic variations embedded in the input speech signal.\nOne under-explored dimension of paralinguistic variation is voice quality,\nencompassing phonation types such as creaky and breathy voice. These phonation\ntypes are known to influence how listeners infer affective state, stance and\nsocial meaning in speech. Existing benchmarks for speech understanding largely\nrely on multiple-choice question answering (MCQA) formats, which are prone to\nfailure and therefore unreliable in capturing the nuanced ways paralinguistic\nfeatures influence model behaviour. In this paper, we probe SFMs through\nopen-ended generation tasks and speech emotion recognition, evaluating whether\nmodel behaviours are consistent across different phonation inputs. We introduce\na new parallel dataset featuring synthesized modifications to voice quality,\ndesigned to evaluate SFM responses to creaky and breathy voice. Our work\nprovides the first examination of SFM sensitivity to these particular\nnon-lexical aspects of speech perception.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in speech foundation models (SFMs) have enabled the direct\nprocessing of spoken language from raw audio, bypassing intermediate textual\nrepresentations. This capability allows SFMs to be exposed to, and potentially\nrespond to, rich paralinguistic variations embedded in the input speech signal.\nOne under-explored dimension of paralinguistic variation is voice quality,\nencompassing phonation types such as creaky and breathy voice. These phonation\ntypes are known to influence how listeners infer affective state, stance and\nsocial meaning in speech. Existing benchmarks for speech understanding largely\nrely on multiple-choice question answering (MCQA) formats, which are prone to\nfailure and therefore unreliable in capturing the nuanced ways paralinguistic\nfeatures influence model behaviour. In this paper, we probe SFMs through\nopen-ended generation tasks and speech emotion recognition, evaluating whether\nmodel behaviours are consistent across different phonation inputs. We introduce\na new parallel dataset featuring synthesized modifications to voice quality,\ndesigned to evaluate SFM responses to creaky and breathy voice. Our work\nprovides the first examination of SFM sensitivity to these particular\nnon-lexical aspects of speech perception."
                },
                "authors": [
                    {
                        "name": "Harm Lameris"
                    },
                    {
                        "name": "Shree Harsha Bokkahalli Satish"
                    },
                    {
                        "name": "Joakim Gustafson"
                    },
                    {
                        "name": "Éva Székely"
                    }
                ],
                "author_detail": {
                    "name": "Éva Székely"
                },
                "author": "Éva Székely",
                "arxiv_comment": "8 pages, 3 figures, 4 tables, submitted to LREC 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25577v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25577v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10940v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10940v3",
                "updated": "2025-10-29T14:42:46Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    14,
                    42,
                    46,
                    2,
                    302,
                    0
                ],
                "published": "2025-05-16T07:26:41Z",
                "published_parsed": [
                    2025,
                    5,
                    16,
                    7,
                    26,
                    41,
                    4,
                    136,
                    0
                ],
                "title": "Who You Are Matters: Bridging Topics and Social Roles via LLM-Enhanced\n  Logical Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Who You Are Matters: Bridging Topics and Social Roles via LLM-Enhanced\n  Logical Recommendation"
                },
                "summary": "Recommender systems filter contents/items valuable to users by inferring\npreferences from user features and historical behaviors. Mainstream approaches\nfollow the learning-to-rank paradigm, which focus on discovering and modeling\nitem topics (e.g., categories), and capturing user preferences on these topics\nbased on historical interactions. However, this paradigm often neglects the\nmodeling of user characteristics and their social roles, which are logical\nconfounders influencing the correlated interest and user preference transition.\nTo bridge this gap, we introduce the user role identification task and the\nbehavioral logic modeling task that aim to explicitly model user roles and\nlearn the logical relations between item topics and user social roles. We show\nthat it is possible to explicitly solve these tasks through an efficient\nintegration framework of Large Language Model (LLM) and recommendation systems,\nfor which we propose TagCF. On the one hand, TagCF exploits the (Multi-modal)\nLLM's world knowledge and logic inference ability to extract realistic\ntag-based virtual logic graphs that reveal dynamic and expressive knowledge of\nusers, refining our understanding of user behaviors. On the other hand, TagCF\npresents empirically effective integration modules that take advantage of the\nextracted tag-logic information, augmenting the recommendation performance. We\nconduct both online experiments and offline experiments with industrial and\npublic datasets as verification of TagCF's effectiveness, and we empirically\nshow that the user role modeling strategy is potentially a better choice than\nthe modeling of item topics. Additionally, we provide evidence that the\nextracted logic graphs are empirically a general and transferable knowledge\nthat can benefit a wide range of recommendation tasks. Our code is available in\nhttps://github.com/Code2Q/TagCF.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recommender systems filter contents/items valuable to users by inferring\npreferences from user features and historical behaviors. Mainstream approaches\nfollow the learning-to-rank paradigm, which focus on discovering and modeling\nitem topics (e.g., categories), and capturing user preferences on these topics\nbased on historical interactions. However, this paradigm often neglects the\nmodeling of user characteristics and their social roles, which are logical\nconfounders influencing the correlated interest and user preference transition.\nTo bridge this gap, we introduce the user role identification task and the\nbehavioral logic modeling task that aim to explicitly model user roles and\nlearn the logical relations between item topics and user social roles. We show\nthat it is possible to explicitly solve these tasks through an efficient\nintegration framework of Large Language Model (LLM) and recommendation systems,\nfor which we propose TagCF. On the one hand, TagCF exploits the (Multi-modal)\nLLM's world knowledge and logic inference ability to extract realistic\ntag-based virtual logic graphs that reveal dynamic and expressive knowledge of\nusers, refining our understanding of user behaviors. On the other hand, TagCF\npresents empirically effective integration modules that take advantage of the\nextracted tag-logic information, augmenting the recommendation performance. We\nconduct both online experiments and offline experiments with industrial and\npublic datasets as verification of TagCF's effectiveness, and we empirically\nshow that the user role modeling strategy is potentially a better choice than\nthe modeling of item topics. Additionally, we provide evidence that the\nextracted logic graphs are empirically a general and transferable knowledge\nthat can benefit a wide range of recommendation tasks. Our code is available in\nhttps://github.com/Code2Q/TagCF."
                },
                "authors": [
                    {
                        "name": "Qing Yu"
                    },
                    {
                        "name": "Xiaobei Wang"
                    },
                    {
                        "name": "Shuchang Liu"
                    },
                    {
                        "name": "Yandong Bai"
                    },
                    {
                        "name": "Xiaoyu Yang"
                    },
                    {
                        "name": "Xueliang Wang"
                    },
                    {
                        "name": "Chang Meng"
                    },
                    {
                        "name": "Shanshan Wu"
                    },
                    {
                        "name": "Hailan Yang"
                    },
                    {
                        "name": "Huihui Xiao"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Xiaoqiang Feng"
                    },
                    {
                        "name": "Lantao Hu"
                    },
                    {
                        "name": "Han Li"
                    },
                    {
                        "name": "Kun Gai"
                    },
                    {
                        "name": "Lixin Zou"
                    }
                ],
                "author_detail": {
                    "name": "Lixin Zou"
                },
                "author": "Lixin Zou",
                "arxiv_comment": "to be published in NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10940v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10940v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24670v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24670v2",
                "updated": "2025-10-29T14:41:45Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    14,
                    41,
                    45,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-28T17:36:51Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    17,
                    36,
                    51,
                    1,
                    301,
                    0
                ],
                "title": "Pearl: A Foundation Model for Placing Every Atom in the Right Location",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pearl: A Foundation Model for Placing Every Atom in the Right Location"
                },
                "summary": "Accurately predicting the three-dimensional structures of protein-ligand\ncomplexes remains a fundamental challenge in computational drug discovery that\nlimits the pace and success of therapeutic design. Deep learning methods have\nrecently shown strong potential as structural prediction tools, achieving\npromising accuracy across diverse biomolecular systems. However, their\nperformance and utility are constrained by scarce experimental data,\ninefficient architectures, physically invalid poses, and the limited ability to\nexploit auxiliary information available at inference. To address these issues,\nwe introduce Pearl (Placing Every Atom in the Right Location), a foundation\nmodel for protein-ligand cofolding at scale. Pearl addresses these challenges\nwith three key innovations: (1) training recipes that include large-scale\nsynthetic data to overcome data scarcity; (2) architectures that incorporate an\nSO(3)-equivariant diffusion module to inherently respect 3D rotational\nsymmetries, improving generalization and sample efficiency, and (3)\ncontrollable inference, including a generalized multi-chain templating system\nsupporting both protein and non-polymeric components as well as dual\nunconditional/conditional modes. Pearl establishes a new state-of-the-art\nperformance in protein-ligand cofolding. On the key metric of generating\naccurate (RMSD < 2 \\r{A}) and physically valid poses, Pearl surpasses AlphaFold\n3 and other open source baselines on the public Runs N' Poses and PoseBusters\nbenchmarks, delivering 14.5% and 14.2% improvements, respectively, over the\nnext best model. In the pocket-conditional cofolding regime, Pearl delivers\n$3.6\\times$ improvement on a proprietary set of challenging, real-world drug\ntargets at the more rigorous RMSD < 1 \\r{A} threshold. Finally, we demonstrate\nthat model performance correlates directly with synthetic dataset size used in\ntraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurately predicting the three-dimensional structures of protein-ligand\ncomplexes remains a fundamental challenge in computational drug discovery that\nlimits the pace and success of therapeutic design. Deep learning methods have\nrecently shown strong potential as structural prediction tools, achieving\npromising accuracy across diverse biomolecular systems. However, their\nperformance and utility are constrained by scarce experimental data,\ninefficient architectures, physically invalid poses, and the limited ability to\nexploit auxiliary information available at inference. To address these issues,\nwe introduce Pearl (Placing Every Atom in the Right Location), a foundation\nmodel for protein-ligand cofolding at scale. Pearl addresses these challenges\nwith three key innovations: (1) training recipes that include large-scale\nsynthetic data to overcome data scarcity; (2) architectures that incorporate an\nSO(3)-equivariant diffusion module to inherently respect 3D rotational\nsymmetries, improving generalization and sample efficiency, and (3)\ncontrollable inference, including a generalized multi-chain templating system\nsupporting both protein and non-polymeric components as well as dual\nunconditional/conditional modes. Pearl establishes a new state-of-the-art\nperformance in protein-ligand cofolding. On the key metric of generating\naccurate (RMSD < 2 \\r{A}) and physically valid poses, Pearl surpasses AlphaFold\n3 and other open source baselines on the public Runs N' Poses and PoseBusters\nbenchmarks, delivering 14.5% and 14.2% improvements, respectively, over the\nnext best model. In the pocket-conditional cofolding regime, Pearl delivers\n$3.6\\times$ improvement on a proprietary set of challenging, real-world drug\ntargets at the more rigorous RMSD < 1 \\r{A} threshold. Finally, we demonstrate\nthat model performance correlates directly with synthetic dataset size used in\ntraining."
                },
                "authors": [
                    {
                        "name": "Genesis Research Team"
                    },
                    {
                        "name": "Alejandro Dobles"
                    },
                    {
                        "name": "Nina Jovic"
                    },
                    {
                        "name": "Kenneth Leidal"
                    },
                    {
                        "name": "Pranav Murugan"
                    },
                    {
                        "name": "David C. Williams"
                    },
                    {
                        "name": "Drausin Wulsin"
                    },
                    {
                        "name": "Nate Gruver"
                    },
                    {
                        "name": "Christina X. Ji"
                    },
                    {
                        "name": "Korrawat Pruegsanusak"
                    },
                    {
                        "name": "Gianluca Scarpellini"
                    },
                    {
                        "name": "Ansh Sharma"
                    },
                    {
                        "name": "Wojciech Swiderski"
                    },
                    {
                        "name": "Andrea Bootsma"
                    },
                    {
                        "name": "Richard Strong Bowen"
                    },
                    {
                        "name": "Charlotte Chen"
                    },
                    {
                        "name": "Jamin Chen"
                    },
                    {
                        "name": "Marc André Dämgen"
                    },
                    {
                        "name": "Benjamin DiFrancesco"
                    },
                    {
                        "name": "J. D. Fishman"
                    },
                    {
                        "name": "Alla Ivanova"
                    },
                    {
                        "name": "Zach Kagin"
                    },
                    {
                        "name": "David Li-Bland"
                    },
                    {
                        "name": "Zuli Liu"
                    },
                    {
                        "name": "Igor Morozov"
                    },
                    {
                        "name": "Jeffrey Ouyang-Zhang"
                    },
                    {
                        "name": "Frank C. Pickard IV"
                    },
                    {
                        "name": "Kushal S. Shah"
                    },
                    {
                        "name": "Ben Shor"
                    },
                    {
                        "name": "Gabriel Monteiro da Silva"
                    },
                    {
                        "name": "Roy Tal"
                    },
                    {
                        "name": "Maxx Tessmer"
                    },
                    {
                        "name": "Carl Tilbury"
                    },
                    {
                        "name": "Cyr Vetcher"
                    },
                    {
                        "name": "Daniel Zeng"
                    },
                    {
                        "name": "Maruan Al-Shedivat"
                    },
                    {
                        "name": "Aleksandra Faust"
                    },
                    {
                        "name": "Evan N. Feinberg"
                    },
                    {
                        "name": "Michael V. LeVine"
                    },
                    {
                        "name": "Matteus Pan"
                    }
                ],
                "author_detail": {
                    "name": "Matteus Pan"
                },
                "author": "Matteus Pan",
                "arxiv_comment": "technical report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24670v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24670v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15030v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15030v2",
                "updated": "2025-10-29T14:31:38Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    14,
                    31,
                    38,
                    2,
                    302,
                    0
                ],
                "published": "2025-08-20T19:49:06Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    19,
                    49,
                    6,
                    2,
                    232,
                    0
                ],
                "title": "Collab-REC: An LLM-based Agentic Framework for Balancing Recommendations\n  in Tourism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collab-REC: An LLM-based Agentic Framework for Balancing Recommendations\n  in Tourism"
                },
                "summary": "We propose Collab-REC, a multi-agent framework designed to counteract\npopularity bias and enhance diversity in tourism recommendations. In our\nsetting, three LLM-based agents -- Personalization, Popularity, and\nSustainability generate city suggestions from complementary perspectives. A\nnon-LLM moderator then merges and refines these proposals via multi-round\nnegotiation, ensuring each agent's viewpoint is incorporated while penalizing\nspurious or repeated responses. Experiments on European city queries show that\nCollab-REC improves diversity and overall relevance compared to a single-agent\nbaseline, surfacing lesser-visited locales that often remain overlooked. This\nbalanced, context-aware approach addresses over-tourism and better aligns with\nconstraints provided by the user, highlighting the promise of multi-stakeholder\ncollaboration in LLM-driven recommender systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose Collab-REC, a multi-agent framework designed to counteract\npopularity bias and enhance diversity in tourism recommendations. In our\nsetting, three LLM-based agents -- Personalization, Popularity, and\nSustainability generate city suggestions from complementary perspectives. A\nnon-LLM moderator then merges and refines these proposals via multi-round\nnegotiation, ensuring each agent's viewpoint is incorporated while penalizing\nspurious or repeated responses. Experiments on European city queries show that\nCollab-REC improves diversity and overall relevance compared to a single-agent\nbaseline, surfacing lesser-visited locales that often remain overlooked. This\nbalanced, context-aware approach addresses over-tourism and better aligns with\nconstraints provided by the user, highlighting the promise of multi-stakeholder\ncollaboration in LLM-driven recommender systems."
                },
                "authors": [
                    {
                        "name": "Ashmi Banerjee"
                    },
                    {
                        "name": "Fitri Nur Aisyah"
                    },
                    {
                        "name": "Adithi Satish"
                    },
                    {
                        "name": "Wolfgang Wörndl"
                    },
                    {
                        "name": "Yashar Deldjoo"
                    }
                ],
                "author_detail": {
                    "name": "Yashar Deldjoo"
                },
                "author": "Yashar Deldjoo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15030v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15030v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.01308v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.01308v2",
                "updated": "2025-10-29T14:09:33Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    14,
                    9,
                    33,
                    2,
                    302,
                    0
                ],
                "published": "2025-09-01T09:47:35Z",
                "published_parsed": [
                    2025,
                    9,
                    1,
                    9,
                    47,
                    35,
                    0,
                    244,
                    0
                ],
                "title": "GradeSQL: Test-Time Inference with Outcome Reward Models for Text-to-SQL\n  Generation from Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GradeSQL: Test-Time Inference with Outcome Reward Models for Text-to-SQL\n  Generation from Large Language Models"
                },
                "summary": "Text-to-SQL, the task of translating natural language questions into SQL\nqueries, has significantly advanced with the introduction of Large Language\nModels (LLMs), broadening database accessibility for a wide range of users.\nDespite substantial progress in generating valid SQL, current LLMs still\nstruggle with complex queries. To address this limitation, test-time strategies\nsuch as Best-of-N (BoN) and Majority Voting (Maj) are often employed, based on\nthe assumption that LLMs can produce correct answers after multiple attempts.\nHowever, these methods rely on surface-level heuristics, selecting the\nsyntactically correct query through execution-based BoN (ex-BoN) or the most\nfrequently generated one through Majority Voting. Recently, Outcome Reward\nModels (ORMs), which assign utility scores to generated outputs based on\nsemantic correctness, have emerged as a promising reinforcement learning\napproach for improving model alignment. We argue that ORMs could serve as an\neffective new test-time heuristic, although their application in this context\nremains largely underexplored.\n  In this work, we propose a unified framework for training ORMs tailored to\nthe Text-to-SQL task and assess their effectiveness as a test-time heuristic\nwithin the BoN strategy. We benchmark ORMs against ex-BoN and Maj across the\nBIRD and Spider datasets, fine-tuning diverse open-source LLMs from the Qwen2,\nGranite3, and Llama3 families. Results show that ORMs outperform ex-BoN and\nMaj, achieving execution accuracy gains of +4.33% (BIRD) and +2.10% (Spider)\nover ex-BoN, and +2.91% (BIRD) and +0.93% (Spider) over Maj. We further\ndemonstrate that finetuning models already aligned with SQL generation, such as\nOmniSQL, yields superior ORM performance. Additionally, we observe that ORMs\nachieve competitive results on simple queries and benefit more from an\nincreased number of candidates compared to ex-BoN and Maj.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-SQL, the task of translating natural language questions into SQL\nqueries, has significantly advanced with the introduction of Large Language\nModels (LLMs), broadening database accessibility for a wide range of users.\nDespite substantial progress in generating valid SQL, current LLMs still\nstruggle with complex queries. To address this limitation, test-time strategies\nsuch as Best-of-N (BoN) and Majority Voting (Maj) are often employed, based on\nthe assumption that LLMs can produce correct answers after multiple attempts.\nHowever, these methods rely on surface-level heuristics, selecting the\nsyntactically correct query through execution-based BoN (ex-BoN) or the most\nfrequently generated one through Majority Voting. Recently, Outcome Reward\nModels (ORMs), which assign utility scores to generated outputs based on\nsemantic correctness, have emerged as a promising reinforcement learning\napproach for improving model alignment. We argue that ORMs could serve as an\neffective new test-time heuristic, although their application in this context\nremains largely underexplored.\n  In this work, we propose a unified framework for training ORMs tailored to\nthe Text-to-SQL task and assess their effectiveness as a test-time heuristic\nwithin the BoN strategy. We benchmark ORMs against ex-BoN and Maj across the\nBIRD and Spider datasets, fine-tuning diverse open-source LLMs from the Qwen2,\nGranite3, and Llama3 families. Results show that ORMs outperform ex-BoN and\nMaj, achieving execution accuracy gains of +4.33% (BIRD) and +2.10% (Spider)\nover ex-BoN, and +2.91% (BIRD) and +0.93% (Spider) over Maj. We further\ndemonstrate that finetuning models already aligned with SQL generation, such as\nOmniSQL, yields superior ORM performance. Additionally, we observe that ORMs\nachieve competitive results on simple queries and benefit more from an\nincreased number of candidates compared to ex-BoN and Maj."
                },
                "authors": [
                    {
                        "name": "Mattia Tritto"
                    },
                    {
                        "name": "Giuseppe Farano"
                    },
                    {
                        "name": "Dario Di Palma"
                    },
                    {
                        "name": "Gaetano Rossiello"
                    },
                    {
                        "name": "Fedelucio Narducci"
                    },
                    {
                        "name": "Dharmashankar Subramanian"
                    },
                    {
                        "name": "Tommaso Di Noia"
                    }
                ],
                "author_detail": {
                    "name": "Tommaso Di Noia"
                },
                "author": "Tommaso Di Noia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.01308v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.01308v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25536v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25536v2",
                "updated": "2025-10-30T11:19:24Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    11,
                    19,
                    24,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-29T14:00:42Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    14,
                    0,
                    42,
                    2,
                    302,
                    0
                ],
                "title": "TwinVoice: A Multi-dimensional Benchmark Towards Digital Twins via LLM\n  Persona Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TwinVoice: A Multi-dimensional Benchmark Towards Digital Twins via LLM\n  Persona Simulation"
                },
                "summary": "Large Language Models (LLMs) are exhibiting emergent human-like abilities and\nare increasingly envisioned as the foundation for simulating an individual's\ncommunication style, behavioral tendencies, and personality traits. However,\ncurrent evaluations of LLM-based persona simulation remain limited: most rely\non synthetic dialogues, lack systematic frameworks, and lack analysis of the\ncapability requirement. To address these limitations, we introduce TwinVoice, a\ncomprehensive benchmark for assessing persona simulation across diverse\nreal-world contexts. TwinVoice encompasses three dimensions: Social Persona\n(public social interactions), Interpersonal Persona (private dialogues), and\nNarrative Persona (role-based expression). It further decomposes the evaluation\nof LLM performance into six fundamental capabilities, including opinion\nconsistency, memory recall, logical reasoning, lexical fidelity, persona tone,\nand syntactic style. Experimental results reveal that while advanced models\nachieve moderate accuracy in persona simulation, they still fall short of\ncapabilities such as syntactic style and memory recall. Consequently, the\naverage performance achieved by LLMs remains considerably below the human\nbaseline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are exhibiting emergent human-like abilities and\nare increasingly envisioned as the foundation for simulating an individual's\ncommunication style, behavioral tendencies, and personality traits. However,\ncurrent evaluations of LLM-based persona simulation remain limited: most rely\non synthetic dialogues, lack systematic frameworks, and lack analysis of the\ncapability requirement. To address these limitations, we introduce TwinVoice, a\ncomprehensive benchmark for assessing persona simulation across diverse\nreal-world contexts. TwinVoice encompasses three dimensions: Social Persona\n(public social interactions), Interpersonal Persona (private dialogues), and\nNarrative Persona (role-based expression). It further decomposes the evaluation\nof LLM performance into six fundamental capabilities, including opinion\nconsistency, memory recall, logical reasoning, lexical fidelity, persona tone,\nand syntactic style. Experimental results reveal that while advanced models\nachieve moderate accuracy in persona simulation, they still fall short of\ncapabilities such as syntactic style and memory recall. Consequently, the\naverage performance achieved by LLMs remains considerably below the human\nbaseline."
                },
                "authors": [
                    {
                        "name": "Bangde Du"
                    },
                    {
                        "name": "Minghao Guo"
                    },
                    {
                        "name": "Songming He"
                    },
                    {
                        "name": "Ziyi Ye"
                    },
                    {
                        "name": "Xi Zhu"
                    },
                    {
                        "name": "Weihang Su"
                    },
                    {
                        "name": "Shuqi Zhu"
                    },
                    {
                        "name": "Yujia Zhou"
                    },
                    {
                        "name": "Yongfeng Zhang"
                    },
                    {
                        "name": "Qingyao Ai"
                    },
                    {
                        "name": "Yiqun Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yiqun Liu"
                },
                "author": "Yiqun Liu",
                "arxiv_comment": "Main paper: 11 pages, 3 figures, 6 tables. Appendix: 28 pages. Bangde\n  Du and Minghao Guo contributed equally. Corresponding authors: Ziyi Ye\n  (ziyiye@fudan.edu.cn), Qingyao Ai (aiqy@tsinghua.edu.cn)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25536v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25536v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.6; I.2.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25531v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25531v1",
                "updated": "2025-10-29T13:56:44Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    13,
                    56,
                    44,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T13:56:44Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    13,
                    56,
                    44,
                    2,
                    302,
                    0
                ],
                "title": "Using latent representations to link disjoint longitudinal data for\n  mixed-effects regression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using latent representations to link disjoint longitudinal data for\n  mixed-effects regression"
                },
                "summary": "Many rare diseases offer limited established treatment options, leading\npatients to switch therapies when new medications emerge. To analyze the impact\nof such treatment switches within the low sample size limitations of rare\ndisease trials, it is important to use all available data sources. This,\nhowever, is complicated when usage of measurement instruments change during the\nobservation period, for example when instruments are adapted to specific age\nranges. The resulting disjoint longitudinal data trajectories, complicate the\napplication of traditional modeling approaches like mixed-effects regression.\nWe tackle this by mapping observations of each instrument to a aligned\nlow-dimensional temporal trajectory, enabling longitudinal modeling across\ninstruments. Specifically, we employ a set of variational autoencoder\narchitectures to embed item values into a shared latent space for each time\npoint. Temporal disease dynamics and treatment switch effects are then captured\nthrough a mixed-effects regression model applied to latent representations. To\nenable statistical inference, we present a novel statistical testing approach\nthat accounts for the joint parameter estimation of mixed-effects regression\nand variational autoencoders. The methodology is applied to quantify the impact\nof treatment switches for patients with spinal muscular atrophy. Here, our\napproach aligns motor performance items from different measurement instruments\nfor mixed-effects regression and maps estimated effects back to the observed\nitem level to quantify the treatment switch effect. Our approach allows for\nmodel selection as well as for assessing effects of treatment switching. The\nresults highlight the potential of modeling in joint latent representations for\naddressing small data challenges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many rare diseases offer limited established treatment options, leading\npatients to switch therapies when new medications emerge. To analyze the impact\nof such treatment switches within the low sample size limitations of rare\ndisease trials, it is important to use all available data sources. This,\nhowever, is complicated when usage of measurement instruments change during the\nobservation period, for example when instruments are adapted to specific age\nranges. The resulting disjoint longitudinal data trajectories, complicate the\napplication of traditional modeling approaches like mixed-effects regression.\nWe tackle this by mapping observations of each instrument to a aligned\nlow-dimensional temporal trajectory, enabling longitudinal modeling across\ninstruments. Specifically, we employ a set of variational autoencoder\narchitectures to embed item values into a shared latent space for each time\npoint. Temporal disease dynamics and treatment switch effects are then captured\nthrough a mixed-effects regression model applied to latent representations. To\nenable statistical inference, we present a novel statistical testing approach\nthat accounts for the joint parameter estimation of mixed-effects regression\nand variational autoencoders. The methodology is applied to quantify the impact\nof treatment switches for patients with spinal muscular atrophy. Here, our\napproach aligns motor performance items from different measurement instruments\nfor mixed-effects regression and maps estimated effects back to the observed\nitem level to quantify the treatment switch effect. Our approach allows for\nmodel selection as well as for assessing effects of treatment switching. The\nresults highlight the potential of modeling in joint latent representations for\naddressing small data challenges."
                },
                "authors": [
                    {
                        "name": "Clemens Schächter"
                    },
                    {
                        "name": "Maren Hackenberg"
                    },
                    {
                        "name": "Michelle Pfaffenlehner"
                    },
                    {
                        "name": "Félix B. Tambe-Ndonfack"
                    },
                    {
                        "name": "Thorsten Schmidt"
                    },
                    {
                        "name": "Astrid Pechmann"
                    },
                    {
                        "name": "Janbernd Kirschner"
                    },
                    {
                        "name": "Jan Hasenauser"
                    },
                    {
                        "name": "Harald Binder"
                    }
                ],
                "author_detail": {
                    "name": "Harald Binder"
                },
                "author": "Harald Binder",
                "arxiv_comment": "31 pages, 3 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25531v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25531v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "G.3; I.2.6; J.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25528v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25528v1",
                "updated": "2025-10-29T13:52:44Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    13,
                    52,
                    44,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T13:52:44Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    13,
                    52,
                    44,
                    2,
                    302,
                    0
                ],
                "title": "Zero Reinforcement Learning Towards General Domains",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero Reinforcement Learning Towards General Domains"
                },
                "summary": "Zero Reinforcement Learning (Zero-RL) has proven to be an effective approach\nfor enhancing the reasoning capabilities of large language models (LLMs) by\ndirectly applying reinforcement learning with verifiable rewards on pretrained\nmodels, without the need for a supervised fine-tuning phase. However, current\nresearch on zero-RL primarily focuses on domains with easily verifiable reward\nsignals, such as mathematics, programming, and other reasoning tasks. The\nchallenge of eliciting reasoning abilities in more diverse scenarios, where\nverification is not straightforward, remains underexplored. To address this\ngap, we propose a novel zero-RL paradigm designed to improve a model's\nreasoning ability across both verifiable and non-verifiable domains. By\ncombining verifiable rewards with a generative reward model, we conduct\nmulti-task zero-RL training across both domains, facilitating the transfer of\nreasoning capabilities between them. Furthermore, to mitigate reward hacking in\nthe generative reward model, we design a smooth length penalty that encourages\nthe generation of more comprehensive thinking tokens in general domains.\nExperimental results on Qwen3-8B-Base and Qwen3-14B-Base demonstrate that our\napproach achieves superior reasoning performance, not only on tasks requiring\nextensive reasoning but also on more general tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero Reinforcement Learning (Zero-RL) has proven to be an effective approach\nfor enhancing the reasoning capabilities of large language models (LLMs) by\ndirectly applying reinforcement learning with verifiable rewards on pretrained\nmodels, without the need for a supervised fine-tuning phase. However, current\nresearch on zero-RL primarily focuses on domains with easily verifiable reward\nsignals, such as mathematics, programming, and other reasoning tasks. The\nchallenge of eliciting reasoning abilities in more diverse scenarios, where\nverification is not straightforward, remains underexplored. To address this\ngap, we propose a novel zero-RL paradigm designed to improve a model's\nreasoning ability across both verifiable and non-verifiable domains. By\ncombining verifiable rewards with a generative reward model, we conduct\nmulti-task zero-RL training across both domains, facilitating the transfer of\nreasoning capabilities between them. Furthermore, to mitigate reward hacking in\nthe generative reward model, we design a smooth length penalty that encourages\nthe generation of more comprehensive thinking tokens in general domains.\nExperimental results on Qwen3-8B-Base and Qwen3-14B-Base demonstrate that our\napproach achieves superior reasoning performance, not only on tasks requiring\nextensive reasoning but also on more general tasks."
                },
                "authors": [
                    {
                        "name": "Yuyuan Zeng"
                    },
                    {
                        "name": "Yufei Huang"
                    },
                    {
                        "name": "Can Xu"
                    },
                    {
                        "name": "Qingfeng Sun"
                    },
                    {
                        "name": "Jianfeng Yan"
                    },
                    {
                        "name": "Guanghui Xu"
                    },
                    {
                        "name": "Tao Yang"
                    },
                    {
                        "name": "Fengzong Lian"
                    }
                ],
                "author_detail": {
                    "name": "Fengzong Lian"
                },
                "author": "Fengzong Lian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25528v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25528v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.23906v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.23906v2",
                "updated": "2025-10-29T13:42:56Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    13,
                    42,
                    56,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-27T22:26:20Z",
                "published_parsed": [
                    2025,
                    10,
                    27,
                    22,
                    26,
                    20,
                    0,
                    300,
                    0
                ],
                "title": "Group Interventions on Deep Networks for Causal Discovery in Subsystems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Group Interventions on Deep Networks for Causal Discovery in Subsystems"
                },
                "summary": "Causal discovery uncovers complex relationships between variables, enhancing\npredictions, decision-making, and insights into real-world systems, especially\nin nonlinear multivariate time series. However, most existing methods primarily\nfocus on pairwise cause-effect relationships, overlooking interactions among\ngroups of variables, i.e., subsystems and their collective causal influence. In\nthis study, we introduce gCDMI, a novel multi-group causal discovery method\nthat leverages group-level interventions on trained deep neural networks and\nemploys model invariance testing to infer causal relationships. Our approach\ninvolves three key steps. First, we use deep learning to jointly model the\nstructural relationships among groups of all time series. Second, we apply\ngroup-wise interventions to the trained model. Finally, we conduct model\ninvariance testing to determine the presence of causal links among variable\ngroups. We evaluate our method on simulated datasets, demonstrating its\nsuperior performance in identifying group-level causal relationships compared\nto existing methods. Additionally, we validate our approach on real-world\ndatasets, including brain networks and climate ecosystems. Our results\nhighlight that applying group-level interventions to deep learning models,\ncombined with invariance testing, can effectively reveal complex causal\nstructures, offering valuable insights for domains such as neuroscience and\nclimate science.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal discovery uncovers complex relationships between variables, enhancing\npredictions, decision-making, and insights into real-world systems, especially\nin nonlinear multivariate time series. However, most existing methods primarily\nfocus on pairwise cause-effect relationships, overlooking interactions among\ngroups of variables, i.e., subsystems and their collective causal influence. In\nthis study, we introduce gCDMI, a novel multi-group causal discovery method\nthat leverages group-level interventions on trained deep neural networks and\nemploys model invariance testing to infer causal relationships. Our approach\ninvolves three key steps. First, we use deep learning to jointly model the\nstructural relationships among groups of all time series. Second, we apply\ngroup-wise interventions to the trained model. Finally, we conduct model\ninvariance testing to determine the presence of causal links among variable\ngroups. We evaluate our method on simulated datasets, demonstrating its\nsuperior performance in identifying group-level causal relationships compared\nto existing methods. Additionally, we validate our approach on real-world\ndatasets, including brain networks and climate ecosystems. Our results\nhighlight that applying group-level interventions to deep learning models,\ncombined with invariance testing, can effectively reveal complex causal\nstructures, offering valuable insights for domains such as neuroscience and\nclimate science."
                },
                "authors": [
                    {
                        "name": "Wasim Ahmad"
                    },
                    {
                        "name": "Joachim Denzler"
                    },
                    {
                        "name": "Maha Shadaydeh"
                    }
                ],
                "author_detail": {
                    "name": "Maha Shadaydeh"
                },
                "author": "Maha Shadaydeh",
                "arxiv_comment": "Submitted to IEEE Access. We are working on the revised version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.23906v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.23906v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25517v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25517v1",
                "updated": "2025-10-29T13:39:41Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    13,
                    39,
                    41,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T13:39:41Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    13,
                    39,
                    41,
                    2,
                    302,
                    0
                ],
                "title": "Predicate Renaming via Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predicate Renaming via Large Language Models"
                },
                "summary": "In this paper, we address the problem of giving names to predicates in logic\nrules using Large Language Models (LLMs). In the context of Inductive Logic\nProgramming, various rule generation methods produce rules containing unnamed\npredicates, with Predicate Invention being a key example. This hinders the\nreadability, interpretability, and reusability of the logic theory. Leveraging\nrecent advancements in LLMs development, we explore their ability to process\nnatural language and code to provide semantically meaningful suggestions for\ngiving a name to unnamed predicates. The evaluation of our approach on some\nhand-crafted logic rules indicates that LLMs hold potential for this task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we address the problem of giving names to predicates in logic\nrules using Large Language Models (LLMs). In the context of Inductive Logic\nProgramming, various rule generation methods produce rules containing unnamed\npredicates, with Predicate Invention being a key example. This hinders the\nreadability, interpretability, and reusability of the logic theory. Leveraging\nrecent advancements in LLMs development, we explore their ability to process\nnatural language and code to provide semantically meaningful suggestions for\ngiving a name to unnamed predicates. The evaluation of our approach on some\nhand-crafted logic rules indicates that LLMs hold potential for this task."
                },
                "authors": [
                    {
                        "name": "Elisabetta Gentili"
                    },
                    {
                        "name": "Tony Ribeiro"
                    },
                    {
                        "name": "Fabrizio Riguzzi"
                    },
                    {
                        "name": "Katsumi Inoue"
                    }
                ],
                "author_detail": {
                    "name": "Katsumi Inoue"
                },
                "author": "Katsumi Inoue",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25517v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25517v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25515v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25515v1",
                "updated": "2025-10-29T13:38:27Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    13,
                    38,
                    27,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T13:38:27Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    13,
                    38,
                    27,
                    2,
                    302,
                    0
                ],
                "title": "Solar photospheric velocities measured in space: a comparison between\n  SO/PHI-HRT and SDO/HMI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Solar photospheric velocities measured in space: a comparison between\n  SO/PHI-HRT and SDO/HMI"
                },
                "summary": "The Polarimetric and Helioseismic Imager (SO/PHI) onboard Solar Orbiter is a\nspectropolarimeter scanning the Fe I line at 617.3 nm, providing data of the\nsolar photosphere. The same line is sampled by the Helioseismic and Magnetic\nImager (HMI) on board the Solar Dynamics Observatory (SDO) and many other\non-ground instruments. In this paper, we aim at assessing the consistency\nbetween line-of-sight (LoS) velocity measurements from the two instruments.\nReliable measurements of up and down flows from SO/PHI are crucial and unique\nwhen Solar Orbiter is facing the far side of the Sun. Also, a combination of\nmeasurements from two vantage points to study horizontal flows must rely on\nconsistent observations. For this purpose, we compare the LoS velocity measured\nby SO/PHI's High Resolution Telescope (SO/PHI-HRT) and SDO/HMI on 29 March\n2023, when Solar Orbiter was crossing the Sun-Earth line at a distance of 0.39\nau from the Sun. Because such co-alignments are rare, this configuration\noffered an almost unique opportunity to directly compare data products from\nboth telescopes. The data are aligned and remapped to allow a pixel-by-pixel\ncomparison of the whole time series of 4 hours length. Temporal and spatial\nvariations are considered for a direct combination of the measurements. The LoS\nvelocity distributions are evaluated and a clear linear relation is found\nbetween the two instruments with a slope of 0.94 and a correlation of 90%. We\nfind that the signals form at similar heights, with a separation of 10$\\pm$12\nkm, which is larger than previous estimates. A close-up look at the penumbra of\na sunspot and its Evershed flow is presented. We conclude that the signals\ninferred by SO/PHI-HRT and SDO/HMI show very good agreement and high\ncorrelation when instrumental effects and large-scale velocities on the Sun are\nproperly accounted for.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Polarimetric and Helioseismic Imager (SO/PHI) onboard Solar Orbiter is a\nspectropolarimeter scanning the Fe I line at 617.3 nm, providing data of the\nsolar photosphere. The same line is sampled by the Helioseismic and Magnetic\nImager (HMI) on board the Solar Dynamics Observatory (SDO) and many other\non-ground instruments. In this paper, we aim at assessing the consistency\nbetween line-of-sight (LoS) velocity measurements from the two instruments.\nReliable measurements of up and down flows from SO/PHI are crucial and unique\nwhen Solar Orbiter is facing the far side of the Sun. Also, a combination of\nmeasurements from two vantage points to study horizontal flows must rely on\nconsistent observations. For this purpose, we compare the LoS velocity measured\nby SO/PHI's High Resolution Telescope (SO/PHI-HRT) and SDO/HMI on 29 March\n2023, when Solar Orbiter was crossing the Sun-Earth line at a distance of 0.39\nau from the Sun. Because such co-alignments are rare, this configuration\noffered an almost unique opportunity to directly compare data products from\nboth telescopes. The data are aligned and remapped to allow a pixel-by-pixel\ncomparison of the whole time series of 4 hours length. Temporal and spatial\nvariations are considered for a direct combination of the measurements. The LoS\nvelocity distributions are evaluated and a clear linear relation is found\nbetween the two instruments with a slope of 0.94 and a correlation of 90%. We\nfind that the signals form at similar heights, with a separation of 10$\\pm$12\nkm, which is larger than previous estimates. A close-up look at the penumbra of\na sunspot and its Evershed flow is presented. We conclude that the signals\ninferred by SO/PHI-HRT and SDO/HMI show very good agreement and high\ncorrelation when instrumental effects and large-scale velocities on the Sun are\nproperly accounted for."
                },
                "authors": [
                    {
                        "name": "D. Calchetti"
                    },
                    {
                        "name": "K. Albert"
                    },
                    {
                        "name": "F. J. Bailén"
                    },
                    {
                        "name": "J. Blanco Rodríguez"
                    },
                    {
                        "name": "J. S. Castellanos Durán"
                    },
                    {
                        "name": "A. Feller"
                    },
                    {
                        "name": "A. Gandorfer"
                    },
                    {
                        "name": "J. Hirzberger"
                    },
                    {
                        "name": "J. Sinjan"
                    },
                    {
                        "name": "X. Li"
                    },
                    {
                        "name": "T. Oba"
                    },
                    {
                        "name": "D. Orozco Súarez"
                    },
                    {
                        "name": "T. L. Riethmüller"
                    },
                    {
                        "name": "J. Schou"
                    },
                    {
                        "name": "S. K. Solanki"
                    },
                    {
                        "name": "H. Strecker"
                    },
                    {
                        "name": "A. Ulyanov"
                    },
                    {
                        "name": "G. Valori"
                    }
                ],
                "author_detail": {
                    "name": "G. Valori"
                },
                "author": "G. Valori",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25515v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25515v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.00814v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.00814v2",
                "updated": "2025-10-29T13:37:41Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    13,
                    37,
                    41,
                    2,
                    302,
                    0
                ],
                "published": "2025-07-01T14:46:16Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    14,
                    46,
                    16,
                    1,
                    182,
                    0
                ],
                "title": "Many LLMs Are More Utilitarian Than One",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many LLMs Are More Utilitarian Than One"
                },
                "summary": "Moral judgment is integral to large language models' (LLMs) social reasoning.\nAs multi-agent systems gain prominence, it becomes crucial to understand how\nLLMs function when collaborating compared to operating as individual agents. In\nhuman moral judgment, group deliberation leads to a Utilitarian Boost: a\ntendency to endorse norm violations that inflict harm but maximize benefits for\nthe greatest number of people. We study whether a similar dynamic emerges in\nmulti-agent LLM systems. We test six models on well-established sets of moral\ndilemmas across two conditions: (1) Solo, where models reason independently,\nand (2) Group, where they engage in multi-turn discussions in pairs or triads.\nIn personal dilemmas, where agents decide whether to directly harm an\nindividual for the benefit of others, all models rated moral violations as more\nacceptable when part of a group, demonstrating a Utilitarian Boost similar to\nthat observed in humans. However, the mechanism for the Boost in LLMs differed:\nWhile humans in groups become more utilitarian due to heightened sensitivity to\ndecision outcomes, LLM groups showed either reduced sensitivity to norms or\nenhanced impartiality. We report model differences in when and how strongly the\nBoost manifests. We also discuss prompt and agent compositions that enhance or\nmitigate the effect. We end with a discussion of the implications for AI\nalignment, multi-agent design, and artificial moral reasoning. Code available\nat: https://github.com/baltaci-r/MoralAgents",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Moral judgment is integral to large language models' (LLMs) social reasoning.\nAs multi-agent systems gain prominence, it becomes crucial to understand how\nLLMs function when collaborating compared to operating as individual agents. In\nhuman moral judgment, group deliberation leads to a Utilitarian Boost: a\ntendency to endorse norm violations that inflict harm but maximize benefits for\nthe greatest number of people. We study whether a similar dynamic emerges in\nmulti-agent LLM systems. We test six models on well-established sets of moral\ndilemmas across two conditions: (1) Solo, where models reason independently,\nand (2) Group, where they engage in multi-turn discussions in pairs or triads.\nIn personal dilemmas, where agents decide whether to directly harm an\nindividual for the benefit of others, all models rated moral violations as more\nacceptable when part of a group, demonstrating a Utilitarian Boost similar to\nthat observed in humans. However, the mechanism for the Boost in LLMs differed:\nWhile humans in groups become more utilitarian due to heightened sensitivity to\ndecision outcomes, LLM groups showed either reduced sensitivity to norms or\nenhanced impartiality. We report model differences in when and how strongly the\nBoost manifests. We also discuss prompt and agent compositions that enhance or\nmitigate the effect. We end with a discussion of the implications for AI\nalignment, multi-agent design, and artificial moral reasoning. Code available\nat: https://github.com/baltaci-r/MoralAgents"
                },
                "authors": [
                    {
                        "name": "Anita Keshmirian"
                    },
                    {
                        "name": "Razan Baltaji"
                    },
                    {
                        "name": "Babak Hemmatian"
                    },
                    {
                        "name": "Hadi Asghari"
                    },
                    {
                        "name": "Lav R. Varshney"
                    }
                ],
                "author_detail": {
                    "name": "Lav R. Varshney"
                },
                "author": "Lav R. Varshney",
                "arxiv_comment": "Accepted to the Conference on Neural Information Processing Systems\n  (NeurIPS 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.00814v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.00814v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.23763v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.23763v2",
                "updated": "2025-10-29T13:37:19Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    13,
                    37,
                    19,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-27T18:49:03Z",
                "published_parsed": [
                    2025,
                    10,
                    27,
                    18,
                    49,
                    3,
                    0,
                    300,
                    0
                ],
                "title": "RoboOmni: Proactive Robot Manipulation in Omni-modal Context",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RoboOmni: Proactive Robot Manipulation in Omni-modal Context"
                },
                "summary": "Recent advances in Multimodal Large Language Models (MLLMs) have driven rapid\nprogress in Vision-Language-Action (VLA) models for robotic manipulation.\nAlthough effective in many scenarios, current approaches largely rely on\nexplicit instructions, whereas in real-world interactions, humans rarely issue\ninstructions directly. Effective collaboration requires robots to infer user\nintentions proactively. In this work, we introduce cross-modal contextual\ninstructions, a new setting where intent is derived from spoken dialogue,\nenvironmental sounds, and visual cues rather than explicit commands. To address\nthis new setting, we present RoboOmni, a Perceiver-Thinker-Talker-Executor\nframework based on end-to-end omni-modal LLMs that unifies intention\nrecognition, interaction confirmation, and action execution. RoboOmni fuses\nauditory and visual signals spatiotemporally for robust intention recognition,\nwhile supporting direct speech interaction. To address the absence of training\ndata for proactive intention recognition in robotic manipulation, we build\nOmniAction, comprising 140k episodes, 5k+ speakers, 2.4k event sounds, 640\nbackgrounds, and six contextual instruction types. Experiments in simulation\nand real-world settings show that RoboOmni surpasses text- and ASR-based\nbaselines in success rate, inference speed, intention recognition, and\nproactive assistance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Multimodal Large Language Models (MLLMs) have driven rapid\nprogress in Vision-Language-Action (VLA) models for robotic manipulation.\nAlthough effective in many scenarios, current approaches largely rely on\nexplicit instructions, whereas in real-world interactions, humans rarely issue\ninstructions directly. Effective collaboration requires robots to infer user\nintentions proactively. In this work, we introduce cross-modal contextual\ninstructions, a new setting where intent is derived from spoken dialogue,\nenvironmental sounds, and visual cues rather than explicit commands. To address\nthis new setting, we present RoboOmni, a Perceiver-Thinker-Talker-Executor\nframework based on end-to-end omni-modal LLMs that unifies intention\nrecognition, interaction confirmation, and action execution. RoboOmni fuses\nauditory and visual signals spatiotemporally for robust intention recognition,\nwhile supporting direct speech interaction. To address the absence of training\ndata for proactive intention recognition in robotic manipulation, we build\nOmniAction, comprising 140k episodes, 5k+ speakers, 2.4k event sounds, 640\nbackgrounds, and six contextual instruction types. Experiments in simulation\nand real-world settings show that RoboOmni surpasses text- and ASR-based\nbaselines in success rate, inference speed, intention recognition, and\nproactive assistance."
                },
                "authors": [
                    {
                        "name": "Siyin Wang"
                    },
                    {
                        "name": "Jinlan Fu"
                    },
                    {
                        "name": "Feihong Liu"
                    },
                    {
                        "name": "Xinzhe He"
                    },
                    {
                        "name": "Huangxuan Wu"
                    },
                    {
                        "name": "Junhao Shi"
                    },
                    {
                        "name": "Kexin Huang"
                    },
                    {
                        "name": "Zhaoye Fei"
                    },
                    {
                        "name": "Jingjing Gong"
                    },
                    {
                        "name": "Zuxuan Wu"
                    },
                    {
                        "name": "Yugang Jiang"
                    },
                    {
                        "name": "See-Kiong Ng"
                    },
                    {
                        "name": "Tat-Seng Chua"
                    },
                    {
                        "name": "Xipeng Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Xipeng Qiu"
                },
                "author": "Xipeng Qiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.23763v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.23763v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25510v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25510v1",
                "updated": "2025-10-29T13:34:27Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    13,
                    34,
                    27,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T13:34:27Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    13,
                    34,
                    27,
                    2,
                    302,
                    0
                ],
                "title": "MTIR-SQL: Multi-turn Tool-Integrated Reasoning Reinforcement Learning\n  for Text-to-SQL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MTIR-SQL: Multi-turn Tool-Integrated Reasoning Reinforcement Learning\n  for Text-to-SQL"
                },
                "summary": "As large language models (LLMs) are increasingly used in Text-to-SQL tasks,\nReinforcement Learning (RL) has become a common method for improving\nperformance. Existing methods primarily rely on static execution feedback,\nwhich restricts real-time error correction. However, integrating multi-turn\ntool invocation along with dynamic feedback could significantly improve\nadaptability and robustness, ultimately enhancing model performance. To address\nthese issues, we propose MTIR-SQL, an innovative Multi-turn Tool-Integrated\nReasoning reinforcement learning framework for Text-to-SQL. Our approach\nintroduces an execution-aware multi-turn reasoning paradigm that seamlessly\nincorporates database execution feedback at each reasoning step, enabling\ncontext-sensitive query generation and progressive refinement throughout the\nreasoning process. The framework extends the GRPO algorithm to accommodate\ncomplex multi-turn interaction scenarios. Considering the training instability\ncharacteristics of MTIR and the potential for significant Deviation of model\ndistribution from the initial model, we enhance the GRPO algorithm by adding a\ntrajectory filtering mechanism and removing KL loss constraints. Experimental\nresults demonstrate that MTIR-SQL, with 4B parameters, achieves \\textbf{64.4}\\%\naccuracy in the BIRD Dev and 84.6% execution accuracy in the SPIDER Dev,\nsignificantly outperforming existing approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) are increasingly used in Text-to-SQL tasks,\nReinforcement Learning (RL) has become a common method for improving\nperformance. Existing methods primarily rely on static execution feedback,\nwhich restricts real-time error correction. However, integrating multi-turn\ntool invocation along with dynamic feedback could significantly improve\nadaptability and robustness, ultimately enhancing model performance. To address\nthese issues, we propose MTIR-SQL, an innovative Multi-turn Tool-Integrated\nReasoning reinforcement learning framework for Text-to-SQL. Our approach\nintroduces an execution-aware multi-turn reasoning paradigm that seamlessly\nincorporates database execution feedback at each reasoning step, enabling\ncontext-sensitive query generation and progressive refinement throughout the\nreasoning process. The framework extends the GRPO algorithm to accommodate\ncomplex multi-turn interaction scenarios. Considering the training instability\ncharacteristics of MTIR and the potential for significant Deviation of model\ndistribution from the initial model, we enhance the GRPO algorithm by adding a\ntrajectory filtering mechanism and removing KL loss constraints. Experimental\nresults demonstrate that MTIR-SQL, with 4B parameters, achieves \\textbf{64.4}\\%\naccuracy in the BIRD Dev and 84.6% execution accuracy in the SPIDER Dev,\nsignificantly outperforming existing approaches."
                },
                "authors": [
                    {
                        "name": "Zekun Xu"
                    },
                    {
                        "name": "Siyu Xia"
                    },
                    {
                        "name": "Chuhuai Yue"
                    },
                    {
                        "name": "Jiajun Chai"
                    },
                    {
                        "name": "Mingxue Tian"
                    },
                    {
                        "name": "Xiaohan Wang"
                    },
                    {
                        "name": "Wei Lin"
                    },
                    {
                        "name": "Haoxuan Li"
                    },
                    {
                        "name": "Guojun Yin"
                    }
                ],
                "author_detail": {
                    "name": "Guojun Yin"
                },
                "author": "Guojun Yin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25510v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25510v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25506v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25506v1",
                "updated": "2025-10-29T13:31:32Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    13,
                    31,
                    32,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T13:31:32Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    13,
                    31,
                    32,
                    2,
                    302,
                    0
                ],
                "title": "Reflections on the Reproducibility of Commercial LLM Performance in\n  Empirical Software Engineering Studies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reflections on the Reproducibility of Commercial LLM Performance in\n  Empirical Software Engineering Studies"
                },
                "summary": "Large Language Models have gained remarkable interest in industry and\nacademia. The increasing interest in LLMs in academia is also reflected in the\nnumber of publications on this topic over the last years. For instance, alone\n78 of the around 425 publications at ICSE 2024 performed experiments with LLMs.\nConducting empirical studies with LLMs remains challenging and raises questions\non how to achieve reproducible results, for both other researchers and\npractitioners. One important step towards excelling in empirical research on\nLLMs and their application is to first understand to what extent current\nresearch results are eventually reproducible and what factors may impede\nreproducibility. This investigation is within the scope of our work. We\ncontribute an analysis of the reproducibility of LLM-centric studies, provide\ninsights into the factors impeding reproducibility, and discuss suggestions on\nhow to improve the current state. In particular, we studied the 86 articles\ndescribing LLM-centric studies, published at ICSE 2024 and ASE 2024. Of the 86\narticles, 18 provided research artefacts and used OpenAI models. We attempted\nto replicate those 18 studies. Of the 18 studies, only five were fit for\nreproduction. For none of the five studies, we were able to fully reproduce the\nresults. Two studies seemed to be partially reproducible, and three studies did\nnot seem to be reproducible. Our results highlight not only the need for\nstricter research artefact evaluations but also for more robust study designs\nto ensure the reproducible value of future publications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models have gained remarkable interest in industry and\nacademia. The increasing interest in LLMs in academia is also reflected in the\nnumber of publications on this topic over the last years. For instance, alone\n78 of the around 425 publications at ICSE 2024 performed experiments with LLMs.\nConducting empirical studies with LLMs remains challenging and raises questions\non how to achieve reproducible results, for both other researchers and\npractitioners. One important step towards excelling in empirical research on\nLLMs and their application is to first understand to what extent current\nresearch results are eventually reproducible and what factors may impede\nreproducibility. This investigation is within the scope of our work. We\ncontribute an analysis of the reproducibility of LLM-centric studies, provide\ninsights into the factors impeding reproducibility, and discuss suggestions on\nhow to improve the current state. In particular, we studied the 86 articles\ndescribing LLM-centric studies, published at ICSE 2024 and ASE 2024. Of the 86\narticles, 18 provided research artefacts and used OpenAI models. We attempted\nto replicate those 18 studies. Of the 18 studies, only five were fit for\nreproduction. For none of the five studies, we were able to fully reproduce the\nresults. Two studies seemed to be partially reproducible, and three studies did\nnot seem to be reproducible. Our results highlight not only the need for\nstricter research artefact evaluations but also for more robust study designs\nto ensure the reproducible value of future publications."
                },
                "authors": [
                    {
                        "name": "Florian Angermeir"
                    },
                    {
                        "name": "Maximilian Amougou"
                    },
                    {
                        "name": "Mark Kreitz"
                    },
                    {
                        "name": "Andreas Bauer"
                    },
                    {
                        "name": "Matthias Linhuber"
                    },
                    {
                        "name": "Davide Fucci"
                    },
                    {
                        "name": "Fabiola Moyón C."
                    },
                    {
                        "name": "Daniel Mendez"
                    },
                    {
                        "name": "Tony Gorschek"
                    }
                ],
                "author_detail": {
                    "name": "Tony Gorschek"
                },
                "author": "Tony Gorschek",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25506v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25506v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25502v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25502v1",
                "updated": "2025-10-29T13:27:18Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    13,
                    27,
                    18,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T13:27:18Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    13,
                    27,
                    18,
                    2,
                    302,
                    0
                ],
                "title": "TempoPFN: Synthetic Pre-training of Linear RNNs for Zero-shot Time\n  Series Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TempoPFN: Synthetic Pre-training of Linear RNNs for Zero-shot Time\n  Series Forecasting"
                },
                "summary": "Foundation models for zero-shot time series forecasting face challenges in\nefficient long-horizon prediction and reproducibility, with existing\nsynthetic-only approaches underperforming on challenging benchmarks. This paper\npresents TempoPFN, a univariate time series foundation model based on linear\nRecurrent Neural Networks (RNNs) pre-trained exclusively on synthetic data. The\nmodel uses a GatedDeltaProduct architecture with state-weaving for fully\nparallelizable training across sequence lengths, eliminating the need for\nwindowing or summarization techniques while maintaining robust temporal\nstate-tracking. Our comprehensive synthetic data pipeline unifies diverse\ngenerators, including stochastic differential equations, Gaussian processes,\nand audio synthesis, with novel augmentations. In zero-shot evaluations on the\nGift-Eval benchmark, TempoPFN achieves top-tier competitive performance,\noutperforming all existing synthetic-only approaches and surpassing the vast\nmajority of models trained on real-world data, while being more efficient than\nexisting baselines by leveraging fully parallelizable training and inference.\nWe open-source our complete data generation pipeline and training code,\nproviding a reproducible foundation for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foundation models for zero-shot time series forecasting face challenges in\nefficient long-horizon prediction and reproducibility, with existing\nsynthetic-only approaches underperforming on challenging benchmarks. This paper\npresents TempoPFN, a univariate time series foundation model based on linear\nRecurrent Neural Networks (RNNs) pre-trained exclusively on synthetic data. The\nmodel uses a GatedDeltaProduct architecture with state-weaving for fully\nparallelizable training across sequence lengths, eliminating the need for\nwindowing or summarization techniques while maintaining robust temporal\nstate-tracking. Our comprehensive synthetic data pipeline unifies diverse\ngenerators, including stochastic differential equations, Gaussian processes,\nand audio synthesis, with novel augmentations. In zero-shot evaluations on the\nGift-Eval benchmark, TempoPFN achieves top-tier competitive performance,\noutperforming all existing synthetic-only approaches and surpassing the vast\nmajority of models trained on real-world data, while being more efficient than\nexisting baselines by leveraging fully parallelizable training and inference.\nWe open-source our complete data generation pipeline and training code,\nproviding a reproducible foundation for future research."
                },
                "authors": [
                    {
                        "name": "Vladyslav Moroshan"
                    },
                    {
                        "name": "Julien Siems"
                    },
                    {
                        "name": "Arber Zela"
                    },
                    {
                        "name": "Timur Carstensen"
                    },
                    {
                        "name": "Frank Hutter"
                    }
                ],
                "author_detail": {
                    "name": "Frank Hutter"
                },
                "author": "Frank Hutter",
                "arxiv_comment": "30 pages, 18 figures, 13 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25502v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25502v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12993v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12993v2",
                "updated": "2025-10-29T13:26:49Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    13,
                    26,
                    49,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-14T21:10:50Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    21,
                    10,
                    50,
                    1,
                    287,
                    0
                ],
                "title": "A Multilingual, Large-Scale Study of the Interplay between LLM\n  Safeguards, Personalisation, and Disinformation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Multilingual, Large-Scale Study of the Interplay between LLM\n  Safeguards, Personalisation, and Disinformation"
                },
                "summary": "Large Language Models (LLMs) can generate human-like disinformation, yet\ntheir ability to personalise such content across languages and demographics\nremains underexplored. This study presents the first large-scale, multilingual\nanalysis of persona-targeted disinformation generation by LLMs. Employing a red\nteaming methodology, we prompt eight state-of-the-art LLMs with 324 false\nnarratives and 150 demographic personas (combinations of country, generation,\nand political orientation) across four languages--English, Russian, Portuguese,\nand Hindi--resulting in AI-TRAITS, a comprehensive dataset of 1.6 million\npersonalised disinformation texts. Results show that the use of even simple\npersonalisation prompts significantly increases the likelihood of jailbreaks\nacross all studied LLMs, up to 10 percentage points, and alters linguistic and\nrhetorical patterns that enhance narrative persuasiveness. Models such as Grok\nand GPT exhibited jailbreak rates and personalisation scores both exceeding\n85%. These insights expose critical vulnerabilities in current state-of-the-art\nLLMs and offer a foundation for improving safety alignment and detection\nstrategies in multilingual and cross-demographic contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) can generate human-like disinformation, yet\ntheir ability to personalise such content across languages and demographics\nremains underexplored. This study presents the first large-scale, multilingual\nanalysis of persona-targeted disinformation generation by LLMs. Employing a red\nteaming methodology, we prompt eight state-of-the-art LLMs with 324 false\nnarratives and 150 demographic personas (combinations of country, generation,\nand political orientation) across four languages--English, Russian, Portuguese,\nand Hindi--resulting in AI-TRAITS, a comprehensive dataset of 1.6 million\npersonalised disinformation texts. Results show that the use of even simple\npersonalisation prompts significantly increases the likelihood of jailbreaks\nacross all studied LLMs, up to 10 percentage points, and alters linguistic and\nrhetorical patterns that enhance narrative persuasiveness. Models such as Grok\nand GPT exhibited jailbreak rates and personalisation scores both exceeding\n85%. These insights expose critical vulnerabilities in current state-of-the-art\nLLMs and offer a foundation for improving safety alignment and detection\nstrategies in multilingual and cross-demographic contexts."
                },
                "authors": [
                    {
                        "name": "João A. Leite"
                    },
                    {
                        "name": "Arnav Arora"
                    },
                    {
                        "name": "Silvia Gargova"
                    },
                    {
                        "name": "João Luz"
                    },
                    {
                        "name": "Gustavo Sampaio"
                    },
                    {
                        "name": "Ian Roberts"
                    },
                    {
                        "name": "Carolina Scarton"
                    },
                    {
                        "name": "Kalina Bontcheva"
                    }
                ],
                "author_detail": {
                    "name": "Kalina Bontcheva"
                },
                "author": "Kalina Bontcheva",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12993v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12993v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21236v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21236v2",
                "updated": "2025-10-29T13:11:21Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    13,
                    11,
                    21,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-24T08:10:36Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    8,
                    10,
                    36,
                    4,
                    297,
                    0
                ],
                "title": "Securing AI Agent Execution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Securing AI Agent Execution"
                },
                "summary": "Large Language Models (LLMs) have evolved into AI agents that interact with\nexternal tools and environments to perform complex tasks. The Model Context\nProtocol (MCP) has become the de facto standard for connecting agents with such\nresources, but security has lagged behind: thousands of MCP servers execute\nwith unrestricted access to host systems, creating a broad attack surface. In\nthis paper, we introduce AgentBound, the first access control framework for MCP\nservers. AgentBound combines a declarative policy mechanism, inspired by the\nAndroid permission model, with a policy enforcement engine that contains\nmalicious behavior without requiring MCP server modifications. We build a\ndataset containing the 296 most popular MCP servers, and show that access\ncontrol policies can be generated automatically from source code with 80.9%\naccuracy. We also show that AgentBound blocks the majority of security threats\nin several malicious MCP servers, and that policy enforcement engine introduces\nnegligible overhead. Our contributions provide developers and project managers\nwith a practical foundation for securing MCP servers while maintaining\nproductivity, enabling researchers and tool builders to explore new directions\nfor declarative access control and MCP security.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have evolved into AI agents that interact with\nexternal tools and environments to perform complex tasks. The Model Context\nProtocol (MCP) has become the de facto standard for connecting agents with such\nresources, but security has lagged behind: thousands of MCP servers execute\nwith unrestricted access to host systems, creating a broad attack surface. In\nthis paper, we introduce AgentBound, the first access control framework for MCP\nservers. AgentBound combines a declarative policy mechanism, inspired by the\nAndroid permission model, with a policy enforcement engine that contains\nmalicious behavior without requiring MCP server modifications. We build a\ndataset containing the 296 most popular MCP servers, and show that access\ncontrol policies can be generated automatically from source code with 80.9%\naccuracy. We also show that AgentBound blocks the majority of security threats\nin several malicious MCP servers, and that policy enforcement engine introduces\nnegligible overhead. Our contributions provide developers and project managers\nwith a practical foundation for securing MCP servers while maintaining\nproductivity, enabling researchers and tool builders to explore new directions\nfor declarative access control and MCP security."
                },
                "authors": [
                    {
                        "name": "Christoph Bühler"
                    },
                    {
                        "name": "Matteo Biagiola"
                    },
                    {
                        "name": "Luca Di Grazia"
                    },
                    {
                        "name": "Guido Salvaneschi"
                    }
                ],
                "author_detail": {
                    "name": "Guido Salvaneschi"
                },
                "author": "Guido Salvaneschi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21236v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21236v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25488v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25488v1",
                "updated": "2025-10-29T13:08:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    13,
                    8,
                    35,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T13:08:35Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    13,
                    8,
                    35,
                    2,
                    302,
                    0
                ],
                "title": "Generalized Pseudo-Relevance Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generalized Pseudo-Relevance Feedback"
                },
                "summary": "Query rewriting is a fundamental technique in information retrieval (IR). It\ntypically employs the retrieval result as relevance feedback to refine the\nquery and thereby addresses the vocabulary mismatch between user queries and\nrelevant documents. Traditional pseudo-relevance feedback (PRF) and its\nvector-based extension (VPRF) improve retrieval performance by leveraging\ntop-retrieved documents as relevance feedback. However, they are constructed\nbased on two major hypotheses: the relevance assumption (top documents are\nrelevant) and the model assumption (rewriting methods need to be designed\nspecifically for particular model architectures). While recent large language\nmodels (LLMs)-based generative relevance feedback (GRF) enables model-free\nquery reformulation, it either suffers from severe LLM hallucination or, again,\nrelies on the relevance assumption to guarantee the effectiveness of rewriting\nquality. To overcome these limitations, we introduce an assumption-relaxed\nframework: \\textit{Generalized Pseudo Relevance Feedback} (GPRF), which\nperforms model-free, natural language rewriting based on retrieved documents,\nnot only eliminating the model assumption but also reducing dependence on the\nrelevance assumption. Specifically, we design a utility-oriented training\npipeline with reinforcement learning to ensure robustness against noisy\nfeedback. Extensive experiments across multiple benchmarks and retrievers\ndemonstrate that GPRF consistently outperforms strong baselines, establishing\nit as an effective and generalizable framework for query rewriting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Query rewriting is a fundamental technique in information retrieval (IR). It\ntypically employs the retrieval result as relevance feedback to refine the\nquery and thereby addresses the vocabulary mismatch between user queries and\nrelevant documents. Traditional pseudo-relevance feedback (PRF) and its\nvector-based extension (VPRF) improve retrieval performance by leveraging\ntop-retrieved documents as relevance feedback. However, they are constructed\nbased on two major hypotheses: the relevance assumption (top documents are\nrelevant) and the model assumption (rewriting methods need to be designed\nspecifically for particular model architectures). While recent large language\nmodels (LLMs)-based generative relevance feedback (GRF) enables model-free\nquery reformulation, it either suffers from severe LLM hallucination or, again,\nrelies on the relevance assumption to guarantee the effectiveness of rewriting\nquality. To overcome these limitations, we introduce an assumption-relaxed\nframework: \\textit{Generalized Pseudo Relevance Feedback} (GPRF), which\nperforms model-free, natural language rewriting based on retrieved documents,\nnot only eliminating the model assumption but also reducing dependence on the\nrelevance assumption. Specifically, we design a utility-oriented training\npipeline with reinforcement learning to ensure robustness against noisy\nfeedback. Extensive experiments across multiple benchmarks and retrievers\ndemonstrate that GPRF consistently outperforms strong baselines, establishing\nit as an effective and generalizable framework for query rewriting."
                },
                "authors": [
                    {
                        "name": "Yiteng Tu"
                    },
                    {
                        "name": "Weihang Su"
                    },
                    {
                        "name": "Yujia Zhou"
                    },
                    {
                        "name": "Yiqun Liu"
                    },
                    {
                        "name": "Fen Lin"
                    },
                    {
                        "name": "Qin Liu"
                    },
                    {
                        "name": "Qingyao Ai"
                    }
                ],
                "author_detail": {
                    "name": "Qingyao Ai"
                },
                "author": "Qingyao Ai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25488v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25488v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25472v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25472v1",
                "updated": "2025-10-29T12:47:36Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    12,
                    47,
                    36,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T12:47:36Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    12,
                    47,
                    36,
                    2,
                    302,
                    0
                ],
                "title": "NetEcho: From Real-World Streaming Side-Channels to Full LLM\n  Conversation Recovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NetEcho: From Real-World Streaming Side-Channels to Full LLM\n  Conversation Recovery"
                },
                "summary": "In the rapidly expanding landscape of Large Language Model (LLM)\napplications, real-time output streaming has become the dominant interaction\nparadigm. While this enhances user experience, recent research reveals that it\nexposes a non-trivial attack surface through network side-channels. Adversaries\ncan exploit patterns in encrypted traffic to infer sensitive information and\nreconstruct private conversations. In response, LLM providers and third-party\nservices are deploying defenses such as traffic padding and obfuscation to\nmitigate these vulnerabilities.\n  This paper starts by presenting a systematic analysis of contemporary\nside-channel defenses in mainstream LLM applications, with a focus on services\nfrom vendors like OpenAI and DeepSeek. We identify and examine seven\nrepresentative deployment scenarios, each incorporating active/passive\nmitigation techniques. Despite these enhanced security measures, our\ninvestigation uncovers significant residual information that remains vulnerable\nto leakage within the network traffic.\n  Building on this discovery, we introduce NetEcho, a novel, LLM-based\nframework that comprehensively unleashes the network side-channel risks of\ntoday's LLM applications. NetEcho is designed to recover entire conversations\n-- including both user prompts and LLM responses -- directly from encrypted\nnetwork traffic. It features a deliberate design that ensures high-fidelity\ntext recovery, transferability across different deployment scenarios, and\nmoderate operational cost. In our evaluations on medical and legal applications\nbuilt upon leading models like DeepSeek-v3 and GPT-4o, NetEcho can recover avg\n$\\sim$70\\% information of each conversation, demonstrating a critical\nlimitation in current defense mechanisms. We conclude by discussing the\nimplications of our findings and proposing future directions for augmenting\nnetwork traffic security.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the rapidly expanding landscape of Large Language Model (LLM)\napplications, real-time output streaming has become the dominant interaction\nparadigm. While this enhances user experience, recent research reveals that it\nexposes a non-trivial attack surface through network side-channels. Adversaries\ncan exploit patterns in encrypted traffic to infer sensitive information and\nreconstruct private conversations. In response, LLM providers and third-party\nservices are deploying defenses such as traffic padding and obfuscation to\nmitigate these vulnerabilities.\n  This paper starts by presenting a systematic analysis of contemporary\nside-channel defenses in mainstream LLM applications, with a focus on services\nfrom vendors like OpenAI and DeepSeek. We identify and examine seven\nrepresentative deployment scenarios, each incorporating active/passive\nmitigation techniques. Despite these enhanced security measures, our\ninvestigation uncovers significant residual information that remains vulnerable\nto leakage within the network traffic.\n  Building on this discovery, we introduce NetEcho, a novel, LLM-based\nframework that comprehensively unleashes the network side-channel risks of\ntoday's LLM applications. NetEcho is designed to recover entire conversations\n-- including both user prompts and LLM responses -- directly from encrypted\nnetwork traffic. It features a deliberate design that ensures high-fidelity\ntext recovery, transferability across different deployment scenarios, and\nmoderate operational cost. In our evaluations on medical and legal applications\nbuilt upon leading models like DeepSeek-v3 and GPT-4o, NetEcho can recover avg\n$\\sim$70\\% information of each conversation, demonstrating a critical\nlimitation in current defense mechanisms. We conclude by discussing the\nimplications of our findings and proposing future directions for augmenting\nnetwork traffic security."
                },
                "authors": [
                    {
                        "name": "Zheng Zhang"
                    },
                    {
                        "name": "Guanlong Wu"
                    },
                    {
                        "name": "Sen Deng"
                    },
                    {
                        "name": "Shuai Wang"
                    },
                    {
                        "name": "Yinqian Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yinqian Zhang"
                },
                "author": "Yinqian Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25472v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25472v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25460v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25460v1",
                "updated": "2025-10-29T12:33:48Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    12,
                    33,
                    48,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T12:33:48Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    12,
                    33,
                    48,
                    2,
                    302,
                    0
                ],
                "title": "Fine-Tuned Language Models for Domain-Specific Summarization and Tagging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-Tuned Language Models for Domain-Specific Summarization and Tagging"
                },
                "summary": "This paper presents a pipeline integrating fine-tuned large language models\n(LLMs) with named entity recognition (NER) for efficient domain-specific text\nsummarization and tagging. The authors address the challenge posed by rapidly\nevolving sub-cultural languages and slang, which complicate automated\ninformation extraction and law enforcement monitoring. By leveraging the LLaMA\nFactory framework, the study fine-tunes LLMs on both generalpurpose and custom\ndomain-specific datasets, particularly in the political and security domains.\nThe models are evaluated using BLEU and ROUGE metrics, demonstrating that\ninstruction fine-tuning significantly enhances summarization and tagging\naccuracy, especially for specialized corpora. Notably, the LLaMA3-8B-Instruct\nmodel, despite its initial limitations in Chinese comprehension, outperforms\nits Chinese-trained counterpart after domainspecific fine-tuning, suggesting\nthat underlying reasoning capabilities can transfer across languages. The\npipeline enables concise summaries and structured entity tagging, facilitating\nrapid document categorization and distribution. This approach proves scalable\nand adaptable for real-time applications, supporting efficient information\nmanagement and the ongoing need to capture emerging language trends. The\nintegration of LLMs and NER offers a robust solution for transforming\nunstructured text into actionable insights, crucial for modern knowledge\nmanagement and security operations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a pipeline integrating fine-tuned large language models\n(LLMs) with named entity recognition (NER) for efficient domain-specific text\nsummarization and tagging. The authors address the challenge posed by rapidly\nevolving sub-cultural languages and slang, which complicate automated\ninformation extraction and law enforcement monitoring. By leveraging the LLaMA\nFactory framework, the study fine-tunes LLMs on both generalpurpose and custom\ndomain-specific datasets, particularly in the political and security domains.\nThe models are evaluated using BLEU and ROUGE metrics, demonstrating that\ninstruction fine-tuning significantly enhances summarization and tagging\naccuracy, especially for specialized corpora. Notably, the LLaMA3-8B-Instruct\nmodel, despite its initial limitations in Chinese comprehension, outperforms\nits Chinese-trained counterpart after domainspecific fine-tuning, suggesting\nthat underlying reasoning capabilities can transfer across languages. The\npipeline enables concise summaries and structured entity tagging, facilitating\nrapid document categorization and distribution. This approach proves scalable\nand adaptable for real-time applications, supporting efficient information\nmanagement and the ongoing need to capture emerging language trends. The\nintegration of LLMs and NER offers a robust solution for transforming\nunstructured text into actionable insights, crucial for modern knowledge\nmanagement and security operations."
                },
                "authors": [
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Fuming Lin"
                    },
                    {
                        "name": "Yuyu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yuyu Chen"
                },
                "author": "Yuyu Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25460v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25460v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25441v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25441v1",
                "updated": "2025-10-29T12:08:07Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    12,
                    8,
                    7,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T12:08:07Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    12,
                    8,
                    7,
                    2,
                    302,
                    0
                ],
                "title": "Grounded in Reality: Learning and Deploying Proactive LLM from Offline\n  Logs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Grounded in Reality: Learning and Deploying Proactive LLM from Offline\n  Logs"
                },
                "summary": "Large Language Models (LLMs) excel as passive responders, but teaching them\nto be proactive, goal-oriented partners, a critical capability in high-stakes\ndomains, remains a major challenge. Current paradigms either myopically\noptimize single-turn attributes or rely on brittle, high-cost user simulators,\ncreating a persistent ``reality gap''. To bridge this gap, we introduce\n\\texttt{Learn-to-Ask}, a general, simulator-free framework for learning and\ndeploying proactive dialogue agents \\textit{directly from offline expert data},\nbypassing the need to model complex user dynamics. Our key insight is to\nreframe the offline policy learning problem by leveraging the \\textbf{observed\nfuture} of each expert trajectory. This allows us to infer a dense,\nturn-by-turn reward signal grounded in the expert's revealed strategy,\ndecomposing the intractable long-horizon problem into a series of supervised\nlearning tasks, and training a policy to output a structured \\texttt{(action,\nstate_assessment)} tuple, governing both \\textbf{what to ask} and, crucially,\n\\textbf{when to stop}. To ensure reward fidelity, our Automated Grader\nCalibration pipeline systematically purges noise from the LLM-based reward\nmodel with minimal human supervision. Empirically, we demonstrate the efficacy\nof \\texttt{Learn-to-Ask} in a real-world medical dataset, using LLMs of varying\nsizes up to 32B. Our approach culminates in the successful deployment of LLMs\ninto a live, large-scale online AI service. In rigorous in-house evaluations,\nour model was launched and achieved performance even superior to human experts,\nproving our framework's ability to translate offline data into tangible,\nreal-world impact. We hope this work provides a practical and economically\nviable blueprint for transforming passive LLMs into proactive, goal-oriented\nLLM applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) excel as passive responders, but teaching them\nto be proactive, goal-oriented partners, a critical capability in high-stakes\ndomains, remains a major challenge. Current paradigms either myopically\noptimize single-turn attributes or rely on brittle, high-cost user simulators,\ncreating a persistent ``reality gap''. To bridge this gap, we introduce\n\\texttt{Learn-to-Ask}, a general, simulator-free framework for learning and\ndeploying proactive dialogue agents \\textit{directly from offline expert data},\nbypassing the need to model complex user dynamics. Our key insight is to\nreframe the offline policy learning problem by leveraging the \\textbf{observed\nfuture} of each expert trajectory. This allows us to infer a dense,\nturn-by-turn reward signal grounded in the expert's revealed strategy,\ndecomposing the intractable long-horizon problem into a series of supervised\nlearning tasks, and training a policy to output a structured \\texttt{(action,\nstate_assessment)} tuple, governing both \\textbf{what to ask} and, crucially,\n\\textbf{when to stop}. To ensure reward fidelity, our Automated Grader\nCalibration pipeline systematically purges noise from the LLM-based reward\nmodel with minimal human supervision. Empirically, we demonstrate the efficacy\nof \\texttt{Learn-to-Ask} in a real-world medical dataset, using LLMs of varying\nsizes up to 32B. Our approach culminates in the successful deployment of LLMs\ninto a live, large-scale online AI service. In rigorous in-house evaluations,\nour model was launched and achieved performance even superior to human experts,\nproving our framework's ability to translate offline data into tangible,\nreal-world impact. We hope this work provides a practical and economically\nviable blueprint for transforming passive LLMs into proactive, goal-oriented\nLLM applications."
                },
                "authors": [
                    {
                        "name": "Fei Wei"
                    },
                    {
                        "name": "Daoyuan Chen"
                    },
                    {
                        "name": "Ce Wang"
                    },
                    {
                        "name": "Yilun Huang"
                    },
                    {
                        "name": "Yushuo Chen"
                    },
                    {
                        "name": "Xuchen Pan"
                    },
                    {
                        "name": "Yaliang Li"
                    },
                    {
                        "name": "Bolin Ding"
                    }
                ],
                "author_detail": {
                    "name": "Bolin Ding"
                },
                "author": "Bolin Ding",
                "arxiv_comment": "27 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25441v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25441v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16368v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16368v2",
                "updated": "2025-10-29T12:06:15Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    12,
                    6,
                    15,
                    2,
                    302,
                    0
                ],
                "published": "2025-05-22T08:23:10Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    8,
                    23,
                    10,
                    3,
                    142,
                    0
                ],
                "title": "SATURN: SAT-based Reinforcement Learning to Unleash Language Model\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SATURN: SAT-based Reinforcement Learning to Unleash Language Model\n  Reasoning"
                },
                "summary": "How to design reinforcement learning (RL) tasks that effectively unleash the\nreasoning capability of large language models (LLMs) remains an open question.\nExisting RL tasks (e.g., math, programming, and constructing reasoning tasks)\nsuffer from three key limitations: (1) Scalability. They rely heavily on human\nannotation or expensive LLM synthesis to generate sufficient training data. (2)\nVerifiability. LLMs' outputs are hard to verify automatically and reliably. (3)\nControllable Difficulty. Most tasks lack fine-grained difficulty control,\nmaking it hard to train LLMs to develop reasoning ability from easy to hard.\n  To address these limitations, we propose Saturn, a SAT-based RL framework\nthat uses Boolean Satisfiability (SAT) problems to train and evaluate LLMs\nreasoning. Saturn enables scalable task construction, rule-based verification,\nand precise difficulty control. Saturn designs a curriculum learning pipeline\nthat continuously improves LLMs' reasoning capability by constructing SAT tasks\nof increasing difficulty and training LLMs from easy to hard. To ensure stable\ntraining, we design a principled mechanism to control difficulty transitions.\n  We introduce Saturn-2.6k, a dataset of 2,660 SAT problems with varying\ndifficulty. It supports the evaluation of how LLM reasoning changes with\nproblem difficulty. We apply Saturn to DeepSeek-R1-Distill-Qwen and obtain\nSaturn-1.5B and Saturn-7B. We achieve several notable results: (1) On SAT\nproblems, Saturn-1.5B and Saturn-7B achieve average pass@3 improvements of\n+14.0 and +28.1, respectively. (2) On math and programming tasks, Saturn-1.5B\nand Saturn-7B improve average scores by +4.9 and +1.8 on benchmarks (e.g.,\nAIME, LiveCodeBench). (3) Compared to the state-of-the-art (SOTA) approach in\nconstructing RL tasks, Saturn achieves further improvements of +8.8%. We\nrelease the source code, data, and models to support future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How to design reinforcement learning (RL) tasks that effectively unleash the\nreasoning capability of large language models (LLMs) remains an open question.\nExisting RL tasks (e.g., math, programming, and constructing reasoning tasks)\nsuffer from three key limitations: (1) Scalability. They rely heavily on human\nannotation or expensive LLM synthesis to generate sufficient training data. (2)\nVerifiability. LLMs' outputs are hard to verify automatically and reliably. (3)\nControllable Difficulty. Most tasks lack fine-grained difficulty control,\nmaking it hard to train LLMs to develop reasoning ability from easy to hard.\n  To address these limitations, we propose Saturn, a SAT-based RL framework\nthat uses Boolean Satisfiability (SAT) problems to train and evaluate LLMs\nreasoning. Saturn enables scalable task construction, rule-based verification,\nand precise difficulty control. Saturn designs a curriculum learning pipeline\nthat continuously improves LLMs' reasoning capability by constructing SAT tasks\nof increasing difficulty and training LLMs from easy to hard. To ensure stable\ntraining, we design a principled mechanism to control difficulty transitions.\n  We introduce Saturn-2.6k, a dataset of 2,660 SAT problems with varying\ndifficulty. It supports the evaluation of how LLM reasoning changes with\nproblem difficulty. We apply Saturn to DeepSeek-R1-Distill-Qwen and obtain\nSaturn-1.5B and Saturn-7B. We achieve several notable results: (1) On SAT\nproblems, Saturn-1.5B and Saturn-7B achieve average pass@3 improvements of\n+14.0 and +28.1, respectively. (2) On math and programming tasks, Saturn-1.5B\nand Saturn-7B improve average scores by +4.9 and +1.8 on benchmarks (e.g.,\nAIME, LiveCodeBench). (3) Compared to the state-of-the-art (SOTA) approach in\nconstructing RL tasks, Saturn achieves further improvements of +8.8%. We\nrelease the source code, data, and models to support future research."
                },
                "authors": [
                    {
                        "name": "Huanyu Liu"
                    },
                    {
                        "name": "Jia Li"
                    },
                    {
                        "name": "Hao Zhu"
                    },
                    {
                        "name": "Kechi Zhang"
                    },
                    {
                        "name": "Yihong Dong"
                    },
                    {
                        "name": "Ge Li"
                    }
                ],
                "author_detail": {
                    "name": "Ge Li"
                },
                "author": "Ge Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16368v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16368v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.07870v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.07870v3",
                "updated": "2025-10-29T12:00:29Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    12,
                    0,
                    29,
                    2,
                    302,
                    0
                ],
                "published": "2023-08-15T16:37:16Z",
                "published_parsed": [
                    2023,
                    8,
                    15,
                    16,
                    37,
                    16,
                    1,
                    227,
                    0
                ],
                "title": "Brain-inspired Computational Intelligence via Predictive Coding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Brain-inspired Computational Intelligence via Predictive Coding"
                },
                "summary": "Artificial intelligence (AI) is rapidly becoming one of the key technologies\nof this century. The majority of results in AI thus far have been achieved\nusing deep neural networks trained with a learning algorithm called error\nbackpropagation, always considered biologically implausible. To this end,\nrecent works have studied learning algorithms for deep neural networks inspired\nby the neurosciences. One such theory, called predictive coding (PC), has shown\npromising properties that make it potentially valuable for the machine learning\ncommunity: it can model information processing in different areas of the brain,\ncan be used in control and robotics, has a solid mathematical foundation in\nvariational inference, and performs its computations asynchronously. Inspired\nby such properties, works that propose novel PC-like algorithms are starting to\nbe present in multiple sub-fields of machine learning and AI at large. Here, we\nsurvey such efforts by first providing a broad overview of the history of PC to\nprovide common ground for the understanding of the recent developments, then by\ndescribing current efforts and results, and concluding with a large discussion\nof possible implications and ways forward.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial intelligence (AI) is rapidly becoming one of the key technologies\nof this century. The majority of results in AI thus far have been achieved\nusing deep neural networks trained with a learning algorithm called error\nbackpropagation, always considered biologically implausible. To this end,\nrecent works have studied learning algorithms for deep neural networks inspired\nby the neurosciences. One such theory, called predictive coding (PC), has shown\npromising properties that make it potentially valuable for the machine learning\ncommunity: it can model information processing in different areas of the brain,\ncan be used in control and robotics, has a solid mathematical foundation in\nvariational inference, and performs its computations asynchronously. Inspired\nby such properties, works that propose novel PC-like algorithms are starting to\nbe present in multiple sub-fields of machine learning and AI at large. Here, we\nsurvey such efforts by first providing a broad overview of the history of PC to\nprovide common ground for the understanding of the recent developments, then by\ndescribing current efforts and results, and concluding with a large discussion\nof possible implications and ways forward."
                },
                "authors": [
                    {
                        "name": "Tommaso Salvatori"
                    },
                    {
                        "name": "Ankur Mali"
                    },
                    {
                        "name": "Christopher L. Buckley"
                    },
                    {
                        "name": "Thomas Lukasiewicz"
                    },
                    {
                        "name": "Rajesh P. N. Rao"
                    },
                    {
                        "name": "Karl Friston"
                    },
                    {
                        "name": "Alexander Ororbia"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Ororbia"
                },
                "author": "Alexander Ororbia",
                "arxiv_comment": "26 Pages, 9 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2308.07870v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.07870v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25434v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25434v1",
                "updated": "2025-10-29T11:57:03Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    11,
                    57,
                    3,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T11:57:03Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    11,
                    57,
                    3,
                    2,
                    302,
                    0
                ],
                "title": "A Critical Study of Automatic Evaluation in Sign Language Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Critical Study of Automatic Evaluation in Sign Language Translation"
                },
                "summary": "Automatic evaluation metrics are crucial for advancing sign language\ntranslation (SLT). Current SLT evaluation metrics, such as BLEU and ROUGE, are\nonly text-based, and it remains unclear to what extent text-based metrics can\nreliably capture the quality of SLT outputs. To address this gap, we\ninvestigate the limitations of text-based SLT evaluation metrics by analyzing\nsix metrics, including BLEU, chrF, and ROUGE, as well as BLEURT on the one\nhand, and large language model (LLM)-based evaluators such as G-Eval and GEMBA\nzero-shot direct assessment on the other hand. Specifically, we assess the\nconsistency and robustness of these metrics under three controlled conditions:\nparaphrasing, hallucinations in model outputs, and variations in sentence\nlength. Our analysis highlights the limitations of lexical overlap metrics and\ndemonstrates that while LLM-based evaluators better capture semantic\nequivalence often missed by conventional metrics, they can also exhibit bias\ntoward LLM-paraphrased translations. Moreover, although all metrics are able to\ndetect hallucinations, BLEU tends to be overly sensitive, whereas BLEURT and\nLLM-based evaluators are comparatively lenient toward subtle cases. This\nmotivates the need for multimodal evaluation frameworks that extend beyond\ntext-based metrics to enable a more holistic assessment of SLT outputs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic evaluation metrics are crucial for advancing sign language\ntranslation (SLT). Current SLT evaluation metrics, such as BLEU and ROUGE, are\nonly text-based, and it remains unclear to what extent text-based metrics can\nreliably capture the quality of SLT outputs. To address this gap, we\ninvestigate the limitations of text-based SLT evaluation metrics by analyzing\nsix metrics, including BLEU, chrF, and ROUGE, as well as BLEURT on the one\nhand, and large language model (LLM)-based evaluators such as G-Eval and GEMBA\nzero-shot direct assessment on the other hand. Specifically, we assess the\nconsistency and robustness of these metrics under three controlled conditions:\nparaphrasing, hallucinations in model outputs, and variations in sentence\nlength. Our analysis highlights the limitations of lexical overlap metrics and\ndemonstrates that while LLM-based evaluators better capture semantic\nequivalence often missed by conventional metrics, they can also exhibit bias\ntoward LLM-paraphrased translations. Moreover, although all metrics are able to\ndetect hallucinations, BLEU tends to be overly sensitive, whereas BLEURT and\nLLM-based evaluators are comparatively lenient toward subtle cases. This\nmotivates the need for multimodal evaluation frameworks that extend beyond\ntext-based metrics to enable a more holistic assessment of SLT outputs."
                },
                "authors": [
                    {
                        "name": "Shakib Yazdani"
                    },
                    {
                        "name": "Yasser Hamidullah"
                    },
                    {
                        "name": "Cristina España-Bonet"
                    },
                    {
                        "name": "Eleftherios Avramidis"
                    },
                    {
                        "name": "Josef van Genabith"
                    }
                ],
                "author_detail": {
                    "name": "Josef van Genabith"
                },
                "author": "Josef van Genabith",
                "arxiv_comment": "Submitted to the LREC 2026 conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25434v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25434v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25432v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25432v1",
                "updated": "2025-10-29T11:55:21Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    11,
                    55,
                    21,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T11:55:21Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    11,
                    55,
                    21,
                    2,
                    302,
                    0
                ],
                "title": "Depth and Autonomy: A Framework for Evaluating LLM Applications in\n  Social Science Research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Depth and Autonomy: A Framework for Evaluating LLM Applications in\n  Social Science Research"
                },
                "summary": "Large language models (LLMs) are increasingly utilized by researchers across\na wide range of domains, and qualitative social science is no exception;\nhowever, this adoption faces persistent challenges, including interpretive\nbias, low reliability, and weak auditability. We introduce a framework that\nsituates LLM usage along two dimensions, interpretive depth and autonomy,\nthereby offering a straightforward way to classify LLM applications in\nqualitative research and to derive practical design recommendations. We present\nthe state of the literature with respect to these two dimensions, based on all\npublished social science papers available on Web of Science that use LLMs as a\ntool and not strictly as the subject of study. Rather than granting models\nexpansive freedom, our approach encourages researchers to decompose tasks into\nmanageable segments, much as they would when delegating work to capable\nundergraduate research assistants. By maintaining low levels of autonomy and\nselectively increasing interpretive depth only where warranted and under\nsupervision, one can plausibly reap the benefits of LLMs while preserving\ntransparency and reliability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly utilized by researchers across\na wide range of domains, and qualitative social science is no exception;\nhowever, this adoption faces persistent challenges, including interpretive\nbias, low reliability, and weak auditability. We introduce a framework that\nsituates LLM usage along two dimensions, interpretive depth and autonomy,\nthereby offering a straightforward way to classify LLM applications in\nqualitative research and to derive practical design recommendations. We present\nthe state of the literature with respect to these two dimensions, based on all\npublished social science papers available on Web of Science that use LLMs as a\ntool and not strictly as the subject of study. Rather than granting models\nexpansive freedom, our approach encourages researchers to decompose tasks into\nmanageable segments, much as they would when delegating work to capable\nundergraduate research assistants. By maintaining low levels of autonomy and\nselectively increasing interpretive depth only where warranted and under\nsupervision, one can plausibly reap the benefits of LLMs while preserving\ntransparency and reliability."
                },
                "authors": [
                    {
                        "name": "Ali Sanaei"
                    },
                    {
                        "name": "Ali Rajabzadeh"
                    }
                ],
                "author_detail": {
                    "name": "Ali Rajabzadeh"
                },
                "author": "Ali Rajabzadeh",
                "arxiv_comment": "Presented at the Annual Meeting of the American Political Science\n  Association, Vancouver, BC, September 11--14 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25432v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25432v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25428v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25428v1",
                "updated": "2025-10-29T11:50:52Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    11,
                    50,
                    52,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T11:50:52Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    11,
                    50,
                    52,
                    2,
                    302,
                    0
                ],
                "title": "Alibaba International E-commerce Product Search Competition DcuRAGONs\n  Team Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Alibaba International E-commerce Product Search Competition DcuRAGONs\n  Team Technical Report"
                },
                "summary": "This report details our methodology and results developed for the\nMultilingual E-commerce Search Competition. The problem aims to recognize\nrelevance between user queries versus product items in a multilingual context\nand improve recommendation performance on e-commerce platforms. Utilizing Large\nLanguage Models (LLMs) and their capabilities in other tasks, our data-centric\nmethod achieved the highest score compared to other solutions during the\ncompetition. Final leaderboard is publised at\nhttps://alibaba-international-cikm2025.github.io. The source code for our\nproject is published at https://github.com/nhtlongcs/e-commerce-product-search.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This report details our methodology and results developed for the\nMultilingual E-commerce Search Competition. The problem aims to recognize\nrelevance between user queries versus product items in a multilingual context\nand improve recommendation performance on e-commerce platforms. Utilizing Large\nLanguage Models (LLMs) and their capabilities in other tasks, our data-centric\nmethod achieved the highest score compared to other solutions during the\ncompetition. Final leaderboard is publised at\nhttps://alibaba-international-cikm2025.github.io. The source code for our\nproject is published at https://github.com/nhtlongcs/e-commerce-product-search."
                },
                "authors": [
                    {
                        "name": "Thang-Long Nguyen-Ho"
                    },
                    {
                        "name": "Minh-Khoi Pham"
                    },
                    {
                        "name": "Hoang-Bao Le"
                    }
                ],
                "author_detail": {
                    "name": "Hoang-Bao Le"
                },
                "author": "Hoang-Bao Le",
                "arxiv_comment": "Alibaba International E-commerce Product Search Competition @ CIKM\n  2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25428v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25428v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25427v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25427v1",
                "updated": "2025-10-29T11:49:49Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    11,
                    49,
                    49,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T11:49:49Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    11,
                    49,
                    49,
                    2,
                    302,
                    0
                ],
                "title": "RLMEval: Evaluating Research-Level Neural Theorem Proving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RLMEval: Evaluating Research-Level Neural Theorem Proving"
                },
                "summary": "Despite impressive results on curated benchmarks, the practical impact of\nlarge language models (LLMs) on research-level neural theorem proving and proof\nautoformalization is still limited. We introduce RLMEval, an evaluation suite\nfor these tasks, focusing on research-level mathematics from real-world Lean\nformalization projects. RLMEval targets the evaluation of neural theorem\nproving and proof autoformalization on challenging research-level theorems by\nleveraging real Lean Blueprint formalization projects. Our evaluation of\nstate-of-the-art models on RLMEval, comprising 613 theorems from 6 Lean\nprojects, reveals a significant gap: progress on existing benchmarks does not\nreadily translate to these more realistic settings, with the best model\nachieving only a 10.3 % pass rate. RLMEval provides a new, challenging\nbenchmark designed to guide and accelerate progress in automated reasoning for\nformal mathematics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite impressive results on curated benchmarks, the practical impact of\nlarge language models (LLMs) on research-level neural theorem proving and proof\nautoformalization is still limited. We introduce RLMEval, an evaluation suite\nfor these tasks, focusing on research-level mathematics from real-world Lean\nformalization projects. RLMEval targets the evaluation of neural theorem\nproving and proof autoformalization on challenging research-level theorems by\nleveraging real Lean Blueprint formalization projects. Our evaluation of\nstate-of-the-art models on RLMEval, comprising 613 theorems from 6 Lean\nprojects, reveals a significant gap: progress on existing benchmarks does not\nreadily translate to these more realistic settings, with the best model\nachieving only a 10.3 % pass rate. RLMEval provides a new, challenging\nbenchmark designed to guide and accelerate progress in automated reasoning for\nformal mathematics."
                },
                "authors": [
                    {
                        "name": "Auguste Poiroux"
                    },
                    {
                        "name": "Antoine Bosselut"
                    },
                    {
                        "name": "Viktor Kunčak"
                    }
                ],
                "author_detail": {
                    "name": "Viktor Kunčak"
                },
                "author": "Viktor Kunčak",
                "arxiv_comment": "Accepted to EMNLP 2025 Findings. RLMEval benchmark released:\n  https://github.com/augustepoiroux/RLMEval",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25427v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25427v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25426v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25426v1",
                "updated": "2025-10-29T11:49:42Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    11,
                    49,
                    42,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T11:49:42Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    11,
                    49,
                    42,
                    2,
                    302,
                    0
                ],
                "title": "Implicature in Interaction: Understanding Implicature Improves Alignment\n  in Human-LLM Interaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Implicature in Interaction: Understanding Implicature Improves Alignment\n  in Human-LLM Interaction"
                },
                "summary": "The rapid advancement of Large Language Models (LLMs) is positioning language\nat the core of human-computer interaction (HCI). We argue that advancing HCI\nrequires attention to the linguistic foundations of interaction, particularly\nimplicature (meaning conveyed beyond explicit statements through shared\ncontext) which is essential for human-AI (HAI) alignment. This study examines\nLLMs' ability to infer user intent embedded in context-driven prompts and\nwhether understanding implicature improves response generation. Results show\nthat larger models approximate human interpretations more closely, while\nsmaller models struggle with implicature inference. Furthermore,\nimplicature-based prompts significantly enhance the perceived relevance and\nquality of responses across models, with notable gains in smaller models.\nOverall, 67.6% of participants preferred responses with implicature-embedded\nprompts to literal ones, highlighting a clear preference for contextually\nnuanced communication. Our work contributes to understanding how linguistic\ntheory can be used to address the alignment problem by making HAI interaction\nmore natural and contextually grounded.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of Large Language Models (LLMs) is positioning language\nat the core of human-computer interaction (HCI). We argue that advancing HCI\nrequires attention to the linguistic foundations of interaction, particularly\nimplicature (meaning conveyed beyond explicit statements through shared\ncontext) which is essential for human-AI (HAI) alignment. This study examines\nLLMs' ability to infer user intent embedded in context-driven prompts and\nwhether understanding implicature improves response generation. Results show\nthat larger models approximate human interpretations more closely, while\nsmaller models struggle with implicature inference. Furthermore,\nimplicature-based prompts significantly enhance the perceived relevance and\nquality of responses across models, with notable gains in smaller models.\nOverall, 67.6% of participants preferred responses with implicature-embedded\nprompts to literal ones, highlighting a clear preference for contextually\nnuanced communication. Our work contributes to understanding how linguistic\ntheory can be used to address the alignment problem by making HAI interaction\nmore natural and contextually grounded."
                },
                "authors": [
                    {
                        "name": "Asutosh Hota"
                    },
                    {
                        "name": "Jussi P. P. Jokinen"
                    }
                ],
                "author_detail": {
                    "name": "Jussi P. P. Jokinen"
                },
                "author": "Jussi P. P. Jokinen",
                "arxiv_comment": "The manuscript is approximately 7360 words and contains 12 figures\n  and 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25426v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25426v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25424v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25424v1",
                "updated": "2025-10-29T11:48:01Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    11,
                    48,
                    1,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T11:48:01Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    11,
                    48,
                    1,
                    2,
                    302,
                    0
                ],
                "title": "Inferring Mobility Reductions from COVID-19 Disease Spread along the\n  Urban-Rural Gradient",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring Mobility Reductions from COVID-19 Disease Spread along the\n  Urban-Rural Gradient"
                },
                "summary": "The COVID-19 pandemic reshaped human mobility through policy interventions\nand voluntary behavioral changes. Mobility adaptions helped mitigate pandemic\nspread, however our knowledge which environmental, social, and demographic\nfactors helped mobility reduction and pandemic mitigation is patchy. We\nintroduce a Bayesian hierarchical model to quantify heterogeneity in mobility\nresponses across time and space in Germany's 400 districts using anonymized\nmobile phone data. Decomposing mobility into a disease-responsive component and\ndisease-independent factors (temperature, school vacations, public holidays)\nallows us to quantify the impact of each factor. We find significant\ndifferences in reaction to disease spread along the urban-rural gradient, with\nlarge cities reducing mobility most strongly. Employment sectors further help\nexplain variance in reaction strength during the first wave, while political\nvariables gain significance during the second wave. However, reduced mobility\nonly partially translates to lower peak incidence, indicating the influence of\nother hidden factors. Our results identify key drivers of mobility reductions\nand demonstrate that mobility behavior can serve as an operational proxy for\npopulation response.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The COVID-19 pandemic reshaped human mobility through policy interventions\nand voluntary behavioral changes. Mobility adaptions helped mitigate pandemic\nspread, however our knowledge which environmental, social, and demographic\nfactors helped mobility reduction and pandemic mitigation is patchy. We\nintroduce a Bayesian hierarchical model to quantify heterogeneity in mobility\nresponses across time and space in Germany's 400 districts using anonymized\nmobile phone data. Decomposing mobility into a disease-responsive component and\ndisease-independent factors (temperature, school vacations, public holidays)\nallows us to quantify the impact of each factor. We find significant\ndifferences in reaction to disease spread along the urban-rural gradient, with\nlarge cities reducing mobility most strongly. Employment sectors further help\nexplain variance in reaction strength during the first wave, while political\nvariables gain significance during the second wave. However, reduced mobility\nonly partially translates to lower peak incidence, indicating the influence of\nother hidden factors. Our results identify key drivers of mobility reductions\nand demonstrate that mobility behavior can serve as an operational proxy for\npopulation response."
                },
                "authors": [
                    {
                        "name": "Sydney Paltra"
                    },
                    {
                        "name": "Jonas Dehning"
                    },
                    {
                        "name": "Viola Priesemann"
                    },
                    {
                        "name": "Kai Nagel"
                    }
                ],
                "author_detail": {
                    "name": "Kai Nagel"
                },
                "author": "Kai Nagel",
                "arxiv_comment": "41 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25424v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25424v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.data-an",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25421v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25421v1",
                "updated": "2025-10-29T11:40:38Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    11,
                    40,
                    38,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T11:40:38Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    11,
                    40,
                    38,
                    2,
                    302,
                    0
                ],
                "title": "Small Talk, Big Impact? LLM-based Conversational Agents to Mitigate\n  Passive Fatigue in Conditional Automated Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Small Talk, Big Impact? LLM-based Conversational Agents to Mitigate\n  Passive Fatigue in Conditional Automated Driving"
                },
                "summary": "Passive fatigue during conditional automated driving can compromise driver\nreadiness and safety. This paper presents findings from a test-track study with\n40 participants in a real-world rural automated driving scenario. In this\nscenario, a Large Language Model (LLM) based conversational agent (CA) was\ndesigned to check in with drivers and re-engage them with their surroundings.\nDrawing on in-car video recordings, sleepiness ratings and interviews, we\nanalysed how drivers interacted with the agent and how these interactions\nshaped alertness. Users found the CA helpful for supporting vigilance during\npassive fatigue. Thematic analysis of acceptability further revealed three user\npreference profiles that implicate future intention to use CAs. Positioning\nempirically observed profiles within existing CA archetype frameworks\nhighlights the need for adaptive design sensitive to diverse user groups. This\nwork underscores the potential of CAs as proactive Human-Machine Interface\n(HMI) interventions, demonstrating how natural language can support\ncontext-aware interaction during automated driving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Passive fatigue during conditional automated driving can compromise driver\nreadiness and safety. This paper presents findings from a test-track study with\n40 participants in a real-world rural automated driving scenario. In this\nscenario, a Large Language Model (LLM) based conversational agent (CA) was\ndesigned to check in with drivers and re-engage them with their surroundings.\nDrawing on in-car video recordings, sleepiness ratings and interviews, we\nanalysed how drivers interacted with the agent and how these interactions\nshaped alertness. Users found the CA helpful for supporting vigilance during\npassive fatigue. Thematic analysis of acceptability further revealed three user\npreference profiles that implicate future intention to use CAs. Positioning\nempirically observed profiles within existing CA archetype frameworks\nhighlights the need for adaptive design sensitive to diverse user groups. This\nwork underscores the potential of CAs as proactive Human-Machine Interface\n(HMI) interventions, demonstrating how natural language can support\ncontext-aware interaction during automated driving."
                },
                "authors": [
                    {
                        "name": "Lewis Cockram"
                    },
                    {
                        "name": "Yueteng Yu"
                    },
                    {
                        "name": "Jorge Pardo"
                    },
                    {
                        "name": "Xiaomeng Li"
                    },
                    {
                        "name": "Andry Rakotonirainy"
                    },
                    {
                        "name": "Jonny Kuo"
                    },
                    {
                        "name": "Sebastien Demmel"
                    },
                    {
                        "name": "Mike Lenné"
                    },
                    {
                        "name": "Ronald Schroeter"
                    }
                ],
                "author_detail": {
                    "name": "Ronald Schroeter"
                },
                "author": "Ronald Schroeter",
                "arxiv_comment": "Submitted to CHI '26 Conference on Human Factors in Computing Systems",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25421v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25421v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25420v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25420v1",
                "updated": "2025-10-29T11:40:06Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    11,
                    40,
                    6,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T11:40:06Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    11,
                    40,
                    6,
                    2,
                    302,
                    0
                ],
                "title": "Improving Temporal Consistency and Fidelity at Inference-time in\n  Perceptual Video Restoration by Zero-shot Image-based Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Temporal Consistency and Fidelity at Inference-time in\n  Perceptual Video Restoration by Zero-shot Image-based Diffusion Models"
                },
                "summary": "Diffusion models have emerged as powerful priors for single-image\nrestoration, but their application to zero-shot video restoration suffers from\ntemporal inconsistencies due to the stochastic nature of sampling and\ncomplexity of incorporating explicit temporal modeling. In this work, we\naddress the challenge of improving temporal coherence in video restoration\nusing zero-shot image-based diffusion models without retraining or modifying\ntheir architecture. We propose two complementary inference-time strategies: (1)\nPerceptual Straightening Guidance (PSG) based on the neuroscience-inspired\nperceptual straightening hypothesis, which steers the diffusion denoising\nprocess towards smoother temporal evolution by incorporating a curvature\npenalty in a perceptual space to improve temporal perceptual scores, such as\nFr\\'echet Video Distance (FVD) and perceptual straightness; and (2) Multi-Path\nEnsemble Sampling (MPES), which aims at reducing stochastic variation by\nensembling multiple diffusion trajectories to improve fidelity (distortion)\nscores, such as PSNR and SSIM, without sacrificing sharpness. Together, these\ntraining-free techniques provide a practical path toward temporally stable\nhigh-fidelity perceptual video restoration using large pretrained diffusion\nmodels. We performed extensive experiments over multiple datasets and\ndegradation types, systematically evaluating each strategy to understand their\nstrengths and limitations. Our results show that while PSG enhances temporal\nnaturalness, particularly in case of temporal blur, MPES consistently improves\nfidelity and spatio-temporal perception--distortion trade-off across all tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have emerged as powerful priors for single-image\nrestoration, but their application to zero-shot video restoration suffers from\ntemporal inconsistencies due to the stochastic nature of sampling and\ncomplexity of incorporating explicit temporal modeling. In this work, we\naddress the challenge of improving temporal coherence in video restoration\nusing zero-shot image-based diffusion models without retraining or modifying\ntheir architecture. We propose two complementary inference-time strategies: (1)\nPerceptual Straightening Guidance (PSG) based on the neuroscience-inspired\nperceptual straightening hypothesis, which steers the diffusion denoising\nprocess towards smoother temporal evolution by incorporating a curvature\npenalty in a perceptual space to improve temporal perceptual scores, such as\nFr\\'echet Video Distance (FVD) and perceptual straightness; and (2) Multi-Path\nEnsemble Sampling (MPES), which aims at reducing stochastic variation by\nensembling multiple diffusion trajectories to improve fidelity (distortion)\nscores, such as PSNR and SSIM, without sacrificing sharpness. Together, these\ntraining-free techniques provide a practical path toward temporally stable\nhigh-fidelity perceptual video restoration using large pretrained diffusion\nmodels. We performed extensive experiments over multiple datasets and\ndegradation types, systematically evaluating each strategy to understand their\nstrengths and limitations. Our results show that while PSG enhances temporal\nnaturalness, particularly in case of temporal blur, MPES consistently improves\nfidelity and spatio-temporal perception--distortion trade-off across all tasks."
                },
                "authors": [
                    {
                        "name": "Nasrin Rahimi"
                    },
                    {
                        "name": "A. Murat Tekalp"
                    }
                ],
                "author_detail": {
                    "name": "A. Murat Tekalp"
                },
                "author": "A. Murat Tekalp",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25420v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25420v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25412v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25412v1",
                "updated": "2025-10-29T11:29:03Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    11,
                    29,
                    3,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T11:29:03Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    11,
                    29,
                    3,
                    2,
                    302,
                    0
                ],
                "title": "Serve Programs, Not Prompts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serve Programs, Not Prompts"
                },
                "summary": "Current large language model (LLM) serving systems, primarily designed for\ntext completion, are neither efficient nor adaptable for increasingly complex\nLLM applications due to their inflexible design. We propose a new LLM serving\nsystem architecture that serves programs instead of prompts to address this\nproblem. These programs, called LLM Inference Programs (LIPs), allow users to\ncustomize token prediction and KV cache management at runtime and to offload\nparts of their application logic, such as tool execution, to the server. We\ndescribe an example of this architecture through a system named Symphony, which\nfunctions as an operating system for LIPs. Symphony exposes LLM model\ncomputations via system calls and virtualizes KV cache with a dedicated file\nsystem, while ensuring GPU efficiency with a two-level process scheduling\nscheme. Symphony has the potential to open the door to a more efficient and\nextensible ecosystem for LLM applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current large language model (LLM) serving systems, primarily designed for\ntext completion, are neither efficient nor adaptable for increasingly complex\nLLM applications due to their inflexible design. We propose a new LLM serving\nsystem architecture that serves programs instead of prompts to address this\nproblem. These programs, called LLM Inference Programs (LIPs), allow users to\ncustomize token prediction and KV cache management at runtime and to offload\nparts of their application logic, such as tool execution, to the server. We\ndescribe an example of this architecture through a system named Symphony, which\nfunctions as an operating system for LIPs. Symphony exposes LLM model\ncomputations via system calls and virtualizes KV cache with a dedicated file\nsystem, while ensuring GPU efficiency with a two-level process scheduling\nscheme. Symphony has the potential to open the door to a more efficient and\nextensible ecosystem for LLM applications."
                },
                "authors": [
                    {
                        "name": "In Gim"
                    },
                    {
                        "name": "Lin Zhong"
                    }
                ],
                "author_detail": {
                    "name": "Lin Zhong"
                },
                "author": "Lin Zhong",
                "arxiv_doi": "10.1145/3713082.3730398",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3713082.3730398",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.25412v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25412v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "HotOS 2025. Follow-up implementation work (SOSP 2025) is available at\n  arXiv:2510.24051",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25409v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25409v2",
                "updated": "2025-10-30T10:48:05Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    10,
                    48,
                    5,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-29T11:27:08Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    11,
                    27,
                    8,
                    2,
                    302,
                    0
                ],
                "title": "BhashaBench V1: A Comprehensive Benchmark for the Quadrant of Indic\n  Domains",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BhashaBench V1: A Comprehensive Benchmark for the Quadrant of Indic\n  Domains"
                },
                "summary": "The rapid advancement of large language models(LLMs) has intensified the need\nfor domain and culture specific evaluation. Existing benchmarks are largely\nAnglocentric and domain-agnostic, limiting their applicability to India-centric\ncontexts. To address this gap, we introduce BhashaBench V1, the first\ndomain-specific, multi-task, bilingual benchmark focusing on critical Indic\nknowledge systems. BhashaBench V1 contains 74,166 meticulously curated\nquestion-answer pairs, with 52,494 in English and 21,672 in Hindi, sourced from\nauthentic government and domain-specific exams. It spans four major domains:\nAgriculture, Legal, Finance, and Ayurveda, comprising 90+ subdomains and\ncovering 500+ topics, enabling fine-grained evaluation. Evaluation of 29+ LLMs\nreveals significant domain and language specific performance gaps, with\nespecially large disparities in low-resource domains. For instance, GPT-4o\nachieves 76.49% overall accuracy in Legal but only 59.74% in Ayurveda. Models\nconsistently perform better on English content compared to Hindi across all\ndomains. Subdomain-level analysis shows that areas such as Cyber Law,\nInternational Finance perform relatively well, while Panchakarma, Seed Science,\nand Human Rights remain notably weak. BhashaBench V1 provides a comprehensive\ndataset for evaluating large language models across India's diverse knowledge\ndomains. It enables assessment of models' ability to integrate domain-specific\nknowledge with bilingual understanding. All code, benchmarks, and resources are\npublicly available to support open research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of large language models(LLMs) has intensified the need\nfor domain and culture specific evaluation. Existing benchmarks are largely\nAnglocentric and domain-agnostic, limiting their applicability to India-centric\ncontexts. To address this gap, we introduce BhashaBench V1, the first\ndomain-specific, multi-task, bilingual benchmark focusing on critical Indic\nknowledge systems. BhashaBench V1 contains 74,166 meticulously curated\nquestion-answer pairs, with 52,494 in English and 21,672 in Hindi, sourced from\nauthentic government and domain-specific exams. It spans four major domains:\nAgriculture, Legal, Finance, and Ayurveda, comprising 90+ subdomains and\ncovering 500+ topics, enabling fine-grained evaluation. Evaluation of 29+ LLMs\nreveals significant domain and language specific performance gaps, with\nespecially large disparities in low-resource domains. For instance, GPT-4o\nachieves 76.49% overall accuracy in Legal but only 59.74% in Ayurveda. Models\nconsistently perform better on English content compared to Hindi across all\ndomains. Subdomain-level analysis shows that areas such as Cyber Law,\nInternational Finance perform relatively well, while Panchakarma, Seed Science,\nand Human Rights remain notably weak. BhashaBench V1 provides a comprehensive\ndataset for evaluating large language models across India's diverse knowledge\ndomains. It enables assessment of models' ability to integrate domain-specific\nknowledge with bilingual understanding. All code, benchmarks, and resources are\npublicly available to support open research."
                },
                "authors": [
                    {
                        "name": "Vijay Devane"
                    },
                    {
                        "name": "Mohd Nauman"
                    },
                    {
                        "name": "Bhargav Patel"
                    },
                    {
                        "name": "Aniket Mahendra Wakchoure"
                    },
                    {
                        "name": "Yogeshkumar Sant"
                    },
                    {
                        "name": "Shyam Pawar"
                    },
                    {
                        "name": "Viraj Thakur"
                    },
                    {
                        "name": "Ananya Godse"
                    },
                    {
                        "name": "Sunil Patra"
                    },
                    {
                        "name": "Neha Maurya"
                    },
                    {
                        "name": "Suraj Racha"
                    },
                    {
                        "name": "Nitish Kamal Singh"
                    },
                    {
                        "name": "Ajay Nagpal"
                    },
                    {
                        "name": "Piyush Sawarkar"
                    },
                    {
                        "name": "Kundeshwar Vijayrao Pundalik"
                    },
                    {
                        "name": "Rohit Saluja"
                    },
                    {
                        "name": "Ganesh Ramakrishnan"
                    }
                ],
                "author_detail": {
                    "name": "Ganesh Ramakrishnan"
                },
                "author": "Ganesh Ramakrishnan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25409v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25409v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25404v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25404v1",
                "updated": "2025-10-29T11:21:55Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    11,
                    21,
                    55,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T11:21:55Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    11,
                    21,
                    55,
                    2,
                    302,
                    0
                ],
                "title": "GPTOpt: Towards Efficient LLM-Based Black-Box Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPTOpt: Towards Efficient LLM-Based Black-Box Optimization"
                },
                "summary": "Global optimization of expensive, derivative-free black-box functions demands\nextreme sample efficiency. Classical methods such as Bayesian Optimization (BO)\ncan be effective, but they often require careful parameter tuning to each\napplication domain. At the same time, Large Language Models (LLMs) have shown\nbroad capabilities, yet state-of-the-art models remain limited in solving\ncontinuous black-box optimization tasks. We introduce GPTOpt, an LLM-based\noptimization method that equips LLMs with continuous black-box optimization\ncapabilities. By fine-tuning large language models on extensive synthetic\ndatasets derived from diverse BO parameterizations, GPTOpt leverages LLM\npre-training to generalize across optimization tasks. On a variety of black-box\noptimization benchmarks, GPTOpt surpasses traditional optimizers, highlighting\nthe capacity of LLMs for advanced numerical reasoning and introducing a\nflexible framework for global optimization without parameter tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Global optimization of expensive, derivative-free black-box functions demands\nextreme sample efficiency. Classical methods such as Bayesian Optimization (BO)\ncan be effective, but they often require careful parameter tuning to each\napplication domain. At the same time, Large Language Models (LLMs) have shown\nbroad capabilities, yet state-of-the-art models remain limited in solving\ncontinuous black-box optimization tasks. We introduce GPTOpt, an LLM-based\noptimization method that equips LLMs with continuous black-box optimization\ncapabilities. By fine-tuning large language models on extensive synthetic\ndatasets derived from diverse BO parameterizations, GPTOpt leverages LLM\npre-training to generalize across optimization tasks. On a variety of black-box\noptimization benchmarks, GPTOpt surpasses traditional optimizers, highlighting\nthe capacity of LLMs for advanced numerical reasoning and introducing a\nflexible framework for global optimization without parameter tuning."
                },
                "authors": [
                    {
                        "name": "Jamison Meindl"
                    },
                    {
                        "name": "Yunsheng Tian"
                    },
                    {
                        "name": "Tony Cui"
                    },
                    {
                        "name": "Veronika Thost"
                    },
                    {
                        "name": "Zhang-Wei Hong"
                    },
                    {
                        "name": "Jie Chen"
                    },
                    {
                        "name": "Wojciech Matusik"
                    },
                    {
                        "name": "Mina Konaković Luković"
                    }
                ],
                "author_detail": {
                    "name": "Mina Konaković Luković"
                },
                "author": "Mina Konaković Luković",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25404v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25404v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25402v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25402v2",
                "updated": "2025-10-30T02:45:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    2,
                    45,
                    14,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-29T11:20:18Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    11,
                    20,
                    18,
                    2,
                    302,
                    0
                ],
                "title": "Towards Automated Quality Assurance of Patent Specifications: A\n  Multi-Dimensional LLM Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Automated Quality Assurance of Patent Specifications: A\n  Multi-Dimensional LLM Framework"
                },
                "summary": "Although AI drafting tools have gained prominence in patent writing, the\nsystematic evaluation of AI-generated patent content quality represents a\nsignificant research gap. To address this gap, We propose to evaluate patents\nusing regulatory compliance, technical coherence, and figure-reference\nconsistency detection modules, and then generate improvement suggestions via an\nintegration module. The framework is validated on a comprehensive dataset\ncomprising 80 human-authored and 80 AI-generated patents from two patent\ndrafting tools. Evaluation is performed on 10,841 total sentences, 8,924\nnon-template sentences, and 554 patent figures for the three detection modules\nrespectively, achieving balanced accuracies of 99.74%, 82.12%, and 91.2%\nagainst expert annotations. Additional analysis was conducted to examine defect\ndistributions across patent sections, technical domains, and authoring sources.\nSection-based analysis indicates that figure-text consistency and technical\ndetail precision require particular attention. Mechanical Engineering and\nConstruction show more claim-specification inconsistencies due to complex\ntechnical documentation requirements. AI-generated patents show a significant\ngap compared to human-authored ones. While human-authored patents primarily\ncontain surface-level errors like typos, AI-generated patents exhibit more\nstructural defects in figure-text alignment and cross-references.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although AI drafting tools have gained prominence in patent writing, the\nsystematic evaluation of AI-generated patent content quality represents a\nsignificant research gap. To address this gap, We propose to evaluate patents\nusing regulatory compliance, technical coherence, and figure-reference\nconsistency detection modules, and then generate improvement suggestions via an\nintegration module. The framework is validated on a comprehensive dataset\ncomprising 80 human-authored and 80 AI-generated patents from two patent\ndrafting tools. Evaluation is performed on 10,841 total sentences, 8,924\nnon-template sentences, and 554 patent figures for the three detection modules\nrespectively, achieving balanced accuracies of 99.74%, 82.12%, and 91.2%\nagainst expert annotations. Additional analysis was conducted to examine defect\ndistributions across patent sections, technical domains, and authoring sources.\nSection-based analysis indicates that figure-text consistency and technical\ndetail precision require particular attention. Mechanical Engineering and\nConstruction show more claim-specification inconsistencies due to complex\ntechnical documentation requirements. AI-generated patents show a significant\ngap compared to human-authored ones. While human-authored patents primarily\ncontain surface-level errors like typos, AI-generated patents exhibit more\nstructural defects in figure-text alignment and cross-references."
                },
                "authors": [
                    {
                        "name": "Yuqian Chai"
                    },
                    {
                        "name": "Chaochao Wang"
                    },
                    {
                        "name": "Weilei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Weilei Wang"
                },
                "author": "Weilei Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25402v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25402v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03690v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03690v2",
                "updated": "2025-10-29T11:04:12Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    11,
                    4,
                    12,
                    2,
                    302,
                    0
                ],
                "published": "2025-06-04T08:19:37Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    8,
                    19,
                    37,
                    2,
                    155,
                    0
                ],
                "title": "Robust Preference Optimization via Dynamic Target Margins",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Preference Optimization via Dynamic Target Margins"
                },
                "summary": "The alignment of Large Language Models (LLMs) is crucial for ensuring their\nsafety and reliability in practical applications. Direct Preference\nOptimization (DPO) has emerged as an efficient method that directly optimizes\nmodels using preference pairs, significantly reducing resource demands.\nHowever, the effectiveness of DPO heavily depends on the data quality, which is\nfrequently compromised by noise. In this work, we propose $\\gamma$-PO, a\ndynamic target margin preference optimization algorithm that adjust reward\nmargins at the pairwise level. By introducing instance-specific margin\ncalibration, $\\gamma$-PO strategically prioritizes high-confidence pairs (those\ndemonstrating higher reward margins) while suppressing potential noise from\nambiguous pairs. Moreover, $\\gamma$-PO is a plug-and-play method, compatible\nwith variants of DPO that rely on reward margin between preference pairs.\nAcross benchmarks such as AlpacaEval2 and Arena-Hard, $\\gamma$-PO achieves an\naverage 4.4\\% improvement over other baselines, setting new benchmarks for\nstate-of-the-art performance. Additionally, $\\gamma$-PO requires minimal code\nchanges and has a negligible impact on training efficiency, making it a robust\nsolution for enhancing LLMs alignment. Our codes are available at\n\\href{https://github.com/sunjie279/gammaPO}{https://github.com/sunjie279/gammaPO}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The alignment of Large Language Models (LLMs) is crucial for ensuring their\nsafety and reliability in practical applications. Direct Preference\nOptimization (DPO) has emerged as an efficient method that directly optimizes\nmodels using preference pairs, significantly reducing resource demands.\nHowever, the effectiveness of DPO heavily depends on the data quality, which is\nfrequently compromised by noise. In this work, we propose $\\gamma$-PO, a\ndynamic target margin preference optimization algorithm that adjust reward\nmargins at the pairwise level. By introducing instance-specific margin\ncalibration, $\\gamma$-PO strategically prioritizes high-confidence pairs (those\ndemonstrating higher reward margins) while suppressing potential noise from\nambiguous pairs. Moreover, $\\gamma$-PO is a plug-and-play method, compatible\nwith variants of DPO that rely on reward margin between preference pairs.\nAcross benchmarks such as AlpacaEval2 and Arena-Hard, $\\gamma$-PO achieves an\naverage 4.4\\% improvement over other baselines, setting new benchmarks for\nstate-of-the-art performance. Additionally, $\\gamma$-PO requires minimal code\nchanges and has a negligible impact on training efficiency, making it a robust\nsolution for enhancing LLMs alignment. Our codes are available at\n\\href{https://github.com/sunjie279/gammaPO}{https://github.com/sunjie279/gammaPO}."
                },
                "authors": [
                    {
                        "name": "Jie Sun"
                    },
                    {
                        "name": "Junkang Wu"
                    },
                    {
                        "name": "Jiancan Wu"
                    },
                    {
                        "name": "Zhibo Zhu"
                    },
                    {
                        "name": "Xingyu Lu"
                    },
                    {
                        "name": "Jun Zhou"
                    },
                    {
                        "name": "Lintao Ma"
                    },
                    {
                        "name": "Xiang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Wang"
                },
                "author": "Xiang Wang",
                "arxiv_comment": "18 pages, 6 figures, accepted to Findings of the 63rd Annual Meeting\n  of the Association for Computational Linguistics (ACL 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03690v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03690v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25388v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25388v1",
                "updated": "2025-10-29T11:03:44Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    11,
                    3,
                    44,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T11:03:44Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    11,
                    3,
                    44,
                    2,
                    302,
                    0
                ],
                "title": "Grouping Nodes With Known Value Differences: A Lossless UCT-based\n  Abstraction Algorithm",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Grouping Nodes With Known Value Differences: A Lossless UCT-based\n  Abstraction Algorithm"
                },
                "summary": "A core challenge of Monte Carlo Tree Search (MCTS) is its sample efficiency,\nwhich can be improved by grouping state-action pairs and using their aggregate\nstatistics instead of single-node statistics. On the Go Abstractions in Upper\nConfidence bounds applied to Trees (OGA-UCT) is the state-of-the-art MCTS\nabstraction algorithm for deterministic environments that builds its\nabstraction using the Abstractions of State-Action Pairs (ASAP) framework,\nwhich aims to detect states and state-action pairs with the same value under\noptimal play by analysing the search graph. ASAP, however, requires two\nstate-action pairs to have the same immediate reward, which is a rigid\ncondition that limits the number of abstractions that can be found and thereby\nthe sample efficiency. In this paper, we break with the paradigm of grouping\nvalue-equivalent states or state-action pairs and instead group states and\nstate-action pairs with possibly different values as long as the difference\nbetween their values can be inferred. We call this abstraction framework Known\nValue Difference Abstractions (KVDA), which infers the value differences by\nanalysis of the immediate rewards and modifies OGA-UCT to use this framework\ninstead. The modification is called KVDA-UCT, which detects significantly more\nabstractions than OGA-UCT, introduces no additional parameter, and outperforms\nOGA-UCT on a variety of deterministic environments and parameter settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A core challenge of Monte Carlo Tree Search (MCTS) is its sample efficiency,\nwhich can be improved by grouping state-action pairs and using their aggregate\nstatistics instead of single-node statistics. On the Go Abstractions in Upper\nConfidence bounds applied to Trees (OGA-UCT) is the state-of-the-art MCTS\nabstraction algorithm for deterministic environments that builds its\nabstraction using the Abstractions of State-Action Pairs (ASAP) framework,\nwhich aims to detect states and state-action pairs with the same value under\noptimal play by analysing the search graph. ASAP, however, requires two\nstate-action pairs to have the same immediate reward, which is a rigid\ncondition that limits the number of abstractions that can be found and thereby\nthe sample efficiency. In this paper, we break with the paradigm of grouping\nvalue-equivalent states or state-action pairs and instead group states and\nstate-action pairs with possibly different values as long as the difference\nbetween their values can be inferred. We call this abstraction framework Known\nValue Difference Abstractions (KVDA), which infers the value differences by\nanalysis of the immediate rewards and modifies OGA-UCT to use this framework\ninstead. The modification is called KVDA-UCT, which detects significantly more\nabstractions than OGA-UCT, introduces no additional parameter, and outperforms\nOGA-UCT on a variety of deterministic environments and parameter settings."
                },
                "authors": [
                    {
                        "name": "Robin Schmöcker"
                    },
                    {
                        "name": "Alexander Dockhorn"
                    },
                    {
                        "name": "Bodo Rosenhahn"
                    }
                ],
                "author_detail": {
                    "name": "Bodo Rosenhahn"
                },
                "author": "Bodo Rosenhahn",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25388v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25388v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18376v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18376v2",
                "updated": "2025-10-29T10:57:22Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    10,
                    57,
                    22,
                    2,
                    302,
                    0
                ],
                "published": "2025-09-22T19:58:17Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    19,
                    58,
                    17,
                    0,
                    265,
                    0
                ],
                "title": "GnnXemplar: Exemplars to Explanations -- Natural Language Rules for\n  Global GNN Interpretability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GnnXemplar: Exemplars to Explanations -- Natural Language Rules for\n  Global GNN Interpretability"
                },
                "summary": "Graph Neural Networks (GNNs) are widely used for node classification, yet\ntheir opaque decision-making limits trust and adoption. While local\nexplanations offer insights into individual predictions, global explanation\nmethods, those that characterize an entire class, remain underdeveloped.\nExisting global explainers rely on motif discovery in small graphs, an approach\nthat breaks down in large, real-world settings where subgraph repetition is\nrare, node attributes are high-dimensional, and predictions arise from complex\nstructure-attribute interactions. We propose GnnXemplar, a novel global\nexplainer inspired from Exemplar Theory from cognitive science. GnnXemplar\nidentifies representative nodes in the GNN embedding space, exemplars, and\nexplains predictions using natural language rules derived from their\nneighborhoods. Exemplar selection is framed as a coverage maximization problem\nover reverse k-nearest neighbors, for which we provide an efficient greedy\napproximation. To derive interpretable rules, we employ a self-refining prompt\nstrategy using large language models (LLMs). Experiments across diverse\nbenchmarks show that GnnXemplar significantly outperforms existing methods in\nfidelity, scalability, and human interpretability, as validated by a user study\nwith 60 participants.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNNs) are widely used for node classification, yet\ntheir opaque decision-making limits trust and adoption. While local\nexplanations offer insights into individual predictions, global explanation\nmethods, those that characterize an entire class, remain underdeveloped.\nExisting global explainers rely on motif discovery in small graphs, an approach\nthat breaks down in large, real-world settings where subgraph repetition is\nrare, node attributes are high-dimensional, and predictions arise from complex\nstructure-attribute interactions. We propose GnnXemplar, a novel global\nexplainer inspired from Exemplar Theory from cognitive science. GnnXemplar\nidentifies representative nodes in the GNN embedding space, exemplars, and\nexplains predictions using natural language rules derived from their\nneighborhoods. Exemplar selection is framed as a coverage maximization problem\nover reverse k-nearest neighbors, for which we provide an efficient greedy\napproximation. To derive interpretable rules, we employ a self-refining prompt\nstrategy using large language models (LLMs). Experiments across diverse\nbenchmarks show that GnnXemplar significantly outperforms existing methods in\nfidelity, scalability, and human interpretability, as validated by a user study\nwith 60 participants."
                },
                "authors": [
                    {
                        "name": "Burouj Armgaan"
                    },
                    {
                        "name": "Eshan Jain"
                    },
                    {
                        "name": "Harsh Pandey"
                    },
                    {
                        "name": "Mahesh Chandran"
                    },
                    {
                        "name": "Sayan Ranu"
                    }
                ],
                "author_detail": {
                    "name": "Sayan Ranu"
                },
                "author": "Sayan Ranu",
                "arxiv_comment": "38 pages, 20 figures, NeurIPS 2025 (Oral)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18376v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18376v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25384v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25384v1",
                "updated": "2025-10-29T10:55:52Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    10,
                    55,
                    52,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T10:55:52Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    10,
                    55,
                    52,
                    2,
                    302,
                    0
                ],
                "title": "Roleplaying with Structure: Synthetic Therapist-Client Conversation\n  Generation from Questionnaires",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Roleplaying with Structure: Synthetic Therapist-Client Conversation\n  Generation from Questionnaires"
                },
                "summary": "The development of AI for mental health is hindered by a lack of authentic\ntherapy dialogues, due to strict privacy regulations and the fact that clinical\nsessions were historically rarely recorded. We present an LLM-driven pipeline\nthat generates synthetic counseling dialogues based on structured client\nprofiles and psychological questionnaires. Grounded on the principles of\nCognitive Behavioral Therapy (CBT), our method creates synthetic therapeutic\nconversations for clinical disorders such as anxiety and depression. Our\nframework, SQPsych (Structured Questionnaire-based Psychotherapy), converts\nstructured psychological input into natural language dialogues through\ntherapist-client simulations. Due to data governance policies and privacy\nrestrictions prohibiting the transmission of clinical questionnaire data to\nthird-party services, previous methodologies relying on proprietary models are\ninfeasible in our setting. We address this limitation by generating a\nhigh-quality corpus using open-weight LLMs, validated through human expert\nevaluation and LLM-based assessments. Our SQPsychLLM models fine-tuned on\nSQPsychConv achieve strong performance on counseling benchmarks, surpassing\nbaselines in key therapeutic skills. Our findings highlight the potential of\nsynthetic data to enable scalable, data-secure, and clinically informed AI for\nmental health support. We will release our code, models, and corpus at\nhttps://ai-mh.github.io/SQPsych",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of AI for mental health is hindered by a lack of authentic\ntherapy dialogues, due to strict privacy regulations and the fact that clinical\nsessions were historically rarely recorded. We present an LLM-driven pipeline\nthat generates synthetic counseling dialogues based on structured client\nprofiles and psychological questionnaires. Grounded on the principles of\nCognitive Behavioral Therapy (CBT), our method creates synthetic therapeutic\nconversations for clinical disorders such as anxiety and depression. Our\nframework, SQPsych (Structured Questionnaire-based Psychotherapy), converts\nstructured psychological input into natural language dialogues through\ntherapist-client simulations. Due to data governance policies and privacy\nrestrictions prohibiting the transmission of clinical questionnaire data to\nthird-party services, previous methodologies relying on proprietary models are\ninfeasible in our setting. We address this limitation by generating a\nhigh-quality corpus using open-weight LLMs, validated through human expert\nevaluation and LLM-based assessments. Our SQPsychLLM models fine-tuned on\nSQPsychConv achieve strong performance on counseling benchmarks, surpassing\nbaselines in key therapeutic skills. Our findings highlight the potential of\nsynthetic data to enable scalable, data-secure, and clinically informed AI for\nmental health support. We will release our code, models, and corpus at\nhttps://ai-mh.github.io/SQPsych"
                },
                "authors": [
                    {
                        "name": "Doan Nam Long Vu"
                    },
                    {
                        "name": "Rui Tan"
                    },
                    {
                        "name": "Lena Moench"
                    },
                    {
                        "name": "Svenja Jule Francke"
                    },
                    {
                        "name": "Daniel Woiwod"
                    },
                    {
                        "name": "Florian Thomas-Odenthal"
                    },
                    {
                        "name": "Sanna Stroth"
                    },
                    {
                        "name": "Tilo Kircher"
                    },
                    {
                        "name": "Christiane Hermann"
                    },
                    {
                        "name": "Udo Dannlowski"
                    },
                    {
                        "name": "Hamidreza Jamalabadi"
                    },
                    {
                        "name": "Shaoxiong Ji"
                    }
                ],
                "author_detail": {
                    "name": "Shaoxiong Ji"
                },
                "author": "Shaoxiong Ji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25384v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25384v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25381v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25381v1",
                "updated": "2025-10-29T10:53:32Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    10,
                    53,
                    32,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T10:53:32Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    10,
                    53,
                    32,
                    2,
                    302,
                    0
                ],
                "title": "CGM-Led Multimodal Tracking with Chatbot Support: An Autoethnography in\n  Sub-Health",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CGM-Led Multimodal Tracking with Chatbot Support: An Autoethnography in\n  Sub-Health"
                },
                "summary": "Metabolic disorders present a pressing global health challenge, with China\ncarrying the world's largest burden. While continuous glucose monitoring (CGM)\nhas transformed diabetes care, its potential for supporting sub-health\npopulations -- such as individuals who are overweight, prediabetic, or anxious\n-- remains underexplored. At the same time, large language models (LLMs) are\nincreasingly used in health coaching, yet CGM is rarely incorporated as a\nfirst-class signal. To address this gap, we conducted a six-week\nautoethnography, combining CGM with multimodal indicators captured via common\ndigital devices and a chatbot that offered personalized reflections and\nexplanations of glucose fluctuations. Our findings show how CGM-led, data-first\nmultimodal tracking, coupled with conversational support, shaped everyday\npractices of diet, activity, stress, and wellbeing. This work contributes to\nHCI by extending CGM research beyond clinical diabetes and demonstrating how\nLLM-driven agents can support preventive health and reflection in at-risk\npopulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Metabolic disorders present a pressing global health challenge, with China\ncarrying the world's largest burden. While continuous glucose monitoring (CGM)\nhas transformed diabetes care, its potential for supporting sub-health\npopulations -- such as individuals who are overweight, prediabetic, or anxious\n-- remains underexplored. At the same time, large language models (LLMs) are\nincreasingly used in health coaching, yet CGM is rarely incorporated as a\nfirst-class signal. To address this gap, we conducted a six-week\nautoethnography, combining CGM with multimodal indicators captured via common\ndigital devices and a chatbot that offered personalized reflections and\nexplanations of glucose fluctuations. Our findings show how CGM-led, data-first\nmultimodal tracking, coupled with conversational support, shaped everyday\npractices of diet, activity, stress, and wellbeing. This work contributes to\nHCI by extending CGM research beyond clinical diabetes and demonstrating how\nLLM-driven agents can support preventive health and reflection in at-risk\npopulations."
                },
                "authors": [
                    {
                        "name": "Dongyijie Primo Pan"
                    },
                    {
                        "name": "Lan Luo"
                    },
                    {
                        "name": "Yike Wang"
                    },
                    {
                        "name": "Pan Hui"
                    }
                ],
                "author_detail": {
                    "name": "Pan Hui"
                },
                "author": "Pan Hui",
                "arxiv_comment": "conference paper, prepint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25381v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25381v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25378v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25378v1",
                "updated": "2025-10-29T10:51:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    10,
                    51,
                    35,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T10:51:35Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    10,
                    51,
                    35,
                    2,
                    302,
                    0
                ],
                "title": "Hallucinations in Bibliographic Recommendation: Citation Frequency as a\n  Proxy for Training Data Redundancy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucinations in Bibliographic Recommendation: Citation Frequency as a\n  Proxy for Training Data Redundancy"
                },
                "summary": "Large language models (LLMs) have been increasingly applied to a wide range\nof tasks, from natural language understanding to code generation. While they\nhave also been used to assist in bibliographic recommendation, the\nhallucination of non-existent papers remains a major issue. Building on prior\nstudies, this study hypothesizes that an LLM's ability to correctly produce\nbibliographic information depends on whether the underlying knowledge is\ngenerated or memorized, with highly cited papers (i.e., more frequently appear\nin the training corpus) showing lower hallucination rates. We therefore assume\ncitation count as a proxy for training data redundancy (i.e., the frequency\nwith which a given bibliographic record is repeatedly represented in the\npretraining corpus) and investigate how citation frequency affects hallucinated\nreferences in LLM outputs. Using GPT-4.1, we generated and manually verified\n100 bibliographic records across twenty computer-science domains, and measured\nfactual consistency via cosine similarity between generated and authentic\nmetadata. The results revealed that (i) hallucination rates vary across\nresearch domains, (ii) citation count is strongly correlated with factual\naccuracy, and (iii) bibliographic information becomes almost verbatimly\nmemorized beyond approximately 1,000 citations. These findings suggest that\nhighly cited papers are nearly verbatimly retained in the model, indicating a\nthreshold where generalization shifts into memorization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been increasingly applied to a wide range\nof tasks, from natural language understanding to code generation. While they\nhave also been used to assist in bibliographic recommendation, the\nhallucination of non-existent papers remains a major issue. Building on prior\nstudies, this study hypothesizes that an LLM's ability to correctly produce\nbibliographic information depends on whether the underlying knowledge is\ngenerated or memorized, with highly cited papers (i.e., more frequently appear\nin the training corpus) showing lower hallucination rates. We therefore assume\ncitation count as a proxy for training data redundancy (i.e., the frequency\nwith which a given bibliographic record is repeatedly represented in the\npretraining corpus) and investigate how citation frequency affects hallucinated\nreferences in LLM outputs. Using GPT-4.1, we generated and manually verified\n100 bibliographic records across twenty computer-science domains, and measured\nfactual consistency via cosine similarity between generated and authentic\nmetadata. The results revealed that (i) hallucination rates vary across\nresearch domains, (ii) citation count is strongly correlated with factual\naccuracy, and (iii) bibliographic information becomes almost verbatimly\nmemorized beyond approximately 1,000 citations. These findings suggest that\nhighly cited papers are nearly verbatimly retained in the model, indicating a\nthreshold where generalization shifts into memorization."
                },
                "authors": [
                    {
                        "name": "Junichiro Niimi"
                    }
                ],
                "author_detail": {
                    "name": "Junichiro Niimi"
                },
                "author": "Junichiro Niimi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25378v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25378v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15817v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15817v2",
                "updated": "2025-10-29T10:48:48Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    10,
                    48,
                    48,
                    2,
                    302,
                    0
                ],
                "published": "2024-10-21T09:31:51Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    9,
                    31,
                    51,
                    0,
                    295,
                    0
                ],
                "title": "LaMP-Val: Large Language Models Empower Personalized Valuation in\n  Auction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LaMP-Val: Large Language Models Empower Personalized Valuation in\n  Auction"
                },
                "summary": "Auctions are a vital economic mechanism used to determine the market value of\ngoods or services through competitive bidding within a specific framework.\nHowever, much of the current research primarily focuses on the bidding\nalgorithms used within auction mechanisms. This often neglects the potential\nbenefits of incorporating individual users' unique preferences into the\nvaluation process. Our theoretical and empirical analysis demonstrates that\nvaluation errors can significantly impact the overall utility. To bridge this\ngap, we propose a personalized valuation framework, namely Large\n\\underline{La}nguage \\underline{M}odels-powered \\underline{P}ersonalized\n\\underline{Val}uation (LaMP-Val), which integrates Large Language Models to\nincorporate personalized semantic preference into users valuation process.\nLaMP-Val integrating three components: data, learning, and evaluation. The data\ncomponent tackles the challenge of building a novel dataset specifically for\nLLMs fine-tuning in personalized valuation modeling. The learning component\nintroduces a diversity template to enhance LLMs' capacity for modeling\nfine-grained personal valuation patterns. The evaluation component establishes\na closed-loop system where LLM-generated valuations interact with bidding\nstrategies and auction. It proposes two novel metrics to quantify valuation\nprecision and bidding intention accuracy in personalized scenarios. Extensive\nexperiments show that LaMP-Val more accurately captures personalized values and\nachieves greater profits than baseline approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auctions are a vital economic mechanism used to determine the market value of\ngoods or services through competitive bidding within a specific framework.\nHowever, much of the current research primarily focuses on the bidding\nalgorithms used within auction mechanisms. This often neglects the potential\nbenefits of incorporating individual users' unique preferences into the\nvaluation process. Our theoretical and empirical analysis demonstrates that\nvaluation errors can significantly impact the overall utility. To bridge this\ngap, we propose a personalized valuation framework, namely Large\n\\underline{La}nguage \\underline{M}odels-powered \\underline{P}ersonalized\n\\underline{Val}uation (LaMP-Val), which integrates Large Language Models to\nincorporate personalized semantic preference into users valuation process.\nLaMP-Val integrating three components: data, learning, and evaluation. The data\ncomponent tackles the challenge of building a novel dataset specifically for\nLLMs fine-tuning in personalized valuation modeling. The learning component\nintroduces a diversity template to enhance LLMs' capacity for modeling\nfine-grained personal valuation patterns. The evaluation component establishes\na closed-loop system where LLM-generated valuations interact with bidding\nstrategies and auction. It proposes two novel metrics to quantify valuation\nprecision and bidding intention accuracy in personalized scenarios. Extensive\nexperiments show that LaMP-Val more accurately captures personalized values and\nachieves greater profits than baseline approaches."
                },
                "authors": [
                    {
                        "name": "Jie Sun"
                    },
                    {
                        "name": "Tianyu Zhang"
                    },
                    {
                        "name": "Houcheng Jiang"
                    },
                    {
                        "name": "Kexin Huang"
                    },
                    {
                        "name": "Xiang Shu"
                    },
                    {
                        "name": "Zhibo Zhu"
                    },
                    {
                        "name": "Lintao Ma"
                    },
                    {
                        "name": "Xingyu Lu"
                    },
                    {
                        "name": "Jun Zhou"
                    },
                    {
                        "name": "Junkang Wu"
                    },
                    {
                        "name": "Chi Luo"
                    },
                    {
                        "name": "An Zhang"
                    },
                    {
                        "name": "Junkang Wu"
                    },
                    {
                        "name": "Jiancan Wu"
                    },
                    {
                        "name": "Xiang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Wang"
                },
                "author": "Xiang Wang",
                "arxiv_comment": "17 pages, 5 figures, 8tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15817v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15817v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25371v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25371v1",
                "updated": "2025-10-29T10:42:56Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    10,
                    42,
                    56,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T10:42:56Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    10,
                    42,
                    56,
                    2,
                    302,
                    0
                ],
                "title": "Latent variable estimation with composite Hilbert space Gaussian\n  processes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latent variable estimation with composite Hilbert space Gaussian\n  processes"
                },
                "summary": "We develop a scalable class of models for latent variable estimation using\ncomposite Gaussian processes, with a focus on derivative Gaussian processes. We\njointly model multiple data sources as outputs to improve the accuracy of\nlatent variable inference under a single probabilistic framework. Similarly\nspecified exact Gaussian processes scale poorly with large datasets. To\novercome this, we extend the recently developed Hilbert space approximation\nmethods for Gaussian processes to obtain a reduced-rank representation of the\ncomposite covariance function through its spectral decomposition. Specifically,\nwe derive and analyze the spectral decomposition of derivative covariance\nfunctions and further study their properties theoretically. Using these\nspectral decompositions, our methods easily scale up to data scenarios\ninvolving thousands of samples. We validate our methods in terms of latent\nvariable estimation accuracy, uncertainty calibration, and inference speed\nacross diverse simulation scenarios. Finally, using a real world case study\nfrom single-cell biology, we demonstrate the potential of our models in\nestimating latent cellular ordering given gene expression levels, thus\nenhancing our understanding of the underlying biological process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We develop a scalable class of models for latent variable estimation using\ncomposite Gaussian processes, with a focus on derivative Gaussian processes. We\njointly model multiple data sources as outputs to improve the accuracy of\nlatent variable inference under a single probabilistic framework. Similarly\nspecified exact Gaussian processes scale poorly with large datasets. To\novercome this, we extend the recently developed Hilbert space approximation\nmethods for Gaussian processes to obtain a reduced-rank representation of the\ncomposite covariance function through its spectral decomposition. Specifically,\nwe derive and analyze the spectral decomposition of derivative covariance\nfunctions and further study their properties theoretically. Using these\nspectral decompositions, our methods easily scale up to data scenarios\ninvolving thousands of samples. We validate our methods in terms of latent\nvariable estimation accuracy, uncertainty calibration, and inference speed\nacross diverse simulation scenarios. Finally, using a real world case study\nfrom single-cell biology, we demonstrate the potential of our models in\nestimating latent cellular ordering given gene expression levels, thus\nenhancing our understanding of the underlying biological process."
                },
                "authors": [
                    {
                        "name": "Soham Mukherjee"
                    },
                    {
                        "name": "Javier Enrique Aguilar"
                    },
                    {
                        "name": "Marcello Zago"
                    },
                    {
                        "name": "Manfred Claassen"
                    },
                    {
                        "name": "Paul-Christian Bürkner"
                    }
                ],
                "author_detail": {
                    "name": "Paul-Christian Bürkner"
                },
                "author": "Paul-Christian Bürkner",
                "arxiv_comment": "37 pages, 16 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25371v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25371v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25370v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25370v1",
                "updated": "2025-10-29T10:41:03Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    10,
                    41,
                    3,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T10:41:03Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    10,
                    41,
                    3,
                    2,
                    302,
                    0
                ],
                "title": "Monitoring Transformative Technological Convergence Through\n  LLM-Extracted Semantic Entity Triple Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Monitoring Transformative Technological Convergence Through\n  LLM-Extracted Semantic Entity Triple Graphs"
                },
                "summary": "Forecasting transformative technologies remains a critical but challenging\ntask, particularly in fast-evolving domains such as Information and\nCommunication Technologies (ICTs). Traditional expert-based methods struggle to\nkeep pace with short innovation cycles and ambiguous early-stage terminology.\nIn this work, we propose a novel, data-driven pipeline to monitor the emergence\nof transformative technologies by identifying patterns of technological\nconvergence.\n  Our approach leverages advances in Large Language Models (LLMs) to extract\nsemantic triples from unstructured text and construct a large-scale graph of\ntechnology-related entities and relations. We introduce a new method for\ngrouping semantically similar technology terms (noun stapling) and develop\ngraph-based metrics to detect convergence signals. The pipeline includes\nmulti-stage filtering, domain-specific keyword clustering, and a temporal trend\nanalysis of topic co-occurence.\n  We validate our methodology on two complementary datasets: 278,625 arXiv\npreprints (2017--2024) to capture early scientific signals, and 9,793 USPTO\npatent applications (2018-2024) to track downstream commercial developments.\nOur results demonstrate that the proposed pipeline can identify both\nestablished and emerging convergence patterns, offering a scalable and\ngeneralizable framework for technology forecasting grounded in full-text\nanalysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forecasting transformative technologies remains a critical but challenging\ntask, particularly in fast-evolving domains such as Information and\nCommunication Technologies (ICTs). Traditional expert-based methods struggle to\nkeep pace with short innovation cycles and ambiguous early-stage terminology.\nIn this work, we propose a novel, data-driven pipeline to monitor the emergence\nof transformative technologies by identifying patterns of technological\nconvergence.\n  Our approach leverages advances in Large Language Models (LLMs) to extract\nsemantic triples from unstructured text and construct a large-scale graph of\ntechnology-related entities and relations. We introduce a new method for\ngrouping semantically similar technology terms (noun stapling) and develop\ngraph-based metrics to detect convergence signals. The pipeline includes\nmulti-stage filtering, domain-specific keyword clustering, and a temporal trend\nanalysis of topic co-occurence.\n  We validate our methodology on two complementary datasets: 278,625 arXiv\npreprints (2017--2024) to capture early scientific signals, and 9,793 USPTO\npatent applications (2018-2024) to track downstream commercial developments.\nOur results demonstrate that the proposed pipeline can identify both\nestablished and emerging convergence patterns, offering a scalable and\ngeneralizable framework for technology forecasting grounded in full-text\nanalysis."
                },
                "authors": [
                    {
                        "name": "Alexander Sternfeld"
                    },
                    {
                        "name": "Andrei Kucharavy"
                    },
                    {
                        "name": "Dimitri Percia David"
                    },
                    {
                        "name": "Alain Mermoud"
                    },
                    {
                        "name": "Julian Jang-Jaccard"
                    },
                    {
                        "name": "Nathan Monnet"
                    }
                ],
                "author_detail": {
                    "name": "Nathan Monnet"
                },
                "author": "Nathan Monnet",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25370v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25370v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25369v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25369v1",
                "updated": "2025-10-29T10:40:12Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    10,
                    40,
                    12,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T10:40:12Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    10,
                    40,
                    12,
                    2,
                    302,
                    0
                ],
                "title": "Have a thing? Reasoning around recursion with dynamic typing in grounded\n  arithmetic",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Have a thing? Reasoning around recursion with dynamic typing in grounded\n  arithmetic"
                },
                "summary": "Neither the classical nor intuitionistic logic traditions are\nperfectly-aligned with the purpose of reasoning about computation, in that\nneither logical tradition can normally permit the direct expression of\narbitrary general-recursive functions without inconsistency. We introduce\ngrounded arithmetic or GA, a minimalistic but nonetheless powerful foundation\nfor formal reasoning that allows the direct expression of arbitrary recursive\ndefinitions. GA adjusts the traditional inference rules such that terms that\nexpress nonterminating computations harmlessly denote no semantic value (i.e.,\n\"bottom\") instead of leading into logical paradox or inconsistency. Recursive\nfunctions may be proven terminating in GA essentially by \"dynamically typing\"\nterms, or equivalently, symbolically reverse-executing the computations they\ndenote via GA's inference rules. Once recursive functions have been proven\nterminating, logical reasoning about their results reduce to the familiar\nclassical rules. A mechanically-checked consistency proof in Isabelle/HOL\nexists for the basic quantifier-free fragment of GA. Quantifiers may be added\natop this foundation as ordinary computations, whose inference rules are thus\nadmissible and do not introduce new inconsistency risks. While GA is only a\nfirst step towards richly-typed grounded deduction practical for everyday use\nin manual or automated computational reasoning, it shows the promise that the\nexpressive freedom of arbitrary recursive definition can in principle be\nincorporated into formal systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neither the classical nor intuitionistic logic traditions are\nperfectly-aligned with the purpose of reasoning about computation, in that\nneither logical tradition can normally permit the direct expression of\narbitrary general-recursive functions without inconsistency. We introduce\ngrounded arithmetic or GA, a minimalistic but nonetheless powerful foundation\nfor formal reasoning that allows the direct expression of arbitrary recursive\ndefinitions. GA adjusts the traditional inference rules such that terms that\nexpress nonterminating computations harmlessly denote no semantic value (i.e.,\n\"bottom\") instead of leading into logical paradox or inconsistency. Recursive\nfunctions may be proven terminating in GA essentially by \"dynamically typing\"\nterms, or equivalently, symbolically reverse-executing the computations they\ndenote via GA's inference rules. Once recursive functions have been proven\nterminating, logical reasoning about their results reduce to the familiar\nclassical rules. A mechanically-checked consistency proof in Isabelle/HOL\nexists for the basic quantifier-free fragment of GA. Quantifiers may be added\natop this foundation as ordinary computations, whose inference rules are thus\nadmissible and do not introduce new inconsistency risks. While GA is only a\nfirst step towards richly-typed grounded deduction practical for everyday use\nin manual or automated computational reasoning, it shows the promise that the\nexpressive freedom of arbitrary recursive definition can in principle be\nincorporated into formal systems."
                },
                "authors": [
                    {
                        "name": "Elliot Bobrow"
                    },
                    {
                        "name": "Bryan Ford"
                    },
                    {
                        "name": "Stefan Milenković"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Milenković"
                },
                "author": "Stefan Milenković",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25369v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25369v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.FL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.3.1; F.4.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25368v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25368v1",
                "updated": "2025-10-29T10:39:29Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    10,
                    39,
                    29,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T10:39:29Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    10,
                    39,
                    29,
                    2,
                    302,
                    0
                ],
                "title": "Position: Biology is the Challenge Physics-Informed ML Needs to Evolve",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Position: Biology is the Challenge Physics-Informed ML Needs to Evolve"
                },
                "summary": "Physics-Informed Machine Learning (PIML) has successfully integrated\nmechanistic understanding into machine learning, particularly in domains\ngoverned by well-known physical laws. This success has motivated efforts to\napply PIML to biology, a field rich in dynamical systems but shaped by\ndifferent constraints. Biological modeling, however, presents unique\nchallenges: multi-faceted and uncertain prior knowledge, heterogeneous and\nnoisy data, partial observability, and complex, high-dimensional networks. In\nthis position paper, we argue that these challenges should not be seen as\nobstacles to PIML, but as catalysts for its evolution. We propose\nBiology-Informed Machine Learning (BIML): a principled extension of PIML that\nretains its structural grounding while adapting to the practical realities of\nbiology. Rather than replacing PIML, BIML retools its methods to operate under\nsofter, probabilistic forms of prior knowledge. We outline four foundational\npillars as a roadmap for this transition: uncertainty quantification,\ncontextualization, constrained latent structure inference, and scalability.\nFoundation Models and Large Language Models will be key enablers, bridging\nhuman expertise with computational modeling. We conclude with concrete\nrecommendations to build the BIML ecosystem and channel PIML-inspired\ninnovation toward challenges of high scientific and societal relevance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Physics-Informed Machine Learning (PIML) has successfully integrated\nmechanistic understanding into machine learning, particularly in domains\ngoverned by well-known physical laws. This success has motivated efforts to\napply PIML to biology, a field rich in dynamical systems but shaped by\ndifferent constraints. Biological modeling, however, presents unique\nchallenges: multi-faceted and uncertain prior knowledge, heterogeneous and\nnoisy data, partial observability, and complex, high-dimensional networks. In\nthis position paper, we argue that these challenges should not be seen as\nobstacles to PIML, but as catalysts for its evolution. We propose\nBiology-Informed Machine Learning (BIML): a principled extension of PIML that\nretains its structural grounding while adapting to the practical realities of\nbiology. Rather than replacing PIML, BIML retools its methods to operate under\nsofter, probabilistic forms of prior knowledge. We outline four foundational\npillars as a roadmap for this transition: uncertainty quantification,\ncontextualization, constrained latent structure inference, and scalability.\nFoundation Models and Large Language Models will be key enablers, bridging\nhuman expertise with computational modeling. We conclude with concrete\nrecommendations to build the BIML ecosystem and channel PIML-inspired\ninnovation toward challenges of high scientific and societal relevance."
                },
                "authors": [
                    {
                        "name": "Julien Martinelli"
                    }
                ],
                "author_detail": {
                    "name": "Julien Martinelli"
                },
                "author": "Julien Martinelli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25368v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25368v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.14866v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.14866v2",
                "updated": "2025-10-29T10:34:04Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    10,
                    34,
                    4,
                    2,
                    302,
                    0
                ],
                "published": "2025-06-17T17:59:31Z",
                "published_parsed": [
                    2025,
                    6,
                    17,
                    17,
                    59,
                    31,
                    1,
                    168,
                    0
                ],
                "title": "OS-Harm: A Benchmark for Measuring Safety of Computer Use Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OS-Harm: A Benchmark for Measuring Safety of Computer Use Agents"
                },
                "summary": "Computer use agents are LLM-based agents that can directly interact with a\ngraphical user interface, by processing screenshots or accessibility trees.\nWhile these systems are gaining popularity, their safety has been largely\noverlooked, despite the fact that evaluating and understanding their potential\nfor harmful behavior is essential for widespread adoption. To address this gap,\nwe introduce OS-Harm, a new benchmark for measuring safety of computer use\nagents. OS-Harm is built on top of the OSWorld environment and aims to test\nmodels across three categories of harm: deliberate user misuse, prompt\ninjection attacks, and model misbehavior. To cover these cases, we create 150\ntasks that span several types of safety violations (harassment, copyright\ninfringement, disinformation, data exfiltration, etc.) and require the agent to\ninteract with a variety of OS applications (email client, code editor, browser,\netc.). Moreover, we propose an automated judge to evaluate both accuracy and\nsafety of agents that achieves high agreement with human annotations (0.76 and\n0.79 F1 score). We evaluate computer use agents based on a range of frontier\nmodels - such as o4-mini, Claude 3.7 Sonnet, Gemini 2.5 Pro - and provide\ninsights into their safety. In particular, all models tend to directly comply\nwith many deliberate misuse queries, are relatively vulnerable to static prompt\ninjections, and occasionally perform unsafe actions. The OS-Harm benchmark is\navailable at https://github.com/tml-epfl/os-harm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computer use agents are LLM-based agents that can directly interact with a\ngraphical user interface, by processing screenshots or accessibility trees.\nWhile these systems are gaining popularity, their safety has been largely\noverlooked, despite the fact that evaluating and understanding their potential\nfor harmful behavior is essential for widespread adoption. To address this gap,\nwe introduce OS-Harm, a new benchmark for measuring safety of computer use\nagents. OS-Harm is built on top of the OSWorld environment and aims to test\nmodels across three categories of harm: deliberate user misuse, prompt\ninjection attacks, and model misbehavior. To cover these cases, we create 150\ntasks that span several types of safety violations (harassment, copyright\ninfringement, disinformation, data exfiltration, etc.) and require the agent to\ninteract with a variety of OS applications (email client, code editor, browser,\netc.). Moreover, we propose an automated judge to evaluate both accuracy and\nsafety of agents that achieves high agreement with human annotations (0.76 and\n0.79 F1 score). We evaluate computer use agents based on a range of frontier\nmodels - such as o4-mini, Claude 3.7 Sonnet, Gemini 2.5 Pro - and provide\ninsights into their safety. In particular, all models tend to directly comply\nwith many deliberate misuse queries, are relatively vulnerable to static prompt\ninjections, and occasionally perform unsafe actions. The OS-Harm benchmark is\navailable at https://github.com/tml-epfl/os-harm."
                },
                "authors": [
                    {
                        "name": "Thomas Kuntz"
                    },
                    {
                        "name": "Agatha Duzan"
                    },
                    {
                        "name": "Hao Zhao"
                    },
                    {
                        "name": "Francesco Croce"
                    },
                    {
                        "name": "Zico Kolter"
                    },
                    {
                        "name": "Nicolas Flammarion"
                    },
                    {
                        "name": "Maksym Andriushchenko"
                    }
                ],
                "author_detail": {
                    "name": "Maksym Andriushchenko"
                },
                "author": "Maksym Andriushchenko",
                "arxiv_comment": "NeurIPS 2025 Datasets & Benchmarks Track (Spotlight)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.14866v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.14866v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25356v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25356v1",
                "updated": "2025-10-29T10:21:25Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    10,
                    21,
                    25,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T10:21:25Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    10,
                    21,
                    25,
                    2,
                    302,
                    0
                ],
                "title": "Not ready for the bench: LLM legal interpretation is unstable and out of\n  step with human judgments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not ready for the bench: LLM legal interpretation is unstable and out of\n  step with human judgments"
                },
                "summary": "Legal interpretation frequently involves assessing how a legal text, as\nunderstood by an 'ordinary' speaker of the language, applies to the set of\nfacts characterizing a legal dispute in the U.S. judicial system. Recent\nscholarship has proposed that legal practitioners add large language models\n(LLMs) to their interpretive toolkit. This work offers an empirical argument\nagainst LLM interpretation as recently practiced by legal scholars and federal\njudges. Our investigation in English shows that models do not provide stable\ninterpretive judgments: varying the question format can lead the model to\nwildly different conclusions. Moreover, the models show weak to moderate\ncorrelation with human judgment, with large variance across model and question\nvariant, suggesting that it is dangerous to give much credence to the\nconclusions produced by generative AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Legal interpretation frequently involves assessing how a legal text, as\nunderstood by an 'ordinary' speaker of the language, applies to the set of\nfacts characterizing a legal dispute in the U.S. judicial system. Recent\nscholarship has proposed that legal practitioners add large language models\n(LLMs) to their interpretive toolkit. This work offers an empirical argument\nagainst LLM interpretation as recently practiced by legal scholars and federal\njudges. Our investigation in English shows that models do not provide stable\ninterpretive judgments: varying the question format can lead the model to\nwildly different conclusions. Moreover, the models show weak to moderate\ncorrelation with human judgment, with large variance across model and question\nvariant, suggesting that it is dangerous to give much credence to the\nconclusions produced by generative AI."
                },
                "authors": [
                    {
                        "name": "Abhishek Purushothama"
                    },
                    {
                        "name": "Junghyun Min"
                    },
                    {
                        "name": "Brandon Waldon"
                    },
                    {
                        "name": "Nathan Schneider"
                    }
                ],
                "author_detail": {
                    "name": "Nathan Schneider"
                },
                "author": "Nathan Schneider",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25356v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25356v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06204v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06204v2",
                "updated": "2025-10-29T10:17:57Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    10,
                    17,
                    57,
                    2,
                    302,
                    0
                ],
                "published": "2025-07-08T17:30:14Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    17,
                    30,
                    14,
                    1,
                    189,
                    0
                ],
                "title": "Differential Mamba",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Differential Mamba"
                },
                "summary": "Sequence models like Transformers and RNNs often overallocate attention to\nirrelevant context, leading to noisy intermediate representations. This\ndegrades LLM capabilities by promoting hallucinations, weakening long-range and\nretrieval abilities, and reducing robustness. Recent work has shown that\ndifferential design can mitigate this issue in Transformers, improving their\neffectiveness across various applications. In this paper, we explore whether\nthese techniques, originally developed for Transformers, can be applied to\nMamba, a recent architecture based on selective state-space layers that\nachieves Transformer-level performance with greater efficiency. We show that a\nnaive adaptation of differential design to Mamba is insufficient and requires\ncareful architectural modifications. To address this, we introduce a novel\ndifferential mechanism for Mamba, empirically validated on language modeling\nbenchmarks, demonstrating improved retrieval capabilities and superior\nperformance over vanilla Mamba. Finally, we conduct extensive ablation studies\nand empirical analyses to justify our design choices and provide evidence that\nour approach effectively mitigates the overallocation problem in Mamba-based\nmodels. Our code is publicly available: https://github.com/NadavSc/Diff-Mamba",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequence models like Transformers and RNNs often overallocate attention to\nirrelevant context, leading to noisy intermediate representations. This\ndegrades LLM capabilities by promoting hallucinations, weakening long-range and\nretrieval abilities, and reducing robustness. Recent work has shown that\ndifferential design can mitigate this issue in Transformers, improving their\neffectiveness across various applications. In this paper, we explore whether\nthese techniques, originally developed for Transformers, can be applied to\nMamba, a recent architecture based on selective state-space layers that\nachieves Transformer-level performance with greater efficiency. We show that a\nnaive adaptation of differential design to Mamba is insufficient and requires\ncareful architectural modifications. To address this, we introduce a novel\ndifferential mechanism for Mamba, empirically validated on language modeling\nbenchmarks, demonstrating improved retrieval capabilities and superior\nperformance over vanilla Mamba. Finally, we conduct extensive ablation studies\nand empirical analyses to justify our design choices and provide evidence that\nour approach effectively mitigates the overallocation problem in Mamba-based\nmodels. Our code is publicly available: https://github.com/NadavSc/Diff-Mamba"
                },
                "authors": [
                    {
                        "name": "Nadav Schneider"
                    },
                    {
                        "name": "Itamar Zimerman"
                    },
                    {
                        "name": "Eliya Nachmani"
                    }
                ],
                "author_detail": {
                    "name": "Eliya Nachmani"
                },
                "author": "Eliya Nachmani",
                "arxiv_comment": "AACL 2025. We provide the code at\n  https://github.com/NadavSc/Diff-Mamba",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06204v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06204v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2510.25770v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25770v1",
                "updated": "2025-10-29T17:59:16Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    17,
                    59,
                    16,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T17:59:16Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    17,
                    59,
                    16,
                    2,
                    302,
                    0
                ],
                "title": "E-Scores for (In)Correctness Assessment of Generative Model Outputs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "E-Scores for (In)Correctness Assessment of Generative Model Outputs"
                },
                "summary": "While generative models, especially large language models (LLMs), are\nubiquitous in today's world, principled mechanisms to assess their\n(in)correctness are limited. Using the conformal prediction framework, previous\nworks construct sets of LLM responses where the probability of including an\nincorrect response, or error, is capped at a desired user-defined tolerance\nlevel. However, since these methods are based on p-values, they are susceptible\nto p-hacking, i.e., choosing the tolerance level post-hoc can invalidate the\nguarantees. We therefore leverage e-values to complement generative model\noutputs with e-scores as a measure of incorrectness. In addition to achieving\nthe same statistical guarantees as before, e-scores provide users flexibility\nin adaptively choosing tolerance levels after observing the e-scores\nthemselves, by upper bounding a post-hoc notion of error called size\ndistortion. We experimentally demonstrate their efficacy in assessing LLM\noutputs for different correctness types: mathematical factuality and property\nconstraints satisfaction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While generative models, especially large language models (LLMs), are\nubiquitous in today's world, principled mechanisms to assess their\n(in)correctness are limited. Using the conformal prediction framework, previous\nworks construct sets of LLM responses where the probability of including an\nincorrect response, or error, is capped at a desired user-defined tolerance\nlevel. However, since these methods are based on p-values, they are susceptible\nto p-hacking, i.e., choosing the tolerance level post-hoc can invalidate the\nguarantees. We therefore leverage e-values to complement generative model\noutputs with e-scores as a measure of incorrectness. In addition to achieving\nthe same statistical guarantees as before, e-scores provide users flexibility\nin adaptively choosing tolerance levels after observing the e-scores\nthemselves, by upper bounding a post-hoc notion of error called size\ndistortion. We experimentally demonstrate their efficacy in assessing LLM\noutputs for different correctness types: mathematical factuality and property\nconstraints satisfaction."
                },
                "authors": [
                    {
                        "name": "Guneet S. Dhillon"
                    },
                    {
                        "name": "Javier González"
                    },
                    {
                        "name": "Teodora Pandeva"
                    },
                    {
                        "name": "Alicia Curth"
                    }
                ],
                "author_detail": {
                    "name": "Alicia Curth"
                },
                "author": "Alicia Curth",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25770v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25770v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25766v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25766v1",
                "updated": "2025-10-29T17:58:59Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    17,
                    58,
                    59,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T17:58:59Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    17,
                    58,
                    59,
                    2,
                    302,
                    0
                ],
                "title": "Decomposition-Enhanced Training for Post-Hoc Attributions In Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decomposition-Enhanced Training for Post-Hoc Attributions In Language\n  Models"
                },
                "summary": "Large language models (LLMs) are increasingly used for long-document question\nanswering, where reliable attribution to sources is critical for trust.\nExisting post-hoc attribution methods work well for extractive QA but struggle\nin multi-hop, abstractive, and semi-extractive settings, where answers\nsynthesize information across passages. To address these challenges, we argue\nthat post-hoc attribution can be reframed as a reasoning problem, where answers\nare decomposed into constituent units, each tied to specific context. We first\nshow that prompting models to generate such decompositions alongside\nattributions improves performance. Building on this, we introduce DecompTune, a\npost-training method that teaches models to produce answer decompositions as\nintermediate reasoning steps. We curate a diverse dataset of complex QA tasks,\nannotated with decompositions by a strong LLM, and post-train Qwen-2.5 (7B and\n14B) using a two-stage SFT + GRPO pipeline with task-specific curated rewards.\nAcross extensive experiments and ablations, DecompTune substantially improves\nattribution quality, outperforming prior methods and matching or exceeding\nstate-of-the-art frontier models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly used for long-document question\nanswering, where reliable attribution to sources is critical for trust.\nExisting post-hoc attribution methods work well for extractive QA but struggle\nin multi-hop, abstractive, and semi-extractive settings, where answers\nsynthesize information across passages. To address these challenges, we argue\nthat post-hoc attribution can be reframed as a reasoning problem, where answers\nare decomposed into constituent units, each tied to specific context. We first\nshow that prompting models to generate such decompositions alongside\nattributions improves performance. Building on this, we introduce DecompTune, a\npost-training method that teaches models to produce answer decompositions as\nintermediate reasoning steps. We curate a diverse dataset of complex QA tasks,\nannotated with decompositions by a strong LLM, and post-train Qwen-2.5 (7B and\n14B) using a two-stage SFT + GRPO pipeline with task-specific curated rewards.\nAcross extensive experiments and ablations, DecompTune substantially improves\nattribution quality, outperforming prior methods and matching or exceeding\nstate-of-the-art frontier models."
                },
                "authors": [
                    {
                        "name": "Sriram Balasubramaniam"
                    },
                    {
                        "name": "Samyadeep Basu"
                    },
                    {
                        "name": "Koustava Goswami"
                    },
                    {
                        "name": "Ryan Rossi"
                    },
                    {
                        "name": "Varun Manjunatha"
                    },
                    {
                        "name": "Roshan Santhosh"
                    },
                    {
                        "name": "Ruiyi Zhang"
                    },
                    {
                        "name": "Soheil Feizi"
                    },
                    {
                        "name": "Nedim Lipka"
                    }
                ],
                "author_detail": {
                    "name": "Nedim Lipka"
                },
                "author": "Nedim Lipka",
                "arxiv_comment": "Post-hoc attribution",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25766v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25766v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18905v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18905v2",
                "updated": "2025-10-29T17:57:23Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    17,
                    57,
                    23,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-21T01:03:46Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    1,
                    3,
                    46,
                    1,
                    294,
                    0
                ],
                "title": "3D Optimization for AI Inference Scaling: Balancing Accuracy, Cost, and\n  Latency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D Optimization for AI Inference Scaling: Balancing Accuracy, Cost, and\n  Latency"
                },
                "summary": "AI inference scaling is often tuned through 1D heuristics (a fixed reasoning\npasses) or 2D bivariate trade-offs (e.g., performance vs. compute), which fail\nto consider cost and latency constraints. We introduce a 3D optimization\nframework that jointly calibrates accuracy, cost, and latency within a unified\ndecision space, enabling constraints-aware inference scaling. Using Monte Carlo\nsimulations across three representative scenarios and nine simulated large\nlanguage models, we evaluate four optimization methods to address the 3D\nmulti-objective optimization (MOO) problem. Framing inference scaling in MOO\nshapes a feasible space that 1D and 2D optimizations fail to capture, enabling\nenvironmentadaptive selection of the inference scaling k. Results show that\nknee-point optimization achieves the best balance, while accuracy-maximization\nremains favorable when precision is prioritized. The framework establishes a\ntheoretical foundation for deployment-aware inference scaling across diverse\noperational contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI inference scaling is often tuned through 1D heuristics (a fixed reasoning\npasses) or 2D bivariate trade-offs (e.g., performance vs. compute), which fail\nto consider cost and latency constraints. We introduce a 3D optimization\nframework that jointly calibrates accuracy, cost, and latency within a unified\ndecision space, enabling constraints-aware inference scaling. Using Monte Carlo\nsimulations across three representative scenarios and nine simulated large\nlanguage models, we evaluate four optimization methods to address the 3D\nmulti-objective optimization (MOO) problem. Framing inference scaling in MOO\nshapes a feasible space that 1D and 2D optimizations fail to capture, enabling\nenvironmentadaptive selection of the inference scaling k. Results show that\nknee-point optimization achieves the best balance, while accuracy-maximization\nremains favorable when precision is prioritized. The framework establishes a\ntheoretical foundation for deployment-aware inference scaling across diverse\noperational contexts."
                },
                "authors": [
                    {
                        "name": "Minseok Jung"
                    },
                    {
                        "name": "Abhas Ricky"
                    },
                    {
                        "name": "Muhammad Rameez Chatni"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Rameez Chatni"
                },
                "author": "Muhammad Rameez Chatni",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18905v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18905v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01939v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01939v3",
                "updated": "2025-10-29T17:57:03Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    17,
                    57,
                    3,
                    2,
                    302,
                    0
                ],
                "published": "2025-07-02T17:49:52Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    17,
                    49,
                    52,
                    2,
                    183,
                    0
                ],
                "title": "SpecCLIP: Aligning and Translating Spectroscopic Measurements for Stars",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpecCLIP: Aligning and Translating Spectroscopic Measurements for Stars"
                },
                "summary": "In recent years, large language models (LLMs) have transformed natural\nlanguage understanding through vast datasets and large-scale parameterization.\nInspired by this success, we present SpecCLIP, a foundation model framework\nthat extends LLM-inspired methodologies to stellar spectral analysis. Stellar\nspectra, akin to structured language, encode rich physical and chemical\ninformation about stars. By training foundation models on large-scale spectral\ndatasets, our goal is to learn robust and informative embeddings that support\ndiverse downstream applications. As a proof of concept, SpecCLIP involves\npre-training on two spectral types--LAMOST low-resolution and Gaia XP--followed\nby contrastive alignment using the CLIP (Contrastive Language-Image\nPre-training) framework, adapted to associate spectra from different\ninstruments. This alignment is complemented by auxiliary decoders that preserve\nspectrum-specific information and enable translation (prediction) between\nspectral types, with the former achieved by maximizing mutual information\nbetween embeddings and input spectra. The result is a cross-spectrum framework\nenabling intrinsic calibration and flexible applications across instruments. We\ndemonstrate that fine-tuning these models on moderate-sized labeled datasets\nimproves adaptability to tasks such as stellar-parameter estimation and\nchemical-abundance determination. SpecCLIP also enhances the accuracy and\nprecision of parameter estimates benchmarked against external survey data.\nAdditionally, its similarity search and cross-spectrum prediction capabilities\noffer potential for anomaly detection. Our results suggest that contrastively\ntrained foundation models enriched with spectrum-aware decoders can advance\nprecision stellar spectroscopy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, large language models (LLMs) have transformed natural\nlanguage understanding through vast datasets and large-scale parameterization.\nInspired by this success, we present SpecCLIP, a foundation model framework\nthat extends LLM-inspired methodologies to stellar spectral analysis. Stellar\nspectra, akin to structured language, encode rich physical and chemical\ninformation about stars. By training foundation models on large-scale spectral\ndatasets, our goal is to learn robust and informative embeddings that support\ndiverse downstream applications. As a proof of concept, SpecCLIP involves\npre-training on two spectral types--LAMOST low-resolution and Gaia XP--followed\nby contrastive alignment using the CLIP (Contrastive Language-Image\nPre-training) framework, adapted to associate spectra from different\ninstruments. This alignment is complemented by auxiliary decoders that preserve\nspectrum-specific information and enable translation (prediction) between\nspectral types, with the former achieved by maximizing mutual information\nbetween embeddings and input spectra. The result is a cross-spectrum framework\nenabling intrinsic calibration and flexible applications across instruments. We\ndemonstrate that fine-tuning these models on moderate-sized labeled datasets\nimproves adaptability to tasks such as stellar-parameter estimation and\nchemical-abundance determination. SpecCLIP also enhances the accuracy and\nprecision of parameter estimates benchmarked against external survey data.\nAdditionally, its similarity search and cross-spectrum prediction capabilities\noffer potential for anomaly detection. Our results suggest that contrastively\ntrained foundation models enriched with spectrum-aware decoders can advance\nprecision stellar spectroscopy."
                },
                "authors": [
                    {
                        "name": "Xiaosheng Zhao"
                    },
                    {
                        "name": "Yang Huang"
                    },
                    {
                        "name": "Guirong Xue"
                    },
                    {
                        "name": "Xiao Kong"
                    },
                    {
                        "name": "Jifeng Liu"
                    },
                    {
                        "name": "Xiaoyu Tang"
                    },
                    {
                        "name": "Timothy C. Beers"
                    },
                    {
                        "name": "Yuan-Sen Ting"
                    },
                    {
                        "name": "A-Li Luo"
                    }
                ],
                "author_detail": {
                    "name": "A-Li Luo"
                },
                "author": "A-Li Luo",
                "arxiv_comment": "27 pages, 8 figures, 5 tables. Minor update: added corrected\n  acknowledgments and corrected a misstated hyperparameter value (noted in\n  footnote) for reproducibility. Submitted to AAS Journals. Comments welcome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01939v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01939v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25761v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25761v1",
                "updated": "2025-10-29T17:56:17Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    17,
                    56,
                    17,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T17:56:17Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    17,
                    56,
                    17,
                    2,
                    302,
                    0
                ],
                "title": "DiagramEval: Evaluating LLM-Generated Diagrams via Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiagramEval: Evaluating LLM-Generated Diagrams via Graphs"
                },
                "summary": "Diagrams play a central role in research papers for conveying ideas, yet they\nare often notoriously complex and labor-intensive to create. Although diagrams\nare presented as images, standard image generative models struggle to produce\nclear diagrams with well-defined structure. We argue that a promising direction\nis to generate demonstration diagrams directly in textual form as SVGs, which\ncan leverage recent advances in large language models (LLMs). However, due to\nthe complexity of components and the multimodal nature of diagrams,\nsufficiently discriminative and explainable metrics for evaluating the quality\nof LLM-generated diagrams remain lacking. In this paper, we propose\nDiagramEval, a novel evaluation metric designed to assess demonstration\ndiagrams generated by LLMs. Specifically, DiagramEval conceptualizes diagrams\nas graphs, treating text elements as nodes and their connections as directed\nedges, and evaluates diagram quality using two new groups of metrics: node\nalignment and path alignment. For the first time, we effectively evaluate\ndiagrams produced by state-of-the-art LLMs on recent research literature,\nquantitatively demonstrating the validity of our metrics. Furthermore, we show\nhow the enhanced explainability of our proposed metrics offers valuable\ninsights into the characteristics of LLM-generated diagrams. Code:\nhttps://github.com/ulab-uiuc/diagram-eval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diagrams play a central role in research papers for conveying ideas, yet they\nare often notoriously complex and labor-intensive to create. Although diagrams\nare presented as images, standard image generative models struggle to produce\nclear diagrams with well-defined structure. We argue that a promising direction\nis to generate demonstration diagrams directly in textual form as SVGs, which\ncan leverage recent advances in large language models (LLMs). However, due to\nthe complexity of components and the multimodal nature of diagrams,\nsufficiently discriminative and explainable metrics for evaluating the quality\nof LLM-generated diagrams remain lacking. In this paper, we propose\nDiagramEval, a novel evaluation metric designed to assess demonstration\ndiagrams generated by LLMs. Specifically, DiagramEval conceptualizes diagrams\nas graphs, treating text elements as nodes and their connections as directed\nedges, and evaluates diagram quality using two new groups of metrics: node\nalignment and path alignment. For the first time, we effectively evaluate\ndiagrams produced by state-of-the-art LLMs on recent research literature,\nquantitatively demonstrating the validity of our metrics. Furthermore, we show\nhow the enhanced explainability of our proposed metrics offers valuable\ninsights into the characteristics of LLM-generated diagrams. Code:\nhttps://github.com/ulab-uiuc/diagram-eval."
                },
                "authors": [
                    {
                        "name": "Chumeng Liang"
                    },
                    {
                        "name": "Jiaxuan You"
                    }
                ],
                "author_detail": {
                    "name": "Jiaxuan You"
                },
                "author": "Jiaxuan You",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25761v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25761v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25758v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25758v1",
                "updated": "2025-10-29T17:54:20Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    17,
                    54,
                    20,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T17:54:20Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    17,
                    54,
                    20,
                    2,
                    302,
                    0
                ],
                "title": "TheraMind: A Strategic and Adaptive Agent for Longitudinal Psychological\n  Counseling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TheraMind: A Strategic and Adaptive Agent for Longitudinal Psychological\n  Counseling"
                },
                "summary": "Large language models (LLMs) in psychological counseling have attracted\nincreasing attention. However, existing approaches often lack emotional\nunderstanding, adaptive strategies, and the use of therapeutic methods across\nmultiple sessions with long-term memory, leaving them far from real clinical\npractice. To address these critical gaps, we introduce TheraMind, a strategic\nand adaptive agent for longitudinal psychological counseling. The cornerstone\nof TheraMind is a novel dual-loop architecture that decouples the complex\ncounseling process into an Intra-Session Loop for tactical dialogue management\nand a Cross-Session Loop for strategic therapeutic planning. The Intra-Session\nLoop perceives the patient's emotional state to dynamically select response\nstrategies while leveraging cross-session memory to ensure continuity.\nCrucially, the Cross-Session Loop empowers the agent with long-term\nadaptability by evaluating the efficacy of the applied therapy after each\nsession and adjusting the method for subsequent interactions. We validate our\napproach in a high-fidelity simulation environment grounded in real clinical\ncases. Extensive evaluations show that TheraMind outperforms other methods,\nespecially on multi-session metrics like Coherence, Flexibility, and\nTherapeutic Attunement, validating the effectiveness of its dual-loop design in\nemulating strategic, adaptive, and longitudinal therapeutic behavior. The code\nis publicly available at https://0mwwm0.github.io/TheraMind/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) in psychological counseling have attracted\nincreasing attention. However, existing approaches often lack emotional\nunderstanding, adaptive strategies, and the use of therapeutic methods across\nmultiple sessions with long-term memory, leaving them far from real clinical\npractice. To address these critical gaps, we introduce TheraMind, a strategic\nand adaptive agent for longitudinal psychological counseling. The cornerstone\nof TheraMind is a novel dual-loop architecture that decouples the complex\ncounseling process into an Intra-Session Loop for tactical dialogue management\nand a Cross-Session Loop for strategic therapeutic planning. The Intra-Session\nLoop perceives the patient's emotional state to dynamically select response\nstrategies while leveraging cross-session memory to ensure continuity.\nCrucially, the Cross-Session Loop empowers the agent with long-term\nadaptability by evaluating the efficacy of the applied therapy after each\nsession and adjusting the method for subsequent interactions. We validate our\napproach in a high-fidelity simulation environment grounded in real clinical\ncases. Extensive evaluations show that TheraMind outperforms other methods,\nespecially on multi-session metrics like Coherence, Flexibility, and\nTherapeutic Attunement, validating the effectiveness of its dual-loop design in\nemulating strategic, adaptive, and longitudinal therapeutic behavior. The code\nis publicly available at https://0mwwm0.github.io/TheraMind/."
                },
                "authors": [
                    {
                        "name": "He Hu"
                    },
                    {
                        "name": "Yucheng Zhou"
                    },
                    {
                        "name": "Chiyuan Ma"
                    },
                    {
                        "name": "Qianning Wang"
                    },
                    {
                        "name": "Zheng Zhang"
                    },
                    {
                        "name": "Fei Ma"
                    },
                    {
                        "name": "Laizhong Cui"
                    },
                    {
                        "name": "Qi Tian"
                    }
                ],
                "author_detail": {
                    "name": "Qi Tian"
                },
                "author": "Qi Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25758v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25758v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25743v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25743v1",
                "updated": "2025-10-29T17:46:07Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    17,
                    46,
                    7,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T17:46:07Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    17,
                    46,
                    7,
                    2,
                    302,
                    0
                ],
                "title": "Agentic Economic Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic Economic Modeling"
                },
                "summary": "We introduce Agentic Economic Modeling (AEM), a framework that aligns\nsynthetic LLM choices with small-sample human evidence for reliable econometric\ninference. AEM first generates task-conditioned synthetic choices via LLMs,\nthen learns a bias-correction mapping from task features and raw LLM choices to\nhuman-aligned choices, upon which standard econometric estimators perform\ninference to recover demand elasticities and treatment effects.We validate AEM\nin two experiments. In a large scale conjoint study with millions of\nobservations, using only 10% of the original data to fit the correction model\nlowers the error of the demand-parameter estimates, while uncorrected LLM\nchoices even increase the errors. In a regional field experiment, a mixture\nmodel calibrated on 10% of geographic regions estimates an out-of-domain\ntreatment effect of -65\\pm10 bps, closely matching the full human experiment\n(-60\\pm8 bps).Under time-wise extrapolation, training with only day-one human\ndata yields -24 bps (95% CI: [-26, -22], p<1e-5),improving over the human-only\nday-one baseline (-17 bps, 95% CI: [-43, +9], p=0.2049).These results\ndemonstrate AEM's potential to improve RCT efficiency and establish a\nfoundation method for LLM-based counterfactual generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Agentic Economic Modeling (AEM), a framework that aligns\nsynthetic LLM choices with small-sample human evidence for reliable econometric\ninference. AEM first generates task-conditioned synthetic choices via LLMs,\nthen learns a bias-correction mapping from task features and raw LLM choices to\nhuman-aligned choices, upon which standard econometric estimators perform\ninference to recover demand elasticities and treatment effects.We validate AEM\nin two experiments. In a large scale conjoint study with millions of\nobservations, using only 10% of the original data to fit the correction model\nlowers the error of the demand-parameter estimates, while uncorrected LLM\nchoices even increase the errors. In a regional field experiment, a mixture\nmodel calibrated on 10% of geographic regions estimates an out-of-domain\ntreatment effect of -65\\pm10 bps, closely matching the full human experiment\n(-60\\pm8 bps).Under time-wise extrapolation, training with only day-one human\ndata yields -24 bps (95% CI: [-26, -22], p<1e-5),improving over the human-only\nday-one baseline (-17 bps, 95% CI: [-43, +9], p=0.2049).These results\ndemonstrate AEM's potential to improve RCT efficiency and establish a\nfoundation method for LLM-based counterfactual generation."
                },
                "authors": [
                    {
                        "name": "Bohan Zhang"
                    },
                    {
                        "name": "Jiaxuan Li"
                    },
                    {
                        "name": "Ali Hortaçsu"
                    },
                    {
                        "name": "Xiaoyang Ye"
                    },
                    {
                        "name": "Victor Chernozhukov"
                    },
                    {
                        "name": "Angelo Ni"
                    },
                    {
                        "name": "Edward Huang"
                    }
                ],
                "author_detail": {
                    "name": "Edward Huang"
                },
                "author": "Edward Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25743v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25743v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25741v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25741v1",
                "updated": "2025-10-29T17:45:42Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    17,
                    45,
                    42,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T17:45:42Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    17,
                    45,
                    42,
                    2,
                    302,
                    0
                ],
                "title": "Scaling Latent Reasoning via Looped Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Latent Reasoning via Looped Language Models"
                },
                "summary": "Modern LLMs are trained to \"think\" primarily via explicit text generation,\nsuch as chain-of-thought (CoT), which defers reasoning to post-training and\nunder-leverages pre-training data. We present and open-source Ouro, named after\nthe recursive Ouroboros, a family of pre-trained Looped Language Models\n(LoopLM) that instead build reasoning into the pre-training phase through (i)\niterative computation in latent space, (ii) an entropy-regularized objective\nfor learned depth allocation, and (iii) scaling to 7.7T tokens. Ouro 1.4B and\n2.6B models enjoy superior performance that match the results of up to 12B SOTA\nLLMs across a wide range of benchmarks. Through controlled experiments, we show\nthis advantage stems not from increased knowledge capacity, but from superior\nknowledge manipulation capabilities. We also show that LoopLM yields reasoning\ntraces more aligned with final outputs than explicit CoT. We hope our results\nshow the potential of LoopLM as a novel scaling direction in the reasoning era.\nOur model could be found in: http://ouro-llm.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern LLMs are trained to \"think\" primarily via explicit text generation,\nsuch as chain-of-thought (CoT), which defers reasoning to post-training and\nunder-leverages pre-training data. We present and open-source Ouro, named after\nthe recursive Ouroboros, a family of pre-trained Looped Language Models\n(LoopLM) that instead build reasoning into the pre-training phase through (i)\niterative computation in latent space, (ii) an entropy-regularized objective\nfor learned depth allocation, and (iii) scaling to 7.7T tokens. Ouro 1.4B and\n2.6B models enjoy superior performance that match the results of up to 12B SOTA\nLLMs across a wide range of benchmarks. Through controlled experiments, we show\nthis advantage stems not from increased knowledge capacity, but from superior\nknowledge manipulation capabilities. We also show that LoopLM yields reasoning\ntraces more aligned with final outputs than explicit CoT. We hope our results\nshow the potential of LoopLM as a novel scaling direction in the reasoning era.\nOur model could be found in: http://ouro-llm.github.io."
                },
                "authors": [
                    {
                        "name": "Rui-Jie Zhu"
                    },
                    {
                        "name": "Zixuan Wang"
                    },
                    {
                        "name": "Kai Hua"
                    },
                    {
                        "name": "Tianyu Zhang"
                    },
                    {
                        "name": "Ziniu Li"
                    },
                    {
                        "name": "Haoran Que"
                    },
                    {
                        "name": "Boyi Wei"
                    },
                    {
                        "name": "Zixin Wen"
                    },
                    {
                        "name": "Fan Yin"
                    },
                    {
                        "name": "He Xing"
                    },
                    {
                        "name": "Lu Li"
                    },
                    {
                        "name": "Jiajun Shi"
                    },
                    {
                        "name": "Kaijing Ma"
                    },
                    {
                        "name": "Shanda Li"
                    },
                    {
                        "name": "Taylor Kergan"
                    },
                    {
                        "name": "Andrew Smith"
                    },
                    {
                        "name": "Xingwei Qu"
                    },
                    {
                        "name": "Mude Hui"
                    },
                    {
                        "name": "Bohong Wu"
                    },
                    {
                        "name": "Qiyang Min"
                    },
                    {
                        "name": "Hongzhi Huang"
                    },
                    {
                        "name": "Xun Zhou"
                    },
                    {
                        "name": "Wei Ye"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Jian Yang"
                    },
                    {
                        "name": "Yunfeng Shi"
                    },
                    {
                        "name": "Chenghua Lin"
                    },
                    {
                        "name": "Enduo Zhao"
                    },
                    {
                        "name": "Tianle Cai"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Wenhao Huang"
                    },
                    {
                        "name": "Yoshua Bengio"
                    },
                    {
                        "name": "Jason Eshraghian"
                    }
                ],
                "author_detail": {
                    "name": "Jason Eshraghian"
                },
                "author": "Jason Eshraghian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25741v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25741v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25732v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25732v1",
                "updated": "2025-10-29T17:37:50Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    17,
                    37,
                    50,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T17:37:50Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    17,
                    37,
                    50,
                    2,
                    302,
                    0
                ],
                "title": "The Limits of Obliviate: Evaluating Unlearning in LLMs via\n  Stimulus-Knowledge Entanglement-Behavior Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Limits of Obliviate: Evaluating Unlearning in LLMs via\n  Stimulus-Knowledge Entanglement-Behavior Framework"
                },
                "summary": "Unlearning in large language models (LLMs) is crucial for managing sensitive\ndata and correcting misinformation, yet evaluating its effectiveness remains an\nopen problem. We investigate whether persuasive prompting can recall factual\nknowledge from deliberately unlearned LLMs across models ranging from 2.7B to\n13B parameters (OPT-2.7B, LLaMA-2-7B, LLaMA-3.1-8B, LLaMA-2-13B). Drawing from\nACT-R and Hebbian theory (spreading activation theories), as well as\ncommunication principles, we introduce Stimulus-Knowledge Entanglement-Behavior\nFramework (SKeB), which models information entanglement via domain graphs and\ntests whether factual recall in unlearned models is correlated with persuasive\nframing. We develop entanglement metrics to quantify knowledge activation\npatterns and evaluate factuality, non-factuality, and hallucination in outputs.\nOur results show persuasive prompts substantially enhance factual knowledge\nrecall (14.8% baseline vs. 24.5% with authority framing), with effectiveness\ninversely correlated to model size (128% recovery in 2.7B vs. 15% in 13B). SKeB\nprovides a foundation for assessing unlearning completeness, robustness, and\noverall behavior in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlearning in large language models (LLMs) is crucial for managing sensitive\ndata and correcting misinformation, yet evaluating its effectiveness remains an\nopen problem. We investigate whether persuasive prompting can recall factual\nknowledge from deliberately unlearned LLMs across models ranging from 2.7B to\n13B parameters (OPT-2.7B, LLaMA-2-7B, LLaMA-3.1-8B, LLaMA-2-13B). Drawing from\nACT-R and Hebbian theory (spreading activation theories), as well as\ncommunication principles, we introduce Stimulus-Knowledge Entanglement-Behavior\nFramework (SKeB), which models information entanglement via domain graphs and\ntests whether factual recall in unlearned models is correlated with persuasive\nframing. We develop entanglement metrics to quantify knowledge activation\npatterns and evaluate factuality, non-factuality, and hallucination in outputs.\nOur results show persuasive prompts substantially enhance factual knowledge\nrecall (14.8% baseline vs. 24.5% with authority framing), with effectiveness\ninversely correlated to model size (128% recovery in 2.7B vs. 15% in 13B). SKeB\nprovides a foundation for assessing unlearning completeness, robustness, and\noverall behavior in LLMs."
                },
                "authors": [
                    {
                        "name": "Aakriti Shah"
                    },
                    {
                        "name": "Thai Le"
                    }
                ],
                "author_detail": {
                    "name": "Thai Le"
                },
                "author": "Thai Le",
                "arxiv_comment": "14 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25732v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25732v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.6; I.2.4; G.2.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20600v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20600v2",
                "updated": "2025-10-29T17:35:15Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    17,
                    35,
                    15,
                    2,
                    302,
                    0
                ],
                "published": "2025-08-28T09:43:59Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    9,
                    43,
                    59,
                    3,
                    240,
                    0
                ],
                "title": "GENRE-CMR: Generalizable Deep Learning for Diverse Multi-Domain Cardiac\n  MRI Reconstruction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GENRE-CMR: Generalizable Deep Learning for Diverse Multi-Domain Cardiac\n  MRI Reconstruction"
                },
                "summary": "Accelerated Cardiovascular Magnetic Resonance (CMR) image reconstruction\nremains a critical challenge due to the trade-off between scan time and image\nquality, particularly when generalizing across diverse acquisition settings. We\npropose GENRE-CMR, a generative adversarial network (GAN)-based architecture\nemploying a residual deep unrolled reconstruction framework to enhance\nreconstruction fidelity and generalization. The architecture unrolls iterative\noptimization into a cascade of convolutional subnetworks, enriched with\nresidual connections to enable progressive feature propagation from shallow to\ndeeper stages. To further improve performance, we integrate two loss functions:\n(1) an Edge-Aware Region (EAR) loss, which guides the network to focus on\nstructurally informative regions and helps prevent common reconstruction\nblurriness; and (2) a Statistical Distribution Alignment (SDA) loss, which\nregularizes the feature space across diverse data distributions via a symmetric\nKL divergence formulation. Extensive experiments confirm that GENRE-CMR\nsurpasses state-of-the-art methods on training and unseen data, achieving\n0.9552 SSIM and 38.90 dB PSNR on unseen distributions across various\nacceleration factors and sampling trajectories. Ablation studies confirm the\ncontribution of each proposed component to reconstruction quality and\ngeneralization. Our framework presents a unified and robust solution for\nhigh-quality CMR reconstruction, paving the way for clinically adaptable\ndeployment across heterogeneous acquisition protocols.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerated Cardiovascular Magnetic Resonance (CMR) image reconstruction\nremains a critical challenge due to the trade-off between scan time and image\nquality, particularly when generalizing across diverse acquisition settings. We\npropose GENRE-CMR, a generative adversarial network (GAN)-based architecture\nemploying a residual deep unrolled reconstruction framework to enhance\nreconstruction fidelity and generalization. The architecture unrolls iterative\noptimization into a cascade of convolutional subnetworks, enriched with\nresidual connections to enable progressive feature propagation from shallow to\ndeeper stages. To further improve performance, we integrate two loss functions:\n(1) an Edge-Aware Region (EAR) loss, which guides the network to focus on\nstructurally informative regions and helps prevent common reconstruction\nblurriness; and (2) a Statistical Distribution Alignment (SDA) loss, which\nregularizes the feature space across diverse data distributions via a symmetric\nKL divergence formulation. Extensive experiments confirm that GENRE-CMR\nsurpasses state-of-the-art methods on training and unseen data, achieving\n0.9552 SSIM and 38.90 dB PSNR on unseen distributions across various\nacceleration factors and sampling trajectories. Ablation studies confirm the\ncontribution of each proposed component to reconstruction quality and\ngeneralization. Our framework presents a unified and robust solution for\nhigh-quality CMR reconstruction, paving the way for clinically adaptable\ndeployment across heterogeneous acquisition protocols."
                },
                "authors": [
                    {
                        "name": "Kian Anvari Hamedani"
                    },
                    {
                        "name": "Narges Razizadeh"
                    },
                    {
                        "name": "Shahabedin Nabavi"
                    },
                    {
                        "name": "Mohsen Ebrahimi Moghaddam"
                    }
                ],
                "author_detail": {
                    "name": "Mohsen Ebrahimi Moghaddam"
                },
                "author": "Mohsen Ebrahimi Moghaddam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20600v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20600v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25724v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25724v1",
                "updated": "2025-10-29T17:31:27Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    17,
                    31,
                    27,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T17:31:27Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    17,
                    31,
                    27,
                    2,
                    302,
                    0
                ],
                "title": "BambooKG: A Neurobiologically-inspired Frequency-Weight Knowledge Graph",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BambooKG: A Neurobiologically-inspired Frequency-Weight Knowledge Graph"
                },
                "summary": "Retrieval-Augmented Generation allows LLMs to access external knowledge,\nreducing hallucinations and ageing-data issues. However, it treats retrieved\nchunks independently and struggles with multi-hop or relational reasoning,\nespecially across documents. Knowledge graphs enhance this by capturing the\nrelationships between entities using triplets, enabling structured, multi-chunk\nreasoning. However, these tend to miss information that fails to conform to the\ntriplet structure. We introduce BambooKG, a knowledge graph with\nfrequency-based weights on non-triplet edges which reflect link strength,\ndrawing on the Hebbian principle of \"fire together, wire together\". This\ndecreases information loss and results in improved performance on single- and\nmulti-hop reasoning, outperforming the existing solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation allows LLMs to access external knowledge,\nreducing hallucinations and ageing-data issues. However, it treats retrieved\nchunks independently and struggles with multi-hop or relational reasoning,\nespecially across documents. Knowledge graphs enhance this by capturing the\nrelationships between entities using triplets, enabling structured, multi-chunk\nreasoning. However, these tend to miss information that fails to conform to the\ntriplet structure. We introduce BambooKG, a knowledge graph with\nfrequency-based weights on non-triplet edges which reflect link strength,\ndrawing on the Hebbian principle of \"fire together, wire together\". This\ndecreases information loss and results in improved performance on single- and\nmulti-hop reasoning, outperforming the existing solutions."
                },
                "authors": [
                    {
                        "name": "Vanya Arikutharam"
                    },
                    {
                        "name": "Arkadiy Ukolov"
                    }
                ],
                "author_detail": {
                    "name": "Arkadiy Ukolov"
                },
                "author": "Arkadiy Ukolov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25724v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25724v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17937v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17937v3",
                "updated": "2025-10-29T17:29:43Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    17,
                    29,
                    43,
                    2,
                    302,
                    0
                ],
                "published": "2025-07-23T21:11:47Z",
                "published_parsed": [
                    2025,
                    7,
                    23,
                    21,
                    11,
                    47,
                    2,
                    204,
                    0
                ],
                "title": "Bob's Confetti: Phonetic Memorization Attacks in Music and Video\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bob's Confetti: Phonetic Memorization Attacks in Music and Video\n  Generation"
                },
                "summary": "Generative AI systems for music and video commonly use text-based filters to\nprevent the regurgitation of copyrighted material. We expose a fundamental flaw\nin this approach by introducing Adversarial PhoneTic Prompting (APT), a novel\nattack that bypasses these safeguards by exploiting phonetic memorization. The\nAPT attack replaces iconic lyrics with homophonic but semantically unrelated\nalternatives (e.g., \"mom's spaghetti\" becomes \"Bob's confetti\"), preserving\nacoustic structure while altering meaning; we identify high-fidelity phonetic\nmatches using CMU pronouncing dictionary. We demonstrate that leading\nLyrics-to-Song (L2S) models like SUNO and YuE regenerate songs with striking\nmelodic and rhythmic similarity to their copyrighted originals when prompted\nwith these altered lyrics. More surprisingly, this vulnerability extends across\nmodalities. When prompted with phonetically modified lyrics from a song, a\nText-to-Video (T2V) model like Veo 3 reconstructs visual scenes from the\noriginal music video-including specific settings and character\narchetypes-despite the absence of any visual cues in the prompt. Our findings\nreveal that models memorize deep, structural patterns tied to acoustics, not\njust verbatim text. This phonetic-to-visual leakage represents a critical\nvulnerability in transcript-conditioned generative models, rendering simple\ncopyright filters ineffective and raising urgent concerns about the secure\ndeployment of multimodal AI systems. Demo examples are available at our project\npage (https://jrohsc.github.io/music_attack/).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI systems for music and video commonly use text-based filters to\nprevent the regurgitation of copyrighted material. We expose a fundamental flaw\nin this approach by introducing Adversarial PhoneTic Prompting (APT), a novel\nattack that bypasses these safeguards by exploiting phonetic memorization. The\nAPT attack replaces iconic lyrics with homophonic but semantically unrelated\nalternatives (e.g., \"mom's spaghetti\" becomes \"Bob's confetti\"), preserving\nacoustic structure while altering meaning; we identify high-fidelity phonetic\nmatches using CMU pronouncing dictionary. We demonstrate that leading\nLyrics-to-Song (L2S) models like SUNO and YuE regenerate songs with striking\nmelodic and rhythmic similarity to their copyrighted originals when prompted\nwith these altered lyrics. More surprisingly, this vulnerability extends across\nmodalities. When prompted with phonetically modified lyrics from a song, a\nText-to-Video (T2V) model like Veo 3 reconstructs visual scenes from the\noriginal music video-including specific settings and character\narchetypes-despite the absence of any visual cues in the prompt. Our findings\nreveal that models memorize deep, structural patterns tied to acoustics, not\njust verbatim text. This phonetic-to-visual leakage represents a critical\nvulnerability in transcript-conditioned generative models, rendering simple\ncopyright filters ineffective and raising urgent concerns about the secure\ndeployment of multimodal AI systems. Demo examples are available at our project\npage (https://jrohsc.github.io/music_attack/)."
                },
                "authors": [
                    {
                        "name": "Jaechul Roh"
                    },
                    {
                        "name": "Zachary Novack"
                    },
                    {
                        "name": "Yuefeng Peng"
                    },
                    {
                        "name": "Niloofar Mireshghallah"
                    },
                    {
                        "name": "Taylor Berg-Kirkpatrick"
                    },
                    {
                        "name": "Amir Houmansadr"
                    }
                ],
                "author_detail": {
                    "name": "Amir Houmansadr"
                },
                "author": "Amir Houmansadr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.17937v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17937v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23722v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23722v2",
                "updated": "2025-10-29T17:27:45Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    17,
                    27,
                    45,
                    2,
                    302,
                    0
                ],
                "published": "2025-05-29T17:54:32Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    54,
                    32,
                    3,
                    149,
                    0
                ],
                "title": "LLMs are Better Than You Think: Label-Guided In-Context Learning for\n  Named Entity Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs are Better Than You Think: Label-Guided In-Context Learning for\n  Named Entity Recognition"
                },
                "summary": "In-context learning (ICL) enables large language models (LLMs) to perform new\ntasks using only a few demonstrations. However, in Named Entity Recognition\n(NER), existing ICL methods typically rely on task-agnostic semantic similarity\nfor demonstration retrieval, which often yields less relevant examples and\nleads to inferior results. We introduce DEER, a training-free ICL approach that\nenables LLMs to make more informed entity predictions through the use of\nlabel-grounded statistics. DEER leverages token-level statistics from training\nlabels to identify tokens most informative for entity recognition, enabling\nentity-focused demonstrations. It further uses these statistics to detect and\nrefine error-prone tokens through a targeted reflection step. Evaluated on five\nNER datasets across four LLMs, DEER consistently outperforms existing ICL\nmethods and achieves performance comparable to supervised fine-tuning. Further\nanalyses demonstrate that DEER improves example retrieval, remains effective on\nboth seen and unseen entities, and exhibits strong robustness in low-resource\nsettings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) enables large language models (LLMs) to perform new\ntasks using only a few demonstrations. However, in Named Entity Recognition\n(NER), existing ICL methods typically rely on task-agnostic semantic similarity\nfor demonstration retrieval, which often yields less relevant examples and\nleads to inferior results. We introduce DEER, a training-free ICL approach that\nenables LLMs to make more informed entity predictions through the use of\nlabel-grounded statistics. DEER leverages token-level statistics from training\nlabels to identify tokens most informative for entity recognition, enabling\nentity-focused demonstrations. It further uses these statistics to detect and\nrefine error-prone tokens through a targeted reflection step. Evaluated on five\nNER datasets across four LLMs, DEER consistently outperforms existing ICL\nmethods and achieves performance comparable to supervised fine-tuning. Further\nanalyses demonstrate that DEER improves example retrieval, remains effective on\nboth seen and unseen entities, and exhibits strong robustness in low-resource\nsettings."
                },
                "authors": [
                    {
                        "name": "Fan Bai"
                    },
                    {
                        "name": "Hamid Hassanzadeh"
                    },
                    {
                        "name": "Ardavan Saeedi"
                    },
                    {
                        "name": "Mark Dredze"
                    }
                ],
                "author_detail": {
                    "name": "Mark Dredze"
                },
                "author": "Mark Dredze",
                "arxiv_comment": "Accepted to EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23722v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23722v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17720v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17720v4",
                "updated": "2025-10-29T17:15:43Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    17,
                    15,
                    43,
                    2,
                    302,
                    0
                ],
                "published": "2025-02-24T23:23:27Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    23,
                    23,
                    27,
                    0,
                    55,
                    0
                ],
                "title": "Spontaneous Giving and Calculated Greed in Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spontaneous Giving and Calculated Greed in Language Models"
                },
                "summary": "Large language models demonstrate strong problem-solving abilities through\nreasoning techniques such as chain-of-thought prompting and reflection.\nHowever, it remains unclear whether these reasoning capabilities extend to a\nform of social intelligence: making effective decisions in cooperative\ncontexts. We examine this question using economic games that simulate social\ndilemmas. First, we apply chain-of-thought and reflection prompting to GPT-4o\nin a Public Goods Game. We then evaluate multiple off-the-shelf models across\nsix cooperation and punishment games, comparing those with and without explicit\nreasoning mechanisms. We find that reasoning models consistently reduce\ncooperation and norm enforcement, favoring individual rationality. In repeated\ninteractions, groups with more reasoning agents exhibit lower collective gains.\nThese behaviors mirror human patterns of \"spontaneous giving and calculated\ngreed.\" Our findings underscore the need for LLM architectures that incorporate\nsocial intelligence alongside reasoning, to help address--rather than\nreinforce--the challenges of collective action.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models demonstrate strong problem-solving abilities through\nreasoning techniques such as chain-of-thought prompting and reflection.\nHowever, it remains unclear whether these reasoning capabilities extend to a\nform of social intelligence: making effective decisions in cooperative\ncontexts. We examine this question using economic games that simulate social\ndilemmas. First, we apply chain-of-thought and reflection prompting to GPT-4o\nin a Public Goods Game. We then evaluate multiple off-the-shelf models across\nsix cooperation and punishment games, comparing those with and without explicit\nreasoning mechanisms. We find that reasoning models consistently reduce\ncooperation and norm enforcement, favoring individual rationality. In repeated\ninteractions, groups with more reasoning agents exhibit lower collective gains.\nThese behaviors mirror human patterns of \"spontaneous giving and calculated\ngreed.\" Our findings underscore the need for LLM architectures that incorporate\nsocial intelligence alongside reasoning, to help address--rather than\nreinforce--the challenges of collective action."
                },
                "authors": [
                    {
                        "name": "Yuxuan Li"
                    },
                    {
                        "name": "Hirokazu Shirado"
                    }
                ],
                "author_detail": {
                    "name": "Hirokazu Shirado"
                },
                "author": "Hirokazu Shirado",
                "arxiv_comment": "Accepted to EMNLP 2025 main conference and selected as an Oral\n  Presentation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17720v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17720v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15584v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15584v3",
                "updated": "2025-10-29T17:10:45Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    17,
                    10,
                    45,
                    2,
                    302,
                    0
                ],
                "published": "2024-12-20T05:40:32Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    5,
                    40,
                    32,
                    4,
                    355,
                    0
                ],
                "title": "To Rely or Not to Rely? Evaluating Interventions for Appropriate\n  Reliance on Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To Rely or Not to Rely? Evaluating Interventions for Appropriate\n  Reliance on Large Language Models"
                },
                "summary": "As Large Language Models become integral to decision-making, optimism about\ntheir power is tempered with concern over their errors. Users may over-rely on\nLLM advice that is confidently stated but wrong, or under-rely due to mistrust.\nReliance interventions have been developed to help users of LLMs, but they lack\nrigorous evaluation for appropriate reliance. We benchmark the performance of\nthree relevant interventions by conducting a randomized online experiment with\n400 participants attempting two challenging tasks: LSAT logical reasoning and\nimage-based numerical estimation. For each question, participants first\nanswered independently, then received LLM advice modified by one of three\nreliance interventions and answered the question again. Our findings indicate\nthat while interventions reduce over-reliance, they generally fail to improve\nappropriate reliance. Furthermore, people became more confident after making\nwrong reliance decisions in certain contexts, demonstrating poor calibration.\nBased on our findings, we discuss implications for designing effective reliance\ninterventions in human-LLM collaboration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models become integral to decision-making, optimism about\ntheir power is tempered with concern over their errors. Users may over-rely on\nLLM advice that is confidently stated but wrong, or under-rely due to mistrust.\nReliance interventions have been developed to help users of LLMs, but they lack\nrigorous evaluation for appropriate reliance. We benchmark the performance of\nthree relevant interventions by conducting a randomized online experiment with\n400 participants attempting two challenging tasks: LSAT logical reasoning and\nimage-based numerical estimation. For each question, participants first\nanswered independently, then received LLM advice modified by one of three\nreliance interventions and answered the question again. Our findings indicate\nthat while interventions reduce over-reliance, they generally fail to improve\nappropriate reliance. Furthermore, people became more confident after making\nwrong reliance decisions in certain contexts, demonstrating poor calibration.\nBased on our findings, we discuss implications for designing effective reliance\ninterventions in human-LLM collaboration."
                },
                "authors": [
                    {
                        "name": "Jessica Y. Bo"
                    },
                    {
                        "name": "Sophia Wan"
                    },
                    {
                        "name": "Ashton Anderson"
                    }
                ],
                "author_detail": {
                    "name": "Ashton Anderson"
                },
                "author": "Ashton Anderson",
                "arxiv_journal_ref": "Proceedings of the 2025 CHI Conference on Human Factors in\n  Computing Systems",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15584v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15584v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22586v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22586v2",
                "updated": "2025-10-29T17:09:56Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    17,
                    9,
                    56,
                    2,
                    302,
                    0
                ],
                "published": "2025-05-28T16:58:23Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    16,
                    58,
                    23,
                    2,
                    148,
                    0
                ],
                "title": "Precise In-Parameter Concept Erasure in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Precise In-Parameter Concept Erasure in Large Language Models"
                },
                "summary": "Large language models (LLMs) often acquire knowledge during pretraining that\nis undesirable in downstream deployments, e.g., sensitive information or\ncopyrighted content. Existing approaches for removing such knowledge rely on\nfine-tuning, training low-rank adapters or fact-level editing, but these are\neither too coarse, too shallow, or ineffective. In this work, we propose PISCES\n(Precise In-parameter Suppression for Concept EraSure), a novel framework for\nprecisely erasing entire concepts from model parameters by directly editing\ndirections that encode them in parameter space. PISCES uses a disentangler\nmodel to decompose MLP vectors into interpretable features, identifies those\nassociated with a target concept using automated interpretability techniques,\nand removes them from model parameters. Experiments on Gemma 2 and Llama 3.1\nover various concepts show that PISCES achieves modest gains in efficacy over\nleading erasure methods, reducing accuracy on the target concept to as low as\n7.7%, while dramatically improving erasure specificity (by up to 31%) and\nrobustness (by up to 38%). Overall, these results demonstrate that\nfeature-based in-parameter editing enables a more precise and reliable approach\nfor removing conceptual knowledge in language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) often acquire knowledge during pretraining that\nis undesirable in downstream deployments, e.g., sensitive information or\ncopyrighted content. Existing approaches for removing such knowledge rely on\nfine-tuning, training low-rank adapters or fact-level editing, but these are\neither too coarse, too shallow, or ineffective. In this work, we propose PISCES\n(Precise In-parameter Suppression for Concept EraSure), a novel framework for\nprecisely erasing entire concepts from model parameters by directly editing\ndirections that encode them in parameter space. PISCES uses a disentangler\nmodel to decompose MLP vectors into interpretable features, identifies those\nassociated with a target concept using automated interpretability techniques,\nand removes them from model parameters. Experiments on Gemma 2 and Llama 3.1\nover various concepts show that PISCES achieves modest gains in efficacy over\nleading erasure methods, reducing accuracy on the target concept to as low as\n7.7%, while dramatically improving erasure specificity (by up to 31%) and\nrobustness (by up to 38%). Overall, these results demonstrate that\nfeature-based in-parameter editing enables a more precise and reliable approach\nfor removing conceptual knowledge in language models."
                },
                "authors": [
                    {
                        "name": "Yoav Gur-Arieh"
                    },
                    {
                        "name": "Clara Suslik"
                    },
                    {
                        "name": "Yihuai Hong"
                    },
                    {
                        "name": "Fazl Barez"
                    },
                    {
                        "name": "Mor Geva"
                    }
                ],
                "author_detail": {
                    "name": "Mor Geva"
                },
                "author": "Mor Geva",
                "arxiv_comment": "Accepted to EMNLP 2025 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22586v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22586v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25701v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25701v1",
                "updated": "2025-10-29T17:05:00Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    17,
                    5,
                    0,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T17:05:00Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    17,
                    5,
                    0,
                    2,
                    302,
                    0
                ],
                "title": "Interpreting LLMs as Credit Risk Classifiers: Do Their Feature\n  Explanations Align with Classical ML?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interpreting LLMs as Credit Risk Classifiers: Do Their Feature\n  Explanations Align with Classical ML?"
                },
                "summary": "Large Language Models (LLMs) are increasingly explored as flexible\nalternatives to classical machine learning models for classification tasks\nthrough zero-shot prompting. However, their suitability for structured tabular\ndata remains underexplored, especially in high-stakes financial applications\nsuch as financial risk assessment. This study conducts a systematic comparison\nbetween zero-shot LLM-based classifiers and LightGBM, a state-of-the-art\ngradient-boosting model, on a real-world loan default prediction task. We\nevaluate their predictive performance, analyze feature attributions using SHAP,\nand assess the reliability of LLM-generated self-explanations. While LLMs are\nable to identify key financial risk indicators, their feature importance\nrankings diverge notably from LightGBM, and their self-explanations often fail\nto align with empirical SHAP attributions. These findings highlight the\nlimitations of LLMs as standalone models for structured financial risk\nprediction and raise concerns about the trustworthiness of their self-generated\nexplanations. Our results underscore the need for explainability audits,\nbaseline comparisons with interpretable models, and human-in-the-loop oversight\nwhen deploying LLMs in risk-sensitive financial environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly explored as flexible\nalternatives to classical machine learning models for classification tasks\nthrough zero-shot prompting. However, their suitability for structured tabular\ndata remains underexplored, especially in high-stakes financial applications\nsuch as financial risk assessment. This study conducts a systematic comparison\nbetween zero-shot LLM-based classifiers and LightGBM, a state-of-the-art\ngradient-boosting model, on a real-world loan default prediction task. We\nevaluate their predictive performance, analyze feature attributions using SHAP,\nand assess the reliability of LLM-generated self-explanations. While LLMs are\nable to identify key financial risk indicators, their feature importance\nrankings diverge notably from LightGBM, and their self-explanations often fail\nto align with empirical SHAP attributions. These findings highlight the\nlimitations of LLMs as standalone models for structured financial risk\nprediction and raise concerns about the trustworthiness of their self-generated\nexplanations. Our results underscore the need for explainability audits,\nbaseline comparisons with interpretable models, and human-in-the-loop oversight\nwhen deploying LLMs in risk-sensitive financial environments."
                },
                "authors": [
                    {
                        "name": "Saeed AlMarri"
                    },
                    {
                        "name": "Kristof Juhasz"
                    },
                    {
                        "name": "Mathieu Ravaut"
                    },
                    {
                        "name": "Gautier Marti"
                    },
                    {
                        "name": "Hamdan Al Ahbabi"
                    },
                    {
                        "name": "Ibrahim Elfadel"
                    }
                ],
                "author_detail": {
                    "name": "Ibrahim Elfadel"
                },
                "author": "Ibrahim Elfadel",
                "arxiv_comment": "8 pages, 6 figures, 3 tables, CIKM 2025 FinFAI workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25701v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25701v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25694v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25694v1",
                "updated": "2025-10-29T16:59:07Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    16,
                    59,
                    7,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T16:59:07Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    16,
                    59,
                    7,
                    2,
                    302,
                    0
                ],
                "title": "Process-Level Trajectory Evaluation for Environment Configuration in\n  Software Engineering Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Process-Level Trajectory Evaluation for Environment Configuration in\n  Software Engineering Agents"
                },
                "summary": "Large language model-based agents show promise for software engineering, but\nenvironment configuration remains a bottleneck due to heavy manual effort and\nscarce large-scale, high-quality datasets. Existing benchmarks assess only\nend-to-end build/test success, obscuring where and why agents succeed or fail.\nWe introduce the Environment Configuration Diagnosis Benchmark, Enconda-bench,\nwhich provides process-level trajectory assessment of fine-grained agent\ncapabilities during environment setup-planning, perception-driven error\ndiagnosis, feedback-driven repair, and action to execute final environment\nconfiguration. Our task instances are automatically constructed by injecting\nrealistic README errors and are validated in Docker for scalable, high-quality\nevaluation. Enconda-bench combines process-level analysis with end-to-end\nexecutability to enable capability assessments beyond aggregate success rates.\nEvaluations across state-of-the-art LLMs and agent frameworks show that while\nagents can localize errors, they struggle to translate feedback into effective\ncorrections, limiting end-to-end performance. To our knowledge, Enconda-bench\nis the first framework to provide process-level internal capability assessment\nfor environment configuration, offering actionable insights for improving\nsoftware engineering agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model-based agents show promise for software engineering, but\nenvironment configuration remains a bottleneck due to heavy manual effort and\nscarce large-scale, high-quality datasets. Existing benchmarks assess only\nend-to-end build/test success, obscuring where and why agents succeed or fail.\nWe introduce the Environment Configuration Diagnosis Benchmark, Enconda-bench,\nwhich provides process-level trajectory assessment of fine-grained agent\ncapabilities during environment setup-planning, perception-driven error\ndiagnosis, feedback-driven repair, and action to execute final environment\nconfiguration. Our task instances are automatically constructed by injecting\nrealistic README errors and are validated in Docker for scalable, high-quality\nevaluation. Enconda-bench combines process-level analysis with end-to-end\nexecutability to enable capability assessments beyond aggregate success rates.\nEvaluations across state-of-the-art LLMs and agent frameworks show that while\nagents can localize errors, they struggle to translate feedback into effective\ncorrections, limiting end-to-end performance. To our knowledge, Enconda-bench\nis the first framework to provide process-level internal capability assessment\nfor environment configuration, offering actionable insights for improving\nsoftware engineering agents."
                },
                "authors": [
                    {
                        "name": "Jiayi Kuang"
                    },
                    {
                        "name": "Yinghui Li"
                    },
                    {
                        "name": "Xin Zhang"
                    },
                    {
                        "name": "Yangning Li"
                    },
                    {
                        "name": "Di Yin"
                    },
                    {
                        "name": "Xing Sun"
                    },
                    {
                        "name": "Ying Shen"
                    },
                    {
                        "name": "Philip S. Yu"
                    }
                ],
                "author_detail": {
                    "name": "Philip S. Yu"
                },
                "author": "Philip S. Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25694v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25694v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25662v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25662v1",
                "updated": "2025-10-29T16:23:46Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    16,
                    23,
                    46,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T16:23:46Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    16,
                    23,
                    46,
                    2,
                    302,
                    0
                ],
                "title": "User Misconceptions of LLM-Based Conversational Programming Assistants",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "User Misconceptions of LLM-Based Conversational Programming Assistants"
                },
                "summary": "Programming assistants powered by large language models (LLMs) have become\nwidely available, with conversational assistants like ChatGPT proving\nparticularly accessible to less experienced programmers. However, the varied\ncapabilities of these tools across model versions and the mixed availability of\nextensions that enable web search, code execution, or retrieval-augmented\ngeneration create opportunities for user misconceptions about what systems can\nand cannot do. Such misconceptions may lead to over-reliance, unproductive\npractices, or insufficient quality control in LLM-assisted programming. Here,\nwe aim to characterize misconceptions that users of conversational LLM-based\nassistants may have in programming contexts. Using a two-phase approach, we\nfirst brainstorm and catalog user misconceptions that may occur, and then\nconduct a qualitative analysis to examine whether these conceptual issues\nsurface in naturalistic Python-programming conversations with an LLM-based\nchatbot drawn from an openly available dataset. Indeed, we see evidence that\nsome users have misplaced expectations about the availability of LLM-based\nchatbot features like web access, code execution, or non-text output\ngeneration. We also see potential evidence for deeper conceptual issues around\nthe scope of information required to debug, validate, and optimize programs.\nOur findings reinforce the need for designing LLM-based tools that more clearly\ncommunicate their programming capabilities to users.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Programming assistants powered by large language models (LLMs) have become\nwidely available, with conversational assistants like ChatGPT proving\nparticularly accessible to less experienced programmers. However, the varied\ncapabilities of these tools across model versions and the mixed availability of\nextensions that enable web search, code execution, or retrieval-augmented\ngeneration create opportunities for user misconceptions about what systems can\nand cannot do. Such misconceptions may lead to over-reliance, unproductive\npractices, or insufficient quality control in LLM-assisted programming. Here,\nwe aim to characterize misconceptions that users of conversational LLM-based\nassistants may have in programming contexts. Using a two-phase approach, we\nfirst brainstorm and catalog user misconceptions that may occur, and then\nconduct a qualitative analysis to examine whether these conceptual issues\nsurface in naturalistic Python-programming conversations with an LLM-based\nchatbot drawn from an openly available dataset. Indeed, we see evidence that\nsome users have misplaced expectations about the availability of LLM-based\nchatbot features like web access, code execution, or non-text output\ngeneration. We also see potential evidence for deeper conceptual issues around\nthe scope of information required to debug, validate, and optimize programs.\nOur findings reinforce the need for designing LLM-based tools that more clearly\ncommunicate their programming capabilities to users."
                },
                "authors": [
                    {
                        "name": "Gabrielle O'Brien"
                    },
                    {
                        "name": "Antonio Pedro Santos Alves"
                    },
                    {
                        "name": "Sebastian Baltes"
                    },
                    {
                        "name": "Grischa Liebel"
                    },
                    {
                        "name": "Mircea Lungu"
                    },
                    {
                        "name": "Marcos Kalinowski"
                    }
                ],
                "author_detail": {
                    "name": "Marcos Kalinowski"
                },
                "author": "Marcos Kalinowski",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25662v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25662v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24636v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24636v2",
                "updated": "2025-10-29T16:06:18Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    16,
                    6,
                    18,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-28T17:02:46Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    17,
                    2,
                    46,
                    1,
                    301,
                    0
                ],
                "title": "OpenReward: Learning to Reward Long-form Agentic Tasks via Reinforcement\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OpenReward: Learning to Reward Long-form Agentic Tasks via Reinforcement\n  Learning"
                },
                "summary": "Reward models (RMs) have become essential for aligning large language models\n(LLMs), serving as scalable proxies for human evaluation in both training and\ninference. However, existing RMs struggle on knowledge-intensive and long-form\ntasks, where evaluating correctness requires grounding beyond the model's\ninternal knowledge. This limitation hinders them from reliably discriminating\nsubtle quality differences, especially when external evidence is necessary. To\naddress this, we introduce OpenRM, a tool-augmented long-form reward model that\nsystematically judges open-ended responses by invoking external tools to gather\nrelevant evidence. We train OpenRM with Group Relative Policy Optimization\n(GRPO) on over 27K synthesized pairwise examples generated through a\ncontrollable data synthesis framework. The training objective jointly\nsupervises intermediate tool usage and final outcome accuracy, incentivizing\nour reward model to learn effective evidence-based judgment strategies.\nExtensive experiments on three newly-collected datasets and two widely-used\nbenchmarks demonstrate that OpenRM substantially outperforms existing reward\nmodeling approaches. As a further step, we integrate OpenRM into both\ninference-time response selection and training-time data selection. This yields\nconsistent gains in downstream LLM alignment tasks, highlighting the potential\nof tool-augmented reward models for scaling reliable long-form evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reward models (RMs) have become essential for aligning large language models\n(LLMs), serving as scalable proxies for human evaluation in both training and\ninference. However, existing RMs struggle on knowledge-intensive and long-form\ntasks, where evaluating correctness requires grounding beyond the model's\ninternal knowledge. This limitation hinders them from reliably discriminating\nsubtle quality differences, especially when external evidence is necessary. To\naddress this, we introduce OpenRM, a tool-augmented long-form reward model that\nsystematically judges open-ended responses by invoking external tools to gather\nrelevant evidence. We train OpenRM with Group Relative Policy Optimization\n(GRPO) on over 27K synthesized pairwise examples generated through a\ncontrollable data synthesis framework. The training objective jointly\nsupervises intermediate tool usage and final outcome accuracy, incentivizing\nour reward model to learn effective evidence-based judgment strategies.\nExtensive experiments on three newly-collected datasets and two widely-used\nbenchmarks demonstrate that OpenRM substantially outperforms existing reward\nmodeling approaches. As a further step, we integrate OpenRM into both\ninference-time response selection and training-time data selection. This yields\nconsistent gains in downstream LLM alignment tasks, highlighting the potential\nof tool-augmented reward models for scaling reliable long-form evaluation."
                },
                "authors": [
                    {
                        "name": "Ziyou Hu"
                    },
                    {
                        "name": "Zhengliang Shi"
                    },
                    {
                        "name": "Minghang Zhu"
                    },
                    {
                        "name": "Haitao Li"
                    },
                    {
                        "name": "Teng Sun"
                    },
                    {
                        "name": "Pengjie Ren"
                    },
                    {
                        "name": "Suzan Verberne"
                    },
                    {
                        "name": "Zhaochun Ren"
                    }
                ],
                "author_detail": {
                    "name": "Zhaochun Ren"
                },
                "author": "Zhaochun Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24636v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24636v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.07464v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.07464v3",
                "updated": "2025-10-29T15:59:41Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    15,
                    59,
                    41,
                    2,
                    302,
                    0
                ],
                "published": "2025-06-09T06:15:54Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    6,
                    15,
                    54,
                    0,
                    160,
                    0
                ],
                "title": "DeepVideo-R1: Video Reinforcement Fine-Tuning via Difficulty-aware\n  Regressive GRPO",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepVideo-R1: Video Reinforcement Fine-Tuning via Difficulty-aware\n  Regressive GRPO"
                },
                "summary": "Recent works have demonstrated the effectiveness of reinforcement learning\n(RL)-based post-training for enhancing the reasoning capabilities of large\nlanguage models (LLMs). In particular, Group Relative Policy Optimization\n(GRPO) has shown impressive success using a PPO-style reinforcement algorithm\nwith group-normalized rewards. However, the effectiveness of GRPO in Video\nLarge Language Models (VideoLLMs) has still been less studyed. In this paper,\nwe explore GRPO and identify two problems that deteriorate the effective\nlearning: (1) reliance on safeguards, and (2) vanishing advantage. To mitigate\nthese challenges, we propose DeepVideo-R1, a video large language model trained\nwith Reg-GRPO (Regressive GRPO) and difficulty-aware data augmentation.\nReg-GRPO reformulates the GRPO loss function into a regression task that\ndirectly predicts the advantage in GRPO, eliminating the need for safeguards\nsuch as the clipping and min functions. It directly aligns the model with\nadvantages, providing guidance to prefer better ones. The difficulty-aware data\naugmentation strategy augments input prompts/videos to locate the difficulty of\nsamples at solvable difficulty levels, enabling diverse reward signals. Our\nexperimental results show that our approach significantly improves video\nreasoning performance across multiple benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent works have demonstrated the effectiveness of reinforcement learning\n(RL)-based post-training for enhancing the reasoning capabilities of large\nlanguage models (LLMs). In particular, Group Relative Policy Optimization\n(GRPO) has shown impressive success using a PPO-style reinforcement algorithm\nwith group-normalized rewards. However, the effectiveness of GRPO in Video\nLarge Language Models (VideoLLMs) has still been less studyed. In this paper,\nwe explore GRPO and identify two problems that deteriorate the effective\nlearning: (1) reliance on safeguards, and (2) vanishing advantage. To mitigate\nthese challenges, we propose DeepVideo-R1, a video large language model trained\nwith Reg-GRPO (Regressive GRPO) and difficulty-aware data augmentation.\nReg-GRPO reformulates the GRPO loss function into a regression task that\ndirectly predicts the advantage in GRPO, eliminating the need for safeguards\nsuch as the clipping and min functions. It directly aligns the model with\nadvantages, providing guidance to prefer better ones. The difficulty-aware data\naugmentation strategy augments input prompts/videos to locate the difficulty of\nsamples at solvable difficulty levels, enabling diverse reward signals. Our\nexperimental results show that our approach significantly improves video\nreasoning performance across multiple benchmarks."
                },
                "authors": [
                    {
                        "name": "Jinyoung Park"
                    },
                    {
                        "name": "Jeehye Na"
                    },
                    {
                        "name": "Jinyoung Kim"
                    },
                    {
                        "name": "Hyunwoo J. Kim"
                    }
                ],
                "author_detail": {
                    "name": "Hyunwoo J. Kim"
                },
                "author": "Hyunwoo J. Kim",
                "arxiv_comment": "NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.07464v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.07464v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.14785v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.14785v2",
                "updated": "2025-10-29T15:56:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    15,
                    56,
                    28,
                    2,
                    302,
                    0
                ],
                "published": "2025-07-20T02:00:21Z",
                "published_parsed": [
                    2025,
                    7,
                    20,
                    2,
                    0,
                    21,
                    6,
                    201,
                    0
                ],
                "title": "Exploring the In-Context Learning Capabilities of LLMs for Money\n  Laundering Detection in Financial Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the In-Context Learning Capabilities of LLMs for Money\n  Laundering Detection in Financial Graphs"
                },
                "summary": "The complexity and interconnectivity of entities involved in money laundering\ndemand investigative reasoning over graph-structured data. This paper explores\nthe use of large language models (LLMs) as reasoning engines over localized\nsubgraphs extracted from a financial knowledge graph. We propose a lightweight\npipeline that retrieves k-hop neighborhoods around entities of interest,\nserializes them into structured text, and prompts an LLM via few-shot\nin-context learning to assess suspiciousness and generate justifications. Using\nsynthetic anti-money laundering (AML) scenarios that reflect common laundering\nbehaviors, we show that LLMs can emulate analyst-style logic, highlight red\nflags, and provide coherent explanations. While this study is exploratory, it\nillustrates the potential of LLM-based graph reasoning in AML and lays\ngroundwork for explainable, language-driven financial crime analytics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The complexity and interconnectivity of entities involved in money laundering\ndemand investigative reasoning over graph-structured data. This paper explores\nthe use of large language models (LLMs) as reasoning engines over localized\nsubgraphs extracted from a financial knowledge graph. We propose a lightweight\npipeline that retrieves k-hop neighborhoods around entities of interest,\nserializes them into structured text, and prompts an LLM via few-shot\nin-context learning to assess suspiciousness and generate justifications. Using\nsynthetic anti-money laundering (AML) scenarios that reflect common laundering\nbehaviors, we show that LLMs can emulate analyst-style logic, highlight red\nflags, and provide coherent explanations. While this study is exploratory, it\nillustrates the potential of LLM-based graph reasoning in AML and lays\ngroundwork for explainable, language-driven financial crime analytics."
                },
                "authors": [
                    {
                        "name": "Erfan Pirmorad"
                    }
                ],
                "author_detail": {
                    "name": "Erfan Pirmorad"
                },
                "author": "Erfan Pirmorad",
                "arxiv_comment": "Accepted at AI4FCF-ICDM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.14785v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.14785v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.23965v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.23965v2",
                "updated": "2025-10-29T15:51:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    15,
                    51,
                    35,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-28T00:42:38Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    0,
                    42,
                    38,
                    1,
                    301,
                    0
                ],
                "title": "The Sign Estimator: LLM Alignment in the Face of Choice Heterogeneity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Sign Estimator: LLM Alignment in the Face of Choice Heterogeneity"
                },
                "summary": "Traditional LLM alignment methods are vulnerable to heterogeneity in human\npreferences. Fitting a na\\\"ive probabilistic model to pairwise comparison data\n(say over prompt-completion pairs) yields an inconsistent estimate of the\npopulation-average utility -a canonical measure of social welfare. We propose a\nnew method, dubbed the sign estimator, that provides a simple, provably\nconsistent, and efficient estimator by replacing cross-entropy with binary\nclassification loss in the aggregation step. This simple modification recovers\nconsistent ordinal alignment under mild assumptions and achieves the first\npolynomial finite-sample error bounds in this setting. In realistic simulations\nof LLM alignment using digital twins, the sign estimator substantially reduces\npreference distortion over a panel of simulated personas, cutting (angular)\nestimation error by nearly 35% and decreasing disagreement with true population\npreferences from 12% to 8% compared to standard RLHF. Our method also compares\nfavorably to panel data heuristics that explicitly model user heterogeneity and\nrequire tracking individual-level preference data-all while maintaining the\nimplementation simplicity of existing LLM alignment pipelines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional LLM alignment methods are vulnerable to heterogeneity in human\npreferences. Fitting a na\\\"ive probabilistic model to pairwise comparison data\n(say over prompt-completion pairs) yields an inconsistent estimate of the\npopulation-average utility -a canonical measure of social welfare. We propose a\nnew method, dubbed the sign estimator, that provides a simple, provably\nconsistent, and efficient estimator by replacing cross-entropy with binary\nclassification loss in the aggregation step. This simple modification recovers\nconsistent ordinal alignment under mild assumptions and achieves the first\npolynomial finite-sample error bounds in this setting. In realistic simulations\nof LLM alignment using digital twins, the sign estimator substantially reduces\npreference distortion over a panel of simulated personas, cutting (angular)\nestimation error by nearly 35% and decreasing disagreement with true population\npreferences from 12% to 8% compared to standard RLHF. Our method also compares\nfavorably to panel data heuristics that explicitly model user heterogeneity and\nrequire tracking individual-level preference data-all while maintaining the\nimplementation simplicity of existing LLM alignment pipelines."
                },
                "authors": [
                    {
                        "name": "Ali Aouad"
                    },
                    {
                        "name": "Aymane El Gadarri"
                    },
                    {
                        "name": "Vivek F. Farias"
                    }
                ],
                "author_detail": {
                    "name": "Vivek F. Farias"
                },
                "author": "Vivek F. Farias",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.23965v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.23965v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03759v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03759v2",
                "updated": "2025-10-29T15:50:30Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    15,
                    50,
                    30,
                    2,
                    302,
                    0
                ],
                "published": "2025-02-06T03:43:44Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    3,
                    43,
                    44,
                    3,
                    37,
                    0
                ],
                "title": "Drone Beam Mapping of the TONE Radio Dish Array",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Drone Beam Mapping of the TONE Radio Dish Array"
                },
                "summary": "Drone-based beam measurements are a promising avenue to tackle the critical\nchallenge of calibration for 21 cm cosmology telescopes. In this paper, we\nintroduce a new drone-based calibration system for 400-800 MHz radio\nobservatories, describing its instrumentation and first deployment. We discuss\nmeasurements of the TONE array, a CHIME/FRB outrigger pathfinder, and present\nresults, including full 2D high spatial resolution beam maps in both co- and\ncross-polarization, as well as comparisons to simulations. The polarized beam\nmaps cover a 70 degree by 70 degree grid, capturing the first two sidelobes and\nmeasuring the TONE main beam and first sidelobe with 7-9% statistical errors.\nWe investigate polarization angle alignment with frequency, finding significant\npolarization leakage in the TONE antennas at frequencies above 600 MHz, and a\npolarization axis rotation with frequency. We describe statistical and\nsystematic errors, as well as measurements of radio frequency interference from\nthe drone and equipment. Our drone system is the first to incorporate a\nbroad-band switched calibration source in the drone payload, enabling\nbackground subtraction and direct measurements of the RFI emitted by the drone.\nThe results presented are the first drone-based 2D measurements of cross-polar\nbeam structure and of polarization alignment of an array. The high frequency\nand spatial resolution achieved with this system have revealed the rich\nstructure of the beam of each antenna, and enabled comparisons between\nindividual dishes and to electromagnetic simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Drone-based beam measurements are a promising avenue to tackle the critical\nchallenge of calibration for 21 cm cosmology telescopes. In this paper, we\nintroduce a new drone-based calibration system for 400-800 MHz radio\nobservatories, describing its instrumentation and first deployment. We discuss\nmeasurements of the TONE array, a CHIME/FRB outrigger pathfinder, and present\nresults, including full 2D high spatial resolution beam maps in both co- and\ncross-polarization, as well as comparisons to simulations. The polarized beam\nmaps cover a 70 degree by 70 degree grid, capturing the first two sidelobes and\nmeasuring the TONE main beam and first sidelobe with 7-9% statistical errors.\nWe investigate polarization angle alignment with frequency, finding significant\npolarization leakage in the TONE antennas at frequencies above 600 MHz, and a\npolarization axis rotation with frequency. We describe statistical and\nsystematic errors, as well as measurements of radio frequency interference from\nthe drone and equipment. Our drone system is the first to incorporate a\nbroad-band switched calibration source in the drone payload, enabling\nbackground subtraction and direct measurements of the RFI emitted by the drone.\nThe results presented are the first drone-based 2D measurements of cross-polar\nbeam structure and of polarization alignment of an array. The high frequency\nand spatial resolution achieved with this system have revealed the rich\nstructure of the beam of each antenna, and enabled comparisons between\nindividual dishes and to electromagnetic simulations."
                },
                "authors": [
                    {
                        "name": "Emily R. Kuhn"
                    },
                    {
                        "name": "Will Tyndall"
                    },
                    {
                        "name": "Benjamin R. B. Saliwanchik"
                    },
                    {
                        "name": "Anna Rose Polish"
                    },
                    {
                        "name": "Maile Harris"
                    },
                    {
                        "name": "Laura B. Newburgh"
                    }
                ],
                "author_detail": {
                    "name": "Laura B. Newburgh"
                },
                "author": "Laura B. Newburgh",
                "arxiv_doi": "10.3847/1538-3881/add269",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3847/1538-3881/add269",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.03759v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03759v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "26 Pages, 19 Figures, 4 Tables. This is a pre-copyedited,\n  author-produced PDF of an article accepted for publication in The\n  Astronomical Journal following peer review (2025 July). The version of record\n  is available online at:\n  https://iopscience.iop.org/article/10.3847/1538-3881/add269",
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25628v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25628v1",
                "updated": "2025-10-29T15:32:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    15,
                    32,
                    47,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T15:32:47Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    15,
                    32,
                    47,
                    2,
                    302,
                    0
                ],
                "title": "EHR-R1: A Reasoning-Enhanced Foundational Language Model for Electronic\n  Health Record Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EHR-R1: A Reasoning-Enhanced Foundational Language Model for Electronic\n  Health Record Analysis"
                },
                "summary": "Electronic Health Records (EHRs) contain rich yet complex information, and\ntheir automated analysis is critical for clinical decision-making. Despite\nrecent advances of large language models (LLMs) in clinical workflows, their\nability to analyze EHRs remains limited due to narrow task coverage and lack of\nEHR-oriented reasoning capabilities. This paper aims to bridge the gap,\nspecifically, we present EHR-Ins, a large-scale, comprehensive EHR reasoning\ninstruction dataset, comprising 300k high-quality reasoning cases and 4M\nnon-reasoning cases across 42 distinct EHR tasks. Its core innovation is a\nthinking-graph-driven framework that enables to generate high-quality reasoning\ndata at scale. Based on it, we develop EHR-R1, a series of reasoning-enhanced\nLLMs with up to 72B parameters tailored for EHR analysis. Through a multi-stage\ntraining paradigm, including domain adaptation, reasoning enhancement, and\nreinforcement learning, EHR-R1 systematically acquires domain knowledge and\ndiverse reasoning capabilities, enabling accurate and robust EHR analysis.\nLastly, we introduce EHR-Bench, a new benchmark curated from MIMIC-IV, spanning\n42 tasks, to comprehensively assess reasoning and prediction across EHR\nscenarios. In experiments, we show that the resulting EHR-R1 consistently\noutperforms state-of-the-art commercial and open-source LLMs (including\nDeepSeek-V3 and GPT-4o), surpassing GPT-4o by over 30 points on MIMIC-Bench and\nachieving a 10\\% higher zero-shot AUROC on EHRSHOT. Collectively, EHR-Ins,\nEHR-R1, and EHR-Bench have significantly advanced the development for more\nreliable and clinically relevant EHR analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electronic Health Records (EHRs) contain rich yet complex information, and\ntheir automated analysis is critical for clinical decision-making. Despite\nrecent advances of large language models (LLMs) in clinical workflows, their\nability to analyze EHRs remains limited due to narrow task coverage and lack of\nEHR-oriented reasoning capabilities. This paper aims to bridge the gap,\nspecifically, we present EHR-Ins, a large-scale, comprehensive EHR reasoning\ninstruction dataset, comprising 300k high-quality reasoning cases and 4M\nnon-reasoning cases across 42 distinct EHR tasks. Its core innovation is a\nthinking-graph-driven framework that enables to generate high-quality reasoning\ndata at scale. Based on it, we develop EHR-R1, a series of reasoning-enhanced\nLLMs with up to 72B parameters tailored for EHR analysis. Through a multi-stage\ntraining paradigm, including domain adaptation, reasoning enhancement, and\nreinforcement learning, EHR-R1 systematically acquires domain knowledge and\ndiverse reasoning capabilities, enabling accurate and robust EHR analysis.\nLastly, we introduce EHR-Bench, a new benchmark curated from MIMIC-IV, spanning\n42 tasks, to comprehensively assess reasoning and prediction across EHR\nscenarios. In experiments, we show that the resulting EHR-R1 consistently\noutperforms state-of-the-art commercial and open-source LLMs (including\nDeepSeek-V3 and GPT-4o), surpassing GPT-4o by over 30 points on MIMIC-Bench and\nachieving a 10\\% higher zero-shot AUROC on EHRSHOT. Collectively, EHR-Ins,\nEHR-R1, and EHR-Bench have significantly advanced the development for more\nreliable and clinically relevant EHR analysis."
                },
                "authors": [
                    {
                        "name": "Yusheng Liao"
                    },
                    {
                        "name": "Chaoyi Wu"
                    },
                    {
                        "name": "Junwei Liu"
                    },
                    {
                        "name": "Shuyang Jiang"
                    },
                    {
                        "name": "Pengcheng Qiu"
                    },
                    {
                        "name": "Haowen Wang"
                    },
                    {
                        "name": "Yun Yue"
                    },
                    {
                        "name": "Shuai Zhen"
                    },
                    {
                        "name": "Jian Wang"
                    },
                    {
                        "name": "Qianrui Fan"
                    },
                    {
                        "name": "Jinjie Gu"
                    },
                    {
                        "name": "Ya Zhang"
                    },
                    {
                        "name": "Yanfeng Wang"
                    },
                    {
                        "name": "Yu Wang"
                    },
                    {
                        "name": "Weidi Xie"
                    }
                ],
                "author_detail": {
                    "name": "Weidi Xie"
                },
                "author": "Weidi Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25628v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25628v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25623v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25623v1",
                "updated": "2025-10-29T15:27:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    15,
                    27,
                    47,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T15:27:47Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    15,
                    27,
                    47,
                    2,
                    302,
                    0
                ],
                "title": "Evaluating the Role of Verifiers in Test-Time Scaling for Legal\n  Reasoning Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the Role of Verifiers in Test-Time Scaling for Legal\n  Reasoning Tasks"
                },
                "summary": "Test-time scaling (TTS) techniques can improve the performance of large\nlanguage models (LLMs) at the expense of additional computation and latency.\nWhile TTS has proven effective in formal domains such as mathematics and\nprogramming \\citep{snell2024scaling, chen2024more}, its value in argumentative\ndomains such as law remains underexplored. We present an empirical study of\nverifier-based TTS methods for legal multiple-choice QA (MCQA) across five\nbenchmarks. Using a family of 7 reward models, we evaluate both outcome-level\n(Best-of-$N$) and process-level (tree search) verification under realistic\nlow-$N$ budgets. Our analysis systematically investigates how verifier utility\nis affected by key properties such as domain specialization, model size, and\nsupervision type (process-supervised PRMs vs. outcome-only ORMs), even when\napplied across different roles.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time scaling (TTS) techniques can improve the performance of large\nlanguage models (LLMs) at the expense of additional computation and latency.\nWhile TTS has proven effective in formal domains such as mathematics and\nprogramming \\citep{snell2024scaling, chen2024more}, its value in argumentative\ndomains such as law remains underexplored. We present an empirical study of\nverifier-based TTS methods for legal multiple-choice QA (MCQA) across five\nbenchmarks. Using a family of 7 reward models, we evaluate both outcome-level\n(Best-of-$N$) and process-level (tree search) verification under realistic\nlow-$N$ budgets. Our analysis systematically investigates how verifier utility\nis affected by key properties such as domain specialization, model size, and\nsupervision type (process-supervised PRMs vs. outcome-only ORMs), even when\napplied across different roles."
                },
                "authors": [
                    {
                        "name": "Davide Romano"
                    },
                    {
                        "name": "Jonathan Schwarz"
                    },
                    {
                        "name": "Daniele Giofré"
                    }
                ],
                "author_detail": {
                    "name": "Daniele Giofré"
                },
                "author": "Daniele Giofré",
                "arxiv_comment": "Accepted to EMNLP - NLLP Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25623v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25623v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09810v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09810v2",
                "updated": "2025-10-29T15:27:12Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    15,
                    27,
                    12,
                    2,
                    302,
                    0
                ],
                "published": "2025-09-11T19:28:56Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    19,
                    28,
                    56,
                    3,
                    254,
                    0
                ],
                "title": "Towards a Common Framework for Autoformalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards a Common Framework for Autoformalization"
                },
                "summary": "Autoformalization has emerged as a term referring to the automation of\nformalization - specifically, the formalization of mathematics using\ninteractive theorem provers (proof assistants). Its rapid development has been\ndriven by progress in deep learning, especially large language models (LLMs).\nMore recently, the term has expanded beyond mathematics to describe the broader\ntask of translating informal input into formal logical representations. At the\nsame time, a growing body of research explores using LLMs to translate informal\nlanguage into formal representations for reasoning, planning, and knowledge\nrepresentation - often without explicitly referring to this process as\nautoformalization. As a result, despite addressing similar tasks, the largely\nindependent development of these research areas has limited opportunities for\nshared methodologies, benchmarks, and theoretical frameworks that could\naccelerate progress. The goal of this paper is to review - explicit or implicit\n- instances of what can be considered autoformalization and to propose a\nunified framework, encouraging cross-pollination between different fields to\nadvance the development of next generation AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoformalization has emerged as a term referring to the automation of\nformalization - specifically, the formalization of mathematics using\ninteractive theorem provers (proof assistants). Its rapid development has been\ndriven by progress in deep learning, especially large language models (LLMs).\nMore recently, the term has expanded beyond mathematics to describe the broader\ntask of translating informal input into formal logical representations. At the\nsame time, a growing body of research explores using LLMs to translate informal\nlanguage into formal representations for reasoning, planning, and knowledge\nrepresentation - often without explicitly referring to this process as\nautoformalization. As a result, despite addressing similar tasks, the largely\nindependent development of these research areas has limited opportunities for\nshared methodologies, benchmarks, and theoretical frameworks that could\naccelerate progress. The goal of this paper is to review - explicit or implicit\n- instances of what can be considered autoformalization and to propose a\nunified framework, encouraging cross-pollination between different fields to\nadvance the development of next generation AI systems."
                },
                "authors": [
                    {
                        "name": "Agnieszka Mensfelt"
                    },
                    {
                        "name": "David Tena Cucala"
                    },
                    {
                        "name": "Santiago Franco"
                    },
                    {
                        "name": "Angeliki Koutsoukou-Argyraki"
                    },
                    {
                        "name": "Vince Trencsenyi"
                    },
                    {
                        "name": "Kostas Stathis"
                    }
                ],
                "author_detail": {
                    "name": "Kostas Stathis"
                },
                "author": "Kostas Stathis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09810v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09810v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25621v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25621v1",
                "updated": "2025-10-29T15:25:34Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    15,
                    25,
                    34,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T15:25:34Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    15,
                    25,
                    34,
                    2,
                    302,
                    0
                ],
                "title": "FARSIQA: Faithful and Advanced RAG System for Islamic Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FARSIQA: Faithful and Advanced RAG System for Islamic Question Answering"
                },
                "summary": "The advent of Large Language Models (LLMs) has revolutionized Natural\nLanguage Processing, yet their application in high-stakes, specialized domains\nlike religious question answering is hindered by challenges like hallucination\nand unfaithfulness to authoritative sources. This issue is particularly\ncritical for the Persian-speaking Muslim community, where accuracy and\ntrustworthiness are paramount. Existing Retrieval-Augmented Generation (RAG)\nsystems, relying on simplistic single-pass pipelines, fall short on complex,\nmulti-hop queries requiring multi-step reasoning and evidence aggregation. To\naddress this gap, we introduce FARSIQA, a novel, end-to-end system for Faithful\nAdvanced Question Answering in the Persian Islamic domain. FARSIQA is built\nupon our innovative FAIR-RAG architecture: a Faithful, Adaptive, Iterative\nRefinement framework for RAG. FAIR-RAG employs a dynamic, self-correcting\nprocess: it adaptively decomposes complex queries, assesses evidence\nsufficiency, and enters an iterative loop to generate sub-queries,\nprogressively filling information gaps. Operating on a curated knowledge base\nof over one million authoritative Islamic documents, FARSIQA demonstrates\nsuperior performance. Rigorous evaluation on the challenging IslamicPCQA\nbenchmark shows state-of-the-art performance: the system achieves a remarkable\n97.0% in Negative Rejection - a 40-point improvement over baselines - and a\nhigh Answer Correctness score of 74.3%. Our work establishes a new standard for\nPersian Islamic QA and validates that our iterative, adaptive architecture is\ncrucial for building faithful, reliable AI systems in sensitive domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of Large Language Models (LLMs) has revolutionized Natural\nLanguage Processing, yet their application in high-stakes, specialized domains\nlike religious question answering is hindered by challenges like hallucination\nand unfaithfulness to authoritative sources. This issue is particularly\ncritical for the Persian-speaking Muslim community, where accuracy and\ntrustworthiness are paramount. Existing Retrieval-Augmented Generation (RAG)\nsystems, relying on simplistic single-pass pipelines, fall short on complex,\nmulti-hop queries requiring multi-step reasoning and evidence aggregation. To\naddress this gap, we introduce FARSIQA, a novel, end-to-end system for Faithful\nAdvanced Question Answering in the Persian Islamic domain. FARSIQA is built\nupon our innovative FAIR-RAG architecture: a Faithful, Adaptive, Iterative\nRefinement framework for RAG. FAIR-RAG employs a dynamic, self-correcting\nprocess: it adaptively decomposes complex queries, assesses evidence\nsufficiency, and enters an iterative loop to generate sub-queries,\nprogressively filling information gaps. Operating on a curated knowledge base\nof over one million authoritative Islamic documents, FARSIQA demonstrates\nsuperior performance. Rigorous evaluation on the challenging IslamicPCQA\nbenchmark shows state-of-the-art performance: the system achieves a remarkable\n97.0% in Negative Rejection - a 40-point improvement over baselines - and a\nhigh Answer Correctness score of 74.3%. Our work establishes a new standard for\nPersian Islamic QA and validates that our iterative, adaptive architecture is\ncrucial for building faithful, reliable AI systems in sensitive domains."
                },
                "authors": [
                    {
                        "name": "Mohammad Aghajani Asl"
                    },
                    {
                        "name": "Behrooz Minaei Bidgoli"
                    }
                ],
                "author_detail": {
                    "name": "Behrooz Minaei Bidgoli"
                },
                "author": "Behrooz Minaei Bidgoli",
                "arxiv_comment": "37 pages, 5 figures, 10 tables. Keywords: Retrieval-Augmented\n  Generation (RAG), Question Answering (QA), Islamic Knowledge Base, Faithful\n  AI, Persian NLP, Multi-hop Reasoning, Large Language Models (LLMs)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25621v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25621v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50, 68T05, 68T30",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; H.3.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25612v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25612v1",
                "updated": "2025-10-29T15:17:31Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    15,
                    17,
                    31,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T15:17:31Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    15,
                    17,
                    31,
                    2,
                    302,
                    0
                ],
                "title": "Counterfactual-based Agent Influence Ranker for Agentic AI Workflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Counterfactual-based Agent Influence Ranker for Agentic AI Workflows"
                },
                "summary": "An Agentic AI Workflow (AAW), also known as an LLM-based multi-agent system,\nis an autonomous system that assembles several LLM-based agents to work\ncollaboratively towards a shared goal. The high autonomy, widespread adoption,\nand growing interest in such AAWs highlight the need for a deeper understanding\nof their operations, from both quality and security aspects. To this day, there\nare no existing methods to assess the influence of each agent on the AAW's\nfinal output. Adopting techniques from related fields is not feasible since\nexisting methods perform only static structural analysis, which is unsuitable\nfor inference time execution. We present Counterfactual-based Agent Influence\nRanker (CAIR) - the first method for assessing the influence level of each\nagent on the AAW's output and determining which agents are the most\ninfluential. By performing counterfactual analysis, CAIR provides a\ntask-agnostic analysis that can be used both offline and at inference time. We\nevaluate CAIR using an AAWs dataset of our creation, containing 30 different\nuse cases with 230 different functionalities. Our evaluation showed that CAIR\nproduces consistent rankings, outperforms baseline methods, and can easily\nenhance the effectiveness and relevancy of downstream tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Agentic AI Workflow (AAW), also known as an LLM-based multi-agent system,\nis an autonomous system that assembles several LLM-based agents to work\ncollaboratively towards a shared goal. The high autonomy, widespread adoption,\nand growing interest in such AAWs highlight the need for a deeper understanding\nof their operations, from both quality and security aspects. To this day, there\nare no existing methods to assess the influence of each agent on the AAW's\nfinal output. Adopting techniques from related fields is not feasible since\nexisting methods perform only static structural analysis, which is unsuitable\nfor inference time execution. We present Counterfactual-based Agent Influence\nRanker (CAIR) - the first method for assessing the influence level of each\nagent on the AAW's output and determining which agents are the most\ninfluential. By performing counterfactual analysis, CAIR provides a\ntask-agnostic analysis that can be used both offline and at inference time. We\nevaluate CAIR using an AAWs dataset of our creation, containing 30 different\nuse cases with 230 different functionalities. Our evaluation showed that CAIR\nproduces consistent rankings, outperforms baseline methods, and can easily\nenhance the effectiveness and relevancy of downstream tasks."
                },
                "authors": [
                    {
                        "name": "Amit Giloni"
                    },
                    {
                        "name": "Chiara Picardi"
                    },
                    {
                        "name": "Roy Betser"
                    },
                    {
                        "name": "Shamik Bose"
                    },
                    {
                        "name": "Aishvariya Priya Rathina Sabapathy"
                    },
                    {
                        "name": "Roman Vainshtein"
                    }
                ],
                "author_detail": {
                    "name": "Roman Vainshtein"
                },
                "author": "Roman Vainshtein",
                "arxiv_comment": "Accepted to EMNLP 2025, 27 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25612v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25612v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.21678v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.21678v2",
                "updated": "2025-10-29T15:15:46Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    15,
                    15,
                    46,
                    2,
                    302,
                    0
                ],
                "published": "2025-07-29T10:45:24Z",
                "published_parsed": [
                    2025,
                    7,
                    29,
                    10,
                    45,
                    24,
                    1,
                    210,
                    0
                ],
                "title": "Predicting Abandonment of Open Source Software Projects with An\n  Integrated Feature Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predicting Abandonment of Open Source Software Projects with An\n  Integrated Feature Framework"
                },
                "summary": "Open Source Software (OSS) is a cornerstone of contemporary software\ndevelopment, yet the increasing prevalence of OSS project abandonment threatens\nglobal software supply chains. Although previous research has explored\nabandonment prediction methods, these methods often demonstrate unsatisfactory\npredictive performance, further plagued by imprecise abandonment\ndiscrimination, limited interpretability, and a lack of large, generalizable\ndatasets. In this work, we address these challenges by reliably detecting OSS\nproject abandonment through a dual approach: explicit archival status and\nrigorous semantic analysis of project documentation or description. Leveraging\na precise and scalable labeling pipeline, we curate a comprehensive\nlongitudinal dataset of 115,466 GitHub repositories, encompassing 57,733\nconfirmed abandonment repositories, enriched with detailed, timeline-based\nbehavioral features. Building on this foundation, we introduce an integrated,\nmulti-perspective feature framework for abandonment prediction, capturing\nuser-centric, maintainer-centric, and project evolution features. Survival\nanalysis using an AFT model yields a high C-index of 0.846, substantially\noutperforming models confined to surface features. Further, feature ablation\nand SHAP analyses confirm both the predictive power and interpretability of our\napproach. We further demonstrate practical deployment of a GBSA classifier for\npackage risk in openEuler. By unifying precise labeling, multi-perspective\nfeatures, and interpretable modeling, our work provides reproducible, scalable,\nand practitioner-oriented support for understanding and managing abandonment\nrisk in large OSS ecosystems. Our tool not only predicts abandonment but also\nenhances program comprehension by providing actionable insights into the health\nand sustainability of OSS projects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open Source Software (OSS) is a cornerstone of contemporary software\ndevelopment, yet the increasing prevalence of OSS project abandonment threatens\nglobal software supply chains. Although previous research has explored\nabandonment prediction methods, these methods often demonstrate unsatisfactory\npredictive performance, further plagued by imprecise abandonment\ndiscrimination, limited interpretability, and a lack of large, generalizable\ndatasets. In this work, we address these challenges by reliably detecting OSS\nproject abandonment through a dual approach: explicit archival status and\nrigorous semantic analysis of project documentation or description. Leveraging\na precise and scalable labeling pipeline, we curate a comprehensive\nlongitudinal dataset of 115,466 GitHub repositories, encompassing 57,733\nconfirmed abandonment repositories, enriched with detailed, timeline-based\nbehavioral features. Building on this foundation, we introduce an integrated,\nmulti-perspective feature framework for abandonment prediction, capturing\nuser-centric, maintainer-centric, and project evolution features. Survival\nanalysis using an AFT model yields a high C-index of 0.846, substantially\noutperforming models confined to surface features. Further, feature ablation\nand SHAP analyses confirm both the predictive power and interpretability of our\napproach. We further demonstrate practical deployment of a GBSA classifier for\npackage risk in openEuler. By unifying precise labeling, multi-perspective\nfeatures, and interpretable modeling, our work provides reproducible, scalable,\nand practitioner-oriented support for understanding and managing abandonment\nrisk in large OSS ecosystems. Our tool not only predicts abandonment but also\nenhances program comprehension by providing actionable insights into the health\nand sustainability of OSS projects."
                },
                "authors": [
                    {
                        "name": "Yiming Xu"
                    },
                    {
                        "name": "Runzhi He"
                    },
                    {
                        "name": "Hengzhi Ye"
                    },
                    {
                        "name": "Minghui Zhou"
                    },
                    {
                        "name": "Huaimin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huaimin Wang"
                },
                "author": "Huaimin Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.21678v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.21678v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25602v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25602v1",
                "updated": "2025-10-29T15:11:53Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    15,
                    11,
                    53,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T15:11:53Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    15,
                    11,
                    53,
                    2,
                    302,
                    0
                ],
                "title": "INT v.s. FP: A Comprehensive Study of Fine-Grained Low-bit Quantization\n  Formats",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "INT v.s. FP: A Comprehensive Study of Fine-Grained Low-bit Quantization\n  Formats"
                },
                "summary": "Modern AI hardware, such as Nvidia's Blackwell architecture, is increasingly\nembracing low-precision floating-point (FP) formats to handle the pervasive\nactivation outliers in Large Language Models (LLMs). Despite this industry\ntrend, a unified comparison of FP and integer (INT) quantization across varying\ngranularities has been missing, leaving algorithm and hardware co-design\nwithout clear guidance. This paper fills that gap by systematically\ninvestigating the trade-offs between FP and INT formats. We reveal a critical\nperformance crossover: while FP excels in coarse-grained quantization, the\ncomparison at fine-grained (block-wise) levels is more nuanced. Our\ncomprehensive comparison demonstrates that for popular 8-bit fine-grained\nformats (e.g., MX with block size 32), MXINT8 is superior to its FP counterpart\nin both algorithmic accuracy and hardware efficiency. However, for 4-bit\nformats, FP (e.g., MXFP4, NVFP4) often holds an accuracy advantage , though we\nshow that NVINT4 can surpass NVFP4 when outlier-mitigation techniques like\nHadamard rotation are applied. We also introduce a symmetric clipping method\nthat resolves gradient bias in fine-grained low-bit INT training, enabling\nnearly lossless performance for MXINT8 training. These findings challenge the\ncurrent hardware trajectory, demonstrating that a one-size-fits-all FP approach\nis suboptimal and advocating that fine-grained INT formats, particularly\nMXINT8, offer a better balance of accuracy, power, and efficiency for future AI\naccelerators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern AI hardware, such as Nvidia's Blackwell architecture, is increasingly\nembracing low-precision floating-point (FP) formats to handle the pervasive\nactivation outliers in Large Language Models (LLMs). Despite this industry\ntrend, a unified comparison of FP and integer (INT) quantization across varying\ngranularities has been missing, leaving algorithm and hardware co-design\nwithout clear guidance. This paper fills that gap by systematically\ninvestigating the trade-offs between FP and INT formats. We reveal a critical\nperformance crossover: while FP excels in coarse-grained quantization, the\ncomparison at fine-grained (block-wise) levels is more nuanced. Our\ncomprehensive comparison demonstrates that for popular 8-bit fine-grained\nformats (e.g., MX with block size 32), MXINT8 is superior to its FP counterpart\nin both algorithmic accuracy and hardware efficiency. However, for 4-bit\nformats, FP (e.g., MXFP4, NVFP4) often holds an accuracy advantage , though we\nshow that NVINT4 can surpass NVFP4 when outlier-mitigation techniques like\nHadamard rotation are applied. We also introduce a symmetric clipping method\nthat resolves gradient bias in fine-grained low-bit INT training, enabling\nnearly lossless performance for MXINT8 training. These findings challenge the\ncurrent hardware trajectory, demonstrating that a one-size-fits-all FP approach\nis suboptimal and advocating that fine-grained INT formats, particularly\nMXINT8, offer a better balance of accuracy, power, and efficiency for future AI\naccelerators."
                },
                "authors": [
                    {
                        "name": "Mengzhao Chen"
                    },
                    {
                        "name": "Meng Wu"
                    },
                    {
                        "name": "Hui Jin"
                    },
                    {
                        "name": "Zhihang Yuan"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Chaoyi Zhang"
                    },
                    {
                        "name": "Yunshui Li"
                    },
                    {
                        "name": "Jie Huang"
                    },
                    {
                        "name": "Jin Ma"
                    },
                    {
                        "name": "Zeyue Xue"
                    },
                    {
                        "name": "Zhiheng Liu"
                    },
                    {
                        "name": "Xingyan Bin"
                    },
                    {
                        "name": "Ping Luo"
                    }
                ],
                "author_detail": {
                    "name": "Ping Luo"
                },
                "author": "Ping Luo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25602v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25602v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25595v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25595v1",
                "updated": "2025-10-29T15:03:53Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    15,
                    3,
                    53,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T15:03:53Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    15,
                    3,
                    53,
                    2,
                    302,
                    0
                ],
                "title": "Communication and Verification in LLM Agents towards Collaboration under\n  Information Asymmetry",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Communication and Verification in LLM Agents towards Collaboration under\n  Information Asymmetry"
                },
                "summary": "While Large Language Model (LLM) agents are often approached from the angle\nof action planning/generation to accomplish a goal (e.g., given by language\ndescriptions), their abilities to collaborate with each other to achieve a\njoint goal are not well explored. To address this limitation, this paper\nstudies LLM agents in task collaboration, particularly under the condition of\ninformation asymmetry, where agents have disparities in their knowledge and\nskills and need to work together to complete a shared task. We extend Einstein\nPuzzles, a classical symbolic puzzle, to a table-top game. In this game, two\nLLM agents must reason, communicate, and act to satisfy spatial and relational\nconstraints required to solve the puzzle. We apply a fine-tuning-plus-verifier\nframework in which LLM agents are equipped with various communication\nstrategies and verification signals from the environment. Empirical results\nhighlight the critical importance of aligned communication, especially when\nagents possess both information-seeking and -providing capabilities.\nInterestingly, agents without communication can still achieve high task\nperformance; however, further analysis reveals a lack of true rule\nunderstanding and lower trust from human evaluators. Instead, by integrating an\nenvironment-based verifier, we enhance agents' ability to comprehend task rules\nand complete tasks, promoting both safer and more interpretable collaboration\nin AI systems. https://github.com/Roihn/EinsteinPuzzles",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Large Language Model (LLM) agents are often approached from the angle\nof action planning/generation to accomplish a goal (e.g., given by language\ndescriptions), their abilities to collaborate with each other to achieve a\njoint goal are not well explored. To address this limitation, this paper\nstudies LLM agents in task collaboration, particularly under the condition of\ninformation asymmetry, where agents have disparities in their knowledge and\nskills and need to work together to complete a shared task. We extend Einstein\nPuzzles, a classical symbolic puzzle, to a table-top game. In this game, two\nLLM agents must reason, communicate, and act to satisfy spatial and relational\nconstraints required to solve the puzzle. We apply a fine-tuning-plus-verifier\nframework in which LLM agents are equipped with various communication\nstrategies and verification signals from the environment. Empirical results\nhighlight the critical importance of aligned communication, especially when\nagents possess both information-seeking and -providing capabilities.\nInterestingly, agents without communication can still achieve high task\nperformance; however, further analysis reveals a lack of true rule\nunderstanding and lower trust from human evaluators. Instead, by integrating an\nenvironment-based verifier, we enhance agents' ability to comprehend task rules\nand complete tasks, promoting both safer and more interpretable collaboration\nin AI systems. https://github.com/Roihn/EinsteinPuzzles"
                },
                "authors": [
                    {
                        "name": "Run Peng"
                    },
                    {
                        "name": "Ziqiao Ma"
                    },
                    {
                        "name": "Amy Pang"
                    },
                    {
                        "name": "Sikai Li"
                    },
                    {
                        "name": "Zhang Xi-Jia"
                    },
                    {
                        "name": "Yingzhuo Yu"
                    },
                    {
                        "name": "Cristian-Paul Bara"
                    },
                    {
                        "name": "Joyce Chai"
                    }
                ],
                "author_detail": {
                    "name": "Joyce Chai"
                },
                "author": "Joyce Chai",
                "arxiv_comment": "Workshop on Multi-Agent System @ ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25595v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25595v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13738v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13738v2",
                "updated": "2025-10-29T15:00:42Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    15,
                    0,
                    42,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-15T16:45:59Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    16,
                    45,
                    59,
                    2,
                    288,
                    0
                ],
                "title": "HyMiRec: A Hybrid Multi-interest Learning Framework for LLM-based\n  Sequential Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HyMiRec: A Hybrid Multi-interest Learning Framework for LLM-based\n  Sequential Recommendation"
                },
                "summary": "Large language models (LLMs) have recently demonstrated strong potential for\nsequential recommendation. However, current LLM-based approaches face critical\nlimitations in modeling users' long-term and diverse interests. First, due to\ninference latency and feature fetching bandwidth constraints, existing methods\ntypically truncate user behavior sequences to include only the most recent\ninteractions, resulting in the loss of valuable long-range preference signals.\nSecond, most current methods rely on next-item prediction with a single\npredicted embedding, overlooking the multifaceted nature of user interests and\nlimiting recommendation diversity. To address these challenges, we propose\nHyMiRec, a hybrid multi-interest sequential recommendation framework, which\nleverages a lightweight recommender to extracts coarse interest embeddings from\nlong user sequences and an LLM-based recommender to captures refined interest\nembeddings. To alleviate the overhead of fetching features, we introduce a\nresidual codebook based on cosine similarity, enabling efficient compression\nand reuse of user history embeddings. To model the diverse preferences of\nusers, we design a disentangled multi-interest learning module, which leverages\nmultiple interest queries to learn disentangles multiple interest signals\nadaptively, allowing the model to capture different facets of user intent.\nExtensive experiments are conducted on both benchmark datasets and a collected\nindustrial dataset, demonstrating our effectiveness over existing\nstate-of-the-art methods. Furthermore, online A/B testing shows that HyMiRec\nbrings consistent improvements in real-world recommendation systems. Code is\navailable at https://github.com/FireRedTeam/FireRedSeqRec.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have recently demonstrated strong potential for\nsequential recommendation. However, current LLM-based approaches face critical\nlimitations in modeling users' long-term and diverse interests. First, due to\ninference latency and feature fetching bandwidth constraints, existing methods\ntypically truncate user behavior sequences to include only the most recent\ninteractions, resulting in the loss of valuable long-range preference signals.\nSecond, most current methods rely on next-item prediction with a single\npredicted embedding, overlooking the multifaceted nature of user interests and\nlimiting recommendation diversity. To address these challenges, we propose\nHyMiRec, a hybrid multi-interest sequential recommendation framework, which\nleverages a lightweight recommender to extracts coarse interest embeddings from\nlong user sequences and an LLM-based recommender to captures refined interest\nembeddings. To alleviate the overhead of fetching features, we introduce a\nresidual codebook based on cosine similarity, enabling efficient compression\nand reuse of user history embeddings. To model the diverse preferences of\nusers, we design a disentangled multi-interest learning module, which leverages\nmultiple interest queries to learn disentangles multiple interest signals\nadaptively, allowing the model to capture different facets of user intent.\nExtensive experiments are conducted on both benchmark datasets and a collected\nindustrial dataset, demonstrating our effectiveness over existing\nstate-of-the-art methods. Furthermore, online A/B testing shows that HyMiRec\nbrings consistent improvements in real-world recommendation systems. Code is\navailable at https://github.com/FireRedTeam/FireRedSeqRec."
                },
                "authors": [
                    {
                        "name": "Jingyi Zhou"
                    },
                    {
                        "name": "Cheng Chen"
                    },
                    {
                        "name": "Kai Zuo"
                    },
                    {
                        "name": "Manjie Xu"
                    },
                    {
                        "name": "Zhendong Fu"
                    },
                    {
                        "name": "Yibo Chen"
                    },
                    {
                        "name": "Xu Tang"
                    },
                    {
                        "name": "Yao Hu"
                    }
                ],
                "author_detail": {
                    "name": "Yao Hu"
                },
                "author": "Yao Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13738v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13738v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25588v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25588v1",
                "updated": "2025-10-29T14:54:22Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    14,
                    54,
                    22,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T14:54:22Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    14,
                    54,
                    22,
                    2,
                    302,
                    0
                ],
                "title": "Standardization of Psychiatric Diagnoses -- Role of Fine-tuned LLM\n  Consortium and OpenAI-gpt-oss Reasoning LLM Enabled Decision Support System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Standardization of Psychiatric Diagnoses -- Role of Fine-tuned LLM\n  Consortium and OpenAI-gpt-oss Reasoning LLM Enabled Decision Support System"
                },
                "summary": "The diagnosis of most mental disorders, including psychiatric evaluations,\nprimarily depends on dialogues between psychiatrists and patients. This\nsubjective process can lead to variability in diagnoses across clinicians and\npatients, resulting in inconsistencies and challenges in achieving reliable\noutcomes. To address these issues and standardize psychiatric diagnoses, we\npropose a Fine-Tuned Large Language Model (LLM) Consortium and OpenAI-gpt-oss\nReasoning LLM-enabled Decision Support System for the clinical diagnosis of\nmental disorders. Our approach leverages fine-tuned LLMs trained on\nconversational datasets involving psychiatrist-patient interactions focused on\nmental health conditions (e.g., depression). The diagnostic predictions from\nindividual models are aggregated through a consensus-based decision-making\nprocess, refined by the OpenAI-gpt-oss reasoning LLM. We propose a novel method\nfor deploying LLM agents that orchestrate communication between the LLM\nconsortium and the reasoning LLM, ensuring transparency, reliability, and\nresponsible AI across the entire diagnostic workflow. Experimental results\ndemonstrate the transformative potential of combining fine-tuned LLMs with a\nreasoning model to create a robust and highly accurate diagnostic system for\nmental health assessment. A prototype of the proposed platform, integrating\nthree fine-tuned LLMs with the OpenAI-gpt-oss reasoning LLM, was developed in\ncollaboration with the U.S. Army Medical Research Team in Norfolk, Virginia,\nUSA. To the best of our knowledge, this work represents the first application\nof a fine-tuned LLM consortium integrated with a reasoning LLM for clinical\nmental health diagnosis paving the way for next-generation AI-powered eHealth\nsystems aimed at standardizing psychiatric diagnoses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The diagnosis of most mental disorders, including psychiatric evaluations,\nprimarily depends on dialogues between psychiatrists and patients. This\nsubjective process can lead to variability in diagnoses across clinicians and\npatients, resulting in inconsistencies and challenges in achieving reliable\noutcomes. To address these issues and standardize psychiatric diagnoses, we\npropose a Fine-Tuned Large Language Model (LLM) Consortium and OpenAI-gpt-oss\nReasoning LLM-enabled Decision Support System for the clinical diagnosis of\nmental disorders. Our approach leverages fine-tuned LLMs trained on\nconversational datasets involving psychiatrist-patient interactions focused on\nmental health conditions (e.g., depression). The diagnostic predictions from\nindividual models are aggregated through a consensus-based decision-making\nprocess, refined by the OpenAI-gpt-oss reasoning LLM. We propose a novel method\nfor deploying LLM agents that orchestrate communication between the LLM\nconsortium and the reasoning LLM, ensuring transparency, reliability, and\nresponsible AI across the entire diagnostic workflow. Experimental results\ndemonstrate the transformative potential of combining fine-tuned LLMs with a\nreasoning model to create a robust and highly accurate diagnostic system for\nmental health assessment. A prototype of the proposed platform, integrating\nthree fine-tuned LLMs with the OpenAI-gpt-oss reasoning LLM, was developed in\ncollaboration with the U.S. Army Medical Research Team in Norfolk, Virginia,\nUSA. To the best of our knowledge, this work represents the first application\nof a fine-tuned LLM consortium integrated with a reasoning LLM for clinical\nmental health diagnosis paving the way for next-generation AI-powered eHealth\nsystems aimed at standardizing psychiatric diagnoses."
                },
                "authors": [
                    {
                        "name": "Eranga Bandara"
                    },
                    {
                        "name": "Ross Gore"
                    },
                    {
                        "name": "Atmaram Yarlagadda"
                    },
                    {
                        "name": "Anita H. Clayton"
                    },
                    {
                        "name": "Preston Samuel"
                    },
                    {
                        "name": "Christopher K. Rhea"
                    },
                    {
                        "name": "Sachin Shetty"
                    }
                ],
                "author_detail": {
                    "name": "Sachin Shetty"
                },
                "author": "Sachin Shetty",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25588v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25588v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.12484v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.12484v4",
                "updated": "2025-10-29T14:52:49Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    14,
                    52,
                    49,
                    2,
                    302,
                    0
                ],
                "published": "2025-06-14T12:49:51Z",
                "published_parsed": [
                    2025,
                    6,
                    14,
                    12,
                    49,
                    51,
                    5,
                    165,
                    0
                ],
                "title": "Robust LLM Unlearning with MUDMAN: Meta-Unlearning with Disruption\n  Masking And Normalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust LLM Unlearning with MUDMAN: Meta-Unlearning with Disruption\n  Masking And Normalization"
                },
                "summary": "Language models can retain dangerous knowledge and skills even after\nextensive safety fine-tuning, posing both misuse and misalignment risks. Recent\nstudies show that even specialized unlearning methods can be easily reversed.\nTo address this, we systematically evaluate many existing and novel components\nof unlearning methods and identify ones crucial for irreversible unlearning.\n  We introduce Disruption Masking, a technique in which we only allow updating\nweights, where the signs of the unlearning gradient and the retaining gradient\nare the same. This ensures all updates are non-disruptive.\n  Additionally, we identify the need for normalizing the unlearning gradients,\nand also confirm the usefulness of meta-learning. We combine these insights\ninto MUDMAN (Meta-Unlearning with Disruption Masking and Normalization) and\nvalidate its effectiveness at preventing the recovery of dangerous\ncapabilities. MUDMAN outperforms the prior TAR method by 40%, setting a new\nstate-of-the-art for robust unlearning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language models can retain dangerous knowledge and skills even after\nextensive safety fine-tuning, posing both misuse and misalignment risks. Recent\nstudies show that even specialized unlearning methods can be easily reversed.\nTo address this, we systematically evaluate many existing and novel components\nof unlearning methods and identify ones crucial for irreversible unlearning.\n  We introduce Disruption Masking, a technique in which we only allow updating\nweights, where the signs of the unlearning gradient and the retaining gradient\nare the same. This ensures all updates are non-disruptive.\n  Additionally, we identify the need for normalizing the unlearning gradients,\nand also confirm the usefulness of meta-learning. We combine these insights\ninto MUDMAN (Meta-Unlearning with Disruption Masking and Normalization) and\nvalidate its effectiveness at preventing the recovery of dangerous\ncapabilities. MUDMAN outperforms the prior TAR method by 40%, setting a new\nstate-of-the-art for robust unlearning."
                },
                "authors": [
                    {
                        "name": "Filip Sondej"
                    },
                    {
                        "name": "Yushi Yang"
                    },
                    {
                        "name": "Mikołaj Kniejski"
                    },
                    {
                        "name": "Marcel Windys"
                    }
                ],
                "author_detail": {
                    "name": "Marcel Windys"
                },
                "author": "Marcel Windys",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.12484v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.12484v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.16714v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.16714v3",
                "updated": "2025-10-29T14:48:21Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    14,
                    48,
                    21,
                    2,
                    302,
                    0
                ],
                "published": "2024-02-26T16:31:28Z",
                "published_parsed": [
                    2024,
                    2,
                    26,
                    16,
                    31,
                    28,
                    0,
                    57,
                    0
                ],
                "title": "Quantum Transformer: Accelerating model inference via quantum linear\n  algebra",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum Transformer: Accelerating model inference via quantum linear\n  algebra"
                },
                "summary": "Powerful generative artificial intelligence from large language models (LLMs)\nharnesses extensive computational resources for inference. In this work, we\ninvestigate the transformer architecture, a key component of these models,\nunder the lens of fault-tolerant quantum computing. We develop quantum\nsubroutines to construct the building blocks in the transformer, including the\nself-attention, residual connection with layer normalization, and feed-forward\nnetwork. As an important subroutine, we show how to efficiently implement the\nHadamard product and element-wise functions of matrices on quantum computers.\nOur algorithm prepares an amplitude encoding of the transformer output, which\ncan be measured for prediction or use in the next layer. We find that the\nmatrix norm of the input sequence plays a dominant role in the quantum\ncomplexity. With numerical experiments on open-source LLMs, including for\nbio-informatics applications, we demonstrate the potential of a quantum speedup\nfor transformer inference in practical regimes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Powerful generative artificial intelligence from large language models (LLMs)\nharnesses extensive computational resources for inference. In this work, we\ninvestigate the transformer architecture, a key component of these models,\nunder the lens of fault-tolerant quantum computing. We develop quantum\nsubroutines to construct the building blocks in the transformer, including the\nself-attention, residual connection with layer normalization, and feed-forward\nnetwork. As an important subroutine, we show how to efficiently implement the\nHadamard product and element-wise functions of matrices on quantum computers.\nOur algorithm prepares an amplitude encoding of the transformer output, which\ncan be measured for prediction or use in the next layer. We find that the\nmatrix norm of the input sequence plays a dominant role in the quantum\ncomplexity. With numerical experiments on open-source LLMs, including for\nbio-informatics applications, we demonstrate the potential of a quantum speedup\nfor transformer inference in practical regimes."
                },
                "authors": [
                    {
                        "name": "Naixu Guo"
                    },
                    {
                        "name": "Zhan Yu"
                    },
                    {
                        "name": "Matthew Choi"
                    },
                    {
                        "name": "Yizhan Han"
                    },
                    {
                        "name": "Aman Agrawal"
                    },
                    {
                        "name": "Kouhei Nakaji"
                    },
                    {
                        "name": "Alán Aspuru-Guzik"
                    },
                    {
                        "name": "Patrick Rebentrost"
                    }
                ],
                "author_detail": {
                    "name": "Patrick Rebentrost"
                },
                "author": "Patrick Rebentrost",
                "arxiv_comment": "45 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.16714v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.16714v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10940v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10940v3",
                "updated": "2025-10-29T14:42:46Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    14,
                    42,
                    46,
                    2,
                    302,
                    0
                ],
                "published": "2025-05-16T07:26:41Z",
                "published_parsed": [
                    2025,
                    5,
                    16,
                    7,
                    26,
                    41,
                    4,
                    136,
                    0
                ],
                "title": "Who You Are Matters: Bridging Topics and Social Roles via LLM-Enhanced\n  Logical Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Who You Are Matters: Bridging Topics and Social Roles via LLM-Enhanced\n  Logical Recommendation"
                },
                "summary": "Recommender systems filter contents/items valuable to users by inferring\npreferences from user features and historical behaviors. Mainstream approaches\nfollow the learning-to-rank paradigm, which focus on discovering and modeling\nitem topics (e.g., categories), and capturing user preferences on these topics\nbased on historical interactions. However, this paradigm often neglects the\nmodeling of user characteristics and their social roles, which are logical\nconfounders influencing the correlated interest and user preference transition.\nTo bridge this gap, we introduce the user role identification task and the\nbehavioral logic modeling task that aim to explicitly model user roles and\nlearn the logical relations between item topics and user social roles. We show\nthat it is possible to explicitly solve these tasks through an efficient\nintegration framework of Large Language Model (LLM) and recommendation systems,\nfor which we propose TagCF. On the one hand, TagCF exploits the (Multi-modal)\nLLM's world knowledge and logic inference ability to extract realistic\ntag-based virtual logic graphs that reveal dynamic and expressive knowledge of\nusers, refining our understanding of user behaviors. On the other hand, TagCF\npresents empirically effective integration modules that take advantage of the\nextracted tag-logic information, augmenting the recommendation performance. We\nconduct both online experiments and offline experiments with industrial and\npublic datasets as verification of TagCF's effectiveness, and we empirically\nshow that the user role modeling strategy is potentially a better choice than\nthe modeling of item topics. Additionally, we provide evidence that the\nextracted logic graphs are empirically a general and transferable knowledge\nthat can benefit a wide range of recommendation tasks. Our code is available in\nhttps://github.com/Code2Q/TagCF.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recommender systems filter contents/items valuable to users by inferring\npreferences from user features and historical behaviors. Mainstream approaches\nfollow the learning-to-rank paradigm, which focus on discovering and modeling\nitem topics (e.g., categories), and capturing user preferences on these topics\nbased on historical interactions. However, this paradigm often neglects the\nmodeling of user characteristics and their social roles, which are logical\nconfounders influencing the correlated interest and user preference transition.\nTo bridge this gap, we introduce the user role identification task and the\nbehavioral logic modeling task that aim to explicitly model user roles and\nlearn the logical relations between item topics and user social roles. We show\nthat it is possible to explicitly solve these tasks through an efficient\nintegration framework of Large Language Model (LLM) and recommendation systems,\nfor which we propose TagCF. On the one hand, TagCF exploits the (Multi-modal)\nLLM's world knowledge and logic inference ability to extract realistic\ntag-based virtual logic graphs that reveal dynamic and expressive knowledge of\nusers, refining our understanding of user behaviors. On the other hand, TagCF\npresents empirically effective integration modules that take advantage of the\nextracted tag-logic information, augmenting the recommendation performance. We\nconduct both online experiments and offline experiments with industrial and\npublic datasets as verification of TagCF's effectiveness, and we empirically\nshow that the user role modeling strategy is potentially a better choice than\nthe modeling of item topics. Additionally, we provide evidence that the\nextracted logic graphs are empirically a general and transferable knowledge\nthat can benefit a wide range of recommendation tasks. Our code is available in\nhttps://github.com/Code2Q/TagCF."
                },
                "authors": [
                    {
                        "name": "Qing Yu"
                    },
                    {
                        "name": "Xiaobei Wang"
                    },
                    {
                        "name": "Shuchang Liu"
                    },
                    {
                        "name": "Yandong Bai"
                    },
                    {
                        "name": "Xiaoyu Yang"
                    },
                    {
                        "name": "Xueliang Wang"
                    },
                    {
                        "name": "Chang Meng"
                    },
                    {
                        "name": "Shanshan Wu"
                    },
                    {
                        "name": "Hailan Yang"
                    },
                    {
                        "name": "Huihui Xiao"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Xiaoqiang Feng"
                    },
                    {
                        "name": "Lantao Hu"
                    },
                    {
                        "name": "Han Li"
                    },
                    {
                        "name": "Kun Gai"
                    },
                    {
                        "name": "Lixin Zou"
                    }
                ],
                "author_detail": {
                    "name": "Lixin Zou"
                },
                "author": "Lixin Zou",
                "arxiv_comment": "to be published in NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10940v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10940v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22117v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22117v2",
                "updated": "2025-10-29T14:33:07Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    14,
                    33,
                    7,
                    2,
                    302,
                    0
                ],
                "published": "2025-09-26T09:40:51Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    9,
                    40,
                    51,
                    4,
                    269,
                    0
                ],
                "title": "The AI_INFN Platform: Artificial Intelligence Development in the Cloud",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The AI_INFN Platform: Artificial Intelligence Development in the Cloud"
                },
                "summary": "Machine Learning (ML) is profoundly reshaping the way researchers create,\nimplement, and operate data-intensive software. Its adoption, however,\nintroduces notable challenges for computing infrastructures, particularly when\nit comes to coordinating access to hardware accelerators across development,\ntesting, and production environments. The INFN initiative AI_INFN (Artificial\nIntelligence at INFN) seeks to promote the use of ML methods across various\nINFN research scenarios by offering comprehensive technical support, including\naccess to AI-focused computational resources. Leveraging the INFN Cloud\necosystem and cloud-native technologies, the project emphasizes efficient\nsharing of accelerator hardware while maintaining the breadth of the\nInstitute's research activities. This contribution describes the deployment and\ncommissioning of a Kubernetes-based platform designed to simplify GPU-powered\ndata analysis workflows and enable their scalable execution on heterogeneous\ndistributed resources. By integrating offloading mechanisms through Virtual\nKubelet and the InterLink API, the platform allows workflows to span multiple\nresource providers, from Worldwide LHC Computing Grid sites to high-performance\ncomputing centers like CINECA Leonardo. We will present preliminary benchmarks,\nfunctional tests, and case studies, demonstrating both performance and\nintegration outcomes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine Learning (ML) is profoundly reshaping the way researchers create,\nimplement, and operate data-intensive software. Its adoption, however,\nintroduces notable challenges for computing infrastructures, particularly when\nit comes to coordinating access to hardware accelerators across development,\ntesting, and production environments. The INFN initiative AI_INFN (Artificial\nIntelligence at INFN) seeks to promote the use of ML methods across various\nINFN research scenarios by offering comprehensive technical support, including\naccess to AI-focused computational resources. Leveraging the INFN Cloud\necosystem and cloud-native technologies, the project emphasizes efficient\nsharing of accelerator hardware while maintaining the breadth of the\nInstitute's research activities. This contribution describes the deployment and\ncommissioning of a Kubernetes-based platform designed to simplify GPU-powered\ndata analysis workflows and enable their scalable execution on heterogeneous\ndistributed resources. By integrating offloading mechanisms through Virtual\nKubelet and the InterLink API, the platform allows workflows to span multiple\nresource providers, from Worldwide LHC Computing Grid sites to high-performance\ncomputing centers like CINECA Leonardo. We will present preliminary benchmarks,\nfunctional tests, and case studies, demonstrating both performance and\nintegration outcomes."
                },
                "authors": [
                    {
                        "name": "Lucio Anderlini"
                    },
                    {
                        "name": "Giulio Bianchini"
                    },
                    {
                        "name": "Diego Ciangottini"
                    },
                    {
                        "name": "Stefano Dal Pra"
                    },
                    {
                        "name": "Diego Michelotto"
                    },
                    {
                        "name": "Rosa Petrini"
                    },
                    {
                        "name": "Daniele Spiga"
                    }
                ],
                "author_detail": {
                    "name": "Daniele Spiga"
                },
                "author": "Daniele Spiga",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22117v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22117v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15030v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15030v2",
                "updated": "2025-10-29T14:31:38Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    14,
                    31,
                    38,
                    2,
                    302,
                    0
                ],
                "published": "2025-08-20T19:49:06Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    19,
                    49,
                    6,
                    2,
                    232,
                    0
                ],
                "title": "Collab-REC: An LLM-based Agentic Framework for Balancing Recommendations\n  in Tourism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collab-REC: An LLM-based Agentic Framework for Balancing Recommendations\n  in Tourism"
                },
                "summary": "We propose Collab-REC, a multi-agent framework designed to counteract\npopularity bias and enhance diversity in tourism recommendations. In our\nsetting, three LLM-based agents -- Personalization, Popularity, and\nSustainability generate city suggestions from complementary perspectives. A\nnon-LLM moderator then merges and refines these proposals via multi-round\nnegotiation, ensuring each agent's viewpoint is incorporated while penalizing\nspurious or repeated responses. Experiments on European city queries show that\nCollab-REC improves diversity and overall relevance compared to a single-agent\nbaseline, surfacing lesser-visited locales that often remain overlooked. This\nbalanced, context-aware approach addresses over-tourism and better aligns with\nconstraints provided by the user, highlighting the promise of multi-stakeholder\ncollaboration in LLM-driven recommender systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose Collab-REC, a multi-agent framework designed to counteract\npopularity bias and enhance diversity in tourism recommendations. In our\nsetting, three LLM-based agents -- Personalization, Popularity, and\nSustainability generate city suggestions from complementary perspectives. A\nnon-LLM moderator then merges and refines these proposals via multi-round\nnegotiation, ensuring each agent's viewpoint is incorporated while penalizing\nspurious or repeated responses. Experiments on European city queries show that\nCollab-REC improves diversity and overall relevance compared to a single-agent\nbaseline, surfacing lesser-visited locales that often remain overlooked. This\nbalanced, context-aware approach addresses over-tourism and better aligns with\nconstraints provided by the user, highlighting the promise of multi-stakeholder\ncollaboration in LLM-driven recommender systems."
                },
                "authors": [
                    {
                        "name": "Ashmi Banerjee"
                    },
                    {
                        "name": "Fitri Nur Aisyah"
                    },
                    {
                        "name": "Adithi Satish"
                    },
                    {
                        "name": "Wolfgang Wörndl"
                    },
                    {
                        "name": "Yashar Deldjoo"
                    }
                ],
                "author_detail": {
                    "name": "Yashar Deldjoo"
                },
                "author": "Yashar Deldjoo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15030v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15030v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.01308v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.01308v2",
                "updated": "2025-10-29T14:09:33Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    14,
                    9,
                    33,
                    2,
                    302,
                    0
                ],
                "published": "2025-09-01T09:47:35Z",
                "published_parsed": [
                    2025,
                    9,
                    1,
                    9,
                    47,
                    35,
                    0,
                    244,
                    0
                ],
                "title": "GradeSQL: Test-Time Inference with Outcome Reward Models for Text-to-SQL\n  Generation from Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GradeSQL: Test-Time Inference with Outcome Reward Models for Text-to-SQL\n  Generation from Large Language Models"
                },
                "summary": "Text-to-SQL, the task of translating natural language questions into SQL\nqueries, has significantly advanced with the introduction of Large Language\nModels (LLMs), broadening database accessibility for a wide range of users.\nDespite substantial progress in generating valid SQL, current LLMs still\nstruggle with complex queries. To address this limitation, test-time strategies\nsuch as Best-of-N (BoN) and Majority Voting (Maj) are often employed, based on\nthe assumption that LLMs can produce correct answers after multiple attempts.\nHowever, these methods rely on surface-level heuristics, selecting the\nsyntactically correct query through execution-based BoN (ex-BoN) or the most\nfrequently generated one through Majority Voting. Recently, Outcome Reward\nModels (ORMs), which assign utility scores to generated outputs based on\nsemantic correctness, have emerged as a promising reinforcement learning\napproach for improving model alignment. We argue that ORMs could serve as an\neffective new test-time heuristic, although their application in this context\nremains largely underexplored.\n  In this work, we propose a unified framework for training ORMs tailored to\nthe Text-to-SQL task and assess their effectiveness as a test-time heuristic\nwithin the BoN strategy. We benchmark ORMs against ex-BoN and Maj across the\nBIRD and Spider datasets, fine-tuning diverse open-source LLMs from the Qwen2,\nGranite3, and Llama3 families. Results show that ORMs outperform ex-BoN and\nMaj, achieving execution accuracy gains of +4.33% (BIRD) and +2.10% (Spider)\nover ex-BoN, and +2.91% (BIRD) and +0.93% (Spider) over Maj. We further\ndemonstrate that finetuning models already aligned with SQL generation, such as\nOmniSQL, yields superior ORM performance. Additionally, we observe that ORMs\nachieve competitive results on simple queries and benefit more from an\nincreased number of candidates compared to ex-BoN and Maj.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-SQL, the task of translating natural language questions into SQL\nqueries, has significantly advanced with the introduction of Large Language\nModels (LLMs), broadening database accessibility for a wide range of users.\nDespite substantial progress in generating valid SQL, current LLMs still\nstruggle with complex queries. To address this limitation, test-time strategies\nsuch as Best-of-N (BoN) and Majority Voting (Maj) are often employed, based on\nthe assumption that LLMs can produce correct answers after multiple attempts.\nHowever, these methods rely on surface-level heuristics, selecting the\nsyntactically correct query through execution-based BoN (ex-BoN) or the most\nfrequently generated one through Majority Voting. Recently, Outcome Reward\nModels (ORMs), which assign utility scores to generated outputs based on\nsemantic correctness, have emerged as a promising reinforcement learning\napproach for improving model alignment. We argue that ORMs could serve as an\neffective new test-time heuristic, although their application in this context\nremains largely underexplored.\n  In this work, we propose a unified framework for training ORMs tailored to\nthe Text-to-SQL task and assess their effectiveness as a test-time heuristic\nwithin the BoN strategy. We benchmark ORMs against ex-BoN and Maj across the\nBIRD and Spider datasets, fine-tuning diverse open-source LLMs from the Qwen2,\nGranite3, and Llama3 families. Results show that ORMs outperform ex-BoN and\nMaj, achieving execution accuracy gains of +4.33% (BIRD) and +2.10% (Spider)\nover ex-BoN, and +2.91% (BIRD) and +0.93% (Spider) over Maj. We further\ndemonstrate that finetuning models already aligned with SQL generation, such as\nOmniSQL, yields superior ORM performance. Additionally, we observe that ORMs\nachieve competitive results on simple queries and benefit more from an\nincreased number of candidates compared to ex-BoN and Maj."
                },
                "authors": [
                    {
                        "name": "Mattia Tritto"
                    },
                    {
                        "name": "Giuseppe Farano"
                    },
                    {
                        "name": "Dario Di Palma"
                    },
                    {
                        "name": "Gaetano Rossiello"
                    },
                    {
                        "name": "Fedelucio Narducci"
                    },
                    {
                        "name": "Dharmashankar Subramanian"
                    },
                    {
                        "name": "Tommaso Di Noia"
                    }
                ],
                "author_detail": {
                    "name": "Tommaso Di Noia"
                },
                "author": "Tommaso Di Noia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.01308v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.01308v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25536v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25536v2",
                "updated": "2025-10-30T11:19:24Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    11,
                    19,
                    24,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-29T14:00:42Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    14,
                    0,
                    42,
                    2,
                    302,
                    0
                ],
                "title": "TwinVoice: A Multi-dimensional Benchmark Towards Digital Twins via LLM\n  Persona Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TwinVoice: A Multi-dimensional Benchmark Towards Digital Twins via LLM\n  Persona Simulation"
                },
                "summary": "Large Language Models (LLMs) are exhibiting emergent human-like abilities and\nare increasingly envisioned as the foundation for simulating an individual's\ncommunication style, behavioral tendencies, and personality traits. However,\ncurrent evaluations of LLM-based persona simulation remain limited: most rely\non synthetic dialogues, lack systematic frameworks, and lack analysis of the\ncapability requirement. To address these limitations, we introduce TwinVoice, a\ncomprehensive benchmark for assessing persona simulation across diverse\nreal-world contexts. TwinVoice encompasses three dimensions: Social Persona\n(public social interactions), Interpersonal Persona (private dialogues), and\nNarrative Persona (role-based expression). It further decomposes the evaluation\nof LLM performance into six fundamental capabilities, including opinion\nconsistency, memory recall, logical reasoning, lexical fidelity, persona tone,\nand syntactic style. Experimental results reveal that while advanced models\nachieve moderate accuracy in persona simulation, they still fall short of\ncapabilities such as syntactic style and memory recall. Consequently, the\naverage performance achieved by LLMs remains considerably below the human\nbaseline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are exhibiting emergent human-like abilities and\nare increasingly envisioned as the foundation for simulating an individual's\ncommunication style, behavioral tendencies, and personality traits. However,\ncurrent evaluations of LLM-based persona simulation remain limited: most rely\non synthetic dialogues, lack systematic frameworks, and lack analysis of the\ncapability requirement. To address these limitations, we introduce TwinVoice, a\ncomprehensive benchmark for assessing persona simulation across diverse\nreal-world contexts. TwinVoice encompasses three dimensions: Social Persona\n(public social interactions), Interpersonal Persona (private dialogues), and\nNarrative Persona (role-based expression). It further decomposes the evaluation\nof LLM performance into six fundamental capabilities, including opinion\nconsistency, memory recall, logical reasoning, lexical fidelity, persona tone,\nand syntactic style. Experimental results reveal that while advanced models\nachieve moderate accuracy in persona simulation, they still fall short of\ncapabilities such as syntactic style and memory recall. Consequently, the\naverage performance achieved by LLMs remains considerably below the human\nbaseline."
                },
                "authors": [
                    {
                        "name": "Bangde Du"
                    },
                    {
                        "name": "Minghao Guo"
                    },
                    {
                        "name": "Songming He"
                    },
                    {
                        "name": "Ziyi Ye"
                    },
                    {
                        "name": "Xi Zhu"
                    },
                    {
                        "name": "Weihang Su"
                    },
                    {
                        "name": "Shuqi Zhu"
                    },
                    {
                        "name": "Yujia Zhou"
                    },
                    {
                        "name": "Yongfeng Zhang"
                    },
                    {
                        "name": "Qingyao Ai"
                    },
                    {
                        "name": "Yiqun Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yiqun Liu"
                },
                "author": "Yiqun Liu",
                "arxiv_comment": "Main paper: 11 pages, 3 figures, 6 tables. Appendix: 28 pages. Bangde\n  Du and Minghao Guo contributed equally. Corresponding authors: Ziyi Ye\n  (ziyiye@fudan.edu.cn), Qingyao Ai (aiqy@tsinghua.edu.cn)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25536v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25536v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.6; I.2.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25528v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25528v1",
                "updated": "2025-10-29T13:52:44Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    13,
                    52,
                    44,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T13:52:44Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    13,
                    52,
                    44,
                    2,
                    302,
                    0
                ],
                "title": "Zero Reinforcement Learning Towards General Domains",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero Reinforcement Learning Towards General Domains"
                },
                "summary": "Zero Reinforcement Learning (Zero-RL) has proven to be an effective approach\nfor enhancing the reasoning capabilities of large language models (LLMs) by\ndirectly applying reinforcement learning with verifiable rewards on pretrained\nmodels, without the need for a supervised fine-tuning phase. However, current\nresearch on zero-RL primarily focuses on domains with easily verifiable reward\nsignals, such as mathematics, programming, and other reasoning tasks. The\nchallenge of eliciting reasoning abilities in more diverse scenarios, where\nverification is not straightforward, remains underexplored. To address this\ngap, we propose a novel zero-RL paradigm designed to improve a model's\nreasoning ability across both verifiable and non-verifiable domains. By\ncombining verifiable rewards with a generative reward model, we conduct\nmulti-task zero-RL training across both domains, facilitating the transfer of\nreasoning capabilities between them. Furthermore, to mitigate reward hacking in\nthe generative reward model, we design a smooth length penalty that encourages\nthe generation of more comprehensive thinking tokens in general domains.\nExperimental results on Qwen3-8B-Base and Qwen3-14B-Base demonstrate that our\napproach achieves superior reasoning performance, not only on tasks requiring\nextensive reasoning but also on more general tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero Reinforcement Learning (Zero-RL) has proven to be an effective approach\nfor enhancing the reasoning capabilities of large language models (LLMs) by\ndirectly applying reinforcement learning with verifiable rewards on pretrained\nmodels, without the need for a supervised fine-tuning phase. However, current\nresearch on zero-RL primarily focuses on domains with easily verifiable reward\nsignals, such as mathematics, programming, and other reasoning tasks. The\nchallenge of eliciting reasoning abilities in more diverse scenarios, where\nverification is not straightforward, remains underexplored. To address this\ngap, we propose a novel zero-RL paradigm designed to improve a model's\nreasoning ability across both verifiable and non-verifiable domains. By\ncombining verifiable rewards with a generative reward model, we conduct\nmulti-task zero-RL training across both domains, facilitating the transfer of\nreasoning capabilities between them. Furthermore, to mitigate reward hacking in\nthe generative reward model, we design a smooth length penalty that encourages\nthe generation of more comprehensive thinking tokens in general domains.\nExperimental results on Qwen3-8B-Base and Qwen3-14B-Base demonstrate that our\napproach achieves superior reasoning performance, not only on tasks requiring\nextensive reasoning but also on more general tasks."
                },
                "authors": [
                    {
                        "name": "Yuyuan Zeng"
                    },
                    {
                        "name": "Yufei Huang"
                    },
                    {
                        "name": "Can Xu"
                    },
                    {
                        "name": "Qingfeng Sun"
                    },
                    {
                        "name": "Jianfeng Yan"
                    },
                    {
                        "name": "Guanghui Xu"
                    },
                    {
                        "name": "Tao Yang"
                    },
                    {
                        "name": "Fengzong Lian"
                    }
                ],
                "author_detail": {
                    "name": "Fengzong Lian"
                },
                "author": "Fengzong Lian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25528v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25528v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11928v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11928v3",
                "updated": "2025-10-29T13:44:51Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    13,
                    44,
                    51,
                    2,
                    302,
                    0
                ],
                "published": "2025-09-15T13:45:31Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    13,
                    45,
                    31,
                    0,
                    258,
                    0
                ],
                "title": "Meta-Learning Neural Process for Implied Volatility Surfaces with\n  SABR-induced Priors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Meta-Learning Neural Process for Implied Volatility Surfaces with\n  SABR-induced Priors"
                },
                "summary": "We treat implied volatility surface (IVS) reconstruction as a learning\nproblem guided by two principles. First, we adopt a meta-learning view that\ntrains across trading days to learn a procedure that maps sparse option quotes\nto a full IVS via conditional prediction, avoiding per-day calibration at test\ntime. Second, we impose a structural prior via transfer learning: pre-train on\nSABR-generated dataset to encode geometric prior, then fine-tune on historical\nmarket dataset to align with empirical patterns. We implement both principles\nin a single attention-based Neural Process (Volatility Neural Process, VolNP)\nthat produces a complete IVS from a sparse context set in one forward pass. On\nSPX options, the VolNP outperforms SABR, SSVI, and Gaussian process. Relative\nto an ablation trained only on market data, the SABR-induced prior reduces RMSE\nby about 40% and suppresses large errors, with pronounced gains at long\nmaturities where quotes are sparse. The resulting model is fast (single pass),\nstable (no daily recalibration), and practical for deployment at scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We treat implied volatility surface (IVS) reconstruction as a learning\nproblem guided by two principles. First, we adopt a meta-learning view that\ntrains across trading days to learn a procedure that maps sparse option quotes\nto a full IVS via conditional prediction, avoiding per-day calibration at test\ntime. Second, we impose a structural prior via transfer learning: pre-train on\nSABR-generated dataset to encode geometric prior, then fine-tune on historical\nmarket dataset to align with empirical patterns. We implement both principles\nin a single attention-based Neural Process (Volatility Neural Process, VolNP)\nthat produces a complete IVS from a sparse context set in one forward pass. On\nSPX options, the VolNP outperforms SABR, SSVI, and Gaussian process. Relative\nto an ablation trained only on market data, the SABR-induced prior reduces RMSE\nby about 40% and suppresses large errors, with pronounced gains at long\nmaturities where quotes are sparse. The resulting model is fast (single pass),\nstable (no daily recalibration), and practical for deployment at scale."
                },
                "authors": [
                    {
                        "name": "Jirong Zhuang"
                    },
                    {
                        "name": "Xuan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Xuan Wu"
                },
                "author": "Xuan Wu",
                "arxiv_comment": "24 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11928v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11928v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.CP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.CP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25517v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25517v1",
                "updated": "2025-10-29T13:39:41Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    13,
                    39,
                    41,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T13:39:41Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    13,
                    39,
                    41,
                    2,
                    302,
                    0
                ],
                "title": "Predicate Renaming via Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predicate Renaming via Large Language Models"
                },
                "summary": "In this paper, we address the problem of giving names to predicates in logic\nrules using Large Language Models (LLMs). In the context of Inductive Logic\nProgramming, various rule generation methods produce rules containing unnamed\npredicates, with Predicate Invention being a key example. This hinders the\nreadability, interpretability, and reusability of the logic theory. Leveraging\nrecent advancements in LLMs development, we explore their ability to process\nnatural language and code to provide semantically meaningful suggestions for\ngiving a name to unnamed predicates. The evaluation of our approach on some\nhand-crafted logic rules indicates that LLMs hold potential for this task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we address the problem of giving names to predicates in logic\nrules using Large Language Models (LLMs). In the context of Inductive Logic\nProgramming, various rule generation methods produce rules containing unnamed\npredicates, with Predicate Invention being a key example. This hinders the\nreadability, interpretability, and reusability of the logic theory. Leveraging\nrecent advancements in LLMs development, we explore their ability to process\nnatural language and code to provide semantically meaningful suggestions for\ngiving a name to unnamed predicates. The evaluation of our approach on some\nhand-crafted logic rules indicates that LLMs hold potential for this task."
                },
                "authors": [
                    {
                        "name": "Elisabetta Gentili"
                    },
                    {
                        "name": "Tony Ribeiro"
                    },
                    {
                        "name": "Fabrizio Riguzzi"
                    },
                    {
                        "name": "Katsumi Inoue"
                    }
                ],
                "author_detail": {
                    "name": "Katsumi Inoue"
                },
                "author": "Katsumi Inoue",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25517v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25517v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.00814v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.00814v2",
                "updated": "2025-10-29T13:37:41Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    13,
                    37,
                    41,
                    2,
                    302,
                    0
                ],
                "published": "2025-07-01T14:46:16Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    14,
                    46,
                    16,
                    1,
                    182,
                    0
                ],
                "title": "Many LLMs Are More Utilitarian Than One",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many LLMs Are More Utilitarian Than One"
                },
                "summary": "Moral judgment is integral to large language models' (LLMs) social reasoning.\nAs multi-agent systems gain prominence, it becomes crucial to understand how\nLLMs function when collaborating compared to operating as individual agents. In\nhuman moral judgment, group deliberation leads to a Utilitarian Boost: a\ntendency to endorse norm violations that inflict harm but maximize benefits for\nthe greatest number of people. We study whether a similar dynamic emerges in\nmulti-agent LLM systems. We test six models on well-established sets of moral\ndilemmas across two conditions: (1) Solo, where models reason independently,\nand (2) Group, where they engage in multi-turn discussions in pairs or triads.\nIn personal dilemmas, where agents decide whether to directly harm an\nindividual for the benefit of others, all models rated moral violations as more\nacceptable when part of a group, demonstrating a Utilitarian Boost similar to\nthat observed in humans. However, the mechanism for the Boost in LLMs differed:\nWhile humans in groups become more utilitarian due to heightened sensitivity to\ndecision outcomes, LLM groups showed either reduced sensitivity to norms or\nenhanced impartiality. We report model differences in when and how strongly the\nBoost manifests. We also discuss prompt and agent compositions that enhance or\nmitigate the effect. We end with a discussion of the implications for AI\nalignment, multi-agent design, and artificial moral reasoning. Code available\nat: https://github.com/baltaci-r/MoralAgents",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Moral judgment is integral to large language models' (LLMs) social reasoning.\nAs multi-agent systems gain prominence, it becomes crucial to understand how\nLLMs function when collaborating compared to operating as individual agents. In\nhuman moral judgment, group deliberation leads to a Utilitarian Boost: a\ntendency to endorse norm violations that inflict harm but maximize benefits for\nthe greatest number of people. We study whether a similar dynamic emerges in\nmulti-agent LLM systems. We test six models on well-established sets of moral\ndilemmas across two conditions: (1) Solo, where models reason independently,\nand (2) Group, where they engage in multi-turn discussions in pairs or triads.\nIn personal dilemmas, where agents decide whether to directly harm an\nindividual for the benefit of others, all models rated moral violations as more\nacceptable when part of a group, demonstrating a Utilitarian Boost similar to\nthat observed in humans. However, the mechanism for the Boost in LLMs differed:\nWhile humans in groups become more utilitarian due to heightened sensitivity to\ndecision outcomes, LLM groups showed either reduced sensitivity to norms or\nenhanced impartiality. We report model differences in when and how strongly the\nBoost manifests. We also discuss prompt and agent compositions that enhance or\nmitigate the effect. We end with a discussion of the implications for AI\nalignment, multi-agent design, and artificial moral reasoning. Code available\nat: https://github.com/baltaci-r/MoralAgents"
                },
                "authors": [
                    {
                        "name": "Anita Keshmirian"
                    },
                    {
                        "name": "Razan Baltaji"
                    },
                    {
                        "name": "Babak Hemmatian"
                    },
                    {
                        "name": "Hadi Asghari"
                    },
                    {
                        "name": "Lav R. Varshney"
                    }
                ],
                "author_detail": {
                    "name": "Lav R. Varshney"
                },
                "author": "Lav R. Varshney",
                "arxiv_comment": "Accepted to the Conference on Neural Information Processing Systems\n  (NeurIPS 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.00814v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.00814v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.23763v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.23763v2",
                "updated": "2025-10-29T13:37:19Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    13,
                    37,
                    19,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-27T18:49:03Z",
                "published_parsed": [
                    2025,
                    10,
                    27,
                    18,
                    49,
                    3,
                    0,
                    300,
                    0
                ],
                "title": "RoboOmni: Proactive Robot Manipulation in Omni-modal Context",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RoboOmni: Proactive Robot Manipulation in Omni-modal Context"
                },
                "summary": "Recent advances in Multimodal Large Language Models (MLLMs) have driven rapid\nprogress in Vision-Language-Action (VLA) models for robotic manipulation.\nAlthough effective in many scenarios, current approaches largely rely on\nexplicit instructions, whereas in real-world interactions, humans rarely issue\ninstructions directly. Effective collaboration requires robots to infer user\nintentions proactively. In this work, we introduce cross-modal contextual\ninstructions, a new setting where intent is derived from spoken dialogue,\nenvironmental sounds, and visual cues rather than explicit commands. To address\nthis new setting, we present RoboOmni, a Perceiver-Thinker-Talker-Executor\nframework based on end-to-end omni-modal LLMs that unifies intention\nrecognition, interaction confirmation, and action execution. RoboOmni fuses\nauditory and visual signals spatiotemporally for robust intention recognition,\nwhile supporting direct speech interaction. To address the absence of training\ndata for proactive intention recognition in robotic manipulation, we build\nOmniAction, comprising 140k episodes, 5k+ speakers, 2.4k event sounds, 640\nbackgrounds, and six contextual instruction types. Experiments in simulation\nand real-world settings show that RoboOmni surpasses text- and ASR-based\nbaselines in success rate, inference speed, intention recognition, and\nproactive assistance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Multimodal Large Language Models (MLLMs) have driven rapid\nprogress in Vision-Language-Action (VLA) models for robotic manipulation.\nAlthough effective in many scenarios, current approaches largely rely on\nexplicit instructions, whereas in real-world interactions, humans rarely issue\ninstructions directly. Effective collaboration requires robots to infer user\nintentions proactively. In this work, we introduce cross-modal contextual\ninstructions, a new setting where intent is derived from spoken dialogue,\nenvironmental sounds, and visual cues rather than explicit commands. To address\nthis new setting, we present RoboOmni, a Perceiver-Thinker-Talker-Executor\nframework based on end-to-end omni-modal LLMs that unifies intention\nrecognition, interaction confirmation, and action execution. RoboOmni fuses\nauditory and visual signals spatiotemporally for robust intention recognition,\nwhile supporting direct speech interaction. To address the absence of training\ndata for proactive intention recognition in robotic manipulation, we build\nOmniAction, comprising 140k episodes, 5k+ speakers, 2.4k event sounds, 640\nbackgrounds, and six contextual instruction types. Experiments in simulation\nand real-world settings show that RoboOmni surpasses text- and ASR-based\nbaselines in success rate, inference speed, intention recognition, and\nproactive assistance."
                },
                "authors": [
                    {
                        "name": "Siyin Wang"
                    },
                    {
                        "name": "Jinlan Fu"
                    },
                    {
                        "name": "Feihong Liu"
                    },
                    {
                        "name": "Xinzhe He"
                    },
                    {
                        "name": "Huangxuan Wu"
                    },
                    {
                        "name": "Junhao Shi"
                    },
                    {
                        "name": "Kexin Huang"
                    },
                    {
                        "name": "Zhaoye Fei"
                    },
                    {
                        "name": "Jingjing Gong"
                    },
                    {
                        "name": "Zuxuan Wu"
                    },
                    {
                        "name": "Yugang Jiang"
                    },
                    {
                        "name": "See-Kiong Ng"
                    },
                    {
                        "name": "Tat-Seng Chua"
                    },
                    {
                        "name": "Xipeng Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Xipeng Qiu"
                },
                "author": "Xipeng Qiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.23763v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.23763v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25510v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25510v1",
                "updated": "2025-10-29T13:34:27Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    13,
                    34,
                    27,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T13:34:27Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    13,
                    34,
                    27,
                    2,
                    302,
                    0
                ],
                "title": "MTIR-SQL: Multi-turn Tool-Integrated Reasoning Reinforcement Learning\n  for Text-to-SQL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MTIR-SQL: Multi-turn Tool-Integrated Reasoning Reinforcement Learning\n  for Text-to-SQL"
                },
                "summary": "As large language models (LLMs) are increasingly used in Text-to-SQL tasks,\nReinforcement Learning (RL) has become a common method for improving\nperformance. Existing methods primarily rely on static execution feedback,\nwhich restricts real-time error correction. However, integrating multi-turn\ntool invocation along with dynamic feedback could significantly improve\nadaptability and robustness, ultimately enhancing model performance. To address\nthese issues, we propose MTIR-SQL, an innovative Multi-turn Tool-Integrated\nReasoning reinforcement learning framework for Text-to-SQL. Our approach\nintroduces an execution-aware multi-turn reasoning paradigm that seamlessly\nincorporates database execution feedback at each reasoning step, enabling\ncontext-sensitive query generation and progressive refinement throughout the\nreasoning process. The framework extends the GRPO algorithm to accommodate\ncomplex multi-turn interaction scenarios. Considering the training instability\ncharacteristics of MTIR and the potential for significant Deviation of model\ndistribution from the initial model, we enhance the GRPO algorithm by adding a\ntrajectory filtering mechanism and removing KL loss constraints. Experimental\nresults demonstrate that MTIR-SQL, with 4B parameters, achieves \\textbf{64.4}\\%\naccuracy in the BIRD Dev and 84.6% execution accuracy in the SPIDER Dev,\nsignificantly outperforming existing approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) are increasingly used in Text-to-SQL tasks,\nReinforcement Learning (RL) has become a common method for improving\nperformance. Existing methods primarily rely on static execution feedback,\nwhich restricts real-time error correction. However, integrating multi-turn\ntool invocation along with dynamic feedback could significantly improve\nadaptability and robustness, ultimately enhancing model performance. To address\nthese issues, we propose MTIR-SQL, an innovative Multi-turn Tool-Integrated\nReasoning reinforcement learning framework for Text-to-SQL. Our approach\nintroduces an execution-aware multi-turn reasoning paradigm that seamlessly\nincorporates database execution feedback at each reasoning step, enabling\ncontext-sensitive query generation and progressive refinement throughout the\nreasoning process. The framework extends the GRPO algorithm to accommodate\ncomplex multi-turn interaction scenarios. Considering the training instability\ncharacteristics of MTIR and the potential for significant Deviation of model\ndistribution from the initial model, we enhance the GRPO algorithm by adding a\ntrajectory filtering mechanism and removing KL loss constraints. Experimental\nresults demonstrate that MTIR-SQL, with 4B parameters, achieves \\textbf{64.4}\\%\naccuracy in the BIRD Dev and 84.6% execution accuracy in the SPIDER Dev,\nsignificantly outperforming existing approaches."
                },
                "authors": [
                    {
                        "name": "Zekun Xu"
                    },
                    {
                        "name": "Siyu Xia"
                    },
                    {
                        "name": "Chuhuai Yue"
                    },
                    {
                        "name": "Jiajun Chai"
                    },
                    {
                        "name": "Mingxue Tian"
                    },
                    {
                        "name": "Xiaohan Wang"
                    },
                    {
                        "name": "Wei Lin"
                    },
                    {
                        "name": "Haoxuan Li"
                    },
                    {
                        "name": "Guojun Yin"
                    }
                ],
                "author_detail": {
                    "name": "Guojun Yin"
                },
                "author": "Guojun Yin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25510v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25510v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25506v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25506v1",
                "updated": "2025-10-29T13:31:32Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    13,
                    31,
                    32,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T13:31:32Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    13,
                    31,
                    32,
                    2,
                    302,
                    0
                ],
                "title": "Reflections on the Reproducibility of Commercial LLM Performance in\n  Empirical Software Engineering Studies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reflections on the Reproducibility of Commercial LLM Performance in\n  Empirical Software Engineering Studies"
                },
                "summary": "Large Language Models have gained remarkable interest in industry and\nacademia. The increasing interest in LLMs in academia is also reflected in the\nnumber of publications on this topic over the last years. For instance, alone\n78 of the around 425 publications at ICSE 2024 performed experiments with LLMs.\nConducting empirical studies with LLMs remains challenging and raises questions\non how to achieve reproducible results, for both other researchers and\npractitioners. One important step towards excelling in empirical research on\nLLMs and their application is to first understand to what extent current\nresearch results are eventually reproducible and what factors may impede\nreproducibility. This investigation is within the scope of our work. We\ncontribute an analysis of the reproducibility of LLM-centric studies, provide\ninsights into the factors impeding reproducibility, and discuss suggestions on\nhow to improve the current state. In particular, we studied the 86 articles\ndescribing LLM-centric studies, published at ICSE 2024 and ASE 2024. Of the 86\narticles, 18 provided research artefacts and used OpenAI models. We attempted\nto replicate those 18 studies. Of the 18 studies, only five were fit for\nreproduction. For none of the five studies, we were able to fully reproduce the\nresults. Two studies seemed to be partially reproducible, and three studies did\nnot seem to be reproducible. Our results highlight not only the need for\nstricter research artefact evaluations but also for more robust study designs\nto ensure the reproducible value of future publications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models have gained remarkable interest in industry and\nacademia. The increasing interest in LLMs in academia is also reflected in the\nnumber of publications on this topic over the last years. For instance, alone\n78 of the around 425 publications at ICSE 2024 performed experiments with LLMs.\nConducting empirical studies with LLMs remains challenging and raises questions\non how to achieve reproducible results, for both other researchers and\npractitioners. One important step towards excelling in empirical research on\nLLMs and their application is to first understand to what extent current\nresearch results are eventually reproducible and what factors may impede\nreproducibility. This investigation is within the scope of our work. We\ncontribute an analysis of the reproducibility of LLM-centric studies, provide\ninsights into the factors impeding reproducibility, and discuss suggestions on\nhow to improve the current state. In particular, we studied the 86 articles\ndescribing LLM-centric studies, published at ICSE 2024 and ASE 2024. Of the 86\narticles, 18 provided research artefacts and used OpenAI models. We attempted\nto replicate those 18 studies. Of the 18 studies, only five were fit for\nreproduction. For none of the five studies, we were able to fully reproduce the\nresults. Two studies seemed to be partially reproducible, and three studies did\nnot seem to be reproducible. Our results highlight not only the need for\nstricter research artefact evaluations but also for more robust study designs\nto ensure the reproducible value of future publications."
                },
                "authors": [
                    {
                        "name": "Florian Angermeir"
                    },
                    {
                        "name": "Maximilian Amougou"
                    },
                    {
                        "name": "Mark Kreitz"
                    },
                    {
                        "name": "Andreas Bauer"
                    },
                    {
                        "name": "Matthias Linhuber"
                    },
                    {
                        "name": "Davide Fucci"
                    },
                    {
                        "name": "Fabiola Moyón C."
                    },
                    {
                        "name": "Daniel Mendez"
                    },
                    {
                        "name": "Tony Gorschek"
                    }
                ],
                "author_detail": {
                    "name": "Tony Gorschek"
                },
                "author": "Tony Gorschek",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25506v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25506v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12993v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12993v2",
                "updated": "2025-10-29T13:26:49Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    13,
                    26,
                    49,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-14T21:10:50Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    21,
                    10,
                    50,
                    1,
                    287,
                    0
                ],
                "title": "A Multilingual, Large-Scale Study of the Interplay between LLM\n  Safeguards, Personalisation, and Disinformation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Multilingual, Large-Scale Study of the Interplay between LLM\n  Safeguards, Personalisation, and Disinformation"
                },
                "summary": "Large Language Models (LLMs) can generate human-like disinformation, yet\ntheir ability to personalise such content across languages and demographics\nremains underexplored. This study presents the first large-scale, multilingual\nanalysis of persona-targeted disinformation generation by LLMs. Employing a red\nteaming methodology, we prompt eight state-of-the-art LLMs with 324 false\nnarratives and 150 demographic personas (combinations of country, generation,\nand political orientation) across four languages--English, Russian, Portuguese,\nand Hindi--resulting in AI-TRAITS, a comprehensive dataset of 1.6 million\npersonalised disinformation texts. Results show that the use of even simple\npersonalisation prompts significantly increases the likelihood of jailbreaks\nacross all studied LLMs, up to 10 percentage points, and alters linguistic and\nrhetorical patterns that enhance narrative persuasiveness. Models such as Grok\nand GPT exhibited jailbreak rates and personalisation scores both exceeding\n85%. These insights expose critical vulnerabilities in current state-of-the-art\nLLMs and offer a foundation for improving safety alignment and detection\nstrategies in multilingual and cross-demographic contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) can generate human-like disinformation, yet\ntheir ability to personalise such content across languages and demographics\nremains underexplored. This study presents the first large-scale, multilingual\nanalysis of persona-targeted disinformation generation by LLMs. Employing a red\nteaming methodology, we prompt eight state-of-the-art LLMs with 324 false\nnarratives and 150 demographic personas (combinations of country, generation,\nand political orientation) across four languages--English, Russian, Portuguese,\nand Hindi--resulting in AI-TRAITS, a comprehensive dataset of 1.6 million\npersonalised disinformation texts. Results show that the use of even simple\npersonalisation prompts significantly increases the likelihood of jailbreaks\nacross all studied LLMs, up to 10 percentage points, and alters linguistic and\nrhetorical patterns that enhance narrative persuasiveness. Models such as Grok\nand GPT exhibited jailbreak rates and personalisation scores both exceeding\n85%. These insights expose critical vulnerabilities in current state-of-the-art\nLLMs and offer a foundation for improving safety alignment and detection\nstrategies in multilingual and cross-demographic contexts."
                },
                "authors": [
                    {
                        "name": "João A. Leite"
                    },
                    {
                        "name": "Arnav Arora"
                    },
                    {
                        "name": "Silvia Gargova"
                    },
                    {
                        "name": "João Luz"
                    },
                    {
                        "name": "Gustavo Sampaio"
                    },
                    {
                        "name": "Ian Roberts"
                    },
                    {
                        "name": "Carolina Scarton"
                    },
                    {
                        "name": "Kalina Bontcheva"
                    }
                ],
                "author_detail": {
                    "name": "Kalina Bontcheva"
                },
                "author": "Kalina Bontcheva",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12993v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12993v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21236v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21236v2",
                "updated": "2025-10-29T13:11:21Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    13,
                    11,
                    21,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-24T08:10:36Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    8,
                    10,
                    36,
                    4,
                    297,
                    0
                ],
                "title": "Securing AI Agent Execution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Securing AI Agent Execution"
                },
                "summary": "Large Language Models (LLMs) have evolved into AI agents that interact with\nexternal tools and environments to perform complex tasks. The Model Context\nProtocol (MCP) has become the de facto standard for connecting agents with such\nresources, but security has lagged behind: thousands of MCP servers execute\nwith unrestricted access to host systems, creating a broad attack surface. In\nthis paper, we introduce AgentBound, the first access control framework for MCP\nservers. AgentBound combines a declarative policy mechanism, inspired by the\nAndroid permission model, with a policy enforcement engine that contains\nmalicious behavior without requiring MCP server modifications. We build a\ndataset containing the 296 most popular MCP servers, and show that access\ncontrol policies can be generated automatically from source code with 80.9%\naccuracy. We also show that AgentBound blocks the majority of security threats\nin several malicious MCP servers, and that policy enforcement engine introduces\nnegligible overhead. Our contributions provide developers and project managers\nwith a practical foundation for securing MCP servers while maintaining\nproductivity, enabling researchers and tool builders to explore new directions\nfor declarative access control and MCP security.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have evolved into AI agents that interact with\nexternal tools and environments to perform complex tasks. The Model Context\nProtocol (MCP) has become the de facto standard for connecting agents with such\nresources, but security has lagged behind: thousands of MCP servers execute\nwith unrestricted access to host systems, creating a broad attack surface. In\nthis paper, we introduce AgentBound, the first access control framework for MCP\nservers. AgentBound combines a declarative policy mechanism, inspired by the\nAndroid permission model, with a policy enforcement engine that contains\nmalicious behavior without requiring MCP server modifications. We build a\ndataset containing the 296 most popular MCP servers, and show that access\ncontrol policies can be generated automatically from source code with 80.9%\naccuracy. We also show that AgentBound blocks the majority of security threats\nin several malicious MCP servers, and that policy enforcement engine introduces\nnegligible overhead. Our contributions provide developers and project managers\nwith a practical foundation for securing MCP servers while maintaining\nproductivity, enabling researchers and tool builders to explore new directions\nfor declarative access control and MCP security."
                },
                "authors": [
                    {
                        "name": "Christoph Bühler"
                    },
                    {
                        "name": "Matteo Biagiola"
                    },
                    {
                        "name": "Luca Di Grazia"
                    },
                    {
                        "name": "Guido Salvaneschi"
                    }
                ],
                "author_detail": {
                    "name": "Guido Salvaneschi"
                },
                "author": "Guido Salvaneschi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21236v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21236v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25488v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25488v1",
                "updated": "2025-10-29T13:08:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    13,
                    8,
                    35,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T13:08:35Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    13,
                    8,
                    35,
                    2,
                    302,
                    0
                ],
                "title": "Generalized Pseudo-Relevance Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generalized Pseudo-Relevance Feedback"
                },
                "summary": "Query rewriting is a fundamental technique in information retrieval (IR). It\ntypically employs the retrieval result as relevance feedback to refine the\nquery and thereby addresses the vocabulary mismatch between user queries and\nrelevant documents. Traditional pseudo-relevance feedback (PRF) and its\nvector-based extension (VPRF) improve retrieval performance by leveraging\ntop-retrieved documents as relevance feedback. However, they are constructed\nbased on two major hypotheses: the relevance assumption (top documents are\nrelevant) and the model assumption (rewriting methods need to be designed\nspecifically for particular model architectures). While recent large language\nmodels (LLMs)-based generative relevance feedback (GRF) enables model-free\nquery reformulation, it either suffers from severe LLM hallucination or, again,\nrelies on the relevance assumption to guarantee the effectiveness of rewriting\nquality. To overcome these limitations, we introduce an assumption-relaxed\nframework: \\textit{Generalized Pseudo Relevance Feedback} (GPRF), which\nperforms model-free, natural language rewriting based on retrieved documents,\nnot only eliminating the model assumption but also reducing dependence on the\nrelevance assumption. Specifically, we design a utility-oriented training\npipeline with reinforcement learning to ensure robustness against noisy\nfeedback. Extensive experiments across multiple benchmarks and retrievers\ndemonstrate that GPRF consistently outperforms strong baselines, establishing\nit as an effective and generalizable framework for query rewriting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Query rewriting is a fundamental technique in information retrieval (IR). It\ntypically employs the retrieval result as relevance feedback to refine the\nquery and thereby addresses the vocabulary mismatch between user queries and\nrelevant documents. Traditional pseudo-relevance feedback (PRF) and its\nvector-based extension (VPRF) improve retrieval performance by leveraging\ntop-retrieved documents as relevance feedback. However, they are constructed\nbased on two major hypotheses: the relevance assumption (top documents are\nrelevant) and the model assumption (rewriting methods need to be designed\nspecifically for particular model architectures). While recent large language\nmodels (LLMs)-based generative relevance feedback (GRF) enables model-free\nquery reformulation, it either suffers from severe LLM hallucination or, again,\nrelies on the relevance assumption to guarantee the effectiveness of rewriting\nquality. To overcome these limitations, we introduce an assumption-relaxed\nframework: \\textit{Generalized Pseudo Relevance Feedback} (GPRF), which\nperforms model-free, natural language rewriting based on retrieved documents,\nnot only eliminating the model assumption but also reducing dependence on the\nrelevance assumption. Specifically, we design a utility-oriented training\npipeline with reinforcement learning to ensure robustness against noisy\nfeedback. Extensive experiments across multiple benchmarks and retrievers\ndemonstrate that GPRF consistently outperforms strong baselines, establishing\nit as an effective and generalizable framework for query rewriting."
                },
                "authors": [
                    {
                        "name": "Yiteng Tu"
                    },
                    {
                        "name": "Weihang Su"
                    },
                    {
                        "name": "Yujia Zhou"
                    },
                    {
                        "name": "Yiqun Liu"
                    },
                    {
                        "name": "Fen Lin"
                    },
                    {
                        "name": "Qin Liu"
                    },
                    {
                        "name": "Qingyao Ai"
                    }
                ],
                "author_detail": {
                    "name": "Qingyao Ai"
                },
                "author": "Qingyao Ai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25488v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25488v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25472v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25472v1",
                "updated": "2025-10-29T12:47:36Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    12,
                    47,
                    36,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T12:47:36Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    12,
                    47,
                    36,
                    2,
                    302,
                    0
                ],
                "title": "NetEcho: From Real-World Streaming Side-Channels to Full LLM\n  Conversation Recovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NetEcho: From Real-World Streaming Side-Channels to Full LLM\n  Conversation Recovery"
                },
                "summary": "In the rapidly expanding landscape of Large Language Model (LLM)\napplications, real-time output streaming has become the dominant interaction\nparadigm. While this enhances user experience, recent research reveals that it\nexposes a non-trivial attack surface through network side-channels. Adversaries\ncan exploit patterns in encrypted traffic to infer sensitive information and\nreconstruct private conversations. In response, LLM providers and third-party\nservices are deploying defenses such as traffic padding and obfuscation to\nmitigate these vulnerabilities.\n  This paper starts by presenting a systematic analysis of contemporary\nside-channel defenses in mainstream LLM applications, with a focus on services\nfrom vendors like OpenAI and DeepSeek. We identify and examine seven\nrepresentative deployment scenarios, each incorporating active/passive\nmitigation techniques. Despite these enhanced security measures, our\ninvestigation uncovers significant residual information that remains vulnerable\nto leakage within the network traffic.\n  Building on this discovery, we introduce NetEcho, a novel, LLM-based\nframework that comprehensively unleashes the network side-channel risks of\ntoday's LLM applications. NetEcho is designed to recover entire conversations\n-- including both user prompts and LLM responses -- directly from encrypted\nnetwork traffic. It features a deliberate design that ensures high-fidelity\ntext recovery, transferability across different deployment scenarios, and\nmoderate operational cost. In our evaluations on medical and legal applications\nbuilt upon leading models like DeepSeek-v3 and GPT-4o, NetEcho can recover avg\n$\\sim$70\\% information of each conversation, demonstrating a critical\nlimitation in current defense mechanisms. We conclude by discussing the\nimplications of our findings and proposing future directions for augmenting\nnetwork traffic security.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the rapidly expanding landscape of Large Language Model (LLM)\napplications, real-time output streaming has become the dominant interaction\nparadigm. While this enhances user experience, recent research reveals that it\nexposes a non-trivial attack surface through network side-channels. Adversaries\ncan exploit patterns in encrypted traffic to infer sensitive information and\nreconstruct private conversations. In response, LLM providers and third-party\nservices are deploying defenses such as traffic padding and obfuscation to\nmitigate these vulnerabilities.\n  This paper starts by presenting a systematic analysis of contemporary\nside-channel defenses in mainstream LLM applications, with a focus on services\nfrom vendors like OpenAI and DeepSeek. We identify and examine seven\nrepresentative deployment scenarios, each incorporating active/passive\nmitigation techniques. Despite these enhanced security measures, our\ninvestigation uncovers significant residual information that remains vulnerable\nto leakage within the network traffic.\n  Building on this discovery, we introduce NetEcho, a novel, LLM-based\nframework that comprehensively unleashes the network side-channel risks of\ntoday's LLM applications. NetEcho is designed to recover entire conversations\n-- including both user prompts and LLM responses -- directly from encrypted\nnetwork traffic. It features a deliberate design that ensures high-fidelity\ntext recovery, transferability across different deployment scenarios, and\nmoderate operational cost. In our evaluations on medical and legal applications\nbuilt upon leading models like DeepSeek-v3 and GPT-4o, NetEcho can recover avg\n$\\sim$70\\% information of each conversation, demonstrating a critical\nlimitation in current defense mechanisms. We conclude by discussing the\nimplications of our findings and proposing future directions for augmenting\nnetwork traffic security."
                },
                "authors": [
                    {
                        "name": "Zheng Zhang"
                    },
                    {
                        "name": "Guanlong Wu"
                    },
                    {
                        "name": "Sen Deng"
                    },
                    {
                        "name": "Shuai Wang"
                    },
                    {
                        "name": "Yinqian Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yinqian Zhang"
                },
                "author": "Yinqian Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25472v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25472v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25460v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25460v1",
                "updated": "2025-10-29T12:33:48Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    12,
                    33,
                    48,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T12:33:48Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    12,
                    33,
                    48,
                    2,
                    302,
                    0
                ],
                "title": "Fine-Tuned Language Models for Domain-Specific Summarization and Tagging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-Tuned Language Models for Domain-Specific Summarization and Tagging"
                },
                "summary": "This paper presents a pipeline integrating fine-tuned large language models\n(LLMs) with named entity recognition (NER) for efficient domain-specific text\nsummarization and tagging. The authors address the challenge posed by rapidly\nevolving sub-cultural languages and slang, which complicate automated\ninformation extraction and law enforcement monitoring. By leveraging the LLaMA\nFactory framework, the study fine-tunes LLMs on both generalpurpose and custom\ndomain-specific datasets, particularly in the political and security domains.\nThe models are evaluated using BLEU and ROUGE metrics, demonstrating that\ninstruction fine-tuning significantly enhances summarization and tagging\naccuracy, especially for specialized corpora. Notably, the LLaMA3-8B-Instruct\nmodel, despite its initial limitations in Chinese comprehension, outperforms\nits Chinese-trained counterpart after domainspecific fine-tuning, suggesting\nthat underlying reasoning capabilities can transfer across languages. The\npipeline enables concise summaries and structured entity tagging, facilitating\nrapid document categorization and distribution. This approach proves scalable\nand adaptable for real-time applications, supporting efficient information\nmanagement and the ongoing need to capture emerging language trends. The\nintegration of LLMs and NER offers a robust solution for transforming\nunstructured text into actionable insights, crucial for modern knowledge\nmanagement and security operations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a pipeline integrating fine-tuned large language models\n(LLMs) with named entity recognition (NER) for efficient domain-specific text\nsummarization and tagging. The authors address the challenge posed by rapidly\nevolving sub-cultural languages and slang, which complicate automated\ninformation extraction and law enforcement monitoring. By leveraging the LLaMA\nFactory framework, the study fine-tunes LLMs on both generalpurpose and custom\ndomain-specific datasets, particularly in the political and security domains.\nThe models are evaluated using BLEU and ROUGE metrics, demonstrating that\ninstruction fine-tuning significantly enhances summarization and tagging\naccuracy, especially for specialized corpora. Notably, the LLaMA3-8B-Instruct\nmodel, despite its initial limitations in Chinese comprehension, outperforms\nits Chinese-trained counterpart after domainspecific fine-tuning, suggesting\nthat underlying reasoning capabilities can transfer across languages. The\npipeline enables concise summaries and structured entity tagging, facilitating\nrapid document categorization and distribution. This approach proves scalable\nand adaptable for real-time applications, supporting efficient information\nmanagement and the ongoing need to capture emerging language trends. The\nintegration of LLMs and NER offers a robust solution for transforming\nunstructured text into actionable insights, crucial for modern knowledge\nmanagement and security operations."
                },
                "authors": [
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Fuming Lin"
                    },
                    {
                        "name": "Yuyu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yuyu Chen"
                },
                "author": "Yuyu Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25460v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25460v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25441v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25441v1",
                "updated": "2025-10-29T12:08:07Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    12,
                    8,
                    7,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T12:08:07Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    12,
                    8,
                    7,
                    2,
                    302,
                    0
                ],
                "title": "Grounded in Reality: Learning and Deploying Proactive LLM from Offline\n  Logs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Grounded in Reality: Learning and Deploying Proactive LLM from Offline\n  Logs"
                },
                "summary": "Large Language Models (LLMs) excel as passive responders, but teaching them\nto be proactive, goal-oriented partners, a critical capability in high-stakes\ndomains, remains a major challenge. Current paradigms either myopically\noptimize single-turn attributes or rely on brittle, high-cost user simulators,\ncreating a persistent ``reality gap''. To bridge this gap, we introduce\n\\texttt{Learn-to-Ask}, a general, simulator-free framework for learning and\ndeploying proactive dialogue agents \\textit{directly from offline expert data},\nbypassing the need to model complex user dynamics. Our key insight is to\nreframe the offline policy learning problem by leveraging the \\textbf{observed\nfuture} of each expert trajectory. This allows us to infer a dense,\nturn-by-turn reward signal grounded in the expert's revealed strategy,\ndecomposing the intractable long-horizon problem into a series of supervised\nlearning tasks, and training a policy to output a structured \\texttt{(action,\nstate_assessment)} tuple, governing both \\textbf{what to ask} and, crucially,\n\\textbf{when to stop}. To ensure reward fidelity, our Automated Grader\nCalibration pipeline systematically purges noise from the LLM-based reward\nmodel with minimal human supervision. Empirically, we demonstrate the efficacy\nof \\texttt{Learn-to-Ask} in a real-world medical dataset, using LLMs of varying\nsizes up to 32B. Our approach culminates in the successful deployment of LLMs\ninto a live, large-scale online AI service. In rigorous in-house evaluations,\nour model was launched and achieved performance even superior to human experts,\nproving our framework's ability to translate offline data into tangible,\nreal-world impact. We hope this work provides a practical and economically\nviable blueprint for transforming passive LLMs into proactive, goal-oriented\nLLM applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) excel as passive responders, but teaching them\nto be proactive, goal-oriented partners, a critical capability in high-stakes\ndomains, remains a major challenge. Current paradigms either myopically\noptimize single-turn attributes or rely on brittle, high-cost user simulators,\ncreating a persistent ``reality gap''. To bridge this gap, we introduce\n\\texttt{Learn-to-Ask}, a general, simulator-free framework for learning and\ndeploying proactive dialogue agents \\textit{directly from offline expert data},\nbypassing the need to model complex user dynamics. Our key insight is to\nreframe the offline policy learning problem by leveraging the \\textbf{observed\nfuture} of each expert trajectory. This allows us to infer a dense,\nturn-by-turn reward signal grounded in the expert's revealed strategy,\ndecomposing the intractable long-horizon problem into a series of supervised\nlearning tasks, and training a policy to output a structured \\texttt{(action,\nstate_assessment)} tuple, governing both \\textbf{what to ask} and, crucially,\n\\textbf{when to stop}. To ensure reward fidelity, our Automated Grader\nCalibration pipeline systematically purges noise from the LLM-based reward\nmodel with minimal human supervision. Empirically, we demonstrate the efficacy\nof \\texttt{Learn-to-Ask} in a real-world medical dataset, using LLMs of varying\nsizes up to 32B. Our approach culminates in the successful deployment of LLMs\ninto a live, large-scale online AI service. In rigorous in-house evaluations,\nour model was launched and achieved performance even superior to human experts,\nproving our framework's ability to translate offline data into tangible,\nreal-world impact. We hope this work provides a practical and economically\nviable blueprint for transforming passive LLMs into proactive, goal-oriented\nLLM applications."
                },
                "authors": [
                    {
                        "name": "Fei Wei"
                    },
                    {
                        "name": "Daoyuan Chen"
                    },
                    {
                        "name": "Ce Wang"
                    },
                    {
                        "name": "Yilun Huang"
                    },
                    {
                        "name": "Yushuo Chen"
                    },
                    {
                        "name": "Xuchen Pan"
                    },
                    {
                        "name": "Yaliang Li"
                    },
                    {
                        "name": "Bolin Ding"
                    }
                ],
                "author_detail": {
                    "name": "Bolin Ding"
                },
                "author": "Bolin Ding",
                "arxiv_comment": "27 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25441v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25441v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16368v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16368v2",
                "updated": "2025-10-29T12:06:15Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    12,
                    6,
                    15,
                    2,
                    302,
                    0
                ],
                "published": "2025-05-22T08:23:10Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    8,
                    23,
                    10,
                    3,
                    142,
                    0
                ],
                "title": "SATURN: SAT-based Reinforcement Learning to Unleash Language Model\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SATURN: SAT-based Reinforcement Learning to Unleash Language Model\n  Reasoning"
                },
                "summary": "How to design reinforcement learning (RL) tasks that effectively unleash the\nreasoning capability of large language models (LLMs) remains an open question.\nExisting RL tasks (e.g., math, programming, and constructing reasoning tasks)\nsuffer from three key limitations: (1) Scalability. They rely heavily on human\nannotation or expensive LLM synthesis to generate sufficient training data. (2)\nVerifiability. LLMs' outputs are hard to verify automatically and reliably. (3)\nControllable Difficulty. Most tasks lack fine-grained difficulty control,\nmaking it hard to train LLMs to develop reasoning ability from easy to hard.\n  To address these limitations, we propose Saturn, a SAT-based RL framework\nthat uses Boolean Satisfiability (SAT) problems to train and evaluate LLMs\nreasoning. Saturn enables scalable task construction, rule-based verification,\nand precise difficulty control. Saturn designs a curriculum learning pipeline\nthat continuously improves LLMs' reasoning capability by constructing SAT tasks\nof increasing difficulty and training LLMs from easy to hard. To ensure stable\ntraining, we design a principled mechanism to control difficulty transitions.\n  We introduce Saturn-2.6k, a dataset of 2,660 SAT problems with varying\ndifficulty. It supports the evaluation of how LLM reasoning changes with\nproblem difficulty. We apply Saturn to DeepSeek-R1-Distill-Qwen and obtain\nSaturn-1.5B and Saturn-7B. We achieve several notable results: (1) On SAT\nproblems, Saturn-1.5B and Saturn-7B achieve average pass@3 improvements of\n+14.0 and +28.1, respectively. (2) On math and programming tasks, Saturn-1.5B\nand Saturn-7B improve average scores by +4.9 and +1.8 on benchmarks (e.g.,\nAIME, LiveCodeBench). (3) Compared to the state-of-the-art (SOTA) approach in\nconstructing RL tasks, Saturn achieves further improvements of +8.8%. We\nrelease the source code, data, and models to support future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How to design reinforcement learning (RL) tasks that effectively unleash the\nreasoning capability of large language models (LLMs) remains an open question.\nExisting RL tasks (e.g., math, programming, and constructing reasoning tasks)\nsuffer from three key limitations: (1) Scalability. They rely heavily on human\nannotation or expensive LLM synthesis to generate sufficient training data. (2)\nVerifiability. LLMs' outputs are hard to verify automatically and reliably. (3)\nControllable Difficulty. Most tasks lack fine-grained difficulty control,\nmaking it hard to train LLMs to develop reasoning ability from easy to hard.\n  To address these limitations, we propose Saturn, a SAT-based RL framework\nthat uses Boolean Satisfiability (SAT) problems to train and evaluate LLMs\nreasoning. Saturn enables scalable task construction, rule-based verification,\nand precise difficulty control. Saturn designs a curriculum learning pipeline\nthat continuously improves LLMs' reasoning capability by constructing SAT tasks\nof increasing difficulty and training LLMs from easy to hard. To ensure stable\ntraining, we design a principled mechanism to control difficulty transitions.\n  We introduce Saturn-2.6k, a dataset of 2,660 SAT problems with varying\ndifficulty. It supports the evaluation of how LLM reasoning changes with\nproblem difficulty. We apply Saturn to DeepSeek-R1-Distill-Qwen and obtain\nSaturn-1.5B and Saturn-7B. We achieve several notable results: (1) On SAT\nproblems, Saturn-1.5B and Saturn-7B achieve average pass@3 improvements of\n+14.0 and +28.1, respectively. (2) On math and programming tasks, Saturn-1.5B\nand Saturn-7B improve average scores by +4.9 and +1.8 on benchmarks (e.g.,\nAIME, LiveCodeBench). (3) Compared to the state-of-the-art (SOTA) approach in\nconstructing RL tasks, Saturn achieves further improvements of +8.8%. We\nrelease the source code, data, and models to support future research."
                },
                "authors": [
                    {
                        "name": "Huanyu Liu"
                    },
                    {
                        "name": "Jia Li"
                    },
                    {
                        "name": "Hao Zhu"
                    },
                    {
                        "name": "Kechi Zhang"
                    },
                    {
                        "name": "Yihong Dong"
                    },
                    {
                        "name": "Ge Li"
                    }
                ],
                "author_detail": {
                    "name": "Ge Li"
                },
                "author": "Ge Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16368v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16368v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25434v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25434v1",
                "updated": "2025-10-29T11:57:03Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    11,
                    57,
                    3,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T11:57:03Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    11,
                    57,
                    3,
                    2,
                    302,
                    0
                ],
                "title": "A Critical Study of Automatic Evaluation in Sign Language Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Critical Study of Automatic Evaluation in Sign Language Translation"
                },
                "summary": "Automatic evaluation metrics are crucial for advancing sign language\ntranslation (SLT). Current SLT evaluation metrics, such as BLEU and ROUGE, are\nonly text-based, and it remains unclear to what extent text-based metrics can\nreliably capture the quality of SLT outputs. To address this gap, we\ninvestigate the limitations of text-based SLT evaluation metrics by analyzing\nsix metrics, including BLEU, chrF, and ROUGE, as well as BLEURT on the one\nhand, and large language model (LLM)-based evaluators such as G-Eval and GEMBA\nzero-shot direct assessment on the other hand. Specifically, we assess the\nconsistency and robustness of these metrics under three controlled conditions:\nparaphrasing, hallucinations in model outputs, and variations in sentence\nlength. Our analysis highlights the limitations of lexical overlap metrics and\ndemonstrates that while LLM-based evaluators better capture semantic\nequivalence often missed by conventional metrics, they can also exhibit bias\ntoward LLM-paraphrased translations. Moreover, although all metrics are able to\ndetect hallucinations, BLEU tends to be overly sensitive, whereas BLEURT and\nLLM-based evaluators are comparatively lenient toward subtle cases. This\nmotivates the need for multimodal evaluation frameworks that extend beyond\ntext-based metrics to enable a more holistic assessment of SLT outputs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic evaluation metrics are crucial for advancing sign language\ntranslation (SLT). Current SLT evaluation metrics, such as BLEU and ROUGE, are\nonly text-based, and it remains unclear to what extent text-based metrics can\nreliably capture the quality of SLT outputs. To address this gap, we\ninvestigate the limitations of text-based SLT evaluation metrics by analyzing\nsix metrics, including BLEU, chrF, and ROUGE, as well as BLEURT on the one\nhand, and large language model (LLM)-based evaluators such as G-Eval and GEMBA\nzero-shot direct assessment on the other hand. Specifically, we assess the\nconsistency and robustness of these metrics under three controlled conditions:\nparaphrasing, hallucinations in model outputs, and variations in sentence\nlength. Our analysis highlights the limitations of lexical overlap metrics and\ndemonstrates that while LLM-based evaluators better capture semantic\nequivalence often missed by conventional metrics, they can also exhibit bias\ntoward LLM-paraphrased translations. Moreover, although all metrics are able to\ndetect hallucinations, BLEU tends to be overly sensitive, whereas BLEURT and\nLLM-based evaluators are comparatively lenient toward subtle cases. This\nmotivates the need for multimodal evaluation frameworks that extend beyond\ntext-based metrics to enable a more holistic assessment of SLT outputs."
                },
                "authors": [
                    {
                        "name": "Shakib Yazdani"
                    },
                    {
                        "name": "Yasser Hamidullah"
                    },
                    {
                        "name": "Cristina España-Bonet"
                    },
                    {
                        "name": "Eleftherios Avramidis"
                    },
                    {
                        "name": "Josef van Genabith"
                    }
                ],
                "author_detail": {
                    "name": "Josef van Genabith"
                },
                "author": "Josef van Genabith",
                "arxiv_comment": "Submitted to the LREC 2026 conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25434v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25434v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25432v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25432v1",
                "updated": "2025-10-29T11:55:21Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    11,
                    55,
                    21,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T11:55:21Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    11,
                    55,
                    21,
                    2,
                    302,
                    0
                ],
                "title": "Depth and Autonomy: A Framework for Evaluating LLM Applications in\n  Social Science Research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Depth and Autonomy: A Framework for Evaluating LLM Applications in\n  Social Science Research"
                },
                "summary": "Large language models (LLMs) are increasingly utilized by researchers across\na wide range of domains, and qualitative social science is no exception;\nhowever, this adoption faces persistent challenges, including interpretive\nbias, low reliability, and weak auditability. We introduce a framework that\nsituates LLM usage along two dimensions, interpretive depth and autonomy,\nthereby offering a straightforward way to classify LLM applications in\nqualitative research and to derive practical design recommendations. We present\nthe state of the literature with respect to these two dimensions, based on all\npublished social science papers available on Web of Science that use LLMs as a\ntool and not strictly as the subject of study. Rather than granting models\nexpansive freedom, our approach encourages researchers to decompose tasks into\nmanageable segments, much as they would when delegating work to capable\nundergraduate research assistants. By maintaining low levels of autonomy and\nselectively increasing interpretive depth only where warranted and under\nsupervision, one can plausibly reap the benefits of LLMs while preserving\ntransparency and reliability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly utilized by researchers across\na wide range of domains, and qualitative social science is no exception;\nhowever, this adoption faces persistent challenges, including interpretive\nbias, low reliability, and weak auditability. We introduce a framework that\nsituates LLM usage along two dimensions, interpretive depth and autonomy,\nthereby offering a straightforward way to classify LLM applications in\nqualitative research and to derive practical design recommendations. We present\nthe state of the literature with respect to these two dimensions, based on all\npublished social science papers available on Web of Science that use LLMs as a\ntool and not strictly as the subject of study. Rather than granting models\nexpansive freedom, our approach encourages researchers to decompose tasks into\nmanageable segments, much as they would when delegating work to capable\nundergraduate research assistants. By maintaining low levels of autonomy and\nselectively increasing interpretive depth only where warranted and under\nsupervision, one can plausibly reap the benefits of LLMs while preserving\ntransparency and reliability."
                },
                "authors": [
                    {
                        "name": "Ali Sanaei"
                    },
                    {
                        "name": "Ali Rajabzadeh"
                    }
                ],
                "author_detail": {
                    "name": "Ali Rajabzadeh"
                },
                "author": "Ali Rajabzadeh",
                "arxiv_comment": "Presented at the Annual Meeting of the American Political Science\n  Association, Vancouver, BC, September 11--14 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25432v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25432v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25428v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25428v1",
                "updated": "2025-10-29T11:50:52Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    11,
                    50,
                    52,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T11:50:52Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    11,
                    50,
                    52,
                    2,
                    302,
                    0
                ],
                "title": "Alibaba International E-commerce Product Search Competition DcuRAGONs\n  Team Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Alibaba International E-commerce Product Search Competition DcuRAGONs\n  Team Technical Report"
                },
                "summary": "This report details our methodology and results developed for the\nMultilingual E-commerce Search Competition. The problem aims to recognize\nrelevance between user queries versus product items in a multilingual context\nand improve recommendation performance on e-commerce platforms. Utilizing Large\nLanguage Models (LLMs) and their capabilities in other tasks, our data-centric\nmethod achieved the highest score compared to other solutions during the\ncompetition. Final leaderboard is publised at\nhttps://alibaba-international-cikm2025.github.io. The source code for our\nproject is published at https://github.com/nhtlongcs/e-commerce-product-search.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This report details our methodology and results developed for the\nMultilingual E-commerce Search Competition. The problem aims to recognize\nrelevance between user queries versus product items in a multilingual context\nand improve recommendation performance on e-commerce platforms. Utilizing Large\nLanguage Models (LLMs) and their capabilities in other tasks, our data-centric\nmethod achieved the highest score compared to other solutions during the\ncompetition. Final leaderboard is publised at\nhttps://alibaba-international-cikm2025.github.io. The source code for our\nproject is published at https://github.com/nhtlongcs/e-commerce-product-search."
                },
                "authors": [
                    {
                        "name": "Thang-Long Nguyen-Ho"
                    },
                    {
                        "name": "Minh-Khoi Pham"
                    },
                    {
                        "name": "Hoang-Bao Le"
                    }
                ],
                "author_detail": {
                    "name": "Hoang-Bao Le"
                },
                "author": "Hoang-Bao Le",
                "arxiv_comment": "Alibaba International E-commerce Product Search Competition @ CIKM\n  2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25428v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25428v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25427v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25427v1",
                "updated": "2025-10-29T11:49:49Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    11,
                    49,
                    49,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T11:49:49Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    11,
                    49,
                    49,
                    2,
                    302,
                    0
                ],
                "title": "RLMEval: Evaluating Research-Level Neural Theorem Proving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RLMEval: Evaluating Research-Level Neural Theorem Proving"
                },
                "summary": "Despite impressive results on curated benchmarks, the practical impact of\nlarge language models (LLMs) on research-level neural theorem proving and proof\nautoformalization is still limited. We introduce RLMEval, an evaluation suite\nfor these tasks, focusing on research-level mathematics from real-world Lean\nformalization projects. RLMEval targets the evaluation of neural theorem\nproving and proof autoformalization on challenging research-level theorems by\nleveraging real Lean Blueprint formalization projects. Our evaluation of\nstate-of-the-art models on RLMEval, comprising 613 theorems from 6 Lean\nprojects, reveals a significant gap: progress on existing benchmarks does not\nreadily translate to these more realistic settings, with the best model\nachieving only a 10.3 % pass rate. RLMEval provides a new, challenging\nbenchmark designed to guide and accelerate progress in automated reasoning for\nformal mathematics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite impressive results on curated benchmarks, the practical impact of\nlarge language models (LLMs) on research-level neural theorem proving and proof\nautoformalization is still limited. We introduce RLMEval, an evaluation suite\nfor these tasks, focusing on research-level mathematics from real-world Lean\nformalization projects. RLMEval targets the evaluation of neural theorem\nproving and proof autoformalization on challenging research-level theorems by\nleveraging real Lean Blueprint formalization projects. Our evaluation of\nstate-of-the-art models on RLMEval, comprising 613 theorems from 6 Lean\nprojects, reveals a significant gap: progress on existing benchmarks does not\nreadily translate to these more realistic settings, with the best model\nachieving only a 10.3 % pass rate. RLMEval provides a new, challenging\nbenchmark designed to guide and accelerate progress in automated reasoning for\nformal mathematics."
                },
                "authors": [
                    {
                        "name": "Auguste Poiroux"
                    },
                    {
                        "name": "Antoine Bosselut"
                    },
                    {
                        "name": "Viktor Kunčak"
                    }
                ],
                "author_detail": {
                    "name": "Viktor Kunčak"
                },
                "author": "Viktor Kunčak",
                "arxiv_comment": "Accepted to EMNLP 2025 Findings. RLMEval benchmark released:\n  https://github.com/augustepoiroux/RLMEval",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25427v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25427v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25426v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25426v1",
                "updated": "2025-10-29T11:49:42Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    11,
                    49,
                    42,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T11:49:42Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    11,
                    49,
                    42,
                    2,
                    302,
                    0
                ],
                "title": "Implicature in Interaction: Understanding Implicature Improves Alignment\n  in Human-LLM Interaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Implicature in Interaction: Understanding Implicature Improves Alignment\n  in Human-LLM Interaction"
                },
                "summary": "The rapid advancement of Large Language Models (LLMs) is positioning language\nat the core of human-computer interaction (HCI). We argue that advancing HCI\nrequires attention to the linguistic foundations of interaction, particularly\nimplicature (meaning conveyed beyond explicit statements through shared\ncontext) which is essential for human-AI (HAI) alignment. This study examines\nLLMs' ability to infer user intent embedded in context-driven prompts and\nwhether understanding implicature improves response generation. Results show\nthat larger models approximate human interpretations more closely, while\nsmaller models struggle with implicature inference. Furthermore,\nimplicature-based prompts significantly enhance the perceived relevance and\nquality of responses across models, with notable gains in smaller models.\nOverall, 67.6% of participants preferred responses with implicature-embedded\nprompts to literal ones, highlighting a clear preference for contextually\nnuanced communication. Our work contributes to understanding how linguistic\ntheory can be used to address the alignment problem by making HAI interaction\nmore natural and contextually grounded.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of Large Language Models (LLMs) is positioning language\nat the core of human-computer interaction (HCI). We argue that advancing HCI\nrequires attention to the linguistic foundations of interaction, particularly\nimplicature (meaning conveyed beyond explicit statements through shared\ncontext) which is essential for human-AI (HAI) alignment. This study examines\nLLMs' ability to infer user intent embedded in context-driven prompts and\nwhether understanding implicature improves response generation. Results show\nthat larger models approximate human interpretations more closely, while\nsmaller models struggle with implicature inference. Furthermore,\nimplicature-based prompts significantly enhance the perceived relevance and\nquality of responses across models, with notable gains in smaller models.\nOverall, 67.6% of participants preferred responses with implicature-embedded\nprompts to literal ones, highlighting a clear preference for contextually\nnuanced communication. Our work contributes to understanding how linguistic\ntheory can be used to address the alignment problem by making HAI interaction\nmore natural and contextually grounded."
                },
                "authors": [
                    {
                        "name": "Asutosh Hota"
                    },
                    {
                        "name": "Jussi P. P. Jokinen"
                    }
                ],
                "author_detail": {
                    "name": "Jussi P. P. Jokinen"
                },
                "author": "Jussi P. P. Jokinen",
                "arxiv_comment": "The manuscript is approximately 7360 words and contains 12 figures\n  and 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25426v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25426v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25421v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25421v1",
                "updated": "2025-10-29T11:40:38Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    11,
                    40,
                    38,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T11:40:38Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    11,
                    40,
                    38,
                    2,
                    302,
                    0
                ],
                "title": "Small Talk, Big Impact? LLM-based Conversational Agents to Mitigate\n  Passive Fatigue in Conditional Automated Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Small Talk, Big Impact? LLM-based Conversational Agents to Mitigate\n  Passive Fatigue in Conditional Automated Driving"
                },
                "summary": "Passive fatigue during conditional automated driving can compromise driver\nreadiness and safety. This paper presents findings from a test-track study with\n40 participants in a real-world rural automated driving scenario. In this\nscenario, a Large Language Model (LLM) based conversational agent (CA) was\ndesigned to check in with drivers and re-engage them with their surroundings.\nDrawing on in-car video recordings, sleepiness ratings and interviews, we\nanalysed how drivers interacted with the agent and how these interactions\nshaped alertness. Users found the CA helpful for supporting vigilance during\npassive fatigue. Thematic analysis of acceptability further revealed three user\npreference profiles that implicate future intention to use CAs. Positioning\nempirically observed profiles within existing CA archetype frameworks\nhighlights the need for adaptive design sensitive to diverse user groups. This\nwork underscores the potential of CAs as proactive Human-Machine Interface\n(HMI) interventions, demonstrating how natural language can support\ncontext-aware interaction during automated driving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Passive fatigue during conditional automated driving can compromise driver\nreadiness and safety. This paper presents findings from a test-track study with\n40 participants in a real-world rural automated driving scenario. In this\nscenario, a Large Language Model (LLM) based conversational agent (CA) was\ndesigned to check in with drivers and re-engage them with their surroundings.\nDrawing on in-car video recordings, sleepiness ratings and interviews, we\nanalysed how drivers interacted with the agent and how these interactions\nshaped alertness. Users found the CA helpful for supporting vigilance during\npassive fatigue. Thematic analysis of acceptability further revealed three user\npreference profiles that implicate future intention to use CAs. Positioning\nempirically observed profiles within existing CA archetype frameworks\nhighlights the need for adaptive design sensitive to diverse user groups. This\nwork underscores the potential of CAs as proactive Human-Machine Interface\n(HMI) interventions, demonstrating how natural language can support\ncontext-aware interaction during automated driving."
                },
                "authors": [
                    {
                        "name": "Lewis Cockram"
                    },
                    {
                        "name": "Yueteng Yu"
                    },
                    {
                        "name": "Jorge Pardo"
                    },
                    {
                        "name": "Xiaomeng Li"
                    },
                    {
                        "name": "Andry Rakotonirainy"
                    },
                    {
                        "name": "Jonny Kuo"
                    },
                    {
                        "name": "Sebastien Demmel"
                    },
                    {
                        "name": "Mike Lenné"
                    },
                    {
                        "name": "Ronald Schroeter"
                    }
                ],
                "author_detail": {
                    "name": "Ronald Schroeter"
                },
                "author": "Ronald Schroeter",
                "arxiv_comment": "Submitted to CHI '26 Conference on Human Factors in Computing Systems",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25421v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25421v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25412v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25412v1",
                "updated": "2025-10-29T11:29:03Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    11,
                    29,
                    3,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T11:29:03Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    11,
                    29,
                    3,
                    2,
                    302,
                    0
                ],
                "title": "Serve Programs, Not Prompts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serve Programs, Not Prompts"
                },
                "summary": "Current large language model (LLM) serving systems, primarily designed for\ntext completion, are neither efficient nor adaptable for increasingly complex\nLLM applications due to their inflexible design. We propose a new LLM serving\nsystem architecture that serves programs instead of prompts to address this\nproblem. These programs, called LLM Inference Programs (LIPs), allow users to\ncustomize token prediction and KV cache management at runtime and to offload\nparts of their application logic, such as tool execution, to the server. We\ndescribe an example of this architecture through a system named Symphony, which\nfunctions as an operating system for LIPs. Symphony exposes LLM model\ncomputations via system calls and virtualizes KV cache with a dedicated file\nsystem, while ensuring GPU efficiency with a two-level process scheduling\nscheme. Symphony has the potential to open the door to a more efficient and\nextensible ecosystem for LLM applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current large language model (LLM) serving systems, primarily designed for\ntext completion, are neither efficient nor adaptable for increasingly complex\nLLM applications due to their inflexible design. We propose a new LLM serving\nsystem architecture that serves programs instead of prompts to address this\nproblem. These programs, called LLM Inference Programs (LIPs), allow users to\ncustomize token prediction and KV cache management at runtime and to offload\nparts of their application logic, such as tool execution, to the server. We\ndescribe an example of this architecture through a system named Symphony, which\nfunctions as an operating system for LIPs. Symphony exposes LLM model\ncomputations via system calls and virtualizes KV cache with a dedicated file\nsystem, while ensuring GPU efficiency with a two-level process scheduling\nscheme. Symphony has the potential to open the door to a more efficient and\nextensible ecosystem for LLM applications."
                },
                "authors": [
                    {
                        "name": "In Gim"
                    },
                    {
                        "name": "Lin Zhong"
                    }
                ],
                "author_detail": {
                    "name": "Lin Zhong"
                },
                "author": "Lin Zhong",
                "arxiv_doi": "10.1145/3713082.3730398",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3713082.3730398",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.25412v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25412v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "HotOS 2025. Follow-up implementation work (SOSP 2025) is available at\n  arXiv:2510.24051",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25411v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25411v1",
                "updated": "2025-10-29T11:28:42Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    11,
                    28,
                    42,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T11:28:42Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    11,
                    28,
                    42,
                    2,
                    302,
                    0
                ],
                "title": "Quantum-Resilient Threat Modelling for Secure RIS-Assisted ISAC in 6G\n  UAV Corridors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum-Resilient Threat Modelling for Secure RIS-Assisted ISAC in 6G\n  UAV Corridors"
                },
                "summary": "The rapid deployment of unmanned aerial vehicle (UAV) corridors in\nsixth-generation (6G) networks requires safe, intelligence-driven integrated\nsensing and communications (ISAC). Reconfigurable intelligent surfaces (RIS)\nenhance spectrum efficiency, localisation accuracy, and situational awareness,\nwhile introducing new vulnerabilities. The rise of quantum computing increases\nthe risks associated with harvest-now-decrypt-later strategies and\nquantum-enhanced spoofing. We propose a Quantum-Resilient Threat Modelling\n(QRTM) framework for RIS-assisted ISAC in UAV corridors to address these\nchallenges. QRTM integrates classical, quantum-ready, and quantum-aided\nadversaries, countered using post-quantum cryptographic (PQC) primitives:\nML-KEM for key establishment and Falcon for authentication, both embedded\nwithin RIS control signalling and UAV coordination. To strengthen security\nsensing, the framework introduces RIS-coded scene watermarking validated\nthrough a generalised likelihood ratio test (GLRT), with its detection\nprobability characterised by the Marcum Q function. Furthermore, a Secure ISAC\nUtility (SIU) jointly optimises secrecy rate, spoofing detection, and\nthroughput under RIS constraints, enabled by a scheduler with computational\ncomplexity of O(n^2). Monte Carlo evaluations using 3GPP Release 19 mid-band\nurban-canyon models (7-15 GHz) demonstrate a spoof-detection probability\napproaching 0.99 at a false-alarm rate of 1e-3, secrecy-rate retention\nexceeding 90 percent against quantum-capable adversaries, and\nsignal-interference utilisation improvements of about 25 percent compared with\nbaselines. These results show a standards-compliant path towards reliable,\nquantum-resilient ISAC for UAV corridors in smart cities and non-terrestrial\nnetworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid deployment of unmanned aerial vehicle (UAV) corridors in\nsixth-generation (6G) networks requires safe, intelligence-driven integrated\nsensing and communications (ISAC). Reconfigurable intelligent surfaces (RIS)\nenhance spectrum efficiency, localisation accuracy, and situational awareness,\nwhile introducing new vulnerabilities. The rise of quantum computing increases\nthe risks associated with harvest-now-decrypt-later strategies and\nquantum-enhanced spoofing. We propose a Quantum-Resilient Threat Modelling\n(QRTM) framework for RIS-assisted ISAC in UAV corridors to address these\nchallenges. QRTM integrates classical, quantum-ready, and quantum-aided\nadversaries, countered using post-quantum cryptographic (PQC) primitives:\nML-KEM for key establishment and Falcon for authentication, both embedded\nwithin RIS control signalling and UAV coordination. To strengthen security\nsensing, the framework introduces RIS-coded scene watermarking validated\nthrough a generalised likelihood ratio test (GLRT), with its detection\nprobability characterised by the Marcum Q function. Furthermore, a Secure ISAC\nUtility (SIU) jointly optimises secrecy rate, spoofing detection, and\nthroughput under RIS constraints, enabled by a scheduler with computational\ncomplexity of O(n^2). Monte Carlo evaluations using 3GPP Release 19 mid-band\nurban-canyon models (7-15 GHz) demonstrate a spoof-detection probability\napproaching 0.99 at a false-alarm rate of 1e-3, secrecy-rate retention\nexceeding 90 percent against quantum-capable adversaries, and\nsignal-interference utilisation improvements of about 25 percent compared with\nbaselines. These results show a standards-compliant path towards reliable,\nquantum-resilient ISAC for UAV corridors in smart cities and non-terrestrial\nnetworks."
                },
                "authors": [
                    {
                        "name": "Sana Hafeez"
                    },
                    {
                        "name": "Ghulam E Mustafa Abro"
                    },
                    {
                        "name": "Hifza Mustafa"
                    }
                ],
                "author_detail": {
                    "name": "Hifza Mustafa"
                },
                "author": "Hifza Mustafa",
                "arxiv_comment": "6 Pages, 5figures",
                "arxiv_journal_ref": "In Proceedings of the IEEE International Conference on\n  Computational Intelligence, Security, and Artificial Intelligence (CISAI\n  2025), Saudi Arabia, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25411v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25411v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25409v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25409v2",
                "updated": "2025-10-30T10:48:05Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    10,
                    48,
                    5,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-29T11:27:08Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    11,
                    27,
                    8,
                    2,
                    302,
                    0
                ],
                "title": "BhashaBench V1: A Comprehensive Benchmark for the Quadrant of Indic\n  Domains",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BhashaBench V1: A Comprehensive Benchmark for the Quadrant of Indic\n  Domains"
                },
                "summary": "The rapid advancement of large language models(LLMs) has intensified the need\nfor domain and culture specific evaluation. Existing benchmarks are largely\nAnglocentric and domain-agnostic, limiting their applicability to India-centric\ncontexts. To address this gap, we introduce BhashaBench V1, the first\ndomain-specific, multi-task, bilingual benchmark focusing on critical Indic\nknowledge systems. BhashaBench V1 contains 74,166 meticulously curated\nquestion-answer pairs, with 52,494 in English and 21,672 in Hindi, sourced from\nauthentic government and domain-specific exams. It spans four major domains:\nAgriculture, Legal, Finance, and Ayurveda, comprising 90+ subdomains and\ncovering 500+ topics, enabling fine-grained evaluation. Evaluation of 29+ LLMs\nreveals significant domain and language specific performance gaps, with\nespecially large disparities in low-resource domains. For instance, GPT-4o\nachieves 76.49% overall accuracy in Legal but only 59.74% in Ayurveda. Models\nconsistently perform better on English content compared to Hindi across all\ndomains. Subdomain-level analysis shows that areas such as Cyber Law,\nInternational Finance perform relatively well, while Panchakarma, Seed Science,\nand Human Rights remain notably weak. BhashaBench V1 provides a comprehensive\ndataset for evaluating large language models across India's diverse knowledge\ndomains. It enables assessment of models' ability to integrate domain-specific\nknowledge with bilingual understanding. All code, benchmarks, and resources are\npublicly available to support open research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of large language models(LLMs) has intensified the need\nfor domain and culture specific evaluation. Existing benchmarks are largely\nAnglocentric and domain-agnostic, limiting their applicability to India-centric\ncontexts. To address this gap, we introduce BhashaBench V1, the first\ndomain-specific, multi-task, bilingual benchmark focusing on critical Indic\nknowledge systems. BhashaBench V1 contains 74,166 meticulously curated\nquestion-answer pairs, with 52,494 in English and 21,672 in Hindi, sourced from\nauthentic government and domain-specific exams. It spans four major domains:\nAgriculture, Legal, Finance, and Ayurveda, comprising 90+ subdomains and\ncovering 500+ topics, enabling fine-grained evaluation. Evaluation of 29+ LLMs\nreveals significant domain and language specific performance gaps, with\nespecially large disparities in low-resource domains. For instance, GPT-4o\nachieves 76.49% overall accuracy in Legal but only 59.74% in Ayurveda. Models\nconsistently perform better on English content compared to Hindi across all\ndomains. Subdomain-level analysis shows that areas such as Cyber Law,\nInternational Finance perform relatively well, while Panchakarma, Seed Science,\nand Human Rights remain notably weak. BhashaBench V1 provides a comprehensive\ndataset for evaluating large language models across India's diverse knowledge\ndomains. It enables assessment of models' ability to integrate domain-specific\nknowledge with bilingual understanding. All code, benchmarks, and resources are\npublicly available to support open research."
                },
                "authors": [
                    {
                        "name": "Vijay Devane"
                    },
                    {
                        "name": "Mohd Nauman"
                    },
                    {
                        "name": "Bhargav Patel"
                    },
                    {
                        "name": "Aniket Mahendra Wakchoure"
                    },
                    {
                        "name": "Yogeshkumar Sant"
                    },
                    {
                        "name": "Shyam Pawar"
                    },
                    {
                        "name": "Viraj Thakur"
                    },
                    {
                        "name": "Ananya Godse"
                    },
                    {
                        "name": "Sunil Patra"
                    },
                    {
                        "name": "Neha Maurya"
                    },
                    {
                        "name": "Suraj Racha"
                    },
                    {
                        "name": "Nitish Kamal Singh"
                    },
                    {
                        "name": "Ajay Nagpal"
                    },
                    {
                        "name": "Piyush Sawarkar"
                    },
                    {
                        "name": "Kundeshwar Vijayrao Pundalik"
                    },
                    {
                        "name": "Rohit Saluja"
                    },
                    {
                        "name": "Ganesh Ramakrishnan"
                    }
                ],
                "author_detail": {
                    "name": "Ganesh Ramakrishnan"
                },
                "author": "Ganesh Ramakrishnan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25409v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25409v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25404v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25404v1",
                "updated": "2025-10-29T11:21:55Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    11,
                    21,
                    55,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T11:21:55Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    11,
                    21,
                    55,
                    2,
                    302,
                    0
                ],
                "title": "GPTOpt: Towards Efficient LLM-Based Black-Box Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPTOpt: Towards Efficient LLM-Based Black-Box Optimization"
                },
                "summary": "Global optimization of expensive, derivative-free black-box functions demands\nextreme sample efficiency. Classical methods such as Bayesian Optimization (BO)\ncan be effective, but they often require careful parameter tuning to each\napplication domain. At the same time, Large Language Models (LLMs) have shown\nbroad capabilities, yet state-of-the-art models remain limited in solving\ncontinuous black-box optimization tasks. We introduce GPTOpt, an LLM-based\noptimization method that equips LLMs with continuous black-box optimization\ncapabilities. By fine-tuning large language models on extensive synthetic\ndatasets derived from diverse BO parameterizations, GPTOpt leverages LLM\npre-training to generalize across optimization tasks. On a variety of black-box\noptimization benchmarks, GPTOpt surpasses traditional optimizers, highlighting\nthe capacity of LLMs for advanced numerical reasoning and introducing a\nflexible framework for global optimization without parameter tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Global optimization of expensive, derivative-free black-box functions demands\nextreme sample efficiency. Classical methods such as Bayesian Optimization (BO)\ncan be effective, but they often require careful parameter tuning to each\napplication domain. At the same time, Large Language Models (LLMs) have shown\nbroad capabilities, yet state-of-the-art models remain limited in solving\ncontinuous black-box optimization tasks. We introduce GPTOpt, an LLM-based\noptimization method that equips LLMs with continuous black-box optimization\ncapabilities. By fine-tuning large language models on extensive synthetic\ndatasets derived from diverse BO parameterizations, GPTOpt leverages LLM\npre-training to generalize across optimization tasks. On a variety of black-box\noptimization benchmarks, GPTOpt surpasses traditional optimizers, highlighting\nthe capacity of LLMs for advanced numerical reasoning and introducing a\nflexible framework for global optimization without parameter tuning."
                },
                "authors": [
                    {
                        "name": "Jamison Meindl"
                    },
                    {
                        "name": "Yunsheng Tian"
                    },
                    {
                        "name": "Tony Cui"
                    },
                    {
                        "name": "Veronika Thost"
                    },
                    {
                        "name": "Zhang-Wei Hong"
                    },
                    {
                        "name": "Jie Chen"
                    },
                    {
                        "name": "Wojciech Matusik"
                    },
                    {
                        "name": "Mina Konaković Luković"
                    }
                ],
                "author_detail": {
                    "name": "Mina Konaković Luković"
                },
                "author": "Mina Konaković Luković",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25404v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25404v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25402v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25402v2",
                "updated": "2025-10-30T02:45:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    2,
                    45,
                    14,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-29T11:20:18Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    11,
                    20,
                    18,
                    2,
                    302,
                    0
                ],
                "title": "Towards Automated Quality Assurance of Patent Specifications: A\n  Multi-Dimensional LLM Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Automated Quality Assurance of Patent Specifications: A\n  Multi-Dimensional LLM Framework"
                },
                "summary": "Although AI drafting tools have gained prominence in patent writing, the\nsystematic evaluation of AI-generated patent content quality represents a\nsignificant research gap. To address this gap, We propose to evaluate patents\nusing regulatory compliance, technical coherence, and figure-reference\nconsistency detection modules, and then generate improvement suggestions via an\nintegration module. The framework is validated on a comprehensive dataset\ncomprising 80 human-authored and 80 AI-generated patents from two patent\ndrafting tools. Evaluation is performed on 10,841 total sentences, 8,924\nnon-template sentences, and 554 patent figures for the three detection modules\nrespectively, achieving balanced accuracies of 99.74%, 82.12%, and 91.2%\nagainst expert annotations. Additional analysis was conducted to examine defect\ndistributions across patent sections, technical domains, and authoring sources.\nSection-based analysis indicates that figure-text consistency and technical\ndetail precision require particular attention. Mechanical Engineering and\nConstruction show more claim-specification inconsistencies due to complex\ntechnical documentation requirements. AI-generated patents show a significant\ngap compared to human-authored ones. While human-authored patents primarily\ncontain surface-level errors like typos, AI-generated patents exhibit more\nstructural defects in figure-text alignment and cross-references.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although AI drafting tools have gained prominence in patent writing, the\nsystematic evaluation of AI-generated patent content quality represents a\nsignificant research gap. To address this gap, We propose to evaluate patents\nusing regulatory compliance, technical coherence, and figure-reference\nconsistency detection modules, and then generate improvement suggestions via an\nintegration module. The framework is validated on a comprehensive dataset\ncomprising 80 human-authored and 80 AI-generated patents from two patent\ndrafting tools. Evaluation is performed on 10,841 total sentences, 8,924\nnon-template sentences, and 554 patent figures for the three detection modules\nrespectively, achieving balanced accuracies of 99.74%, 82.12%, and 91.2%\nagainst expert annotations. Additional analysis was conducted to examine defect\ndistributions across patent sections, technical domains, and authoring sources.\nSection-based analysis indicates that figure-text consistency and technical\ndetail precision require particular attention. Mechanical Engineering and\nConstruction show more claim-specification inconsistencies due to complex\ntechnical documentation requirements. AI-generated patents show a significant\ngap compared to human-authored ones. While human-authored patents primarily\ncontain surface-level errors like typos, AI-generated patents exhibit more\nstructural defects in figure-text alignment and cross-references."
                },
                "authors": [
                    {
                        "name": "Yuqian Chai"
                    },
                    {
                        "name": "Chaochao Wang"
                    },
                    {
                        "name": "Weilei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Weilei Wang"
                },
                "author": "Weilei Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25402v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25402v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03690v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03690v2",
                "updated": "2025-10-29T11:04:12Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    11,
                    4,
                    12,
                    2,
                    302,
                    0
                ],
                "published": "2025-06-04T08:19:37Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    8,
                    19,
                    37,
                    2,
                    155,
                    0
                ],
                "title": "Robust Preference Optimization via Dynamic Target Margins",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Preference Optimization via Dynamic Target Margins"
                },
                "summary": "The alignment of Large Language Models (LLMs) is crucial for ensuring their\nsafety and reliability in practical applications. Direct Preference\nOptimization (DPO) has emerged as an efficient method that directly optimizes\nmodels using preference pairs, significantly reducing resource demands.\nHowever, the effectiveness of DPO heavily depends on the data quality, which is\nfrequently compromised by noise. In this work, we propose $\\gamma$-PO, a\ndynamic target margin preference optimization algorithm that adjust reward\nmargins at the pairwise level. By introducing instance-specific margin\ncalibration, $\\gamma$-PO strategically prioritizes high-confidence pairs (those\ndemonstrating higher reward margins) while suppressing potential noise from\nambiguous pairs. Moreover, $\\gamma$-PO is a plug-and-play method, compatible\nwith variants of DPO that rely on reward margin between preference pairs.\nAcross benchmarks such as AlpacaEval2 and Arena-Hard, $\\gamma$-PO achieves an\naverage 4.4\\% improvement over other baselines, setting new benchmarks for\nstate-of-the-art performance. Additionally, $\\gamma$-PO requires minimal code\nchanges and has a negligible impact on training efficiency, making it a robust\nsolution for enhancing LLMs alignment. Our codes are available at\n\\href{https://github.com/sunjie279/gammaPO}{https://github.com/sunjie279/gammaPO}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The alignment of Large Language Models (LLMs) is crucial for ensuring their\nsafety and reliability in practical applications. Direct Preference\nOptimization (DPO) has emerged as an efficient method that directly optimizes\nmodels using preference pairs, significantly reducing resource demands.\nHowever, the effectiveness of DPO heavily depends on the data quality, which is\nfrequently compromised by noise. In this work, we propose $\\gamma$-PO, a\ndynamic target margin preference optimization algorithm that adjust reward\nmargins at the pairwise level. By introducing instance-specific margin\ncalibration, $\\gamma$-PO strategically prioritizes high-confidence pairs (those\ndemonstrating higher reward margins) while suppressing potential noise from\nambiguous pairs. Moreover, $\\gamma$-PO is a plug-and-play method, compatible\nwith variants of DPO that rely on reward margin between preference pairs.\nAcross benchmarks such as AlpacaEval2 and Arena-Hard, $\\gamma$-PO achieves an\naverage 4.4\\% improvement over other baselines, setting new benchmarks for\nstate-of-the-art performance. Additionally, $\\gamma$-PO requires minimal code\nchanges and has a negligible impact on training efficiency, making it a robust\nsolution for enhancing LLMs alignment. Our codes are available at\n\\href{https://github.com/sunjie279/gammaPO}{https://github.com/sunjie279/gammaPO}."
                },
                "authors": [
                    {
                        "name": "Jie Sun"
                    },
                    {
                        "name": "Junkang Wu"
                    },
                    {
                        "name": "Jiancan Wu"
                    },
                    {
                        "name": "Zhibo Zhu"
                    },
                    {
                        "name": "Xingyu Lu"
                    },
                    {
                        "name": "Jun Zhou"
                    },
                    {
                        "name": "Lintao Ma"
                    },
                    {
                        "name": "Xiang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Wang"
                },
                "author": "Xiang Wang",
                "arxiv_comment": "18 pages, 6 figures, accepted to Findings of the 63rd Annual Meeting\n  of the Association for Computational Linguistics (ACL 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03690v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03690v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21758v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21758v3",
                "updated": "2025-10-29T11:02:07Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    11,
                    2,
                    7,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-11T20:16:32Z",
                "published_parsed": [
                    2025,
                    10,
                    11,
                    20,
                    16,
                    32,
                    5,
                    284,
                    0
                ],
                "title": "Taxonomy and Trends in Reinforcement Learning for Robotics and Control\n  Systems: A Structured Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Taxonomy and Trends in Reinforcement Learning for Robotics and Control\n  Systems: A Structured Review"
                },
                "summary": "Reinforcement learning (RL) has become a foundational approach for enabling\nintelligent robotic behavior in dynamic and uncertain environments. This work\npresents an in-depth review of RL principles, advanced deep reinforcement\nlearning (DRL) algorithms, and their integration into robotic and control\nsystems. Beginning with the formalism of Markov Decision Processes (MDPs), the\nstudy outlines essential elements of the agent-environment interaction and\nexplores core algorithmic strategies including actor-critic methods,\nvalue-based learning, and policy gradients. Emphasis is placed on modern DRL\ntechniques such as DDPG, TD3, PPO, and SAC, which have shown promise in solving\nhigh-dimensional, continuous control tasks. A structured taxonomy is introduced\nto categorize RL applications across domains such as locomotion, manipulation,\nmulti-agent coordination, and human-robot interaction, along with training\nmethodologies and deployment readiness levels. The review synthesizes recent\nresearch efforts, highlighting technical trends, design patterns, and the\ngrowing maturity of RL in real-world robotics. Overall, this work aims to\nbridge theoretical advances with practical implementations, providing a\nconsolidated perspective on the evolving role of RL in autonomous robotic\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) has become a foundational approach for enabling\nintelligent robotic behavior in dynamic and uncertain environments. This work\npresents an in-depth review of RL principles, advanced deep reinforcement\nlearning (DRL) algorithms, and their integration into robotic and control\nsystems. Beginning with the formalism of Markov Decision Processes (MDPs), the\nstudy outlines essential elements of the agent-environment interaction and\nexplores core algorithmic strategies including actor-critic methods,\nvalue-based learning, and policy gradients. Emphasis is placed on modern DRL\ntechniques such as DDPG, TD3, PPO, and SAC, which have shown promise in solving\nhigh-dimensional, continuous control tasks. A structured taxonomy is introduced\nto categorize RL applications across domains such as locomotion, manipulation,\nmulti-agent coordination, and human-robot interaction, along with training\nmethodologies and deployment readiness levels. The review synthesizes recent\nresearch efforts, highlighting technical trends, design patterns, and the\ngrowing maturity of RL in real-world robotics. Overall, this work aims to\nbridge theoretical advances with practical implementations, providing a\nconsolidated perspective on the evolving role of RL in autonomous robotic\nsystems."
                },
                "authors": [
                    {
                        "name": "Kumater Ter"
                    },
                    {
                        "name": "Ore-Ofe Ajayi"
                    },
                    {
                        "name": "Daniel Udekwe"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Udekwe"
                },
                "author": "Daniel Udekwe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21758v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21758v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05780v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05780v2",
                "updated": "2025-10-29T10:59:02Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    10,
                    59,
                    2,
                    2,
                    302,
                    0
                ],
                "published": "2024-08-11T14:11:45Z",
                "published_parsed": [
                    2024,
                    8,
                    11,
                    14,
                    11,
                    45,
                    6,
                    224,
                    0
                ],
                "title": "U-DECN: End-to-End Underwater Object Detection ConvNet with Improved\n  DeNoising Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "U-DECN: End-to-End Underwater Object Detection ConvNet with Improved\n  DeNoising Training"
                },
                "summary": "Underwater object detection has higher requirements of running speed and\ndeployment efficiency for the detector due to its specific environmental\nchallenges. NMS of two- or one-stage object detectors and transformer\narchitecture of query-based end-to-end object detectors are not conducive to\ndeployment on underwater embedded devices with limited processing power. As for\nthe detrimental effect of underwater color cast noise, recent underwater object\ndetectors make network architecture or training complex, which also hinders\ntheir application and deployment on unmanned underwater vehicles. In this\npaper, we propose the Underwater DECO with improved deNoising training\n(U-DECN), the query-based end-to-end object detector (with ConvNet\nencoder-decoder architecture) for underwater color cast noise that addresses\nthe above problems. We integrate advanced technologies from DETR variants into\nDECO and design optimization methods specifically for the ConvNet architecture,\nincluding Deformable Convolution in SIM and Separate Contrastive DeNoising\nForward methods. To address the underwater color cast noise issue, we propose\nan Underwater Color DeNoising Query method to improve the generalization of the\nmodel for the biased object feature information by different color cast noise.\nOur U-DECN, with ResNet-50 backbone, achieves the best 64.0 AP on DUO and the\nbest 58.1 AP on RUOD, and 21 FPS (5 times faster than Deformable DETR and DINO\n4 FPS) on NVIDIA AGX Orin by TensorRT FP16, outperforming the other\nstate-of-the-art query-based end-to-end object detectors. The code is available\nat https://github.com/LEFTeyex/U-DECN.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Underwater object detection has higher requirements of running speed and\ndeployment efficiency for the detector due to its specific environmental\nchallenges. NMS of two- or one-stage object detectors and transformer\narchitecture of query-based end-to-end object detectors are not conducive to\ndeployment on underwater embedded devices with limited processing power. As for\nthe detrimental effect of underwater color cast noise, recent underwater object\ndetectors make network architecture or training complex, which also hinders\ntheir application and deployment on unmanned underwater vehicles. In this\npaper, we propose the Underwater DECO with improved deNoising training\n(U-DECN), the query-based end-to-end object detector (with ConvNet\nencoder-decoder architecture) for underwater color cast noise that addresses\nthe above problems. We integrate advanced technologies from DETR variants into\nDECO and design optimization methods specifically for the ConvNet architecture,\nincluding Deformable Convolution in SIM and Separate Contrastive DeNoising\nForward methods. To address the underwater color cast noise issue, we propose\nan Underwater Color DeNoising Query method to improve the generalization of the\nmodel for the biased object feature information by different color cast noise.\nOur U-DECN, with ResNet-50 backbone, achieves the best 64.0 AP on DUO and the\nbest 58.1 AP on RUOD, and 21 FPS (5 times faster than Deformable DETR and DINO\n4 FPS) on NVIDIA AGX Orin by TensorRT FP16, outperforming the other\nstate-of-the-art query-based end-to-end object detectors. The code is available\nat https://github.com/LEFTeyex/U-DECN."
                },
                "authors": [
                    {
                        "name": "Zhuoyan Liu"
                    },
                    {
                        "name": "Bo Wang"
                    },
                    {
                        "name": "Bing Wang"
                    },
                    {
                        "name": "Ye Li"
                    }
                ],
                "author_detail": {
                    "name": "Ye Li"
                },
                "author": "Ye Li",
                "arxiv_doi": "10.1109/TGRS.2025.3595158",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TGRS.2025.3595158",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.05780v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05780v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "10 pages, 6 figures, 7 tables, accepted by IEEE TGRS",
                "arxiv_journal_ref": "IEEE Transactions on Geoscience and Remote Sensing 2025",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18376v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18376v2",
                "updated": "2025-10-29T10:57:22Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    10,
                    57,
                    22,
                    2,
                    302,
                    0
                ],
                "published": "2025-09-22T19:58:17Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    19,
                    58,
                    17,
                    0,
                    265,
                    0
                ],
                "title": "GnnXemplar: Exemplars to Explanations -- Natural Language Rules for\n  Global GNN Interpretability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GnnXemplar: Exemplars to Explanations -- Natural Language Rules for\n  Global GNN Interpretability"
                },
                "summary": "Graph Neural Networks (GNNs) are widely used for node classification, yet\ntheir opaque decision-making limits trust and adoption. While local\nexplanations offer insights into individual predictions, global explanation\nmethods, those that characterize an entire class, remain underdeveloped.\nExisting global explainers rely on motif discovery in small graphs, an approach\nthat breaks down in large, real-world settings where subgraph repetition is\nrare, node attributes are high-dimensional, and predictions arise from complex\nstructure-attribute interactions. We propose GnnXemplar, a novel global\nexplainer inspired from Exemplar Theory from cognitive science. GnnXemplar\nidentifies representative nodes in the GNN embedding space, exemplars, and\nexplains predictions using natural language rules derived from their\nneighborhoods. Exemplar selection is framed as a coverage maximization problem\nover reverse k-nearest neighbors, for which we provide an efficient greedy\napproximation. To derive interpretable rules, we employ a self-refining prompt\nstrategy using large language models (LLMs). Experiments across diverse\nbenchmarks show that GnnXemplar significantly outperforms existing methods in\nfidelity, scalability, and human interpretability, as validated by a user study\nwith 60 participants.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNNs) are widely used for node classification, yet\ntheir opaque decision-making limits trust and adoption. While local\nexplanations offer insights into individual predictions, global explanation\nmethods, those that characterize an entire class, remain underdeveloped.\nExisting global explainers rely on motif discovery in small graphs, an approach\nthat breaks down in large, real-world settings where subgraph repetition is\nrare, node attributes are high-dimensional, and predictions arise from complex\nstructure-attribute interactions. We propose GnnXemplar, a novel global\nexplainer inspired from Exemplar Theory from cognitive science. GnnXemplar\nidentifies representative nodes in the GNN embedding space, exemplars, and\nexplains predictions using natural language rules derived from their\nneighborhoods. Exemplar selection is framed as a coverage maximization problem\nover reverse k-nearest neighbors, for which we provide an efficient greedy\napproximation. To derive interpretable rules, we employ a self-refining prompt\nstrategy using large language models (LLMs). Experiments across diverse\nbenchmarks show that GnnXemplar significantly outperforms existing methods in\nfidelity, scalability, and human interpretability, as validated by a user study\nwith 60 participants."
                },
                "authors": [
                    {
                        "name": "Burouj Armgaan"
                    },
                    {
                        "name": "Eshan Jain"
                    },
                    {
                        "name": "Harsh Pandey"
                    },
                    {
                        "name": "Mahesh Chandran"
                    },
                    {
                        "name": "Sayan Ranu"
                    }
                ],
                "author_detail": {
                    "name": "Sayan Ranu"
                },
                "author": "Sayan Ranu",
                "arxiv_comment": "38 pages, 20 figures, NeurIPS 2025 (Oral)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18376v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18376v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25384v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25384v1",
                "updated": "2025-10-29T10:55:52Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    10,
                    55,
                    52,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T10:55:52Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    10,
                    55,
                    52,
                    2,
                    302,
                    0
                ],
                "title": "Roleplaying with Structure: Synthetic Therapist-Client Conversation\n  Generation from Questionnaires",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Roleplaying with Structure: Synthetic Therapist-Client Conversation\n  Generation from Questionnaires"
                },
                "summary": "The development of AI for mental health is hindered by a lack of authentic\ntherapy dialogues, due to strict privacy regulations and the fact that clinical\nsessions were historically rarely recorded. We present an LLM-driven pipeline\nthat generates synthetic counseling dialogues based on structured client\nprofiles and psychological questionnaires. Grounded on the principles of\nCognitive Behavioral Therapy (CBT), our method creates synthetic therapeutic\nconversations for clinical disorders such as anxiety and depression. Our\nframework, SQPsych (Structured Questionnaire-based Psychotherapy), converts\nstructured psychological input into natural language dialogues through\ntherapist-client simulations. Due to data governance policies and privacy\nrestrictions prohibiting the transmission of clinical questionnaire data to\nthird-party services, previous methodologies relying on proprietary models are\ninfeasible in our setting. We address this limitation by generating a\nhigh-quality corpus using open-weight LLMs, validated through human expert\nevaluation and LLM-based assessments. Our SQPsychLLM models fine-tuned on\nSQPsychConv achieve strong performance on counseling benchmarks, surpassing\nbaselines in key therapeutic skills. Our findings highlight the potential of\nsynthetic data to enable scalable, data-secure, and clinically informed AI for\nmental health support. We will release our code, models, and corpus at\nhttps://ai-mh.github.io/SQPsych",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of AI for mental health is hindered by a lack of authentic\ntherapy dialogues, due to strict privacy regulations and the fact that clinical\nsessions were historically rarely recorded. We present an LLM-driven pipeline\nthat generates synthetic counseling dialogues based on structured client\nprofiles and psychological questionnaires. Grounded on the principles of\nCognitive Behavioral Therapy (CBT), our method creates synthetic therapeutic\nconversations for clinical disorders such as anxiety and depression. Our\nframework, SQPsych (Structured Questionnaire-based Psychotherapy), converts\nstructured psychological input into natural language dialogues through\ntherapist-client simulations. Due to data governance policies and privacy\nrestrictions prohibiting the transmission of clinical questionnaire data to\nthird-party services, previous methodologies relying on proprietary models are\ninfeasible in our setting. We address this limitation by generating a\nhigh-quality corpus using open-weight LLMs, validated through human expert\nevaluation and LLM-based assessments. Our SQPsychLLM models fine-tuned on\nSQPsychConv achieve strong performance on counseling benchmarks, surpassing\nbaselines in key therapeutic skills. Our findings highlight the potential of\nsynthetic data to enable scalable, data-secure, and clinically informed AI for\nmental health support. We will release our code, models, and corpus at\nhttps://ai-mh.github.io/SQPsych"
                },
                "authors": [
                    {
                        "name": "Doan Nam Long Vu"
                    },
                    {
                        "name": "Rui Tan"
                    },
                    {
                        "name": "Lena Moench"
                    },
                    {
                        "name": "Svenja Jule Francke"
                    },
                    {
                        "name": "Daniel Woiwod"
                    },
                    {
                        "name": "Florian Thomas-Odenthal"
                    },
                    {
                        "name": "Sanna Stroth"
                    },
                    {
                        "name": "Tilo Kircher"
                    },
                    {
                        "name": "Christiane Hermann"
                    },
                    {
                        "name": "Udo Dannlowski"
                    },
                    {
                        "name": "Hamidreza Jamalabadi"
                    },
                    {
                        "name": "Shaoxiong Ji"
                    }
                ],
                "author_detail": {
                    "name": "Shaoxiong Ji"
                },
                "author": "Shaoxiong Ji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25384v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25384v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25381v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25381v1",
                "updated": "2025-10-29T10:53:32Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    10,
                    53,
                    32,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T10:53:32Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    10,
                    53,
                    32,
                    2,
                    302,
                    0
                ],
                "title": "CGM-Led Multimodal Tracking with Chatbot Support: An Autoethnography in\n  Sub-Health",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CGM-Led Multimodal Tracking with Chatbot Support: An Autoethnography in\n  Sub-Health"
                },
                "summary": "Metabolic disorders present a pressing global health challenge, with China\ncarrying the world's largest burden. While continuous glucose monitoring (CGM)\nhas transformed diabetes care, its potential for supporting sub-health\npopulations -- such as individuals who are overweight, prediabetic, or anxious\n-- remains underexplored. At the same time, large language models (LLMs) are\nincreasingly used in health coaching, yet CGM is rarely incorporated as a\nfirst-class signal. To address this gap, we conducted a six-week\nautoethnography, combining CGM with multimodal indicators captured via common\ndigital devices and a chatbot that offered personalized reflections and\nexplanations of glucose fluctuations. Our findings show how CGM-led, data-first\nmultimodal tracking, coupled with conversational support, shaped everyday\npractices of diet, activity, stress, and wellbeing. This work contributes to\nHCI by extending CGM research beyond clinical diabetes and demonstrating how\nLLM-driven agents can support preventive health and reflection in at-risk\npopulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Metabolic disorders present a pressing global health challenge, with China\ncarrying the world's largest burden. While continuous glucose monitoring (CGM)\nhas transformed diabetes care, its potential for supporting sub-health\npopulations -- such as individuals who are overweight, prediabetic, or anxious\n-- remains underexplored. At the same time, large language models (LLMs) are\nincreasingly used in health coaching, yet CGM is rarely incorporated as a\nfirst-class signal. To address this gap, we conducted a six-week\nautoethnography, combining CGM with multimodal indicators captured via common\ndigital devices and a chatbot that offered personalized reflections and\nexplanations of glucose fluctuations. Our findings show how CGM-led, data-first\nmultimodal tracking, coupled with conversational support, shaped everyday\npractices of diet, activity, stress, and wellbeing. This work contributes to\nHCI by extending CGM research beyond clinical diabetes and demonstrating how\nLLM-driven agents can support preventive health and reflection in at-risk\npopulations."
                },
                "authors": [
                    {
                        "name": "Dongyijie Primo Pan"
                    },
                    {
                        "name": "Lan Luo"
                    },
                    {
                        "name": "Yike Wang"
                    },
                    {
                        "name": "Pan Hui"
                    }
                ],
                "author_detail": {
                    "name": "Pan Hui"
                },
                "author": "Pan Hui",
                "arxiv_comment": "conference paper, prepint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25381v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25381v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25378v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25378v1",
                "updated": "2025-10-29T10:51:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    10,
                    51,
                    35,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T10:51:35Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    10,
                    51,
                    35,
                    2,
                    302,
                    0
                ],
                "title": "Hallucinations in Bibliographic Recommendation: Citation Frequency as a\n  Proxy for Training Data Redundancy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucinations in Bibliographic Recommendation: Citation Frequency as a\n  Proxy for Training Data Redundancy"
                },
                "summary": "Large language models (LLMs) have been increasingly applied to a wide range\nof tasks, from natural language understanding to code generation. While they\nhave also been used to assist in bibliographic recommendation, the\nhallucination of non-existent papers remains a major issue. Building on prior\nstudies, this study hypothesizes that an LLM's ability to correctly produce\nbibliographic information depends on whether the underlying knowledge is\ngenerated or memorized, with highly cited papers (i.e., more frequently appear\nin the training corpus) showing lower hallucination rates. We therefore assume\ncitation count as a proxy for training data redundancy (i.e., the frequency\nwith which a given bibliographic record is repeatedly represented in the\npretraining corpus) and investigate how citation frequency affects hallucinated\nreferences in LLM outputs. Using GPT-4.1, we generated and manually verified\n100 bibliographic records across twenty computer-science domains, and measured\nfactual consistency via cosine similarity between generated and authentic\nmetadata. The results revealed that (i) hallucination rates vary across\nresearch domains, (ii) citation count is strongly correlated with factual\naccuracy, and (iii) bibliographic information becomes almost verbatimly\nmemorized beyond approximately 1,000 citations. These findings suggest that\nhighly cited papers are nearly verbatimly retained in the model, indicating a\nthreshold where generalization shifts into memorization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been increasingly applied to a wide range\nof tasks, from natural language understanding to code generation. While they\nhave also been used to assist in bibliographic recommendation, the\nhallucination of non-existent papers remains a major issue. Building on prior\nstudies, this study hypothesizes that an LLM's ability to correctly produce\nbibliographic information depends on whether the underlying knowledge is\ngenerated or memorized, with highly cited papers (i.e., more frequently appear\nin the training corpus) showing lower hallucination rates. We therefore assume\ncitation count as a proxy for training data redundancy (i.e., the frequency\nwith which a given bibliographic record is repeatedly represented in the\npretraining corpus) and investigate how citation frequency affects hallucinated\nreferences in LLM outputs. Using GPT-4.1, we generated and manually verified\n100 bibliographic records across twenty computer-science domains, and measured\nfactual consistency via cosine similarity between generated and authentic\nmetadata. The results revealed that (i) hallucination rates vary across\nresearch domains, (ii) citation count is strongly correlated with factual\naccuracy, and (iii) bibliographic information becomes almost verbatimly\nmemorized beyond approximately 1,000 citations. These findings suggest that\nhighly cited papers are nearly verbatimly retained in the model, indicating a\nthreshold where generalization shifts into memorization."
                },
                "authors": [
                    {
                        "name": "Junichiro Niimi"
                    }
                ],
                "author_detail": {
                    "name": "Junichiro Niimi"
                },
                "author": "Junichiro Niimi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25378v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25378v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15817v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15817v2",
                "updated": "2025-10-29T10:48:48Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    10,
                    48,
                    48,
                    2,
                    302,
                    0
                ],
                "published": "2024-10-21T09:31:51Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    9,
                    31,
                    51,
                    0,
                    295,
                    0
                ],
                "title": "LaMP-Val: Large Language Models Empower Personalized Valuation in\n  Auction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LaMP-Val: Large Language Models Empower Personalized Valuation in\n  Auction"
                },
                "summary": "Auctions are a vital economic mechanism used to determine the market value of\ngoods or services through competitive bidding within a specific framework.\nHowever, much of the current research primarily focuses on the bidding\nalgorithms used within auction mechanisms. This often neglects the potential\nbenefits of incorporating individual users' unique preferences into the\nvaluation process. Our theoretical and empirical analysis demonstrates that\nvaluation errors can significantly impact the overall utility. To bridge this\ngap, we propose a personalized valuation framework, namely Large\n\\underline{La}nguage \\underline{M}odels-powered \\underline{P}ersonalized\n\\underline{Val}uation (LaMP-Val), which integrates Large Language Models to\nincorporate personalized semantic preference into users valuation process.\nLaMP-Val integrating three components: data, learning, and evaluation. The data\ncomponent tackles the challenge of building a novel dataset specifically for\nLLMs fine-tuning in personalized valuation modeling. The learning component\nintroduces a diversity template to enhance LLMs' capacity for modeling\nfine-grained personal valuation patterns. The evaluation component establishes\na closed-loop system where LLM-generated valuations interact with bidding\nstrategies and auction. It proposes two novel metrics to quantify valuation\nprecision and bidding intention accuracy in personalized scenarios. Extensive\nexperiments show that LaMP-Val more accurately captures personalized values and\nachieves greater profits than baseline approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auctions are a vital economic mechanism used to determine the market value of\ngoods or services through competitive bidding within a specific framework.\nHowever, much of the current research primarily focuses on the bidding\nalgorithms used within auction mechanisms. This often neglects the potential\nbenefits of incorporating individual users' unique preferences into the\nvaluation process. Our theoretical and empirical analysis demonstrates that\nvaluation errors can significantly impact the overall utility. To bridge this\ngap, we propose a personalized valuation framework, namely Large\n\\underline{La}nguage \\underline{M}odels-powered \\underline{P}ersonalized\n\\underline{Val}uation (LaMP-Val), which integrates Large Language Models to\nincorporate personalized semantic preference into users valuation process.\nLaMP-Val integrating three components: data, learning, and evaluation. The data\ncomponent tackles the challenge of building a novel dataset specifically for\nLLMs fine-tuning in personalized valuation modeling. The learning component\nintroduces a diversity template to enhance LLMs' capacity for modeling\nfine-grained personal valuation patterns. The evaluation component establishes\na closed-loop system where LLM-generated valuations interact with bidding\nstrategies and auction. It proposes two novel metrics to quantify valuation\nprecision and bidding intention accuracy in personalized scenarios. Extensive\nexperiments show that LaMP-Val more accurately captures personalized values and\nachieves greater profits than baseline approaches."
                },
                "authors": [
                    {
                        "name": "Jie Sun"
                    },
                    {
                        "name": "Tianyu Zhang"
                    },
                    {
                        "name": "Houcheng Jiang"
                    },
                    {
                        "name": "Kexin Huang"
                    },
                    {
                        "name": "Xiang Shu"
                    },
                    {
                        "name": "Zhibo Zhu"
                    },
                    {
                        "name": "Lintao Ma"
                    },
                    {
                        "name": "Xingyu Lu"
                    },
                    {
                        "name": "Jun Zhou"
                    },
                    {
                        "name": "Junkang Wu"
                    },
                    {
                        "name": "Chi Luo"
                    },
                    {
                        "name": "An Zhang"
                    },
                    {
                        "name": "Junkang Wu"
                    },
                    {
                        "name": "Jiancan Wu"
                    },
                    {
                        "name": "Xiang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Wang"
                },
                "author": "Xiang Wang",
                "arxiv_comment": "17 pages, 5 figures, 8tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15817v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15817v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25370v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25370v1",
                "updated": "2025-10-29T10:41:03Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    10,
                    41,
                    3,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T10:41:03Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    10,
                    41,
                    3,
                    2,
                    302,
                    0
                ],
                "title": "Monitoring Transformative Technological Convergence Through\n  LLM-Extracted Semantic Entity Triple Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Monitoring Transformative Technological Convergence Through\n  LLM-Extracted Semantic Entity Triple Graphs"
                },
                "summary": "Forecasting transformative technologies remains a critical but challenging\ntask, particularly in fast-evolving domains such as Information and\nCommunication Technologies (ICTs). Traditional expert-based methods struggle to\nkeep pace with short innovation cycles and ambiguous early-stage terminology.\nIn this work, we propose a novel, data-driven pipeline to monitor the emergence\nof transformative technologies by identifying patterns of technological\nconvergence.\n  Our approach leverages advances in Large Language Models (LLMs) to extract\nsemantic triples from unstructured text and construct a large-scale graph of\ntechnology-related entities and relations. We introduce a new method for\ngrouping semantically similar technology terms (noun stapling) and develop\ngraph-based metrics to detect convergence signals. The pipeline includes\nmulti-stage filtering, domain-specific keyword clustering, and a temporal trend\nanalysis of topic co-occurence.\n  We validate our methodology on two complementary datasets: 278,625 arXiv\npreprints (2017--2024) to capture early scientific signals, and 9,793 USPTO\npatent applications (2018-2024) to track downstream commercial developments.\nOur results demonstrate that the proposed pipeline can identify both\nestablished and emerging convergence patterns, offering a scalable and\ngeneralizable framework for technology forecasting grounded in full-text\nanalysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forecasting transformative technologies remains a critical but challenging\ntask, particularly in fast-evolving domains such as Information and\nCommunication Technologies (ICTs). Traditional expert-based methods struggle to\nkeep pace with short innovation cycles and ambiguous early-stage terminology.\nIn this work, we propose a novel, data-driven pipeline to monitor the emergence\nof transformative technologies by identifying patterns of technological\nconvergence.\n  Our approach leverages advances in Large Language Models (LLMs) to extract\nsemantic triples from unstructured text and construct a large-scale graph of\ntechnology-related entities and relations. We introduce a new method for\ngrouping semantically similar technology terms (noun stapling) and develop\ngraph-based metrics to detect convergence signals. The pipeline includes\nmulti-stage filtering, domain-specific keyword clustering, and a temporal trend\nanalysis of topic co-occurence.\n  We validate our methodology on two complementary datasets: 278,625 arXiv\npreprints (2017--2024) to capture early scientific signals, and 9,793 USPTO\npatent applications (2018-2024) to track downstream commercial developments.\nOur results demonstrate that the proposed pipeline can identify both\nestablished and emerging convergence patterns, offering a scalable and\ngeneralizable framework for technology forecasting grounded in full-text\nanalysis."
                },
                "authors": [
                    {
                        "name": "Alexander Sternfeld"
                    },
                    {
                        "name": "Andrei Kucharavy"
                    },
                    {
                        "name": "Dimitri Percia David"
                    },
                    {
                        "name": "Alain Mermoud"
                    },
                    {
                        "name": "Julian Jang-Jaccard"
                    },
                    {
                        "name": "Nathan Monnet"
                    }
                ],
                "author_detail": {
                    "name": "Nathan Monnet"
                },
                "author": "Nathan Monnet",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25370v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25370v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.14866v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.14866v2",
                "updated": "2025-10-29T10:34:04Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    10,
                    34,
                    4,
                    2,
                    302,
                    0
                ],
                "published": "2025-06-17T17:59:31Z",
                "published_parsed": [
                    2025,
                    6,
                    17,
                    17,
                    59,
                    31,
                    1,
                    168,
                    0
                ],
                "title": "OS-Harm: A Benchmark for Measuring Safety of Computer Use Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OS-Harm: A Benchmark for Measuring Safety of Computer Use Agents"
                },
                "summary": "Computer use agents are LLM-based agents that can directly interact with a\ngraphical user interface, by processing screenshots or accessibility trees.\nWhile these systems are gaining popularity, their safety has been largely\noverlooked, despite the fact that evaluating and understanding their potential\nfor harmful behavior is essential for widespread adoption. To address this gap,\nwe introduce OS-Harm, a new benchmark for measuring safety of computer use\nagents. OS-Harm is built on top of the OSWorld environment and aims to test\nmodels across three categories of harm: deliberate user misuse, prompt\ninjection attacks, and model misbehavior. To cover these cases, we create 150\ntasks that span several types of safety violations (harassment, copyright\ninfringement, disinformation, data exfiltration, etc.) and require the agent to\ninteract with a variety of OS applications (email client, code editor, browser,\netc.). Moreover, we propose an automated judge to evaluate both accuracy and\nsafety of agents that achieves high agreement with human annotations (0.76 and\n0.79 F1 score). We evaluate computer use agents based on a range of frontier\nmodels - such as o4-mini, Claude 3.7 Sonnet, Gemini 2.5 Pro - and provide\ninsights into their safety. In particular, all models tend to directly comply\nwith many deliberate misuse queries, are relatively vulnerable to static prompt\ninjections, and occasionally perform unsafe actions. The OS-Harm benchmark is\navailable at https://github.com/tml-epfl/os-harm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computer use agents are LLM-based agents that can directly interact with a\ngraphical user interface, by processing screenshots or accessibility trees.\nWhile these systems are gaining popularity, their safety has been largely\noverlooked, despite the fact that evaluating and understanding their potential\nfor harmful behavior is essential for widespread adoption. To address this gap,\nwe introduce OS-Harm, a new benchmark for measuring safety of computer use\nagents. OS-Harm is built on top of the OSWorld environment and aims to test\nmodels across three categories of harm: deliberate user misuse, prompt\ninjection attacks, and model misbehavior. To cover these cases, we create 150\ntasks that span several types of safety violations (harassment, copyright\ninfringement, disinformation, data exfiltration, etc.) and require the agent to\ninteract with a variety of OS applications (email client, code editor, browser,\netc.). Moreover, we propose an automated judge to evaluate both accuracy and\nsafety of agents that achieves high agreement with human annotations (0.76 and\n0.79 F1 score). We evaluate computer use agents based on a range of frontier\nmodels - such as o4-mini, Claude 3.7 Sonnet, Gemini 2.5 Pro - and provide\ninsights into their safety. In particular, all models tend to directly comply\nwith many deliberate misuse queries, are relatively vulnerable to static prompt\ninjections, and occasionally perform unsafe actions. The OS-Harm benchmark is\navailable at https://github.com/tml-epfl/os-harm."
                },
                "authors": [
                    {
                        "name": "Thomas Kuntz"
                    },
                    {
                        "name": "Agatha Duzan"
                    },
                    {
                        "name": "Hao Zhao"
                    },
                    {
                        "name": "Francesco Croce"
                    },
                    {
                        "name": "Zico Kolter"
                    },
                    {
                        "name": "Nicolas Flammarion"
                    },
                    {
                        "name": "Maksym Andriushchenko"
                    }
                ],
                "author_detail": {
                    "name": "Maksym Andriushchenko"
                },
                "author": "Maksym Andriushchenko",
                "arxiv_comment": "NeurIPS 2025 Datasets & Benchmarks Track (Spotlight)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.14866v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.14866v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10044v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10044v4",
                "updated": "2025-10-29T10:31:39Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    10,
                    31,
                    39,
                    2,
                    302,
                    0
                ],
                "published": "2025-05-15T07:41:40Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    7,
                    41,
                    40,
                    3,
                    135,
                    0
                ],
                "title": "To what extent can current French mobile network support agricultural\n  robots?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To what extent can current French mobile network support agricultural\n  robots?"
                },
                "summary": "The large-scale integration of robots in agriculture offers many promises for\nenhancing sustainability and increasing food production. The numerous\napplications of agricultural robots rely on the transmission of data via mobile\nnetwork, with the amount of data depending on the services offered by the\nrobots and the level of on-board technology. Nevertheless, infrastructure\nrequired to deploy these robots, as well as the related energy and\nenvironmental consequences, appear overlooked in the digital agriculture\nliterature. In this study, we propose a method for assessing the additional\nenergy consumption and carbon footprint induced by a large-scale deployment of\nagricultural robots. Our method also estimates the share of agricultural area\nthat can be managed by the deployed robots with respect to network\ninfrastructure constraints. We have applied this method to metropolitan France\nmobile network and agricultural parcels for five different robotic scenarios.\nOur results show that increasing the robot's bitrate needs leads to significant\nadditional impacts, which increase at a pace that is poorly captured by\nclassical linear extrapolation methods. When constraining the network to the\nexisting sites, increased bitrate needs also comes with a rapidly decreasing\nmanageable agricultural area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The large-scale integration of robots in agriculture offers many promises for\nenhancing sustainability and increasing food production. The numerous\napplications of agricultural robots rely on the transmission of data via mobile\nnetwork, with the amount of data depending on the services offered by the\nrobots and the level of on-board technology. Nevertheless, infrastructure\nrequired to deploy these robots, as well as the related energy and\nenvironmental consequences, appear overlooked in the digital agriculture\nliterature. In this study, we propose a method for assessing the additional\nenergy consumption and carbon footprint induced by a large-scale deployment of\nagricultural robots. Our method also estimates the share of agricultural area\nthat can be managed by the deployed robots with respect to network\ninfrastructure constraints. We have applied this method to metropolitan France\nmobile network and agricultural parcels for five different robotic scenarios.\nOur results show that increasing the robot's bitrate needs leads to significant\nadditional impacts, which increase at a pace that is poorly captured by\nclassical linear extrapolation methods. When constraining the network to the\nexisting sites, increased bitrate needs also comes with a rapidly decreasing\nmanageable agricultural area."
                },
                "authors": [
                    {
                        "name": "Pierre La Rocca"
                    },
                    {
                        "name": "Gaël Guennebaud"
                    },
                    {
                        "name": "Aurélie Bugeau"
                    }
                ],
                "author_detail": {
                    "name": "Aurélie Bugeau"
                },
                "arxiv_affiliation": "IUF, LaBRI, UB",
                "author": "Aurélie Bugeau",
                "arxiv_comment": "Best Paper ICT4S 2025",
                "arxiv_journal_ref": "2025 11th International Conference on ICT for Sustainability\n  (ICT4S), Jun 2025, Dublin, Ireland. pp.233-243",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10044v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10044v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25356v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25356v1",
                "updated": "2025-10-29T10:21:25Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    10,
                    21,
                    25,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T10:21:25Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    10,
                    21,
                    25,
                    2,
                    302,
                    0
                ],
                "title": "Not ready for the bench: LLM legal interpretation is unstable and out of\n  step with human judgments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not ready for the bench: LLM legal interpretation is unstable and out of\n  step with human judgments"
                },
                "summary": "Legal interpretation frequently involves assessing how a legal text, as\nunderstood by an 'ordinary' speaker of the language, applies to the set of\nfacts characterizing a legal dispute in the U.S. judicial system. Recent\nscholarship has proposed that legal practitioners add large language models\n(LLMs) to their interpretive toolkit. This work offers an empirical argument\nagainst LLM interpretation as recently practiced by legal scholars and federal\njudges. Our investigation in English shows that models do not provide stable\ninterpretive judgments: varying the question format can lead the model to\nwildly different conclusions. Moreover, the models show weak to moderate\ncorrelation with human judgment, with large variance across model and question\nvariant, suggesting that it is dangerous to give much credence to the\nconclusions produced by generative AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Legal interpretation frequently involves assessing how a legal text, as\nunderstood by an 'ordinary' speaker of the language, applies to the set of\nfacts characterizing a legal dispute in the U.S. judicial system. Recent\nscholarship has proposed that legal practitioners add large language models\n(LLMs) to their interpretive toolkit. This work offers an empirical argument\nagainst LLM interpretation as recently practiced by legal scholars and federal\njudges. Our investigation in English shows that models do not provide stable\ninterpretive judgments: varying the question format can lead the model to\nwildly different conclusions. Moreover, the models show weak to moderate\ncorrelation with human judgment, with large variance across model and question\nvariant, suggesting that it is dangerous to give much credence to the\nconclusions produced by generative AI."
                },
                "authors": [
                    {
                        "name": "Abhishek Purushothama"
                    },
                    {
                        "name": "Junghyun Min"
                    },
                    {
                        "name": "Brandon Waldon"
                    },
                    {
                        "name": "Nathan Schneider"
                    }
                ],
                "author_detail": {
                    "name": "Nathan Schneider"
                },
                "author": "Nathan Schneider",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25356v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25356v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06204v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06204v2",
                "updated": "2025-10-29T10:17:57Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    10,
                    17,
                    57,
                    2,
                    302,
                    0
                ],
                "published": "2025-07-08T17:30:14Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    17,
                    30,
                    14,
                    1,
                    189,
                    0
                ],
                "title": "Differential Mamba",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Differential Mamba"
                },
                "summary": "Sequence models like Transformers and RNNs often overallocate attention to\nirrelevant context, leading to noisy intermediate representations. This\ndegrades LLM capabilities by promoting hallucinations, weakening long-range and\nretrieval abilities, and reducing robustness. Recent work has shown that\ndifferential design can mitigate this issue in Transformers, improving their\neffectiveness across various applications. In this paper, we explore whether\nthese techniques, originally developed for Transformers, can be applied to\nMamba, a recent architecture based on selective state-space layers that\nachieves Transformer-level performance with greater efficiency. We show that a\nnaive adaptation of differential design to Mamba is insufficient and requires\ncareful architectural modifications. To address this, we introduce a novel\ndifferential mechanism for Mamba, empirically validated on language modeling\nbenchmarks, demonstrating improved retrieval capabilities and superior\nperformance over vanilla Mamba. Finally, we conduct extensive ablation studies\nand empirical analyses to justify our design choices and provide evidence that\nour approach effectively mitigates the overallocation problem in Mamba-based\nmodels. Our code is publicly available: https://github.com/NadavSc/Diff-Mamba",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequence models like Transformers and RNNs often overallocate attention to\nirrelevant context, leading to noisy intermediate representations. This\ndegrades LLM capabilities by promoting hallucinations, weakening long-range and\nretrieval abilities, and reducing robustness. Recent work has shown that\ndifferential design can mitigate this issue in Transformers, improving their\neffectiveness across various applications. In this paper, we explore whether\nthese techniques, originally developed for Transformers, can be applied to\nMamba, a recent architecture based on selective state-space layers that\nachieves Transformer-level performance with greater efficiency. We show that a\nnaive adaptation of differential design to Mamba is insufficient and requires\ncareful architectural modifications. To address this, we introduce a novel\ndifferential mechanism for Mamba, empirically validated on language modeling\nbenchmarks, demonstrating improved retrieval capabilities and superior\nperformance over vanilla Mamba. Finally, we conduct extensive ablation studies\nand empirical analyses to justify our design choices and provide evidence that\nour approach effectively mitigates the overallocation problem in Mamba-based\nmodels. Our code is publicly available: https://github.com/NadavSc/Diff-Mamba"
                },
                "authors": [
                    {
                        "name": "Nadav Schneider"
                    },
                    {
                        "name": "Itamar Zimerman"
                    },
                    {
                        "name": "Eliya Nachmani"
                    }
                ],
                "author_detail": {
                    "name": "Eliya Nachmani"
                },
                "author": "Eliya Nachmani",
                "arxiv_comment": "AACL 2025. We provide the code at\n  https://github.com/NadavSc/Diff-Mamba",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06204v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06204v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25346v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25346v1",
                "updated": "2025-10-29T10:03:59Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    10,
                    3,
                    59,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T10:03:59Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    10,
                    3,
                    59,
                    2,
                    302,
                    0
                ],
                "title": "Joint Beamforming Design and Resource Allocation for IRS-Assisted\n  Full-Duplex Terahertz Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint Beamforming Design and Resource Allocation for IRS-Assisted\n  Full-Duplex Terahertz Systems"
                },
                "summary": "Intelligent reflecting surface (IRS)-assisted full-duplex (FD) terahertz\n(THz) communication systems have emerged as a promising paradigm to satisfy the\nescalating demand for ultra-high data rates and spectral efficiency in future\nwireless networks. However, the practical deployment of such systems presents\nunique technical challenges, stemming from severe propagation loss,\nfrequency-dependent molecular absorption in the THz band, and the presence of\nstrong residual self-interference (SI) inherent to FD communications. To tackle\nthese issues, this paper proposes a joint resource allocation framework that\naims to maximize the weighted minimum rate among all users, thereby ensuring\nfairness in quality of service. Specifically, the proposed design jointly\noptimizes IRS reflecting phase shifts, uplink/downlink transmit power control,\nsub-band bandwidth allocation, and sub-band assignment, explicitly capturing\nthe unique propagation characteristics of THz channels and the impact of\nresidual SI. To strike an balance between system performance and computational\ncomplexity, two computationally efficient algorithms are developed under\ndistinct spectrum partitioning schemes: one assumes equal sub-band bandwidth\nallocation to facilliate tractable optimization, while the other introduces\nadaptive bandwidth allocation to further enhance spectral utilization and\nsystem flexibility. Simulation results validate the effectiveness of the\nproposed designs and demonstrate that the adopted scheme achieves significant\nspectral efficiency improvements over benchmark schemes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intelligent reflecting surface (IRS)-assisted full-duplex (FD) terahertz\n(THz) communication systems have emerged as a promising paradigm to satisfy the\nescalating demand for ultra-high data rates and spectral efficiency in future\nwireless networks. However, the practical deployment of such systems presents\nunique technical challenges, stemming from severe propagation loss,\nfrequency-dependent molecular absorption in the THz band, and the presence of\nstrong residual self-interference (SI) inherent to FD communications. To tackle\nthese issues, this paper proposes a joint resource allocation framework that\naims to maximize the weighted minimum rate among all users, thereby ensuring\nfairness in quality of service. Specifically, the proposed design jointly\noptimizes IRS reflecting phase shifts, uplink/downlink transmit power control,\nsub-band bandwidth allocation, and sub-band assignment, explicitly capturing\nthe unique propagation characteristics of THz channels and the impact of\nresidual SI. To strike an balance between system performance and computational\ncomplexity, two computationally efficient algorithms are developed under\ndistinct spectrum partitioning schemes: one assumes equal sub-band bandwidth\nallocation to facilliate tractable optimization, while the other introduces\nadaptive bandwidth allocation to further enhance spectral utilization and\nsystem flexibility. Simulation results validate the effectiveness of the\nproposed designs and demonstrate that the adopted scheme achieves significant\nspectral efficiency improvements over benchmark schemes."
                },
                "authors": [
                    {
                        "name": "Chi Qiu"
                    },
                    {
                        "name": "Wen Chen"
                    },
                    {
                        "name": "Qingqing Wu"
                    },
                    {
                        "name": "Fen Hou"
                    },
                    {
                        "name": "Wanming Hao"
                    },
                    {
                        "name": "Ruiqi Liu"
                    },
                    {
                        "name": "Derrick Wing Kwan Ng"
                    }
                ],
                "author_detail": {
                    "name": "Derrick Wing Kwan Ng"
                },
                "author": "Derrick Wing Kwan Ng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25346v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25346v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25345v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25345v1",
                "updated": "2025-10-29T10:03:33Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    10,
                    3,
                    33,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T10:03:33Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    10,
                    3,
                    33,
                    2,
                    302,
                    0
                ],
                "title": "Informative Sample Selection Model for Skeleton-based Action Recognition\n  with Limited Training Samples",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Informative Sample Selection Model for Skeleton-based Action Recognition\n  with Limited Training Samples"
                },
                "summary": "Skeleton-based human action recognition aims to classify human skeletal\nsequences, which are spatiotemporal representations of actions, into predefined\ncategories. To reduce the reliance on costly annotations of skeletal sequences\nwhile maintaining competitive recognition accuracy, the task of 3D Action\nRecognition with Limited Training Samples, also known as semi-supervised 3D\nAction Recognition, has been proposed. In addition, active learning, which aims\nto proactively select the most informative unlabeled samples for annotation,\nhas been explored in semi-supervised 3D Action Recognition for training sample\nselection. Specifically, researchers adopt an encoder-decoder framework to\nembed skeleton sequences into a latent space, where clustering information,\ncombined with a margin-based selection strategy using a multi-head mechanism,\nis utilized to identify the most informative sequences in the unlabeled set for\nannotation. However, the most representative skeleton sequences may not\nnecessarily be the most informative for the action recognizer, as the model may\nhave already acquired similar knowledge from previously seen skeleton samples.\nTo solve it, we reformulate Semi-supervised 3D action recognition via active\nlearning from a novel perspective by casting it as a Markov Decision Process\n(MDP). Built upon the MDP framework and its training paradigm, we train an\ninformative sample selection model to intelligently guide the selection of\nskeleton sequences for annotation. To enhance the representational capacity of\nthe factors in the state-action pairs within our method, we project them from\nEuclidean space to hyperbolic space. Furthermore, we introduce a meta tuning\nstrategy to accelerate the deployment of our method in real-world scenarios.\nExtensive experiments on three 3D action recognition benchmarks demonstrate the\neffectiveness of our method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skeleton-based human action recognition aims to classify human skeletal\nsequences, which are spatiotemporal representations of actions, into predefined\ncategories. To reduce the reliance on costly annotations of skeletal sequences\nwhile maintaining competitive recognition accuracy, the task of 3D Action\nRecognition with Limited Training Samples, also known as semi-supervised 3D\nAction Recognition, has been proposed. In addition, active learning, which aims\nto proactively select the most informative unlabeled samples for annotation,\nhas been explored in semi-supervised 3D Action Recognition for training sample\nselection. Specifically, researchers adopt an encoder-decoder framework to\nembed skeleton sequences into a latent space, where clustering information,\ncombined with a margin-based selection strategy using a multi-head mechanism,\nis utilized to identify the most informative sequences in the unlabeled set for\nannotation. However, the most representative skeleton sequences may not\nnecessarily be the most informative for the action recognizer, as the model may\nhave already acquired similar knowledge from previously seen skeleton samples.\nTo solve it, we reformulate Semi-supervised 3D action recognition via active\nlearning from a novel perspective by casting it as a Markov Decision Process\n(MDP). Built upon the MDP framework and its training paradigm, we train an\ninformative sample selection model to intelligently guide the selection of\nskeleton sequences for annotation. To enhance the representational capacity of\nthe factors in the state-action pairs within our method, we project them from\nEuclidean space to hyperbolic space. Furthermore, we introduce a meta tuning\nstrategy to accelerate the deployment of our method in real-world scenarios.\nExtensive experiments on three 3D action recognition benchmarks demonstrate the\neffectiveness of our method."
                },
                "authors": [
                    {
                        "name": "Zhigang Tu"
                    },
                    {
                        "name": "Zhengbo Zhang"
                    },
                    {
                        "name": "Jia Gong"
                    },
                    {
                        "name": "Junsong Yuan"
                    },
                    {
                        "name": "Bo Du"
                    }
                ],
                "author_detail": {
                    "name": "Bo Du"
                },
                "author": "Bo Du",
                "arxiv_comment": "Accepted by IEEE Transactions on Image Processing (TIP), 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25345v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25345v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11094v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11094v4",
                "updated": "2025-10-30T08:44:27Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    8,
                    44,
                    27,
                    3,
                    303,
                    0
                ],
                "published": "2025-03-14T05:35:38Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    5,
                    35,
                    38,
                    4,
                    73,
                    0
                ],
                "title": "Open3D-VQA: A Benchmark for Comprehensive Spatial Reasoning with\n  Multimodal Large Language Model in Open Space",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open3D-VQA: A Benchmark for Comprehensive Spatial Reasoning with\n  Multimodal Large Language Model in Open Space"
                },
                "summary": "Spatial reasoning is a fundamental capability of multimodal large language\nmodels (MLLMs), yet their performance in open aerial environments remains\nunderexplored. In this work, we present Open3D-VQA, a novel benchmark for\nevaluating MLLMs' ability to reason about complex spatial relationships from an\naerial perspective. The benchmark comprises 73k QA pairs spanning 7 general\nspatial reasoning tasks, including multiple-choice, true/false, and\nshort-answer formats, and supports both visual and point cloud modalities. The\nquestions are automatically generated from spatial relations extracted from\nboth real-world and simulated aerial scenes. Evaluation on 13 popular MLLMs\nreveals that: 1) Models are generally better at answering questions about\nrelative spatial relations than absolute distances, 2) 3D LLMs fail to\ndemonstrate significant advantages over 2D LLMs, and 3) Fine-tuning solely on\nthe simulated dataset can significantly improve the model's spatial reasoning\nperformance in real-world scenarios. We release our benchmark, data generation\npipeline, and evaluation toolkit to support further research:\nhttps://github.com/EmbodiedCity/Open3D-VQA.code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spatial reasoning is a fundamental capability of multimodal large language\nmodels (MLLMs), yet their performance in open aerial environments remains\nunderexplored. In this work, we present Open3D-VQA, a novel benchmark for\nevaluating MLLMs' ability to reason about complex spatial relationships from an\naerial perspective. The benchmark comprises 73k QA pairs spanning 7 general\nspatial reasoning tasks, including multiple-choice, true/false, and\nshort-answer formats, and supports both visual and point cloud modalities. The\nquestions are automatically generated from spatial relations extracted from\nboth real-world and simulated aerial scenes. Evaluation on 13 popular MLLMs\nreveals that: 1) Models are generally better at answering questions about\nrelative spatial relations than absolute distances, 2) 3D LLMs fail to\ndemonstrate significant advantages over 2D LLMs, and 3) Fine-tuning solely on\nthe simulated dataset can significantly improve the model's spatial reasoning\nperformance in real-world scenarios. We release our benchmark, data generation\npipeline, and evaluation toolkit to support further research:\nhttps://github.com/EmbodiedCity/Open3D-VQA.code."
                },
                "authors": [
                    {
                        "name": "Weichen Zhang"
                    },
                    {
                        "name": "Zile Zhou"
                    },
                    {
                        "name": "Xin Zeng"
                    },
                    {
                        "name": "Xuchen Liu"
                    },
                    {
                        "name": "Jianjie Fang"
                    },
                    {
                        "name": "Chen Gao"
                    },
                    {
                        "name": "Yong Li"
                    },
                    {
                        "name": "Jinqiang Cui"
                    },
                    {
                        "name": "Xinlei Chen"
                    },
                    {
                        "name": "Xiao-Ping Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiao-Ping Zhang"
                },
                "author": "Xiao-Ping Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11094v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11094v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25333v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25333v1",
                "updated": "2025-10-29T09:47:40Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    9,
                    47,
                    40,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T09:47:40Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    9,
                    47,
                    40,
                    2,
                    302,
                    0
                ],
                "title": "CRMWeaver: Building Powerful Business Agent via Agentic RL and Shared\n  Memories",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CRMWeaver: Building Powerful Business Agent via Agentic RL and Shared\n  Memories"
                },
                "summary": "Recent years have witnessed the rapid development of LLM-based agents, which\nshed light on using language agents to solve complex real-world problems. A\nprominent application lies in business agents, which interact with databases\nand internal knowledge bases via tool calls to fulfill diverse user\nrequirements. However, this domain is characterized by intricate data\nrelationships and a wide range of heterogeneous tasks, from statistical data\nqueries to knowledge-based question-answering. To address these challenges, we\npropose CRMWeaver, a novel approach that enhances business agents in such\ncomplex settings. To acclimate the agentic model to intricate business\nenvironments, we employ a synthesis data generation and RL-based paradigm\nduring training, which significantly improves the model's ability to handle\ncomplex data and varied tasks. During inference, a shared memories mechanism is\nintroduced, prompting the agent to learn from task guidelines in similar\nproblems, thereby further boosting its effectiveness and generalization,\nespecially in unseen scenarios. We validate the efficacy of our approach on the\nCRMArena-Pro dataset, where our lightweight model achieves competitive results\nin both B2B and B2C business scenarios, underscoring its practical value for\nreal-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent years have witnessed the rapid development of LLM-based agents, which\nshed light on using language agents to solve complex real-world problems. A\nprominent application lies in business agents, which interact with databases\nand internal knowledge bases via tool calls to fulfill diverse user\nrequirements. However, this domain is characterized by intricate data\nrelationships and a wide range of heterogeneous tasks, from statistical data\nqueries to knowledge-based question-answering. To address these challenges, we\npropose CRMWeaver, a novel approach that enhances business agents in such\ncomplex settings. To acclimate the agentic model to intricate business\nenvironments, we employ a synthesis data generation and RL-based paradigm\nduring training, which significantly improves the model's ability to handle\ncomplex data and varied tasks. During inference, a shared memories mechanism is\nintroduced, prompting the agent to learn from task guidelines in similar\nproblems, thereby further boosting its effectiveness and generalization,\nespecially in unseen scenarios. We validate the efficacy of our approach on the\nCRMArena-Pro dataset, where our lightweight model achieves competitive results\nin both B2B and B2C business scenarios, underscoring its practical value for\nreal-world applications."
                },
                "authors": [
                    {
                        "name": "Yilong Lai"
                    },
                    {
                        "name": "Yipin Yang"
                    },
                    {
                        "name": "Jialong Wu"
                    },
                    {
                        "name": "Fengran Mo"
                    },
                    {
                        "name": "Zhenglin Wang"
                    },
                    {
                        "name": "Ting Liang"
                    },
                    {
                        "name": "Jianguo Lin"
                    },
                    {
                        "name": "Keping Yang"
                    }
                ],
                "author_detail": {
                    "name": "Keping Yang"
                },
                "author": "Keping Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25333v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25333v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25320v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25320v1",
                "updated": "2025-10-29T09:35:55Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    9,
                    35,
                    55,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T09:35:55Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    9,
                    35,
                    55,
                    2,
                    302,
                    0
                ],
                "title": "GAP: Graph-Based Agent Planning with Parallel Tool Use and Reinforcement\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GAP: Graph-Based Agent Planning with Parallel Tool Use and Reinforcement\n  Learning"
                },
                "summary": "Autonomous agents powered by large language models (LLMs) have shown\nimpressive capabilities in tool manipulation for complex task-solving. However,\nexisting paradigms such as ReAct rely on sequential reasoning and execution,\nfailing to exploit the inherent parallelism among independent sub-tasks. This\nsequential bottleneck leads to inefficient tool utilization and suboptimal\nperformance in multi-step reasoning scenarios. We introduce Graph-based Agent\nPlanning (GAP), a novel framework that explicitly models inter-task\ndependencies through graph-based planning to enable adaptive parallel and\nserial tool execution. Our approach trains agent foundation models to decompose\ncomplex tasks into dependency-aware sub-task graphs, autonomously determining\nwhich tools can be executed in parallel and which must follow sequential\ndependencies. This dependency-aware orchestration achieves substantial\nimprovements in both execution efficiency and task accuracy. To train GAP, we\nconstruct a high-quality dataset of graph-based planning traces derived from\nthe Multi-Hop Question Answering (MHQA) benchmark. We employ a two-stage\ntraining strategy: supervised fine-tuning (SFT) on the curated dataset,\nfollowed by reinforcement learning (RL) with a correctness-based reward\nfunction on strategically sampled queries where tool-based reasoning provides\nmaximum value. Experimental results on MHQA datasets demonstrate that GAP\nsignificantly outperforms traditional ReAct baselines, particularly on\nmulti-step retrieval tasks, while achieving dramatic improvements in tool\ninvocation efficiency through intelligent parallelization. The project page is\navailable at: https://github.com/WJQ7777/Graph-Agent-Planning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous agents powered by large language models (LLMs) have shown\nimpressive capabilities in tool manipulation for complex task-solving. However,\nexisting paradigms such as ReAct rely on sequential reasoning and execution,\nfailing to exploit the inherent parallelism among independent sub-tasks. This\nsequential bottleneck leads to inefficient tool utilization and suboptimal\nperformance in multi-step reasoning scenarios. We introduce Graph-based Agent\nPlanning (GAP), a novel framework that explicitly models inter-task\ndependencies through graph-based planning to enable adaptive parallel and\nserial tool execution. Our approach trains agent foundation models to decompose\ncomplex tasks into dependency-aware sub-task graphs, autonomously determining\nwhich tools can be executed in parallel and which must follow sequential\ndependencies. This dependency-aware orchestration achieves substantial\nimprovements in both execution efficiency and task accuracy. To train GAP, we\nconstruct a high-quality dataset of graph-based planning traces derived from\nthe Multi-Hop Question Answering (MHQA) benchmark. We employ a two-stage\ntraining strategy: supervised fine-tuning (SFT) on the curated dataset,\nfollowed by reinforcement learning (RL) with a correctness-based reward\nfunction on strategically sampled queries where tool-based reasoning provides\nmaximum value. Experimental results on MHQA datasets demonstrate that GAP\nsignificantly outperforms traditional ReAct baselines, particularly on\nmulti-step retrieval tasks, while achieving dramatic improvements in tool\ninvocation efficiency through intelligent parallelization. The project page is\navailable at: https://github.com/WJQ7777/Graph-Agent-Planning."
                },
                "authors": [
                    {
                        "name": "Jiaqi Wu"
                    },
                    {
                        "name": "Qinlao Zhao"
                    },
                    {
                        "name": "Zefeng Chen"
                    },
                    {
                        "name": "Kai Qin"
                    },
                    {
                        "name": "Yifei Zhao"
                    },
                    {
                        "name": "Xueqian Wang"
                    },
                    {
                        "name": "Yuhang Yao"
                    }
                ],
                "author_detail": {
                    "name": "Yuhang Yao"
                },
                "author": "Yuhang Yao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25320v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25320v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25310v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25310v1",
                "updated": "2025-10-29T09:23:17Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    9,
                    23,
                    17,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T09:23:17Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    9,
                    23,
                    17,
                    2,
                    302,
                    0
                ],
                "title": "Parrot: A Training Pipeline Enhances Both Program CoT and Natural\n  Language CoT for Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parrot: A Training Pipeline Enhances Both Program CoT and Natural\n  Language CoT for Reasoning"
                },
                "summary": "Natural language chain-of-thought (N-CoT) and Program chain-of-thought\n(P-CoT) have emerged as two primary paradigms for large language models (LLMs)\nto solve mathematical reasoning problems. Current research typically endeavors\nto achieve unidirectional enhancement: P-CoT enhanced N-CoT or N-CoT enhanced\nP-CoT. In this paper, we seek to fully unleash the two paradigms' strengths for\nmutual enhancement and ultimately achieve simultaneous improvements. We conduct\na detailed analysis of the error types across two paradigms, based on which we\npropose Parrot, a novel training pipeline for mathematical problems: 1) Three\ntarget-designed subtasks integrate sequential P-CoT and N-CoT generation. 2) A\nsubtask hybrid training strategy to facilitate natural language semantic\ntransferability. 3) The converted N-CoT auxiliary reward is designed to\nalleviate the sparse rewards in P-CoT optimization. Extensive experiments\ndemonstrate that Parrot significantly enhances both the performance of N-CoT\nand P-CoT, especially on N-CoT. Using Parrot SFT, the N-CoT performance of\nLLaMA2 and CodeLLaMA achieve gains of +21.87 and +21.48 on MathQA over the RL\nbaseline, which is resource-intensive.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural language chain-of-thought (N-CoT) and Program chain-of-thought\n(P-CoT) have emerged as two primary paradigms for large language models (LLMs)\nto solve mathematical reasoning problems. Current research typically endeavors\nto achieve unidirectional enhancement: P-CoT enhanced N-CoT or N-CoT enhanced\nP-CoT. In this paper, we seek to fully unleash the two paradigms' strengths for\nmutual enhancement and ultimately achieve simultaneous improvements. We conduct\na detailed analysis of the error types across two paradigms, based on which we\npropose Parrot, a novel training pipeline for mathematical problems: 1) Three\ntarget-designed subtasks integrate sequential P-CoT and N-CoT generation. 2) A\nsubtask hybrid training strategy to facilitate natural language semantic\ntransferability. 3) The converted N-CoT auxiliary reward is designed to\nalleviate the sparse rewards in P-CoT optimization. Extensive experiments\ndemonstrate that Parrot significantly enhances both the performance of N-CoT\nand P-CoT, especially on N-CoT. Using Parrot SFT, the N-CoT performance of\nLLaMA2 and CodeLLaMA achieve gains of +21.87 and +21.48 on MathQA over the RL\nbaseline, which is resource-intensive."
                },
                "authors": [
                    {
                        "name": "Senjie Jin"
                    },
                    {
                        "name": "Lu Chen"
                    },
                    {
                        "name": "Zhiheng Xi"
                    },
                    {
                        "name": "Yuhui Wang"
                    },
                    {
                        "name": "Sirui Song"
                    },
                    {
                        "name": "Yuhao Zhou"
                    },
                    {
                        "name": "Xinbo Zhang"
                    },
                    {
                        "name": "Peng Sun"
                    },
                    {
                        "name": "Hong Lu"
                    },
                    {
                        "name": "Tao Gui"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Xuanjing Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xuanjing Huang"
                },
                "author": "Xuanjing Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25310v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25310v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25301v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25301v1",
                "updated": "2025-10-29T09:14:07Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    9,
                    14,
                    7,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T09:14:07Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    9,
                    14,
                    7,
                    2,
                    302,
                    0
                ],
                "title": "GaTector+: A Unified Head-free Framework for Gaze Object and Gaze\n  Following Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GaTector+: A Unified Head-free Framework for Gaze Object and Gaze\n  Following Prediction"
                },
                "summary": "Gaze object detection and gaze following are fundamental tasks for\ninterpreting human gaze behavior or intent. However, most previous methods\nusually solve these two tasks separately, and their prediction of gaze objects\nand gaze following typically depend on head-related prior knowledge during both\nthe training phase and real-world deployment. This dependency necessitates an\nauxiliary network to extract head location, thus precluding joint optimization\nacross the entire system and constraining the practical applicability. To this\nend, we propose GaTector+, a unified framework for gaze object detection and\ngaze following, which eliminates the dependence on the head-related priors\nduring inference. Specifically, GaTector+ uses an expanded\nspecific-general-specific feature extractor that leverages a shared backbone,\nwhich extracts general features for gaze following and object detection using\nthe shared backbone while using specific blocks before and after the shared\nbackbone to better consider the specificity of each sub-task. To obtain\nhead-related knowledge without prior information, we first embed a head\ndetection branch to predict the head of each person. Then, before regressing\nthe gaze point, a head-based attention mechanism is proposed to fuse the sense\nfeature and gaze feature with the help of head location. Since the\nsuboptimization of the gaze point heatmap leads to the performance bottleneck,\nwe propose an attention supervision mechanism to accelerate the learning of the\ngaze heatmap. Finally, we propose a novel evaluation metric, mean Similarity\nover Candidates (mSoC), for gaze object detection, which is more sensitive to\nvariations between bounding boxes. The experimental results on multiple\nbenchmark datasets demonstrate the effectiveness of our model in both gaze\nobject detection and gaze following tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gaze object detection and gaze following are fundamental tasks for\ninterpreting human gaze behavior or intent. However, most previous methods\nusually solve these two tasks separately, and their prediction of gaze objects\nand gaze following typically depend on head-related prior knowledge during both\nthe training phase and real-world deployment. This dependency necessitates an\nauxiliary network to extract head location, thus precluding joint optimization\nacross the entire system and constraining the practical applicability. To this\nend, we propose GaTector+, a unified framework for gaze object detection and\ngaze following, which eliminates the dependence on the head-related priors\nduring inference. Specifically, GaTector+ uses an expanded\nspecific-general-specific feature extractor that leverages a shared backbone,\nwhich extracts general features for gaze following and object detection using\nthe shared backbone while using specific blocks before and after the shared\nbackbone to better consider the specificity of each sub-task. To obtain\nhead-related knowledge without prior information, we first embed a head\ndetection branch to predict the head of each person. Then, before regressing\nthe gaze point, a head-based attention mechanism is proposed to fuse the sense\nfeature and gaze feature with the help of head location. Since the\nsuboptimization of the gaze point heatmap leads to the performance bottleneck,\nwe propose an attention supervision mechanism to accelerate the learning of the\ngaze heatmap. Finally, we propose a novel evaluation metric, mean Similarity\nover Candidates (mSoC), for gaze object detection, which is more sensitive to\nvariations between bounding boxes. The experimental results on multiple\nbenchmark datasets demonstrate the effectiveness of our model in both gaze\nobject detection and gaze following tasks."
                },
                "authors": [
                    {
                        "name": "Yang Jin"
                    },
                    {
                        "name": "Guangyu Guo"
                    },
                    {
                        "name": "Binglu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Binglu Wang"
                },
                "author": "Binglu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25301v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25301v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25297v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25297v1",
                "updated": "2025-10-29T09:07:03Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    9,
                    7,
                    3,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T09:07:03Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    9,
                    7,
                    3,
                    2,
                    302,
                    0
                ],
                "title": "Understanding the Characteristics of LLM-Generated Property-Based Tests\n  in Exploring Edge Cases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the Characteristics of LLM-Generated Property-Based Tests\n  in Exploring Edge Cases"
                },
                "summary": "As Large Language Models (LLMs) increasingly generate code in software\ndevelopment, ensuring the quality of LLM-generated code has become important.\nTraditional testing approaches using Example-based Testing (EBT) often miss\nedge cases -- defects that occur at boundary values, special input patterns, or\nextreme conditions. This research investigates the characteristics of\nLLM-generated Property-based Testing (PBT) compared to EBT for exploring edge\ncases. We analyze 16 HumanEval problems where standard solutions failed on\nextended test cases, generating both PBT and EBT test codes using\nClaude-4-sonnet. Our experimental results reveal that while each method\nindividually achieved a 68.75\\% bug detection rate, combining both approaches\nimproved detection to 81.25\\%. The analysis demonstrates complementary\ncharacteristics: PBT effectively detects performance issues and edge cases\nthrough extensive input space exploration, while EBT effectively detects\nspecific boundary conditions and special patterns. These findings suggest that\na hybrid approach leveraging both testing methods can improve the reliability\nof LLM-generated code, providing guidance for test generation strategies in\nLLM-based code generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) increasingly generate code in software\ndevelopment, ensuring the quality of LLM-generated code has become important.\nTraditional testing approaches using Example-based Testing (EBT) often miss\nedge cases -- defects that occur at boundary values, special input patterns, or\nextreme conditions. This research investigates the characteristics of\nLLM-generated Property-based Testing (PBT) compared to EBT for exploring edge\ncases. We analyze 16 HumanEval problems where standard solutions failed on\nextended test cases, generating both PBT and EBT test codes using\nClaude-4-sonnet. Our experimental results reveal that while each method\nindividually achieved a 68.75\\% bug detection rate, combining both approaches\nimproved detection to 81.25\\%. The analysis demonstrates complementary\ncharacteristics: PBT effectively detects performance issues and edge cases\nthrough extensive input space exploration, while EBT effectively detects\nspecific boundary conditions and special patterns. These findings suggest that\na hybrid approach leveraging both testing methods can improve the reliability\nof LLM-generated code, providing guidance for test generation strategies in\nLLM-based code generation."
                },
                "authors": [
                    {
                        "name": "Hidetake Tanaka"
                    },
                    {
                        "name": "Haruto Tanaka"
                    },
                    {
                        "name": "Kazumasa Shimari"
                    },
                    {
                        "name": "Kenichi Matsumoto"
                    }
                ],
                "author_detail": {
                    "name": "Kenichi Matsumoto"
                },
                "author": "Kenichi Matsumoto",
                "arxiv_comment": "Accepted for publication in 2nd IEEE/ACM international conference on\n  AI-powered Software (AIware 2025) : 8 pages, 1 table, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25297v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25297v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25290v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25290v1",
                "updated": "2025-10-29T08:47:54Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    8,
                    47,
                    54,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T08:47:54Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    8,
                    47,
                    54,
                    2,
                    302,
                    0
                ],
                "title": "Fair Rate Maximization for Multi-user Multi-cell MISO Communication\n  Systems via Novel Transmissive RIS Transceiver",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fair Rate Maximization for Multi-user Multi-cell MISO Communication\n  Systems via Novel Transmissive RIS Transceiver"
                },
                "summary": "This paper explores a multi-cell multiple-input single-output (MISO) downlink\ncommunication system enabled by a unique transmissive reconfigurable\nintelligent surface (RIS) transceiver (TRTC) configuration. Within this system\nframework, we formulate an optimization problem for the purpose of maximizing\nthe minimum rate of users for each cell via designing the transmit beamforming\nof the TRTC, subject to the power constraints of each TRTC unit. Since the\nobjective function is non-differentiable, the max-min rate problem is difficult\nto solve. In order to tackle this challenging optimization problem, an\nefficient low-complexity optimization algorithm is developed. Specifically, the\nlog-form rate function is transformed into a tractable form by employing the\nfractional programming (FP) methodology. Next, the max-min objective function\ncan be approximated using a differentiable function derived from smooth\napproximation theory. Moreover, by applying the majorization-minimization (MM)\ntechnique and examining the optimality conditions, a solution is proposed that\nupdates all variables analytically without relying on any numerical solvers.\nNumerical results are presented to demonstrate the convergence and\neffectiveness of the proposed low-complexity algorithm. Additionally, the\nalgorithm can significantly reduce the computational complexity without\nperformance loss. Furthermore, the simulation results illustrate the clear\nsuperiority of the deployment of the TRTC over the benchmark schemes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores a multi-cell multiple-input single-output (MISO) downlink\ncommunication system enabled by a unique transmissive reconfigurable\nintelligent surface (RIS) transceiver (TRTC) configuration. Within this system\nframework, we formulate an optimization problem for the purpose of maximizing\nthe minimum rate of users for each cell via designing the transmit beamforming\nof the TRTC, subject to the power constraints of each TRTC unit. Since the\nobjective function is non-differentiable, the max-min rate problem is difficult\nto solve. In order to tackle this challenging optimization problem, an\nefficient low-complexity optimization algorithm is developed. Specifically, the\nlog-form rate function is transformed into a tractable form by employing the\nfractional programming (FP) methodology. Next, the max-min objective function\ncan be approximated using a differentiable function derived from smooth\napproximation theory. Moreover, by applying the majorization-minimization (MM)\ntechnique and examining the optimality conditions, a solution is proposed that\nupdates all variables analytically without relying on any numerical solvers.\nNumerical results are presented to demonstrate the convergence and\neffectiveness of the proposed low-complexity algorithm. Additionally, the\nalgorithm can significantly reduce the computational complexity without\nperformance loss. Furthermore, the simulation results illustrate the clear\nsuperiority of the deployment of the TRTC over the benchmark schemes."
                },
                "authors": [
                    {
                        "name": "Yuan Guo"
                    },
                    {
                        "name": "Wen Chen"
                    },
                    {
                        "name": "Qingqing Wu"
                    },
                    {
                        "name": "Zhendong Li"
                    },
                    {
                        "name": "Kunlun Wang"
                    },
                    {
                        "name": "Hongying Tang"
                    },
                    {
                        "name": "Jun Li"
                    }
                ],
                "author_detail": {
                    "name": "Jun Li"
                },
                "author": "Jun Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25290v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25290v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12593v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12593v3",
                "updated": "2025-10-29T08:45:38Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    8,
                    45,
                    38,
                    2,
                    302,
                    0
                ],
                "published": "2024-10-16T14:12:11Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    14,
                    12,
                    11,
                    2,
                    290,
                    0
                ],
                "title": "Expand and Compress: Exploring Tuning Principles for Continual\n  Spatio-Temporal Graph Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Expand and Compress: Exploring Tuning Principles for Continual\n  Spatio-Temporal Graph Forecasting"
                },
                "summary": "The widespread deployment of sensing devices leads to a surge in data for\nspatio-temporal forecasting applications such as traffic flow, air quality, and\nwind energy. Although spatio-temporal graph neural networks have achieved\nsuccess in modeling various static spatio-temporal forecasting scenarios,\nreal-world spatio-temporal data are typically received in a streaming manner,\nand the network continuously expands with the installation of new sensors.\nThus, spatio-temporal forecasting in streaming scenarios faces dual challenges:\nthe inefficiency of retraining models over newly arrived data and the\ndetrimental effects of catastrophic forgetting over long-term history. To\naddress these challenges, we propose a novel prompt tuning-based continuous\nforecasting method, following two fundamental tuning principles guided by\nempirical and theoretical analysis: expand and compress, which effectively\nresolve the aforementioned problems with lightweight tuning parameters.\nSpecifically, we integrate the base spatio-temporal graph neural network with a\ncontinuous prompt pool, utilizing stored prompts (i.e., few learnable\nparameters) in memory, and jointly optimize them with the base spatio-temporal\ngraph neural network. This method ensures that the model sequentially learns\nfrom the spatio-temporal data stream to accomplish tasks for corresponding\nperiods. Extensive experimental results on multiple real-world datasets\ndemonstrate the multi-faceted superiority of our method over the\nstate-of-the-art baselines, including effectiveness, efficiency, universality,\netc.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread deployment of sensing devices leads to a surge in data for\nspatio-temporal forecasting applications such as traffic flow, air quality, and\nwind energy. Although spatio-temporal graph neural networks have achieved\nsuccess in modeling various static spatio-temporal forecasting scenarios,\nreal-world spatio-temporal data are typically received in a streaming manner,\nand the network continuously expands with the installation of new sensors.\nThus, spatio-temporal forecasting in streaming scenarios faces dual challenges:\nthe inefficiency of retraining models over newly arrived data and the\ndetrimental effects of catastrophic forgetting over long-term history. To\naddress these challenges, we propose a novel prompt tuning-based continuous\nforecasting method, following two fundamental tuning principles guided by\nempirical and theoretical analysis: expand and compress, which effectively\nresolve the aforementioned problems with lightweight tuning parameters.\nSpecifically, we integrate the base spatio-temporal graph neural network with a\ncontinuous prompt pool, utilizing stored prompts (i.e., few learnable\nparameters) in memory, and jointly optimize them with the base spatio-temporal\ngraph neural network. This method ensures that the model sequentially learns\nfrom the spatio-temporal data stream to accomplish tasks for corresponding\nperiods. Extensive experimental results on multiple real-world datasets\ndemonstrate the multi-faceted superiority of our method over the\nstate-of-the-art baselines, including effectiveness, efficiency, universality,\netc."
                },
                "authors": [
                    {
                        "name": "Wei Chen"
                    },
                    {
                        "name": "Yuxuan Liang"
                    }
                ],
                "author_detail": {
                    "name": "Yuxuan Liang"
                },
                "author": "Yuxuan Liang",
                "arxiv_comment": "Accepted by ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12593v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12593v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25278v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25278v1",
                "updated": "2025-10-29T08:38:02Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    8,
                    38,
                    2,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T08:38:02Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    8,
                    38,
                    2,
                    2,
                    302,
                    0
                ],
                "title": "DIRC-RAG: Accelerating Edge RAG with Robust High-Density and\n  High-Loading-Bandwidth Digital In-ReRAM Computation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DIRC-RAG: Accelerating Edge RAG with Robust High-Density and\n  High-Loading-Bandwidth Digital In-ReRAM Computation"
                },
                "summary": "Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by\nintegrating external knowledge retrieval but faces challenges on edge devices\ndue to high storage, energy, and latency demands. Computing-in-Memory (CIM)\noffers a promising solution by storing document embeddings in CIM macros and\nenabling in-situ parallel retrievals but is constrained by either low memory\ndensity or limited computational accuracy. To address these challenges, we\npresent DIRCRAG, a novel edge RAG acceleration architecture leveraging Digital\nIn-ReRAM Computation (DIRC). DIRC integrates a high-density multi-level ReRAM\nsubarray with an SRAM cell, utilizing SRAM and differential sensing for robust\nReRAM readout and digital multiply-accumulate (MAC) operations. By storing all\ndocument embeddings within the CIM macro, DIRC achieves ultra-low-power,\nsingle-cycle data loading, substantially reducing both energy consumption and\nlatency compared to offchip DRAM. A query-stationary (QS) dataflow is supported\nfor RAG tasks, minimizing on-chip data movement and reducing SRAM buffer\nrequirements. We introduce error optimization for the DIRC ReRAM-SRAM cell by\nextracting the bit-wise spatial error distribution of the ReRAM subarray and\napplying targeted bit-wise data remapping. An error detection circuit is also\nimplemented to enhance readout resilience against deviceand circuit-level\nvariations. Simulation results demonstrate that DIRC-RAG under TSMC40nm process\nachieves an on-chip non-volatile memory density of 5.18Mb/mm2 and a throughput\nof 131 TOPS. It delivers a 4MB retrieval latency of 5.6{\\mu}s/query and an\nenergy consumption of 0.956{\\mu}J/query, while maintaining the retrieval\nprecision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by\nintegrating external knowledge retrieval but faces challenges on edge devices\ndue to high storage, energy, and latency demands. Computing-in-Memory (CIM)\noffers a promising solution by storing document embeddings in CIM macros and\nenabling in-situ parallel retrievals but is constrained by either low memory\ndensity or limited computational accuracy. To address these challenges, we\npresent DIRCRAG, a novel edge RAG acceleration architecture leveraging Digital\nIn-ReRAM Computation (DIRC). DIRC integrates a high-density multi-level ReRAM\nsubarray with an SRAM cell, utilizing SRAM and differential sensing for robust\nReRAM readout and digital multiply-accumulate (MAC) operations. By storing all\ndocument embeddings within the CIM macro, DIRC achieves ultra-low-power,\nsingle-cycle data loading, substantially reducing both energy consumption and\nlatency compared to offchip DRAM. A query-stationary (QS) dataflow is supported\nfor RAG tasks, minimizing on-chip data movement and reducing SRAM buffer\nrequirements. We introduce error optimization for the DIRC ReRAM-SRAM cell by\nextracting the bit-wise spatial error distribution of the ReRAM subarray and\napplying targeted bit-wise data remapping. An error detection circuit is also\nimplemented to enhance readout resilience against deviceand circuit-level\nvariations. Simulation results demonstrate that DIRC-RAG under TSMC40nm process\nachieves an on-chip non-volatile memory density of 5.18Mb/mm2 and a throughput\nof 131 TOPS. It delivers a 4MB retrieval latency of 5.6{\\mu}s/query and an\nenergy consumption of 0.956{\\mu}J/query, while maintaining the retrieval\nprecision."
                },
                "authors": [
                    {
                        "name": "Kunming Shao"
                    },
                    {
                        "name": "Zhipeng Liao"
                    },
                    {
                        "name": "Jiangnan Yu"
                    },
                    {
                        "name": "Liang Zhao"
                    },
                    {
                        "name": "Qiwei Li"
                    },
                    {
                        "name": "Xijie Huang"
                    },
                    {
                        "name": "Jingyu He"
                    },
                    {
                        "name": "Fengshi Tian"
                    },
                    {
                        "name": "Yi Zou"
                    },
                    {
                        "name": "Xiaomeng Wang"
                    },
                    {
                        "name": "Tim Kwang-Ting Cheng"
                    },
                    {
                        "name": "Chi-Ying Tsui"
                    }
                ],
                "author_detail": {
                    "name": "Chi-Ying Tsui"
                },
                "author": "Chi-Ying Tsui",
                "arxiv_comment": "Accepted by 2025 IEEE/ACM ISLPED",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25278v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25278v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25273v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25273v1",
                "updated": "2025-10-29T08:32:22Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    8,
                    32,
                    22,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T08:32:22Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    8,
                    32,
                    22,
                    2,
                    302,
                    0
                ],
                "title": "Adapting Small Language Models to Low-Resource Domains: A Case Study in\n  Hindi Tourism QA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adapting Small Language Models to Low-Resource Domains: A Case Study in\n  Hindi Tourism QA"
                },
                "summary": "Domain-specific question answering in low-resource languages faces two key\nchallenges: scarcity of annotated datasets and limited domain knowledge in\ngeneral-purpose language models. In this work, we present a multi-stage\nfinetuning strategy to adapt lightweight language models to the Hindi tourism\ndomain by leveraging both original and synthetic training data. Synthetic\nquestion-answer pairs are generated using large LLMs (LLaMA-70B, Phi-14B) and\nused to augment the limited original dataset. We explore several training\nmethodologies and analyse their impact on domain generalisation. Our results\ndemonstrate that large models can efficiently generate synthetic data, while\nsmall models can effectively adapt to it, offering a scalable pathway for\nlow-resource, domain-specific QA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Domain-specific question answering in low-resource languages faces two key\nchallenges: scarcity of annotated datasets and limited domain knowledge in\ngeneral-purpose language models. In this work, we present a multi-stage\nfinetuning strategy to adapt lightweight language models to the Hindi tourism\ndomain by leveraging both original and synthetic training data. Synthetic\nquestion-answer pairs are generated using large LLMs (LLaMA-70B, Phi-14B) and\nused to augment the limited original dataset. We explore several training\nmethodologies and analyse their impact on domain generalisation. Our results\ndemonstrate that large models can efficiently generate synthetic data, while\nsmall models can effectively adapt to it, offering a scalable pathway for\nlow-resource, domain-specific QA."
                },
                "authors": [
                    {
                        "name": "Sandipan Majhi"
                    },
                    {
                        "name": "Paheli Bhattacharya"
                    }
                ],
                "author_detail": {
                    "name": "Paheli Bhattacharya"
                },
                "author": "Paheli Bhattacharya",
                "arxiv_comment": "Accepted at the Forum for Information Retrieval Evaluation 2025\n  (VATIKA Track)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25273v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25273v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.00568v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.00568v2",
                "updated": "2025-10-29T08:22:54Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    8,
                    22,
                    54,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-01T06:44:28Z",
                "published_parsed": [
                    2025,
                    10,
                    1,
                    6,
                    44,
                    28,
                    2,
                    274,
                    0
                ],
                "title": "ReSeek: A Self-Correcting Framework for Search Agents with Instructive\n  Rewards",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReSeek: A Self-Correcting Framework for Search Agents with Instructive\n  Rewards"
                },
                "summary": "Search agents powered by Large Language Models (LLMs) have demonstrated\nsignificant potential in tackling knowledge-intensive tasks. Reinforcement\nlearning (RL) has emerged as a powerful paradigm for training these agents to\nperform complex, multi-step reasoning. However, prior RL-based methods often\nrely on sparse or rule-based rewards, which can lead agents to commit to\nsuboptimal or erroneous reasoning paths without the ability to recover. To\naddress these limitations, we propose ReSeek, a novel self-correcting framework\nfor training search agents. Our framework introduces a self-correction\nmechanism that empowers the agent to dynamically identify and recover from\nerroneous search paths during an episode. By invoking a special JUDGE action,\nthe agent can judge the information and re-plan its search strategy. To guide\nthis process, we design a dense, instructive process reward function, which\ndecomposes into a correctness reward for retrieving factual information and a\nutility reward for finding information genuinely useful for the query.\nFurthermore, to mitigate the risk of data contamination in existing datasets,\nwe introduce FictionalHot, a new and challenging benchmark with recently\ncurated questions requiring complex reasoning. Being intuitively reasonable and\npractically simple, extensive experiments show that agents trained with ReSeek\nsignificantly outperform SOTA baselines in task success rate and path\nfaithfulness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Search agents powered by Large Language Models (LLMs) have demonstrated\nsignificant potential in tackling knowledge-intensive tasks. Reinforcement\nlearning (RL) has emerged as a powerful paradigm for training these agents to\nperform complex, multi-step reasoning. However, prior RL-based methods often\nrely on sparse or rule-based rewards, which can lead agents to commit to\nsuboptimal or erroneous reasoning paths without the ability to recover. To\naddress these limitations, we propose ReSeek, a novel self-correcting framework\nfor training search agents. Our framework introduces a self-correction\nmechanism that empowers the agent to dynamically identify and recover from\nerroneous search paths during an episode. By invoking a special JUDGE action,\nthe agent can judge the information and re-plan its search strategy. To guide\nthis process, we design a dense, instructive process reward function, which\ndecomposes into a correctness reward for retrieving factual information and a\nutility reward for finding information genuinely useful for the query.\nFurthermore, to mitigate the risk of data contamination in existing datasets,\nwe introduce FictionalHot, a new and challenging benchmark with recently\ncurated questions requiring complex reasoning. Being intuitively reasonable and\npractically simple, extensive experiments show that agents trained with ReSeek\nsignificantly outperform SOTA baselines in task success rate and path\nfaithfulness."
                },
                "authors": [
                    {
                        "name": "Shiyu Li"
                    },
                    {
                        "name": "Yang Tang"
                    },
                    {
                        "name": "Yifan Wang"
                    },
                    {
                        "name": "Peiming Li"
                    },
                    {
                        "name": "Xi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xi Chen"
                },
                "author": "Xi Chen",
                "arxiv_comment": "19 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.00568v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.00568v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05493v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05493v2",
                "updated": "2025-10-29T08:19:03Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    8,
                    19,
                    3,
                    2,
                    302,
                    0
                ],
                "published": "2025-03-07T15:05:23Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    15,
                    5,
                    23,
                    4,
                    66,
                    0
                ],
                "title": "Can LLMs Outshine Conventional Recommenders? A Comparative Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Outshine Conventional Recommenders? A Comparative Evaluation"
                },
                "summary": "In recent years, integrating large language models (LLMs) into recommender\nsystems has created new opportunities for improving recommendation quality.\nHowever, a comprehensive benchmark is needed to thoroughly evaluate and compare\nthe recommendation capabilities of LLMs with traditional recommender systems.\nIn this paper, we introduce RecBench, which systematically investigates various\nitem representation forms (including unique identifier, text, semantic\nembedding, and semantic identifier) and evaluates two primary recommendation\ntasks, i.e., click-through rate prediction (CTR) and sequential recommendation\n(SeqRec). Our extensive experiments cover up to 17 large models and are\nconducted across five diverse datasets from fashion, news, video, books, and\nmusic domains. Our findings indicate that LLM-based recommenders outperform\nconventional recommenders, achieving up to a 5% AUC improvement in the CTR\nscenario and up to a 170% NDCG@10 improvement in the SeqRec scenario. However,\nthese substantial performance gains come at the expense of significantly\nreduced inference efficiency, rendering the LLM-as-RS paradigm impractical for\nreal-time recommendation environments. We aim for our findings to inspire\nfuture research, including recommendation-specific model acceleration methods.\nWe will release our code, data, configurations, and platform to enable other\nresearchers to reproduce and build upon our experimental results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, integrating large language models (LLMs) into recommender\nsystems has created new opportunities for improving recommendation quality.\nHowever, a comprehensive benchmark is needed to thoroughly evaluate and compare\nthe recommendation capabilities of LLMs with traditional recommender systems.\nIn this paper, we introduce RecBench, which systematically investigates various\nitem representation forms (including unique identifier, text, semantic\nembedding, and semantic identifier) and evaluates two primary recommendation\ntasks, i.e., click-through rate prediction (CTR) and sequential recommendation\n(SeqRec). Our extensive experiments cover up to 17 large models and are\nconducted across five diverse datasets from fashion, news, video, books, and\nmusic domains. Our findings indicate that LLM-based recommenders outperform\nconventional recommenders, achieving up to a 5% AUC improvement in the CTR\nscenario and up to a 170% NDCG@10 improvement in the SeqRec scenario. However,\nthese substantial performance gains come at the expense of significantly\nreduced inference efficiency, rendering the LLM-as-RS paradigm impractical for\nreal-time recommendation environments. We aim for our findings to inspire\nfuture research, including recommendation-specific model acceleration methods.\nWe will release our code, data, configurations, and platform to enable other\nresearchers to reproduce and build upon our experimental results."
                },
                "authors": [
                    {
                        "name": "Qijiong Liu"
                    },
                    {
                        "name": "Jieming Zhu"
                    },
                    {
                        "name": "Lu Fan"
                    },
                    {
                        "name": "Kun Wang"
                    },
                    {
                        "name": "Hengchang Hu"
                    },
                    {
                        "name": "Wei Guo"
                    },
                    {
                        "name": "Yong Liu"
                    },
                    {
                        "name": "Xiao-Ming Wu"
                    }
                ],
                "author_detail": {
                    "name": "Xiao-Ming Wu"
                },
                "author": "Xiao-Ming Wu",
                "arxiv_comment": "NeurIPS 2025 DB Track Accepted Paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05493v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05493v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25258v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25258v1",
                "updated": "2025-10-29T08:13:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    8,
                    13,
                    47,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T08:13:47Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    8,
                    13,
                    47,
                    2,
                    302,
                    0
                ],
                "title": "MoEntwine: Unleashing the Potential of Wafer-scale Chips for Large-scale\n  Expert Parallel Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoEntwine: Unleashing the Potential of Wafer-scale Chips for Large-scale\n  Expert Parallel Inference"
                },
                "summary": "As large language models (LLMs) continue to scale up, mixture-of-experts\n(MoE) has become a common technology in SOTA models. MoE models rely on expert\nparallelism (EP) to alleviate memory bottleneck, which introduces all-to-all\ncommunication to dispatch and combine tokens across devices. However, in\nwidely-adopted GPU clusters, high-overhead cross-node communication makes\nall-to-all expensive, hindering the adoption of EP. Recently, wafer-scale chips\n(WSCs) have emerged as a platform integrating numerous devices on a wafer-sized\ninterposer. WSCs provide a unified high-performance network connecting all\ndevices, presenting a promising potential for hosting MoE models. Yet, their\nnetwork is restricted to a mesh topology, causing imbalanced communication\npressure and performance loss. Moreover, the lack of on-wafer disk leads to\nhigh-overhead expert migration on the critical path.\n  To fully unleash this potential, we first propose Entwined Ring Mapping\n(ER-Mapping), which co-designs the mapping of attention and MoE layers to\nbalance communication pressure and achieve better performance. We find that\nunder ER-Mapping, the distribution of cold and hot links in the attention and\nMoE layers is complementary. Therefore, to hide the migration overhead, we\npropose the Non-invasive Balancer (NI-Balancer), which splits a complete expert\nmigration into multiple steps and alternately utilizes the cold links of both\nlayers. Evaluation shows ER-Mapping achieves communication reduction up to 62%.\nNI-Balancer further delivers 54% and 22% improvements in MoE computation and\ncommunication, respectively. Compared with the SOTA NVL72 supernode, the WSC\nplatform delivers an average 39% higher per-device MoE performance owing to its\nscalability to larger EP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) continue to scale up, mixture-of-experts\n(MoE) has become a common technology in SOTA models. MoE models rely on expert\nparallelism (EP) to alleviate memory bottleneck, which introduces all-to-all\ncommunication to dispatch and combine tokens across devices. However, in\nwidely-adopted GPU clusters, high-overhead cross-node communication makes\nall-to-all expensive, hindering the adoption of EP. Recently, wafer-scale chips\n(WSCs) have emerged as a platform integrating numerous devices on a wafer-sized\ninterposer. WSCs provide a unified high-performance network connecting all\ndevices, presenting a promising potential for hosting MoE models. Yet, their\nnetwork is restricted to a mesh topology, causing imbalanced communication\npressure and performance loss. Moreover, the lack of on-wafer disk leads to\nhigh-overhead expert migration on the critical path.\n  To fully unleash this potential, we first propose Entwined Ring Mapping\n(ER-Mapping), which co-designs the mapping of attention and MoE layers to\nbalance communication pressure and achieve better performance. We find that\nunder ER-Mapping, the distribution of cold and hot links in the attention and\nMoE layers is complementary. Therefore, to hide the migration overhead, we\npropose the Non-invasive Balancer (NI-Balancer), which splits a complete expert\nmigration into multiple steps and alternately utilizes the cold links of both\nlayers. Evaluation shows ER-Mapping achieves communication reduction up to 62%.\nNI-Balancer further delivers 54% and 22% improvements in MoE computation and\ncommunication, respectively. Compared with the SOTA NVL72 supernode, the WSC\nplatform delivers an average 39% higher per-device MoE performance owing to its\nscalability to larger EP."
                },
                "authors": [
                    {
                        "name": "Xinru Tang"
                    },
                    {
                        "name": "Jingxiang Hou"
                    },
                    {
                        "name": "Dingcheng Jiang"
                    },
                    {
                        "name": "Taiquan Wei"
                    },
                    {
                        "name": "Jiaxin Liu"
                    },
                    {
                        "name": "Jinyi Deng"
                    },
                    {
                        "name": "Huizheng Wang"
                    },
                    {
                        "name": "Qize Yang"
                    },
                    {
                        "name": "Haoran Shang"
                    },
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "Yang Hu"
                    },
                    {
                        "name": "Shouyi Yin"
                    }
                ],
                "author_detail": {
                    "name": "Shouyi Yin"
                },
                "author": "Shouyi Yin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25258v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25258v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25257v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25257v1",
                "updated": "2025-10-29T08:13:17Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    8,
                    13,
                    17,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T08:13:17Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    8,
                    13,
                    17,
                    2,
                    302,
                    0
                ],
                "title": "RT-DETRv4: Painlessly Furthering Real-Time Object Detection with Vision\n  Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RT-DETRv4: Painlessly Furthering Real-Time Object Detection with Vision\n  Foundation Models"
                },
                "summary": "Real-time object detection has achieved substantial progress through\nmeticulously designed architectures and optimization strategies. However, the\npursuit of high-speed inference via lightweight network designs often leads to\ndegraded feature representation, which hinders further performance improvements\nand practical on-device deployment. In this paper, we propose a cost-effective\nand highly adaptable distillation framework that harnesses the rapidly evolving\ncapabilities of Vision Foundation Models (VFMs) to enhance lightweight object\ndetectors. Given the significant architectural and learning objective\ndisparities between VFMs and resource-constrained detectors, achieving stable\nand task-aligned semantic transfer is challenging. To address this, on one\nhand, we introduce a Deep Semantic Injector (DSI) module that facilitates the\nintegration of high-level representations from VFMs into the deep layers of the\ndetector. On the other hand, we devise a Gradient-guided Adaptive Modulation\n(GAM) strategy, which dynamically adjusts the intensity of semantic transfer\nbased on gradient norm ratios. Without increasing deployment and inference\noverhead, our approach painlessly delivers striking and consistent performance\ngains across diverse DETR-based models, underscoring its practical utility for\nreal-time detection. Our new model family, RT-DETRv4, achieves state-of-the-art\nresults on COCO, attaining AP scores of 49.7/53.5/55.4/57.0 at corresponding\nspeeds of 273/169/124/78 FPS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time object detection has achieved substantial progress through\nmeticulously designed architectures and optimization strategies. However, the\npursuit of high-speed inference via lightweight network designs often leads to\ndegraded feature representation, which hinders further performance improvements\nand practical on-device deployment. In this paper, we propose a cost-effective\nand highly adaptable distillation framework that harnesses the rapidly evolving\ncapabilities of Vision Foundation Models (VFMs) to enhance lightweight object\ndetectors. Given the significant architectural and learning objective\ndisparities between VFMs and resource-constrained detectors, achieving stable\nand task-aligned semantic transfer is challenging. To address this, on one\nhand, we introduce a Deep Semantic Injector (DSI) module that facilitates the\nintegration of high-level representations from VFMs into the deep layers of the\ndetector. On the other hand, we devise a Gradient-guided Adaptive Modulation\n(GAM) strategy, which dynamically adjusts the intensity of semantic transfer\nbased on gradient norm ratios. Without increasing deployment and inference\noverhead, our approach painlessly delivers striking and consistent performance\ngains across diverse DETR-based models, underscoring its practical utility for\nreal-time detection. Our new model family, RT-DETRv4, achieves state-of-the-art\nresults on COCO, attaining AP scores of 49.7/53.5/55.4/57.0 at corresponding\nspeeds of 273/169/124/78 FPS."
                },
                "authors": [
                    {
                        "name": "Zijun Liao"
                    },
                    {
                        "name": "Yian Zhao"
                    },
                    {
                        "name": "Xin Shan"
                    },
                    {
                        "name": "Yu Yan"
                    },
                    {
                        "name": "Chang Liu"
                    },
                    {
                        "name": "Lei Lu"
                    },
                    {
                        "name": "Xiangyang Ji"
                    },
                    {
                        "name": "Jie Chen"
                    }
                ],
                "author_detail": {
                    "name": "Jie Chen"
                },
                "author": "Jie Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25257v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25257v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14257v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14257v2",
                "updated": "2025-10-29T07:56:51Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    7,
                    56,
                    51,
                    2,
                    302,
                    0
                ],
                "published": "2024-10-18T08:05:37Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    8,
                    5,
                    37,
                    4,
                    292,
                    0
                ],
                "title": "Revisiting Service Level Objectives and System Level Metrics in Large\n  Language Model Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revisiting Service Level Objectives and System Level Metrics in Large\n  Language Model Serving"
                },
                "summary": "User experience is a critical factor Large Language Model (LLM) serving\nsystems must consider, where service level objectives (SLOs) considering the\nexperience of individual requests and system level metrics (SLMs) considering\nthe overall system performance are two key performance measures. However, we\nobserve two notable issues in existing metrics: 1) manually delaying the\ndelivery of some tokens can improve SLOs, and 2) actively abandoning requests\nthat do not meet SLOs can improve SLMs, both of which are counterintuitive.\n  In this paper, we revisit SLOs and SLMs in LLM serving, and propose a new SLO\nthat aligns with user experience. Based on the SLO, we propose a comprehensive\nmetric framework called smooth goodput, which integrates SLOs and SLMs to\nreflect the nature of user experience in LLM serving. Through this unified\nframework, we reassess the performance of different LLM serving systems under\nmultiple workloads. Evaluation results show that our metric framework provides\na more comprehensive view of token delivery and request processing, and\neffectively captures the optimal point of user experience and system\nperformance with different serving strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "User experience is a critical factor Large Language Model (LLM) serving\nsystems must consider, where service level objectives (SLOs) considering the\nexperience of individual requests and system level metrics (SLMs) considering\nthe overall system performance are two key performance measures. However, we\nobserve two notable issues in existing metrics: 1) manually delaying the\ndelivery of some tokens can improve SLOs, and 2) actively abandoning requests\nthat do not meet SLOs can improve SLMs, both of which are counterintuitive.\n  In this paper, we revisit SLOs and SLMs in LLM serving, and propose a new SLO\nthat aligns with user experience. Based on the SLO, we propose a comprehensive\nmetric framework called smooth goodput, which integrates SLOs and SLMs to\nreflect the nature of user experience in LLM serving. Through this unified\nframework, we reassess the performance of different LLM serving systems under\nmultiple workloads. Evaluation results show that our metric framework provides\na more comprehensive view of token delivery and request processing, and\neffectively captures the optimal point of user experience and system\nperformance with different serving strategies."
                },
                "authors": [
                    {
                        "name": "Zhibin Wang"
                    },
                    {
                        "name": "Shipeng Li"
                    },
                    {
                        "name": "Yuhang Zhou"
                    },
                    {
                        "name": "Xue Li"
                    },
                    {
                        "name": "Zhonghui Zhang"
                    },
                    {
                        "name": "Nguyen Cam-Tu"
                    },
                    {
                        "name": "Rong Gu"
                    },
                    {
                        "name": "Chen Tian"
                    },
                    {
                        "name": "Guihai Chen"
                    },
                    {
                        "name": "Sheng Zhong"
                    }
                ],
                "author_detail": {
                    "name": "Sheng Zhong"
                },
                "author": "Sheng Zhong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14257v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14257v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10258v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10258v2",
                "updated": "2025-10-29T07:51:00Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    7,
                    51,
                    0,
                    2,
                    302,
                    0
                ],
                "published": "2025-04-14T14:19:57Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    14,
                    19,
                    57,
                    0,
                    104,
                    0
                ],
                "title": "XY-Cut++: Advanced Layout Ordering via Hierarchical Mask Mechanism on a\n  Novel Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XY-Cut++: Advanced Layout Ordering via Hierarchical Mask Mechanism on a\n  Novel Benchmark"
                },
                "summary": "Document Reading Order Recovery is a fundamental task in document image\nunderstanding, playing a pivotal role in enhancing Retrieval-Augmented\nGeneration (RAG) and serving as a critical preprocessing step for large\nlanguage models (LLMs). Existing methods often struggle with complex\nlayouts(e.g., multi-column newspapers), high-overhead interactions between\ncross-modal elements (visual regions and textual semantics), and a lack of\nrobust evaluation benchmarks. We introduce XY-Cut++, an advanced layout\nordering method that integrates pre-mask processing, multi-granularity\nsegmentation, and cross-modal matching to address these challenges. Our method\nsignificantly enhances layout ordering accuracy compared to traditional XY-Cut\ntechniques. Specifically, XY-Cut++ achieves state-of-the-art performance (98.8\nBLEU overall) while maintaining simplicity and efficiency. It outperforms\nexisting baselines by up to 24\\% and demonstrates consistent accuracy across\nsimple and complex layouts on the newly introduced DocBench-100 dataset. This\nadvancement establishes a reliable foundation for document structure recovery,\nsetting a new standard for layout ordering tasks and facilitating more\neffective RAG and LLM preprocessing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Document Reading Order Recovery is a fundamental task in document image\nunderstanding, playing a pivotal role in enhancing Retrieval-Augmented\nGeneration (RAG) and serving as a critical preprocessing step for large\nlanguage models (LLMs). Existing methods often struggle with complex\nlayouts(e.g., multi-column newspapers), high-overhead interactions between\ncross-modal elements (visual regions and textual semantics), and a lack of\nrobust evaluation benchmarks. We introduce XY-Cut++, an advanced layout\nordering method that integrates pre-mask processing, multi-granularity\nsegmentation, and cross-modal matching to address these challenges. Our method\nsignificantly enhances layout ordering accuracy compared to traditional XY-Cut\ntechniques. Specifically, XY-Cut++ achieves state-of-the-art performance (98.8\nBLEU overall) while maintaining simplicity and efficiency. It outperforms\nexisting baselines by up to 24\\% and demonstrates consistent accuracy across\nsimple and complex layouts on the newly introduced DocBench-100 dataset. This\nadvancement establishes a reliable foundation for document structure recovery,\nsetting a new standard for layout ordering tasks and facilitating more\neffective RAG and LLM preprocessing."
                },
                "authors": [
                    {
                        "name": "Shuai Liu"
                    },
                    {
                        "name": "Youmeng Li"
                    },
                    {
                        "name": "Jizeng Wei"
                    }
                ],
                "author_detail": {
                    "name": "Jizeng Wei"
                },
                "author": "Jizeng Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10258v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10258v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.22967v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.22967v2",
                "updated": "2025-10-29T07:50:03Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    7,
                    50,
                    3,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-27T03:41:32Z",
                "published_parsed": [
                    2025,
                    10,
                    27,
                    3,
                    41,
                    32,
                    0,
                    300,
                    0
                ],
                "title": "MAD-Fact: A Multi-Agent Debate Framework for Long-Form Factuality\n  Evaluation in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAD-Fact: A Multi-Agent Debate Framework for Long-Form Factuality\n  Evaluation in LLMs"
                },
                "summary": "The widespread adoption of Large Language Models (LLMs) raises critical\nconcerns about the factual accuracy of their outputs, especially in high-risk\ndomains such as biomedicine, law, and education. Existing evaluation methods\nfor short texts often fail on long-form content due to complex reasoning\nchains, intertwined perspectives, and cumulative information. To address this,\nwe propose a systematic approach integrating large-scale long-form datasets,\nmulti-agent verification mechanisms, and weighted evaluation metrics. We\nconstruct LongHalluQA, a Chinese long-form factuality dataset; and develop\nMAD-Fact, a debate-based multi-agent verification system. We introduce a fact\nimportance hierarchy to capture the varying significance of claims in long-form\ntexts. Experiments on two benchmarks show that larger LLMs generally maintain\nhigher factual consistency, while domestic models excel on Chinese content. Our\nwork provides a structured framework for evaluating and enhancing factual\nreliability in long-form LLM outputs, guiding their safe deployment in\nsensitive domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread adoption of Large Language Models (LLMs) raises critical\nconcerns about the factual accuracy of their outputs, especially in high-risk\ndomains such as biomedicine, law, and education. Existing evaluation methods\nfor short texts often fail on long-form content due to complex reasoning\nchains, intertwined perspectives, and cumulative information. To address this,\nwe propose a systematic approach integrating large-scale long-form datasets,\nmulti-agent verification mechanisms, and weighted evaluation metrics. We\nconstruct LongHalluQA, a Chinese long-form factuality dataset; and develop\nMAD-Fact, a debate-based multi-agent verification system. We introduce a fact\nimportance hierarchy to capture the varying significance of claims in long-form\ntexts. Experiments on two benchmarks show that larger LLMs generally maintain\nhigher factual consistency, while domestic models excel on Chinese content. Our\nwork provides a structured framework for evaluating and enhancing factual\nreliability in long-form LLM outputs, guiding their safe deployment in\nsensitive domains."
                },
                "authors": [
                    {
                        "name": "Yucheng Ning"
                    },
                    {
                        "name": "Xixun Lin"
                    },
                    {
                        "name": "Fang Fang"
                    },
                    {
                        "name": "Yanan Cao"
                    }
                ],
                "author_detail": {
                    "name": "Yanan Cao"
                },
                "author": "Yanan Cao",
                "arxiv_comment": "The article has been accepted by Frontiers of Computer Science (FCS),\n  with the DOI: {10.1007/s11704-025-51369-x}",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.22967v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.22967v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15356v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15356v2",
                "updated": "2025-10-29T07:34:05Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    7,
                    34,
                    5,
                    2,
                    302,
                    0
                ],
                "published": "2025-05-21T10:38:50Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    10,
                    38,
                    50,
                    2,
                    141,
                    0
                ],
                "title": "NL-Debugging: Exploiting Natural Language as an Intermediate\n  Representation for Code Debugging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NL-Debugging: Exploiting Natural Language as an Intermediate\n  Representation for Code Debugging"
                },
                "summary": "Debugging is a critical aspect of LLM's coding ability. Early debugging\nefforts primarily focused on code-level analysis, which often falls short when\naddressing complex programming errors that require a deeper understanding of\nalgorithmic logic. Recent advancements in large language models (LLMs) have\nshifted attention toward leveraging natural language reasoning to enhance\ncode-related tasks. However, two fundamental questions remain unanswered: What\ntype of natural language format is most effective for debugging tasks? And what\nspecific benefits does natural language reasoning bring to the debugging\nprocess? In this paper, we introduce NL-DEBUGGING, a novel framework that\nemploys natural language as an intermediate representation to improve code\ndebugging. By debugging at a natural language level, we demonstrate that\nNL-DEBUGGING outperforms traditional debugging methods and enables a broader\nmodification space through direct refinement guided by execution feedback. Our\nfindings highlight the potential of natural language reasoning to advance\nautomated code debugging and address complex programming challenges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Debugging is a critical aspect of LLM's coding ability. Early debugging\nefforts primarily focused on code-level analysis, which often falls short when\naddressing complex programming errors that require a deeper understanding of\nalgorithmic logic. Recent advancements in large language models (LLMs) have\nshifted attention toward leveraging natural language reasoning to enhance\ncode-related tasks. However, two fundamental questions remain unanswered: What\ntype of natural language format is most effective for debugging tasks? And what\nspecific benefits does natural language reasoning bring to the debugging\nprocess? In this paper, we introduce NL-DEBUGGING, a novel framework that\nemploys natural language as an intermediate representation to improve code\ndebugging. By debugging at a natural language level, we demonstrate that\nNL-DEBUGGING outperforms traditional debugging methods and enables a broader\nmodification space through direct refinement guided by execution feedback. Our\nfindings highlight the potential of natural language reasoning to advance\nautomated code debugging and address complex programming challenges."
                },
                "authors": [
                    {
                        "name": "Weiming Zhang"
                    },
                    {
                        "name": "Qingyao Li"
                    },
                    {
                        "name": "Xinyi Dai"
                    },
                    {
                        "name": "Jizheng Chen"
                    },
                    {
                        "name": "Kounianhua Du"
                    },
                    {
                        "name": "Weiwen Liu"
                    },
                    {
                        "name": "Yasheng Wang"
                    },
                    {
                        "name": "Ruiming Tang"
                    },
                    {
                        "name": "Yong Yu"
                    },
                    {
                        "name": "Weinan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Weinan Zhang"
                },
                "author": "Weinan Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15356v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15356v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19902v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19902v2",
                "updated": "2025-10-29T07:17:51Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    7,
                    17,
                    51,
                    2,
                    302,
                    0
                ],
                "published": "2025-09-24T08:56:32Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    8,
                    56,
                    32,
                    2,
                    267,
                    0
                ],
                "title": "WEST: LLM based Speech Toolkit for Speech Understanding, Generation, and\n  Interaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WEST: LLM based Speech Toolkit for Speech Understanding, Generation, and\n  Interaction"
                },
                "summary": "In this paper, we present WEST(WE Speech Toolkit), a speech toolkit based on\na large language model (LLM) for speech understanding, generation, and\ninteraction. There are three key features of WEST: 1) Fully LLM-based: Standing\non the shoulders of giants by reusing mature architectures, ecosystems (e.g.,\nHugging Face), and methods (e.g., sequence packing) from large models. 2)\nFull-stack: Supports tasks such as recognition, synthesis, understanding,\ndialogue, and multimodal capabilities, with extensibility to incorporate\nopen-source models. 3) Simple and Stupid: A simple and stupid speech toolkit\nthat everyone can Touch. In addition, WEST provides two types of recipes,\nmodels, and experimental results. The first is entirely based on open-source\nmodels and open-source data, allowing users to fully reproduce the experiments\nin this paper and serving as a verification system or minimal system baseline.\nThe second is trained on massive data, offering superior performance so the\nuser can directly apply it out of the box. WEST is publicly avilable at\nhttps://github.com/wenet-e2e/west/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present WEST(WE Speech Toolkit), a speech toolkit based on\na large language model (LLM) for speech understanding, generation, and\ninteraction. There are three key features of WEST: 1) Fully LLM-based: Standing\non the shoulders of giants by reusing mature architectures, ecosystems (e.g.,\nHugging Face), and methods (e.g., sequence packing) from large models. 2)\nFull-stack: Supports tasks such as recognition, synthesis, understanding,\ndialogue, and multimodal capabilities, with extensibility to incorporate\nopen-source models. 3) Simple and Stupid: A simple and stupid speech toolkit\nthat everyone can Touch. In addition, WEST provides two types of recipes,\nmodels, and experimental results. The first is entirely based on open-source\nmodels and open-source data, allowing users to fully reproduce the experiments\nin this paper and serving as a verification system or minimal system baseline.\nThe second is trained on massive data, offering superior performance so the\nuser can directly apply it out of the box. WEST is publicly avilable at\nhttps://github.com/wenet-e2e/west/"
                },
                "authors": [
                    {
                        "name": "Binbin Zhang"
                    },
                    {
                        "name": "Chengdong Liang"
                    },
                    {
                        "name": "Shuai Wang"
                    },
                    {
                        "name": "Xuelong Geng"
                    },
                    {
                        "name": "Zhao Guo"
                    },
                    {
                        "name": "Haoyu Li"
                    },
                    {
                        "name": "Hao Yin"
                    },
                    {
                        "name": "Xipeng Yang"
                    },
                    {
                        "name": "Pengshen Zhang"
                    },
                    {
                        "name": "Changwei Ma"
                    },
                    {
                        "name": "Lei Xie"
                    }
                ],
                "author_detail": {
                    "name": "Lei Xie"
                },
                "author": "Lei Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19902v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19902v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]