[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2508.02558v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02558v2",
                "updated": "2025-11-05T14:29:12Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    14,
                    29,
                    12,
                    2,
                    309,
                    0
                ],
                "published": "2025-08-04T16:14:03Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    16,
                    14,
                    3,
                    0,
                    216,
                    0
                ],
                "title": "Sparse-dLLM: Accelerating Diffusion LLMs with Dynamic Cache Eviction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse-dLLM: Accelerating Diffusion LLMs with Dynamic Cache Eviction"
                },
                "summary": "Diffusion Large Language Models (dLLMs) enable breakthroughs in reasoning and\nparallel decoding but suffer from prohibitive quadratic computational\ncomplexity and memory overhead during inference. Current caching techniques\naccelerate decoding by storing full-layer states, yet impose substantial memory\nusage that limit long-context applications. Our analysis of attention patterns\nin dLLMs reveals persistent cross-layer sparsity, with pivotal tokens remaining\nsalient across decoding steps and low-relevance tokens staying unimportant,\nmotivating selective cache eviction. We propose Sparse-dLLM, the first\ntraining-free framework integrating dynamic cache eviction with sparse\nattention via delayed bidirectional sparse caching. By leveraging the stability\nof token saliency over steps, it retains critical tokens and dynamically evicts\nunimportant prefix/suffix entries using an attention-guided strategy. Extensive\nexperiments on LLaDA and Dream series demonstrate Sparse-dLLM achieves up to\n10$\\times$ higher throughput than vanilla dLLMs, with comparable performance\nand similar peak memory costs, outperforming previous methods in efficiency and\neffectiveness. The code is available at\nhttps://github.com/OpenMOSS/Sparse-dLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Large Language Models (dLLMs) enable breakthroughs in reasoning and\nparallel decoding but suffer from prohibitive quadratic computational\ncomplexity and memory overhead during inference. Current caching techniques\naccelerate decoding by storing full-layer states, yet impose substantial memory\nusage that limit long-context applications. Our analysis of attention patterns\nin dLLMs reveals persistent cross-layer sparsity, with pivotal tokens remaining\nsalient across decoding steps and low-relevance tokens staying unimportant,\nmotivating selective cache eviction. We propose Sparse-dLLM, the first\ntraining-free framework integrating dynamic cache eviction with sparse\nattention via delayed bidirectional sparse caching. By leveraging the stability\nof token saliency over steps, it retains critical tokens and dynamically evicts\nunimportant prefix/suffix entries using an attention-guided strategy. Extensive\nexperiments on LLaDA and Dream series demonstrate Sparse-dLLM achieves up to\n10$\\times$ higher throughput than vanilla dLLMs, with comparable performance\nand similar peak memory costs, outperforming previous methods in efficiency and\neffectiveness. The code is available at\nhttps://github.com/OpenMOSS/Sparse-dLLM."
                },
                "authors": [
                    {
                        "name": "Yuerong Song"
                    },
                    {
                        "name": "Xiaoran Liu"
                    },
                    {
                        "name": "Ruixiao Li"
                    },
                    {
                        "name": "Zhigeng Liu"
                    },
                    {
                        "name": "Zengfeng Huang"
                    },
                    {
                        "name": "Qipeng Guo"
                    },
                    {
                        "name": "Ziwei He"
                    },
                    {
                        "name": "Xipeng Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Xipeng Qiu"
                },
                "author": "Xipeng Qiu",
                "arxiv_comment": "12 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02558v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02558v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03475v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03475v1",
                "updated": "2025-11-05T13:59:01Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    13,
                    59,
                    1,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T13:59:01Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    13,
                    59,
                    1,
                    2,
                    309,
                    0
                ],
                "title": "RAGBoost: Efficient Retrieval-Augmented Generation with\n  Accuracy-Preserving Context Reuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAGBoost: Efficient Retrieval-Augmented Generation with\n  Accuracy-Preserving Context Reuse"
                },
                "summary": "Retrieval-augmented generation (RAG) enhances large language models (LLMs)\nwith retrieved context but often suffers from downgraded prefill performance as\nmodern applications demand longer and more complex inputs. Existing caching\ntechniques either preserve accuracy with low cache reuse or improve reuse at\nthe cost of degraded reasoning quality. We present RAGBoost, an efficient RAG\nsystem that achieves high cache reuse without sacrificing accuracy through\naccuracy-preserving context reuse. RAGBoost detects overlapping retrieved items\nacross concurrent sessions and multi-turn interactions, using efficient context\nindexing, ordering, and de-duplication to maximize reuse, while lightweight\ncontextual hints maintain reasoning fidelity. It integrates seamlessly with\nexisting LLM inference engines and improves their prefill performance by 1.5-3X\nover state-of-the-art methods, while preserving or even enhancing reasoning\naccuracy across diverse RAG and agentic AI workloads. Our code is released at:\nhttps://github.com/Edinburgh-AgenticAI/RAGBoost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) enhances large language models (LLMs)\nwith retrieved context but often suffers from downgraded prefill performance as\nmodern applications demand longer and more complex inputs. Existing caching\ntechniques either preserve accuracy with low cache reuse or improve reuse at\nthe cost of degraded reasoning quality. We present RAGBoost, an efficient RAG\nsystem that achieves high cache reuse without sacrificing accuracy through\naccuracy-preserving context reuse. RAGBoost detects overlapping retrieved items\nacross concurrent sessions and multi-turn interactions, using efficient context\nindexing, ordering, and de-duplication to maximize reuse, while lightweight\ncontextual hints maintain reasoning fidelity. It integrates seamlessly with\nexisting LLM inference engines and improves their prefill performance by 1.5-3X\nover state-of-the-art methods, while preserving or even enhancing reasoning\naccuracy across diverse RAG and agentic AI workloads. Our code is released at:\nhttps://github.com/Edinburgh-AgenticAI/RAGBoost."
                },
                "authors": [
                    {
                        "name": "Yinsicheng Jiang"
                    },
                    {
                        "name": "Yeqi Huang"
                    },
                    {
                        "name": "Liang Cheng"
                    },
                    {
                        "name": "Cheng Deng"
                    },
                    {
                        "name": "Xuan Sun"
                    },
                    {
                        "name": "Luo Mai"
                    }
                ],
                "author_detail": {
                    "name": "Luo Mai"
                },
                "author": "Luo Mai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03475v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03475v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09045v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09045v2",
                "updated": "2025-11-05T09:18:48Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    9,
                    18,
                    48,
                    2,
                    309,
                    0
                ],
                "published": "2025-06-10T17:59:02Z",
                "published_parsed": [
                    2025,
                    6,
                    10,
                    17,
                    59,
                    2,
                    1,
                    161,
                    0
                ],
                "title": "MagCache: Fast Video Generation with Magnitude-Aware Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MagCache: Fast Video Generation with Magnitude-Aware Cache"
                },
                "summary": "Existing acceleration techniques for video diffusion models often rely on\nuniform heuristics or time-embedding variants to skip timesteps and reuse\ncached features. These approaches typically require extensive calibration with\ncurated prompts and risk inconsistent outputs due to prompt-specific\noverfitting. In this paper, we introduce a novel and robust discovery: a\nunified magnitude law observed across different models and prompts.\nSpecifically, the magnitude ratio of successive residual outputs decreases\nmonotonically, steadily in most timesteps while rapidly in the last several\nsteps. Leveraging this insight, we introduce a Magnitude-aware Cache (MagCache)\nthat adaptively skips unimportant timesteps using an error modeling mechanism\nand adaptive caching strategy. Unlike existing methods requiring dozens of\ncurated samples for calibration, MagCache only requires a single sample for\ncalibration. Experimental results show that MagCache achieves 2.10x-2.68x\nspeedups on Open-Sora, CogVideoX, Wan 2.1, and HunyuanVideo, while preserving\nsuperior visual fidelity. It significantly outperforms existing methods in\nLPIPS, SSIM, and PSNR, under similar computational budgets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing acceleration techniques for video diffusion models often rely on\nuniform heuristics or time-embedding variants to skip timesteps and reuse\ncached features. These approaches typically require extensive calibration with\ncurated prompts and risk inconsistent outputs due to prompt-specific\noverfitting. In this paper, we introduce a novel and robust discovery: a\nunified magnitude law observed across different models and prompts.\nSpecifically, the magnitude ratio of successive residual outputs decreases\nmonotonically, steadily in most timesteps while rapidly in the last several\nsteps. Leveraging this insight, we introduce a Magnitude-aware Cache (MagCache)\nthat adaptively skips unimportant timesteps using an error modeling mechanism\nand adaptive caching strategy. Unlike existing methods requiring dozens of\ncurated samples for calibration, MagCache only requires a single sample for\ncalibration. Experimental results show that MagCache achieves 2.10x-2.68x\nspeedups on Open-Sora, CogVideoX, Wan 2.1, and HunyuanVideo, while preserving\nsuperior visual fidelity. It significantly outperforms existing methods in\nLPIPS, SSIM, and PSNR, under similar computational budgets."
                },
                "authors": [
                    {
                        "name": "Zehong Ma"
                    },
                    {
                        "name": "Longhui Wei"
                    },
                    {
                        "name": "Feng Wang"
                    },
                    {
                        "name": "Shiliang Zhang"
                    },
                    {
                        "name": "Qi Tian"
                    }
                ],
                "author_detail": {
                    "name": "Qi Tian"
                },
                "author": "Qi Tian",
                "arxiv_comment": "Project Page: https://zehong-ma.github.io/MagCache Accepted by\n  NeurIPS 2025",
                "arxiv_journal_ref": "In Proceedings of NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09045v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09045v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03159v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03159v1",
                "updated": "2025-11-05T03:40:44Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    3,
                    40,
                    44,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T03:40:44Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    3,
                    40,
                    44,
                    2,
                    309,
                    0
                ],
                "title": "Joint Optimization of DNN Model Caching and Request Routing in Mobile\n  Edge Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint Optimization of DNN Model Caching and Request Routing in Mobile\n  Edge Computing"
                },
                "summary": "Mobile edge computing (MEC) can pre-cache deep neural networks (DNNs) near\nend-users, providing low-latency services and improving users' quality of\nexperience (QoE). However, caching all DNN models at edge servers with limited\ncapacity is difficult, and the impact of model loading time on QoE remains\nunderexplored. Hence, we introduce dynamic DNNs in edge scenarios,\ndisassembling a complete DNN model into interrelated submodels for more\nfine-grained and flexible model caching and request routing solutions. This\nraises the pressing issue of jointly deciding request routing and submodel\ncaching for dynamic DNNs to balance model inference precision and loading\nlatency for QoE optimization. In this paper, we study the joint dynamic model\ncaching and request routing problem in MEC networks, aiming to maximize user\nrequest inference precision under constraints of server resources, latency, and\nmodel loading time. To tackle this problem, we propose CoCaR, an offline\nalgorithm based on linear programming and random rounding that leverages\ndynamic DNNs to optimize caching and routing schemes, achieving near-optimal\nperformance. Furthermore, we develop an online variant of CoCaR, named\nCoCaR-OL, enabling effective adaptation to dynamic and unpredictable online\nrequest patterns. The simulation results demonstrate that the proposed CoCaR\nimproves the average inference precision of user requests by 46\\% compared to\nstate-of-the-art baselines. In addition, in online scenarios, CoCaR-OL achieves\nan improvement of no less than 32.3\\% in user QoE over competitive baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile edge computing (MEC) can pre-cache deep neural networks (DNNs) near\nend-users, providing low-latency services and improving users' quality of\nexperience (QoE). However, caching all DNN models at edge servers with limited\ncapacity is difficult, and the impact of model loading time on QoE remains\nunderexplored. Hence, we introduce dynamic DNNs in edge scenarios,\ndisassembling a complete DNN model into interrelated submodels for more\nfine-grained and flexible model caching and request routing solutions. This\nraises the pressing issue of jointly deciding request routing and submodel\ncaching for dynamic DNNs to balance model inference precision and loading\nlatency for QoE optimization. In this paper, we study the joint dynamic model\ncaching and request routing problem in MEC networks, aiming to maximize user\nrequest inference precision under constraints of server resources, latency, and\nmodel loading time. To tackle this problem, we propose CoCaR, an offline\nalgorithm based on linear programming and random rounding that leverages\ndynamic DNNs to optimize caching and routing schemes, achieving near-optimal\nperformance. Furthermore, we develop an online variant of CoCaR, named\nCoCaR-OL, enabling effective adaptation to dynamic and unpredictable online\nrequest patterns. The simulation results demonstrate that the proposed CoCaR\nimproves the average inference precision of user requests by 46\\% compared to\nstate-of-the-art baselines. In addition, in online scenarios, CoCaR-OL achieves\nan improvement of no less than 32.3\\% in user QoE over competitive baselines."
                },
                "authors": [
                    {
                        "name": "Shuting Qiu"
                    },
                    {
                        "name": "Fang Dong"
                    },
                    {
                        "name": "Siyu Tan"
                    },
                    {
                        "name": "Ruiting Zhou"
                    },
                    {
                        "name": "Dian Shen"
                    },
                    {
                        "name": "Patrick P. C. Lee"
                    },
                    {
                        "name": "Qilin Fan"
                    }
                ],
                "author_detail": {
                    "name": "Qilin Fan"
                },
                "author": "Qilin Fan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03159v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03159v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03092v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03092v1",
                "updated": "2025-11-05T00:38:31Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    0,
                    38,
                    31,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T00:38:31Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    0,
                    38,
                    31,
                    2,
                    309,
                    0
                ],
                "title": "SnapStream: Efficient Long Sequence Decoding on Dataflow Accelerators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SnapStream: Efficient Long Sequence Decoding on Dataflow Accelerators"
                },
                "summary": "The proliferation of 100B+ parameter Large Language Models (LLMs) with 100k+\ncontext length support have resulted in increasing demands for on-chip memory\nto support large KV caches. Techniques such as StreamingLLM and SnapKV\ndemonstrate how to control KV cache size while maintaining model accuracy. Yet,\nthese techniques are not commonly used within industrial deployments using\nframeworks like vLLM or SGLang. The reason is twofold: on one hand, the static\ngraphs and continuous batching methodology employed by these frameworks make it\ndifficult to admit modifications to the standard multi-head attention\nalgorithm, while on the other hand, the accuracy implications of such\ntechniques on modern instruction-following and reasoning models are not well\nunderstood, obfuscating the need for implementing these techniques. In this\npaper, we explore these accuracy implications on Llama-3.1-8B-Instruct and\nDeepSeek-R1, and develop SnapStream, a KV cache compression method that can be\ndeployed at scale. We demonstrate the efficacy of SnapStream in a 16-way\ntensor-parallel deployment of DeepSeek-671B on SambaNova SN40L accelerators\nrunning at 128k context length and up to 1832 tokens per second in a real\nproduction setting. SnapStream enables $4\\times$ improved on-chip memory usage\nand introduces minimal accuracy degradation on LongBench-v2, AIME24 and\nLiveCodeBench. To the best of our knowledge, this is the first implementation\nof sparse KV attention techniques deployed in a production inference system\nwith static graphs and continuous batching.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of 100B+ parameter Large Language Models (LLMs) with 100k+\ncontext length support have resulted in increasing demands for on-chip memory\nto support large KV caches. Techniques such as StreamingLLM and SnapKV\ndemonstrate how to control KV cache size while maintaining model accuracy. Yet,\nthese techniques are not commonly used within industrial deployments using\nframeworks like vLLM or SGLang. The reason is twofold: on one hand, the static\ngraphs and continuous batching methodology employed by these frameworks make it\ndifficult to admit modifications to the standard multi-head attention\nalgorithm, while on the other hand, the accuracy implications of such\ntechniques on modern instruction-following and reasoning models are not well\nunderstood, obfuscating the need for implementing these techniques. In this\npaper, we explore these accuracy implications on Llama-3.1-8B-Instruct and\nDeepSeek-R1, and develop SnapStream, a KV cache compression method that can be\ndeployed at scale. We demonstrate the efficacy of SnapStream in a 16-way\ntensor-parallel deployment of DeepSeek-671B on SambaNova SN40L accelerators\nrunning at 128k context length and up to 1832 tokens per second in a real\nproduction setting. SnapStream enables $4\\times$ improved on-chip memory usage\nand introduces minimal accuracy degradation on LongBench-v2, AIME24 and\nLiveCodeBench. To the best of our knowledge, this is the first implementation\nof sparse KV attention techniques deployed in a production inference system\nwith static graphs and continuous batching."
                },
                "authors": [
                    {
                        "name": "Jonathan Li"
                    },
                    {
                        "name": "Nasim Farahini"
                    },
                    {
                        "name": "Evgenii Iuliugin"
                    },
                    {
                        "name": "Magnus Vesterlund"
                    },
                    {
                        "name": "Christian Haggstrom"
                    },
                    {
                        "name": "Guangtao Wang"
                    },
                    {
                        "name": "Shubhangi Upasani"
                    },
                    {
                        "name": "Ayush Sachdeva"
                    },
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Faline Fu"
                    },
                    {
                        "name": "Chen Wu"
                    },
                    {
                        "name": "Ayesha Siddiqua"
                    },
                    {
                        "name": "John Long"
                    },
                    {
                        "name": "Tuowen Zhao"
                    },
                    {
                        "name": "Matheen Musaddiq"
                    },
                    {
                        "name": "Hakan Zeffer"
                    },
                    {
                        "name": "Yun Du"
                    },
                    {
                        "name": "Mingran Wang"
                    },
                    {
                        "name": "Qinghua Li"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Urmish Thakker"
                    },
                    {
                        "name": "Raghu Prabhakar"
                    }
                ],
                "author_detail": {
                    "name": "Raghu Prabhakar"
                },
                "author": "Raghu Prabhakar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03092v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03092v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02919v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02919v1",
                "updated": "2025-11-04T19:02:29Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    19,
                    2,
                    29,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T19:02:29Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    19,
                    2,
                    29,
                    1,
                    308,
                    0
                ],
                "title": "Cache Mechanism for Agent RAG Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache Mechanism for Agent RAG Systems"
                },
                "summary": "Recent advances in Large Language Model (LLM)-based agents have been\npropelled by Retrieval-Augmented Generation (RAG), which grants the models\naccess to vast external knowledge bases. Despite RAG's success in improving\nagent performance, agent-level cache management, particularly constructing,\nmaintaining, and updating a compact, relevant corpus dynamically tailored to\neach agent's need, remains underexplored. Therefore, we introduce ARC (Agent\nRAG Cache Mechanism), a novel, annotation-free caching framework that\ndynamically manages small, high-value corpora for each agent. By synthesizing\nhistorical query distribution patterns with the intrinsic geometry of cached\nitems in the embedding space, ARC automatically maintains a high-relevance\ncache. With comprehensive experiments on three retrieval datasets, our\nexperimental results demonstrate that ARC reduces storage requirements to\n0.015% of the original corpus while offering up to 79.8% has-answer rate and\nreducing average retrieval latency by 80%. Our results demonstrate that ARC can\ndrastically enhance efficiency and effectiveness in RAG-powered LLM agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large Language Model (LLM)-based agents have been\npropelled by Retrieval-Augmented Generation (RAG), which grants the models\naccess to vast external knowledge bases. Despite RAG's success in improving\nagent performance, agent-level cache management, particularly constructing,\nmaintaining, and updating a compact, relevant corpus dynamically tailored to\neach agent's need, remains underexplored. Therefore, we introduce ARC (Agent\nRAG Cache Mechanism), a novel, annotation-free caching framework that\ndynamically manages small, high-value corpora for each agent. By synthesizing\nhistorical query distribution patterns with the intrinsic geometry of cached\nitems in the embedding space, ARC automatically maintains a high-relevance\ncache. With comprehensive experiments on three retrieval datasets, our\nexperimental results demonstrate that ARC reduces storage requirements to\n0.015% of the original corpus while offering up to 79.8% has-answer rate and\nreducing average retrieval latency by 80%. Our results demonstrate that ARC can\ndrastically enhance efficiency and effectiveness in RAG-powered LLM agents."
                },
                "authors": [
                    {
                        "name": "Shuhang Lin"
                    },
                    {
                        "name": "Zhencan Peng"
                    },
                    {
                        "name": "Lingyao Li"
                    },
                    {
                        "name": "Xiao Lin"
                    },
                    {
                        "name": "Xi Zhu"
                    },
                    {
                        "name": "Yongfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yongfeng Zhang"
                },
                "author": "Yongfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02919v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02919v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02761v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02761v1",
                "updated": "2025-11-04T17:40:31Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    17,
                    40,
                    31,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T17:40:31Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    17,
                    40,
                    31,
                    1,
                    308,
                    0
                ],
                "title": "Non-Contact Manipulation of Induced Magnetic Dipoles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-Contact Manipulation of Induced Magnetic Dipoles"
                },
                "summary": "Extending the field of magnetic manipulation to conductive, non-magnetic\nobjects opens the door for a wide array of applications previously limited to\nhard or soft magnetic materials. Of particular interest is the recycling of\nspace debris through the use of oscillating magnetic fields, which represent a\ncache of raw materials in an environment particularly suited to the low forces\ngenerated from inductive magnetic manipulation. Building upon previous work\nthat demonstrated 3D open-loop position control by leveraging the opposing\ndipole moment created from induced eddy currents, this work demonstrates\nclosed-loop position control of a semi-buoyant aluminum sphere in lab tests,\nand the efficacy of varying methods for force inversion is explored. The\nclosed-loop methods represent a critical first step towards wider applications\nfor 3-DOF position control of induced magnetic dipoles.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extending the field of magnetic manipulation to conductive, non-magnetic\nobjects opens the door for a wide array of applications previously limited to\nhard or soft magnetic materials. Of particular interest is the recycling of\nspace debris through the use of oscillating magnetic fields, which represent a\ncache of raw materials in an environment particularly suited to the low forces\ngenerated from inductive magnetic manipulation. Building upon previous work\nthat demonstrated 3D open-loop position control by leveraging the opposing\ndipole moment created from induced eddy currents, this work demonstrates\nclosed-loop position control of a semi-buoyant aluminum sphere in lab tests,\nand the efficacy of varying methods for force inversion is explored. The\nclosed-loop methods represent a critical first step towards wider applications\nfor 3-DOF position control of induced magnetic dipoles."
                },
                "authors": [
                    {
                        "name": "Seth Stewart"
                    },
                    {
                        "name": "Joseph Pawelski"
                    },
                    {
                        "name": "Steve Ward"
                    },
                    {
                        "name": "Andrew J. Petruska"
                    }
                ],
                "author_detail": {
                    "name": "Andrew J. Petruska"
                },
                "author": "Andrew J. Petruska",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02761v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02761v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02749v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02749v1",
                "updated": "2025-11-04T17:22:49Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    17,
                    22,
                    49,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T17:22:49Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    17,
                    22,
                    49,
                    1,
                    308,
                    0
                ],
                "title": "Using Span Queries to Optimize for Cache and Attention Locality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using Span Queries to Optimize for Cache and Attention Locality"
                },
                "summary": "Clients are evolving beyond chat completion, and now include a variety of\ninnovative inference-time scaling and deep reasoning techniques. At the same\ntime, inference servers remain heavily optimized for chat completion. Prior\nwork has shown that large improvements to KV cache hit rate are possible if\ninference servers evolve towards these non-chat use cases. However, they offer\nsolutions that are also optimized for a single use case, RAG. In this paper, we\nintroduce the span query to generalize the interface to the inference server.\nWe demonstrate that chat, RAG, inference-time scaling, and agentic workloads\ncan all be expressed as span queries. We show how the critical distinction that\nhad been assumed by prior work lies in whether the order of the inputs matter\n-- do they commute? In chat, they do not. In RAG, they often do. This paper\nintroduces span queries, which are expression trees of inference calls, linked\ntogether with commutativity constraints. We describe span query syntax and\nsemantics. We show how they can be automatically optimized to improve KV cache\nlocality. We show how a small change to vLLM (affecting only 492 lines) can\nenable high-performance execution of span queries. Using this stack, we\ndemonstrate that span queries can achieve 10-20x reductions in TTFT for two\ndistinct non-chat use cases. Finally, we show that span queries can also be\noptimized to improve attention locality, so as to avoid the so-called\nlost-in-the-middle problem. We demonstrate that an attention-optimized span\nquery on a 2b parameter model vastly outperforms the accuracy of a stock\ninference server using an 8b model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clients are evolving beyond chat completion, and now include a variety of\ninnovative inference-time scaling and deep reasoning techniques. At the same\ntime, inference servers remain heavily optimized for chat completion. Prior\nwork has shown that large improvements to KV cache hit rate are possible if\ninference servers evolve towards these non-chat use cases. However, they offer\nsolutions that are also optimized for a single use case, RAG. In this paper, we\nintroduce the span query to generalize the interface to the inference server.\nWe demonstrate that chat, RAG, inference-time scaling, and agentic workloads\ncan all be expressed as span queries. We show how the critical distinction that\nhad been assumed by prior work lies in whether the order of the inputs matter\n-- do they commute? In chat, they do not. In RAG, they often do. This paper\nintroduces span queries, which are expression trees of inference calls, linked\ntogether with commutativity constraints. We describe span query syntax and\nsemantics. We show how they can be automatically optimized to improve KV cache\nlocality. We show how a small change to vLLM (affecting only 492 lines) can\nenable high-performance execution of span queries. Using this stack, we\ndemonstrate that span queries can achieve 10-20x reductions in TTFT for two\ndistinct non-chat use cases. Finally, we show that span queries can also be\noptimized to improve attention locality, so as to avoid the so-called\nlost-in-the-middle problem. We demonstrate that an attention-optimized span\nquery on a 2b parameter model vastly outperforms the accuracy of a stock\ninference server using an 8b model."
                },
                "authors": [
                    {
                        "name": "Paul Castro"
                    },
                    {
                        "name": "Nick Mitchell"
                    },
                    {
                        "name": "Nathan Ordonez"
                    },
                    {
                        "name": "Thomas Parnell"
                    },
                    {
                        "name": "Mudhakar Srivatsa"
                    },
                    {
                        "name": "Antoni Viros i Martin"
                    }
                ],
                "author_detail": {
                    "name": "Antoni Viros i Martin"
                },
                "author": "Antoni Viros i Martin",
                "arxiv_comment": "12 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02749v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02749v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02651v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02651v1",
                "updated": "2025-11-04T15:17:43Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    15,
                    17,
                    43,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T15:17:43Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    15,
                    17,
                    43,
                    1,
                    308,
                    0
                ],
                "title": "Apriel-H1: Towards Efficient Enterprise Reasoning Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Apriel-H1: Towards Efficient Enterprise Reasoning Models"
                },
                "summary": "Large Language Models (LLMs) achieve remarkable reasoning capabilities\nthrough transformer architectures with attention mechanisms. However,\ntransformers suffer from quadratic time and memory complexity in the attention\nmodule (MHA) and require caching key-value states during inference, which\nseverely limits throughput and scalability. High inference throughput is\ncritical for agentic tasks, long-context reasoning, efficient deployment under\nhigh request loads, and more efficient test-time compute scaling.\n  State Space Models (SSMs) such as Mamba offer a promising alternative with\nlinear inference complexity and a constant memory footprint via recurrent\ncomputation with fixed-size hidden states. In this technical report we\nintroduce the Apriel-H1 family of hybrid LLMs that combine transformer\nattention and SSM sequence mixers for efficient reasoning at 15B model size.\nThese models are obtained through incremental distillation from a pretrained\nreasoning transformer, Apriel-Nemotron-15B-Thinker, progressively replacing\nless critical attention layers with linear Mamba blocks.\n  We release multiple post-distillation variants of Apriel-H1-15B-Thinker with\ndifferent SSM-to-MHA ratios and analyse how reasoning performance degrades as\nmore Mamba layers replace MHA. Additionally, we release a 30/50 hybrid variant\nof Apriel-H1, further fine-tuned on a supervised dataset of reasoning traces,\nachieving over 2x higher inference throughput when deployed in the\nproduction-ready vLLM environment, with minimal degradation in reasoning\nperformance. This shows that distilled hybrid SSM-Transformer architectures can\ndeliver substantial efficiency gains over the pretrained transformer equivalent\nwithout substantially compromising the reasoning quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) achieve remarkable reasoning capabilities\nthrough transformer architectures with attention mechanisms. However,\ntransformers suffer from quadratic time and memory complexity in the attention\nmodule (MHA) and require caching key-value states during inference, which\nseverely limits throughput and scalability. High inference throughput is\ncritical for agentic tasks, long-context reasoning, efficient deployment under\nhigh request loads, and more efficient test-time compute scaling.\n  State Space Models (SSMs) such as Mamba offer a promising alternative with\nlinear inference complexity and a constant memory footprint via recurrent\ncomputation with fixed-size hidden states. In this technical report we\nintroduce the Apriel-H1 family of hybrid LLMs that combine transformer\nattention and SSM sequence mixers for efficient reasoning at 15B model size.\nThese models are obtained through incremental distillation from a pretrained\nreasoning transformer, Apriel-Nemotron-15B-Thinker, progressively replacing\nless critical attention layers with linear Mamba blocks.\n  We release multiple post-distillation variants of Apriel-H1-15B-Thinker with\ndifferent SSM-to-MHA ratios and analyse how reasoning performance degrades as\nmore Mamba layers replace MHA. Additionally, we release a 30/50 hybrid variant\nof Apriel-H1, further fine-tuned on a supervised dataset of reasoning traces,\nachieving over 2x higher inference throughput when deployed in the\nproduction-ready vLLM environment, with minimal degradation in reasoning\nperformance. This shows that distilled hybrid SSM-Transformer architectures can\ndeliver substantial efficiency gains over the pretrained transformer equivalent\nwithout substantially compromising the reasoning quality."
                },
                "authors": [
                    {
                        "name": "Oleksiy Ostapenko"
                    },
                    {
                        "name": "Luke Kumar"
                    },
                    {
                        "name": "Raymond Li"
                    },
                    {
                        "name": "Denis Kocetkov"
                    },
                    {
                        "name": "Joel Lamy-Poirier"
                    },
                    {
                        "name": "Shruthan Radhakrishna"
                    },
                    {
                        "name": "Soham Parikh"
                    },
                    {
                        "name": "Shambhavi Mishra"
                    },
                    {
                        "name": "Sebastien Paquet"
                    },
                    {
                        "name": "Srinivas Sunkara"
                    },
                    {
                        "name": "Valrie Bcaert"
                    },
                    {
                        "name": "Sathwik Tejaswi Madhusudhan"
                    },
                    {
                        "name": "Torsten Scholak"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Scholak"
                },
                "author": "Torsten Scholak",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02651v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02651v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02647v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02647v1",
                "updated": "2025-11-04T15:14:58Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    15,
                    14,
                    58,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T15:14:58Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    15,
                    14,
                    58,
                    1,
                    308,
                    0
                ],
                "title": "Federated Attention: A Distributed Paradigm for Collaborative LLM\n  Inference over Edge Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Attention: A Distributed Paradigm for Collaborative LLM\n  Inference over Edge Networks"
                },
                "summary": "Large language models (LLMs) are proliferating rapidly at the edge,\ndelivering intelligent capabilities across diverse application scenarios.\nHowever, their practical deployment in collaborative scenarios confronts\nfundamental challenges: privacy vulnerabilities, communication overhead, and\ncomputational bottlenecks. To address these, we propose Federated Attention\n(FedAttn), which integrates the federated paradigm into the self-attention\nmechanism, creating a new distributed LLM inference framework that\nsimultaneously achieves privacy protection, communication efficiency, and\ncomputational efficiency. FedAttn enables participants to perform local\nself-attention over their own token representations while periodically\nexchanging and aggregating Key-Value (KV) matrices across multiple Transformer\nblocks, collaboratively generating LLM responses without exposing private\nprompts. Further, we identify a structural duality between contextual\nrepresentation refinement in FedAttn and parameter optimization in FL across\nprivate data, local computation, and global aggregation. This key insight\nprovides a principled foundation for systematically porting federated\noptimization techniques to collaborative LLM inference. Building on this\nframework, we theoretically analyze how local self-attention computation within\nparticipants and heterogeneous token relevance among participants shape error\npropagation dynamics across Transformer blocks. Moreover, we characterize the\nfundamental trade-off between response quality and communication/computation\nefficiency, which is governed by the synchronization interval and the number of\nparticipants. Experimental results validate our theoretical analysis, and\nreveal significant optimization opportunities through sparse attention and\nadaptive KV aggregation, highlighting FedAttn's potential to deliver\nscalability and efficiency in real-world edge deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are proliferating rapidly at the edge,\ndelivering intelligent capabilities across diverse application scenarios.\nHowever, their practical deployment in collaborative scenarios confronts\nfundamental challenges: privacy vulnerabilities, communication overhead, and\ncomputational bottlenecks. To address these, we propose Federated Attention\n(FedAttn), which integrates the federated paradigm into the self-attention\nmechanism, creating a new distributed LLM inference framework that\nsimultaneously achieves privacy protection, communication efficiency, and\ncomputational efficiency. FedAttn enables participants to perform local\nself-attention over their own token representations while periodically\nexchanging and aggregating Key-Value (KV) matrices across multiple Transformer\nblocks, collaboratively generating LLM responses without exposing private\nprompts. Further, we identify a structural duality between contextual\nrepresentation refinement in FedAttn and parameter optimization in FL across\nprivate data, local computation, and global aggregation. This key insight\nprovides a principled foundation for systematically porting federated\noptimization techniques to collaborative LLM inference. Building on this\nframework, we theoretically analyze how local self-attention computation within\nparticipants and heterogeneous token relevance among participants shape error\npropagation dynamics across Transformer blocks. Moreover, we characterize the\nfundamental trade-off between response quality and communication/computation\nefficiency, which is governed by the synchronization interval and the number of\nparticipants. Experimental results validate our theoretical analysis, and\nreveal significant optimization opportunities through sparse attention and\nadaptive KV aggregation, highlighting FedAttn's potential to deliver\nscalability and efficiency in real-world edge deployments."
                },
                "authors": [
                    {
                        "name": "Xiumei Deng"
                    },
                    {
                        "name": "Zehui Xiong"
                    },
                    {
                        "name": "Binbin Chen"
                    },
                    {
                        "name": "Dong In Kim"
                    },
                    {
                        "name": "Merouane Debbah"
                    },
                    {
                        "name": "H. Vincent Poor"
                    }
                ],
                "author_detail": {
                    "name": "H. Vincent Poor"
                },
                "author": "H. Vincent Poor",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02647v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02647v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02770v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02770v5",
                "updated": "2025-11-04T12:04:06Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    12,
                    4,
                    6,
                    1,
                    308,
                    0
                ],
                "published": "2025-02-04T23:26:10Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    23,
                    26,
                    10,
                    1,
                    35,
                    0
                ],
                "title": "Twilight: Adaptive Attention Sparsity with Hierarchical Top-$p$ Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Twilight: Adaptive Attention Sparsity with Hierarchical Top-$p$ Pruning"
                },
                "summary": "Leveraging attention sparsity to accelerate long-context large language\nmodels (LLMs) has been a hot research topic. However, current algorithms such\nas sparse attention or key-value (KV) cache compression tend to use a fixed\nbudget, which presents a significant challenge during deployment because it\nfails to account for the dynamic nature of real-world scenarios, where the\noptimal balance between accuracy and efficiency can vary greatly. In this\npaper, we find that borrowing top-$p$ sampling (nucleus sampling) to sparse\nattention can surprisingly achieve adaptive budgeting. Based on this, we\npropose Twilight, a framework to bring adaptive sparsity to any existing sparse\nattention algorithm without sacrificing their accuracy. Empirical results show\nthat Twilight can adaptively prune at most 98% of redundant tokens, leading to\n$15.4\\times$ acceleration in self-attention operations and $3.9\\times$\nacceleration in end-to-end per token latency in long context LLM decoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging attention sparsity to accelerate long-context large language\nmodels (LLMs) has been a hot research topic. However, current algorithms such\nas sparse attention or key-value (KV) cache compression tend to use a fixed\nbudget, which presents a significant challenge during deployment because it\nfails to account for the dynamic nature of real-world scenarios, where the\noptimal balance between accuracy and efficiency can vary greatly. In this\npaper, we find that borrowing top-$p$ sampling (nucleus sampling) to sparse\nattention can surprisingly achieve adaptive budgeting. Based on this, we\npropose Twilight, a framework to bring adaptive sparsity to any existing sparse\nattention algorithm without sacrificing their accuracy. Empirical results show\nthat Twilight can adaptively prune at most 98% of redundant tokens, leading to\n$15.4\\times$ acceleration in self-attention operations and $3.9\\times$\nacceleration in end-to-end per token latency in long context LLM decoding."
                },
                "authors": [
                    {
                        "name": "Chaofan Lin"
                    },
                    {
                        "name": "Jiaming Tang"
                    },
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Hanshuo Wang"
                    },
                    {
                        "name": "Tian Tang"
                    },
                    {
                        "name": "Boyu Tian"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Mingyu Gao"
                    }
                ],
                "author_detail": {
                    "name": "Mingyu Gao"
                },
                "author": "Mingyu Gao",
                "arxiv_comment": "To appear on NeurIPS 2025 (spotlight)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02770v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02770v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02381v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02381v1",
                "updated": "2025-11-04T09:03:09Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    9,
                    3,
                    9,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T09:03:09Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    9,
                    3,
                    9,
                    1,
                    308,
                    0
                ],
                "title": "Laser diagnostics for negative ion source optimization: insights from\n  SPIDER at the ITER Neutral Beam Test Facility",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Laser diagnostics for negative ion source optimization: insights from\n  SPIDER at the ITER Neutral Beam Test Facility"
                },
                "summary": "The ITER Heating Neutral Beams (HNBs) require large, high-energy H/D atom\nbeams (285/330 A/m^2 extracted current density, and 1/0.87 MeV acceleration\nenergy, respectively for H and D). To address the associated challenges, the\nSPIDER negative ion RF beam source at the Neutral Beam Test Facility (NBTF) in\nPadova (Italy) serves as a full-scale source prototype with a 100 kV triode\naccelerator, for design validation and performance verification. SPIDER is\nequipped with two advanced laser diagnostics to monitor key plasma parameters;\nCavity Ring-Down Spectroscopy (CRDS) is used to measure H$^-$\\slash D$^-$ ion\ndensities, while Laser Absorption Spectroscopy (LAS) tracks caesium neutral\ndensity in the source. These measurements are essential for optimizing negative\nion production and meeting ITER source targets. We present diagnostic upgrade\ndetails, recent experimental results, and correlations with other machine\nparameters. Since CRDS relies on a single 4.637-meter-long optical cavity, the\nlongest used in such sources, it has demonstrated sensitivity to alignment.\nBased on recent experimental experience, structural improvements are being\nimplemented to enhance both stability and measurement reliability. LAS has\nmainly been employed as a tool to monitor the caesium conditioning status of\nSPIDER. Additionally, due to a distributed measurement over four lines of\nsight, LAS has proven effective in monitoring the caesium distribution within\nthe source. This work demonstrates the essential role of laser diagnostics in\ndeveloping ITER-relevant plasma sources and informs ongoing efforts to improve\nmeasurement accuracy in challenging environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ITER Heating Neutral Beams (HNBs) require large, high-energy H/D atom\nbeams (285/330 A/m^2 extracted current density, and 1/0.87 MeV acceleration\nenergy, respectively for H and D). To address the associated challenges, the\nSPIDER negative ion RF beam source at the Neutral Beam Test Facility (NBTF) in\nPadova (Italy) serves as a full-scale source prototype with a 100 kV triode\naccelerator, for design validation and performance verification. SPIDER is\nequipped with two advanced laser diagnostics to monitor key plasma parameters;\nCavity Ring-Down Spectroscopy (CRDS) is used to measure H$^-$\\slash D$^-$ ion\ndensities, while Laser Absorption Spectroscopy (LAS) tracks caesium neutral\ndensity in the source. These measurements are essential for optimizing negative\nion production and meeting ITER source targets. We present diagnostic upgrade\ndetails, recent experimental results, and correlations with other machine\nparameters. Since CRDS relies on a single 4.637-meter-long optical cavity, the\nlongest used in such sources, it has demonstrated sensitivity to alignment.\nBased on recent experimental experience, structural improvements are being\nimplemented to enhance both stability and measurement reliability. LAS has\nmainly been employed as a tool to monitor the caesium conditioning status of\nSPIDER. Additionally, due to a distributed measurement over four lines of\nsight, LAS has proven effective in monitoring the caesium distribution within\nthe source. This work demonstrates the essential role of laser diagnostics in\ndeveloping ITER-relevant plasma sources and informs ongoing efforts to improve\nmeasurement accuracy in challenging environments."
                },
                "authors": [
                    {
                        "name": "R. Agnello"
                    },
                    {
                        "name": "M. Barbisan"
                    },
                    {
                        "name": "R. Pasqualotto"
                    },
                    {
                        "name": "B. Pouradier-Duteil"
                    },
                    {
                        "name": "E. Sartori"
                    },
                    {
                        "name": "A. Tiso"
                    },
                    {
                        "name": "B. Zaniol"
                    }
                ],
                "author_detail": {
                    "name": "B. Zaniol"
                },
                "author": "B. Zaniol",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02381v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02381v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02230v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02230v1",
                "updated": "2025-11-04T03:43:05Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    3,
                    43,
                    5,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T03:43:05Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    3,
                    43,
                    5,
                    1,
                    308,
                    0
                ],
                "title": "Continuum: Efficient and Robust Multi-Turn LLM Agent Scheduling with KV\n  Cache Time-to-Live",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continuum: Efficient and Robust Multi-Turn LLM Agent Scheduling with KV\n  Cache Time-to-Live"
                },
                "summary": "Agentic LLM applications interleave LLM generation requests with tool calls.\nThese tool calls break the continuity of the workflow by creating pauses\nbetween LLM requests, bringing many challenges for the serving system,\nespecially under multi-turn scenarios. Each pause potentially causes KV cache\neviction and extra waiting time before entering the continuous batch for the\nfollowing LLM request. Since these pauses happen for each call, this problem\nbecomes increasingly severe as turn number grow for agentic programs. Previous\nworks either fail to incorporate information from the tool call, evicting KV\ncache that leads to repetitive prefill or loading, or ignore the continuity of\na multi-turn program, creating waiting time between turns that increases\nper-request latency.\n  We present Continuum, a serving system to optimize job completion time for\nmulti-turn agent workloads by combining tool-aware KV cache timeout with\nprogram-level scheduling. By predicting tool call durations in agentic\nworkflows, Continuum selectively pins the KV cache in GPU memory with a\ntime-to-live value based on total turn number. When combined with program-level\nfirst-come-first-serve, Continuum prevents scheduling bubbles, preserves\nmulti-turn continuity, and optimizes for throughput for complex agentic\nworkflows. By modeling the variability of tool call and agent program\ncontinuity, Continuum outperforms state-of-the-art baselines. Our evaluation on\nreal-world agentic workloads (SWE-Bench and BFCL) with Llama-3.1 8B/70B models\nshows that Continuum significantly improves the average job completion times,\nand remains performant across different hardware setups and DRAM offloading\nschemes. Preview code is available at:\nhttps://github.com/Hanchenli/vllm-continuum",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic LLM applications interleave LLM generation requests with tool calls.\nThese tool calls break the continuity of the workflow by creating pauses\nbetween LLM requests, bringing many challenges for the serving system,\nespecially under multi-turn scenarios. Each pause potentially causes KV cache\neviction and extra waiting time before entering the continuous batch for the\nfollowing LLM request. Since these pauses happen for each call, this problem\nbecomes increasingly severe as turn number grow for agentic programs. Previous\nworks either fail to incorporate information from the tool call, evicting KV\ncache that leads to repetitive prefill or loading, or ignore the continuity of\na multi-turn program, creating waiting time between turns that increases\nper-request latency.\n  We present Continuum, a serving system to optimize job completion time for\nmulti-turn agent workloads by combining tool-aware KV cache timeout with\nprogram-level scheduling. By predicting tool call durations in agentic\nworkflows, Continuum selectively pins the KV cache in GPU memory with a\ntime-to-live value based on total turn number. When combined with program-level\nfirst-come-first-serve, Continuum prevents scheduling bubbles, preserves\nmulti-turn continuity, and optimizes for throughput for complex agentic\nworkflows. By modeling the variability of tool call and agent program\ncontinuity, Continuum outperforms state-of-the-art baselines. Our evaluation on\nreal-world agentic workloads (SWE-Bench and BFCL) with Llama-3.1 8B/70B models\nshows that Continuum significantly improves the average job completion times,\nand remains performant across different hardware setups and DRAM offloading\nschemes. Preview code is available at:\nhttps://github.com/Hanchenli/vllm-continuum"
                },
                "authors": [
                    {
                        "name": "Hanchen Li"
                    },
                    {
                        "name": "Qiuyang Mang"
                    },
                    {
                        "name": "Runyuan He"
                    },
                    {
                        "name": "Qizheng Zhang"
                    },
                    {
                        "name": "Huanzhi Mao"
                    },
                    {
                        "name": "Xiaokun Chen"
                    },
                    {
                        "name": "Alvin Cheung"
                    },
                    {
                        "name": "Joseph Gonzalez"
                    },
                    {
                        "name": "Ion Stoica"
                    }
                ],
                "author_detail": {
                    "name": "Ion Stoica"
                },
                "author": "Ion Stoica",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02230v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02230v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02132v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02132v1",
                "updated": "2025-11-03T23:48:39Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    23,
                    48,
                    39,
                    0,
                    307,
                    0
                ],
                "published": "2025-11-03T23:48:39Z",
                "published_parsed": [
                    2025,
                    11,
                    3,
                    23,
                    48,
                    39,
                    0,
                    307,
                    0
                ],
                "title": "Optimizing Attention on GPUs by Exploiting GPU Architectural NUMA\n  Effects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Attention on GPUs by Exploiting GPU Architectural NUMA\n  Effects"
                },
                "summary": "The rise of disaggregated AI GPUs has exposed a critical bottleneck in\nlarge-scale attention workloads: non-uniform memory access (NUMA). As\nmulti-chiplet designs become the norm for scaling compute capabilities, memory\nlatency and bandwidth vary sharply across compute regions, undermining the\nperformance of traditional GPU kernel scheduling strategies that assume uniform\nmemory access. We identify how these NUMA effects distort locality in\nmulti-head attention (MHA) and present Swizzled Head-first Mapping, a\nspatially-aware scheduling strategy that aligns attention heads with GPU NUMA\ndomains to exploit intra-chiplet cache reuse. On AMD's MI300X architecture, our\nmethod achieves up to 50% higher performance over state-of-the-art attention\nalgorithms using conventional scheduling techniques and sustains consistently\nhigh L2 cache hit rates of 80-97%. These results demonstrate that NUMA-aware\nscheduling is now fundamental to achieving full efficiency on next-generation\ndisaggregated GPUs, offering a path forward for scalable AI training and\ninference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of disaggregated AI GPUs has exposed a critical bottleneck in\nlarge-scale attention workloads: non-uniform memory access (NUMA). As\nmulti-chiplet designs become the norm for scaling compute capabilities, memory\nlatency and bandwidth vary sharply across compute regions, undermining the\nperformance of traditional GPU kernel scheduling strategies that assume uniform\nmemory access. We identify how these NUMA effects distort locality in\nmulti-head attention (MHA) and present Swizzled Head-first Mapping, a\nspatially-aware scheduling strategy that aligns attention heads with GPU NUMA\ndomains to exploit intra-chiplet cache reuse. On AMD's MI300X architecture, our\nmethod achieves up to 50% higher performance over state-of-the-art attention\nalgorithms using conventional scheduling techniques and sustains consistently\nhigh L2 cache hit rates of 80-97%. These results demonstrate that NUMA-aware\nscheduling is now fundamental to achieving full efficiency on next-generation\ndisaggregated GPUs, offering a path forward for scalable AI training and\ninference."
                },
                "authors": [
                    {
                        "name": "Mansi Choudhary"
                    },
                    {
                        "name": "Karthik Sangaiah"
                    },
                    {
                        "name": "Sonali Singh"
                    },
                    {
                        "name": "Muhammad Osama"
                    },
                    {
                        "name": "Lisa Wu Wills"
                    },
                    {
                        "name": "Ganesh Dasika"
                    }
                ],
                "author_detail": {
                    "name": "Ganesh Dasika"
                },
                "author": "Ganesh Dasika",
                "arxiv_comment": "11 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02132v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02132v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.01815v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.01815v1",
                "updated": "2025-11-03T18:20:35Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    18,
                    20,
                    35,
                    0,
                    307,
                    0
                ],
                "published": "2025-11-03T18:20:35Z",
                "published_parsed": [
                    2025,
                    11,
                    3,
                    18,
                    20,
                    35,
                    0,
                    307,
                    0
                ],
                "title": "KV Cache Transform Coding for Compact Storage in LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV Cache Transform Coding for Compact Storage in LLM Inference"
                },
                "summary": "Serving large language models (LLMs) at scale necessitates efficient\nkey-value (KV) cache management. KV caches can be reused across conversation\nturns via shared-prefix prompts that are common in iterative code editing and\nchat. However, stale caches consume scarce GPU memory, require offloading, or\nforce recomputation. We present KVTC, a lightweight transform coder that\ncompresses KV caches for compact on-GPU and off-GPU storage. Drawing on\nclassical media compression, KVTC combines PCA-based feature decorrelation,\nadaptive quantization, and entropy coding. It requires only a brief initial\ncalibration and leaves model parameters unchanged. By exploiting redundancies\nin KV caches, KVTC achieves up to 20$\\times$ compression while maintaining\nreasoning and long-context accuracy, and 40$\\times$ or higher for specific use\ncases. We test KVTC with Llama 3, Mistral NeMo, and R1-Qwen 2.5 models across\nbenchmarks including AIME25, LiveCodeBench, GSM8K, MMLU, Qasper, RULER, and\nMATH-500. It consistently outperforms inference-time baselines such as token\neviction, quantization, and SVD-based methods, while achieving higher\ncompression ratios. These results support KVTC as a practical building block\nfor memory-efficient LLM serving with reusable KV caches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving large language models (LLMs) at scale necessitates efficient\nkey-value (KV) cache management. KV caches can be reused across conversation\nturns via shared-prefix prompts that are common in iterative code editing and\nchat. However, stale caches consume scarce GPU memory, require offloading, or\nforce recomputation. We present KVTC, a lightweight transform coder that\ncompresses KV caches for compact on-GPU and off-GPU storage. Drawing on\nclassical media compression, KVTC combines PCA-based feature decorrelation,\nadaptive quantization, and entropy coding. It requires only a brief initial\ncalibration and leaves model parameters unchanged. By exploiting redundancies\nin KV caches, KVTC achieves up to 20$\\times$ compression while maintaining\nreasoning and long-context accuracy, and 40$\\times$ or higher for specific use\ncases. We test KVTC with Llama 3, Mistral NeMo, and R1-Qwen 2.5 models across\nbenchmarks including AIME25, LiveCodeBench, GSM8K, MMLU, Qasper, RULER, and\nMATH-500. It consistently outperforms inference-time baselines such as token\neviction, quantization, and SVD-based methods, while achieving higher\ncompression ratios. These results support KVTC as a practical building block\nfor memory-efficient LLM serving with reusable KV caches."
                },
                "authors": [
                    {
                        "name": "Konrad Staniszewski"
                    },
                    {
                        "name": "Adrian acucki"
                    }
                ],
                "author_detail": {
                    "name": "Adrian acucki"
                },
                "author": "Adrian acucki",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.01815v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.01815v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.01633v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.01633v1",
                "updated": "2025-11-03T14:42:53Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    14,
                    42,
                    53,
                    0,
                    307,
                    0
                ],
                "published": "2025-11-03T14:42:53Z",
                "published_parsed": [
                    2025,
                    11,
                    3,
                    14,
                    42,
                    53,
                    0,
                    307,
                    0
                ],
                "title": "Scaling Graph Chain-of-Thought Reasoning: A Multi-Agent Framework with\n  Efficient LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Graph Chain-of-Thought Reasoning: A Multi-Agent Framework with\n  Efficient LLM Serving"
                },
                "summary": "Graph Chain-of-Thought (Graph-CoT) enables large language models (LLMs) to\nperform step-by-step reasoning over graph-structured knowledge, but existing\npipelines suffer from low accuracy, excessive token usage, high latency, and\nlow throughput due to single-agent monolithic prompts, repeated context\nre-encoding, and inefficient serving execution. We present GLM, the first\nmulti-agent Graph-CoT system co-designed with an optimized LLM serving\narchitecture. GLM decomposes reasoning into specialized agents for\nclassification, reasoning, action generation, and graph retrieval, enabling\nbranching and selective context sharing to reduce prompt length and reasoning\niterations while preserving reasoning quality, thereby improving accuracy and\nreducing overall token consumption. To scale inference, we introduce a\nGraph-CoT-aware LLM inference mechanism with graph-specific KV-cache\nmanagement, priority-based eviction, and pipelined execution to improve serving\nefficiency. Experiments demonstrate that GLM improves answer accuracy by up to\n38%, reduces token cost by up to 95.7%, lowers inference latency by 90.3%, and\nachieves up to 15.1x higher throughput compared to state-of-the-art Graph-CoT\nbaselines, enabling efficient adoption for complex real-world reasoning at\nscale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Chain-of-Thought (Graph-CoT) enables large language models (LLMs) to\nperform step-by-step reasoning over graph-structured knowledge, but existing\npipelines suffer from low accuracy, excessive token usage, high latency, and\nlow throughput due to single-agent monolithic prompts, repeated context\nre-encoding, and inefficient serving execution. We present GLM, the first\nmulti-agent Graph-CoT system co-designed with an optimized LLM serving\narchitecture. GLM decomposes reasoning into specialized agents for\nclassification, reasoning, action generation, and graph retrieval, enabling\nbranching and selective context sharing to reduce prompt length and reasoning\niterations while preserving reasoning quality, thereby improving accuracy and\nreducing overall token consumption. To scale inference, we introduce a\nGraph-CoT-aware LLM inference mechanism with graph-specific KV-cache\nmanagement, priority-based eviction, and pipelined execution to improve serving\nefficiency. Experiments demonstrate that GLM improves answer accuracy by up to\n38%, reduces token cost by up to 95.7%, lowers inference latency by 90.3%, and\nachieves up to 15.1x higher throughput compared to state-of-the-art Graph-CoT\nbaselines, enabling efficient adoption for complex real-world reasoning at\nscale."
                },
                "authors": [
                    {
                        "name": "Chengying Huan"
                    },
                    {
                        "name": "Ziheng Meng"
                    },
                    {
                        "name": "Yongchao Liu"
                    },
                    {
                        "name": "Zhengyi Yang"
                    },
                    {
                        "name": "Yun Zhu"
                    },
                    {
                        "name": "Yue Yun"
                    },
                    {
                        "name": "Shipeng Li"
                    },
                    {
                        "name": "Rong Gu"
                    },
                    {
                        "name": "Xiabao Wu"
                    },
                    {
                        "name": "Haitao Zhang"
                    },
                    {
                        "name": "Chuntao Hong"
                    },
                    {
                        "name": "Shaonan Ma"
                    },
                    {
                        "name": "Guihai Chen"
                    },
                    {
                        "name": "Chen Tian"
                    }
                ],
                "author_detail": {
                    "name": "Chen Tian"
                },
                "author": "Chen Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.01633v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.01633v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21590v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21590v2",
                "updated": "2025-11-03T11:32:13Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    11,
                    32,
                    13,
                    0,
                    307,
                    0
                ],
                "published": "2025-06-18T05:07:47Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    5,
                    7,
                    47,
                    2,
                    169,
                    0
                ],
                "title": "Representation Consistency for Accurate and Coherent LLM Answer\n  Aggregation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Representation Consistency for Accurate and Coherent LLM Answer\n  Aggregation"
                },
                "summary": "Test-time scaling improves large language models' (LLMs) performance by\nallocating more compute budget during inference. To achieve this, existing\nmethods often require intricate modifications to prompting and sampling\nstrategies. In this work, we introduce representation consistency (RC), a\ntest-time scaling method for aggregating answers drawn from multiple candidate\nresponses of an LLM regardless of how they were generated, including variations\nin prompt phrasing and sampling strategy. RC enhances answer aggregation by not\nonly considering the number of occurrences of each answer in the candidate\nresponse set, but also the consistency of the model's internal activations\nwhile generating the set of responses leading to each answer. These activations\ncan be either dense (raw model activations) or sparse (encoded via pretrained\nsparse autoencoders). Our rationale is that if the model's representations of\nmultiple responses converging on the same answer are highly variable, this\nanswer is more likely to be the result of incoherent reasoning and should be\ndown-weighted during aggregation. Importantly, our method only uses cached\nactivations and lightweight similarity computations and requires no additional\nmodel queries. Through experiments with four open-source LLMs and four\nreasoning datasets, we validate the effectiveness of RC for improving task\nperformance during inference, with consistent accuracy improvements (up to 4%)\nover strong test-time scaling baselines. We also show that consistency in the\nsparse activation signals aligns well with the common notion of coherent\nreasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time scaling improves large language models' (LLMs) performance by\nallocating more compute budget during inference. To achieve this, existing\nmethods often require intricate modifications to prompting and sampling\nstrategies. In this work, we introduce representation consistency (RC), a\ntest-time scaling method for aggregating answers drawn from multiple candidate\nresponses of an LLM regardless of how they were generated, including variations\nin prompt phrasing and sampling strategy. RC enhances answer aggregation by not\nonly considering the number of occurrences of each answer in the candidate\nresponse set, but also the consistency of the model's internal activations\nwhile generating the set of responses leading to each answer. These activations\ncan be either dense (raw model activations) or sparse (encoded via pretrained\nsparse autoencoders). Our rationale is that if the model's representations of\nmultiple responses converging on the same answer are highly variable, this\nanswer is more likely to be the result of incoherent reasoning and should be\ndown-weighted during aggregation. Importantly, our method only uses cached\nactivations and lightweight similarity computations and requires no additional\nmodel queries. Through experiments with four open-source LLMs and four\nreasoning datasets, we validate the effectiveness of RC for improving task\nperformance during inference, with consistent accuracy improvements (up to 4%)\nover strong test-time scaling baselines. We also show that consistency in the\nsparse activation signals aligns well with the common notion of coherent\nreasoning."
                },
                "authors": [
                    {
                        "name": "Junqi Jiang"
                    },
                    {
                        "name": "Tom Bewley"
                    },
                    {
                        "name": "Salim I. Amoukou"
                    },
                    {
                        "name": "Francesco Leofante"
                    },
                    {
                        "name": "Antonio Rago"
                    },
                    {
                        "name": "Saumitra Mishra"
                    },
                    {
                        "name": "Francesca Toni"
                    }
                ],
                "author_detail": {
                    "name": "Francesca Toni"
                },
                "author": "Francesca Toni",
                "arxiv_comment": "Accepted at NeurIPS 2025. Camera-ready version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21590v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21590v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.01385v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.01385v1",
                "updated": "2025-11-03T09:36:11Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    9,
                    36,
                    11,
                    0,
                    307,
                    0
                ],
                "published": "2025-11-03T09:36:11Z",
                "published_parsed": [
                    2025,
                    11,
                    3,
                    9,
                    36,
                    11,
                    0,
                    307,
                    0
                ],
                "title": "Memory-Efficient Training with In-Place FFT Implementation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory-Efficient Training with In-Place FFT Implementation"
                },
                "summary": "Fast Fourier Transforms (FFT) are widely used to reduce memory and\ncomputational costs in deep learning. However, existing implementations,\nincluding standard FFT and real FFT (rFFT), cannot achieve true in-place\ncomputation. In particular, rFFT maps an input of size n to a complex output of\nsize n/2+1, causing dimensional mismatch and requiring additional memory\nallocation. We propose the first real-domain, fully in-place FFT framework\n(rdFFT) that preserves input-output memory space consistency. By leveraging\nbutterfly operation symmetry and conjugate properties in the frequency domain,\nwe design an implicit complex encoding scheme that eliminates intermediate\ncache usage entirely. Experiments on multiple natural language understanding\ntasks demonstrate the method effectiveness in reducing training memory cost,\noffering a promising direction for frequency-domain lightweight adaptation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast Fourier Transforms (FFT) are widely used to reduce memory and\ncomputational costs in deep learning. However, existing implementations,\nincluding standard FFT and real FFT (rFFT), cannot achieve true in-place\ncomputation. In particular, rFFT maps an input of size n to a complex output of\nsize n/2+1, causing dimensional mismatch and requiring additional memory\nallocation. We propose the first real-domain, fully in-place FFT framework\n(rdFFT) that preserves input-output memory space consistency. By leveraging\nbutterfly operation symmetry and conjugate properties in the frequency domain,\nwe design an implicit complex encoding scheme that eliminates intermediate\ncache usage entirely. Experiments on multiple natural language understanding\ntasks demonstrate the method effectiveness in reducing training memory cost,\noffering a promising direction for frequency-domain lightweight adaptation."
                },
                "authors": [
                    {
                        "name": "Xinyu Ding"
                    },
                    {
                        "name": "Bangtian Liu"
                    },
                    {
                        "name": "Siyu Liao"
                    },
                    {
                        "name": "Zhongfeng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhongfeng Wang"
                },
                "author": "Zhongfeng Wang",
                "arxiv_comment": "Accepted at NeurIPS 2025. Presents a real-domain in-place FFT (rdFFT)\n  operator for memory-efficient fine-tuning of large language models",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.01385v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.01385v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6; G.1.2; D.1.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.01266v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.01266v1",
                "updated": "2025-11-03T06:37:53Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    6,
                    37,
                    53,
                    0,
                    307,
                    0
                ],
                "published": "2025-11-03T06:37:53Z",
                "published_parsed": [
                    2025,
                    11,
                    3,
                    6,
                    37,
                    53,
                    0,
                    307,
                    0
                ],
                "title": "MotionStream: Real-Time Video Generation with Interactive Motion\n  Controls",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MotionStream: Real-Time Video Generation with Interactive Motion\n  Controls"
                },
                "summary": "Current motion-conditioned video generation methods suffer from prohibitive\nlatency (minutes per video) and non-causal processing that prevents real-time\ninteraction. We present MotionStream, enabling sub-second latency with up to 29\nFPS streaming generation on a single GPU. Our approach begins by augmenting a\ntext-to-video model with motion control, which generates high-quality videos\nthat adhere to the global text prompt and local motion guidance, but does not\nperform inference on the fly. As such, we distill this bidirectional teacher\ninto a causal student through Self Forcing with Distribution Matching\nDistillation, enabling real-time streaming inference. Several key challenges\narise when generating videos of long, potentially infinite time-horizons: (1)\nbridging the domain gap from training on finite length and extrapolating to\ninfinite horizons, (2) sustaining high quality by preventing error\naccumulation, and (3) maintaining fast inference, without incurring growth in\ncomputational cost due to increasing context windows. A key to our approach is\nintroducing carefully designed sliding-window causal attention, combined with\nattention sinks. By incorporating self-rollout with attention sinks and KV\ncache rolling during training, we properly simulate inference-time\nextrapolations with a fixed context window, enabling constant-speed generation\nof arbitrarily long videos. Our models achieve state-of-the-art results in\nmotion following and video quality while being two orders of magnitude faster,\nuniquely enabling infinite-length streaming. With MotionStream, users can paint\ntrajectories, control cameras, or transfer motion, and see results unfold in\nreal-time, delivering a truly interactive experience.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current motion-conditioned video generation methods suffer from prohibitive\nlatency (minutes per video) and non-causal processing that prevents real-time\ninteraction. We present MotionStream, enabling sub-second latency with up to 29\nFPS streaming generation on a single GPU. Our approach begins by augmenting a\ntext-to-video model with motion control, which generates high-quality videos\nthat adhere to the global text prompt and local motion guidance, but does not\nperform inference on the fly. As such, we distill this bidirectional teacher\ninto a causal student through Self Forcing with Distribution Matching\nDistillation, enabling real-time streaming inference. Several key challenges\narise when generating videos of long, potentially infinite time-horizons: (1)\nbridging the domain gap from training on finite length and extrapolating to\ninfinite horizons, (2) sustaining high quality by preventing error\naccumulation, and (3) maintaining fast inference, without incurring growth in\ncomputational cost due to increasing context windows. A key to our approach is\nintroducing carefully designed sliding-window causal attention, combined with\nattention sinks. By incorporating self-rollout with attention sinks and KV\ncache rolling during training, we properly simulate inference-time\nextrapolations with a fixed context window, enabling constant-speed generation\nof arbitrarily long videos. Our models achieve state-of-the-art results in\nmotion following and video quality while being two orders of magnitude faster,\nuniquely enabling infinite-length streaming. With MotionStream, users can paint\ntrajectories, control cameras, or transfer motion, and see results unfold in\nreal-time, delivering a truly interactive experience."
                },
                "authors": [
                    {
                        "name": "Joonghyuk Shin"
                    },
                    {
                        "name": "Zhengqi Li"
                    },
                    {
                        "name": "Richard Zhang"
                    },
                    {
                        "name": "Jun-Yan Zhu"
                    },
                    {
                        "name": "Jaesik Park"
                    },
                    {
                        "name": "Eli Schechtman"
                    },
                    {
                        "name": "Xun Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xun Huang"
                },
                "author": "Xun Huang",
                "arxiv_comment": "Project webpage: https://joonghyuk.com/motionstream-web/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.01266v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.01266v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13544v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13544v3",
                "updated": "2025-11-02T20:27:27Z",
                "updated_parsed": [
                    2025,
                    11,
                    2,
                    20,
                    27,
                    27,
                    6,
                    306,
                    0
                ],
                "published": "2025-05-19T02:09:41Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    2,
                    9,
                    41,
                    0,
                    139,
                    0
                ],
                "title": "Multi-head Temporal Latent Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-head Temporal Latent Attention"
                },
                "summary": "While Transformer self-attention offers strong parallelism, the Key-Value\n(KV) cache grows linearly with sequence length and becomes a bottleneck for\ninference efficiency. Multi-head latent attention was recently developed to\ncompress the KV cache into a low-rank latent space. This paper proposes\nMulti-head Temporal Latent Attention (MTLA), which further reduces the KV cache\nsize along the temporal dimension, greatly lowering the memory footprint of\nself-attention inference. MTLA employs a hyper-network to dynamically merge\ntemporally adjacent KV cache vectors. To address the mismatch between the\ncompressed KV cache and processed sequence lengths, a stride-aware causal mask\nis proposed to ensure efficient parallel training and consistency with\ninference behaviour. Experiments across tasks, including speech translation,\nspeech recognition, speech understanding and text summarisation, demonstrate\nthat MTLA achieves competitive performance compared to standard Multi-Head\nAttention (MHA), while greatly improving inference speed and GPU memory usage.\nFor example, on a English-German speech translation task, MTLA achieves a 5.3x\nspeedup and a reduction in GPU memory usage by a factor of 8.3 compared to MHA,\nwhile maintaining translation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Transformer self-attention offers strong parallelism, the Key-Value\n(KV) cache grows linearly with sequence length and becomes a bottleneck for\ninference efficiency. Multi-head latent attention was recently developed to\ncompress the KV cache into a low-rank latent space. This paper proposes\nMulti-head Temporal Latent Attention (MTLA), which further reduces the KV cache\nsize along the temporal dimension, greatly lowering the memory footprint of\nself-attention inference. MTLA employs a hyper-network to dynamically merge\ntemporally adjacent KV cache vectors. To address the mismatch between the\ncompressed KV cache and processed sequence lengths, a stride-aware causal mask\nis proposed to ensure efficient parallel training and consistency with\ninference behaviour. Experiments across tasks, including speech translation,\nspeech recognition, speech understanding and text summarisation, demonstrate\nthat MTLA achieves competitive performance compared to standard Multi-Head\nAttention (MHA), while greatly improving inference speed and GPU memory usage.\nFor example, on a English-German speech translation task, MTLA achieves a 5.3x\nspeedup and a reduction in GPU memory usage by a factor of 8.3 compared to MHA,\nwhile maintaining translation quality."
                },
                "authors": [
                    {
                        "name": "Keqi Deng"
                    },
                    {
                        "name": "Philip C. Woodland"
                    }
                ],
                "author_detail": {
                    "name": "Philip C. Woodland"
                },
                "author": "Philip C. Woodland",
                "arxiv_comment": "Accepted by NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13544v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13544v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.00868v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.00868v1",
                "updated": "2025-11-02T09:33:12Z",
                "updated_parsed": [
                    2025,
                    11,
                    2,
                    9,
                    33,
                    12,
                    6,
                    306,
                    0
                ],
                "published": "2025-11-02T09:33:12Z",
                "published_parsed": [
                    2025,
                    11,
                    2,
                    9,
                    33,
                    12,
                    6,
                    306,
                    0
                ],
                "title": "FlexiCache: Leveraging Temporal Stability of Attention Heads for\n  Efficient KV Cache Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlexiCache: Leveraging Temporal Stability of Attention Heads for\n  Efficient KV Cache Management"
                },
                "summary": "Large Language Model (LLM) serving is increasingly constrained by the growing\nsize of the key-value (KV) cache, which scales with both context length and\ngeneration length. Prior work shows that attention is dominated by a small\nsubset of critical tokens, yet existing systems struggle to exploit this\nefficiently without degrading accuracy, especially in long generation. We make\na key observation: the temporal stability of these critical tokens varies\nsignificantly across KV heads: some heads consistently focus on the same\ntokens, while others shift frequently. Building on this insight, we introduce\nFlexiCache, a hierarchical KV-cache management system that leverages the\ntemporal stability of KV heads to reduce GPU memory usage and computation\noverhead, while preserving model accuracy. FlexiCache classifies KV heads as\nstable or unstable: it retains all KV-cache pages from unstable heads in GPU\nmemory, whereas for stable heads, it keeps only the top-K pages on the GPU and\noffloads the rest to host memory. By exploiting temporal stability, FlexiCache\nperforms periodic reranking for stable heads to fetch newly promoted top pages.\nImplemented atop vLLM, FlexiCache reduces GPU memory footprint for long-context\nrequests by up to 70%, improves offline serving throughput by 1.38-1.55x, and\nlowers online token latency by 1.6-2.1x, all while maintaining accuracy in\nlong-context, long-generation scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) serving is increasingly constrained by the growing\nsize of the key-value (KV) cache, which scales with both context length and\ngeneration length. Prior work shows that attention is dominated by a small\nsubset of critical tokens, yet existing systems struggle to exploit this\nefficiently without degrading accuracy, especially in long generation. We make\na key observation: the temporal stability of these critical tokens varies\nsignificantly across KV heads: some heads consistently focus on the same\ntokens, while others shift frequently. Building on this insight, we introduce\nFlexiCache, a hierarchical KV-cache management system that leverages the\ntemporal stability of KV heads to reduce GPU memory usage and computation\noverhead, while preserving model accuracy. FlexiCache classifies KV heads as\nstable or unstable: it retains all KV-cache pages from unstable heads in GPU\nmemory, whereas for stable heads, it keeps only the top-K pages on the GPU and\noffloads the rest to host memory. By exploiting temporal stability, FlexiCache\nperforms periodic reranking for stable heads to fetch newly promoted top pages.\nImplemented atop vLLM, FlexiCache reduces GPU memory footprint for long-context\nrequests by up to 70%, improves offline serving throughput by 1.38-1.55x, and\nlowers online token latency by 1.6-2.1x, all while maintaining accuracy in\nlong-context, long-generation scenarios."
                },
                "authors": [
                    {
                        "name": "Nazmul Takbir"
                    },
                    {
                        "name": "Hamidreza Alikhani"
                    },
                    {
                        "name": "Nikil Dutt"
                    },
                    {
                        "name": "Sangeetha Abdu Jyothi"
                    }
                ],
                "author_detail": {
                    "name": "Sangeetha Abdu Jyothi"
                },
                "author": "Sangeetha Abdu Jyothi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.00868v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.00868v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.00819v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.00819v1",
                "updated": "2025-11-02T06:15:14Z",
                "updated_parsed": [
                    2025,
                    11,
                    2,
                    6,
                    15,
                    14,
                    6,
                    306,
                    0
                ],
                "published": "2025-11-02T06:15:14Z",
                "published_parsed": [
                    2025,
                    11,
                    2,
                    6,
                    15,
                    14,
                    6,
                    306,
                    0
                ],
                "title": "Optimizing Native Sparse Attention with Latent Attention and Local\n  Global Alternating Strategies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Native Sparse Attention with Latent Attention and Local\n  Global Alternating Strategies"
                },
                "summary": "In this work, we conduct a systematic analysis of Native Sparse Attention\n(NSA) and propose targeted improvements that enhance long-context modeling. A\nkey insight is that alternating between local (sliding-window) and global\n(compression, selective) attention across layers, rather than using fixed\npatterns, enables more effective propagation of long-range dependencies and\nsubstantially boosts performance on long-sequence tasks. Meanwhile, we further\nrefine NSA's branches with Latent Attention that the sliding-window branch is\nenhanced with Multi-head Latent Attention (MLA) while compression and selective\nbranches adopt Group-head Latent Attention (GLA). These changes reduce KV-cache\nmemory by 50\\% versus NSA while improving the model's common-sense reasoning\nand long-text understanding capabilities. Experiments on models from 340M to\n1.3B parameters (trained on 15B and 100B tokens) show our method matches or\nexceeds full attention and native sparse attention in both common-sense\nreasoning and long-context understanding tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we conduct a systematic analysis of Native Sparse Attention\n(NSA) and propose targeted improvements that enhance long-context modeling. A\nkey insight is that alternating between local (sliding-window) and global\n(compression, selective) attention across layers, rather than using fixed\npatterns, enables more effective propagation of long-range dependencies and\nsubstantially boosts performance on long-sequence tasks. Meanwhile, we further\nrefine NSA's branches with Latent Attention that the sliding-window branch is\nenhanced with Multi-head Latent Attention (MLA) while compression and selective\nbranches adopt Group-head Latent Attention (GLA). These changes reduce KV-cache\nmemory by 50\\% versus NSA while improving the model's common-sense reasoning\nand long-text understanding capabilities. Experiments on models from 340M to\n1.3B parameters (trained on 15B and 100B tokens) show our method matches or\nexceeds full attention and native sparse attention in both common-sense\nreasoning and long-context understanding tasks."
                },
                "authors": [
                    {
                        "name": "Yuxuan Hu"
                    },
                    {
                        "name": "Jianchao Tan"
                    },
                    {
                        "name": "Jiaqi Zhang"
                    },
                    {
                        "name": "Wen Zan"
                    },
                    {
                        "name": "Pingwei Sun"
                    },
                    {
                        "name": "Yifan Lu"
                    },
                    {
                        "name": "Yerui Sun"
                    },
                    {
                        "name": "Yuchen Xie"
                    },
                    {
                        "name": "Xunliang Cai"
                    },
                    {
                        "name": "Jing Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jing Zhang"
                },
                "author": "Jing Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.00819v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.00819v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.00745v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.00745v1",
                "updated": "2025-11-02T00:04:54Z",
                "updated_parsed": [
                    2025,
                    11,
                    2,
                    0,
                    4,
                    54,
                    6,
                    306,
                    0
                ],
                "published": "2025-11-02T00:04:54Z",
                "published_parsed": [
                    2025,
                    11,
                    2,
                    0,
                    4,
                    54,
                    6,
                    306,
                    0
                ],
                "title": "High-Power Dual-Channel Field Chamber for High-Frequency Magnetic\n  Neuromodulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-Power Dual-Channel Field Chamber for High-Frequency Magnetic\n  Neuromodulation"
                },
                "summary": "Several novel methods, including magnetogenetics and magnetoelectric\nstimulation, use high frequency alternating magnetic fields to precisely\nmanipulate neural activity. To quantify the behavioral effects of such\ninterventions in a freely moving mouse, we developed a dual-channel magnetic\nchamber, specifically designed for rate-sensitive magnetothermal-genetic\nstimulation, and adaptable for other uses of alternating magnetic fields.\nThrough an optimized coil design, the system allows independent control of two\nspatially orthogonal uniform magnetic fields delivered at different frequencies\nwithin a 10 cm x 10 cm x 6 cm chamber. The two channels have nominal\nfrequencies of 50 and 550 kHz with peak magnetic field strengths of 88 and 12.5\nmT, achieved with resonant coil drives having peak voltages of 1.6 and 1.8 kV\nand currents of 1.0 and 0.26 kA, respectively. Additionally, a liquid cooling\nsystem enables magnetic field generation for second-level duration, and an\nobservation port and camera allow video capture of the animal's behavior within\nthe chamber. The system generates high-amplitude magnetic fields across two\nwidely separated frequency channels with negligible interference (< 1%).\nRelatively uniform magnetic field distribution (+/-10% across 94% of the\nchamber volume) is maintained throughout the chamber, and temperature increase\nof the inner side of the coil enclosure during the operation is limited to <\n0.35 {\\deg}C/s to ensure in vivo safety. Using cobalt-doped and undoped iron\noxide nanoparticles, we demonstrate channel-specific heating rates of 3.5\n{\\deg}C/s and 1.5 {\\deg}C/s, respectively, validating frequency-selectivity.\nBoth channels can run continuously for four seconds stably.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Several novel methods, including magnetogenetics and magnetoelectric\nstimulation, use high frequency alternating magnetic fields to precisely\nmanipulate neural activity. To quantify the behavioral effects of such\ninterventions in a freely moving mouse, we developed a dual-channel magnetic\nchamber, specifically designed for rate-sensitive magnetothermal-genetic\nstimulation, and adaptable for other uses of alternating magnetic fields.\nThrough an optimized coil design, the system allows independent control of two\nspatially orthogonal uniform magnetic fields delivered at different frequencies\nwithin a 10 cm x 10 cm x 6 cm chamber. The two channels have nominal\nfrequencies of 50 and 550 kHz with peak magnetic field strengths of 88 and 12.5\nmT, achieved with resonant coil drives having peak voltages of 1.6 and 1.8 kV\nand currents of 1.0 and 0.26 kA, respectively. Additionally, a liquid cooling\nsystem enables magnetic field generation for second-level duration, and an\nobservation port and camera allow video capture of the animal's behavior within\nthe chamber. The system generates high-amplitude magnetic fields across two\nwidely separated frequency channels with negligible interference (< 1%).\nRelatively uniform magnetic field distribution (+/-10% across 94% of the\nchamber volume) is maintained throughout the chamber, and temperature increase\nof the inner side of the coil enclosure during the operation is limited to <\n0.35 {\\deg}C/s to ensure in vivo safety. Using cobalt-doped and undoped iron\noxide nanoparticles, we demonstrate channel-specific heating rates of 3.5\n{\\deg}C/s and 1.5 {\\deg}C/s, respectively, validating frequency-selectivity.\nBoth channels can run continuously for four seconds stably."
                },
                "authors": [
                    {
                        "name": "Xiaoyang Tian"
                    },
                    {
                        "name": "Hui Wang"
                    },
                    {
                        "name": "Boshuo Wang"
                    },
                    {
                        "name": "Jinshui Zhang"
                    },
                    {
                        "name": "Dong Yan"
                    },
                    {
                        "name": "Jeannette Ingabire"
                    },
                    {
                        "name": "Samantha Coffler"
                    },
                    {
                        "name": "Guillaume Duret"
                    },
                    {
                        "name": "Quoc-Khanh Pham"
                    },
                    {
                        "name": "Gang Bao"
                    },
                    {
                        "name": "Jacob T. Robinson"
                    },
                    {
                        "name": "Stefan M. Goetz"
                    },
                    {
                        "name": "Angel V. Peterchev"
                    }
                ],
                "author_detail": {
                    "name": "Angel V. Peterchev"
                },
                "author": "Angel V. Peterchev",
                "arxiv_comment": "25 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.00745v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.00745v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26692v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26692v2",
                "updated": "2025-11-01T12:05:18Z",
                "updated_parsed": [
                    2025,
                    11,
                    1,
                    12,
                    5,
                    18,
                    5,
                    305,
                    0
                ],
                "published": "2025-10-30T16:59:43Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    16,
                    59,
                    43,
                    3,
                    303,
                    0
                ],
                "title": "Kimi Linear: An Expressive, Efficient Attention Architecture",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kimi Linear: An Expressive, Efficient Attention Architecture"
                },
                "summary": "We introduce Kimi Linear, a hybrid linear attention architecture that, for\nthe first time, outperforms full attention under fair comparisons across\nvarious scenarios -- including short-context, long-context, and reinforcement\nlearning (RL) scaling regimes. At its core lies Kimi Delta Attention (KDA), an\nexpressive linear attention module that extends Gated DeltaNet with a\nfiner-grained gating mechanism, enabling more effective use of limited\nfinite-state RNN memory. Our bespoke chunkwise algorithm achieves high hardware\nefficiency through a specialized variant of the Diagonal-Plus-Low-Rank (DPLR)\ntransition matrices, which substantially reduces computation compared to the\ngeneral DPLR formulation while remaining more consistent with the classical\ndelta rule.\n  We pretrain a Kimi Linear model with 3B activated parameters and 48B total\nparameters, based on a layerwise hybrid of KDA and Multi-Head Latent Attention\n(MLA). Our experiments show that with an identical training recipe, Kimi Linear\noutperforms full MLA with a sizeable margin across all evaluated tasks, while\nreducing KV cache usage by up to 75% and achieving up to 6 times decoding\nthroughput for a 1M context. These results demonstrate that Kimi Linear can be\na drop-in replacement for full attention architectures with superior\nperformance and efficiency, including tasks with longer input and output\nlengths.\n  To support further research, we open-source the KDA kernel and vLLM\nimplementations, and release the pre-trained and instruction-tuned model\ncheckpoints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Kimi Linear, a hybrid linear attention architecture that, for\nthe first time, outperforms full attention under fair comparisons across\nvarious scenarios -- including short-context, long-context, and reinforcement\nlearning (RL) scaling regimes. At its core lies Kimi Delta Attention (KDA), an\nexpressive linear attention module that extends Gated DeltaNet with a\nfiner-grained gating mechanism, enabling more effective use of limited\nfinite-state RNN memory. Our bespoke chunkwise algorithm achieves high hardware\nefficiency through a specialized variant of the Diagonal-Plus-Low-Rank (DPLR)\ntransition matrices, which substantially reduces computation compared to the\ngeneral DPLR formulation while remaining more consistent with the classical\ndelta rule.\n  We pretrain a Kimi Linear model with 3B activated parameters and 48B total\nparameters, based on a layerwise hybrid of KDA and Multi-Head Latent Attention\n(MLA). Our experiments show that with an identical training recipe, Kimi Linear\noutperforms full MLA with a sizeable margin across all evaluated tasks, while\nreducing KV cache usage by up to 75% and achieving up to 6 times decoding\nthroughput for a 1M context. These results demonstrate that Kimi Linear can be\na drop-in replacement for full attention architectures with superior\nperformance and efficiency, including tasks with longer input and output\nlengths.\n  To support further research, we open-source the KDA kernel and vLLM\nimplementations, and release the pre-trained and instruction-tuned model\ncheckpoints."
                },
                "authors": [
                    {
                        "name": "Kimi Team"
                    },
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Zongyu Lin"
                    },
                    {
                        "name": "Xingcheng Yao"
                    },
                    {
                        "name": "Jiaxi Hu"
                    },
                    {
                        "name": "Fanqing Meng"
                    },
                    {
                        "name": "Chengyin Liu"
                    },
                    {
                        "name": "Xin Men"
                    },
                    {
                        "name": "Songlin Yang"
                    },
                    {
                        "name": "Zhiyuan Li"
                    },
                    {
                        "name": "Wentao Li"
                    },
                    {
                        "name": "Enzhe Lu"
                    },
                    {
                        "name": "Weizhou Liu"
                    },
                    {
                        "name": "Yanru Chen"
                    },
                    {
                        "name": "Weixin Xu"
                    },
                    {
                        "name": "Longhui Yu"
                    },
                    {
                        "name": "Yejie Wang"
                    },
                    {
                        "name": "Yu Fan"
                    },
                    {
                        "name": "Longguang Zhong"
                    },
                    {
                        "name": "Enming Yuan"
                    },
                    {
                        "name": "Dehao Zhang"
                    },
                    {
                        "name": "Yizhi Zhang"
                    },
                    {
                        "name": "T. Y. Liu"
                    },
                    {
                        "name": "Haiming Wang"
                    },
                    {
                        "name": "Shengjun Fang"
                    },
                    {
                        "name": "Weiran He"
                    },
                    {
                        "name": "Shaowei Liu"
                    },
                    {
                        "name": "Yiwei Li"
                    },
                    {
                        "name": "Jianlin Su"
                    },
                    {
                        "name": "Jiezhong Qiu"
                    },
                    {
                        "name": "Bo Pang"
                    },
                    {
                        "name": "Junjie Yan"
                    },
                    {
                        "name": "Zhejun Jiang"
                    },
                    {
                        "name": "Weixiao Huang"
                    },
                    {
                        "name": "Bohong Yin"
                    },
                    {
                        "name": "Jiacheng You"
                    },
                    {
                        "name": "Chu Wei"
                    },
                    {
                        "name": "Zhengtao Wang"
                    },
                    {
                        "name": "Chao Hong"
                    },
                    {
                        "name": "Yutian Chen"
                    },
                    {
                        "name": "Guanduo Chen"
                    },
                    {
                        "name": "Yucheng Wang"
                    },
                    {
                        "name": "Huabin Zheng"
                    },
                    {
                        "name": "Feng Wang"
                    },
                    {
                        "name": "Yibo Liu"
                    },
                    {
                        "name": "Mengnan Dong"
                    },
                    {
                        "name": "Zheng Zhang"
                    },
                    {
                        "name": "Siyuan Pan"
                    },
                    {
                        "name": "Wenhao Wu"
                    },
                    {
                        "name": "Yuhao Wu"
                    },
                    {
                        "name": "Longyu Guan"
                    },
                    {
                        "name": "Jiawen Tao"
                    },
                    {
                        "name": "Guohong Fu"
                    },
                    {
                        "name": "Xinran Xu"
                    },
                    {
                        "name": "Yuzhi Wang"
                    },
                    {
                        "name": "Guokun Lai"
                    },
                    {
                        "name": "Yuxin Wu"
                    },
                    {
                        "name": "Xinyu Zhou"
                    },
                    {
                        "name": "Zhilin Yang"
                    },
                    {
                        "name": "Yulun Du"
                    }
                ],
                "author_detail": {
                    "name": "Yulun Du"
                },
                "author": "Yulun Du",
                "arxiv_comment": "Kimi Linear tech report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26692v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26692v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.00473v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.00473v1",
                "updated": "2025-11-01T09:53:47Z",
                "updated_parsed": [
                    2025,
                    11,
                    1,
                    9,
                    53,
                    47,
                    5,
                    305,
                    0
                ],
                "published": "2025-11-01T09:53:47Z",
                "published_parsed": [
                    2025,
                    11,
                    1,
                    9,
                    53,
                    47,
                    5,
                    305,
                    0
                ],
                "title": "Drinfeld associators and Kashiwara-Vergne associators in higher genera",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Drinfeld associators and Kashiwara-Vergne associators in higher genera"
                },
                "summary": "For $g\\geq 0$, a genus $g$ Kashiwara-Vergne associator, introduced by\nAlekseev-Kawazumi-Kuno-Naef as a solution to the generalised KV equations in\nrelation to the formality problem of the Goldman-Turaev Lie bialgebra on an\noriented surface with a framing, is directly constructed from a genus $g$\nanalogue of a Drinfeld associator formulated by Gonzalez, which we call a\nGonzalez-Drinfeld associator. The proof is based on Massuyeau's work in genus\n0. The framing is automatically determined from the choice of a\nGonzalez-Drinfeld associator, and in the case of genus 1, we show that only one\nparticular framing is realised by our construction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "For $g\\geq 0$, a genus $g$ Kashiwara-Vergne associator, introduced by\nAlekseev-Kawazumi-Kuno-Naef as a solution to the generalised KV equations in\nrelation to the formality problem of the Goldman-Turaev Lie bialgebra on an\noriented surface with a framing, is directly constructed from a genus $g$\nanalogue of a Drinfeld associator formulated by Gonzalez, which we call a\nGonzalez-Drinfeld associator. The proof is based on Massuyeau's work in genus\n0. The framing is automatically determined from the choice of a\nGonzalez-Drinfeld associator, and in the case of genus 1, we show that only one\nparticular framing is realised by our construction."
                },
                "authors": [
                    {
                        "name": "Toyo Taniguchi"
                    }
                ],
                "author_detail": {
                    "name": "Toyo Taniguchi"
                },
                "author": "Toyo Taniguchi",
                "arxiv_comment": "40 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.00473v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.00473v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.QA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.QA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.AT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "57K20(primary), 16T05, 16W70, 18M75, 20F36, 20F40, 57M05 (secondary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19755v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19755v3",
                "updated": "2025-11-01T08:49:20Z",
                "updated_parsed": [
                    2025,
                    11,
                    1,
                    8,
                    49,
                    20,
                    5,
                    305,
                    0
                ],
                "published": "2025-10-22T16:46:05Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    16,
                    46,
                    5,
                    2,
                    295,
                    0
                ],
                "title": "A Survey on Cache Methods in Diffusion Models: Toward Efficient\n  Multi-Modal Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Cache Methods in Diffusion Models: Toward Efficient\n  Multi-Modal Generation"
                },
                "summary": "Diffusion Models have become a cornerstone of modern generative AI for their\nexceptional generation quality and controllability. However, their inherent\n\\textit{multi-step iterations} and \\textit{complex backbone networks} lead to\nprohibitive computational overhead and generation latency, forming a major\nbottleneck for real-time applications. Although existing acceleration\ntechniques have made progress, they still face challenges such as limited\napplicability, high training costs, or quality degradation.\n  Against this backdrop, \\textbf{Diffusion Caching} offers a promising\ntraining-free, architecture-agnostic, and efficient inference paradigm. Its\ncore mechanism identifies and reuses intrinsic computational redundancies in\nthe diffusion process. By enabling feature-level cross-step reuse and\ninter-layer scheduling, it reduces computation without modifying model\nparameters. This paper systematically reviews the theoretical foundations and\nevolution of Diffusion Caching and proposes a unified framework for its\nclassification and analysis.\n  Through comparative analysis of representative methods, we show that\nDiffusion Caching evolves from \\textit{static reuse} to \\textit{dynamic\nprediction}. This trend enhances caching flexibility across diverse tasks and\nenables integration with other acceleration techniques such as sampling\noptimization and model distillation, paving the way for a unified, efficient\ninference framework for future multimodal and interactive applications. We\nargue that this paradigm will become a key enabler of real-time and efficient\ngenerative AI, injecting new vitality into both theory and practice of\n\\textit{Efficient Generative Intelligence}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Models have become a cornerstone of modern generative AI for their\nexceptional generation quality and controllability. However, their inherent\n\\textit{multi-step iterations} and \\textit{complex backbone networks} lead to\nprohibitive computational overhead and generation latency, forming a major\nbottleneck for real-time applications. Although existing acceleration\ntechniques have made progress, they still face challenges such as limited\napplicability, high training costs, or quality degradation.\n  Against this backdrop, \\textbf{Diffusion Caching} offers a promising\ntraining-free, architecture-agnostic, and efficient inference paradigm. Its\ncore mechanism identifies and reuses intrinsic computational redundancies in\nthe diffusion process. By enabling feature-level cross-step reuse and\ninter-layer scheduling, it reduces computation without modifying model\nparameters. This paper systematically reviews the theoretical foundations and\nevolution of Diffusion Caching and proposes a unified framework for its\nclassification and analysis.\n  Through comparative analysis of representative methods, we show that\nDiffusion Caching evolves from \\textit{static reuse} to \\textit{dynamic\nprediction}. This trend enhances caching flexibility across diverse tasks and\nenables integration with other acceleration techniques such as sampling\noptimization and model distillation, paving the way for a unified, efficient\ninference framework for future multimodal and interactive applications. We\nargue that this paradigm will become a key enabler of real-time and efficient\ngenerative AI, injecting new vitality into both theory and practice of\n\\textit{Efficient Generative Intelligence}."
                },
                "authors": [
                    {
                        "name": "Jiacheng Liu"
                    },
                    {
                        "name": "Xinyu Wang"
                    },
                    {
                        "name": "Yuqi Lin"
                    },
                    {
                        "name": "Zhikai Wang"
                    },
                    {
                        "name": "Peiru Wang"
                    },
                    {
                        "name": "Peiliang Cai"
                    },
                    {
                        "name": "Qinming Zhou"
                    },
                    {
                        "name": "Zhengan Yan"
                    },
                    {
                        "name": "Zexuan Yan"
                    },
                    {
                        "name": "Zhengyi Shi"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Yue Ma"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "22 pages,2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19755v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19755v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12872v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12872v2",
                "updated": "2025-11-01T08:26:24Z",
                "updated_parsed": [
                    2025,
                    11,
                    1,
                    8,
                    26,
                    24,
                    5,
                    305,
                    0
                ],
                "published": "2025-10-14T18:00:01Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    18,
                    0,
                    1,
                    1,
                    287,
                    0
                ],
                "title": "KVCOMM: Online Cross-context KV-cache Communication for Efficient\n  LLM-based Multi-agent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVCOMM: Online Cross-context KV-cache Communication for Efficient\n  LLM-based Multi-agent Systems"
                },
                "summary": "Multi-agent large language model (LLM) systems are increasingly adopted for\ncomplex language processing tasks that require communication and coordination\namong agents. However, these systems often suffer substantial overhead from\nrepeated reprocessing of overlapping contexts across agents. In typical\npipelines, once an agent receives a message from its predecessor, the full\ncontext-including prior turns-must be reprocessed from scratch, leading to\ninefficient processing. While key-value (KV) caching is an effective solution\nfor avoiding redundant computation in single-agent settings where prefixes\nremain unchanged, it cannot be directly reused in multi-agent scenarios due to\ndiverging prefixes introduced by agent-specific context extensions. We identify\nthat the core challenge lies in the offset variance of KV-caches across agents.\nTo address this, we propose KVCOMM, a training-free framework that enables\nefficient prefilling in multi-agent inference by reusing KV-caches and aligning\ncache offsets of overlapping contexts under diverse prefix contexts. KVCOMM\nestimates and adjusts KV-caches for shared content by referencing a pool of\ncached examples-termed anchors-that store observed cache deviations under\nvarying prefixes. The anchor pool is maintained and updated online, allowing\ndynamic adaptation to distinct user requests and context structures. KVCOMM\nachieves over 70% reuse rate across diverse multi-agent workloads, including\nretrieval-augmented generation, math reasoning, and collaborative coding tasks,\nall without quality degradation. Particularly, when each fully-connected agent\nreceives 1K input tokens with 512 prefix tokens and 512 output tokens under a\nfive-agent setting, KVCOMM achieves up to 7.8x speedup compared to the standard\nprefill pipeline, reducing TTFT from ~430 ms to ~55 ms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-agent large language model (LLM) systems are increasingly adopted for\ncomplex language processing tasks that require communication and coordination\namong agents. However, these systems often suffer substantial overhead from\nrepeated reprocessing of overlapping contexts across agents. In typical\npipelines, once an agent receives a message from its predecessor, the full\ncontext-including prior turns-must be reprocessed from scratch, leading to\ninefficient processing. While key-value (KV) caching is an effective solution\nfor avoiding redundant computation in single-agent settings where prefixes\nremain unchanged, it cannot be directly reused in multi-agent scenarios due to\ndiverging prefixes introduced by agent-specific context extensions. We identify\nthat the core challenge lies in the offset variance of KV-caches across agents.\nTo address this, we propose KVCOMM, a training-free framework that enables\nefficient prefilling in multi-agent inference by reusing KV-caches and aligning\ncache offsets of overlapping contexts under diverse prefix contexts. KVCOMM\nestimates and adjusts KV-caches for shared content by referencing a pool of\ncached examples-termed anchors-that store observed cache deviations under\nvarying prefixes. The anchor pool is maintained and updated online, allowing\ndynamic adaptation to distinct user requests and context structures. KVCOMM\nachieves over 70% reuse rate across diverse multi-agent workloads, including\nretrieval-augmented generation, math reasoning, and collaborative coding tasks,\nall without quality degradation. Particularly, when each fully-connected agent\nreceives 1K input tokens with 512 prefix tokens and 512 output tokens under a\nfive-agent setting, KVCOMM achieves up to 7.8x speedup compared to the standard\nprefill pipeline, reducing TTFT from ~430 ms to ~55 ms."
                },
                "authors": [
                    {
                        "name": "Hancheng Ye"
                    },
                    {
                        "name": "Zhengqi Gao"
                    },
                    {
                        "name": "Mingyuan Ma"
                    },
                    {
                        "name": "Qinsi Wang"
                    },
                    {
                        "name": "Yuzhe Fu"
                    },
                    {
                        "name": "Ming-Yu Chung"
                    },
                    {
                        "name": "Yueqian Lin"
                    },
                    {
                        "name": "Zhijian Liu"
                    },
                    {
                        "name": "Jianyi Zhang"
                    },
                    {
                        "name": "Danyang Zhuo"
                    },
                    {
                        "name": "Yiran Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yiran Chen"
                },
                "author": "Yiran Chen",
                "arxiv_comment": "Accepted for publication in NeurIPS2025. Code is available at\n  \\url{https://github.com/FastMAS/KVCOMM}",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12872v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12872v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.22765v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.22765v2",
                "updated": "2025-11-01T07:01:00Z",
                "updated_parsed": [
                    2025,
                    11,
                    1,
                    7,
                    1,
                    0,
                    5,
                    305,
                    0
                ],
                "published": "2025-10-26T17:28:05Z",
                "published_parsed": [
                    2025,
                    10,
                    26,
                    17,
                    28,
                    5,
                    6,
                    299,
                    0
                ],
                "title": "Jarvis: Towards Personalized AI Assistant via Personal KV-Cache\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jarvis: Towards Personalized AI Assistant via Personal KV-Cache\n  Retrieval"
                },
                "summary": "The rapid development of Vision-language models (VLMs) enables open-ended\nperception and reasoning. Recent works have started to investigate how to adapt\ngeneral-purpose VLMs into personalized assistants. Even commercial models such\nas ChatGPT now support model personalization by incorporating user-specific\ninformation. However, existing methods either learn a set of concept tokens or\ntrain a VLM to utilize user-specific information. However, both pipelines\nstruggle to generate accurate answers as personalized assistants. We introduce\nJarvis, an innovative framework for a personalized AI assistant through\npersonal KV-Cache retrieval, which stores user-specific information in the\nKV-Caches of both textual and visual tokens. The textual tokens are created by\nsummarizing user information into metadata, while the visual tokens are\nproduced by extracting distinct image patches from the user's images. When\nanswering a question, Jarvis first retrieves related KV-Caches from personal\nstorage and uses them to ensure accuracy in responses. We also introduce a\nfine-grained benchmark built with the same distinct image patch mining\npipeline, emphasizing accurate question answering based on fine-grained\nuser-specific information. Jarvis is capable of providing more accurate\nresponses, particularly when they depend on specific local details. Jarvis\nachieves state-of-the-art results in both visual question answering and\ntext-only tasks across multiple datasets, indicating a practical path toward\npersonalized AI assistants. The code and dataset will be released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of Vision-language models (VLMs) enables open-ended\nperception and reasoning. Recent works have started to investigate how to adapt\ngeneral-purpose VLMs into personalized assistants. Even commercial models such\nas ChatGPT now support model personalization by incorporating user-specific\ninformation. However, existing methods either learn a set of concept tokens or\ntrain a VLM to utilize user-specific information. However, both pipelines\nstruggle to generate accurate answers as personalized assistants. We introduce\nJarvis, an innovative framework for a personalized AI assistant through\npersonal KV-Cache retrieval, which stores user-specific information in the\nKV-Caches of both textual and visual tokens. The textual tokens are created by\nsummarizing user information into metadata, while the visual tokens are\nproduced by extracting distinct image patches from the user's images. When\nanswering a question, Jarvis first retrieves related KV-Caches from personal\nstorage and uses them to ensure accuracy in responses. We also introduce a\nfine-grained benchmark built with the same distinct image patch mining\npipeline, emphasizing accurate question answering based on fine-grained\nuser-specific information. Jarvis is capable of providing more accurate\nresponses, particularly when they depend on specific local details. Jarvis\nachieves state-of-the-art results in both visual question answering and\ntext-only tasks across multiple datasets, indicating a practical path toward\npersonalized AI assistants. The code and dataset will be released."
                },
                "authors": [
                    {
                        "name": "Binxiao Xu"
                    },
                    {
                        "name": "Junyu Feng"
                    },
                    {
                        "name": "Shaolin Lu"
                    },
                    {
                        "name": "Yulin Luo"
                    },
                    {
                        "name": "Shilin Yan"
                    },
                    {
                        "name": "Hao Liang"
                    },
                    {
                        "name": "Ming Lu"
                    },
                    {
                        "name": "Wentao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wentao Zhang"
                },
                "author": "Wentao Zhang",
                "arxiv_comment": "19 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.22765v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.22765v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16242v7",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16242v7",
                "updated": "2025-11-01T04:26:03Z",
                "updated_parsed": [
                    2025,
                    11,
                    1,
                    4,
                    26,
                    3,
                    5,
                    305,
                    0
                ],
                "published": "2025-07-22T05:26:28Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    5,
                    26,
                    28,
                    1,
                    203,
                    0
                ],
                "title": "Robustifying Learning-Augmented Caching Efficiently without Compromising\n  1-Consistency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robustifying Learning-Augmented Caching Efficiently without Compromising\n  1-Consistency"
                },
                "summary": "The online caching problem aims to minimize cache misses when serving a\nsequence of requests under a limited cache size. While naive learning-augmented\ncaching algorithms achieve ideal $1$-consistency, they lack robustness\nguarantees. Existing robustification methods either sacrifice $1$-consistency\nor introduce excessive computational overhead. In this paper, we introduce\nGuard, a lightweight robustification framework that enhances the robustness of\na broad class of learning-augmented caching algorithms to $2H_{k-1} + 2$, while\npreserving their $1$-consistency. Guard achieves the current best-known\ntrade-off between consistency and robustness, with only O(1) additional\nper-request overhead, thereby maintaining the original time complexity of the\nbase algorithm. Extensive experiments across multiple real-world datasets and\nprediction models validate the effectiveness of Guard in practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The online caching problem aims to minimize cache misses when serving a\nsequence of requests under a limited cache size. While naive learning-augmented\ncaching algorithms achieve ideal $1$-consistency, they lack robustness\nguarantees. Existing robustification methods either sacrifice $1$-consistency\nor introduce excessive computational overhead. In this paper, we introduce\nGuard, a lightweight robustification framework that enhances the robustness of\na broad class of learning-augmented caching algorithms to $2H_{k-1} + 2$, while\npreserving their $1$-consistency. Guard achieves the current best-known\ntrade-off between consistency and robustness, with only O(1) additional\nper-request overhead, thereby maintaining the original time complexity of the\nbase algorithm. Extensive experiments across multiple real-world datasets and\nprediction models validate the effectiveness of Guard in practice."
                },
                "authors": [
                    {
                        "name": "Peng Chen"
                    },
                    {
                        "name": "Hailiang Zhao"
                    },
                    {
                        "name": "Jiaji Zhang"
                    },
                    {
                        "name": "Xueyan Tang"
                    },
                    {
                        "name": "Yixuan Wang"
                    },
                    {
                        "name": "Shuiguang Deng"
                    }
                ],
                "author_detail": {
                    "name": "Shuiguang Deng"
                },
                "author": "Shuiguang Deng",
                "arxiv_comment": "Accepted to NeurIPS 2025.\n  https://neurips.cc/virtual/2025/poster/116615",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16242v7",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16242v7",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.00321v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.00321v1",
                "updated": "2025-10-31T23:50:44Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    23,
                    50,
                    44,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T23:50:44Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    23,
                    50,
                    44,
                    4,
                    304,
                    0
                ],
                "title": "Scalable Processing-Near-Memory for 1M-Token LLM Inference: CXL-Enabled\n  KV-Cache Management Beyond GPU Limits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalable Processing-Near-Memory for 1M-Token LLM Inference: CXL-Enabled\n  KV-Cache Management Beyond GPU Limits"
                },
                "summary": "The expansion of context windows in large language models (LLMs) to\nmulti-million tokens introduces severe memory and compute bottlenecks,\nparticularly in managing the growing Key-Value (KV) cache. While Compute\nExpress Link (CXL) enables non-eviction frameworks that offload the full\nKV-cache to scalable external memory, these frameworks still suffer from costly\ndata transfers when recalling non-resident KV tokens to limited GPU memory as\ncontext lengths increase. This work proposes scalable Processing-Near-Memory\n(PNM) for 1M-Token LLM Inference, a CXL-enabled KV-cache management system that\ncoordinates memory and computation beyond GPU limits. Our design offloads token\npage selection to a PNM accelerator within CXL memory, eliminating costly\nrecalls and enabling larger GPU batch sizes. We further introduce a hybrid\nparallelization strategy and a steady-token selection mechanism to enhance\ncompute efficiency and scalability. Implemented atop a state-of-the-art CXL-PNM\nsystem, our solution delivers consistent performance gains for LLMs with up to\n405B parameters and 1M-token contexts. Our PNM-only offloading scheme (PNM-KV)\nand GPU-PNM hybrid with steady-token execution (PnG-KV) achieve up to 21.9x\nthroughput improvement, up to 60x lower energy per token, and up to 7.3x better\ntotal cost efficiency than the baseline, demonstrating that CXL-enabled\nmulti-PNM architectures can serve as a scalable backbone for future\nlong-context LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The expansion of context windows in large language models (LLMs) to\nmulti-million tokens introduces severe memory and compute bottlenecks,\nparticularly in managing the growing Key-Value (KV) cache. While Compute\nExpress Link (CXL) enables non-eviction frameworks that offload the full\nKV-cache to scalable external memory, these frameworks still suffer from costly\ndata transfers when recalling non-resident KV tokens to limited GPU memory as\ncontext lengths increase. This work proposes scalable Processing-Near-Memory\n(PNM) for 1M-Token LLM Inference, a CXL-enabled KV-cache management system that\ncoordinates memory and computation beyond GPU limits. Our design offloads token\npage selection to a PNM accelerator within CXL memory, eliminating costly\nrecalls and enabling larger GPU batch sizes. We further introduce a hybrid\nparallelization strategy and a steady-token selection mechanism to enhance\ncompute efficiency and scalability. Implemented atop a state-of-the-art CXL-PNM\nsystem, our solution delivers consistent performance gains for LLMs with up to\n405B parameters and 1M-token contexts. Our PNM-only offloading scheme (PNM-KV)\nand GPU-PNM hybrid with steady-token execution (PnG-KV) achieve up to 21.9x\nthroughput improvement, up to 60x lower energy per token, and up to 7.3x better\ntotal cost efficiency than the baseline, demonstrating that CXL-enabled\nmulti-PNM architectures can serve as a scalable backbone for future\nlong-context LLM inference."
                },
                "authors": [
                    {
                        "name": "Dowon Kim"
                    },
                    {
                        "name": "MinJae Lee"
                    },
                    {
                        "name": "Janghyeon Kim"
                    },
                    {
                        "name": "HyuckSung Kwon"
                    },
                    {
                        "name": "Hyeonggyu Jeong"
                    },
                    {
                        "name": "Sang-Soo Park"
                    },
                    {
                        "name": "Minyong Yoon"
                    },
                    {
                        "name": "Si-Dong Roh"
                    },
                    {
                        "name": "Yongsuk Kwon"
                    },
                    {
                        "name": "Jinin So"
                    },
                    {
                        "name": "Jungwook Choi"
                    }
                ],
                "author_detail": {
                    "name": "Jungwook Choi"
                },
                "author": "Jungwook Choi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.00321v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.00321v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25979v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25979v2",
                "updated": "2025-10-31T18:19:55Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    18,
                    19,
                    55,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-29T21:26:17Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    21,
                    26,
                    17,
                    2,
                    302,
                    0
                ],
                "title": "AttnCache: Accelerating Self-Attention Inference for LLM Prefill via\n  Attention Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AttnCache: Accelerating Self-Attention Inference for LLM Prefill via\n  Attention Cache"
                },
                "summary": "Large Language Models (LLMs) are widely used in generative applications such\nas chatting, code generation, and reasoning. However, many realworld workloads\nsuch as classification, question answering, recommendation, and text embedding\nrely solely on the prefill stage of inference, where the model encodes input\nsequences without performing autoregressive decoding. In these prefill only\nscenarios, the self-attention computation becomes the primary performance\nbottleneck due to its quadratic complexity with respect to sequence length. In\nthis paper, we observe that semantically different sentences often produce\nsimilar attention maps across layers and heads. Building on this insight, we\npropose AttnCache, a framework that accelerates the prefill stage of LLM\ninference by retrieving and reusing similar attention maps. Based on an\nattention map memorization database, AttnCache employs efficient caching and\nsimilarity search techniques to identify and reuse pre-cached attention maps\nduring inference, thereby reducing the computational overhead of\nself-attention. Experimental results show that AttnCache achieves an average of\n1.2x end-to-end and 2x attention speedup on CPU, and 1.6x end-to-end and 3x\nattention speedup on GPU, with negligible accuracy degradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are widely used in generative applications such\nas chatting, code generation, and reasoning. However, many realworld workloads\nsuch as classification, question answering, recommendation, and text embedding\nrely solely on the prefill stage of inference, where the model encodes input\nsequences without performing autoregressive decoding. In these prefill only\nscenarios, the self-attention computation becomes the primary performance\nbottleneck due to its quadratic complexity with respect to sequence length. In\nthis paper, we observe that semantically different sentences often produce\nsimilar attention maps across layers and heads. Building on this insight, we\npropose AttnCache, a framework that accelerates the prefill stage of LLM\ninference by retrieving and reusing similar attention maps. Based on an\nattention map memorization database, AttnCache employs efficient caching and\nsimilarity search techniques to identify and reuse pre-cached attention maps\nduring inference, thereby reducing the computational overhead of\nself-attention. Experimental results show that AttnCache achieves an average of\n1.2x end-to-end and 2x attention speedup on CPU, and 1.6x end-to-end and 3x\nattention speedup on GPU, with negligible accuracy degradation."
                },
                "authors": [
                    {
                        "name": "Dinghong Song"
                    },
                    {
                        "name": "Yuan Feng"
                    },
                    {
                        "name": "Yiwei Wang"
                    },
                    {
                        "name": "Shangye Chen"
                    },
                    {
                        "name": "Cyril Guyot"
                    },
                    {
                        "name": "Filip Blagojevic"
                    },
                    {
                        "name": "Hyeran Jeon"
                    },
                    {
                        "name": "Pengfei Su"
                    },
                    {
                        "name": "Dong Li"
                    }
                ],
                "author_detail": {
                    "name": "Dong Li"
                },
                "author": "Dong Li",
                "arxiv_comment": "10 pages, 6 figures, submitted to Ninth Annual Conference on Machine\n  Learning and Systems (MLSys'26)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25979v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25979v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27641v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27641v1",
                "updated": "2025-10-31T17:12:34Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    17,
                    12,
                    34,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T17:12:34Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    17,
                    12,
                    34,
                    4,
                    304,
                    0
                ],
                "title": "SpecAttn: Speculating Sparse Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpecAttn: Speculating Sparse Attention"
                },
                "summary": "Large Language Models (LLMs) face significant computational bottlenecks\nduring inference due to the quadratic complexity of self-attention mechanisms,\nparticularly as context lengths increase. We introduce SpecAttn, a novel\ntraining-free approach that seamlessly integrates with existing speculative\ndecoding techniques to enable efficient sparse attention in pre-trained\ntransformers. Our key insight is to exploit the attention weights already\ncomputed by the draft model during speculative decoding to identify important\ntokens for the target model, eliminating redundant computation while\nmaintaining output quality. SpecAttn employs three core techniques: KL\ndivergence-based layer alignment between draft and target models, a\nGPU-optimized sorting-free algorithm for top-p token selection from draft\nattention patterns, and dynamic key-value cache pruning guided by these\npredictions. By leveraging the computational work already performed in standard\nspeculative decoding pipelines, SpecAttn achieves over 75% reduction in\nkey-value cache accesses with a mere 15.29% increase in perplexity on the PG-19\ndataset, significantly outperforming existing sparse attention methods. Our\napproach demonstrates that speculative execution can be enhanced to provide\napproximate verification without significant performance degradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) face significant computational bottlenecks\nduring inference due to the quadratic complexity of self-attention mechanisms,\nparticularly as context lengths increase. We introduce SpecAttn, a novel\ntraining-free approach that seamlessly integrates with existing speculative\ndecoding techniques to enable efficient sparse attention in pre-trained\ntransformers. Our key insight is to exploit the attention weights already\ncomputed by the draft model during speculative decoding to identify important\ntokens for the target model, eliminating redundant computation while\nmaintaining output quality. SpecAttn employs three core techniques: KL\ndivergence-based layer alignment between draft and target models, a\nGPU-optimized sorting-free algorithm for top-p token selection from draft\nattention patterns, and dynamic key-value cache pruning guided by these\npredictions. By leveraging the computational work already performed in standard\nspeculative decoding pipelines, SpecAttn achieves over 75% reduction in\nkey-value cache accesses with a mere 15.29% increase in perplexity on the PG-19\ndataset, significantly outperforming existing sparse attention methods. Our\napproach demonstrates that speculative execution can be enhanced to provide\napproximate verification without significant performance degradation."
                },
                "authors": [
                    {
                        "name": "Harsh Shah"
                    }
                ],
                "author_detail": {
                    "name": "Harsh Shah"
                },
                "author": "Harsh Shah",
                "arxiv_comment": "Accepted to NeurIPS 2025 Workshop on Structured Probabilistic\n  Inference & Generative Modeling",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27641v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27641v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27617v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27617v1",
                "updated": "2025-10-31T16:40:58Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    16,
                    40,
                    58,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T16:40:58Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    16,
                    40,
                    58,
                    4,
                    304,
                    0
                ],
                "title": "VeriMoA: A Mixture-of-Agents Framework for Spec-to-HDL Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VeriMoA: A Mixture-of-Agents Framework for Spec-to-HDL Generation"
                },
                "summary": "Automation of Register Transfer Level (RTL) design can help developers meet\nincreasing computational demands. Large Language Models (LLMs) show promise for\nHardware Description Language (HDL) generation, but face challenges due to\nlimited parametric knowledge and domain-specific constraints. While prompt\nengineering and fine-tuning have limitations in knowledge coverage and training\ncosts, multi-agent architectures offer a training-free paradigm to enhance\nreasoning through collaborative generation. However, current multi-agent\napproaches suffer from two critical deficiencies: susceptibility to noise\npropagation and constrained reasoning space exploration. We propose VeriMoA, a\ntraining-free mixture-of-agents (MoA) framework with two synergistic\ninnovations. First, a quality-guided caching mechanism to maintain all\nintermediate HDL outputs and enables quality-based ranking and selection across\nthe entire generation process, encouraging knowledge accumulation over layers\nof reasoning. Second, a multi-path generation strategy that leverages C++ and\nPython as intermediate representations, decomposing specification-to-HDL\ntranslation into two-stage processes that exploit LLM fluency in high-resource\nlanguages while promoting solution diversity. Comprehensive experiments on\nVerilogEval 2.0 and RTLLM 2.0 benchmarks demonstrate that VeriMoA achieves\n15--30% improvements in Pass@1 across diverse LLM backbones, especially\nenabling smaller models to match larger models and fine-tuned alternatives\nwithout requiring costly training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automation of Register Transfer Level (RTL) design can help developers meet\nincreasing computational demands. Large Language Models (LLMs) show promise for\nHardware Description Language (HDL) generation, but face challenges due to\nlimited parametric knowledge and domain-specific constraints. While prompt\nengineering and fine-tuning have limitations in knowledge coverage and training\ncosts, multi-agent architectures offer a training-free paradigm to enhance\nreasoning through collaborative generation. However, current multi-agent\napproaches suffer from two critical deficiencies: susceptibility to noise\npropagation and constrained reasoning space exploration. We propose VeriMoA, a\ntraining-free mixture-of-agents (MoA) framework with two synergistic\ninnovations. First, a quality-guided caching mechanism to maintain all\nintermediate HDL outputs and enables quality-based ranking and selection across\nthe entire generation process, encouraging knowledge accumulation over layers\nof reasoning. Second, a multi-path generation strategy that leverages C++ and\nPython as intermediate representations, decomposing specification-to-HDL\ntranslation into two-stage processes that exploit LLM fluency in high-resource\nlanguages while promoting solution diversity. Comprehensive experiments on\nVerilogEval 2.0 and RTLLM 2.0 benchmarks demonstrate that VeriMoA achieves\n15--30% improvements in Pass@1 across diverse LLM backbones, especially\nenabling smaller models to match larger models and fine-tuned alternatives\nwithout requiring costly training."
                },
                "authors": [
                    {
                        "name": "Heng Ping"
                    },
                    {
                        "name": "Arijit Bhattacharjee"
                    },
                    {
                        "name": "Peiyu Zhang"
                    },
                    {
                        "name": "Shixuan Li"
                    },
                    {
                        "name": "Wei Yang"
                    },
                    {
                        "name": "Anzhe Cheng"
                    },
                    {
                        "name": "Xiaole Zhang"
                    },
                    {
                        "name": "Jesse Thomason"
                    },
                    {
                        "name": "Ali Jannesari"
                    },
                    {
                        "name": "Nesreen Ahmed"
                    },
                    {
                        "name": "Paul Bogdan"
                    }
                ],
                "author_detail": {
                    "name": "Paul Bogdan"
                },
                "author": "Paul Bogdan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27617v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27617v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04525v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04525v2",
                "updated": "2025-10-31T05:31:58Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    5,
                    31,
                    58,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-06T06:30:22Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    6,
                    30,
                    22,
                    0,
                    279,
                    0
                ],
                "title": "Demystifying MaskGIT Sampler and Beyond: Adaptive Order Selection in\n  Masked Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demystifying MaskGIT Sampler and Beyond: Adaptive Order Selection in\n  Masked Diffusion"
                },
                "summary": "Masked diffusion models have shown promising performance in generating\nhigh-quality samples in a wide range of domains, but accelerating their\nsampling process remains relatively underexplored. To investigate efficient\nsamplers for masked diffusion, this paper theoretically analyzes the MaskGIT\nsampler for image modeling, revealing its implicit temperature sampling\nmechanism. Through this analysis, we introduce the \"moment sampler,\" an\nasymptotically equivalent but more tractable and interpretable alternative to\nMaskGIT, which employs a \"choose-then-sample\" approach by selecting unmasking\npositions before sampling tokens. In addition, we improve the efficiency of\nchoose-then-sample algorithms through two key innovations: a partial caching\ntechnique for transformers that approximates longer sampling trajectories\nwithout proportional computational cost, and a hybrid approach formalizing the\nexploration-exploitation trade-off in adaptive unmasking. Experiments in image\nand text domains demonstrate our theory as well as the efficiency of our\nproposed methods, advancing both theoretical understanding and practical\nimplementation of masked diffusion samplers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Masked diffusion models have shown promising performance in generating\nhigh-quality samples in a wide range of domains, but accelerating their\nsampling process remains relatively underexplored. To investigate efficient\nsamplers for masked diffusion, this paper theoretically analyzes the MaskGIT\nsampler for image modeling, revealing its implicit temperature sampling\nmechanism. Through this analysis, we introduce the \"moment sampler,\" an\nasymptotically equivalent but more tractable and interpretable alternative to\nMaskGIT, which employs a \"choose-then-sample\" approach by selecting unmasking\npositions before sampling tokens. In addition, we improve the efficiency of\nchoose-then-sample algorithms through two key innovations: a partial caching\ntechnique for transformers that approximates longer sampling trajectories\nwithout proportional computational cost, and a hybrid approach formalizing the\nexploration-exploitation trade-off in adaptive unmasking. Experiments in image\nand text domains demonstrate our theory as well as the efficiency of our\nproposed methods, advancing both theoretical understanding and practical\nimplementation of masked diffusion samplers."
                },
                "authors": [
                    {
                        "name": "Satoshi Hayakawa"
                    },
                    {
                        "name": "Yuhta Takida"
                    },
                    {
                        "name": "Masaaki Imaizumi"
                    },
                    {
                        "name": "Hiromi Wakaki"
                    },
                    {
                        "name": "Yuki Mitsufuji"
                    }
                ],
                "author_detail": {
                    "name": "Yuki Mitsufuji"
                },
                "author": "Yuki Mitsufuji",
                "arxiv_comment": "23 pages, fixed cleveref-related issue",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04525v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04525v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27171v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27171v1",
                "updated": "2025-10-31T04:47:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    4,
                    47,
                    14,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T04:47:14Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    4,
                    47,
                    14,
                    4,
                    304,
                    0
                ],
                "title": "H2-Cache: A Novel Hierarchical Dual-Stage Cache for High-Performance\n  Acceleration of Generative Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "H2-Cache: A Novel Hierarchical Dual-Stage Cache for High-Performance\n  Acceleration of Generative Diffusion Models"
                },
                "summary": "Diffusion models have emerged as state-of-the-art in image generation, but\ntheir practical deployment is hindered by the significant computational cost of\ntheir iterative denoising process. While existing caching techniques can\naccelerate inference, they often create a challenging trade-off between speed\nand fidelity, suffering from quality degradation and high computational\noverhead. To address these limitations, we introduce H2-Cache, a novel\nhierarchical caching mechanism designed for modern generative diffusion model\narchitectures. Our method is founded on the key insight that the denoising\nprocess can be functionally separated into a structure-defining stage and a\ndetail-refining stage. H2-cache leverages this by employing a dual-threshold\nsystem, using independent thresholds to selectively cache each stage. To ensure\nthe efficiency of our dual-check approach, we introduce pooled feature\nsummarization (PFS), a lightweight technique for robust and fast similarity\nestimation. Extensive experiments on the Flux architecture demonstrate that\nH2-cache achieves significant acceleration (up to 5.08x) while maintaining\nimage quality nearly identical to the baseline, quantitatively and\nqualitatively outperforming existing caching methods. Our work presents a\nrobust and practical solution that effectively resolves the speed-quality\ndilemma, significantly lowering the barrier for the real-world application of\nhigh-fidelity diffusion models. Source code is available at\nhttps://github.com/Bluear7878/H2-cache-A-Hierarchical-Dual-Stage-Cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have emerged as state-of-the-art in image generation, but\ntheir practical deployment is hindered by the significant computational cost of\ntheir iterative denoising process. While existing caching techniques can\naccelerate inference, they often create a challenging trade-off between speed\nand fidelity, suffering from quality degradation and high computational\noverhead. To address these limitations, we introduce H2-Cache, a novel\nhierarchical caching mechanism designed for modern generative diffusion model\narchitectures. Our method is founded on the key insight that the denoising\nprocess can be functionally separated into a structure-defining stage and a\ndetail-refining stage. H2-cache leverages this by employing a dual-threshold\nsystem, using independent thresholds to selectively cache each stage. To ensure\nthe efficiency of our dual-check approach, we introduce pooled feature\nsummarization (PFS), a lightweight technique for robust and fast similarity\nestimation. Extensive experiments on the Flux architecture demonstrate that\nH2-cache achieves significant acceleration (up to 5.08x) while maintaining\nimage quality nearly identical to the baseline, quantitatively and\nqualitatively outperforming existing caching methods. Our work presents a\nrobust and practical solution that effectively resolves the speed-quality\ndilemma, significantly lowering the barrier for the real-world application of\nhigh-fidelity diffusion models. Source code is available at\nhttps://github.com/Bluear7878/H2-cache-A-Hierarchical-Dual-Stage-Cache."
                },
                "authors": [
                    {
                        "name": "Mingyu Sung"
                    },
                    {
                        "name": "Il-Min Kim"
                    },
                    {
                        "name": "Sangseok Yun"
                    },
                    {
                        "name": "Jae-Mo Kang"
                    }
                ],
                "author_detail": {
                    "name": "Jae-Mo Kang"
                },
                "author": "Jae-Mo Kang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27171v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27171v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18586v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18586v2",
                "updated": "2025-10-31T04:17:05Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    4,
                    17,
                    5,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-21T12:39:32Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    12,
                    39,
                    32,
                    1,
                    294,
                    0
                ],
                "title": "Tokencake: A KV-Cache-centric Serving Framework for LLM-based\n  Multi-Agent Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tokencake: A KV-Cache-centric Serving Framework for LLM-based\n  Multi-Agent Applications"
                },
                "summary": "Large Language Models (LLMs) are increasingly deployed in complex multi-agent\napplications that use external function calls. This workload creates severe\nperformance challenges for the KV Cache: space contention leads to the eviction\nof critical agents' caches and time underutilization leaves the cache of agents\nstalled on long-running tool calls idling in GPU memory. We present Tokencake,\na KV-Cache-centric serving framework that co-optimizes scheduling and memory\nmanagement with an agent-aware design. Tokencake's Space Scheduler uses dynamic\nmemory partitioning to shield critical agents from contention, while its Time\nScheduler employs a proactive offload and predictive upload mechanism to\nrepurpose GPU memory during function call stalls. Our evaluation on\nrepresentative multi-agent benchmarks shows that Tokencake can reduce\nend-to-end latency by over 47.06%, improve effective GPU memory utilization by\nup to 16.9% compared to vLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly deployed in complex multi-agent\napplications that use external function calls. This workload creates severe\nperformance challenges for the KV Cache: space contention leads to the eviction\nof critical agents' caches and time underutilization leaves the cache of agents\nstalled on long-running tool calls idling in GPU memory. We present Tokencake,\na KV-Cache-centric serving framework that co-optimizes scheduling and memory\nmanagement with an agent-aware design. Tokencake's Space Scheduler uses dynamic\nmemory partitioning to shield critical agents from contention, while its Time\nScheduler employs a proactive offload and predictive upload mechanism to\nrepurpose GPU memory during function call stalls. Our evaluation on\nrepresentative multi-agent benchmarks shows that Tokencake can reduce\nend-to-end latency by over 47.06%, improve effective GPU memory utilization by\nup to 16.9% compared to vLLM."
                },
                "authors": [
                    {
                        "name": "Zhuohang Bian"
                    },
                    {
                        "name": "Feiyang Wu"
                    },
                    {
                        "name": "Teng Ma"
                    },
                    {
                        "name": "Youwei Zhuo"
                    }
                ],
                "author_detail": {
                    "name": "Youwei Zhuo"
                },
                "author": "Youwei Zhuo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18586v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18586v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25977v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25977v2",
                "updated": "2025-10-31T01:52:13Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    1,
                    52,
                    13,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-29T21:22:08Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    21,
                    22,
                    8,
                    2,
                    302,
                    0
                ],
                "title": "NeuronMM: High-Performance Matrix Multiplication for LLM Inference on\n  AWS Trainium",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NeuronMM: High-Performance Matrix Multiplication for LLM Inference on\n  AWS Trainium"
                },
                "summary": "AI accelerators, customized to AI workloads, provide cost-effective and\nhigh-performance solutions for training and inference. Trainium, an AI\naccelerator recently developed by Amazon Web Services (AWS), provides an\nattractive option for LLM training and inference through its heterogeneous\narchitecture. However, leveraging Trainium architecture for high performance\ncan be challenging because of its systolic array architecture and special\nrequirement on data layout. In this paper, we design high-performance matrix\nmultiplication (matmul), a critical compute kernel, for LLM inference on\nTrainium. We introduce a series of techniques customized to Trainium based on\nkernel fusion and novel caching strategies to reduce data movement across the\nsoftware-managed memory hierarchy, maximize SRAM bandwidth, and avoid expensive\nmatrix transpose. Evaluating with nine datasets and four recent LLMs, we show\nthat our system largely outperforms the state-of-the-art matmul implemented by\nAWS on Trainium: at the level of matmul kernel, it achieves an average 1.35x\nspeedup (up to 2.22x), which translates to an average 1.66x speedup (up to\n2.49x) for end-to-end LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI accelerators, customized to AI workloads, provide cost-effective and\nhigh-performance solutions for training and inference. Trainium, an AI\naccelerator recently developed by Amazon Web Services (AWS), provides an\nattractive option for LLM training and inference through its heterogeneous\narchitecture. However, leveraging Trainium architecture for high performance\ncan be challenging because of its systolic array architecture and special\nrequirement on data layout. In this paper, we design high-performance matrix\nmultiplication (matmul), a critical compute kernel, for LLM inference on\nTrainium. We introduce a series of techniques customized to Trainium based on\nkernel fusion and novel caching strategies to reduce data movement across the\nsoftware-managed memory hierarchy, maximize SRAM bandwidth, and avoid expensive\nmatrix transpose. Evaluating with nine datasets and four recent LLMs, we show\nthat our system largely outperforms the state-of-the-art matmul implemented by\nAWS on Trainium: at the level of matmul kernel, it achieves an average 1.35x\nspeedup (up to 2.22x), which translates to an average 1.66x speedup (up to\n2.49x) for end-to-end LLM inference."
                },
                "authors": [
                    {
                        "name": "Dinghong Song"
                    },
                    {
                        "name": "Jierui Xu"
                    },
                    {
                        "name": "Weichu Yang"
                    },
                    {
                        "name": "Pengfei Su"
                    },
                    {
                        "name": "Dong Li"
                    }
                ],
                "author_detail": {
                    "name": "Dong Li"
                },
                "author": "Dong Li",
                "arxiv_comment": "12 pages, 8 figures, submitted to the Proceedings of the Twenty-First\n  European Conference on Computer Systems (EuroSys'26)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25977v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25977v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27070v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27070v1",
                "updated": "2025-10-31T00:39:27Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    0,
                    39,
                    27,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T00:39:27Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    0,
                    39,
                    27,
                    4,
                    304,
                    0
                ],
                "title": "Descriptor-Based Object-Aware Memory Systems: A Comprehensive Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Descriptor-Based Object-Aware Memory Systems: A Comprehensive Review"
                },
                "summary": "The security and efficiency of modern computing systems are fundamentally\nundermined by the absence of a native architectural mechanism to propagate\nhigh-level program semantics, such as object identity, bounds, and lifetime,\nacross the hardware/software interface. This paper presents a comprehensive\nsurvey of the architectural paradigm designed to bridge this semantic gap:\ndescriptor-based, object-aware memory systems. By elevating the descriptor to a\nfirst-class architectural abstraction, this paradigm enables hardware to\ndynamically acquire and enforce the rich semantics of software-defined objects.\nThis survey systematically charts the evolution and current landscape of this\napproach. We establish the foundational concepts of memory objects and\ndescriptors and introduce a novel taxonomy of descriptor addressing modes,\nproviding a structured framework for analyzing and comparing diverse\nimplementations. Our unified analysis reveals how this paradigm holistically\naddresses the intertwined challenges of memory protection, management, and\nprocessing. As a culminating case study, we re-examine the CentroID model,\ndemonstrating how its hybrid tagged-pointer encoding and descriptor processing\nmechanisms embody the path toward practical and efficient object-aware designs.\nFinally, we outline how the explicit cross-layer communication of object\nsemantics provides a foundational research direction for next-generation cache\nhierarchies, unified virtual memory, and even 128-bit architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The security and efficiency of modern computing systems are fundamentally\nundermined by the absence of a native architectural mechanism to propagate\nhigh-level program semantics, such as object identity, bounds, and lifetime,\nacross the hardware/software interface. This paper presents a comprehensive\nsurvey of the architectural paradigm designed to bridge this semantic gap:\ndescriptor-based, object-aware memory systems. By elevating the descriptor to a\nfirst-class architectural abstraction, this paradigm enables hardware to\ndynamically acquire and enforce the rich semantics of software-defined objects.\nThis survey systematically charts the evolution and current landscape of this\napproach. We establish the foundational concepts of memory objects and\ndescriptors and introduce a novel taxonomy of descriptor addressing modes,\nproviding a structured framework for analyzing and comparing diverse\nimplementations. Our unified analysis reveals how this paradigm holistically\naddresses the intertwined challenges of memory protection, management, and\nprocessing. As a culminating case study, we re-examine the CentroID model,\ndemonstrating how its hybrid tagged-pointer encoding and descriptor processing\nmechanisms embody the path toward practical and efficient object-aware designs.\nFinally, we outline how the explicit cross-layer communication of object\nsemantics provides a foundational research direction for next-generation cache\nhierarchies, unified virtual memory, and even 128-bit architectures."
                },
                "authors": [
                    {
                        "name": "Dong Tong"
                    }
                ],
                "author_detail": {
                    "name": "Dong Tong"
                },
                "author": "Dong Tong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27070v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27070v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.00413v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.00413v2",
                "updated": "2025-10-30T21:11:33Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    21,
                    11,
                    33,
                    3,
                    303,
                    0
                ],
                "published": "2025-05-31T06:10:10Z",
                "published_parsed": [
                    2025,
                    5,
                    31,
                    6,
                    10,
                    10,
                    5,
                    151,
                    0
                ],
                "title": "Accelerating Diffusion LLMs via Adaptive Parallel Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion LLMs via Adaptive Parallel Decoding"
                },
                "summary": "The generation speed of LLMs are bottlenecked by autoregressive decoding,\nwhere tokens are predicted sequentially one by one. Alternatively, diffusion\nlarge language models (dLLMs) theoretically allow for parallel token\ngeneration, but in practice struggle to achieve the speed of autoregressive\nmodels without significantly sacrificing quality. We therefore introduce\nadaptive parallel decoding (APD), a novel method that dynamically adjusts the\nnumber of tokens sampled in parallel. We achieve this by defining a\nmultiplicative mixture between the dLLM marginal probabilities and the joint\nprobability of sequences under a small auxiliary autoregressive model. This\ninverts the standard setup of speculative decoding, where the goal is to sample\nfrom a large autoregressive verifier by drafting from a smaller model. We\nfurther optimize APD by enabling KV caching and limiting the size of the masked\ninput. Altogether, our method puts forward three tunable parameters to flexibly\ntradeoff throughput and quality. We show that APD provides markedly higher\nthroughput with minimal quality degradations on downstream benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The generation speed of LLMs are bottlenecked by autoregressive decoding,\nwhere tokens are predicted sequentially one by one. Alternatively, diffusion\nlarge language models (dLLMs) theoretically allow for parallel token\ngeneration, but in practice struggle to achieve the speed of autoregressive\nmodels without significantly sacrificing quality. We therefore introduce\nadaptive parallel decoding (APD), a novel method that dynamically adjusts the\nnumber of tokens sampled in parallel. We achieve this by defining a\nmultiplicative mixture between the dLLM marginal probabilities and the joint\nprobability of sequences under a small auxiliary autoregressive model. This\ninverts the standard setup of speculative decoding, where the goal is to sample\nfrom a large autoregressive verifier by drafting from a smaller model. We\nfurther optimize APD by enabling KV caching and limiting the size of the masked\ninput. Altogether, our method puts forward three tunable parameters to flexibly\ntradeoff throughput and quality. We show that APD provides markedly higher\nthroughput with minimal quality degradations on downstream benchmarks."
                },
                "authors": [
                    {
                        "name": "Daniel Israel"
                    },
                    {
                        "name": "Guy Van den Broeck"
                    },
                    {
                        "name": "Aditya Grover"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Grover"
                },
                "author": "Aditya Grover",
                "arxiv_comment": "10 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.00413v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.00413v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26944v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26944v1",
                "updated": "2025-10-30T18:58:02Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    18,
                    58,
                    2,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T18:58:02Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    18,
                    58,
                    2,
                    3,
                    303,
                    0
                ],
                "title": "Choreographer: A Full-System Framework for Fine-Grained Tasks in Cache\n  Hierarchies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Choreographer: A Full-System Framework for Fine-Grained Tasks in Cache\n  Hierarchies"
                },
                "summary": "In this paper, we introduce Choreographer, a simulation framework that\nenables a holistic system-level evaluation of fine-grained accelerators\ndesigned for latency-sensitive tasks. Unlike existing frameworks, Choreographer\ncaptures all hardware and software overheads in core-accelerator and\ncache-accelerator interactions, integrating a detailed gem5-based hardware\nstack featuring an AMBA coherent hub interface (CHI) mesh network and a\ncomplete Linux-based software stack. To facilitate rapid prototyping, it offers\na C++ application programming interface and modular configuration options. Our\ndetailed cache model provides accurate insights into performance variations\ncaused by cache configurations, which are not captured by other frameworks. The\nframework is demonstrated through two case studies: a data-aware prefetcher for\ngraph analytics workloads, and a quicksort accelerator. Our evaluation shows\nthat the prefetcher achieves speedups between 1.08x and 1.88x by reducing\nmemory access latency, while the quicksort accelerator delivers more than 2x\nspeedup with minimal address translation overhead. These findings underscore\nthe ability of Choreographer to model complex hardware-software interactions\nand optimize performance in small task offloading scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce Choreographer, a simulation framework that\nenables a holistic system-level evaluation of fine-grained accelerators\ndesigned for latency-sensitive tasks. Unlike existing frameworks, Choreographer\ncaptures all hardware and software overheads in core-accelerator and\ncache-accelerator interactions, integrating a detailed gem5-based hardware\nstack featuring an AMBA coherent hub interface (CHI) mesh network and a\ncomplete Linux-based software stack. To facilitate rapid prototyping, it offers\na C++ application programming interface and modular configuration options. Our\ndetailed cache model provides accurate insights into performance variations\ncaused by cache configurations, which are not captured by other frameworks. The\nframework is demonstrated through two case studies: a data-aware prefetcher for\ngraph analytics workloads, and a quicksort accelerator. Our evaluation shows\nthat the prefetcher achieves speedups between 1.08x and 1.88x by reducing\nmemory access latency, while the quicksort accelerator delivers more than 2x\nspeedup with minimal address translation overhead. These findings underscore\nthe ability of Choreographer to model complex hardware-software interactions\nand optimize performance in small task offloading scenarios."
                },
                "authors": [
                    {
                        "name": "Hoa Nguyen"
                    },
                    {
                        "name": "Pongstorn Maidee"
                    },
                    {
                        "name": "Jason Lowe-Power"
                    },
                    {
                        "name": "Alireza Kaviani"
                    }
                ],
                "author_detail": {
                    "name": "Alireza Kaviani"
                },
                "author": "Alireza Kaviani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26944v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26944v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26730v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26730v1",
                "updated": "2025-10-30T17:29:27Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    17,
                    29,
                    27,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T17:29:27Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    17,
                    29,
                    27,
                    3,
                    303,
                    0
                ],
                "title": "ExpertFlow: Adaptive Expert Scheduling and Memory Coordination for\n  Efficient MoE Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ExpertFlow: Adaptive Expert Scheduling and Memory Coordination for\n  Efficient MoE Inference"
                },
                "summary": "The expansion of large language models is increasingly limited by the\nconstrained memory capacity of modern GPUs. To mitigate this,\nMixture-of-Experts (MoE) architectures activate only a small portion of\nparameters during inference, significantly lowering both memory demand and\ncomputational overhead. However, conventional MoE inference approaches, which\nselect active experts independently at each layer, often introduce considerable\nlatency because of frequent parameter transfers between host and GPU memory. In\naddition, current cross-layer prediction strategies, which are typically based\non fixed steps, lack adaptability across different hardware platforms and\nworkloads, thereby reducing their robustness and effectiveness.\n  To address these challenges, we present ExpertFlow, a runtime system for MoE\ninference that combines adaptive expert prefetching and cache-aware routing.\nExpertFlow continuously adjusts its prediction horizon for expert activation by\nleveraging runtime statistics such as transfer bandwidth, parameter\ndimensionality, and model feedback signals. Furthermore, it incorporates a\nhybrid cross-layer prediction scheme that fuses pregating information with\nintermediate computational states to anticipate future expert needs. By\nadaptively refining prefetching decisions and aligning them with actual usage\nbehavior, ExpertFlow effectively decreases cache misses and removes latency\ncaused by expert swap-ins. Our evaluation demonstrates that ExpertFlow reduces\nmodel stall time to less than 0.1% of the baseline, highlighting its capability\nto optimize MoE inference under stringent memory constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The expansion of large language models is increasingly limited by the\nconstrained memory capacity of modern GPUs. To mitigate this,\nMixture-of-Experts (MoE) architectures activate only a small portion of\nparameters during inference, significantly lowering both memory demand and\ncomputational overhead. However, conventional MoE inference approaches, which\nselect active experts independently at each layer, often introduce considerable\nlatency because of frequent parameter transfers between host and GPU memory. In\naddition, current cross-layer prediction strategies, which are typically based\non fixed steps, lack adaptability across different hardware platforms and\nworkloads, thereby reducing their robustness and effectiveness.\n  To address these challenges, we present ExpertFlow, a runtime system for MoE\ninference that combines adaptive expert prefetching and cache-aware routing.\nExpertFlow continuously adjusts its prediction horizon for expert activation by\nleveraging runtime statistics such as transfer bandwidth, parameter\ndimensionality, and model feedback signals. Furthermore, it incorporates a\nhybrid cross-layer prediction scheme that fuses pregating information with\nintermediate computational states to anticipate future expert needs. By\nadaptively refining prefetching decisions and aligning them with actual usage\nbehavior, ExpertFlow effectively decreases cache misses and removes latency\ncaused by expert swap-ins. Our evaluation demonstrates that ExpertFlow reduces\nmodel stall time to less than 0.1% of the baseline, highlighting its capability\nto optimize MoE inference under stringent memory constraints."
                },
                "authors": [
                    {
                        "name": "Zixu Shen"
                    },
                    {
                        "name": "Kexin Chu"
                    },
                    {
                        "name": "Yifan Zhang"
                    },
                    {
                        "name": "Dawei Xiang"
                    },
                    {
                        "name": "Runxin Wu"
                    },
                    {
                        "name": "Wei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Zhang"
                },
                "author": "Wei Zhang",
                "arxiv_comment": "12 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26730v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26730v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.20499v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.20499v2",
                "updated": "2025-10-30T13:43:31Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    13,
                    43,
                    31,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-23T12:39:59Z",
                "published_parsed": [
                    2025,
                    10,
                    23,
                    12,
                    39,
                    59,
                    3,
                    296,
                    0
                ],
                "title": "GPU-Accelerated Primal Heuristics for Mixed Integer Programming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPU-Accelerated Primal Heuristics for Mixed Integer Programming"
                },
                "summary": "We introduce a fusion of GPU accelerated primal heuristics for Mixed Integer\nProgramming. Leveraging GPU acceleration enables exploration of larger search\nregions and faster iterations. A GPU-accelerated PDLP serves as an approximate\nLP solver, while a new probing cache facilitates rapid roundings and early\ninfeasibility detection. Several state-of-the-art heuristics, including\nFeasibility Pump, Feasibility Jump, and Fix-and-Propagate, are further\naccelerated and enhanced. The combined approach of these GPU-driven algorithms\nyields significant improvements over existing methods, both in the number of\nfeasible solutions and the quality of objectives by achieving 221 feasible\nsolutions and 22% objective gap in the MIPLIB2017 benchmark on a presolved\ndataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a fusion of GPU accelerated primal heuristics for Mixed Integer\nProgramming. Leveraging GPU acceleration enables exploration of larger search\nregions and faster iterations. A GPU-accelerated PDLP serves as an approximate\nLP solver, while a new probing cache facilitates rapid roundings and early\ninfeasibility detection. Several state-of-the-art heuristics, including\nFeasibility Pump, Feasibility Jump, and Fix-and-Propagate, are further\naccelerated and enhanced. The combined approach of these GPU-driven algorithms\nyields significant improvements over existing methods, both in the number of\nfeasible solutions and the quality of objectives by achieving 221 feasible\nsolutions and 22% objective gap in the MIPLIB2017 benchmark on a presolved\ndataset."
                },
                "authors": [
                    {
                        "name": "Akif rdk"
                    },
                    {
                        "name": "Piotr Sielski"
                    },
                    {
                        "name": "Alice Boucher"
                    },
                    {
                        "name": "Kumar Aatish"
                    }
                ],
                "author_detail": {
                    "name": "Kumar Aatish"
                },
                "author": "Kumar Aatish",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.20499v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.20499v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26486v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26486v1",
                "updated": "2025-10-30T13:39:08Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    13,
                    39,
                    8,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T13:39:08Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    13,
                    39,
                    8,
                    3,
                    303,
                    0
                ],
                "title": "LINK-KG: LLM-Driven Coreference-Resolved Knowledge Graphs for Human\n  Smuggling Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LINK-KG: LLM-Driven Coreference-Resolved Knowledge Graphs for Human\n  Smuggling Networks"
                },
                "summary": "Human smuggling networks are complex and constantly evolving, making them\ndifficult to analyze comprehensively. Legal case documents offer rich factual\nand procedural insights into these networks but are often long, unstructured,\nand filled with ambiguous or shifting references, posing significant challenges\nfor automated knowledge graph (KG) construction. Existing methods either\noverlook coreference resolution or fail to scale beyond short text spans,\nleading to fragmented graphs and inconsistent entity linking. We propose\nLINK-KG, a modular framework that integrates a three-stage, LLM-guided\ncoreference resolution pipeline with downstream KG extraction. At the core of\nour approach is a type-specific Prompt Cache, which consistently tracks and\nresolves references across document chunks, enabling clean and disambiguated\nnarratives for structured knowledge graph construction from both short and long\nlegal texts. LINK-KG reduces average node duplication by 45.21% and noisy nodes\nby 32.22% compared to baseline methods, resulting in cleaner and more coherent\ngraph structures. These improvements establish LINK-KG as a strong foundation\nfor analyzing complex criminal networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human smuggling networks are complex and constantly evolving, making them\ndifficult to analyze comprehensively. Legal case documents offer rich factual\nand procedural insights into these networks but are often long, unstructured,\nand filled with ambiguous or shifting references, posing significant challenges\nfor automated knowledge graph (KG) construction. Existing methods either\noverlook coreference resolution or fail to scale beyond short text spans,\nleading to fragmented graphs and inconsistent entity linking. We propose\nLINK-KG, a modular framework that integrates a three-stage, LLM-guided\ncoreference resolution pipeline with downstream KG extraction. At the core of\nour approach is a type-specific Prompt Cache, which consistently tracks and\nresolves references across document chunks, enabling clean and disambiguated\nnarratives for structured knowledge graph construction from both short and long\nlegal texts. LINK-KG reduces average node duplication by 45.21% and noisy nodes\nby 32.22% compared to baseline methods, resulting in cleaner and more coherent\ngraph structures. These improvements establish LINK-KG as a strong foundation\nfor analyzing complex criminal networks."
                },
                "authors": [
                    {
                        "name": "Dipak Meher"
                    },
                    {
                        "name": "Carlotta Domeniconi"
                    },
                    {
                        "name": "Guadalupe Correa-Cabrera"
                    }
                ],
                "author_detail": {
                    "name": "Guadalupe Correa-Cabrera"
                },
                "author": "Guadalupe Correa-Cabrera",
                "arxiv_comment": "Accepted in ICKG 2025 Conference, 8 Pages, 2 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26486v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26486v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25160v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25160v2",
                "updated": "2025-10-30T08:52:17Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    8,
                    52,
                    17,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-29T04:29:17Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    4,
                    29,
                    17,
                    2,
                    302,
                    0
                ],
                "title": "Model-Document Protocol for AI Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model-Document Protocol for AI Search"
                },
                "summary": "AI search depends on linking large language models (LLMs) with vast external\nknowledge sources. Yet web pages, PDF files, and other raw documents are not\ninherently LLM-ready: they are long, noisy, and unstructured. Conventional\nretrieval methods treat these documents as verbatim text and return raw\npassages, leaving the burden of fragment assembly and contextual reasoning to\nthe LLM. This gap underscores the need for a new retrieval paradigm that\nredefines how models interact with documents.\n  We introduce the Model-Document Protocol (MDP), a general framework that\nformalizes how raw text is bridged to LLMs through consumable knowledge\nrepresentations. Rather than treating retrieval as passage fetching, MDP\ndefines multiple pathways that transform unstructured documents into\ntask-specific, LLM-ready inputs. These include agentic reasoning, which curates\nraw evidence into coherent context; memory grounding, which accumulates\nreusable notes to enrich reasoning; and structured leveraging, which encodes\ndocuments into formal representations such as graphs or key-value caches. All\nthree pathways share the same goal: ensuring that what reaches the LLM is not\nraw fragments but compact, structured knowledge directly consumable for\nreasoning.\n  As an instantiation, we present MDP-Agent, which realizes the protocol\nthrough an agentic process: constructing document-level gist memories for\nglobal coverage, performing diffusion-based exploration with vertical\nexploitation to uncover layered dependencies, and applying map-reduce style\nsynthesis to integrate large-scale evidence into compact yet sufficient\ncontext. Experiments on information-seeking benchmarks demonstrate that\nMDP-Agent outperforms baselines, validating both the soundness of the MDP\nframework and the effectiveness of its agentic instantiation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI search depends on linking large language models (LLMs) with vast external\nknowledge sources. Yet web pages, PDF files, and other raw documents are not\ninherently LLM-ready: they are long, noisy, and unstructured. Conventional\nretrieval methods treat these documents as verbatim text and return raw\npassages, leaving the burden of fragment assembly and contextual reasoning to\nthe LLM. This gap underscores the need for a new retrieval paradigm that\nredefines how models interact with documents.\n  We introduce the Model-Document Protocol (MDP), a general framework that\nformalizes how raw text is bridged to LLMs through consumable knowledge\nrepresentations. Rather than treating retrieval as passage fetching, MDP\ndefines multiple pathways that transform unstructured documents into\ntask-specific, LLM-ready inputs. These include agentic reasoning, which curates\nraw evidence into coherent context; memory grounding, which accumulates\nreusable notes to enrich reasoning; and structured leveraging, which encodes\ndocuments into formal representations such as graphs or key-value caches. All\nthree pathways share the same goal: ensuring that what reaches the LLM is not\nraw fragments but compact, structured knowledge directly consumable for\nreasoning.\n  As an instantiation, we present MDP-Agent, which realizes the protocol\nthrough an agentic process: constructing document-level gist memories for\nglobal coverage, performing diffusion-based exploration with vertical\nexploitation to uncover layered dependencies, and applying map-reduce style\nsynthesis to integrate large-scale evidence into compact yet sufficient\ncontext. Experiments on information-seeking benchmarks demonstrate that\nMDP-Agent outperforms baselines, validating both the soundness of the MDP\nframework and the effectiveness of its agentic instantiation."
                },
                "authors": [
                    {
                        "name": "Hongjin Qian"
                    },
                    {
                        "name": "Zheng Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Liu"
                },
                "author": "Zheng Liu",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25160v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25160v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18480v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18480v2",
                "updated": "2025-10-30T08:46:37Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    8,
                    46,
                    37,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-21T10:00:32Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    10,
                    0,
                    32,
                    1,
                    294,
                    0
                ],
                "title": "How Efficient Are Diffusion Language Models? A Critical Examination of\n  Efficiency Evaluation Practices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Efficient Are Diffusion Language Models? A Critical Examination of\n  Efficiency Evaluation Practices"
                },
                "summary": "Diffusion language models (DLMs) have emerged as a promising alternative to\nthe long-dominant autoregressive (AR) paradigm, offering a parallelable\ndecoding process that could yield greater efficiency. Yet, in practice, current\nopen-source DLMs often underperform their AR counterparts in speed, limiting\ntheir real-world utility. This work presents a systematic study of DLM\nefficiency, identifying key issues in prior evaluation methods. Through\nempirical benchmarking and a roofline-based theoretical analysis, we\ndemonstrate that AR models generally achieve higher throughput, while DLMs\nconsistently lag. We also investigate acceleration strategies, finding that\ntechniques like dual cache and parallel decoding mainly offer gains at small\nbatch sizes, with their benefits diminishing upon scaling. Our findings\nunderscore the necessity of robust evaluation methods and improved acceleration\nstrategies to advance research on DLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion language models (DLMs) have emerged as a promising alternative to\nthe long-dominant autoregressive (AR) paradigm, offering a parallelable\ndecoding process that could yield greater efficiency. Yet, in practice, current\nopen-source DLMs often underperform their AR counterparts in speed, limiting\ntheir real-world utility. This work presents a systematic study of DLM\nefficiency, identifying key issues in prior evaluation methods. Through\nempirical benchmarking and a roofline-based theoretical analysis, we\ndemonstrate that AR models generally achieve higher throughput, while DLMs\nconsistently lag. We also investigate acceleration strategies, finding that\ntechniques like dual cache and parallel decoding mainly offer gains at small\nbatch sizes, with their benefits diminishing upon scaling. Our findings\nunderscore the necessity of robust evaluation methods and improved acceleration\nstrategies to advance research on DLMs."
                },
                "authors": [
                    {
                        "name": "Han Peng"
                    },
                    {
                        "name": "Peiyu Liu"
                    },
                    {
                        "name": "Zican Dong"
                    },
                    {
                        "name": "Daixuan Cheng"
                    },
                    {
                        "name": "Junyi Li"
                    },
                    {
                        "name": "Yiru Tang"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Wayne Xin Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Wayne Xin Zhao"
                },
                "author": "Wayne Xin Zhao",
                "arxiv_comment": "Withdrawn by the authors to better delineate the related work from\n  the paper's original contributions",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18480v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18480v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26234v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26234v1",
                "updated": "2025-10-30T08:12:53Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    8,
                    12,
                    53,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T08:12:53Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    8,
                    12,
                    53,
                    3,
                    303,
                    0
                ],
                "title": "From req/res to pub/sub: Exploring Media over QUIC Transport for DNS",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From req/res to pub/sub: Exploring Media over QUIC Transport for DNS"
                },
                "summary": "The DNS is a key component of the Internet. Originally designed to facilitate\nthe resolution of host names to IP addresses, its scope has continuously\nexpanded over the years, today covering use cases such as load balancing or\nservice discovery. While DNS was initially conceived as a rather static\ndirectory service in which resource records (RR) only change rarely, we have\nseen a number of use cases over the years where a DNS flavor that isn't purely\nbased upon requesting and caching RRs, but rather on an active distribution of\nupdates for all resolvers that showed interest in the respective records in the\npast, would be preferable. In this paper, we thus explore a publish-subscribe\nvariant of DNS based on the Media-over-QUIC architecture, where we devise a\nstrawman system and protocol proposal to enable pushing RR updates. We provide\na prototype implementation, finding that DNS can benefit from a\npublish-subscribe variant: next to limiting update traffic, it can considerably\nreduce the time it takes for a resolver to receive the latest version of a\nrecord, thereby supporting use cases such as load balancing in content\ndistribution networks. The publish-subscribe architecture also brings new\nchallenges to the DNS, including a higher overhead for endpoints due to\nadditional state management, and increased query latencies on first lookup, due\nto session establishment latencies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The DNS is a key component of the Internet. Originally designed to facilitate\nthe resolution of host names to IP addresses, its scope has continuously\nexpanded over the years, today covering use cases such as load balancing or\nservice discovery. While DNS was initially conceived as a rather static\ndirectory service in which resource records (RR) only change rarely, we have\nseen a number of use cases over the years where a DNS flavor that isn't purely\nbased upon requesting and caching RRs, but rather on an active distribution of\nupdates for all resolvers that showed interest in the respective records in the\npast, would be preferable. In this paper, we thus explore a publish-subscribe\nvariant of DNS based on the Media-over-QUIC architecture, where we devise a\nstrawman system and protocol proposal to enable pushing RR updates. We provide\na prototype implementation, finding that DNS can benefit from a\npublish-subscribe variant: next to limiting update traffic, it can considerably\nreduce the time it takes for a resolver to receive the latest version of a\nrecord, thereby supporting use cases such as load balancing in content\ndistribution networks. The publish-subscribe architecture also brings new\nchallenges to the DNS, including a higher overhead for endpoints due to\nadditional state management, and increased query latencies on first lookup, due\nto session establishment latencies."
                },
                "authors": [
                    {
                        "name": "Mathis Engelbart"
                    },
                    {
                        "name": "Mike Kosek"
                    },
                    {
                        "name": "Lars Eggert"
                    },
                    {
                        "name": "Jrg Ott"
                    }
                ],
                "author_detail": {
                    "name": "Jrg Ott"
                },
                "author": "Jrg Ott",
                "arxiv_doi": "10.1145/3772356.3772416",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3772356.3772416",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.26234v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26234v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "HotNets 2025",
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.00090v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.00090v1",
                "updated": "2025-10-30T04:57:26Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    4,
                    57,
                    26,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T04:57:26Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    4,
                    57,
                    26,
                    3,
                    303,
                    0
                ],
                "title": "LeMiCa: Lexicographic Minimax Path Caching for Efficient Diffusion-Based\n  Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LeMiCa: Lexicographic Minimax Path Caching for Efficient Diffusion-Based\n  Video Generation"
                },
                "summary": "We present LeMiCa, a training-free and efficient acceleration framework for\ndiffusion-based video generation. While existing caching strategies primarily\nfocus on reducing local heuristic errors, they often overlook the accumulation\nof global errors, leading to noticeable content degradation between accelerated\nand original videos. To address this issue, we formulate cache scheduling as a\ndirected graph with error-weighted edges and introduce a Lexicographic Minimax\nPath Optimization strategy that explicitly bounds the worst-case path error.\nThis approach substantially improves the consistency of global content and\nstyle across generated frames. Extensive experiments on multiple text-to-video\nbenchmarks demonstrate that LeMiCa delivers dual improvements in both inference\nspeed and generation quality. Notably, our method achieves a 2.9x speedup on\nthe Latte model and reaches an LPIPS score of 0.05 on Open-Sora, outperforming\nprior caching techniques. Importantly, these gains come with minimal perceptual\nquality degradation, making LeMiCa a robust and generalizable paradigm for\naccelerating diffusion-based video generation. We believe this approach can\nserve as a strong foundation for future research on efficient and reliable\nvideo synthesis. Our code is available at :https://github.com/UnicomAI/LeMiCa",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present LeMiCa, a training-free and efficient acceleration framework for\ndiffusion-based video generation. While existing caching strategies primarily\nfocus on reducing local heuristic errors, they often overlook the accumulation\nof global errors, leading to noticeable content degradation between accelerated\nand original videos. To address this issue, we formulate cache scheduling as a\ndirected graph with error-weighted edges and introduce a Lexicographic Minimax\nPath Optimization strategy that explicitly bounds the worst-case path error.\nThis approach substantially improves the consistency of global content and\nstyle across generated frames. Extensive experiments on multiple text-to-video\nbenchmarks demonstrate that LeMiCa delivers dual improvements in both inference\nspeed and generation quality. Notably, our method achieves a 2.9x speedup on\nthe Latte model and reaches an LPIPS score of 0.05 on Open-Sora, outperforming\nprior caching techniques. Importantly, these gains come with minimal perceptual\nquality degradation, making LeMiCa a robust and generalizable paradigm for\naccelerating diffusion-based video generation. We believe this approach can\nserve as a strong foundation for future research on efficient and reliable\nvideo synthesis. Our code is available at :https://github.com/UnicomAI/LeMiCa"
                },
                "authors": [
                    {
                        "name": "Huanlin Gao"
                    },
                    {
                        "name": "Ping Chen"
                    },
                    {
                        "name": "Fuyuan Shi"
                    },
                    {
                        "name": "Chao Tan"
                    },
                    {
                        "name": "Zhaoxiang Liu"
                    },
                    {
                        "name": "Fang Zhao"
                    },
                    {
                        "name": "Kai Wang"
                    },
                    {
                        "name": "Shiguo Lian"
                    }
                ],
                "author_detail": {
                    "name": "Shiguo Lian"
                },
                "author": "Shiguo Lian",
                "arxiv_comment": "NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.00090v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.00090v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25600v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25600v2",
                "updated": "2025-10-30T03:43:02Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    3,
                    43,
                    2,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-29T15:10:17Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    15,
                    10,
                    17,
                    2,
                    302,
                    0
                ],
                "title": "PureKV: Plug-and-Play KV Cache Optimization with Spatial-Temporal Sparse\n  Attention for Vision-Language Large Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PureKV: Plug-and-Play KV Cache Optimization with Spatial-Temporal Sparse\n  Attention for Vision-Language Large Models"
                },
                "summary": "Vision-Language Large Models (VLLMs) face significant efficiency challenges\nwhen processing high-resolution inputs. The quadratic complexity in attention\nand autoregressive generation, as well as the constantly growing key value (KV)\ncache size, severely hinder the prefilling and decoding stages. Recent efforts\nhave attempted to compress KV cache by identifying and pruning KV cache of less\nimportant tokens, but these methods typically rely on attention scores to\nestimate token importance, making them incompatible with efficient attention\nmechanisms such as FlashAttention and Sparse Attention, which do not explicitly\ncompute attention matrices. Moreover, existing methods overlook how sparse\nattention, while accelerating the prefilling stage, alters the information\nstructure of the KV cache, thereby compromising the effectiveness of downstream\nKV cache compression strategies. To address this issue, we propose PureKV, a\nplug-and-play framework for joint optimization of sparse attention and KV cache\ncompression. We first introduce a KV cache compression strategy that is fully\ncompatible with efficient attention accelerators. Our method utilizes lower\nlayer attention scores to estimate the importance of high layers' KV cache,\nenabling active pruning without compromising accuracy. In addition, we have\ndesigned a Spatial-Temporal Sparse Attention (ST-SpAttn) module specifically\ntailored for video KV cache compression algorithms. This module combines\nspatial and temporal attention sparsity to improve the compression efficiency\nof KV cache optimization algorithms by purifying spatial noise and temporal\nredundancy in KV cache. At the same time, ST-SpAttn also accelerated the\nprefilling stage of VLLMs. Extensive experiments on VLLMs (VideoLLaMA2,\nQwen2.5-VL) have shown that PureKV achieves 5.0 times KV cache compression and\n3.16 times prefill acceleration, with negligible quality degradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Large Models (VLLMs) face significant efficiency challenges\nwhen processing high-resolution inputs. The quadratic complexity in attention\nand autoregressive generation, as well as the constantly growing key value (KV)\ncache size, severely hinder the prefilling and decoding stages. Recent efforts\nhave attempted to compress KV cache by identifying and pruning KV cache of less\nimportant tokens, but these methods typically rely on attention scores to\nestimate token importance, making them incompatible with efficient attention\nmechanisms such as FlashAttention and Sparse Attention, which do not explicitly\ncompute attention matrices. Moreover, existing methods overlook how sparse\nattention, while accelerating the prefilling stage, alters the information\nstructure of the KV cache, thereby compromising the effectiveness of downstream\nKV cache compression strategies. To address this issue, we propose PureKV, a\nplug-and-play framework for joint optimization of sparse attention and KV cache\ncompression. We first introduce a KV cache compression strategy that is fully\ncompatible with efficient attention accelerators. Our method utilizes lower\nlayer attention scores to estimate the importance of high layers' KV cache,\nenabling active pruning without compromising accuracy. In addition, we have\ndesigned a Spatial-Temporal Sparse Attention (ST-SpAttn) module specifically\ntailored for video KV cache compression algorithms. This module combines\nspatial and temporal attention sparsity to improve the compression efficiency\nof KV cache optimization algorithms by purifying spatial noise and temporal\nredundancy in KV cache. At the same time, ST-SpAttn also accelerated the\nprefilling stage of VLLMs. Extensive experiments on VLLMs (VideoLLaMA2,\nQwen2.5-VL) have shown that PureKV achieves 5.0 times KV cache compression and\n3.16 times prefill acceleration, with negligible quality degradation."
                },
                "authors": [
                    {
                        "name": "Zhonghua Jiang"
                    },
                    {
                        "name": "Kunxi Li"
                    },
                    {
                        "name": "Yiyun Zhou"
                    },
                    {
                        "name": "Sihao Liu"
                    },
                    {
                        "name": "Zhaode Wang"
                    },
                    {
                        "name": "Chengfei lv"
                    },
                    {
                        "name": "Shengyu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Shengyu Zhang"
                },
                "author": "Shengyu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25600v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25600v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26104v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26104v1",
                "updated": "2025-10-30T03:30:12Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    3,
                    30,
                    12,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T03:30:12Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    3,
                    30,
                    12,
                    3,
                    303,
                    0
                ],
                "title": "OneTrans: Unified Feature Interaction and Sequence Modeling with One\n  Transformer in Industrial Recommender",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OneTrans: Unified Feature Interaction and Sequence Modeling with One\n  Transformer in Industrial Recommender"
                },
                "summary": "In recommendation systems, scaling up feature-interaction modules (e.g.,\nWukong, RankMixer) or user-behavior sequence modules (e.g., LONGER) has\nachieved notable success. However, these efforts typically proceed on separate\ntracks, which not only hinders bidirectional information exchange but also\nprevents unified optimization and scaling. In this paper, we propose OneTrans,\na unified Transformer backbone that simultaneously performs user-behavior\nsequence modeling and feature interaction. OneTrans employs a unified tokenizer\nto convert both sequential and non-sequential attributes into a single token\nsequence. The stacked OneTrans blocks share parameters across similar\nsequential tokens while assigning token-specific parameters to non-sequential\ntokens. Through causal attention and cross-request KV caching, OneTrans enables\nprecomputation and caching of intermediate representations, significantly\nreducing computational costs during both training and inference. Experimental\nresults on industrial-scale datasets demonstrate that OneTrans scales\nefficiently with increasing parameters, consistently outperforms strong\nbaselines, and yields a 5.68% lift in per-user GMV in online A/B tests.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recommendation systems, scaling up feature-interaction modules (e.g.,\nWukong, RankMixer) or user-behavior sequence modules (e.g., LONGER) has\nachieved notable success. However, these efforts typically proceed on separate\ntracks, which not only hinders bidirectional information exchange but also\nprevents unified optimization and scaling. In this paper, we propose OneTrans,\na unified Transformer backbone that simultaneously performs user-behavior\nsequence modeling and feature interaction. OneTrans employs a unified tokenizer\nto convert both sequential and non-sequential attributes into a single token\nsequence. The stacked OneTrans blocks share parameters across similar\nsequential tokens while assigning token-specific parameters to non-sequential\ntokens. Through causal attention and cross-request KV caching, OneTrans enables\nprecomputation and caching of intermediate representations, significantly\nreducing computational costs during both training and inference. Experimental\nresults on industrial-scale datasets demonstrate that OneTrans scales\nefficiently with increasing parameters, consistently outperforms strong\nbaselines, and yields a 5.68% lift in per-user GMV in online A/B tests."
                },
                "authors": [
                    {
                        "name": "Zhaoqi Zhang"
                    },
                    {
                        "name": "Haolei Pei"
                    },
                    {
                        "name": "Jun Guo"
                    },
                    {
                        "name": "Tianyu Wang"
                    },
                    {
                        "name": "Yufei Feng"
                    },
                    {
                        "name": "Hui Sun"
                    },
                    {
                        "name": "Shaowei Liu"
                    },
                    {
                        "name": "Aixin Sun"
                    }
                ],
                "author_detail": {
                    "name": "Aixin Sun"
                },
                "author": "Aixin Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26104v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26104v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.11507v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.11507v2",
                "updated": "2025-10-29T21:56:19Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    21,
                    56,
                    19,
                    2,
                    302,
                    0
                ],
                "published": "2025-07-15T17:23:22Z",
                "published_parsed": [
                    2025,
                    7,
                    15,
                    17,
                    23,
                    22,
                    1,
                    196,
                    0
                ],
                "title": "Oneiros: KV Cache Optimization through Parameter Remapping for\n  Multi-tenant LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Oneiros: KV Cache Optimization through Parameter Remapping for\n  Multi-tenant LLM Serving"
                },
                "summary": "KV cache accelerates LLM inference by avoiding redundant computation, at the\nexpense of memory. To support larger KV caches, prior work extends GPU memory\nwith CPU memory via CPU-offloading. This involves swapping KV cache between GPU\nand CPU memory. However, because the cache updates dynamically, such swapping\nincurs high CPU memory traffic. We make a key observation that model parameters\nremain constant during runtime, unlike the dynamically updated KV cache.\nBuilding on this, we introduce Oneiros, which avoids KV cache swapping by\nremapping, and thereby repurposing, the memory allocated to model parameters\nfor KV cache. This parameter remapping is especially beneficial in multi-tenant\nenvironments, where the memory used for the parameters of the inactive models\ncan be more aggressively reclaimed. Exploiting the high CPU-GPU bandwidth\noffered by the modern hardware, such as the NVIDIA Grace Hopper Superchip, we\nshow that Oneiros significantly outperforms state-of-the-art solutions,\nachieving a reduction of 44.8%-82.5% in tail time-between-token latency,\n20.7%-99.3% in tail time-to-first-token latency, and 6.6%-86.7% higher\nthroughput compared to vLLM. Source code of Oneiros is available at\nhttps://github.com/UT-SysML/Oneiros/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache accelerates LLM inference by avoiding redundant computation, at the\nexpense of memory. To support larger KV caches, prior work extends GPU memory\nwith CPU memory via CPU-offloading. This involves swapping KV cache between GPU\nand CPU memory. However, because the cache updates dynamically, such swapping\nincurs high CPU memory traffic. We make a key observation that model parameters\nremain constant during runtime, unlike the dynamically updated KV cache.\nBuilding on this, we introduce Oneiros, which avoids KV cache swapping by\nremapping, and thereby repurposing, the memory allocated to model parameters\nfor KV cache. This parameter remapping is especially beneficial in multi-tenant\nenvironments, where the memory used for the parameters of the inactive models\ncan be more aggressively reclaimed. Exploiting the high CPU-GPU bandwidth\noffered by the modern hardware, such as the NVIDIA Grace Hopper Superchip, we\nshow that Oneiros significantly outperforms state-of-the-art solutions,\nachieving a reduction of 44.8%-82.5% in tail time-between-token latency,\n20.7%-99.3% in tail time-to-first-token latency, and 6.6%-86.7% higher\nthroughput compared to vLLM. Source code of Oneiros is available at\nhttps://github.com/UT-SysML/Oneiros/."
                },
                "authors": [
                    {
                        "name": "Ruihao Li"
                    },
                    {
                        "name": "Shagnik Pal"
                    },
                    {
                        "name": "Vineeth Narayan Pullu"
                    },
                    {
                        "name": "Prasoon Sinha"
                    },
                    {
                        "name": "Jeeho Ryoo"
                    },
                    {
                        "name": "Lizy K. John"
                    },
                    {
                        "name": "Neeraja J. Yadwadkar"
                    }
                ],
                "author_detail": {
                    "name": "Neeraja J. Yadwadkar"
                },
                "author": "Neeraja J. Yadwadkar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.11507v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.11507v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26835v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26835v1",
                "updated": "2025-10-29T19:59:45Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    19,
                    59,
                    45,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T19:59:45Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    19,
                    59,
                    45,
                    2,
                    302,
                    0
                ],
                "title": "Category-Aware Semantic Caching for Heterogeneous LLM Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Category-Aware Semantic Caching for Heterogeneous LLM Workloads"
                },
                "summary": "LLM serving systems process heterogeneous query workloads where different\ncategories exhibit different characteristics. Code queries cluster densely in\nembedding space while conversational queries distribute sparsely. Content\nstaleness varies from minutes (stock data) to months (code patterns). Query\nrepetition patterns range from power-law (code) to uniform (conversation),\nproducing long tail cache hit rate distributions: high-repetition categories\nachieve 40-60% hit rates while low-repetition or volatile categories achieve\n5-15% hit rates. Vector databases must exclude the long tail because remote\nsearch costs (30ms) require 15--20% hit rates to break even, leaving 20-30% of\nproduction traffic uncached. Uniform cache policies compound this problem:\nfixed thresholds cause false positives in dense spaces and miss valid\nparaphrases in sparse spaces; fixed TTLs waste memory or serve stale data. This\npaper presents category-aware semantic caching where similarity thresholds,\nTTLs, and quotas vary by query category. We present a hybrid architecture\nseparating in-memory HNSW search from external document storage, reducing miss\ncost from 30ms to 2ms. This reduction makes low-hit-rate categories\neconomically viable (break-even at 3-5% versus 15-20%), enabling cache coverage\nacross the entire workload distribution. Adaptive load-based policies extend\nthis framework to respond to downstream model load, dynamically adjusting\nthresholds and TTLs to reduce traffic to overloaded models by 9-17% in\ntheoretical projections.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM serving systems process heterogeneous query workloads where different\ncategories exhibit different characteristics. Code queries cluster densely in\nembedding space while conversational queries distribute sparsely. Content\nstaleness varies from minutes (stock data) to months (code patterns). Query\nrepetition patterns range from power-law (code) to uniform (conversation),\nproducing long tail cache hit rate distributions: high-repetition categories\nachieve 40-60% hit rates while low-repetition or volatile categories achieve\n5-15% hit rates. Vector databases must exclude the long tail because remote\nsearch costs (30ms) require 15--20% hit rates to break even, leaving 20-30% of\nproduction traffic uncached. Uniform cache policies compound this problem:\nfixed thresholds cause false positives in dense spaces and miss valid\nparaphrases in sparse spaces; fixed TTLs waste memory or serve stale data. This\npaper presents category-aware semantic caching where similarity thresholds,\nTTLs, and quotas vary by query category. We present a hybrid architecture\nseparating in-memory HNSW search from external document storage, reducing miss\ncost from 30ms to 2ms. This reduction makes low-hit-rate categories\neconomically viable (break-even at 3-5% versus 15-20%), enabling cache coverage\nacross the entire workload distribution. Adaptive load-based policies extend\nthis framework to respond to downstream model load, dynamically adjusting\nthresholds and TTLs to reduce traffic to overloaded models by 9-17% in\ntheoretical projections."
                },
                "authors": [
                    {
                        "name": "Chen Wang"
                    },
                    {
                        "name": "Xunzhuo Liu"
                    },
                    {
                        "name": "Yue Zhu"
                    },
                    {
                        "name": "Alaa Youssef"
                    },
                    {
                        "name": "Priya Nagpurkar"
                    },
                    {
                        "name": "Huamin Chen"
                    }
                ],
                "author_detail": {
                    "name": "Huamin Chen"
                },
                "author": "Huamin Chen",
                "arxiv_comment": "13 pages including reference, position paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26835v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26835v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25695v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25695v1",
                "updated": "2025-10-29T17:00:16Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    17,
                    0,
                    16,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T17:00:16Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    17,
                    0,
                    16,
                    2,
                    302,
                    0
                ],
                "title": "Over 3 kV and Ultra-Low leakage Vertical (011) \\b{eta}-Ga2O3 Power\n  Diodes with Engineered Schottky Contact and High-permittivity Dielectric\n  Field Plate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Over 3 kV and Ultra-Low leakage Vertical (011) \\b{eta}-Ga2O3 Power\n  Diodes with Engineered Schottky Contact and High-permittivity Dielectric\n  Field Plate"
                },
                "summary": "We report over 3 kV breakdown voltage and ultra-low leakage (011)\n\\b{eta}-Ga2O3 power devices utilizing Schottky barrier engineering and\nhigh-permittivity (\\k{appa}) dielectric (ZrO2) field plate. The (011)\norientation of \\b{eta}-Ga2O3 enabled low background doping and thick drift\nlayers which are promising to support kV-class vertical \\b{eta}-Ga2O3 power\nswitches. The Schottky barrier engineering was performed with a composite Pt\ncap/PtOx/Pt (1.5 nm) anode contact to take advantage of the enhanced reverse\nblocking capabilities enabled by PtOx while allowing low turn-on voltage by the\ninterfacing thin Pt layer. We also performed a systematic study using a\nco-processed Pt/(011) \\b{eta}-Ga2O3 Schottky barrier diodes (SBDs) on the same\nwafer. The bare SBDs revealed a breakdown voltage of ~1.5 kV, while the\nfield-plate Pt/(011) \\b{eta}-Ga2O3 SBDs achieved an increased breakdown voltage\nof 2.75 kV owing to the edge field management. Further enhancement of the\nbreakdown voltage was achieved by tunneling leakage management using composite\nPt cap/PtOx/Pt (1.5 nm) Schottky contacts that ultimately enabled breakdown\nvoltage of 3.7 kV for the field-plate diodes. Remarkably, the Pt cap/PtOx/Pt\n(1.5 nm) Schottky contacts maintained similar turn-on voltage as the Pt/(011)\n\\b{eta}-Ga2O3 SBDs. The combination of efficient tunneling leakage management\nby composite Pt cap/PtOx/Pt (1.5 nm) contacts with similar turn-on voltage,\nedge field reduction by high-\\k{appa} dielectric ZrO2 field plate, as well as\nthe advantageous material properties offered by (011) \\b{eta}-Ga2O3 demonstrate\na promising strategy for developing ultra-low leakage and multi-kV class\nvertical (011) \\b{eta}-Ga2O3 power devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report over 3 kV breakdown voltage and ultra-low leakage (011)\n\\b{eta}-Ga2O3 power devices utilizing Schottky barrier engineering and\nhigh-permittivity (\\k{appa}) dielectric (ZrO2) field plate. The (011)\norientation of \\b{eta}-Ga2O3 enabled low background doping and thick drift\nlayers which are promising to support kV-class vertical \\b{eta}-Ga2O3 power\nswitches. The Schottky barrier engineering was performed with a composite Pt\ncap/PtOx/Pt (1.5 nm) anode contact to take advantage of the enhanced reverse\nblocking capabilities enabled by PtOx while allowing low turn-on voltage by the\ninterfacing thin Pt layer. We also performed a systematic study using a\nco-processed Pt/(011) \\b{eta}-Ga2O3 Schottky barrier diodes (SBDs) on the same\nwafer. The bare SBDs revealed a breakdown voltage of ~1.5 kV, while the\nfield-plate Pt/(011) \\b{eta}-Ga2O3 SBDs achieved an increased breakdown voltage\nof 2.75 kV owing to the edge field management. Further enhancement of the\nbreakdown voltage was achieved by tunneling leakage management using composite\nPt cap/PtOx/Pt (1.5 nm) Schottky contacts that ultimately enabled breakdown\nvoltage of 3.7 kV for the field-plate diodes. Remarkably, the Pt cap/PtOx/Pt\n(1.5 nm) Schottky contacts maintained similar turn-on voltage as the Pt/(011)\n\\b{eta}-Ga2O3 SBDs. The combination of efficient tunneling leakage management\nby composite Pt cap/PtOx/Pt (1.5 nm) contacts with similar turn-on voltage,\nedge field reduction by high-\\k{appa} dielectric ZrO2 field plate, as well as\nthe advantageous material properties offered by (011) \\b{eta}-Ga2O3 demonstrate\na promising strategy for developing ultra-low leakage and multi-kV class\nvertical (011) \\b{eta}-Ga2O3 power devices."
                },
                "authors": [
                    {
                        "name": "Emerson J. Hollar"
                    },
                    {
                        "name": "Esmat Farzana"
                    }
                ],
                "author_detail": {
                    "name": "Esmat Farzana"
                },
                "author": "Esmat Farzana",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25695v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25695v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25604v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25604v1",
                "updated": "2025-10-29T15:12:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    15,
                    12,
                    35,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T15:12:35Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    15,
                    12,
                    35,
                    2,
                    302,
                    0
                ],
                "title": "Quickest Change Point Detection with Measurements over a Lossy Link",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quickest Change Point Detection with Measurements over a Lossy Link"
                },
                "summary": "Motivated by Industry 4.0 applications, we consider quickest change detection\n(QCD) of an abrupt change in a process when its measurements are transmitted by\na sensor over a lossy wireless link to a decision maker (DM). The sensor node\nsamples measurements using a Bernoulli sampling process, and places the\nmeasurement samples in the transmit queue of its transmitter. The transmitter\nuses a retransmit-until-success transmission strategy to deliver packets to the\nDM over the lossy link, in which the packet losses are modeled as a Bernoulli\nprocess, with different loss probabilities before and after the change. We pose\nthe QCD problem in the non-Bayesian setting under Lorden's framework, and\npropose a CUSUM algorithm. By defining a suitable Markov process, involving the\nDM measurements and the queue length process, we show that the problem reduces\nto QCD in a Markov process. Characterizing the information measure per\nmeasurement sample at the DM, we establish the asymptotic optimality of our\nalgorithm when the false alarm rate tends to zero. Further, when the DM\nreceives incomplete data due to channel loss, we present asymptotically optimal\nQCD algorithms by suitably modifying the CUSUM algorithm. We then explore the\nlast-come-first-served (LCFS) queuing discipline at the sensor transmit queue\nto lower detection delay in the non-asymptotic case. Next, we consider the case\nof multiple sensors, each with its own wireless transmitter queue, and show\nthat our analysis extends to the case of multiple homogeneous sensors. When the\nsensors are heterogeneous, we present a sensor scheduling algorithm that\nminimizes detection delay by balancing the trade-off between the age of the\nobservations and their information content. Numerical analysis demonstrate\ntrade-offs that can be used to optimize system design parameters in the\nnon-asymptotic regime.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Motivated by Industry 4.0 applications, we consider quickest change detection\n(QCD) of an abrupt change in a process when its measurements are transmitted by\na sensor over a lossy wireless link to a decision maker (DM). The sensor node\nsamples measurements using a Bernoulli sampling process, and places the\nmeasurement samples in the transmit queue of its transmitter. The transmitter\nuses a retransmit-until-success transmission strategy to deliver packets to the\nDM over the lossy link, in which the packet losses are modeled as a Bernoulli\nprocess, with different loss probabilities before and after the change. We pose\nthe QCD problem in the non-Bayesian setting under Lorden's framework, and\npropose a CUSUM algorithm. By defining a suitable Markov process, involving the\nDM measurements and the queue length process, we show that the problem reduces\nto QCD in a Markov process. Characterizing the information measure per\nmeasurement sample at the DM, we establish the asymptotic optimality of our\nalgorithm when the false alarm rate tends to zero. Further, when the DM\nreceives incomplete data due to channel loss, we present asymptotically optimal\nQCD algorithms by suitably modifying the CUSUM algorithm. We then explore the\nlast-come-first-served (LCFS) queuing discipline at the sensor transmit queue\nto lower detection delay in the non-asymptotic case. Next, we consider the case\nof multiple sensors, each with its own wireless transmitter queue, and show\nthat our analysis extends to the case of multiple homogeneous sensors. When the\nsensors are heterogeneous, we present a sensor scheduling algorithm that\nminimizes detection delay by balancing the trade-off between the age of the\nobservations and their information content. Numerical analysis demonstrate\ntrade-offs that can be used to optimize system design parameters in the\nnon-asymptotic regime."
                },
                "authors": [
                    {
                        "name": "Krishna Chaythanya KV"
                    },
                    {
                        "name": "Saqib Abbas Baba"
                    },
                    {
                        "name": "Anurag Kumar"
                    },
                    {
                        "name": "Arpan Chattopadhyay"
                    },
                    {
                        "name": "Rajesh Sundaresan"
                    }
                ],
                "author_detail": {
                    "name": "Rajesh Sundaresan"
                },
                "author": "Rajesh Sundaresan",
                "arxiv_comment": "17 pages, 6 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25604v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25604v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25590v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25590v1",
                "updated": "2025-10-29T14:58:37Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    14,
                    58,
                    37,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T14:58:37Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    14,
                    58,
                    37,
                    2,
                    302,
                    0
                ],
                "title": "RegionE: Adaptive Region-Aware Generation for Efficient Image Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RegionE: Adaptive Region-Aware Generation for Efficient Image Editing"
                },
                "summary": "Recently, instruction-based image editing (IIE) has received widespread\nattention. In practice, IIE often modifies only specific regions of an image,\nwhile the remaining areas largely remain unchanged. Although these two types of\nregions differ significantly in generation difficulty and computational\nredundancy, existing IIE models do not account for this distinction, instead\napplying a uniform generation process across the entire image. This motivates\nus to propose RegionE, an adaptive, region-aware generation framework that\naccelerates IIE tasks without additional training. Specifically, the RegionE\nframework consists of three main components: 1) Adaptive Region Partition. We\nobserved that the trajectory of unedited regions is straight, allowing for\nmulti-step denoised predictions to be inferred in a single step. Therefore, in\nthe early denoising stages, we partition the image into edited and unedited\nregions based on the difference between the final estimated result and the\nreference image. 2) Region-Aware Generation. After distinguishing the regions,\nwe replace multi-step denoising with one-step prediction for unedited areas.\nFor edited regions, the trajectory is curved, requiring local iterative\ndenoising. To improve the efficiency and quality of local iterative generation,\nwe propose the Region-Instruction KV Cache, which reduces computational cost\nwhile incorporating global information. 3) Adaptive Velocity Decay Cache.\nObserving that adjacent timesteps in edited regions exhibit strong velocity\nsimilarity, we further propose an adaptive velocity decay cache to accelerate\nthe local denoising process. We applied RegionE to state-of-the-art IIE base\nmodels, including Step1X-Edit, FLUX.1 Kontext, and Qwen-Image-Edit. RegionE\nachieved acceleration factors of 2.57, 2.41, and 2.06. Evaluations by GPT-4o\nconfirmed that semantic and perceptual fidelity were well preserved.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, instruction-based image editing (IIE) has received widespread\nattention. In practice, IIE often modifies only specific regions of an image,\nwhile the remaining areas largely remain unchanged. Although these two types of\nregions differ significantly in generation difficulty and computational\nredundancy, existing IIE models do not account for this distinction, instead\napplying a uniform generation process across the entire image. This motivates\nus to propose RegionE, an adaptive, region-aware generation framework that\naccelerates IIE tasks without additional training. Specifically, the RegionE\nframework consists of three main components: 1) Adaptive Region Partition. We\nobserved that the trajectory of unedited regions is straight, allowing for\nmulti-step denoised predictions to be inferred in a single step. Therefore, in\nthe early denoising stages, we partition the image into edited and unedited\nregions based on the difference between the final estimated result and the\nreference image. 2) Region-Aware Generation. After distinguishing the regions,\nwe replace multi-step denoising with one-step prediction for unedited areas.\nFor edited regions, the trajectory is curved, requiring local iterative\ndenoising. To improve the efficiency and quality of local iterative generation,\nwe propose the Region-Instruction KV Cache, which reduces computational cost\nwhile incorporating global information. 3) Adaptive Velocity Decay Cache.\nObserving that adjacent timesteps in edited regions exhibit strong velocity\nsimilarity, we further propose an adaptive velocity decay cache to accelerate\nthe local denoising process. We applied RegionE to state-of-the-art IIE base\nmodels, including Step1X-Edit, FLUX.1 Kontext, and Qwen-Image-Edit. RegionE\nachieved acceleration factors of 2.57, 2.41, and 2.06. Evaluations by GPT-4o\nconfirmed that semantic and perceptual fidelity were well preserved."
                },
                "authors": [
                    {
                        "name": "Pengtao Chen"
                    },
                    {
                        "name": "Xianfang Zeng"
                    },
                    {
                        "name": "Maosen Zhao"
                    },
                    {
                        "name": "Mingzhu Shen"
                    },
                    {
                        "name": "Peng Ye"
                    },
                    {
                        "name": "Bangyin Xiang"
                    },
                    {
                        "name": "Zhibo Wang"
                    },
                    {
                        "name": "Wei Cheng"
                    },
                    {
                        "name": "Gang Yu"
                    },
                    {
                        "name": "Tao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tao Chen"
                },
                "author": "Tao Chen",
                "arxiv_comment": "26 pages, 10 figures, 18 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25590v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25590v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21710v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21710v2",
                "updated": "2025-10-29T14:46:17Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    14,
                    46,
                    17,
                    2,
                    302,
                    0
                ],
                "published": "2025-06-26T18:51:04Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    18,
                    51,
                    4,
                    3,
                    177,
                    0
                ],
                "title": "FOCUS: Internal MLLM Representations for Efficient Fine-Grained Visual\n  Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FOCUS: Internal MLLM Representations for Efficient Fine-Grained Visual\n  Question Answering"
                },
                "summary": "While Multimodal Large Language Models (MLLMs) offer strong perception and\nreasoning capabilities for image-text input, Visual Question Answering (VQA)\nfocusing on small image details still remains a challenge. Although visual\ncropping techniques seem promising, recent approaches have several limitations:\nthe need for task-specific fine-tuning, low efficiency due to uninformed\nexhaustive search, or incompatibility with efficient attention implementations.\nWe address these shortcomings by proposing a training-free visual cropping\nmethod, dubbed FOCUS, that leverages MLLM-internal representations to guide the\nsearch for the most relevant image region. This is accomplished in four steps:\nfirst, we identify the target object(s) in the VQA prompt; second, we compute\nan object relevance map using the key-value (KV) cache; third, we propose and\nrank relevant image regions based on the map; and finally, we perform the\nfine-grained VQA task using the top-ranked region. As a result of this informed\nsearch strategy, FOCUS achieves strong performance across four fine-grained VQA\ndatasets and three types of MLLMs. It outperforms three popular visual cropping\nmethods in both accuracy and efficiency, and matches the best-performing\nbaseline, ZoomEye, while requiring 3 - 6.5 x less compute.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Multimodal Large Language Models (MLLMs) offer strong perception and\nreasoning capabilities for image-text input, Visual Question Answering (VQA)\nfocusing on small image details still remains a challenge. Although visual\ncropping techniques seem promising, recent approaches have several limitations:\nthe need for task-specific fine-tuning, low efficiency due to uninformed\nexhaustive search, or incompatibility with efficient attention implementations.\nWe address these shortcomings by proposing a training-free visual cropping\nmethod, dubbed FOCUS, that leverages MLLM-internal representations to guide the\nsearch for the most relevant image region. This is accomplished in four steps:\nfirst, we identify the target object(s) in the VQA prompt; second, we compute\nan object relevance map using the key-value (KV) cache; third, we propose and\nrank relevant image regions based on the map; and finally, we perform the\nfine-grained VQA task using the top-ranked region. As a result of this informed\nsearch strategy, FOCUS achieves strong performance across four fine-grained VQA\ndatasets and three types of MLLMs. It outperforms three popular visual cropping\nmethods in both accuracy and efficiency, and matches the best-performing\nbaseline, ZoomEye, while requiring 3 - 6.5 x less compute."
                },
                "authors": [
                    {
                        "name": "Liangyu Zhong"
                    },
                    {
                        "name": "Fabio Rosenthal"
                    },
                    {
                        "name": "Joachim Sicking"
                    },
                    {
                        "name": "Fabian Hger"
                    },
                    {
                        "name": "Thorsten Bagdonat"
                    },
                    {
                        "name": "Hanno Gottschalk"
                    },
                    {
                        "name": "Leo Schwinn"
                    }
                ],
                "author_detail": {
                    "name": "Leo Schwinn"
                },
                "author": "Leo Schwinn",
                "arxiv_comment": "Accepted by NeurIPS 2025 - main track. Project page:\n  https://focus-mllm-vqa.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21710v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21710v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25412v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25412v1",
                "updated": "2025-10-29T11:29:03Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    11,
                    29,
                    3,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T11:29:03Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    11,
                    29,
                    3,
                    2,
                    302,
                    0
                ],
                "title": "Serve Programs, Not Prompts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serve Programs, Not Prompts"
                },
                "summary": "Current large language model (LLM) serving systems, primarily designed for\ntext completion, are neither efficient nor adaptable for increasingly complex\nLLM applications due to their inflexible design. We propose a new LLM serving\nsystem architecture that serves programs instead of prompts to address this\nproblem. These programs, called LLM Inference Programs (LIPs), allow users to\ncustomize token prediction and KV cache management at runtime and to offload\nparts of their application logic, such as tool execution, to the server. We\ndescribe an example of this architecture through a system named Symphony, which\nfunctions as an operating system for LIPs. Symphony exposes LLM model\ncomputations via system calls and virtualizes KV cache with a dedicated file\nsystem, while ensuring GPU efficiency with a two-level process scheduling\nscheme. Symphony has the potential to open the door to a more efficient and\nextensible ecosystem for LLM applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current large language model (LLM) serving systems, primarily designed for\ntext completion, are neither efficient nor adaptable for increasingly complex\nLLM applications due to their inflexible design. We propose a new LLM serving\nsystem architecture that serves programs instead of prompts to address this\nproblem. These programs, called LLM Inference Programs (LIPs), allow users to\ncustomize token prediction and KV cache management at runtime and to offload\nparts of their application logic, such as tool execution, to the server. We\ndescribe an example of this architecture through a system named Symphony, which\nfunctions as an operating system for LIPs. Symphony exposes LLM model\ncomputations via system calls and virtualizes KV cache with a dedicated file\nsystem, while ensuring GPU efficiency with a two-level process scheduling\nscheme. Symphony has the potential to open the door to a more efficient and\nextensible ecosystem for LLM applications."
                },
                "authors": [
                    {
                        "name": "In Gim"
                    },
                    {
                        "name": "Lin Zhong"
                    }
                ],
                "author_detail": {
                    "name": "Lin Zhong"
                },
                "author": "Lin Zhong",
                "arxiv_doi": "10.1145/3713082.3730398",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3713082.3730398",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.25412v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25412v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "HotOS 2025. Follow-up implementation work (SOSP 2025) is available at\n  arXiv:2510.24051",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25152v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25152v1",
                "updated": "2025-10-29T04:09:50Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    4,
                    9,
                    50,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T04:09:50Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    4,
                    9,
                    50,
                    2,
                    302,
                    0
                ],
                "title": "Off-Centered WoS-Type Solvers with Statistical Weighting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Off-Centered WoS-Type Solvers with Statistical Weighting"
                },
                "summary": "Stochastic PDE solvers have emerged as a powerful alternative to traditional\ndiscretization-based methods for solving partial differential equations (PDEs),\nespecially in geometry processing and graphics. While off-centered estimators\nenhance sample reuse in WoS-type Monte Carlo solvers, they introduce\ncorrelation artifacts and bias when Green's functions are approximated. In this\npaper, we propose a statistically weighted off-centered WoS-type estimator that\nleverages local similarity filtering to selectively combine samples across\nneighboring evaluation points. Our method balances bias and variance through a\nprincipled weighting strategy that suppresses unreliable estimators. We\ndemonstrate our approach's effectiveness on various PDEs,including screened\nPoisson equations and boundary conditions, achieving consistent improvements\nover existing solvers such as vanilla Walk on Spheres, mean value caching, and\nboundary value caching. Our method also naturally extends to gradient field\nestimation and mixed boundary problems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stochastic PDE solvers have emerged as a powerful alternative to traditional\ndiscretization-based methods for solving partial differential equations (PDEs),\nespecially in geometry processing and graphics. While off-centered estimators\nenhance sample reuse in WoS-type Monte Carlo solvers, they introduce\ncorrelation artifacts and bias when Green's functions are approximated. In this\npaper, we propose a statistically weighted off-centered WoS-type estimator that\nleverages local similarity filtering to selectively combine samples across\nneighboring evaluation points. Our method balances bias and variance through a\nprincipled weighting strategy that suppresses unreliable estimators. We\ndemonstrate our approach's effectiveness on various PDEs,including screened\nPoisson equations and boundary conditions, achieving consistent improvements\nover existing solvers such as vanilla Walk on Spheres, mean value caching, and\nboundary value caching. Our method also naturally extends to gradient field\nestimation and mixed boundary problems."
                },
                "authors": [
                    {
                        "name": "Anchang Bao"
                    },
                    {
                        "name": "Jie Xu"
                    },
                    {
                        "name": "Enya Shen"
                    },
                    {
                        "name": "Jianmin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jianmin Wang"
                },
                "author": "Jianmin Wang",
                "arxiv_comment": "SIGGRAPH Asia 2025 conference paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25152v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25152v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25122v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25122v1",
                "updated": "2025-10-29T03:00:36Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    3,
                    0,
                    36,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T03:00:36Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    3,
                    0,
                    36,
                    2,
                    302,
                    0
                ],
                "title": "NanoVLA: Routing Decoupled Vision-Language Understanding for Nano-sized\n  Generalist Robotic Policies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NanoVLA: Routing Decoupled Vision-Language Understanding for Nano-sized\n  Generalist Robotic Policies"
                },
                "summary": "Vision-language-action (VLA) models have significantly advanced robotic\nmanipulation by integrating vision-language models (VLMs), and action decoders\ninto a unified architecture. However, their deployment on resource-constrained\nedge devices, such as mobile robots or embedded systems (e.g., Jetson Orin\nNano), remains challenging due to high computational demands, especially in\nreal-world scenarios where power, latency, and computational resources are\ncritical. To close this gap, we introduce Nano-scale Vision-Language Action\n(NanoVLA), a family of lightweight VLA architectures that achieve high\nperformance with minimal resources. Our core innovations include: (1)\nvision-language decoupling that moves conventional early vision and language\ninputs fusion in VLM to late stage, achieving better performance while enabling\ncaching and reduce inference overhead and latency; (2) long-short action\nchunking to ensure smooth, coherent multi-step planning without sacrificing\nreal-time responsiveness; (3) dynamic routing that adaptively assigns\nlightweight or heavy backbones based on task complexity, further optimizing\ninference efficiency. Experimental results on several benchmarks, as well as\nreal-world deployments, demonstrate that NanoVLA achieves up to 52x faster\ninference on edge devices compared to previous state-of-the-art VLA models,\nwith 98% less parameters while maintaining or surpassing their task accuracy\nand generalization. Ablation studies confirm that our decoupling strategy\npreserves cross-task transferability, and the routing module enhances\ncost-performance trade-offs, enabling practical, high-precision robotic\nmanipulation on resource-constrained hardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language-action (VLA) models have significantly advanced robotic\nmanipulation by integrating vision-language models (VLMs), and action decoders\ninto a unified architecture. However, their deployment on resource-constrained\nedge devices, such as mobile robots or embedded systems (e.g., Jetson Orin\nNano), remains challenging due to high computational demands, especially in\nreal-world scenarios where power, latency, and computational resources are\ncritical. To close this gap, we introduce Nano-scale Vision-Language Action\n(NanoVLA), a family of lightweight VLA architectures that achieve high\nperformance with minimal resources. Our core innovations include: (1)\nvision-language decoupling that moves conventional early vision and language\ninputs fusion in VLM to late stage, achieving better performance while enabling\ncaching and reduce inference overhead and latency; (2) long-short action\nchunking to ensure smooth, coherent multi-step planning without sacrificing\nreal-time responsiveness; (3) dynamic routing that adaptively assigns\nlightweight or heavy backbones based on task complexity, further optimizing\ninference efficiency. Experimental results on several benchmarks, as well as\nreal-world deployments, demonstrate that NanoVLA achieves up to 52x faster\ninference on edge devices compared to previous state-of-the-art VLA models,\nwith 98% less parameters while maintaining or surpassing their task accuracy\nand generalization. Ablation studies confirm that our decoupling strategy\npreserves cross-task transferability, and the routing module enhances\ncost-performance trade-offs, enabling practical, high-precision robotic\nmanipulation on resource-constrained hardware."
                },
                "authors": [
                    {
                        "name": "Jiahong Chen"
                    },
                    {
                        "name": "Jing Wang"
                    },
                    {
                        "name": "Long Chen"
                    },
                    {
                        "name": "Chuwei Cai"
                    },
                    {
                        "name": "Jinghui Lu"
                    }
                ],
                "author_detail": {
                    "name": "Jinghui Lu"
                },
                "author": "Jinghui Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25122v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25122v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24824v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24824v1",
                "updated": "2025-10-28T15:35:50Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    15,
                    35,
                    50,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T15:35:50Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    15,
                    35,
                    50,
                    1,
                    301,
                    0
                ],
                "title": "Parallel Loop Transformer for Efficient Test-Time Computation Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parallel Loop Transformer for Efficient Test-Time Computation Scaling"
                },
                "summary": "Large Language Models (LLMs) are powerful but often too slow and costly for\nreal-world use during inference. Looped transformers save on parameters by\nreusing the same weights for multiple computational steps, or \"loops.\" However,\nthis approach has a major flaw: the loops run one after another, causing\ninference latency and memory requirements to increase with each added loop.\nThis makes them impractical for fast applications. To solve this problem, we\nintroduce the Parallel Loop Transformer (PLT). PLT is a new architecture that\ndelivers the performance benefits of a deep, looped model but with the low\nlatency of a standard, non-looped model. PLT works using two key techniques.\nFirst, Cross-Loop Parallelism (CLP) breaks the sequential dependency by\ncomputing different loops for different tokens at the same time, all within a\nsingle pass. Second, to prevent memory costs from growing, we use an Efficient\nRepresentation Enhancement strategy. This method shares the memory (KV cache)\nfrom the first loop with all other loops. It then uses a Gated Sliding-Window\nAttention (G-SWA) to combine this shared global information with local\ninformation, maintaining high accuracy. Our experiments show that PLT achieves\nthe high accuracy of a traditional looped model but with almost no extra\nlatency or memory cost compared to a standard transformer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are powerful but often too slow and costly for\nreal-world use during inference. Looped transformers save on parameters by\nreusing the same weights for multiple computational steps, or \"loops.\" However,\nthis approach has a major flaw: the loops run one after another, causing\ninference latency and memory requirements to increase with each added loop.\nThis makes them impractical for fast applications. To solve this problem, we\nintroduce the Parallel Loop Transformer (PLT). PLT is a new architecture that\ndelivers the performance benefits of a deep, looped model but with the low\nlatency of a standard, non-looped model. PLT works using two key techniques.\nFirst, Cross-Loop Parallelism (CLP) breaks the sequential dependency by\ncomputing different loops for different tokens at the same time, all within a\nsingle pass. Second, to prevent memory costs from growing, we use an Efficient\nRepresentation Enhancement strategy. This method shares the memory (KV cache)\nfrom the first loop with all other loops. It then uses a Gated Sliding-Window\nAttention (G-SWA) to combine this shared global information with local\ninformation, maintaining high accuracy. Our experiments show that PLT achieves\nthe high accuracy of a traditional looped model but with almost no extra\nlatency or memory cost compared to a standard transformer."
                },
                "authors": [
                    {
                        "name": "Bohong Wu"
                    },
                    {
                        "name": "Mengzhao Chen"
                    },
                    {
                        "name": "Xiang Luo"
                    },
                    {
                        "name": "Shen Yan"
                    },
                    {
                        "name": "Qifan Yu"
                    },
                    {
                        "name": "Fan Xia"
                    },
                    {
                        "name": "Tianqi Zhang"
                    },
                    {
                        "name": "Hongrui Zhan"
                    },
                    {
                        "name": "Zheng Zhong"
                    },
                    {
                        "name": "Xun Zhou"
                    },
                    {
                        "name": "Siyuan Qiao"
                    },
                    {
                        "name": "Xingyan Bin"
                    }
                ],
                "author_detail": {
                    "name": "Xingyan Bin"
                },
                "author": "Xingyan Bin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24824v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24824v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24359v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24359v1",
                "updated": "2025-10-28T12:28:02Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    12,
                    28,
                    2,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T12:28:02Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    12,
                    28,
                    2,
                    1,
                    301,
                    0
                ],
                "title": "An N-of-1 Artificial Intelligence Ecosystem for Precision Medicine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An N-of-1 Artificial Intelligence Ecosystem for Precision Medicine"
                },
                "summary": "Artificial intelligence in medicine is built to serve the average patient. By\nminimizing error across large datasets, most systems deliver strong aggregate\naccuracy yet falter at the margins: patients with rare variants,\nmultimorbidity, or underrepresented demographics. This average patient fallacy\nerodes both equity and trust. We propose a different design: a multi-agent\necosystem for N-of-1 decision support. In this environment, agents clustered by\norgan systems, patient populations, and analytic modalities draw on a shared\nlibrary of models and evidence synthesis tools. Their results converge in a\ncoordination layer that weighs reliability, uncertainty, and data density\nbefore presenting the clinician with a decision-support packet: risk estimates\nbounded by confidence ranges, outlier flags, and linked evidence. Validation\nshifts from population averages to individual reliability, measured by error in\nlow-density regions, calibration in the small, and risk--coverage trade-offs.\nAnticipated challenges include computational demands, automation bias, and\nregulatory fit, addressed through caching strategies, consensus checks, and\nadaptive trial frameworks. By moving from monolithic models to orchestrated\nintelligence, this approach seeks to align medical AI with the first principle\nof medicine: care that is transparent, equitable, and centered on the\nindividual.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial intelligence in medicine is built to serve the average patient. By\nminimizing error across large datasets, most systems deliver strong aggregate\naccuracy yet falter at the margins: patients with rare variants,\nmultimorbidity, or underrepresented demographics. This average patient fallacy\nerodes both equity and trust. We propose a different design: a multi-agent\necosystem for N-of-1 decision support. In this environment, agents clustered by\norgan systems, patient populations, and analytic modalities draw on a shared\nlibrary of models and evidence synthesis tools. Their results converge in a\ncoordination layer that weighs reliability, uncertainty, and data density\nbefore presenting the clinician with a decision-support packet: risk estimates\nbounded by confidence ranges, outlier flags, and linked evidence. Validation\nshifts from population averages to individual reliability, measured by error in\nlow-density regions, calibration in the small, and risk--coverage trade-offs.\nAnticipated challenges include computational demands, automation bias, and\nregulatory fit, addressed through caching strategies, consensus checks, and\nadaptive trial frameworks. By moving from monolithic models to orchestrated\nintelligence, this approach seeks to align medical AI with the first principle\nof medicine: care that is transparent, equitable, and centered on the\nindividual."
                },
                "authors": [
                    {
                        "name": "Pedram Fard"
                    },
                    {
                        "name": "Alaleh Azhir"
                    },
                    {
                        "name": "Neguine Rezaii"
                    },
                    {
                        "name": "Jiazi Tian"
                    },
                    {
                        "name": "Hossein Estiri"
                    }
                ],
                "author_detail": {
                    "name": "Hossein Estiri"
                },
                "author": "Hossein Estiri",
                "arxiv_comment": "This study has been supported by grants from the National Institutes\n  of Health: The National Institute on Aging R01AG074372 and The National\n  Institute of Allergy and Infectious Diseases R01AI165535",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24359v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24359v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24273v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24273v1",
                "updated": "2025-10-28T10:32:52Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    10,
                    32,
                    52,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T10:32:52Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    10,
                    32,
                    52,
                    1,
                    301,
                    0
                ],
                "title": "SALS: Sparse Attention in Latent Space for KV cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SALS: Sparse Attention in Latent Space for KV cache Compression"
                },
                "summary": "Large Language Models capable of handling extended contexts are in high\ndemand, yet their inference remains challenging due to substantial Key-Value\ncache size and high memory bandwidth requirements. Previous research has\ndemonstrated that KV cache exhibits low-rank characteristics within the hidden\ndimension, suggesting the potential for effective compression. However, due to\nthe widely adopted Rotary Position Embedding mechanism in modern LLMs, naive\nlow-rank compression suffers severe accuracy degradation or creates a new speed\nbottleneck, as the low-rank cache must first be reconstructed in order to apply\nRoPE. In this paper, we introduce two key insights: first, the application of\nRoPE to the key vectors increases their variance, which in turn results in a\nhigher rank; second, after the key vectors are transformed into the latent\nspace, they largely maintain their representation across most layers. Based on\nthese insights, we propose the Sparse Attention in Latent Space framework. SALS\nprojects the KV cache into a compact latent space via low-rank projection, and\nperforms sparse token selection using RoPE-free query-key interactions in this\nspace. By reconstructing only a small subset of important tokens, it avoids the\noverhead of full KV cache reconstruction. We comprehensively evaluate SALS on\nvarious tasks using two large-scale models: LLaMA2-7b-chat and Mistral-7b, and\nadditionally verify its scalability on the RULER-128k benchmark with\nLLaMA3.1-8B-Instruct. Experimental results demonstrate that SALS achieves SOTA\nperformance by maintaining competitive accuracy. Under different settings, SALS\nachieves 6.4-fold KV cache compression and 5.7-fold speed-up in the attention\noperator compared to FlashAttention2 on the 4K sequence. For the end-to-end\nthroughput performance, we achieves 1.4-fold and 4.5-fold improvement compared\nto GPT-fast on 4k and 32K sequences, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models capable of handling extended contexts are in high\ndemand, yet their inference remains challenging due to substantial Key-Value\ncache size and high memory bandwidth requirements. Previous research has\ndemonstrated that KV cache exhibits low-rank characteristics within the hidden\ndimension, suggesting the potential for effective compression. However, due to\nthe widely adopted Rotary Position Embedding mechanism in modern LLMs, naive\nlow-rank compression suffers severe accuracy degradation or creates a new speed\nbottleneck, as the low-rank cache must first be reconstructed in order to apply\nRoPE. In this paper, we introduce two key insights: first, the application of\nRoPE to the key vectors increases their variance, which in turn results in a\nhigher rank; second, after the key vectors are transformed into the latent\nspace, they largely maintain their representation across most layers. Based on\nthese insights, we propose the Sparse Attention in Latent Space framework. SALS\nprojects the KV cache into a compact latent space via low-rank projection, and\nperforms sparse token selection using RoPE-free query-key interactions in this\nspace. By reconstructing only a small subset of important tokens, it avoids the\noverhead of full KV cache reconstruction. We comprehensively evaluate SALS on\nvarious tasks using two large-scale models: LLaMA2-7b-chat and Mistral-7b, and\nadditionally verify its scalability on the RULER-128k benchmark with\nLLaMA3.1-8B-Instruct. Experimental results demonstrate that SALS achieves SOTA\nperformance by maintaining competitive accuracy. Under different settings, SALS\nachieves 6.4-fold KV cache compression and 5.7-fold speed-up in the attention\noperator compared to FlashAttention2 on the 4K sequence. For the end-to-end\nthroughput performance, we achieves 1.4-fold and 4.5-fold improvement compared\nto GPT-fast on 4k and 32K sequences, respectively."
                },
                "authors": [
                    {
                        "name": "Junlin Mu"
                    },
                    {
                        "name": "Hantao Huang"
                    },
                    {
                        "name": "Jihang Zhang"
                    },
                    {
                        "name": "Minghui Yu"
                    },
                    {
                        "name": "Tao Wang"
                    },
                    {
                        "name": "Yidong Li"
                    }
                ],
                "author_detail": {
                    "name": "Yidong Li"
                },
                "author": "Yidong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24273v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24273v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24051v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24051v1",
                "updated": "2025-10-28T04:17:55Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    4,
                    17,
                    55,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T04:17:55Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    4,
                    17,
                    55,
                    1,
                    301,
                    0
                ],
                "title": "Pie: A Programmable Serving System for Emerging LLM Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pie: A Programmable Serving System for Emerging LLM Applications"
                },
                "summary": "Emerging large language model (LLM) applications involve diverse reasoning\nstrategies and agentic workflows, straining the capabilities of existing\nserving systems built on a monolithic token generation loop. This paper\nintroduces Pie, a programmable LLM serving system designed for flexibility and\nefficiency. Pie decomposes the traditional generation loop into fine-grained\nservice handlers exposed via an API and delegates control of the generation\nprocess to user-provided programs, called inferlets. This enables applications\nto implement new KV cache strategies, bespoke generation logic, and seamlessly\nintegrate computation and I/O-entirely within the application, without\nrequiring modifications to the serving system. Pie executes inferlets using\nWebAssembly, benefiting from its lightweight sandboxing. Our evaluation shows\nPie matches state-of-the-art performance on standard tasks (3-12% latency\noverhead) while significantly improving latency and throughput (1.3x-3.4x\nhigher) on agentic workflows by enabling application-specific optimizations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emerging large language model (LLM) applications involve diverse reasoning\nstrategies and agentic workflows, straining the capabilities of existing\nserving systems built on a monolithic token generation loop. This paper\nintroduces Pie, a programmable LLM serving system designed for flexibility and\nefficiency. Pie decomposes the traditional generation loop into fine-grained\nservice handlers exposed via an API and delegates control of the generation\nprocess to user-provided programs, called inferlets. This enables applications\nto implement new KV cache strategies, bespoke generation logic, and seamlessly\nintegrate computation and I/O-entirely within the application, without\nrequiring modifications to the serving system. Pie executes inferlets using\nWebAssembly, benefiting from its lightweight sandboxing. Our evaluation shows\nPie matches state-of-the-art performance on standard tasks (3-12% latency\noverhead) while significantly improving latency and throughput (1.3x-3.4x\nhigher) on agentic workflows by enabling application-specific optimizations."
                },
                "authors": [
                    {
                        "name": "In Gim"
                    },
                    {
                        "name": "Zhiyao Ma"
                    },
                    {
                        "name": "Seung-seob Lee"
                    },
                    {
                        "name": "Lin Zhong"
                    }
                ],
                "author_detail": {
                    "name": "Lin Zhong"
                },
                "author": "Lin Zhong",
                "arxiv_doi": "10.1145/3731569.3764814",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3731569.3764814",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.24051v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24051v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "SOSP 2025. Source code available at\n  https://github.com/pie-project/pie",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01068v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01068v4",
                "updated": "2025-10-28T04:00:18Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    4,
                    0,
                    18,
                    1,
                    301,
                    0
                ],
                "published": "2025-02-03T05:25:09Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    5,
                    25,
                    9,
                    0,
                    34,
                    0
                ],
                "title": "FastKV: KV Cache Compression for Fast Long-Context Processing with\n  Token-Selective Propagation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastKV: KV Cache Compression for Fast Long-Context Processing with\n  Token-Selective Propagation"
                },
                "summary": "While large language models (LLMs) excel at handling long-context sequences,\nthey require substantial prefill computation and key-value (KV) cache, which\ncan heavily burden computational efficiency and memory usage in both prefill\nand decoding stages. Recent works that compress KV caches with prefill\nacceleration reduce this cost but inadvertently tie the prefill compute\nreduction to the decoding KV budget. This coupling arises from overlooking the\nlayer-dependent variation of critical context, often leading to accuracy\ndegradation. To address this issue, we introduce FastKV, a KV cache compression\nframework designed to reduce latency in both prefill and decoding by leveraging\nthe stabilization of token importance in later layers. FastKV performs\nfull-context computation until a Token-Selective Propagation (TSP) layer, which\nforwards only the most informative tokens to subsequent layers. From these\npropagated tokens, FastKV independently selects salient KV entries for caching,\nthereby decoupling KV budget from the prefill compute reduction based on the\nTSP decision. This independent control of the TSP rate and KV retention rate\nenables flexible optimization of efficiency and accuracy. Experimental results\nshow that FastKV achieves speedups of up to 1.82$\\times$ in prefill and\n2.87$\\times$ in decoding compared to the full-context baseline, while matching\nthe accuracy of the baselines that only accelerate the decoding stage. Our code\nis available at https://github.com/dongwonjo/FastKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) excel at handling long-context sequences,\nthey require substantial prefill computation and key-value (KV) cache, which\ncan heavily burden computational efficiency and memory usage in both prefill\nand decoding stages. Recent works that compress KV caches with prefill\nacceleration reduce this cost but inadvertently tie the prefill compute\nreduction to the decoding KV budget. This coupling arises from overlooking the\nlayer-dependent variation of critical context, often leading to accuracy\ndegradation. To address this issue, we introduce FastKV, a KV cache compression\nframework designed to reduce latency in both prefill and decoding by leveraging\nthe stabilization of token importance in later layers. FastKV performs\nfull-context computation until a Token-Selective Propagation (TSP) layer, which\nforwards only the most informative tokens to subsequent layers. From these\npropagated tokens, FastKV independently selects salient KV entries for caching,\nthereby decoupling KV budget from the prefill compute reduction based on the\nTSP decision. This independent control of the TSP rate and KV retention rate\nenables flexible optimization of efficiency and accuracy. Experimental results\nshow that FastKV achieves speedups of up to 1.82$\\times$ in prefill and\n2.87$\\times$ in decoding compared to the full-context baseline, while matching\nthe accuracy of the baselines that only accelerate the decoding stage. Our code\nis available at https://github.com/dongwonjo/FastKV."
                },
                "authors": [
                    {
                        "name": "Dongwon Jo"
                    },
                    {
                        "name": "Jiwon Song"
                    },
                    {
                        "name": "Yulhwa Kim"
                    },
                    {
                        "name": "Jae-Joon Kim"
                    }
                ],
                "author_detail": {
                    "name": "Jae-Joon Kim"
                },
                "author": "Jae-Joon Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01068v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01068v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14969v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14969v2",
                "updated": "2025-10-27T21:48:48Z",
                "updated_parsed": [
                    2025,
                    10,
                    27,
                    21,
                    48,
                    48,
                    0,
                    300,
                    0
                ],
                "published": "2025-05-20T23:12:16Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    23,
                    12,
                    16,
                    1,
                    140,
                    0
                ],
                "title": "STree: Speculative Tree Decoding for Hybrid State-Space Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STree: Speculative Tree Decoding for Hybrid State-Space Models"
                },
                "summary": "Speculative decoding is a technique to leverage hardware concurrency in order\nto enable multiple steps of token generation in a single forward pass, thus\nimproving the efficiency of large-scale autoregressive (AR) Transformer models.\nState-space models (SSMs) are already more efficient than AR Transformers,\nsince their state summarizes all past data with no need to cache or re-process\ntokens in the sliding window context. However, their state can also comprise\nthousands of tokens; so, speculative decoding has recently been extended to\nSSMs. Existing approaches, however, do not leverage the tree-based verification\nmethods, since current SSMs lack the means to compute a token tree efficiently.\nWe propose the first scalable algorithm to perform tree-based speculative\ndecoding in state-space models (SSMs) and hybrid architectures of SSMs and\nTransformer layers. We exploit the structure of accumulated state transition\nmatrices to facilitate tree-based speculative decoding with minimal overhead\nrelative to current SSM implementations. Along with the algorithm, we describe\na hardware-aware implementation that improves naive application of AR\nTransformer tree-based speculative decoding methods to SSMs. Furthermore, we\noutperform vanilla speculative decoding with SSMs even with a baseline drafting\nmodel and tree structure on three different benchmarks, opening up\nopportunities for further speed up with SSM and hybrid model inference. Code\ncan be found at: https://github.com/wyc1997/stree.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding is a technique to leverage hardware concurrency in order\nto enable multiple steps of token generation in a single forward pass, thus\nimproving the efficiency of large-scale autoregressive (AR) Transformer models.\nState-space models (SSMs) are already more efficient than AR Transformers,\nsince their state summarizes all past data with no need to cache or re-process\ntokens in the sliding window context. However, their state can also comprise\nthousands of tokens; so, speculative decoding has recently been extended to\nSSMs. Existing approaches, however, do not leverage the tree-based verification\nmethods, since current SSMs lack the means to compute a token tree efficiently.\nWe propose the first scalable algorithm to perform tree-based speculative\ndecoding in state-space models (SSMs) and hybrid architectures of SSMs and\nTransformer layers. We exploit the structure of accumulated state transition\nmatrices to facilitate tree-based speculative decoding with minimal overhead\nrelative to current SSM implementations. Along with the algorithm, we describe\na hardware-aware implementation that improves naive application of AR\nTransformer tree-based speculative decoding methods to SSMs. Furthermore, we\noutperform vanilla speculative decoding with SSMs even with a baseline drafting\nmodel and tree structure on three different benchmarks, opening up\nopportunities for further speed up with SSM and hybrid model inference. Code\ncan be found at: https://github.com/wyc1997/stree."
                },
                "authors": [
                    {
                        "name": "Yangchao Wu"
                    },
                    {
                        "name": "Zongyue Qin"
                    },
                    {
                        "name": "Alex Wong"
                    },
                    {
                        "name": "Stefano Soatto"
                    }
                ],
                "author_detail": {
                    "name": "Stefano Soatto"
                },
                "author": "Stefano Soatto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14969v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14969v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.12362v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.12362v2",
                "updated": "2025-10-27T17:31:15Z",
                "updated_parsed": [
                    2025,
                    10,
                    27,
                    17,
                    31,
                    15,
                    0,
                    300,
                    0
                ],
                "published": "2024-04-18T17:45:19Z",
                "published_parsed": [
                    2024,
                    4,
                    18,
                    17,
                    45,
                    19,
                    3,
                    109,
                    0
                ],
                "title": "KV-weights are all you need for skipless transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-weights are all you need for skipless transformers"
                },
                "summary": "He and Hofmann (arXiv:2311.01906) detailed a skipless transformer without the\nV and P (post-attention projection) linear layers, which reduces the total\nnumber of weights. However, this scheme is only applicable to MHA (multi-head\nattention), but not for MQA (multi-query attention) and GQA (grouped-query\nattention). The latter schemes are used by many popular LLMs such as Llama 2,\nMistral, Mixtral, PaLM, and Gemma. Therefore, this micro-paper proposes\nmathematically equivalent versions that are suitable for MQA and GQA. For\nexample, removing Q and P from a skipless version of Mistral-7B would remove\n15% of its weights (and thus reduce its compute and memory complexity). Watch\nour explainer video https://youtu.be/Tx_lMpphd2g and see\nhttps://github.com/OpenMachine-ai/transformer-tricks for code and more\ntransformer tricks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "He and Hofmann (arXiv:2311.01906) detailed a skipless transformer without the\nV and P (post-attention projection) linear layers, which reduces the total\nnumber of weights. However, this scheme is only applicable to MHA (multi-head\nattention), but not for MQA (multi-query attention) and GQA (grouped-query\nattention). The latter schemes are used by many popular LLMs such as Llama 2,\nMistral, Mixtral, PaLM, and Gemma. Therefore, this micro-paper proposes\nmathematically equivalent versions that are suitable for MQA and GQA. For\nexample, removing Q and P from a skipless version of Mistral-7B would remove\n15% of its weights (and thus reduce its compute and memory complexity). Watch\nour explainer video https://youtu.be/Tx_lMpphd2g and see\nhttps://github.com/OpenMachine-ai/transformer-tricks for code and more\ntransformer tricks."
                },
                "authors": [
                    {
                        "name": "Nils Graef"
                    }
                ],
                "author_detail": {
                    "name": "Nils Graef"
                },
                "author": "Nils Graef",
                "arxiv_comment": "6 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.12362v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.12362v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05530v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05530v3",
                "updated": "2025-10-27T16:20:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    27,
                    16,
                    20,
                    28,
                    0,
                    300,
                    0
                ],
                "published": "2025-03-07T15:54:04Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    15,
                    54,
                    4,
                    4,
                    66,
                    0
                ],
                "title": "Leveraging Approximate Caching for Faster Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Approximate Caching for Faster Retrieval-Augmented Generation"
                },
                "summary": "Retrieval-augmented generation (RAG) improves the reliability of large\nlanguage model (LLM) answers by integrating external knowledge. However, RAG\nincreases the end-to-end inference time since looking for relevant documents\nfrom large vector databases is computationally expensive. To address this, we\nintroduce Proximity, an approximate key-value cache that optimizes the RAG\nworkflow by leveraging similarities in user queries. Instead of treating each\nquery independently, Proximity reuses previously retrieved documents when\nsimilar queries appear, substantially reducing the reliance on expensive vector\ndatabase lookups. To efficiently scale, Proximity employs a locality-sensitive\nhashing (LSH) scheme that enables fast cache lookups while preserving retrieval\naccuracy. We evaluate Proximity using the MMLU and MedRAG question-answering\nbenchmarks. Our experiments demonstrate that Proximity with our LSH scheme and\na realistically-skewed MedRAG workload reduces database calls by 77.2% while\nmaintaining database recall and test accuracy. We experiment with different\nsimilarity tolerances and cache capacities, and show that the time spent within\nthe Proximity cache remains low and constant (4.8 microseconds) even as the\ncache grows substantially in size. Our results demonstrate that approximate\ncaching is a practical and effective strategy for optimizing RAG-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) improves the reliability of large\nlanguage model (LLM) answers by integrating external knowledge. However, RAG\nincreases the end-to-end inference time since looking for relevant documents\nfrom large vector databases is computationally expensive. To address this, we\nintroduce Proximity, an approximate key-value cache that optimizes the RAG\nworkflow by leveraging similarities in user queries. Instead of treating each\nquery independently, Proximity reuses previously retrieved documents when\nsimilar queries appear, substantially reducing the reliance on expensive vector\ndatabase lookups. To efficiently scale, Proximity employs a locality-sensitive\nhashing (LSH) scheme that enables fast cache lookups while preserving retrieval\naccuracy. We evaluate Proximity using the MMLU and MedRAG question-answering\nbenchmarks. Our experiments demonstrate that Proximity with our LSH scheme and\na realistically-skewed MedRAG workload reduces database calls by 77.2% while\nmaintaining database recall and test accuracy. We experiment with different\nsimilarity tolerances and cache capacities, and show that the time spent within\nthe Proximity cache remains low and constant (4.8 microseconds) even as the\ncache grows substantially in size. Our results demonstrate that approximate\ncaching is a practical and effective strategy for optimizing RAG-based systems."
                },
                "authors": [
                    {
                        "name": "Shai Bergman"
                    },
                    {
                        "name": "Anne-Marie Kermarrec"
                    },
                    {
                        "name": "Diana Petrescu"
                    },
                    {
                        "name": "Rafael Pires"
                    },
                    {
                        "name": "Mathis Randl"
                    },
                    {
                        "name": "Martijn de Vos"
                    },
                    {
                        "name": "Ji Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ji Zhang"
                },
                "author": "Ji Zhang",
                "arxiv_doi": "10.1145/3721462.3770776",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3721462.3770776",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.05530v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05530v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted at Middleware '25",
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08343v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08343v2",
                "updated": "2025-10-27T14:59:46Z",
                "updated_parsed": [
                    2025,
                    10,
                    27,
                    14,
                    59,
                    46,
                    0,
                    300,
                    0
                ],
                "published": "2025-08-11T10:47:35Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    10,
                    47,
                    35,
                    0,
                    223,
                    0
                ],
                "title": "A Data-driven ML Approach for Maximizing Performance in LLM-Adapter\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Data-driven ML Approach for Maximizing Performance in LLM-Adapter\n  Serving"
                },
                "summary": "With the rapid adoption of Large Language Models (LLMs), LLM-adapters have\nbecome increasingly common, providing lightweight specialization of large-scale\nmodels. Serving hundreds or thousands of these adapters on a single GPU allows\nrequest aggregation, increasing throughput, but may also cause request\nstarvation if GPU memory limits are exceeded. To address this issue, this study\nfocuses on determining the joint configuration of concurrent and parallel\nadapters that maximizes GPU throughput without inducing starvation, given\nheterogeneous adapter and traffic properties. We propose a data-driven ML\napproach leveraging interpretable models to tackle this caching problem and\nintroduce the first Digital Twin capable of reproducing an LLM-adapter serving\nsystem, enabling efficient training data generation. Experiments with the vLLM\nframework and LoRA adapters show that the Digital Twin reproduces throughput\nwithin 5.1% of real results, while the ML approach predicts optimal numbers of\nconcurrent and parallel adapters with an error of at most 7.2% under\nheterogeneous, real-world workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid adoption of Large Language Models (LLMs), LLM-adapters have\nbecome increasingly common, providing lightweight specialization of large-scale\nmodels. Serving hundreds or thousands of these adapters on a single GPU allows\nrequest aggregation, increasing throughput, but may also cause request\nstarvation if GPU memory limits are exceeded. To address this issue, this study\nfocuses on determining the joint configuration of concurrent and parallel\nadapters that maximizes GPU throughput without inducing starvation, given\nheterogeneous adapter and traffic properties. We propose a data-driven ML\napproach leveraging interpretable models to tackle this caching problem and\nintroduce the first Digital Twin capable of reproducing an LLM-adapter serving\nsystem, enabling efficient training data generation. Experiments with the vLLM\nframework and LoRA adapters show that the Digital Twin reproduces throughput\nwithin 5.1% of real results, while the ML approach predicts optimal numbers of\nconcurrent and parallel adapters with an error of at most 7.2% under\nheterogeneous, real-world workloads."
                },
                "authors": [
                    {
                        "name": "Ferran Agullo"
                    },
                    {
                        "name": "Joan Oliveras"
                    },
                    {
                        "name": "Chen Wang"
                    },
                    {
                        "name": "Alberto Gutierrez-Torre"
                    },
                    {
                        "name": "Olivier Tardieu"
                    },
                    {
                        "name": "Alaa Youssef"
                    },
                    {
                        "name": "Jordi Torres"
                    },
                    {
                        "name": "Josep Ll. Berral"
                    }
                ],
                "author_detail": {
                    "name": "Josep Ll. Berral"
                },
                "author": "Josep Ll. Berral",
                "arxiv_comment": "Accepted in a computer science workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08343v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08343v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01488v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01488v2",
                "updated": "2025-10-27T11:55:07Z",
                "updated_parsed": [
                    2025,
                    10,
                    27,
                    11,
                    55,
                    7,
                    0,
                    300,
                    0
                ],
                "published": "2025-08-02T21:00:55Z",
                "published_parsed": [
                    2025,
                    8,
                    2,
                    21,
                    0,
                    55,
                    5,
                    214,
                    0
                ],
                "title": "PESTO: Real-Time Pitch Estimation with Self-supervised\n  Transposition-equivariant Objective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PESTO: Real-Time Pitch Estimation with Self-supervised\n  Transposition-equivariant Objective"
                },
                "summary": "In this paper, we introduce PESTO, a self-supervised learning approach for\nsingle-pitch estimation using a Siamese architecture. Our model processes\nindividual frames of a Variable-$Q$ Transform (VQT) and predicts pitch\ndistributions. The neural network is designed to be equivariant to\ntranslations, notably thanks to a Toeplitz fully-connected layer. In addition,\nwe construct pitch-shifted pairs by translating and cropping the VQT frames and\ntrain our model with a novel class-based transposition-equivariant objective,\neliminating the need for annotated data. Thanks to this architecture and\ntraining objective, our model achieves remarkable performances while being very\nlightweight ($130$k parameters). Evaluations on music and speech datasets\n(MIR-1K, MDB-stem-synth, and PTDB) demonstrate that PESTO not only outperforms\nself-supervised baselines but also competes with supervised methods, exhibiting\nsuperior cross-dataset generalization. Finally, we enhance PESTO's practical\nutility by developing a streamable VQT implementation using cached\nconvolutions. Combined with our model's low latency (less than 10 ms) and\nminimal parameter count, this makes PESTO particularly suitable for real-time\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce PESTO, a self-supervised learning approach for\nsingle-pitch estimation using a Siamese architecture. Our model processes\nindividual frames of a Variable-$Q$ Transform (VQT) and predicts pitch\ndistributions. The neural network is designed to be equivariant to\ntranslations, notably thanks to a Toeplitz fully-connected layer. In addition,\nwe construct pitch-shifted pairs by translating and cropping the VQT frames and\ntrain our model with a novel class-based transposition-equivariant objective,\neliminating the need for annotated data. Thanks to this architecture and\ntraining objective, our model achieves remarkable performances while being very\nlightweight ($130$k parameters). Evaluations on music and speech datasets\n(MIR-1K, MDB-stem-synth, and PTDB) demonstrate that PESTO not only outperforms\nself-supervised baselines but also competes with supervised methods, exhibiting\nsuperior cross-dataset generalization. Finally, we enhance PESTO's practical\nutility by developing a streamable VQT implementation using cached\nconvolutions. Combined with our model's low latency (less than 10 ms) and\nminimal parameter count, this makes PESTO particularly suitable for real-time\napplications."
                },
                "authors": [
                    {
                        "name": "Alain Riou"
                    },
                    {
                        "name": "Bernardo Torres"
                    },
                    {
                        "name": "Ben Hayes"
                    },
                    {
                        "name": "Stefan Lattner"
                    },
                    {
                        "name": "Gatan Hadjeres"
                    },
                    {
                        "name": "Gal Richard"
                    },
                    {
                        "name": "Geoffroy Peeters"
                    }
                ],
                "author_detail": {
                    "name": "Geoffroy Peeters"
                },
                "author": "Geoffroy Peeters",
                "arxiv_doi": "10.5334/TISMIR.251",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.5334/TISMIR.251",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.01488v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01488v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Transactions of the International Society for Music Information\n  Retrieval, 8(1): 334-352 (2025)",
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.22876v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.22876v1",
                "updated": "2025-10-26T23:59:23Z",
                "updated_parsed": [
                    2025,
                    10,
                    26,
                    23,
                    59,
                    23,
                    6,
                    299,
                    0
                ],
                "published": "2025-10-26T23:59:23Z",
                "published_parsed": [
                    2025,
                    10,
                    26,
                    23,
                    59,
                    23,
                    6,
                    299,
                    0
                ],
                "title": "Batch Speculative Decoding Done Right",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Batch Speculative Decoding Done Right"
                },
                "summary": "Speculative decoding speeds up LLM inference by using a small draft model to\npropose multiple tokens that a target model verifies in parallel. Extending\nthis idea to batches is essential for production serving, but it introduces the\nragged tensor problem: sequences in the same batch accept different numbers of\ndraft tokens, breaking right-alignment and corrupting position IDs, attention\nmasks, and KV-cache state. We show that several existing batch implementations\nviolate output equivalence-the fundamental requirement that speculative\ndecoding must produce identical token sequences to standard autoregressive\ngeneration. These violations occur precisely due to improper handling of the\nragged tensor problem. In response, we (1) characterize the synchronization\nrequirements that guarantee correctness, (2) present a correctness-first batch\nspeculative decoding EQSPEC that exposes realignment as consuming 40% of\noverhead, and (3) introduce EXSPEC, which maintains a sliding pool of sequences\nand dynamically forms same-length groups, to reduce the realignment overhead\nwhile preserving per-sequence speculative speedups. On the SpecBench dataset,\nacross Vicuna-7B/68M, Qwen3-8B/0.6B, and GLM-4-9B/0.6B target/draft pairs, our\napproach achieves up to 3$\\times$ throughput improvement at batch size 8\ncompared to batch size 1, with efficient scaling through batch size 8, while\nmaintaining 95% output equivalence. Our method requires no custom kernels and\nintegrates cleanly with existing inference stacks. Our code is available at\nhttps://github.com/eBay/spec_dec.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding speeds up LLM inference by using a small draft model to\npropose multiple tokens that a target model verifies in parallel. Extending\nthis idea to batches is essential for production serving, but it introduces the\nragged tensor problem: sequences in the same batch accept different numbers of\ndraft tokens, breaking right-alignment and corrupting position IDs, attention\nmasks, and KV-cache state. We show that several existing batch implementations\nviolate output equivalence-the fundamental requirement that speculative\ndecoding must produce identical token sequences to standard autoregressive\ngeneration. These violations occur precisely due to improper handling of the\nragged tensor problem. In response, we (1) characterize the synchronization\nrequirements that guarantee correctness, (2) present a correctness-first batch\nspeculative decoding EQSPEC that exposes realignment as consuming 40% of\noverhead, and (3) introduce EXSPEC, which maintains a sliding pool of sequences\nand dynamically forms same-length groups, to reduce the realignment overhead\nwhile preserving per-sequence speculative speedups. On the SpecBench dataset,\nacross Vicuna-7B/68M, Qwen3-8B/0.6B, and GLM-4-9B/0.6B target/draft pairs, our\napproach achieves up to 3$\\times$ throughput improvement at batch size 8\ncompared to batch size 1, with efficient scaling through batch size 8, while\nmaintaining 95% output equivalence. Our method requires no custom kernels and\nintegrates cleanly with existing inference stacks. Our code is available at\nhttps://github.com/eBay/spec_dec."
                },
                "authors": [
                    {
                        "name": "Ranran Haoran Zhang"
                    },
                    {
                        "name": "Soumik Dey"
                    },
                    {
                        "name": "Ashirbad Mishra"
                    },
                    {
                        "name": "Hansi Wu"
                    },
                    {
                        "name": "Binbin Li"
                    },
                    {
                        "name": "Rui Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Rui Zhang"
                },
                "author": "Rui Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.22876v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.22876v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10367v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10367v3",
                "updated": "2025-10-26T13:31:41Z",
                "updated_parsed": [
                    2025,
                    10,
                    26,
                    13,
                    31,
                    41,
                    6,
                    299,
                    0
                ],
                "published": "2025-07-14T15:09:01Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    15,
                    9,
                    1,
                    0,
                    195,
                    0
                ],
                "title": "FalconFS: Distributed File System for Large-Scale Deep Learning Pipeline",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FalconFS: Distributed File System for Large-Scale Deep Learning Pipeline"
                },
                "summary": "Client-side metadata caching has long been considered an effective method for\naccelerating metadata operations in distributed file systems (DFSs). However,\nwe have found that client-side state (e.g., caching) is not only ineffective\nbut also consumes valuable memory resources in the deep learning pipelines. We\nthus propose FalconFS, a DFS optimized for deep learning pipelines with the\nstateless-client architecture. Specifically, instead of performing client-side\npath resolution and caching, FalconFS efficiently resolves paths on the server\nside using hybrid metadata indexing and lazy namespace replication. FalconFS\nalso boosts server concurrency with concurrent request merging and provides\neasy deployment with VFS shortcut. Evaluations against CephFS and Lustre show\nthat FalconFS achieves up to 5.72$\\times$ throughput for small file read/write\nand up to 12.81$\\times$ throughput for deep learning model training. FalconFS\nhas been running in Huawei autonomous driving system's production environment\nwith 10,000 NPUs for one year and has been open-sourced.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Client-side metadata caching has long been considered an effective method for\naccelerating metadata operations in distributed file systems (DFSs). However,\nwe have found that client-side state (e.g., caching) is not only ineffective\nbut also consumes valuable memory resources in the deep learning pipelines. We\nthus propose FalconFS, a DFS optimized for deep learning pipelines with the\nstateless-client architecture. Specifically, instead of performing client-side\npath resolution and caching, FalconFS efficiently resolves paths on the server\nside using hybrid metadata indexing and lazy namespace replication. FalconFS\nalso boosts server concurrency with concurrent request merging and provides\neasy deployment with VFS shortcut. Evaluations against CephFS and Lustre show\nthat FalconFS achieves up to 5.72$\\times$ throughput for small file read/write\nand up to 12.81$\\times$ throughput for deep learning model training. FalconFS\nhas been running in Huawei autonomous driving system's production environment\nwith 10,000 NPUs for one year and has been open-sourced."
                },
                "authors": [
                    {
                        "name": "Jingwei Xu"
                    },
                    {
                        "name": "Junbin Kang"
                    },
                    {
                        "name": "Mingkai Dong"
                    },
                    {
                        "name": "Mingyu Liu"
                    },
                    {
                        "name": "Lu Zhang"
                    },
                    {
                        "name": "Shaohong Guo"
                    },
                    {
                        "name": "Ziyan Qiu"
                    },
                    {
                        "name": "Mingzhen You"
                    },
                    {
                        "name": "Ziyi Tian"
                    },
                    {
                        "name": "Anqi Yu"
                    },
                    {
                        "name": "Tianhong Ding"
                    },
                    {
                        "name": "Xinwei Hu"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "arxiv_comment": "Accepted by NSDI'26",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10367v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10367v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.22556v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.22556v1",
                "updated": "2025-10-26T07:17:10Z",
                "updated_parsed": [
                    2025,
                    10,
                    26,
                    7,
                    17,
                    10,
                    6,
                    299,
                    0
                ],
                "published": "2025-10-26T07:17:10Z",
                "published_parsed": [
                    2025,
                    10,
                    26,
                    7,
                    17,
                    10,
                    6,
                    299,
                    0
                ],
                "title": "SABlock: Semantic-Aware KV Cache Eviction with Adaptive Compression\n  Block Size",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SABlock: Semantic-Aware KV Cache Eviction with Adaptive Compression\n  Block Size"
                },
                "summary": "The growing memory footprint of the Key-Value (KV) cache poses a severe\nscalability bottleneck for long-context Large Language Model (LLM) inference.\nWhile KV cache eviction has emerged as an effective solution by discarding less\ncritical tokens, existing token-, block-, and sentence-level compression\nmethods struggle to balance semantic coherence and memory efficiency. To this\nend, we introduce SABlock, a \\underline{s}emantic-aware KV cache eviction\nframework with \\underline{a}daptive \\underline{block} sizes. Specifically,\nSABlock first performs semantic segmentation to align compression boundaries\nwith linguistic structures, then applies segment-guided token scoring to refine\ntoken importance estimation. Finally, for each segment, a budget-driven search\nstrategy adaptively determines the optimal block size that preserves semantic\nintegrity while improving compression efficiency under a given cache budget.\nExtensive experiments on long-context benchmarks demonstrate that SABlock\nconsistently outperforms state-of-the-art baselines under the same memory\nbudgets. For instance, on Needle-in-a-Haystack (NIAH), SABlock achieves 99.9%\nretrieval accuracy with only 96 KV entries, nearly matching the performance of\nthe full-cache baseline that retains up to 8K entries. Under a fixed cache\nbudget of 1,024, SABlock further reduces peak memory usage by 46.28% and\nachieves up to 9.5x faster decoding on a 128K context length.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing memory footprint of the Key-Value (KV) cache poses a severe\nscalability bottleneck for long-context Large Language Model (LLM) inference.\nWhile KV cache eviction has emerged as an effective solution by discarding less\ncritical tokens, existing token-, block-, and sentence-level compression\nmethods struggle to balance semantic coherence and memory efficiency. To this\nend, we introduce SABlock, a \\underline{s}emantic-aware KV cache eviction\nframework with \\underline{a}daptive \\underline{block} sizes. Specifically,\nSABlock first performs semantic segmentation to align compression boundaries\nwith linguistic structures, then applies segment-guided token scoring to refine\ntoken importance estimation. Finally, for each segment, a budget-driven search\nstrategy adaptively determines the optimal block size that preserves semantic\nintegrity while improving compression efficiency under a given cache budget.\nExtensive experiments on long-context benchmarks demonstrate that SABlock\nconsistently outperforms state-of-the-art baselines under the same memory\nbudgets. For instance, on Needle-in-a-Haystack (NIAH), SABlock achieves 99.9%\nretrieval accuracy with only 96 KV entries, nearly matching the performance of\nthe full-cache baseline that retains up to 8K entries. Under a fixed cache\nbudget of 1,024, SABlock further reduces peak memory usage by 46.28% and\nachieves up to 9.5x faster decoding on a 128K context length."
                },
                "authors": [
                    {
                        "name": "Jinhan Chen"
                    },
                    {
                        "name": "Jianchun Liu"
                    },
                    {
                        "name": "Hongli Xu"
                    },
                    {
                        "name": "Xianjun Gao"
                    },
                    {
                        "name": "Shilong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Shilong Wang"
                },
                "author": "Shilong Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.22556v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.22556v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04077v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04077v3",
                "updated": "2025-10-26T04:25:10Z",
                "updated_parsed": [
                    2025,
                    10,
                    26,
                    4,
                    25,
                    10,
                    6,
                    299,
                    0
                ],
                "published": "2025-02-06T13:41:46Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    13,
                    41,
                    46,
                    3,
                    37,
                    0
                ],
                "title": "AttentionPredictor: Temporal Patterns Matter for KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AttentionPredictor: Temporal Patterns Matter for KV Cache Compression"
                },
                "summary": "With the development of large language models (LLMs), efficient inference\nthrough Key-Value (KV) cache compression has attracted considerable attention,\nespecially for long-context generation. To compress the KV cache, recent\nmethods identify critical KV tokens through static modeling of attention\nscores. However, these methods often struggle to accurately determine critical\ntokens as they neglect the temporal patterns in attention scores, resulting in\na noticeable degradation in LLM performance. To address this challenge, we\npropose AttentionPredictor, which is the first learning-based method to\ndirectly predict attention patterns for KV cache compression and critical token\nidentification. Specifically, AttentionPredictor learns a lightweight, unified\nconvolution model to dynamically capture spatiotemporal patterns and predict\nthe next-token attention scores. An appealing feature of AttentionPredictor is\nthat it accurately predicts the attention score and shares the unified\nprediction model, which consumes negligible memory, among all transformer\nlayers. Moreover, we propose a cross-token critical cache prefetching framework\nthat hides the token estimation time overhead to accelerate the decoding stage.\nBy retaining most of the attention information, AttentionPredictor achieves\n13$\\times$ KV cache compression and 5.6$\\times$ speedup in a cache offloading\nscenario with comparable LLM performance, significantly outperforming the\nstate-of-the-arts. The code is available at\nhttps://github.com/MIRALab-USTC/LLM-AttentionPredictor.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the development of large language models (LLMs), efficient inference\nthrough Key-Value (KV) cache compression has attracted considerable attention,\nespecially for long-context generation. To compress the KV cache, recent\nmethods identify critical KV tokens through static modeling of attention\nscores. However, these methods often struggle to accurately determine critical\ntokens as they neglect the temporal patterns in attention scores, resulting in\na noticeable degradation in LLM performance. To address this challenge, we\npropose AttentionPredictor, which is the first learning-based method to\ndirectly predict attention patterns for KV cache compression and critical token\nidentification. Specifically, AttentionPredictor learns a lightweight, unified\nconvolution model to dynamically capture spatiotemporal patterns and predict\nthe next-token attention scores. An appealing feature of AttentionPredictor is\nthat it accurately predicts the attention score and shares the unified\nprediction model, which consumes negligible memory, among all transformer\nlayers. Moreover, we propose a cross-token critical cache prefetching framework\nthat hides the token estimation time overhead to accelerate the decoding stage.\nBy retaining most of the attention information, AttentionPredictor achieves\n13$\\times$ KV cache compression and 5.6$\\times$ speedup in a cache offloading\nscenario with comparable LLM performance, significantly outperforming the\nstate-of-the-arts. The code is available at\nhttps://github.com/MIRALab-USTC/LLM-AttentionPredictor."
                },
                "authors": [
                    {
                        "name": "Qingyue Yang"
                    },
                    {
                        "name": "Jie Wang"
                    },
                    {
                        "name": "Xing Li"
                    },
                    {
                        "name": "Zhihai Wang"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Xianzhi Yu"
                    },
                    {
                        "name": "Wulong Liu"
                    },
                    {
                        "name": "Jianye Hao"
                    },
                    {
                        "name": "Mingxuan Yuan"
                    },
                    {
                        "name": "Bin Li"
                    }
                ],
                "author_detail": {
                    "name": "Bin Li"
                },
                "author": "Bin Li",
                "arxiv_comment": "NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04077v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04077v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.23657v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.23657v1",
                "updated": "2025-10-26T01:25:24Z",
                "updated_parsed": [
                    2025,
                    10,
                    26,
                    1,
                    25,
                    24,
                    6,
                    299,
                    0
                ],
                "published": "2025-10-26T01:25:24Z",
                "published_parsed": [
                    2025,
                    10,
                    26,
                    1,
                    25,
                    24,
                    6,
                    299,
                    0
                ],
                "title": "A machine learning framework integrating seed traits and plasma\n  parameters for predicting germination uplift in crops",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A machine learning framework integrating seed traits and plasma\n  parameters for predicting germination uplift in crops"
                },
                "summary": "Cold plasma (CP) is an eco-friendly method to enhance seed germination, yet\noutcomes remain difficult to predict due to complex seed--plasma--environment\ninteractions. This study introduces the first machine learning framework to\nforecast germination uplift in soybean, barley, sunflower, radish, and tomato\nunder dielectric barrier discharge (DBD) plasma. Among the models tested (GB,\nXGB, ET, and hybrids), Extra Trees (ET) performed best (R\\textsuperscript{2} =\n0.919; RMSE = 3.21; MAE = 2.62), improving to R\\textsuperscript{2} = 0.925\nafter feature reduction. Engineering analysis revealed a hormetic response:\nnegligible effects at $<$7 kV or $<$200 s, maximum germination at 7--15 kV for\n200--500 s, and reduced germination beyond 20 kV or prolonged exposures.\nDischarge power was also a dominant factor, with germination rate maximizing at\n$\\geq$100 W with low exposure time. Species and cultivar-level predictions\nshowed radish (MAE = 1.46) and soybean (MAE = 2.05) were modeled with high\nconsistency, while sunflower remained slightly higher variable (MAE = 3.80).\nAmong cultivars, Williams (MAE = 1.23) and Sari (1.33) were well predicted,\nwhile Arian (2.86) and Ny\\'{\\i}rs\\'{e}gi fekete (3.74) were comparatively\npoorly captured. This framework was also embedded into MLflow, providing a\ndecision-support tool for optimizing CP seed germination in precision\nagriculture.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cold plasma (CP) is an eco-friendly method to enhance seed germination, yet\noutcomes remain difficult to predict due to complex seed--plasma--environment\ninteractions. This study introduces the first machine learning framework to\nforecast germination uplift in soybean, barley, sunflower, radish, and tomato\nunder dielectric barrier discharge (DBD) plasma. Among the models tested (GB,\nXGB, ET, and hybrids), Extra Trees (ET) performed best (R\\textsuperscript{2} =\n0.919; RMSE = 3.21; MAE = 2.62), improving to R\\textsuperscript{2} = 0.925\nafter feature reduction. Engineering analysis revealed a hormetic response:\nnegligible effects at $<$7 kV or $<$200 s, maximum germination at 7--15 kV for\n200--500 s, and reduced germination beyond 20 kV or prolonged exposures.\nDischarge power was also a dominant factor, with germination rate maximizing at\n$\\geq$100 W with low exposure time. Species and cultivar-level predictions\nshowed radish (MAE = 1.46) and soybean (MAE = 2.05) were modeled with high\nconsistency, while sunflower remained slightly higher variable (MAE = 3.80).\nAmong cultivars, Williams (MAE = 1.23) and Sari (1.33) were well predicted,\nwhile Arian (2.86) and Ny\\'{\\i}rs\\'{e}gi fekete (3.74) were comparatively\npoorly captured. This framework was also embedded into MLflow, providing a\ndecision-support tool for optimizing CP seed germination in precision\nagriculture."
                },
                "authors": [
                    {
                        "name": "Saklain Niam"
                    },
                    {
                        "name": "Tashfiqur Rahman"
                    },
                    {
                        "name": "Md. Amjad Patwary"
                    },
                    {
                        "name": "Mukarram Hossain"
                    }
                ],
                "author_detail": {
                    "name": "Mukarram Hossain"
                },
                "author": "Mukarram Hossain",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.23657v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.23657v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.22467v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.22467v1",
                "updated": "2025-10-26T00:50:12Z",
                "updated_parsed": [
                    2025,
                    10,
                    26,
                    0,
                    50,
                    12,
                    6,
                    299,
                    0
                ],
                "published": "2025-10-26T00:50:12Z",
                "published_parsed": [
                    2025,
                    10,
                    26,
                    0,
                    50,
                    12,
                    6,
                    299,
                    0
                ],
                "title": "Backward-Friendly Optimization: Training Large Language Models with\n  Approximate Gradients under Memory Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Backward-Friendly Optimization: Training Large Language Models with\n  Approximate Gradients under Memory Constraints"
                },
                "summary": "Full fine-tuning of Large Language Models (LLMs) is notoriously\nmemory-intensive, primarily because conventional optimizers such as SGD or Adam\nassume access to exact gradients derived from cached activations. Existing\nsolutions either alter the model architecture (e.g., reversible networks) or\ntrade memory for computation (e.g., activation checkpointing), but the\noptimizer itself remains untouched. In this work, we introduce GradLite, a\nbackward-friendly optimizer that relaxes the requirement of exact gradients,\nenabling efficient training even when intermediate activations are aggressively\ndiscarded or approximated. GradLite leverages two key techniques: (i) low-rank\nJacobian approximation, which reduces the dimensionality of backpropagated\nerror signals, and (ii) error-feedback correction, which accumulates and\ncompensates approximation errors across iterations to preserve convergence\nguarantees. We provide a theoretical analysis showing that GradLite maintains\nunbiased gradient estimates with bounded variance, ensuring convergence rates\ncomparable to Adam. Empirically, GradLite reduces optimizer-state and\nactivation memory consumption by up to 50\\% without architectural changes, and\nachieves on-par or superior downstream performance on reasoning (MMLU, GSM8K),\nmultilingual, and dialogue benchmarks compared to checkpointing and\noptimizer-centric baselines (LoMo, GaLore).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Full fine-tuning of Large Language Models (LLMs) is notoriously\nmemory-intensive, primarily because conventional optimizers such as SGD or Adam\nassume access to exact gradients derived from cached activations. Existing\nsolutions either alter the model architecture (e.g., reversible networks) or\ntrade memory for computation (e.g., activation checkpointing), but the\noptimizer itself remains untouched. In this work, we introduce GradLite, a\nbackward-friendly optimizer that relaxes the requirement of exact gradients,\nenabling efficient training even when intermediate activations are aggressively\ndiscarded or approximated. GradLite leverages two key techniques: (i) low-rank\nJacobian approximation, which reduces the dimensionality of backpropagated\nerror signals, and (ii) error-feedback correction, which accumulates and\ncompensates approximation errors across iterations to preserve convergence\nguarantees. We provide a theoretical analysis showing that GradLite maintains\nunbiased gradient estimates with bounded variance, ensuring convergence rates\ncomparable to Adam. Empirically, GradLite reduces optimizer-state and\nactivation memory consumption by up to 50\\% without architectural changes, and\nachieves on-par or superior downstream performance on reasoning (MMLU, GSM8K),\nmultilingual, and dialogue benchmarks compared to checkpointing and\noptimizer-centric baselines (LoMo, GaLore)."
                },
                "authors": [
                    {
                        "name": "Jing Yang"
                    },
                    {
                        "name": "Kaitong Cai"
                    },
                    {
                        "name": "Yijia Fan"
                    },
                    {
                        "name": "Yufeng Yang"
                    },
                    {
                        "name": "Keze Wang"
                    }
                ],
                "author_detail": {
                    "name": "Keze Wang"
                },
                "author": "Keze Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.22467v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.22467v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10524v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10524v3",
                "updated": "2025-10-25T14:12:56Z",
                "updated_parsed": [
                    2025,
                    10,
                    25,
                    14,
                    12,
                    56,
                    5,
                    298,
                    0
                ],
                "published": "2025-07-14T17:49:00Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    17,
                    49,
                    0,
                    0,
                    195,
                    0
                ],
                "title": "Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive\n  Token-Level Computation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive\n  Token-Level Computation"
                },
                "summary": "Scaling language models unlocks impressive capabilities, but the accompanying\ncomputational and memory demands make both training and deployment expensive.\nExisting efficiency efforts typically target either parameter sharing or\nadaptive computation, leaving open the question of how to attain both\nsimultaneously. We introduce Mixture-of-Recursions (MoR), a unified framework\nthat combines the two axes of efficiency inside a single Recursive Transformer.\nMoR reuses a shared stack of layers across recursion steps to achieve parameter\nefficiency, while lightweight routers enable adaptive token-level thinking by\ndynamically assigning different recursion depths to individual tokens. This\nallows MoR to focus quadratic attention computation only among tokens still\nactive at a given recursion depth, further improving memory access efficiency\nby selectively caching only their key-value pairs. Beyond these core\nmechanisms, we also propose a KV sharing variant that reuses KV pairs from the\nfirst recursion, specifically designed to further decrease memory footprint.\nAcross model scales ranging from 135M to 1.7B parameters, MoR forms a new\nPareto frontier: at equal training FLOPs and smaller model sizes, it\nsignificantly lowers validation perplexity and improves few-shot accuracy,\nwhile delivering higher throughput compared with vanilla and existing recursive\nbaselines. These gains demonstrate that MoR is an effective path towards\nlarge-model quality without incurring large-model cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling language models unlocks impressive capabilities, but the accompanying\ncomputational and memory demands make both training and deployment expensive.\nExisting efficiency efforts typically target either parameter sharing or\nadaptive computation, leaving open the question of how to attain both\nsimultaneously. We introduce Mixture-of-Recursions (MoR), a unified framework\nthat combines the two axes of efficiency inside a single Recursive Transformer.\nMoR reuses a shared stack of layers across recursion steps to achieve parameter\nefficiency, while lightweight routers enable adaptive token-level thinking by\ndynamically assigning different recursion depths to individual tokens. This\nallows MoR to focus quadratic attention computation only among tokens still\nactive at a given recursion depth, further improving memory access efficiency\nby selectively caching only their key-value pairs. Beyond these core\nmechanisms, we also propose a KV sharing variant that reuses KV pairs from the\nfirst recursion, specifically designed to further decrease memory footprint.\nAcross model scales ranging from 135M to 1.7B parameters, MoR forms a new\nPareto frontier: at equal training FLOPs and smaller model sizes, it\nsignificantly lowers validation perplexity and improves few-shot accuracy,\nwhile delivering higher throughput compared with vanilla and existing recursive\nbaselines. These gains demonstrate that MoR is an effective path towards\nlarge-model quality without incurring large-model cost."
                },
                "authors": [
                    {
                        "name": "Sangmin Bae"
                    },
                    {
                        "name": "Yujin Kim"
                    },
                    {
                        "name": "Reza Bayat"
                    },
                    {
                        "name": "Sungnyun Kim"
                    },
                    {
                        "name": "Jiyoun Ha"
                    },
                    {
                        "name": "Tal Schuster"
                    },
                    {
                        "name": "Adam Fisch"
                    },
                    {
                        "name": "Hrayr Harutyunyan"
                    },
                    {
                        "name": "Ziwei Ji"
                    },
                    {
                        "name": "Aaron Courville"
                    },
                    {
                        "name": "Se-Young Yun"
                    }
                ],
                "author_detail": {
                    "name": "Se-Young Yun"
                },
                "author": "Se-Young Yun",
                "arxiv_comment": "38 pages, 9 figures, 17 tables, codes at\n  https://github.com/raymin0223/mixture_of_recursions",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10524v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10524v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.23649v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.23649v1",
                "updated": "2025-10-25T11:43:27Z",
                "updated_parsed": [
                    2025,
                    10,
                    25,
                    11,
                    43,
                    27,
                    5,
                    298,
                    0
                ],
                "published": "2025-10-25T11:43:27Z",
                "published_parsed": [
                    2025,
                    10,
                    25,
                    11,
                    43,
                    27,
                    5,
                    298,
                    0
                ],
                "title": "Efficient Low Rank Attention for Long-Context Inference in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Low Rank Attention for Long-Context Inference in Large\n  Language Models"
                },
                "summary": "As the length of input text grows, the key-value (KV) cache in LLMs imposes\nprohibitive GPU memory costs and limits long-context inference on resource\nconstrained devices. Existing approaches, such as KV quantization and pruning,\nreduce memory usage but suffer from numerical precision loss or suboptimal\nretention of key-value pairs. We introduce Low Rank Query and Key attention\n(LRQK), a two-stage framework that jointly decomposes the full-precision query\nand key matrices into compact rank-\\(r\\) factors during the prefill stage, and\nthen uses these low-dimensional projections to compute proxy attention scores\nin \\(\\mathcal{O}(lr)\\) time at each decode step. By selecting only the\ntop-\\(k\\) tokens and a small fixed set of recent tokens, LRQK employs a mixed\nGPU-CPU cache with a hit-and-miss mechanism that transfers only missing\nfull-precision KV pairs, thereby preserving exact attention outputs while\nreducing CPU-GPU data movement. Extensive experiments on the RULER and\nLongBench benchmarks with LLaMA-3-8B and Qwen2.5-7B demonstrate that LRQK\nmatches or surpasses leading sparse-attention methods in long context settings,\nwhile delivering significant memory savings with minimal loss in accuracy. Our\ncode is available at https://github.com/tenghuilee/LRQK.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the length of input text grows, the key-value (KV) cache in LLMs imposes\nprohibitive GPU memory costs and limits long-context inference on resource\nconstrained devices. Existing approaches, such as KV quantization and pruning,\nreduce memory usage but suffer from numerical precision loss or suboptimal\nretention of key-value pairs. We introduce Low Rank Query and Key attention\n(LRQK), a two-stage framework that jointly decomposes the full-precision query\nand key matrices into compact rank-\\(r\\) factors during the prefill stage, and\nthen uses these low-dimensional projections to compute proxy attention scores\nin \\(\\mathcal{O}(lr)\\) time at each decode step. By selecting only the\ntop-\\(k\\) tokens and a small fixed set of recent tokens, LRQK employs a mixed\nGPU-CPU cache with a hit-and-miss mechanism that transfers only missing\nfull-precision KV pairs, thereby preserving exact attention outputs while\nreducing CPU-GPU data movement. Extensive experiments on the RULER and\nLongBench benchmarks with LLaMA-3-8B and Qwen2.5-7B demonstrate that LRQK\nmatches or surpasses leading sparse-attention methods in long context settings,\nwhile delivering significant memory savings with minimal loss in accuracy. Our\ncode is available at https://github.com/tenghuilee/LRQK."
                },
                "authors": [
                    {
                        "name": "Tenghui Li"
                    },
                    {
                        "name": "Guoxu Zhou"
                    },
                    {
                        "name": "Xuyang Zhao"
                    },
                    {
                        "name": "Yuning Qiu"
                    },
                    {
                        "name": "Qibin Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Qibin Zhao"
                },
                "author": "Qibin Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.23649v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.23649v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.22145v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.22145v1",
                "updated": "2025-10-25T03:34:34Z",
                "updated_parsed": [
                    2025,
                    10,
                    25,
                    3,
                    34,
                    34,
                    5,
                    298,
                    0
                ],
                "published": "2025-10-25T03:34:34Z",
                "published_parsed": [
                    2025,
                    10,
                    25,
                    3,
                    34,
                    34,
                    5,
                    298,
                    0
                ],
                "title": "Fundamental Limits of Coded Caching with Fixed Subpacketization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fundamental Limits of Coded Caching with Fixed Subpacketization"
                },
                "summary": "Coded caching is a promising technique to create coded multicast\nopportunities for cache-aided networks. By splitting each file into $F$ equal\npackets (i.e., the subpacketization level $F$) and letting each user cache a\nset of packets, the transmission load can be significantly reduced via coded\nmulticasting. It has been shown that a higher subpacketization level could\npotentially lead to a lower transmission load, as more packets can be combined\nfor efficient transmission. On the other hand, a larger $F$ indicates a higher\ncoding complexity and is problematic from a practical perspective when $F$ is\nextremely large. Despite many works attempting to design coded caching schemes\nwith low subpacketization levels, a fundamental problem remains open: What is\nthe minimum transmission load given any fixed subpacketization level? In this\npaper, we consider the classical cache-aided networks with identically uncoded\nplacement and one-shot delivery strategy, and investigate the fundamental\ntrade-off between the transmission load and the subpacketization level. We\npropose a \\emph{general} lower bound on the transmission load for any fixed\nsubpacketization by reformulating the centralized coded caching schemes via the\ncombinatorial structure of the corresponding placement delivery array. The\nlower bound also recovers existing optimality results for the bipartite graph\nscheme (including the well-known Maddah-Ali and Niesen (MN) scheme and the\nconjugate MN scheme) as well as the grouping bipartite graph scheme.\nFurthermore, by carefully exploiting the combinatorial structure and computing\nthe union size of sorted sets, we establish a new optimality result, i.e., the\npartition scheme can achieve the optimal rate-subpacketization trade-off.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coded caching is a promising technique to create coded multicast\nopportunities for cache-aided networks. By splitting each file into $F$ equal\npackets (i.e., the subpacketization level $F$) and letting each user cache a\nset of packets, the transmission load can be significantly reduced via coded\nmulticasting. It has been shown that a higher subpacketization level could\npotentially lead to a lower transmission load, as more packets can be combined\nfor efficient transmission. On the other hand, a larger $F$ indicates a higher\ncoding complexity and is problematic from a practical perspective when $F$ is\nextremely large. Despite many works attempting to design coded caching schemes\nwith low subpacketization levels, a fundamental problem remains open: What is\nthe minimum transmission load given any fixed subpacketization level? In this\npaper, we consider the classical cache-aided networks with identically uncoded\nplacement and one-shot delivery strategy, and investigate the fundamental\ntrade-off between the transmission load and the subpacketization level. We\npropose a \\emph{general} lower bound on the transmission load for any fixed\nsubpacketization by reformulating the centralized coded caching schemes via the\ncombinatorial structure of the corresponding placement delivery array. The\nlower bound also recovers existing optimality results for the bipartite graph\nscheme (including the well-known Maddah-Ali and Niesen (MN) scheme and the\nconjugate MN scheme) as well as the grouping bipartite graph scheme.\nFurthermore, by carefully exploiting the combinatorial structure and computing\nthe union size of sorted sets, we establish a new optimality result, i.e., the\npartition scheme can achieve the optimal rate-subpacketization trade-off."
                },
                "authors": [
                    {
                        "name": "Minquan Cheng"
                    },
                    {
                        "name": "Yifei Huang"
                    },
                    {
                        "name": "Youlong Wu"
                    },
                    {
                        "name": "Jinyan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jinyan Wang"
                },
                "author": "Jinyan Wang",
                "arxiv_comment": "19 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.22145v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.22145v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10270v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10270v3",
                "updated": "2025-10-25T02:29:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    25,
                    2,
                    29,
                    47,
                    5,
                    298,
                    0
                ],
                "published": "2025-03-13T11:26:45Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    11,
                    26,
                    45,
                    3,
                    72,
                    0
                ],
                "title": "EEdit: Rethinking the Spatial and Temporal Redundancy for Efficient\n  Image Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EEdit: Rethinking the Spatial and Temporal Redundancy for Efficient\n  Image Editing"
                },
                "summary": "Inversion-based image editing is rapidly gaining momentum while suffering\nfrom significant computation overhead, hindering its application in real-time\ninteractive scenarios. In this paper, we rethink that the redundancy in\ninversion-based image editing exists in both the spatial and temporal\ndimensions, such as the unnecessary computation in unedited regions and the\nredundancy in the inversion progress. To tackle these challenges, we propose a\npractical framework, named EEdit, to achieve efficient image editing.\nSpecifically, we introduce three techniques to solve them one by one. For\nspatial redundancy, spatial locality caching is introduced to compute the\nedited region and its neighboring regions while skipping the unedited regions,\nand token indexing preprocessing is designed to further accelerate the caching.\nFor temporal redundancy, inversion step skipping is proposed to reuse the\nlatent for efficient editing. Our experiments demonstrate an average of 2.46\n$\\times$ acceleration without performance drop in a wide range of editing tasks\nincluding prompt-guided image editing, dragging and image composition. Our\ncodes are available at https://github.com/yuriYanZeXuan/EEdit",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inversion-based image editing is rapidly gaining momentum while suffering\nfrom significant computation overhead, hindering its application in real-time\ninteractive scenarios. In this paper, we rethink that the redundancy in\ninversion-based image editing exists in both the spatial and temporal\ndimensions, such as the unnecessary computation in unedited regions and the\nredundancy in the inversion progress. To tackle these challenges, we propose a\npractical framework, named EEdit, to achieve efficient image editing.\nSpecifically, we introduce three techniques to solve them one by one. For\nspatial redundancy, spatial locality caching is introduced to compute the\nedited region and its neighboring regions while skipping the unedited regions,\nand token indexing preprocessing is designed to further accelerate the caching.\nFor temporal redundancy, inversion step skipping is proposed to reuse the\nlatent for efficient editing. Our experiments demonstrate an average of 2.46\n$\\times$ acceleration without performance drop in a wide range of editing tasks\nincluding prompt-guided image editing, dragging and image composition. Our\ncodes are available at https://github.com/yuriYanZeXuan/EEdit"
                },
                "authors": [
                    {
                        "name": "Zexuan Yan"
                    },
                    {
                        "name": "Yue Ma"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Wenteng Chen"
                    },
                    {
                        "name": "Qifeng Chen"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "accepted by ICCV2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10270v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10270v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.22049v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.22049v1",
                "updated": "2025-10-24T22:17:49Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    22,
                    17,
                    49,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T22:17:49Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    22,
                    17,
                    49,
                    4,
                    297,
                    0
                ],
                "title": "Massive Memorization with Hundreds of Trillions of Parameters for\n  Sequential Transducer Generative Recommenders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Massive Memorization with Hundreds of Trillions of Parameters for\n  Sequential Transducer Generative Recommenders"
                },
                "summary": "Modern large-scale recommendation systems rely heavily on user interaction\nhistory sequences to enhance the model performance. The advent of large\nlanguage models and sequential modeling techniques, particularly\ntransformer-like architectures, has led to significant advancements recently\n(e.g., HSTU, SIM, and TWIN models). While scaling to ultra-long user histories\n(10k to 100k items) generally improves model performance, it also creates\nsignificant challenges on latency, queries per second (QPS) and GPU cost in\nindustry-scale recommendation systems. Existing models do not adequately\naddress these industrial scalability issues. In this paper, we propose a novel\ntwo-stage modeling framework, namely VIrtual Sequential Target Attention\n(VISTA), which decomposes traditional target attention from a candidate item to\nuser history items into two distinct stages: (1) user history summarization\ninto a few hundred tokens; followed by (2) candidate item attention to those\ntokens. These summarization token embeddings are then cached in storage system\nand then utilized as sequence features for downstream model training and\ninference. This novel design for scalability enables VISTA to scale to lifelong\nuser histories (up to one million items) while keeping downstream training and\ninference costs fixed, which is essential in industry. Our approach achieves\nsignificant improvements in offline and online metrics and has been\nsuccessfully deployed on an industry leading recommendation platform serving\nbillions of users.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern large-scale recommendation systems rely heavily on user interaction\nhistory sequences to enhance the model performance. The advent of large\nlanguage models and sequential modeling techniques, particularly\ntransformer-like architectures, has led to significant advancements recently\n(e.g., HSTU, SIM, and TWIN models). While scaling to ultra-long user histories\n(10k to 100k items) generally improves model performance, it also creates\nsignificant challenges on latency, queries per second (QPS) and GPU cost in\nindustry-scale recommendation systems. Existing models do not adequately\naddress these industrial scalability issues. In this paper, we propose a novel\ntwo-stage modeling framework, namely VIrtual Sequential Target Attention\n(VISTA), which decomposes traditional target attention from a candidate item to\nuser history items into two distinct stages: (1) user history summarization\ninto a few hundred tokens; followed by (2) candidate item attention to those\ntokens. These summarization token embeddings are then cached in storage system\nand then utilized as sequence features for downstream model training and\ninference. This novel design for scalability enables VISTA to scale to lifelong\nuser histories (up to one million items) while keeping downstream training and\ninference costs fixed, which is essential in industry. Our approach achieves\nsignificant improvements in offline and online metrics and has been\nsuccessfully deployed on an industry leading recommendation platform serving\nbillions of users."
                },
                "authors": [
                    {
                        "name": "Zhimin Chen"
                    },
                    {
                        "name": "Chenyu Zhao"
                    },
                    {
                        "name": "Ka Chun Mo"
                    },
                    {
                        "name": "Yunjiang Jiang"
                    },
                    {
                        "name": "Jane H. Lee"
                    },
                    {
                        "name": "Shouwei Chen"
                    },
                    {
                        "name": "Khushhall Chandra Mahajan"
                    },
                    {
                        "name": "Ning Jiang"
                    },
                    {
                        "name": "Kai Ren"
                    },
                    {
                        "name": "Jinhui Li"
                    },
                    {
                        "name": "Wen-Yun Yang"
                    }
                ],
                "author_detail": {
                    "name": "Wen-Yun Yang"
                },
                "author": "Wen-Yun Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.22049v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.22049v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21696v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21696v1",
                "updated": "2025-10-24T17:56:37Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    17,
                    56,
                    37,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T17:56:37Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    17,
                    56,
                    37,
                    4,
                    297,
                    0
                ],
                "title": "BachVid: Training-Free Video Generation with Consistent Background and\n  Character",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BachVid: Training-Free Video Generation with Consistent Background and\n  Character"
                },
                "summary": "Diffusion Transformers (DiTs) have recently driven significant progress in\ntext-to-video (T2V) generation. However, generating multiple videos with\nconsistent characters and backgrounds remains a significant challenge. Existing\nmethods typically rely on reference images or extensive training, and often\nonly address character consistency, leaving background consistency to\nimage-to-video models. We introduce BachVid, the first training-free method\nthat achieves consistent video generation without needing any reference images.\nOur approach is based on a systematic analysis of DiT's attention mechanism and\nintermediate features, revealing its ability to extract foreground masks and\nidentify matching points during the denoising process. Our method leverages\nthis finding by first generating an identity video and caching the intermediate\nvariables, and then inject these cached variables into corresponding positions\nin newly generated videos, ensuring both foreground and background consistency\nacross multiple videos. Experimental results demonstrate that BachVid achieves\nrobust consistency in generated videos without requiring additional training,\noffering a novel and efficient solution for consistent video generation without\nrelying on reference images or additional training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) have recently driven significant progress in\ntext-to-video (T2V) generation. However, generating multiple videos with\nconsistent characters and backgrounds remains a significant challenge. Existing\nmethods typically rely on reference images or extensive training, and often\nonly address character consistency, leaving background consistency to\nimage-to-video models. We introduce BachVid, the first training-free method\nthat achieves consistent video generation without needing any reference images.\nOur approach is based on a systematic analysis of DiT's attention mechanism and\nintermediate features, revealing its ability to extract foreground masks and\nidentify matching points during the denoising process. Our method leverages\nthis finding by first generating an identity video and caching the intermediate\nvariables, and then inject these cached variables into corresponding positions\nin newly generated videos, ensuring both foreground and background consistency\nacross multiple videos. Experimental results demonstrate that BachVid achieves\nrobust consistency in generated videos without requiring additional training,\noffering a novel and efficient solution for consistent video generation without\nrelying on reference images or additional training."
                },
                "authors": [
                    {
                        "name": "Han Yan"
                    },
                    {
                        "name": "Xibin Song"
                    },
                    {
                        "name": "Yifu Wang"
                    },
                    {
                        "name": "Hongdong Li"
                    },
                    {
                        "name": "Pan Ji"
                    },
                    {
                        "name": "Chao Ma"
                    }
                ],
                "author_detail": {
                    "name": "Chao Ma"
                },
                "author": "Chao Ma",
                "arxiv_comment": "Project page: https://wolfball.github.io/bachvid",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21696v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21696v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.20787v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.20787v2",
                "updated": "2025-10-24T16:56:22Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    16,
                    56,
                    22,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-23T17:53:03Z",
                "published_parsed": [
                    2025,
                    10,
                    23,
                    17,
                    53,
                    3,
                    3,
                    296,
                    0
                ],
                "title": "Alleviating Forgetfulness of Linear Attention by Hybrid Sparse Attention\n  and Contextualized Learnable Token Eviction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Alleviating Forgetfulness of Linear Attention by Hybrid Sparse Attention\n  and Contextualized Learnable Token Eviction"
                },
                "summary": "Linear-attention models that compress the entire input sequence into a\nfixed-size recurrent state offer an efficient alternative to Transformers, but\ntheir finite memory induces forgetfulness that harms retrieval-intensive tasks.\nTo mitigate the issue, we explore a series of hybrid models that restore direct\naccess to past tokens. We interleave token mixers with intermediate time and\nspace complexity between linear and full attention, including sparse attention\nwith token eviction, and the query-aware native sparse attention. Particularly,\nwe propose a novel learnable token eviction approach. Combined with\nsliding-window attention, an end-to-end trainable lightweight CNN aggregates\ninformation from both past and future adjacent tokens to adaptively retain a\nlimited set of critical KV-pairs per head, maintaining linear attention's\nconstant time and space complexity. Efficient Triton kernels for the sparse\nattention mechanisms are provided. Empirical evaluations on retrieval-intensive\nbenchmarks support the effectiveness of our approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linear-attention models that compress the entire input sequence into a\nfixed-size recurrent state offer an efficient alternative to Transformers, but\ntheir finite memory induces forgetfulness that harms retrieval-intensive tasks.\nTo mitigate the issue, we explore a series of hybrid models that restore direct\naccess to past tokens. We interleave token mixers with intermediate time and\nspace complexity between linear and full attention, including sparse attention\nwith token eviction, and the query-aware native sparse attention. Particularly,\nwe propose a novel learnable token eviction approach. Combined with\nsliding-window attention, an end-to-end trainable lightweight CNN aggregates\ninformation from both past and future adjacent tokens to adaptively retain a\nlimited set of critical KV-pairs per head, maintaining linear attention's\nconstant time and space complexity. Efficient Triton kernels for the sparse\nattention mechanisms are provided. Empirical evaluations on retrieval-intensive\nbenchmarks support the effectiveness of our approaches."
                },
                "authors": [
                    {
                        "name": "Mutian He"
                    },
                    {
                        "name": "Philip N. Garner"
                    }
                ],
                "author_detail": {
                    "name": "Philip N. Garner"
                },
                "author": "Philip N. Garner",
                "arxiv_comment": "19 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.20787v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.20787v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17388v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17388v2",
                "updated": "2025-10-24T14:55:42Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    14,
                    55,
                    42,
                    4,
                    297,
                    0
                ],
                "published": "2025-09-22T06:52:35Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    6,
                    52,
                    35,
                    0,
                    265,
                    0
                ],
                "title": "Prefetching in Deep Memory Hierarchies with NVRAM as Main Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prefetching in Deep Memory Hierarchies with NVRAM as Main Memory"
                },
                "summary": "Emerging applications, such as big data analytics and machine learning,\nrequire increasingly large amounts of main memory, often exceeding the capacity\nof current commodity processors built on DRAM technology. To address this,\nrecent research has focused on off-chip memory controllers that facilitate\naccess to diverse memory media, each with unique density and latency\ncharacteristics. While these solutions improve memory system performance, they\nalso exacerbate the already significant memory latency. As a result,\nmulti-level prefetching techniques are essential to mitigate these extended\nlatencies.\n  This paper investigates the advantages of prefetching across both sides of\nthe memory system: the off-chip memory and the on-chip cache hierarchy. Our\nprimary objective is to assess the impact of a multi-level prefetching engine\non overall system performance. Additionally, we analyze the individual\ncontribution of each prefetching level to system efficiency. To achieve this,\nthe study evaluates two key prefetching approaches: HMC (Hybrid Memory\nController) and HMC+L1, both of which employ prefetching mechanisms commonly\nused by processor vendors. The HMC approach integrates a prefetcher within the\noff-chip hybrid memory controller, while the HMC+L1 approach combines this with\nadditional L1 on-chip prefetchers.\n  Experimental results on an out-of-order execution processor show that on-chip\ncache prefetchers are crucial for maximizing the benefits of off-chip\nprefetching, which in turn further enhances performance. Specifically, the\noff-chip HMC prefetcher achieves coverage and accuracy rates exceeding 60% and\nup to 80%, while the combined HMC+L1 approach boosts off-chip prefetcher\ncoverage to as much as 92%. Consequently, overall performance increases from 9%\nwith the HMC approach to 12% when L1 prefetching is also employed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emerging applications, such as big data analytics and machine learning,\nrequire increasingly large amounts of main memory, often exceeding the capacity\nof current commodity processors built on DRAM technology. To address this,\nrecent research has focused on off-chip memory controllers that facilitate\naccess to diverse memory media, each with unique density and latency\ncharacteristics. While these solutions improve memory system performance, they\nalso exacerbate the already significant memory latency. As a result,\nmulti-level prefetching techniques are essential to mitigate these extended\nlatencies.\n  This paper investigates the advantages of prefetching across both sides of\nthe memory system: the off-chip memory and the on-chip cache hierarchy. Our\nprimary objective is to assess the impact of a multi-level prefetching engine\non overall system performance. Additionally, we analyze the individual\ncontribution of each prefetching level to system efficiency. To achieve this,\nthe study evaluates two key prefetching approaches: HMC (Hybrid Memory\nController) and HMC+L1, both of which employ prefetching mechanisms commonly\nused by processor vendors. The HMC approach integrates a prefetcher within the\noff-chip hybrid memory controller, while the HMC+L1 approach combines this with\nadditional L1 on-chip prefetchers.\n  Experimental results on an out-of-order execution processor show that on-chip\ncache prefetchers are crucial for maximizing the benefits of off-chip\nprefetching, which in turn further enhances performance. Specifically, the\noff-chip HMC prefetcher achieves coverage and accuracy rates exceeding 60% and\nup to 80%, while the combined HMC+L1 approach boosts off-chip prefetcher\ncoverage to as much as 92%. Consequently, overall performance increases from 9%\nwith the HMC approach to 12% when L1 prefetching is also employed."
                },
                "authors": [
                    {
                        "name": "Manel Lurbe"
                    },
                    {
                        "name": "Miguel Avargues"
                    },
                    {
                        "name": "Salvador Petit"
                    },
                    {
                        "name": "Maria E. Gomez"
                    },
                    {
                        "name": "Rui Yang"
                    },
                    {
                        "name": "Guanhao Wang"
                    },
                    {
                        "name": "Julio Sahuquillo"
                    }
                ],
                "author_detail": {
                    "name": "Julio Sahuquillo"
                },
                "author": "Julio Sahuquillo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17388v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17388v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22922v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22922v2",
                "updated": "2025-10-24T11:53:34Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    11,
                    53,
                    34,
                    4,
                    297,
                    0
                ],
                "published": "2025-06-28T15:15:31Z",
                "published_parsed": [
                    2025,
                    6,
                    28,
                    15,
                    15,
                    31,
                    5,
                    179,
                    0
                ],
                "title": "Global Predecessor Indexing: Avoiding Binary Search in Weighted Job\n  Scheduling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Global Predecessor Indexing: Avoiding Binary Search in Weighted Job\n  Scheduling"
                },
                "summary": "We present an improved solution to the Weighted Job Scheduling (WJS) problem.\nWhile the classical dynamic programming (DP) solution for $n$ jobs runs in $O(n\n\\log(n))$ time due to comparison-based sorting and per-job binary search, we\neliminate the binary search bottleneck. In its place, we introduce a novel\nmulti-phase preprocessing technique called \\emph{Global Predecessor Indexing\n(GPI)}, which computes the latest non-overlapping job (i.e., the predecessor)\nfor all jobs via a two-pointer linear-time pass after sorting. This yields a\ntime complexity of $O(S(n) + n)$ where $S(n)$ is the time to sort all jobs. GPI\nenables direct use in the classical DP recurrence. When combined with\nlinear-time sorting, GPI yields a complete $O(n)$ solution. Even with\ncomparison-based sorting, GPI significantly outperforms the classical solution\nin practice by avoiding repeated binary searches in favor of the more\ncache-efficient extra sort and two-pointer pass.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present an improved solution to the Weighted Job Scheduling (WJS) problem.\nWhile the classical dynamic programming (DP) solution for $n$ jobs runs in $O(n\n\\log(n))$ time due to comparison-based sorting and per-job binary search, we\neliminate the binary search bottleneck. In its place, we introduce a novel\nmulti-phase preprocessing technique called \\emph{Global Predecessor Indexing\n(GPI)}, which computes the latest non-overlapping job (i.e., the predecessor)\nfor all jobs via a two-pointer linear-time pass after sorting. This yields a\ntime complexity of $O(S(n) + n)$ where $S(n)$ is the time to sort all jobs. GPI\nenables direct use in the classical DP recurrence. When combined with\nlinear-time sorting, GPI yields a complete $O(n)$ solution. Even with\ncomparison-based sorting, GPI significantly outperforms the classical solution\nin practice by avoiding repeated binary searches in favor of the more\ncache-efficient extra sort and two-pointer pass."
                },
                "authors": [
                    {
                        "name": "Amit Joshi"
                    }
                ],
                "author_detail": {
                    "name": "Amit Joshi"
                },
                "author": "Amit Joshi",
                "arxiv_comment": "6 pages, 9 figures including tables. Short theoretical and practical\n  paper on improved dynamic programming for weighted job scheduling with\n  linear-time preprocessing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22922v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22922v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21361v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21361v1",
                "updated": "2025-10-24T11:42:38Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    11,
                    42,
                    38,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T11:42:38Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    11,
                    42,
                    38,
                    4,
                    297,
                    0
                ],
                "title": "Compositional Monte Carlo Tree Diffusion for Extendable Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compositional Monte Carlo Tree Diffusion for Extendable Planning"
                },
                "summary": "Monte Carlo Tree Diffusion (MCTD) integrates diffusion models with structured\ntree search to enable effective trajectory exploration through stepwise\nreasoning. However, MCTD remains fundamentally limited by training trajectory\nlengths. While periodic replanning allows plan concatenation for longer plan\ngeneration, the planning process remains locally confined, as MCTD searches\nwithin individual trajectories without access to global context. We propose\nCompositional Monte Carlo Tree Diffusion (C-MCTD), a framework that elevates\nplanning from individual trajectory optimization to reasoning over complete\nplan compositions. C-MCTD introduces three complementary components: (1) Online\nComposer, which performs globally-aware planning by searching across entire\nplan compositions; (2) Distributed Composer, which reduces search complexity\nthrough parallel exploration from multiple starting points; and (3) Preplan\nComposer, which accelerates inference by leveraging cached plan graphs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Monte Carlo Tree Diffusion (MCTD) integrates diffusion models with structured\ntree search to enable effective trajectory exploration through stepwise\nreasoning. However, MCTD remains fundamentally limited by training trajectory\nlengths. While periodic replanning allows plan concatenation for longer plan\ngeneration, the planning process remains locally confined, as MCTD searches\nwithin individual trajectories without access to global context. We propose\nCompositional Monte Carlo Tree Diffusion (C-MCTD), a framework that elevates\nplanning from individual trajectory optimization to reasoning over complete\nplan compositions. C-MCTD introduces three complementary components: (1) Online\nComposer, which performs globally-aware planning by searching across entire\nplan compositions; (2) Distributed Composer, which reduces search complexity\nthrough parallel exploration from multiple starting points; and (3) Preplan\nComposer, which accelerates inference by leveraging cached plan graphs."
                },
                "authors": [
                    {
                        "name": "Jaesik Yoon"
                    },
                    {
                        "name": "Hyeonseo Cho"
                    },
                    {
                        "name": "Sungjin Ahn"
                    }
                ],
                "author_detail": {
                    "name": "Sungjin Ahn"
                },
                "author": "Sungjin Ahn",
                "arxiv_comment": "24 pages, 4 figures, NeurIPS 25 Spotlight",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21361v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21361v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19240v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19240v2",
                "updated": "2025-10-24T08:35:21Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    8,
                    35,
                    21,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-22T04:48:41Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    4,
                    48,
                    41,
                    2,
                    295,
                    0
                ],
                "title": "A General Solution for the Implementation of CI/CD in Embedded Linux\n  Development",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A General Solution for the Implementation of CI/CD in Embedded Linux\n  Development"
                },
                "summary": "With the growing use of embedded systems in various industries, the need for\nautomated platforms for the development and deployment of customized\nLinux-based operating systems has become more important. This research was\nconducted with the aim of designing and implementing an integrated and\nreproducible infrastructure for the development, building, and testing of a\nLinux-based operating system using the Yocto Project. The proposed structure\nwas implemented based on a three-layer architecture consisting of the main\nYocto repositories, a custom layer (meta-custom), and a coordinating manifest\nlayer to ensure version synchronization, scalability, and reproducibility.\nThree sample projects, including libhelloworld, helloworld, and the kernel\nmodule hello mod, were developed and integrated into the build process.\nContinuous Integration and Continuous Deployment pipelines were implemented\nwith GitLab CI and combined with an isolated Docker environment to automate and\nstreamline the build and testing workflows. Using a local cache server\ncontaining hashserv, downloads and sstate cache significantly reduced the build\ntime. The functionality and stability of the system were verified through six\nboot test scenarios in the QEMU simulator. The results show that the proposed\ndesign not only ensures reproducibility but also can be extended to advanced\napplications such as continuous deployment of real-time Linux versions. Future\nrecommendations include expanding automated tests, implementing system\nmonitoring with Prometheus and Grafana, using distributed builds, optimizing\nwith Docker multi-stage builds, and enabling continuous deployment of real-time\nLinux changes to provide a stable and scalable model for industrial and\nresearch projects in embedded systems with a rapid and reliable development\ncycle.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the growing use of embedded systems in various industries, the need for\nautomated platforms for the development and deployment of customized\nLinux-based operating systems has become more important. This research was\nconducted with the aim of designing and implementing an integrated and\nreproducible infrastructure for the development, building, and testing of a\nLinux-based operating system using the Yocto Project. The proposed structure\nwas implemented based on a three-layer architecture consisting of the main\nYocto repositories, a custom layer (meta-custom), and a coordinating manifest\nlayer to ensure version synchronization, scalability, and reproducibility.\nThree sample projects, including libhelloworld, helloworld, and the kernel\nmodule hello mod, were developed and integrated into the build process.\nContinuous Integration and Continuous Deployment pipelines were implemented\nwith GitLab CI and combined with an isolated Docker environment to automate and\nstreamline the build and testing workflows. Using a local cache server\ncontaining hashserv, downloads and sstate cache significantly reduced the build\ntime. The functionality and stability of the system were verified through six\nboot test scenarios in the QEMU simulator. The results show that the proposed\ndesign not only ensures reproducibility but also can be extended to advanced\napplications such as continuous deployment of real-time Linux versions. Future\nrecommendations include expanding automated tests, implementing system\nmonitoring with Prometheus and Grafana, using distributed builds, optimizing\nwith Docker multi-stage builds, and enabling continuous deployment of real-time\nLinux changes to provide a stable and scalable model for industrial and\nresearch projects in embedded systems with a rapid and reliable development\ncycle."
                },
                "authors": [
                    {
                        "name": "Behnam Agahi"
                    },
                    {
                        "name": "Hamed Farbeh"
                    }
                ],
                "author_detail": {
                    "name": "Hamed Farbeh"
                },
                "author": "Hamed Farbeh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19240v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19240v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15745v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15745v2",
                "updated": "2025-10-24T05:39:03Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    5,
                    39,
                    3,
                    4,
                    297,
                    0
                ],
                "published": "2025-06-18T02:22:14Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    2,
                    22,
                    14,
                    2,
                    169,
                    0
                ],
                "title": "InfiniPot-V: Memory-Constrained KV Cache Compression for Streaming Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InfiniPot-V: Memory-Constrained KV Cache Compression for Streaming Video\n  Understanding"
                },
                "summary": "Modern multimodal large language models (MLLMs) can reason over hour-long\nvideo, yet their key-value (KV) cache grows linearly with time-quickly\nexceeding the fixed memory of phones, AR glasses, and edge robots. Prior\ncompression schemes either assume the whole video and user query are available\noffline or must first build the full cache, so memory still scales with stream\nlength. InfiniPot-V is the first training-free, query-agnostic framework that\nenforces a hard, length-independent memory cap for streaming video\nunderstanding. During video encoding it monitors the cache and, once a user-set\nthreshold is reached, runs a lightweight compression pass that (i) removes\ntemporally redundant tokens via Temporal-axis Redundancy (TaR) metric and (ii)\nkeeps semantically significant tokens via Value-Norm (VaN) ranking. Across four\nopen-source MLLMs and four long-video and streaming-video benchmarks,\nInfiniPot-V cuts peak GPU memory by up to 94%, sustains real-time generation,\nand matches or surpasses full-cache accuracy-even in multi-turn dialogues. By\ndissolving the KV cache bottleneck without retraining or query knowledge,\nInfiniPot-V closes the gap for on-device streaming video assistants.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern multimodal large language models (MLLMs) can reason over hour-long\nvideo, yet their key-value (KV) cache grows linearly with time-quickly\nexceeding the fixed memory of phones, AR glasses, and edge robots. Prior\ncompression schemes either assume the whole video and user query are available\noffline or must first build the full cache, so memory still scales with stream\nlength. InfiniPot-V is the first training-free, query-agnostic framework that\nenforces a hard, length-independent memory cap for streaming video\nunderstanding. During video encoding it monitors the cache and, once a user-set\nthreshold is reached, runs a lightweight compression pass that (i) removes\ntemporally redundant tokens via Temporal-axis Redundancy (TaR) metric and (ii)\nkeeps semantically significant tokens via Value-Norm (VaN) ranking. Across four\nopen-source MLLMs and four long-video and streaming-video benchmarks,\nInfiniPot-V cuts peak GPU memory by up to 94%, sustains real-time generation,\nand matches or surpasses full-cache accuracy-even in multi-turn dialogues. By\ndissolving the KV cache bottleneck without retraining or query knowledge,\nInfiniPot-V closes the gap for on-device streaming video assistants."
                },
                "authors": [
                    {
                        "name": "Minsoo Kim"
                    },
                    {
                        "name": "Kyuhong Shim"
                    },
                    {
                        "name": "Jungwook Choi"
                    },
                    {
                        "name": "Simyung Chang"
                    }
                ],
                "author_detail": {
                    "name": "Simyung Chang"
                },
                "author": "Simyung Chang",
                "arxiv_comment": "NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15745v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15745v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13866v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13866v2",
                "updated": "2025-10-24T04:48:06Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    4,
                    48,
                    6,
                    4,
                    297,
                    0
                ],
                "published": "2025-05-20T03:21:52Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    3,
                    21,
                    52,
                    1,
                    140,
                    0
                ],
                "title": "Reasoning Path Compression: Compressing Generation Trajectories for\n  Efficient LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning Path Compression: Compressing Generation Trajectories for\n  Efficient LLM Reasoning"
                },
                "summary": "Recent reasoning-focused language models achieve high accuracy by generating\nlengthy intermediate reasoning paths before producing final answers. While this\napproach is effective in solving problems that require logical thinking, long\nreasoning paths significantly increase memory usage and reduce throughput of\ntoken generation, limiting the practical deployment of such models. We propose\nReasoning Path Compression (RPC), a training-free method that accelerates\ninference by leveraging the semantic sparsity of reasoning paths. RPC\nperiodically compresses the KV cache by retaining cache entries that receive\nhigh importance score, which are computed using a selector window composed of\nrecently generated queries. Experiments show that RPC improves generation\nthroughput of QwQ-32B by up to 1.60$\\times$ compared to the inference with full\nKV cache, with an accuracy drop of 1.2\\% on the AIME 2024 benchmark. Our\nfindings demonstrate that semantic sparsity in reasoning traces can be\neffectively exploited for compression, offering a practical path toward\nefficient deployment of reasoning LLMs. Our code is available at\nhttps://github.com/jiwonsong-dev/ReasoningPathCompression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent reasoning-focused language models achieve high accuracy by generating\nlengthy intermediate reasoning paths before producing final answers. While this\napproach is effective in solving problems that require logical thinking, long\nreasoning paths significantly increase memory usage and reduce throughput of\ntoken generation, limiting the practical deployment of such models. We propose\nReasoning Path Compression (RPC), a training-free method that accelerates\ninference by leveraging the semantic sparsity of reasoning paths. RPC\nperiodically compresses the KV cache by retaining cache entries that receive\nhigh importance score, which are computed using a selector window composed of\nrecently generated queries. Experiments show that RPC improves generation\nthroughput of QwQ-32B by up to 1.60$\\times$ compared to the inference with full\nKV cache, with an accuracy drop of 1.2\\% on the AIME 2024 benchmark. Our\nfindings demonstrate that semantic sparsity in reasoning traces can be\neffectively exploited for compression, offering a practical path toward\nefficient deployment of reasoning LLMs. Our code is available at\nhttps://github.com/jiwonsong-dev/ReasoningPathCompression."
                },
                "authors": [
                    {
                        "name": "Jiwon Song"
                    },
                    {
                        "name": "Dongwon Jo"
                    },
                    {
                        "name": "Yulhwa Kim"
                    },
                    {
                        "name": "Jae-Joon Kim"
                    }
                ],
                "author_detail": {
                    "name": "Jae-Joon Kim"
                },
                "author": "Jae-Joon Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13866v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13866v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06425v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06425v5",
                "updated": "2025-10-23T23:35:32Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    23,
                    35,
                    32,
                    3,
                    296,
                    0
                ],
                "published": "2025-01-11T03:37:10Z",
                "published_parsed": [
                    2025,
                    1,
                    11,
                    3,
                    37,
                    10,
                    5,
                    11,
                    0
                ],
                "title": "Tensor Product Attention Is All You Need",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tensor Product Attention Is All You Need"
                },
                "summary": "Scaling language models to handle longer input sequences typically\nnecessitates large key-value (KV) caches, resulting in substantial memory\noverhead during inference. In this paper, we propose Tensor Product Attention\n(TPA), a novel attention mechanism that uses tensor decompositions to represent\nqueries, keys, and values compactly, substantially shrinking the KV cache size\nat inference time. By factorizing these representations into contextual\nlow-rank components and seamlessly integrating with Rotary Position Embedding\n(RoPE), TPA achieves improved model quality alongside memory efficiency. Based\non TPA, we introduce the Tensor ProducT ATTenTion Transformer (T6), a new model\narchitecture for sequence modeling. Through extensive empirical evaluation on\nlanguage modeling tasks, we demonstrate that T6 surpasses or matches the\nperformance of standard Transformer baselines including Multi-Head Attention\n(MHA), Multi-Query Attention (MQA), Grouped-Query Attention (GQA), and\nMulti-Head Latent Attention (MLA) across various metrics, including perplexity\nand a range of established evaluation benchmarks. Notably, TPA's memory\nefficiency and computational efficiency at decoding stage enables processing\nlonger sequences under fixed resource constraints, addressing a critical\nscalability challenge in modern language models. Project Page:\nhttps://github.com/tensorgi/TPA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling language models to handle longer input sequences typically\nnecessitates large key-value (KV) caches, resulting in substantial memory\noverhead during inference. In this paper, we propose Tensor Product Attention\n(TPA), a novel attention mechanism that uses tensor decompositions to represent\nqueries, keys, and values compactly, substantially shrinking the KV cache size\nat inference time. By factorizing these representations into contextual\nlow-rank components and seamlessly integrating with Rotary Position Embedding\n(RoPE), TPA achieves improved model quality alongside memory efficiency. Based\non TPA, we introduce the Tensor ProducT ATTenTion Transformer (T6), a new model\narchitecture for sequence modeling. Through extensive empirical evaluation on\nlanguage modeling tasks, we demonstrate that T6 surpasses or matches the\nperformance of standard Transformer baselines including Multi-Head Attention\n(MHA), Multi-Query Attention (MQA), Grouped-Query Attention (GQA), and\nMulti-Head Latent Attention (MLA) across various metrics, including perplexity\nand a range of established evaluation benchmarks. Notably, TPA's memory\nefficiency and computational efficiency at decoding stage enables processing\nlonger sequences under fixed resource constraints, addressing a critical\nscalability challenge in modern language models. Project Page:\nhttps://github.com/tensorgi/TPA."
                },
                "authors": [
                    {
                        "name": "Yifan Zhang"
                    },
                    {
                        "name": "Yifeng Liu"
                    },
                    {
                        "name": "Huizhuo Yuan"
                    },
                    {
                        "name": "Zhen Qin"
                    },
                    {
                        "name": "Yang Yuan"
                    },
                    {
                        "name": "Quanquan Gu"
                    },
                    {
                        "name": "Andrew Chi-Chih Yao"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Chi-Chih Yao"
                },
                "author": "Andrew Chi-Chih Yao",
                "arxiv_comment": "Published in NeurIPS 2025 (Spotlight); Project Page:\n  https://github.com/tensorgi/TPA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06425v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06425v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16986v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16986v2",
                "updated": "2025-10-23T21:31:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    21,
                    31,
                    35,
                    3,
                    296,
                    0
                ],
                "published": "2025-05-22T17:54:32Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    54,
                    32,
                    3,
                    142,
                    0
                ],
                "title": "T1: A Tool-Oriented Conversational Dataset for Multi-Turn Agentic\n  Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "T1: A Tool-Oriented Conversational Dataset for Multi-Turn Agentic\n  Planning"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities as\nintelligent agents capable of solving complex problems. However, effective\nplanning in scenarios involving dependencies between API or tool\ncalls-particularly in multi-turn conversations-remains a significant challenge.\nTo address this, we introduce T1, a tool-augmented, multi-domain, multi-turn\nconversational dataset specifically designed to capture and manage inter-tool\ndependencies across diverse domains. T1 enables rigorous evaluation of agents'\nability to coordinate tool use across nine distinct domains (4 single domain\nand 5 multi-domain) with the help of an integrated caching mechanism for both\nshort- and long-term memory, while supporting dynamic replanning-such as\ndeciding whether to recompute or reuse cached results. Beyond facilitating\nresearch on tool use and planning, T1 also serves as a benchmark for evaluating\nthe performance of open-weight and proprietary large language models. We\npresent results powered by T1-Agent, highlighting their ability to plan and\nreason in complex, tool-dependent scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive capabilities as\nintelligent agents capable of solving complex problems. However, effective\nplanning in scenarios involving dependencies between API or tool\ncalls-particularly in multi-turn conversations-remains a significant challenge.\nTo address this, we introduce T1, a tool-augmented, multi-domain, multi-turn\nconversational dataset specifically designed to capture and manage inter-tool\ndependencies across diverse domains. T1 enables rigorous evaluation of agents'\nability to coordinate tool use across nine distinct domains (4 single domain\nand 5 multi-domain) with the help of an integrated caching mechanism for both\nshort- and long-term memory, while supporting dynamic replanning-such as\ndeciding whether to recompute or reuse cached results. Beyond facilitating\nresearch on tool use and planning, T1 also serves as a benchmark for evaluating\nthe performance of open-weight and proprietary large language models. We\npresent results powered by T1-Agent, highlighting their ability to plan and\nreason in complex, tool-dependent scenarios."
                },
                "authors": [
                    {
                        "name": "Amartya Chakraborty"
                    },
                    {
                        "name": "Paresh Dashore"
                    },
                    {
                        "name": "Nadia Bathaee"
                    },
                    {
                        "name": "Anmol Jain"
                    },
                    {
                        "name": "Anirban Das"
                    },
                    {
                        "name": "Shi-Xiong Zhang"
                    },
                    {
                        "name": "Sambit Sahu"
                    },
                    {
                        "name": "Milind Naphade"
                    },
                    {
                        "name": "Genta Indra Winata"
                    }
                ],
                "author_detail": {
                    "name": "Genta Indra Winata"
                },
                "author": "Genta Indra Winata",
                "arxiv_comment": "Accepted by NeurIPS 2025 Datasets and Benchmarks Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16986v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16986v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.00566v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.00566v3",
                "updated": "2025-10-23T19:45:39Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    19,
                    45,
                    39,
                    3,
                    296,
                    0
                ],
                "published": "2025-10-01T06:38:45Z",
                "published_parsed": [
                    2025,
                    10,
                    1,
                    6,
                    38,
                    45,
                    2,
                    274,
                    0
                ],
                "title": "Panorama: Fast-Track Nearest Neighbors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Panorama: Fast-Track Nearest Neighbors"
                },
                "summary": "Approximate Nearest-Neighbor Search (ANNS) efficiently finds data items whose\nembeddings are close to that of a given query in a high-dimensional space,\naiming to balance accuracy with speed. Used in recommendation systems, image\nand video retrieval, natural language processing, and retrieval-augmented\ngeneration (RAG), ANNS algorithms such as IVFPQ, HNSW graphs, Annoy, and MRPT\nutilize graph, tree, clustering, and quantization techniques to navigate large\nvector spaces. Despite this progress, ANNS systems spend up to 99% of query\ntime to compute distances in their final refinement phase. In this paper, we\npresent PANORAMA, a machine learning-driven approach that tackles the ANNS\nverification bottleneck through data-adaptive learned orthogonal transforms\nthat facilitate the accretive refinement of distance bounds. Such transforms\ncompact over 90% of signal energy into the first half of dimensions, enabling\nearly candidate pruning with partial distance computations. We integrate\nPANORAMA into state-of-the-art ANNS methods, namely IVFPQ/Flat, HNSW, MRPT, and\nAnnoy, without index modification, using level-major memory layouts,\nSIMD-vectorized partial distance computations, and cache-aware access patterns.\nExperiments across diverse datasets -- from image-based CIFAR-10 and GIST to\nmodern embedding spaces including OpenAI's Ada 2 and Large 3 -- demonstrate\nthat PANORAMA affords a 2--30$\\times$ end-to-end speedup with no recall loss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Approximate Nearest-Neighbor Search (ANNS) efficiently finds data items whose\nembeddings are close to that of a given query in a high-dimensional space,\naiming to balance accuracy with speed. Used in recommendation systems, image\nand video retrieval, natural language processing, and retrieval-augmented\ngeneration (RAG), ANNS algorithms such as IVFPQ, HNSW graphs, Annoy, and MRPT\nutilize graph, tree, clustering, and quantization techniques to navigate large\nvector spaces. Despite this progress, ANNS systems spend up to 99% of query\ntime to compute distances in their final refinement phase. In this paper, we\npresent PANORAMA, a machine learning-driven approach that tackles the ANNS\nverification bottleneck through data-adaptive learned orthogonal transforms\nthat facilitate the accretive refinement of distance bounds. Such transforms\ncompact over 90% of signal energy into the first half of dimensions, enabling\nearly candidate pruning with partial distance computations. We integrate\nPANORAMA into state-of-the-art ANNS methods, namely IVFPQ/Flat, HNSW, MRPT, and\nAnnoy, without index modification, using level-major memory layouts,\nSIMD-vectorized partial distance computations, and cache-aware access patterns.\nExperiments across diverse datasets -- from image-based CIFAR-10 and GIST to\nmodern embedding spaces including OpenAI's Ada 2 and Large 3 -- demonstrate\nthat PANORAMA affords a 2--30$\\times$ end-to-end speedup with no recall loss."
                },
                "authors": [
                    {
                        "name": "Vansh Ramani"
                    },
                    {
                        "name": "Alexis Schlomer"
                    },
                    {
                        "name": "Akash Nayar"
                    },
                    {
                        "name": "Sayan Ranu"
                    },
                    {
                        "name": "Jignesh M. Patel"
                    },
                    {
                        "name": "Panagiotis Karras"
                    }
                ],
                "author_detail": {
                    "name": "Panagiotis Karras"
                },
                "author": "Panagiotis Karras",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.00566v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.00566v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.07254v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.07254v3",
                "updated": "2025-10-23T18:52:25Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    18,
                    52,
                    25,
                    3,
                    296,
                    0
                ],
                "published": "2025-06-08T18:43:31Z",
                "published_parsed": [
                    2025,
                    6,
                    8,
                    18,
                    43,
                    31,
                    6,
                    159,
                    0
                ],
                "title": "A Stable Whitening Optimizer for Efficient Neural Network Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Stable Whitening Optimizer for Efficient Neural Network Training"
                },
                "summary": "In this work, we take an experimentally grounded look at neural network\noptimization. Building on the Shampoo family of algorithms, we identify and\nalleviate three key issues, resulting in the proposed SPlus method. First, we\nfind that naive Shampoo is prone to divergence when matrix-inverses are cached\nfor long periods. We introduce an alternate bounded update combining a\nhistorical eigenbasis with instantaneous normalization, resulting in\nacross-the-board stability and significantly lower computational requirements.\nSecond, we adapt a shape-aware scaling to enable learning rate transfer across\nnetwork width. Third, we find that high learning rates result in large\nparameter noise, and propose a simple iterate-averaging scheme which unblocks\nfaster learning. To properly confirm these findings, we introduce a pointed\nTransformer training benchmark, considering three objectives (language\nmodelling, image classification, and diffusion modelling) across different\nstages of training. On average, SPlus is able to reach the validation\nperformance of Adam within 44-58% of the gradient steps and 62-83% of the\nwallclock time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we take an experimentally grounded look at neural network\noptimization. Building on the Shampoo family of algorithms, we identify and\nalleviate three key issues, resulting in the proposed SPlus method. First, we\nfind that naive Shampoo is prone to divergence when matrix-inverses are cached\nfor long periods. We introduce an alternate bounded update combining a\nhistorical eigenbasis with instantaneous normalization, resulting in\nacross-the-board stability and significantly lower computational requirements.\nSecond, we adapt a shape-aware scaling to enable learning rate transfer across\nnetwork width. Third, we find that high learning rates result in large\nparameter noise, and propose a simple iterate-averaging scheme which unblocks\nfaster learning. To properly confirm these findings, we introduce a pointed\nTransformer training benchmark, considering three objectives (language\nmodelling, image classification, and diffusion modelling) across different\nstages of training. On average, SPlus is able to reach the validation\nperformance of Adam within 44-58% of the gradient steps and 62-83% of the\nwallclock time."
                },
                "authors": [
                    {
                        "name": "Kevin Frans"
                    },
                    {
                        "name": "Sergey Levine"
                    },
                    {
                        "name": "Pieter Abbeel"
                    }
                ],
                "author_detail": {
                    "name": "Pieter Abbeel"
                },
                "author": "Pieter Abbeel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.07254v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.07254v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.20707v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.20707v1",
                "updated": "2025-10-23T16:17:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    16,
                    17,
                    47,
                    3,
                    296,
                    0
                ],
                "published": "2025-10-23T16:17:47Z",
                "published_parsed": [
                    2025,
                    10,
                    23,
                    16,
                    17,
                    47,
                    3,
                    296,
                    0
                ],
                "title": "Mixing Importance with Diversity: Joint Optimization for KV Cache\n  Compression in Large Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixing Importance with Diversity: Joint Optimization for KV Cache\n  Compression in Large Vision-Language Models"
                },
                "summary": "Recent large vision-language models (LVLMs) demonstrate remarkable\ncapabilities in processing extended multi-modal sequences, yet the resulting\nkey-value (KV) cache expansion creates a critical memory bottleneck that\nfundamentally limits deployment scalability. While existing KV cache\ncompression methods focus on retaining high-importance KV pairs to minimize\nstorage, they often overlook the modality-specific semantic redundancy patterns\nthat emerge distinctively in multi-modal KV caches. In this work, we first\nanalyze how, beyond simple importance, the KV cache in LVLMs exhibits varying\nlevels of redundancy across attention heads. We show that relying solely on\nimportance can only cover a subset of the full KV cache information\ndistribution, leading to potential loss of semantic coverage. To address this,\nwe propose \\texttt{MixKV}, a novel method that mixes importance with diversity\nfor optimized KV cache compression in LVLMs. \\texttt{MixKV} adapts to head-wise\nsemantic redundancy, selectively balancing diversity and importance when\ncompressing KV pairs. Extensive experiments demonstrate that \\texttt{MixKV}\nconsistently enhances existing methods across multiple LVLMs. Under extreme\ncompression (budget=64), \\texttt{MixKV} improves baseline methods by an average\nof \\textbf{5.1\\%} across five multi-modal understanding benchmarks and achieves\nremarkable gains of \\textbf{8.0\\%} and \\textbf{9.0\\%} for SnapKV and AdaKV on\nGUI grounding tasks, all while maintaining comparable inference efficiency.\nFurthermore, \\texttt{MixKV} extends seamlessly to LLMs with comparable\nperformance gains. Our code is available at\n\\href{https://github.com/xuyang-liu16/MixKV}{\\textcolor{citeblue}{https://github.com/xuyang-liu16/MixKV}}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large vision-language models (LVLMs) demonstrate remarkable\ncapabilities in processing extended multi-modal sequences, yet the resulting\nkey-value (KV) cache expansion creates a critical memory bottleneck that\nfundamentally limits deployment scalability. While existing KV cache\ncompression methods focus on retaining high-importance KV pairs to minimize\nstorage, they often overlook the modality-specific semantic redundancy patterns\nthat emerge distinctively in multi-modal KV caches. In this work, we first\nanalyze how, beyond simple importance, the KV cache in LVLMs exhibits varying\nlevels of redundancy across attention heads. We show that relying solely on\nimportance can only cover a subset of the full KV cache information\ndistribution, leading to potential loss of semantic coverage. To address this,\nwe propose \\texttt{MixKV}, a novel method that mixes importance with diversity\nfor optimized KV cache compression in LVLMs. \\texttt{MixKV} adapts to head-wise\nsemantic redundancy, selectively balancing diversity and importance when\ncompressing KV pairs. Extensive experiments demonstrate that \\texttt{MixKV}\nconsistently enhances existing methods across multiple LVLMs. Under extreme\ncompression (budget=64), \\texttt{MixKV} improves baseline methods by an average\nof \\textbf{5.1\\%} across five multi-modal understanding benchmarks and achieves\nremarkable gains of \\textbf{8.0\\%} and \\textbf{9.0\\%} for SnapKV and AdaKV on\nGUI grounding tasks, all while maintaining comparable inference efficiency.\nFurthermore, \\texttt{MixKV} extends seamlessly to LLMs with comparable\nperformance gains. Our code is available at\n\\href{https://github.com/xuyang-liu16/MixKV}{\\textcolor{citeblue}{https://github.com/xuyang-liu16/MixKV}}."
                },
                "authors": [
                    {
                        "name": "Xuyang Liu"
                    },
                    {
                        "name": "Xiyan Gui"
                    },
                    {
                        "name": "Yuchao Zhang"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "Our code is available at https://github.com/xuyang-liu16/MixKV",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.20707v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.20707v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16407v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16407v2",
                "updated": "2025-10-23T15:26:38Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    15,
                    26,
                    38,
                    3,
                    296,
                    0
                ],
                "published": "2025-09-19T20:31:38Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    20,
                    31,
                    38,
                    4,
                    262,
                    0
                ],
                "title": "WarpSpeed: A High-Performance Library for Concurrent GPU Hash Tables",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WarpSpeed: A High-Performance Library for Concurrent GPU Hash Tables"
                },
                "summary": "GPU hash tables are increasingly used to accelerate data processing, but\ntheir limited functionality restricts adoption in large-scale data processing\napplications. Current limitations include incomplete concurrency support and\nmissing compound operations such as upserts.\n  This paper presents WarpSpeed, a library of high-performance concurrent GPU\nhash tables with a unified benchmarking framework for performance analysis.\nWarpSpeed implements eight state-of-the-art Nvidia GPU hash table designs and\nprovides a rich API designed for modern GPU applications. Our evaluation uses\ndiverse benchmarks to assess both correctness and scalability, and we\ndemonstrate real-world impact by integrating these hash tables into three\ndownstream applications.\n  We propose several optimization techniques to reduce concurrency overhead,\nincluding fingerprint-based metadata to minimize cache line probes and\nspecialized Nvidia GPU instructions for lock-free queries. Our findings provide\nnew insights into concurrent GPU hash table design and offer practical guidance\nfor developing efficient, scalable data structures on modern GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPU hash tables are increasingly used to accelerate data processing, but\ntheir limited functionality restricts adoption in large-scale data processing\napplications. Current limitations include incomplete concurrency support and\nmissing compound operations such as upserts.\n  This paper presents WarpSpeed, a library of high-performance concurrent GPU\nhash tables with a unified benchmarking framework for performance analysis.\nWarpSpeed implements eight state-of-the-art Nvidia GPU hash table designs and\nprovides a rich API designed for modern GPU applications. Our evaluation uses\ndiverse benchmarks to assess both correctness and scalability, and we\ndemonstrate real-world impact by integrating these hash tables into three\ndownstream applications.\n  We propose several optimization techniques to reduce concurrency overhead,\nincluding fingerprint-based metadata to minimize cache line probes and\nspecialized Nvidia GPU instructions for lock-free queries. Our findings provide\nnew insights into concurrent GPU hash table design and offer practical guidance\nfor developing efficient, scalable data structures on modern GPUs."
                },
                "authors": [
                    {
                        "name": "Hunter McCoy"
                    },
                    {
                        "name": "Prashant Pandey"
                    }
                ],
                "author_detail": {
                    "name": "Prashant Pandey"
                },
                "author": "Prashant Pandey",
                "arxiv_comment": "Accepted to ALENEX`26",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16407v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16407v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13251v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13251v4",
                "updated": "2025-10-23T14:23:24Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    14,
                    23,
                    24,
                    3,
                    296,
                    0
                ],
                "published": "2025-02-18T19:22:44Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    19,
                    22,
                    44,
                    1,
                    49,
                    0
                ],
                "title": "Neural Attention Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Attention Search"
                },
                "summary": "We present Neural Attention Search (NAtS), a framework that automatically\nevaluates the importance of each token within a sequence and determines if the\ncorresponding token can be dropped after several steps. This approach can\nefficiently reduce the KV cache sizes required by transformer-based models\nduring inference and thus reduce inference costs. In this paper, we design a\nsearch space that contains three token types: (i) Global Tokens will be\npreserved and queried by all the following tokens. (ii) Local Tokens survive\nuntil the next global token appears. (iii) Sliding Window Tokens have an impact\non the inference of a fixed size of the next following tokens. Similar to the\nOne-Shot Neural Architecture Search approach, this token-type information can\nbe learned jointly with the architecture weights via a learnable attention\nmask. Experiments on both training a new transformer from scratch and\nfine-tuning existing large language models show that NAtS can efficiently\nreduce the KV cache size required for the models while maintaining the models'\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Neural Attention Search (NAtS), a framework that automatically\nevaluates the importance of each token within a sequence and determines if the\ncorresponding token can be dropped after several steps. This approach can\nefficiently reduce the KV cache sizes required by transformer-based models\nduring inference and thus reduce inference costs. In this paper, we design a\nsearch space that contains three token types: (i) Global Tokens will be\npreserved and queried by all the following tokens. (ii) Local Tokens survive\nuntil the next global token appears. (iii) Sliding Window Tokens have an impact\non the inference of a fixed size of the next following tokens. Similar to the\nOne-Shot Neural Architecture Search approach, this token-type information can\nbe learned jointly with the architecture weights via a learnable attention\nmask. Experiments on both training a new transformer from scratch and\nfine-tuning existing large language models show that NAtS can efficiently\nreduce the KV cache size required for the models while maintaining the models'\nperformance."
                },
                "authors": [
                    {
                        "name": "Difan Deng"
                    },
                    {
                        "name": "Marius Lindauer"
                    }
                ],
                "author_detail": {
                    "name": "Marius Lindauer"
                },
                "author": "Marius Lindauer",
                "arxiv_comment": "35 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13251v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13251v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.20878v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.20878v1",
                "updated": "2025-10-23T12:28:58Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    12,
                    28,
                    58,
                    3,
                    296,
                    0
                ],
                "published": "2025-10-23T12:28:58Z",
                "published_parsed": [
                    2025,
                    10,
                    23,
                    12,
                    28,
                    58,
                    3,
                    296,
                    0
                ],
                "title": "HA-RAG: Hotness-Aware RAG Acceleration via Mixed Precision and Data\n  Placement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HA-RAG: Hotness-Aware RAG Acceleration via Mixed Precision and Data\n  Placement"
                },
                "summary": "Retrieval-Augmented Generation (RAG) improves model output accuracy by\nleveraging external knowledge bases, serving as an effective solution to\naddress hallucination issues and knowledge-update delays in Large Language\nModels (LLMs). However, the introduction of external knowledge bases presents\nRAG with challenges in long-context processing, significantly increasing memory\nconsumption and inference latency. Existing research accelerates inference by\nprecomputing Key and Value (KV) of the knowledge base and loading them\non-demand during inference. Based on the access frequency of different KV\nchunks within the external knowledge base, this paper proposes a hotness-aware\nRAG (HA-RAG) inference optimization system. First, leveraging the numerical\ndistribution of KV chunks, we introduce a hotness-aware mixed-precision\ncompressing and loading method to reduce disk I/O and memory access overhead.\nSecond, we design a hotness-aware data placement strategy that prioritizes\nstoring frequently accessed KV chunks in high-speed memory to improve data\naccess efficiency. Experimental results demonstrate that, compared with\nTurboRAG, the proposed HA-RAG achieves an average speedup of 2.10x and maximum\nspeedup of 10.49x in Time-To-First-Token (TTFT) with negligible accuracy loss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) improves model output accuracy by\nleveraging external knowledge bases, serving as an effective solution to\naddress hallucination issues and knowledge-update delays in Large Language\nModels (LLMs). However, the introduction of external knowledge bases presents\nRAG with challenges in long-context processing, significantly increasing memory\nconsumption and inference latency. Existing research accelerates inference by\nprecomputing Key and Value (KV) of the knowledge base and loading them\non-demand during inference. Based on the access frequency of different KV\nchunks within the external knowledge base, this paper proposes a hotness-aware\nRAG (HA-RAG) inference optimization system. First, leveraging the numerical\ndistribution of KV chunks, we introduce a hotness-aware mixed-precision\ncompressing and loading method to reduce disk I/O and memory access overhead.\nSecond, we design a hotness-aware data placement strategy that prioritizes\nstoring frequently accessed KV chunks in high-speed memory to improve data\naccess efficiency. Experimental results demonstrate that, compared with\nTurboRAG, the proposed HA-RAG achieves an average speedup of 2.10x and maximum\nspeedup of 10.49x in Time-To-First-Token (TTFT) with negligible accuracy loss."
                },
                "authors": [
                    {
                        "name": "Danying Ge"
                    },
                    {
                        "name": "Jianhua Gao"
                    },
                    {
                        "name": "Yixue Yang"
                    },
                    {
                        "name": "Weixing Ji"
                    }
                ],
                "author_detail": {
                    "name": "Weixing Ji"
                },
                "author": "Weixing Ji",
                "arxiv_comment": "13 pages,16 figures,2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.20878v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.20878v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.4; E.4; I.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21865v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21865v1",
                "updated": "2025-10-23T10:35:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    10,
                    35,
                    35,
                    3,
                    296,
                    0
                ],
                "published": "2025-10-23T10:35:35Z",
                "published_parsed": [
                    2025,
                    10,
                    23,
                    10,
                    35,
                    35,
                    3,
                    296,
                    0
                ],
                "title": "Prefetching Cache Optimization Using Graph Neural Networks: A Modular\n  Framework and Conceptual Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prefetching Cache Optimization Using Graph Neural Networks: A Modular\n  Framework and Conceptual Analysis"
                },
                "summary": "Caching and prefetching techniques are fundamental to modern computing,\nserving to bridge the growing performance gap between processors and memory.\nTraditional prefetching strategies are often limited by their reliance on\npredefined heuristics or simplified statistical models, which fail to capture\nthe complex, non-linear dependencies in modern data access patterns. This paper\nintroduces a modular framework leveraging Graph Neural Networks (GNNs) to model\nand predict access patterns within graph-structured data, focusing on web\nnavigation and hierarchical file systems. The toolchain consists of: a route\nmapper for extracting structural information, a graph constructor for creating\ngraph representations, a walk session generator for simulating user behaviors,\nand a gnn prefetch module for training and inference. We provide a detailed\nconceptual analysis showing how GNN-based approaches can outperform\nconventional methods by learning intricate dependencies. This work offers both\ntheoretical foundations and a practical, replicable pipeline for future\nresearch in graph-driven systems optimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching and prefetching techniques are fundamental to modern computing,\nserving to bridge the growing performance gap between processors and memory.\nTraditional prefetching strategies are often limited by their reliance on\npredefined heuristics or simplified statistical models, which fail to capture\nthe complex, non-linear dependencies in modern data access patterns. This paper\nintroduces a modular framework leveraging Graph Neural Networks (GNNs) to model\nand predict access patterns within graph-structured data, focusing on web\nnavigation and hierarchical file systems. The toolchain consists of: a route\nmapper for extracting structural information, a graph constructor for creating\ngraph representations, a walk session generator for simulating user behaviors,\nand a gnn prefetch module for training and inference. We provide a detailed\nconceptual analysis showing how GNN-based approaches can outperform\nconventional methods by learning intricate dependencies. This work offers both\ntheoretical foundations and a practical, replicable pipeline for future\nresearch in graph-driven systems optimization."
                },
                "authors": [
                    {
                        "name": "F. I. Qowy"
                    }
                ],
                "author_detail": {
                    "name": "F. I. Qowy"
                },
                "author": "F. I. Qowy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21865v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21865v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.20400v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.20400v1",
                "updated": "2025-10-23T10:06:48Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    10,
                    6,
                    48,
                    3,
                    296,
                    0
                ],
                "published": "2025-10-23T10:06:48Z",
                "published_parsed": [
                    2025,
                    10,
                    23,
                    10,
                    6,
                    48,
                    3,
                    296,
                    0
                ],
                "title": "Squire: A General-Purpose Accelerator to Exploit Fine-Grain Parallelism\n  on Dependency-Bound Kernels",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Squire: A General-Purpose Accelerator to Exploit Fine-Grain Parallelism\n  on Dependency-Bound Kernels"
                },
                "summary": "Multiple HPC applications are often bottlenecked by compute-intensive kernels\nimplementing complex dependency patterns (data-dependency bound). Traditional\ngeneral-purpose accelerators struggle to effectively exploit fine-grain\nparallelism due to limitations in implementing convoluted data-dependency\npatterns (like SIMD) and overheads due to synchronization and data transfers\n(like GPGPUs). In contrast, custom FPGA and ASIC designs offer improved\nperformance and energy efficiency at a high cost in hardware design and\nprogramming complexity and often lack the flexibility to process different\nworkloads. We propose Squire, a general-purpose accelerator designed to exploit\nfine-grain parallelism effectively on dependency-bound kernels. Each Squire\naccelerator has a set of general-purpose low-power in-order cores that can\nrapidly communicate among themselves and directly access data from the L2\ncache. Our proposal integrates one Squire accelerator per core in a typical\nmulticore system, allowing the acceleration of dependency-bound kernels within\nparallel tasks with minimal software changes. As a case study, we evaluate\nSquire's effectiveness by accelerating five kernels that implement complex\ndependency patterns. We use three of these kernels to build an end-to-end\nread-mapping tool that will be used to evaluate Squire. Squire obtains speedups\nup to 7.64$\\times$ in dynamic programming kernels. Overall, Squire provides an\nacceleration for an end-to-end application of 3.66$\\times$. In addition, Squire\nreduces energy consumption by up to 56% with a minimal area overhead of 10.5%\ncompared to a Neoverse-N1 baseline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multiple HPC applications are often bottlenecked by compute-intensive kernels\nimplementing complex dependency patterns (data-dependency bound). Traditional\ngeneral-purpose accelerators struggle to effectively exploit fine-grain\nparallelism due to limitations in implementing convoluted data-dependency\npatterns (like SIMD) and overheads due to synchronization and data transfers\n(like GPGPUs). In contrast, custom FPGA and ASIC designs offer improved\nperformance and energy efficiency at a high cost in hardware design and\nprogramming complexity and often lack the flexibility to process different\nworkloads. We propose Squire, a general-purpose accelerator designed to exploit\nfine-grain parallelism effectively on dependency-bound kernels. Each Squire\naccelerator has a set of general-purpose low-power in-order cores that can\nrapidly communicate among themselves and directly access data from the L2\ncache. Our proposal integrates one Squire accelerator per core in a typical\nmulticore system, allowing the acceleration of dependency-bound kernels within\nparallel tasks with minimal software changes. As a case study, we evaluate\nSquire's effectiveness by accelerating five kernels that implement complex\ndependency patterns. We use three of these kernels to build an end-to-end\nread-mapping tool that will be used to evaluate Squire. Squire obtains speedups\nup to 7.64$\\times$ in dynamic programming kernels. Overall, Squire provides an\nacceleration for an end-to-end application of 3.66$\\times$. In addition, Squire\nreduces energy consumption by up to 56% with a minimal area overhead of 10.5%\ncompared to a Neoverse-N1 baseline."
                },
                "authors": [
                    {
                        "name": "Rubn Langarita"
                    },
                    {
                        "name": "Jess Alastruey-Bened"
                    },
                    {
                        "name": "Pablo Ibez-Marn"
                    },
                    {
                        "name": "Santiago Marco-Sola"
                    },
                    {
                        "name": "Miquel Moret"
                    },
                    {
                        "name": "Adri Armejach"
                    }
                ],
                "author_detail": {
                    "name": "Adri Armejach"
                },
                "author": "Adri Armejach",
                "arxiv_comment": "11 pages, 10 figures, 5 tables, 4 algorithms, accepted on PACT25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.20400v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.20400v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11843v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11843v2",
                "updated": "2025-10-23T09:55:50Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    9,
                    55,
                    50,
                    3,
                    296,
                    0
                ],
                "published": "2024-11-18T18:59:15Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    18,
                    59,
                    15,
                    0,
                    323,
                    0
                ],
                "title": "Bi-Mamba: Towards Accurate 1-Bit State Space Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bi-Mamba: Towards Accurate 1-Bit State Space Models"
                },
                "summary": "The typical Selective State-Space Model (SSM) used in Mamba addresses several\nlimitations of Transformers, such as the quadratic computational complexity\nwith respect to sequence length and the significant memory requirements during\ninference due to the key-value (KV) cache. However, the increasing size of\nMamba models continues to pose challenges for training and deployment,\nparticularly due to their substantial computational demands during both\ntraining and inference. In this work, we introduce $\\texttt{Bi-Mamba}$, a\nscalable and powerful 1-bit Mamba architecture designed to enable more\nefficient large language models (LLMs), with model sizes of 780M, 1.3B, and\n2.7B parameters. $\\texttt{Bi-Mamba}$ models are trained from scratch on a\nstandard LLM-scale dataset using an autoregressive distillation loss. Extensive\nexperiments on language modeling benchmarks demonstrate that\n$\\texttt{Bi-Mamba}$ achieves performance comparable to its full-precision (FP16\nor BF16) counterparts, while outperforming post-training binarization (PTB)\nMamba and binarization-aware training (BAT) Transformer baselines. Moreover,\n$\\texttt{Bi-Mamba}$ drastically reduces memory usage and computational cost\ncompared to the original Mamba. Our work pioneers a new line of\nlinear-complexity LLMs under low-bit representation and provides the way for\nthe design of specialized hardware optimized for efficient 1-bit Mamba-based\nmodels. Code and the pre-trained weights are available at\nhttps://github.com/Tangshengku/Bi-Mamba.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The typical Selective State-Space Model (SSM) used in Mamba addresses several\nlimitations of Transformers, such as the quadratic computational complexity\nwith respect to sequence length and the significant memory requirements during\ninference due to the key-value (KV) cache. However, the increasing size of\nMamba models continues to pose challenges for training and deployment,\nparticularly due to their substantial computational demands during both\ntraining and inference. In this work, we introduce $\\texttt{Bi-Mamba}$, a\nscalable and powerful 1-bit Mamba architecture designed to enable more\nefficient large language models (LLMs), with model sizes of 780M, 1.3B, and\n2.7B parameters. $\\texttt{Bi-Mamba}$ models are trained from scratch on a\nstandard LLM-scale dataset using an autoregressive distillation loss. Extensive\nexperiments on language modeling benchmarks demonstrate that\n$\\texttt{Bi-Mamba}$ achieves performance comparable to its full-precision (FP16\nor BF16) counterparts, while outperforming post-training binarization (PTB)\nMamba and binarization-aware training (BAT) Transformer baselines. Moreover,\n$\\texttt{Bi-Mamba}$ drastically reduces memory usage and computational cost\ncompared to the original Mamba. Our work pioneers a new line of\nlinear-complexity LLMs under low-bit representation and provides the way for\nthe design of specialized hardware optimized for efficient 1-bit Mamba-based\nmodels. Code and the pre-trained weights are available at\nhttps://github.com/Tangshengku/Bi-Mamba."
                },
                "authors": [
                    {
                        "name": "Shengkun Tang"
                    },
                    {
                        "name": "Liqun Ma"
                    },
                    {
                        "name": "Haonan Li"
                    },
                    {
                        "name": "Mingjie Sun"
                    },
                    {
                        "name": "Zhiqiang Shen"
                    }
                ],
                "author_detail": {
                    "name": "Zhiqiang Shen"
                },
                "author": "Zhiqiang Shen",
                "arxiv_comment": "Accepted in TMLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11843v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11843v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.16807v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.16807v2",
                "updated": "2025-10-23T08:29:11Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    8,
                    29,
                    11,
                    3,
                    296,
                    0
                ],
                "published": "2025-10-19T12:17:42Z",
                "published_parsed": [
                    2025,
                    10,
                    19,
                    12,
                    17,
                    42,
                    6,
                    292,
                    0
                ],
                "title": "Improving Model Representation and Reducing KV Cache via Skip\n  Connections with First Value Heads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Model Representation and Reducing KV Cache via Skip\n  Connections with First Value Heads"
                },
                "summary": "Transformer models have driven breakthroughs across various language tasks by\ntheir strong capability to learn rich contextual representations. Scaling them\nto improve representation, however, often demands substantial memory and\ncompute costs, such as the Key-Value (KV) cache used during auto-regressive\ndecoding. Skip connections offer a promising way to improve representation\nwithout bloating resource usage, yet most prior works either improve\nexpressivity while leaving KV costs unchanged, or reduce memory at the cost of\nweaker representation. In this work, we propose SkipV1Former, a Transformer\nvariant that uses skip connections from the first layer's Value heads to\nstrengthen model representation and reduce KV cache. Specifically, from the\nsecond block onward, each layer reuses half of its Value heads from the very\nfirst layer, while computing the other half as usual-cutting Value projections\nand V cache by nearly 50 \\%. Theoretically, we show that routing uncompressed\nfirst-layer Values into deeper layers restores information lost to compression\nand accelerates the model's implicit mesa-optimization-a key pattern of\nTransformer in auto-regressive tasks. Empirically, across different model\nscales, SkipV1Former delivers consistent reductions of approximately 25 \\% in\nKV cache while improving perplexity relative to standard Multi-Head Attention\n(MHA) Transformers and some advanced variants. Moreover, we propose a recipe\nfor uptraining existing MHA Transformer checkpoints to SkipV1Former with only\n10-15\\% additional compute. Finally, SkipV1Former can seamlessly combine\nadvanced methods like Group-Query Attention and Multi-Latent Attention to\nachieve further KV cache savings and performance improvement. When combined\nwith YOCO, it cuts KV cache size by nearly 50 \\% while still improving\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer models have driven breakthroughs across various language tasks by\ntheir strong capability to learn rich contextual representations. Scaling them\nto improve representation, however, often demands substantial memory and\ncompute costs, such as the Key-Value (KV) cache used during auto-regressive\ndecoding. Skip connections offer a promising way to improve representation\nwithout bloating resource usage, yet most prior works either improve\nexpressivity while leaving KV costs unchanged, or reduce memory at the cost of\nweaker representation. In this work, we propose SkipV1Former, a Transformer\nvariant that uses skip connections from the first layer's Value heads to\nstrengthen model representation and reduce KV cache. Specifically, from the\nsecond block onward, each layer reuses half of its Value heads from the very\nfirst layer, while computing the other half as usual-cutting Value projections\nand V cache by nearly 50 \\%. Theoretically, we show that routing uncompressed\nfirst-layer Values into deeper layers restores information lost to compression\nand accelerates the model's implicit mesa-optimization-a key pattern of\nTransformer in auto-regressive tasks. Empirically, across different model\nscales, SkipV1Former delivers consistent reductions of approximately 25 \\% in\nKV cache while improving perplexity relative to standard Multi-Head Attention\n(MHA) Transformers and some advanced variants. Moreover, we propose a recipe\nfor uptraining existing MHA Transformer checkpoints to SkipV1Former with only\n10-15\\% additional compute. Finally, SkipV1Former can seamlessly combine\nadvanced methods like Group-Query Attention and Multi-Latent Attention to\nachieve further KV cache savings and performance improvement. When combined\nwith YOCO, it cuts KV cache size by nearly 50 \\% while still improving\nperformance."
                },
                "authors": [
                    {
                        "name": "Zhoutong Wu"
                    },
                    {
                        "name": "Yuan Zhang"
                    },
                    {
                        "name": "Yiming Dong"
                    },
                    {
                        "name": "Chenheng Zhang"
                    },
                    {
                        "name": "Cong Fang"
                    },
                    {
                        "name": "Kun Yuan"
                    },
                    {
                        "name": "Zhouchen Lin"
                    }
                ],
                "author_detail": {
                    "name": "Zhouchen Lin"
                },
                "author": "Zhouchen Lin",
                "arxiv_comment": "The code is available at:\n  \\url{https://github.com/Zhoutong-Wu/SkipV1Former}",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.16807v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.16807v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.20230v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.20230v1",
                "updated": "2025-10-23T05:22:09Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    5,
                    22,
                    9,
                    3,
                    296,
                    0
                ],
                "published": "2025-10-23T05:22:09Z",
                "published_parsed": [
                    2025,
                    10,
                    23,
                    5,
                    22,
                    9,
                    3,
                    296,
                    0
                ],
                "title": "Soft Phonon Charge-Density Wave Formation in the Kagome Metal\n  KV$_3$Sb$_5$",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Soft Phonon Charge-Density Wave Formation in the Kagome Metal\n  KV$_3$Sb$_5$"
                },
                "summary": "A range of of unusual emergent behaviors have been reported in the\ncharge-density wave (CDW) state of the $A$V$_3$Sb$_5$ ($A=~$K, Rb, Cs) kagome\nmetals, including a CDW formation process without soft phonons, which points to\nan unconventional CDW mechanism. Here, we use inelastic x-ray scattering to\nshow that the CDW in KV$_3$Sb$_5$ forms via phonons that soften to zero energy\nat the CDW ordering vector ($L$-point) around $T_{\\rm CDW}=78$~K. These soft\nphonons exhibit a remarkable in-plane anisotropy, extending over a much larger\nmomentum range along $L$-$A$ relative to $L$-$H$, which leads to diffuse\nscattering common among $A$V$_3$Sb$_5$. Using first-principles calculations, we\nfind that the momentum-dependent electron-phonon coupling (EPC) is peaked at\n$L$ and exhibits the same in-plane anisotropy as the phonon softening.\nConversely, the electronic susceptibility is not peaked at $L$ and shows the\nopposite in-plane anisotropy. Our findings favor momentum-dependent EPC as the\ndriving mechanism of the CDW in KV$_3$Sb$_5$, with a CDW formation process\nsimilar to that of transition metal dichalcogenides.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A range of of unusual emergent behaviors have been reported in the\ncharge-density wave (CDW) state of the $A$V$_3$Sb$_5$ ($A=~$K, Rb, Cs) kagome\nmetals, including a CDW formation process without soft phonons, which points to\nan unconventional CDW mechanism. Here, we use inelastic x-ray scattering to\nshow that the CDW in KV$_3$Sb$_5$ forms via phonons that soften to zero energy\nat the CDW ordering vector ($L$-point) around $T_{\\rm CDW}=78$~K. These soft\nphonons exhibit a remarkable in-plane anisotropy, extending over a much larger\nmomentum range along $L$-$A$ relative to $L$-$H$, which leads to diffuse\nscattering common among $A$V$_3$Sb$_5$. Using first-principles calculations, we\nfind that the momentum-dependent electron-phonon coupling (EPC) is peaked at\n$L$ and exhibits the same in-plane anisotropy as the phonon softening.\nConversely, the electronic susceptibility is not peaked at $L$ and shows the\nopposite in-plane anisotropy. Our findings favor momentum-dependent EPC as the\ndriving mechanism of the CDW in KV$_3$Sb$_5$, with a CDW formation process\nsimilar to that of transition metal dichalcogenides."
                },
                "authors": [
                    {
                        "name": "Yifan Wang"
                    },
                    {
                        "name": "Chenchao Xu"
                    },
                    {
                        "name": "Zhimian Wu"
                    },
                    {
                        "name": "Huachen Rao"
                    },
                    {
                        "name": "Zhaoyang Shan"
                    },
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Guanghan Cao"
                    },
                    {
                        "name": "Michael Smidman"
                    },
                    {
                        "name": "Ming Shi"
                    },
                    {
                        "name": "Huiqiu Yuan"
                    },
                    {
                        "name": "Tao Wu"
                    },
                    {
                        "name": "Xianhui Chen"
                    },
                    {
                        "name": "Chao Cao"
                    },
                    {
                        "name": "Yu Song"
                    }
                ],
                "author_detail": {
                    "name": "Yu Song"
                },
                "author": "Yu Song",
                "arxiv_comment": "submitted to journal in July 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.20230v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.20230v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.supr-con",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.supr-con",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2511.03724v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03724v1",
                "updated": "2025-11-05T18:58:18Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    18,
                    58,
                    18,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T18:58:18Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    18,
                    58,
                    18,
                    2,
                    309,
                    0
                ],
                "title": "Outbidding and Outbluffing Elite Humans: Mastering Liar's Poker via\n  Self-Play and Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Outbidding and Outbluffing Elite Humans: Mastering Liar's Poker via\n  Self-Play and Reinforcement Learning"
                },
                "summary": "AI researchers have long focused on poker-like games as a testbed for\nenvironments characterized by multi-player dynamics, imperfect information, and\nreasoning under uncertainty. While recent breakthroughs have matched elite\nhuman play at no-limit Texas hold'em, the multi-player dynamics are subdued:\nmost hands converge quickly with only two players engaged through multiple\nrounds of bidding. In this paper, we present Solly, the first AI agent to\nachieve elite human play in reduced-format Liar's Poker, a game characterized\nby extensive multi-player engagement. We trained Solly using self-play with a\nmodel-free, actor-critic, deep reinforcement learning algorithm. Solly played\nat an elite human level as measured by win rate (won over 50% of hands) and\nequity (money won) in heads-up and multi-player Liar's Poker. Solly also\noutperformed large language models (LLMs), including those with reasoning\nabilities, on the same metrics. Solly developed novel bidding strategies,\nrandomized play effectively, and was not easily exploitable by world-class\nhuman players.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI researchers have long focused on poker-like games as a testbed for\nenvironments characterized by multi-player dynamics, imperfect information, and\nreasoning under uncertainty. While recent breakthroughs have matched elite\nhuman play at no-limit Texas hold'em, the multi-player dynamics are subdued:\nmost hands converge quickly with only two players engaged through multiple\nrounds of bidding. In this paper, we present Solly, the first AI agent to\nachieve elite human play in reduced-format Liar's Poker, a game characterized\nby extensive multi-player engagement. We trained Solly using self-play with a\nmodel-free, actor-critic, deep reinforcement learning algorithm. Solly played\nat an elite human level as measured by win rate (won over 50% of hands) and\nequity (money won) in heads-up and multi-player Liar's Poker. Solly also\noutperformed large language models (LLMs), including those with reasoning\nabilities, on the same metrics. Solly developed novel bidding strategies,\nrandomized play effectively, and was not easily exploitable by world-class\nhuman players."
                },
                "authors": [
                    {
                        "name": "Richard Dewey"
                    },
                    {
                        "name": "Janos Botyanszki"
                    },
                    {
                        "name": "Ciamac C. Moallemi"
                    },
                    {
                        "name": "Andrew T. Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Andrew T. Zheng"
                },
                "author": "Andrew T. Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03724v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03724v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03720v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03720v1",
                "updated": "2025-11-05T18:53:57Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    18,
                    53,
                    57,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T18:53:57Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    18,
                    53,
                    57,
                    2,
                    309,
                    0
                ],
                "title": "Observation of phase memory and dynamical phase transitions in spinor\n  gases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Observation of phase memory and dynamical phase transitions in spinor\n  gases"
                },
                "summary": "Utilizing ultracold spinor gases as large-scale, many-body quantum simulation\nplatforms, we establish a toolbox for the precise control, characterization,\nand detection of nonequilibrium dynamics via internal spinor phases. We develop\na method to extract the phase evolution from the observed spin population\ndynamics, allowing us to define an order parameter that sharply identifies\ndynamical phase transitions over a wide range of conditions. This work also\ndemonstrates a technique for inferring spin-dependent interactions from a\nsingle experimental time trace, in contrast to the standard approach that\nrequires mapping a cross section of the phase diagram, with immediate\napplications to systems experiencing complex time-dependent interactions.\nAdditionally, we demonstrate experimental access to and control over\nnon-ergodic relaxation dynamics, where states in the (nominally) thermal region\nof the energy spectrum retain memory of the initial state, via the manipulation\nof spinor phases, enabling the study of non-ergodic thermalization dynamics\nconnected to quantum scarring.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Utilizing ultracold spinor gases as large-scale, many-body quantum simulation\nplatforms, we establish a toolbox for the precise control, characterization,\nand detection of nonequilibrium dynamics via internal spinor phases. We develop\na method to extract the phase evolution from the observed spin population\ndynamics, allowing us to define an order parameter that sharply identifies\ndynamical phase transitions over a wide range of conditions. This work also\ndemonstrates a technique for inferring spin-dependent interactions from a\nsingle experimental time trace, in contrast to the standard approach that\nrequires mapping a cross section of the phase diagram, with immediate\napplications to systems experiencing complex time-dependent interactions.\nAdditionally, we demonstrate experimental access to and control over\nnon-ergodic relaxation dynamics, where states in the (nominally) thermal region\nof the energy spectrum retain memory of the initial state, via the manipulation\nof spinor phases, enabling the study of non-ergodic thermalization dynamics\nconnected to quantum scarring."
                },
                "authors": [
                    {
                        "name": "J. O. Austin-Harris"
                    },
                    {
                        "name": "P. Sigdel"
                    },
                    {
                        "name": "C. Binegar"
                    },
                    {
                        "name": "S. E. Begg"
                    },
                    {
                        "name": "T. Bilitewski"
                    },
                    {
                        "name": "Y. Liu"
                    }
                ],
                "author_detail": {
                    "name": "Y. Liu"
                },
                "author": "Y. Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03720v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03720v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.quant-gas",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.quant-gas",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03718v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03718v1",
                "updated": "2025-11-05T18:52:28Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    18,
                    52,
                    28,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T18:52:28Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    18,
                    52,
                    28,
                    2,
                    309,
                    0
                ],
                "title": "Grounded Misunderstandings in Asymmetric Dialogue: A Perspectivist\n  Annotation Scheme for MapTask",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Grounded Misunderstandings in Asymmetric Dialogue: A Perspectivist\n  Annotation Scheme for MapTask"
                },
                "summary": "Collaborative dialogue relies on participants incrementally establishing\ncommon ground, yet in asymmetric settings they may believe they agree while\nreferring to different entities. We introduce a perspectivist annotation scheme\nfor the HCRC MapTask corpus (Anderson et al., 1991) that separately captures\nspeaker and addressee grounded interpretations for each reference expression,\nenabling us to trace how understanding emerges, diverges, and repairs over\ntime. Using a scheme-constrained LLM annotation pipeline, we obtain 13k\nannotated reference expressions with reliability estimates and analyze the\nresulting understanding states. The results show that full misunderstandings\nare rare once lexical variants are unified, but multiplicity discrepancies\nsystematically induce divergences, revealing how apparent grounding can mask\nreferential misalignment. Our framework provides both a resource and an\nanalytic lens for studying grounded misunderstanding and for evaluating\n(V)LLMs' capacity to model perspective-dependent grounding in collaborative\ndialogue.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collaborative dialogue relies on participants incrementally establishing\ncommon ground, yet in asymmetric settings they may believe they agree while\nreferring to different entities. We introduce a perspectivist annotation scheme\nfor the HCRC MapTask corpus (Anderson et al., 1991) that separately captures\nspeaker and addressee grounded interpretations for each reference expression,\nenabling us to trace how understanding emerges, diverges, and repairs over\ntime. Using a scheme-constrained LLM annotation pipeline, we obtain 13k\nannotated reference expressions with reliability estimates and analyze the\nresulting understanding states. The results show that full misunderstandings\nare rare once lexical variants are unified, but multiplicity discrepancies\nsystematically induce divergences, revealing how apparent grounding can mask\nreferential misalignment. Our framework provides both a resource and an\nanalytic lens for studying grounded misunderstanding and for evaluating\n(V)LLMs' capacity to model perspective-dependent grounding in collaborative\ndialogue."
                },
                "authors": [
                    {
                        "name": "Nan Li"
                    },
                    {
                        "name": "Albert Gatt"
                    },
                    {
                        "name": "Massimo Poesio"
                    }
                ],
                "author_detail": {
                    "name": "Massimo Poesio"
                },
                "author": "Massimo Poesio",
                "arxiv_comment": "11 pages, 3 figures, 5 tables; under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03718v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03718v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20637v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20637v2",
                "updated": "2025-11-05T18:39:38Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    18,
                    39,
                    38,
                    2,
                    309,
                    0
                ],
                "published": "2025-08-28T10:35:44Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    10,
                    35,
                    44,
                    3,
                    240,
                    0
                ],
                "title": "GDS Agent for Graph Algorithmic Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GDS Agent for Graph Algorithmic Reasoning"
                },
                "summary": "Large language models (LLMs) have shown remarkable multimodal information\nprocessing and reasoning ability. When equipped with tools through function\ncalling and enhanced with retrieval-augmented techniques, compound LLM-based\nsystems can access closed data sources and answer questions about them.\nHowever, they still struggle to process and reason over large-scale\ngraph-structure data. We introduce the GDS (Graph Data Science) agent in this\ntechnical report. The GDS agent introduces a comprehensive set of graph\nalgorithms as tools, together with preprocessing (retrieval) and postprocessing\nof algorithm results, in a model context protocol (MCP) server. The server can\nbe used with any modern LLM out-of-the-box. GDS agent allows users to ask any\nquestion that implicitly and intrinsically requires graph algorithmic reasoning\nabout their data, and quickly obtain accurate and grounded answers. We\nintroduce new benchmarks that evaluate intermediate tool calls as well as final\nresponses. The results indicate that GDS agent is able to solve a wide spectrum\nof graph tasks. We also provide detailed case studies for more open-ended tasks\nand study scenarios where the agent struggles. Finally, we discuss the\nremaining challenges and the future roadmap.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable multimodal information\nprocessing and reasoning ability. When equipped with tools through function\ncalling and enhanced with retrieval-augmented techniques, compound LLM-based\nsystems can access closed data sources and answer questions about them.\nHowever, they still struggle to process and reason over large-scale\ngraph-structure data. We introduce the GDS (Graph Data Science) agent in this\ntechnical report. The GDS agent introduces a comprehensive set of graph\nalgorithms as tools, together with preprocessing (retrieval) and postprocessing\nof algorithm results, in a model context protocol (MCP) server. The server can\nbe used with any modern LLM out-of-the-box. GDS agent allows users to ask any\nquestion that implicitly and intrinsically requires graph algorithmic reasoning\nabout their data, and quickly obtain accurate and grounded answers. We\nintroduce new benchmarks that evaluate intermediate tool calls as well as final\nresponses. The results indicate that GDS agent is able to solve a wide spectrum\nof graph tasks. We also provide detailed case studies for more open-ended tasks\nand study scenarios where the agent struggles. Finally, we discuss the\nremaining challenges and the future roadmap."
                },
                "authors": [
                    {
                        "name": "Borun Shi"
                    },
                    {
                        "name": "Ioannis Panagiotas"
                    }
                ],
                "author_detail": {
                    "name": "Ioannis Panagiotas"
                },
                "author": "Ioannis Panagiotas",
                "arxiv_comment": "Technical report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20637v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20637v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03706v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03706v1",
                "updated": "2025-11-05T18:38:02Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    18,
                    38,
                    2,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T18:38:02Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    18,
                    38,
                    2,
                    2,
                    309,
                    0
                ],
                "title": "LLM-enhanced Air Quality Monitoring Interface via Model Context Protocol",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-enhanced Air Quality Monitoring Interface via Model Context Protocol"
                },
                "summary": "Air quality monitoring is central to environmental sustainability and public\nhealth, yet traditional systems remain difficult for non-expert users to\ninterpret due to complex visualizations, limited interactivity, and high\ndeployment costs. Recent advances in Large Language Models (LLMs) offer new\nopportunities to make sensor data more accessible, but their tendency to\nproduce hallucinations limits reliability in safety-critical domains. To\naddress these challenges, we present an LLM-enhanced Air Monitoring Interface\n(AMI) that integrates real-time sensor data with a conversational interface via\nthe Model Context Protocol (MCP). Our system grounds LLM outputs in live\nenvironmental data, enabling accurate, context-aware responses while reducing\nhallucination risk. The architecture combines a Django-based backend, a\nresponsive user dashboard, and a secure MCP server that exposes system\nfunctions as discoverable tools, allowing the LLM to act as an active operator\nrather than a passive responder. Expert evaluation demonstrated high factual\naccuracy (4.78), completeness (4.82), and minimal hallucinations (4.84), on a\nscale of 5, supported by inter-rater reliability analysis. These results\nhighlight the potential of combining LLMs with standardized tool protocols to\ncreate reliable, secure, and user-friendly interfaces for real-time\nenvironmental monitoring.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Air quality monitoring is central to environmental sustainability and public\nhealth, yet traditional systems remain difficult for non-expert users to\ninterpret due to complex visualizations, limited interactivity, and high\ndeployment costs. Recent advances in Large Language Models (LLMs) offer new\nopportunities to make sensor data more accessible, but their tendency to\nproduce hallucinations limits reliability in safety-critical domains. To\naddress these challenges, we present an LLM-enhanced Air Monitoring Interface\n(AMI) that integrates real-time sensor data with a conversational interface via\nthe Model Context Protocol (MCP). Our system grounds LLM outputs in live\nenvironmental data, enabling accurate, context-aware responses while reducing\nhallucination risk. The architecture combines a Django-based backend, a\nresponsive user dashboard, and a secure MCP server that exposes system\nfunctions as discoverable tools, allowing the LLM to act as an active operator\nrather than a passive responder. Expert evaluation demonstrated high factual\naccuracy (4.78), completeness (4.82), and minimal hallucinations (4.84), on a\nscale of 5, supported by inter-rater reliability analysis. These results\nhighlight the potential of combining LLMs with standardized tool protocols to\ncreate reliable, secure, and user-friendly interfaces for real-time\nenvironmental monitoring."
                },
                "authors": [
                    {
                        "name": "Yu-Erh Pan"
                    },
                    {
                        "name": "Ayesha Siddika Nipu"
                    }
                ],
                "author_detail": {
                    "name": "Ayesha Siddika Nipu"
                },
                "author": "Ayesha Siddika Nipu",
                "arxiv_comment": "International Symposium on Advanced Electrical and Communication\n  Technologies, ISAECT 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03706v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03706v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03699v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03699v1",
                "updated": "2025-11-05T18:28:28Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    18,
                    28,
                    28,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T18:28:28Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    18,
                    28,
                    28,
                    2,
                    309,
                    0
                ],
                "title": "Do Androids Dream of Unseen Puppeteers? Probing for a Conspiracy Mindset\n  in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Androids Dream of Unseen Puppeteers? Probing for a Conspiracy Mindset\n  in Large Language Models"
                },
                "summary": "In this paper, we investigate whether Large Language Models (LLMs) exhibit\nconspiratorial tendencies, whether they display sociodemographic biases in this\ndomain, and how easily they can be conditioned into adopting conspiratorial\nperspectives. Conspiracy beliefs play a central role in the spread of\nmisinformation and in shaping distrust toward institutions, making them a\ncritical testbed for evaluating the social fidelity of LLMs. LLMs are\nincreasingly used as proxies for studying human behavior, yet little is known\nabout whether they reproduce higher-order psychological constructs such as a\nconspiratorial mindset. To bridge this research gap, we administer validated\npsychometric surveys measuring conspiracy mindset to multiple models under\ndifferent prompting and conditioning strategies. Our findings reveal that LLMs\nshow partial agreement with elements of conspiracy belief, and conditioning\nwith socio-demographic attributes produces uneven effects, exposing latent\ndemographic biases. Moreover, targeted prompts can easily shift model responses\ntoward conspiratorial directions, underscoring both the susceptibility of LLMs\nto manipulation and the potential risks of their deployment in sensitive\ncontexts. These results highlight the importance of critically evaluating the\npsychological dimensions embedded in LLMs, both to advance computational social\nscience and to inform possible mitigation strategies against harmful uses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we investigate whether Large Language Models (LLMs) exhibit\nconspiratorial tendencies, whether they display sociodemographic biases in this\ndomain, and how easily they can be conditioned into adopting conspiratorial\nperspectives. Conspiracy beliefs play a central role in the spread of\nmisinformation and in shaping distrust toward institutions, making them a\ncritical testbed for evaluating the social fidelity of LLMs. LLMs are\nincreasingly used as proxies for studying human behavior, yet little is known\nabout whether they reproduce higher-order psychological constructs such as a\nconspiratorial mindset. To bridge this research gap, we administer validated\npsychometric surveys measuring conspiracy mindset to multiple models under\ndifferent prompting and conditioning strategies. Our findings reveal that LLMs\nshow partial agreement with elements of conspiracy belief, and conditioning\nwith socio-demographic attributes produces uneven effects, exposing latent\ndemographic biases. Moreover, targeted prompts can easily shift model responses\ntoward conspiratorial directions, underscoring both the susceptibility of LLMs\nto manipulation and the potential risks of their deployment in sensitive\ncontexts. These results highlight the importance of critically evaluating the\npsychological dimensions embedded in LLMs, both to advance computational social\nscience and to inform possible mitigation strategies against harmful uses."
                },
                "authors": [
                    {
                        "name": "Francesco Corso"
                    },
                    {
                        "name": "Francesco Pierri"
                    },
                    {
                        "name": "Gianmarco De Francisci Morales"
                    }
                ],
                "author_detail": {
                    "name": "Gianmarco De Francisci Morales"
                },
                "author": "Gianmarco De Francisci Morales",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03699v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03699v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03697v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03697v1",
                "updated": "2025-11-05T18:24:01Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    18,
                    24,
                    1,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T18:24:01Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    18,
                    24,
                    1,
                    2,
                    309,
                    0
                ],
                "title": "AnaFlow: Agentic LLM-based Workflow for Reasoning-Driven Explainable and\n  Sample-Efficient Analog Circuit Sizing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AnaFlow: Agentic LLM-based Workflow for Reasoning-Driven Explainable and\n  Sample-Efficient Analog Circuit Sizing"
                },
                "summary": "Analog/mixed-signal circuits are key for interfacing electronics with the\nphysical world. Their design, however, remains a largely handcrafted process,\nresulting in long and error-prone design cycles. While the recent rise of\nAI-based reinforcement learning and generative AI has created new techniques to\nautomate this task, the need for many time-consuming simulations is a critical\nbottleneck hindering the overall efficiency. Furthermore, the lack of\nexplainability of the resulting design solutions hampers widespread adoption of\nthe tools. To address these issues, a novel agentic AI framework for\nsample-efficient and explainable analog circuit sizing is presented. It employs\na multi-agent workflow where specialized Large Language Model (LLM)-based\nagents collaborate to interpret the circuit topology, to understand the design\ngoals, and to iteratively refine the circuit's design parameters towards the\ntarget goals with human-interpretable reasoning. The adaptive simulation\nstrategy creates an intelligent control that yields a high sample efficiency.\nThe AnaFlow framework is demonstrated for two circuits of varying complexity\nand is able to complete the sizing task fully automatically, differently from\npure Bayesian optimization and reinforcement learning approaches. The system\nlearns from its optimization history to avoid past mistakes and to accelerate\nconvergence. The inherent explainability makes this a powerful tool for analog\ndesign space exploration and a new paradigm in analog EDA, where AI agents\nserve as transparent design assistants.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analog/mixed-signal circuits are key for interfacing electronics with the\nphysical world. Their design, however, remains a largely handcrafted process,\nresulting in long and error-prone design cycles. While the recent rise of\nAI-based reinforcement learning and generative AI has created new techniques to\nautomate this task, the need for many time-consuming simulations is a critical\nbottleneck hindering the overall efficiency. Furthermore, the lack of\nexplainability of the resulting design solutions hampers widespread adoption of\nthe tools. To address these issues, a novel agentic AI framework for\nsample-efficient and explainable analog circuit sizing is presented. It employs\na multi-agent workflow where specialized Large Language Model (LLM)-based\nagents collaborate to interpret the circuit topology, to understand the design\ngoals, and to iteratively refine the circuit's design parameters towards the\ntarget goals with human-interpretable reasoning. The adaptive simulation\nstrategy creates an intelligent control that yields a high sample efficiency.\nThe AnaFlow framework is demonstrated for two circuits of varying complexity\nand is able to complete the sizing task fully automatically, differently from\npure Bayesian optimization and reinforcement learning approaches. The system\nlearns from its optimization history to avoid past mistakes and to accelerate\nconvergence. The inherent explainability makes this a powerful tool for analog\ndesign space exploration and a new paradigm in analog EDA, where AI agents\nserve as transparent design assistants."
                },
                "authors": [
                    {
                        "name": "Mohsen Ahmadzadeh"
                    },
                    {
                        "name": "Kaichang Chen"
                    },
                    {
                        "name": "Georges Gielen"
                    }
                ],
                "author_detail": {
                    "name": "Georges Gielen"
                },
                "author": "Georges Gielen",
                "arxiv_comment": "This article was accepted by 2025 International Conference on\n  Computer-Aided Design (ICCAD 2025) and was presented in Munich, October 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03697v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03697v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04825v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04825v2",
                "updated": "2025-11-05T18:23:44Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    18,
                    23,
                    44,
                    2,
                    309,
                    0
                ],
                "published": "2025-08-06T19:10:58Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    19,
                    10,
                    58,
                    2,
                    218,
                    0
                ],
                "title": "Voost: A Unified and Scalable Diffusion Transformer for Bidirectional\n  Virtual Try-On and Try-Off",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Voost: A Unified and Scalable Diffusion Transformer for Bidirectional\n  Virtual Try-On and Try-Off"
                },
                "summary": "Virtual try-on aims to synthesize a realistic image of a person wearing a\ntarget garment, but accurately modeling garment-body correspondence remains a\npersistent challenge, especially under pose and appearance variation. In this\npaper, we propose Voost - a unified and scalable framework that jointly learns\nvirtual try-on and try-off with a single diffusion transformer. By modeling\nboth tasks jointly, Voost enables each garment-person pair to supervise both\ndirections and supports flexible conditioning over generation direction and\ngarment category, enhancing garment-body relational reasoning without\ntask-specific networks, auxiliary losses, or additional labels. In addition, we\nintroduce two inference-time techniques: attention temperature scaling for\nrobustness to resolution or mask variation, and self-corrective sampling that\nleverages bidirectional consistency between tasks. Extensive experiments\ndemonstrate that Voost achieves state-of-the-art results on both try-on and\ntry-off benchmarks, consistently outperforming strong baselines in alignment\naccuracy, visual fidelity, and generalization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Virtual try-on aims to synthesize a realistic image of a person wearing a\ntarget garment, but accurately modeling garment-body correspondence remains a\npersistent challenge, especially under pose and appearance variation. In this\npaper, we propose Voost - a unified and scalable framework that jointly learns\nvirtual try-on and try-off with a single diffusion transformer. By modeling\nboth tasks jointly, Voost enables each garment-person pair to supervise both\ndirections and supports flexible conditioning over generation direction and\ngarment category, enhancing garment-body relational reasoning without\ntask-specific networks, auxiliary losses, or additional labels. In addition, we\nintroduce two inference-time techniques: attention temperature scaling for\nrobustness to resolution or mask variation, and self-corrective sampling that\nleverages bidirectional consistency between tasks. Extensive experiments\ndemonstrate that Voost achieves state-of-the-art results on both try-on and\ntry-off benchmarks, consistently outperforming strong baselines in alignment\naccuracy, visual fidelity, and generalization."
                },
                "authors": [
                    {
                        "name": "Seungyong Lee"
                    },
                    {
                        "name": "Jeong-gi Kwak"
                    }
                ],
                "author_detail": {
                    "name": "Jeong-gi Kwak"
                },
                "author": "Jeong-gi Kwak",
                "arxiv_comment": "Accepted to SIGGRAPH Asia 2025, project page:\n  https://nxnai.github.io/Voost/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04825v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04825v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03692v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03692v1",
                "updated": "2025-11-05T18:18:06Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    18,
                    18,
                    6,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T18:18:06Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    18,
                    18,
                    6,
                    2,
                    309,
                    0
                ],
                "title": "Improving Gene Trees without more data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Gene Trees without more data"
                },
                "summary": "Estimating species and gene trees from sequence data is challenging. Gene\ntree estimation is often hampered by low phylogenetic signal in alignments,\nleading to inaccurate trees. Species tree estimation is complicated by\nincomplete lineage sorting (ILS), where gene histories differ from the species'\nhistory. Summary methods like MP-EST, ASTRAL2, and ASTRID infer species trees\nfrom gene trees but suffer when gene tree accuracy is low. To address this, the\nStatistical Binning (SB) and Weighted Statistical Binning (WSB) pipelines were\ndeveloped to improve gene tree estimation. However, previous studies only\ntested these pipelines using multi-locus bootstrapping (MLBS), not the BestML\napproach.\n  This thesis proposes a novel pipeline, WSB+WQMC, which shares design features\nwith the existing WSB+CAML pipeline but has other desirable properties and is\nstatistically consistent under the GTR+MSC model. This study evaluated WSB+WQMC\nagainst WSB+CAML using BestML analysis on various simulated datasets. The\nresults confirmed many trends seen in prior MLBS analyses. WSB+WQMC\nsubstantially improved gene tree and species tree accuracy (using ASTRAL2 and\nASTRID) on most datasets with low, medium, and moderately high ILS levels. In a\ndirect comparison, WSB+WQMC computed less accurate trees than WSB+CAML under\ncertain low and medium ILS conditions. However, WSB+WQMC performed better or at\nleast as accurately as WSB+CAML on all datasets with moderately high and high\nILS. It also proved better for estimating gene trees on some medium and low ILS\ndatasets. Thus, WSB+WQMC is a promising alternative to WSB+CAML for\nphylogenetic estimation, especially in the presence of low phylogenetic signal.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimating species and gene trees from sequence data is challenging. Gene\ntree estimation is often hampered by low phylogenetic signal in alignments,\nleading to inaccurate trees. Species tree estimation is complicated by\nincomplete lineage sorting (ILS), where gene histories differ from the species'\nhistory. Summary methods like MP-EST, ASTRAL2, and ASTRID infer species trees\nfrom gene trees but suffer when gene tree accuracy is low. To address this, the\nStatistical Binning (SB) and Weighted Statistical Binning (WSB) pipelines were\ndeveloped to improve gene tree estimation. However, previous studies only\ntested these pipelines using multi-locus bootstrapping (MLBS), not the BestML\napproach.\n  This thesis proposes a novel pipeline, WSB+WQMC, which shares design features\nwith the existing WSB+CAML pipeline but has other desirable properties and is\nstatistically consistent under the GTR+MSC model. This study evaluated WSB+WQMC\nagainst WSB+CAML using BestML analysis on various simulated datasets. The\nresults confirmed many trends seen in prior MLBS analyses. WSB+WQMC\nsubstantially improved gene tree and species tree accuracy (using ASTRAL2 and\nASTRID) on most datasets with low, medium, and moderately high ILS levels. In a\ndirect comparison, WSB+WQMC computed less accurate trees than WSB+CAML under\ncertain low and medium ILS conditions. However, WSB+WQMC performed better or at\nleast as accurately as WSB+CAML on all datasets with moderately high and high\nILS. It also proved better for estimating gene trees on some medium and low ILS\ndatasets. Thus, WSB+WQMC is a promising alternative to WSB+CAML for\nphylogenetic estimation, especially in the presence of low phylogenetic signal."
                },
                "authors": [
                    {
                        "name": "Ashu Gupta"
                    }
                ],
                "author_detail": {
                    "name": "Ashu Gupta"
                },
                "author": "Ashu Gupta",
                "arxiv_comment": "Master's thesis, Adviser: Dr Tandy Warnow",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03692v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03692v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.PE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.PE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03690v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03690v1",
                "updated": "2025-11-05T18:16:44Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    18,
                    16,
                    44,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T18:16:44Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    18,
                    16,
                    44,
                    2,
                    309,
                    0
                ],
                "title": "The OpenHands Software Agent SDK: A Composable and Extensible Foundation\n  for Production Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The OpenHands Software Agent SDK: A Composable and Extensible Foundation\n  for Production Agents"
                },
                "summary": "Agents are now used widely in the process of software development, but\nbuilding production-ready software engineering agents is a complex task.\nDeploying software agents effectively requires flexibility in implementation\nand experimentation, reliable and secure execution, and interfaces for users to\ninteract with agents. In this paper, we present the OpenHands Software Agent\nSDK, a toolkit for implementing software development agents that satisfy these\ndesiderata. This toolkit is a complete architectural redesign of the agent\ncomponents of the popular OpenHands framework for software development agents,\nwhich has 64k+ GitHub stars. To achieve flexibility, we design a simple\ninterface for implementing agents that requires only a few lines of code in the\ndefault case, but is easily extensible to more complex, full-featured agents\nwith features such as custom tools, memory management, and more. For security\nand reliability, it delivers seamless local-to-remote execution portability,\nintegrated REST/WebSocket services. For interaction with human users, it can\nconnect directly to a variety of interfaces, such as visual workspaces (VS\nCode, VNC, browser), command-line interfaces, and APIs. Compared with existing\nSDKs from OpenAI, Claude, and Google, OpenHands uniquely integrates native\nsandboxed execution, lifecycle control, model-agnostic multi-LLM routing, and\nbuilt-in security analysis. Empirical results on SWE-Bench Verified and GAIA\nbenchmarks demonstrate strong performance. Put together, these elements allow\nthe OpenHands Software Agent SDK to provide a practical foundation for\nprototyping, unlocking new classes of custom applications, and reliably\ndeploying agents at scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agents are now used widely in the process of software development, but\nbuilding production-ready software engineering agents is a complex task.\nDeploying software agents effectively requires flexibility in implementation\nand experimentation, reliable and secure execution, and interfaces for users to\ninteract with agents. In this paper, we present the OpenHands Software Agent\nSDK, a toolkit for implementing software development agents that satisfy these\ndesiderata. This toolkit is a complete architectural redesign of the agent\ncomponents of the popular OpenHands framework for software development agents,\nwhich has 64k+ GitHub stars. To achieve flexibility, we design a simple\ninterface for implementing agents that requires only a few lines of code in the\ndefault case, but is easily extensible to more complex, full-featured agents\nwith features such as custom tools, memory management, and more. For security\nand reliability, it delivers seamless local-to-remote execution portability,\nintegrated REST/WebSocket services. For interaction with human users, it can\nconnect directly to a variety of interfaces, such as visual workspaces (VS\nCode, VNC, browser), command-line interfaces, and APIs. Compared with existing\nSDKs from OpenAI, Claude, and Google, OpenHands uniquely integrates native\nsandboxed execution, lifecycle control, model-agnostic multi-LLM routing, and\nbuilt-in security analysis. Empirical results on SWE-Bench Verified and GAIA\nbenchmarks demonstrate strong performance. Put together, these elements allow\nthe OpenHands Software Agent SDK to provide a practical foundation for\nprototyping, unlocking new classes of custom applications, and reliably\ndeploying agents at scale."
                },
                "authors": [
                    {
                        "name": "Xingyao Wang"
                    },
                    {
                        "name": "Simon Rosenberg"
                    },
                    {
                        "name": "Juan Michelini"
                    },
                    {
                        "name": "Calvin Smith"
                    },
                    {
                        "name": "Hoang Tran"
                    },
                    {
                        "name": "Engel Nyst"
                    },
                    {
                        "name": "Rohit Malhotra"
                    },
                    {
                        "name": "Xuhui Zhou"
                    },
                    {
                        "name": "Valerie Chen"
                    },
                    {
                        "name": "Robert Brennan"
                    },
                    {
                        "name": "Graham Neubig"
                    }
                ],
                "author_detail": {
                    "name": "Graham Neubig"
                },
                "author": "Graham Neubig",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03690v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03690v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.00807v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.00807v2",
                "updated": "2025-11-05T18:15:57Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    18,
                    15,
                    57,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-02T05:17:02Z",
                "published_parsed": [
                    2025,
                    11,
                    2,
                    5,
                    17,
                    2,
                    6,
                    306,
                    0
                ],
                "title": "FREESH: Fair, Resource- and Energy-Efficient Scheduling for LLM Serving\n  on Heterogeneous GPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FREESH: Fair, Resource- and Energy-Efficient Scheduling for LLM Serving\n  on Heterogeneous GPUs"
                },
                "summary": "The ever-increasing computation and energy demand for LLM and AI agents call\nfor holistic and efficient optimization of LLM serving systems. In practice,\nheterogeneous GPU clusters can be deployed in a geographically distributed\nmanner, while LLM load also observes diversity in terms of both query traffic\nand serving patterns. LLM queries running on advanced GPUs during a\nhigh-emission hour at one location can lead to significantly higher carbon\nfootprints versus same queries running on mid-level GPUs at a low-emission time\nand location. By observing LLM serving requirements and leveraging\nspatiotemporal computation flexibility, we consider the joint routing and\nscheduling problem, and propose FREESH to cooperatively run a group of data\ncenters while minimizing user-specified carbon or energy objectives. FREESH\nidentifies the optimal configurations of balanced load serving by matching\ndistinct GPU instance's power-throughput characteristics with predictable LLM\nquery length and workloads. To ensure both latency and fairness requirements,\nFREESH identifies optimized parallelism and query routing schedules together\nwith dynamic GPU frequency scaling for power saving, and Least-Laxity-First\n(LLF) serving strategy for query scheduling. During the 1-hour serving on\nproduction workloads, FREESH reduces energy by 28.6% and emissions by 45.45%\ntogether with improvements in SLO attainment and fairness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ever-increasing computation and energy demand for LLM and AI agents call\nfor holistic and efficient optimization of LLM serving systems. In practice,\nheterogeneous GPU clusters can be deployed in a geographically distributed\nmanner, while LLM load also observes diversity in terms of both query traffic\nand serving patterns. LLM queries running on advanced GPUs during a\nhigh-emission hour at one location can lead to significantly higher carbon\nfootprints versus same queries running on mid-level GPUs at a low-emission time\nand location. By observing LLM serving requirements and leveraging\nspatiotemporal computation flexibility, we consider the joint routing and\nscheduling problem, and propose FREESH to cooperatively run a group of data\ncenters while minimizing user-specified carbon or energy objectives. FREESH\nidentifies the optimal configurations of balanced load serving by matching\ndistinct GPU instance's power-throughput characteristics with predictable LLM\nquery length and workloads. To ensure both latency and fairness requirements,\nFREESH identifies optimized parallelism and query routing schedules together\nwith dynamic GPU frequency scaling for power saving, and Least-Laxity-First\n(LLF) serving strategy for query scheduling. During the 1-hour serving on\nproduction workloads, FREESH reduces energy by 28.6% and emissions by 45.45%\ntogether with improvements in SLO attainment and fairness."
                },
                "authors": [
                    {
                        "name": "Xuan He"
                    },
                    {
                        "name": "Zequan Fang"
                    },
                    {
                        "name": "Jinzhao Lian"
                    },
                    {
                        "name": "Danny H. K. Tsang"
                    },
                    {
                        "name": "Baosen Zhang"
                    },
                    {
                        "name": "Yize Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yize Chen"
                },
                "author": "Yize Chen",
                "arxiv_comment": "In Submission, code available at\n  https://github.com/AndrewFangZequan/LLM_Serving_FREESH",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.00807v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.00807v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04677v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04677v2",
                "updated": "2025-11-05T18:12:33Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    18,
                    12,
                    33,
                    2,
                    309,
                    0
                ],
                "published": "2025-02-07T05:49:50Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    5,
                    49,
                    50,
                    4,
                    38,
                    0
                ],
                "title": "LLM Query Scheduling with Prefix Reuse and Latency Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Query Scheduling with Prefix Reuse and Latency Constraints"
                },
                "summary": "The efficient deployment of large language models (LLMs) in online settings\nrequires optimizing inference performance under stringent latency constraints,\nparticularly the time-to-first-token (TTFT) and time-per-output-token (TPOT).\nThis paper focuses on the query scheduling problem for LLM inference with\nprefix reuse, a technique that leverages shared prefixes across queries to\nreduce computational overhead. Our work reveals previously unknown limitations\nof the existing first-come-first-serve (FCFS) and longest-prefix-match (LPM)\nscheduling strategies with respect to satisfying latency constraints. We\npresent a formal theoretical framework for LLM query scheduling under\nRadixAttention, a prefix reuse mechanism that stores and reuses intermediate\nrepresentations in a radix tree structure. Our analysis establishes the\nNP-hardness of the scheduling problem with prefix reuse under TTFT constraints\nand proposes a novel scheduling algorithm, $k$-LPM, which generalizes existing\nmethods by balancing prefix reuse and fairness in query processing. Theoretical\nguarantees demonstrate that $k$-LPM achieves improved TTFT performance under\nrealistic traffic patterns captured by a data generative model. Empirical\nevaluations in a realistic serving setting validates our findings, showing\nsignificant reductions in P99 TTFT compared to baseline methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The efficient deployment of large language models (LLMs) in online settings\nrequires optimizing inference performance under stringent latency constraints,\nparticularly the time-to-first-token (TTFT) and time-per-output-token (TPOT).\nThis paper focuses on the query scheduling problem for LLM inference with\nprefix reuse, a technique that leverages shared prefixes across queries to\nreduce computational overhead. Our work reveals previously unknown limitations\nof the existing first-come-first-serve (FCFS) and longest-prefix-match (LPM)\nscheduling strategies with respect to satisfying latency constraints. We\npresent a formal theoretical framework for LLM query scheduling under\nRadixAttention, a prefix reuse mechanism that stores and reuses intermediate\nrepresentations in a radix tree structure. Our analysis establishes the\nNP-hardness of the scheduling problem with prefix reuse under TTFT constraints\nand proposes a novel scheduling algorithm, $k$-LPM, which generalizes existing\nmethods by balancing prefix reuse and fairness in query processing. Theoretical\nguarantees demonstrate that $k$-LPM achieves improved TTFT performance under\nrealistic traffic patterns captured by a data generative model. Empirical\nevaluations in a realistic serving setting validates our findings, showing\nsignificant reductions in P99 TTFT compared to baseline methods."
                },
                "authors": [
                    {
                        "name": "Gregory Dexter"
                    },
                    {
                        "name": "Shao Tang"
                    },
                    {
                        "name": "Ata Fatahi Baarzi"
                    },
                    {
                        "name": "Qingquan Song"
                    },
                    {
                        "name": "Tejas Dharamsi"
                    },
                    {
                        "name": "Aman Gupta"
                    }
                ],
                "author_detail": {
                    "name": "Aman Gupta"
                },
                "author": "Aman Gupta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04677v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04677v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.00183v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.00183v2",
                "updated": "2025-11-05T17:58:32Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    17,
                    58,
                    32,
                    2,
                    309,
                    0
                ],
                "published": "2025-10-31T18:38:05Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    18,
                    38,
                    5,
                    4,
                    304,
                    0
                ],
                "title": "PDE-SHARP: PDE Solver Hybrids through Analysis and Refinement Passes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PDE-SHARP: PDE Solver Hybrids through Analysis and Refinement Passes"
                },
                "summary": "Current LLM-driven approaches using test-time computing to generate PDE\nsolvers execute a large number of solver samples to identify high-accuracy\nsolvers. These paradigms are especially costly for complex PDEs requiring\nsubstantial computational resources for numerical evaluation. We introduce\nPDE-SHARP, a framework to reduce computational costs by replacing expensive\nscientific computation by cheaper LLM inference that achieves superior solver\naccuracy with 60-75% fewer computational evaluations. PDE-SHARP employs three\nstages: (1) Analysis: mathematical chain-of-thought analysis including PDE\nclassification, solution type detection, and stability analysis; (2) Genesis:\nsolver generation based on mathematical insights from the previous stage; and\n(3) Synthesis: collaborative selection-hybridization tournaments in which LLM\njudges iteratively refine implementations through flexible performance\nfeedback. To generate high-quality solvers, PDE-SHARP requires fewer than 13\nsolver evaluations on average compared to 30+ for baseline methods, improving\naccuracy uniformly across tested PDEs by $4\\times$ on average, and demonstrates\nrobust performance across LLM architectures, from general-purpose to\nspecialized reasoning models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current LLM-driven approaches using test-time computing to generate PDE\nsolvers execute a large number of solver samples to identify high-accuracy\nsolvers. These paradigms are especially costly for complex PDEs requiring\nsubstantial computational resources for numerical evaluation. We introduce\nPDE-SHARP, a framework to reduce computational costs by replacing expensive\nscientific computation by cheaper LLM inference that achieves superior solver\naccuracy with 60-75% fewer computational evaluations. PDE-SHARP employs three\nstages: (1) Analysis: mathematical chain-of-thought analysis including PDE\nclassification, solution type detection, and stability analysis; (2) Genesis:\nsolver generation based on mathematical insights from the previous stage; and\n(3) Synthesis: collaborative selection-hybridization tournaments in which LLM\njudges iteratively refine implementations through flexible performance\nfeedback. To generate high-quality solvers, PDE-SHARP requires fewer than 13\nsolver evaluations on average compared to 30+ for baseline methods, improving\naccuracy uniformly across tested PDEs by $4\\times$ on average, and demonstrates\nrobust performance across LLM architectures, from general-purpose to\nspecialized reasoning models."
                },
                "authors": [
                    {
                        "name": "Shaghayegh Fazliani"
                    },
                    {
                        "name": "Madeleine Udell"
                    }
                ],
                "author_detail": {
                    "name": "Madeleine Udell"
                },
                "author": "Madeleine Udell",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.00183v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.00183v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25531v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25531v2",
                "updated": "2025-11-05T17:49:58Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    17,
                    49,
                    58,
                    2,
                    309,
                    0
                ],
                "published": "2025-10-29T13:56:44Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    13,
                    56,
                    44,
                    2,
                    302,
                    0
                ],
                "title": "Using latent representations to link disjoint longitudinal data for\n  mixed-effects regression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using latent representations to link disjoint longitudinal data for\n  mixed-effects regression"
                },
                "summary": "Many rare diseases offer limited established treatment options, leading\npatients to switch therapies when new medications emerge. To analyze the impact\nof such treatment switches within the low sample size limitations of rare\ndisease trials, it is important to use all available data sources. This,\nhowever, is complicated when usage of measurement instruments change during the\nobservation period, for example when instruments are adapted to specific age\nranges. The resulting disjoint longitudinal data trajectories, complicate the\napplication of traditional modeling approaches like mixed-effects regression.\nWe tackle this by mapping observations of each instrument to a aligned\nlow-dimensional temporal trajectory, enabling longitudinal modeling across\ninstruments. Specifically, we employ a set of variational autoencoder\narchitectures to embed item values into a shared latent space for each time\npoint. Temporal disease dynamics and treatment switch effects are then captured\nthrough a mixed-effects regression model applied to latent representations. To\nenable statistical inference, we present a novel statistical testing approach\nthat accounts for the joint parameter estimation of mixed-effects regression\nand variational autoencoders. The methodology is applied to quantify the impact\nof treatment switches for patients with spinal muscular atrophy. Here, our\napproach aligns motor performance items from different measurement instruments\nfor mixed-effects regression and maps estimated effects back to the observed\nitem level to quantify the treatment switch effect. Our approach allows for\nmodel selection as well as for assessing effects of treatment switching. The\nresults highlight the potential of modeling in joint latent representations for\naddressing small data challenges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many rare diseases offer limited established treatment options, leading\npatients to switch therapies when new medications emerge. To analyze the impact\nof such treatment switches within the low sample size limitations of rare\ndisease trials, it is important to use all available data sources. This,\nhowever, is complicated when usage of measurement instruments change during the\nobservation period, for example when instruments are adapted to specific age\nranges. The resulting disjoint longitudinal data trajectories, complicate the\napplication of traditional modeling approaches like mixed-effects regression.\nWe tackle this by mapping observations of each instrument to a aligned\nlow-dimensional temporal trajectory, enabling longitudinal modeling across\ninstruments. Specifically, we employ a set of variational autoencoder\narchitectures to embed item values into a shared latent space for each time\npoint. Temporal disease dynamics and treatment switch effects are then captured\nthrough a mixed-effects regression model applied to latent representations. To\nenable statistical inference, we present a novel statistical testing approach\nthat accounts for the joint parameter estimation of mixed-effects regression\nand variational autoencoders. The methodology is applied to quantify the impact\nof treatment switches for patients with spinal muscular atrophy. Here, our\napproach aligns motor performance items from different measurement instruments\nfor mixed-effects regression and maps estimated effects back to the observed\nitem level to quantify the treatment switch effect. Our approach allows for\nmodel selection as well as for assessing effects of treatment switching. The\nresults highlight the potential of modeling in joint latent representations for\naddressing small data challenges."
                },
                "authors": [
                    {
                        "name": "Clemens Schchter"
                    },
                    {
                        "name": "Maren Hackenberg"
                    },
                    {
                        "name": "Michelle Pfaffenlehner"
                    },
                    {
                        "name": "Flix B. Tambe-Ndonfack"
                    },
                    {
                        "name": "Thorsten Schmidt"
                    },
                    {
                        "name": "Astrid Pechmann"
                    },
                    {
                        "name": "Janbernd Kirschner"
                    },
                    {
                        "name": "Jan Hasenauer"
                    },
                    {
                        "name": "Harald Binder"
                    }
                ],
                "author_detail": {
                    "name": "Harald Binder"
                },
                "author": "Harald Binder",
                "arxiv_comment": "31 pages, 3 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25531v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25531v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "G.3; I.2.6; J.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03675v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03675v1",
                "updated": "2025-11-05T17:47:46Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    17,
                    47,
                    46,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T17:47:46Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    17,
                    47,
                    46,
                    2,
                    309,
                    0
                ],
                "title": "Whisper Leak: a side-channel attack on Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Whisper Leak: a side-channel attack on Large Language Models"
                },
                "summary": "Large Language Models (LLMs) are increasingly deployed in sensitive domains\nincluding healthcare, legal services, and confidential communications, where\nprivacy is paramount. This paper introduces Whisper Leak, a side-channel attack\nthat infers user prompt topics from encrypted LLM traffic by analyzing packet\nsize and timing patterns in streaming responses. Despite TLS encryption\nprotecting content, these metadata patterns leak sufficient information to\nenable topic classification. We demonstrate the attack across 28 popular LLMs\nfrom major providers, achieving near-perfect classification (often >98% AUPRC)\nand high precision even at extreme class imbalance (10,000:1 noise-to-target\nratio). For many models, we achieve 100% precision in identifying sensitive\ntopics like \"money laundering\" while recovering 5-20% of target conversations.\nThis industry-wide vulnerability poses significant risks for users under\nnetwork surveillance by ISPs, governments, or local adversaries. We evaluate\nthree mitigation strategies - random padding, token batching, and packet\ninjection - finding that while each reduces attack effectiveness, none provides\ncomplete protection. Through responsible disclosure, we have collaborated with\nproviders to implement initial countermeasures. Our findings underscore the\nneed for LLM providers to address metadata leakage as AI systems handle\nincreasingly sensitive information.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly deployed in sensitive domains\nincluding healthcare, legal services, and confidential communications, where\nprivacy is paramount. This paper introduces Whisper Leak, a side-channel attack\nthat infers user prompt topics from encrypted LLM traffic by analyzing packet\nsize and timing patterns in streaming responses. Despite TLS encryption\nprotecting content, these metadata patterns leak sufficient information to\nenable topic classification. We demonstrate the attack across 28 popular LLMs\nfrom major providers, achieving near-perfect classification (often >98% AUPRC)\nand high precision even at extreme class imbalance (10,000:1 noise-to-target\nratio). For many models, we achieve 100% precision in identifying sensitive\ntopics like \"money laundering\" while recovering 5-20% of target conversations.\nThis industry-wide vulnerability poses significant risks for users under\nnetwork surveillance by ISPs, governments, or local adversaries. We evaluate\nthree mitigation strategies - random padding, token batching, and packet\ninjection - finding that while each reduces attack effectiveness, none provides\ncomplete protection. Through responsible disclosure, we have collaborated with\nproviders to implement initial countermeasures. Our findings underscore the\nneed for LLM providers to address metadata leakage as AI systems handle\nincreasingly sensitive information."
                },
                "authors": [
                    {
                        "name": "Geoff McDonald"
                    },
                    {
                        "name": "Jonathan Bar Or"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan Bar Or"
                },
                "author": "Jonathan Bar Or",
                "arxiv_comment": "14 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03675v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03675v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "K.4.1; C.2.0; K.6.5; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16638v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16638v4",
                "updated": "2025-11-05T17:42:36Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    17,
                    42,
                    36,
                    2,
                    309,
                    0
                ],
                "published": "2024-11-25T18:15:15Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    18,
                    15,
                    15,
                    0,
                    330,
                    0
                ],
                "title": "Do Automatic Factuality Metrics Measure Factuality? A Critical\n  Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Automatic Factuality Metrics Measure Factuality? A Critical\n  Evaluation"
                },
                "summary": "Modern LLMs can now produce highly readable abstractive summaries, to the\npoint that traditional automated metrics for evaluating summary quality, such\nas ROUGE, have saturated. However, LLMs still sometimes introduce inaccuracies\ninto summaries, i.e., information inconsistent with or unsupported by the\ncorresponding source. Measuring the occurrence of these often subtle factual\ninconsistencies automatically has proved challenging. This in turn has\nmotivated development of metrics intended to measure the factual consistency of\ngenerated summaries against sources. But are these approaches measuring what\nthey purport to? Or are they mostly exploiting artifacts? In this work, we\nstress test a range of automatic factuality metrics, including specialized\nmodels and LLM-based prompting methods, to probe what they actually capture.\nUsing a shallow classifier to separate ``easy'' examples for factual evaluation\nwhere surface features suffice from ``hard'' cases requiring deeper reasoning,\nwe find that all metrics show substantial performance drops on the latter.\nFurthermore, some metrics are more sensitive to benign, fact-preserving edits\nthan to factual corrections. Building on this observation, we demonstrate that\nmost automatic factuality metrics can be gamed, i.e., their scores can be\nartificially inflated by appending innocuous, content-free sentences to\nsummaries. Among the metrics tested, the prompt based ChatGPT-DA approach is\nthe most robust and reliable. However, this comes with a notable caveat:\nPrompting LLMs to assess factuality may overly rely on their parametric\nknowledge rather than the provided reference when making judgments. Taken\ntogether, our findings call into question the reliability of current factuality\nmetrics and prompt a broader reflection on what these metrics are truly\nmeasuring.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern LLMs can now produce highly readable abstractive summaries, to the\npoint that traditional automated metrics for evaluating summary quality, such\nas ROUGE, have saturated. However, LLMs still sometimes introduce inaccuracies\ninto summaries, i.e., information inconsistent with or unsupported by the\ncorresponding source. Measuring the occurrence of these often subtle factual\ninconsistencies automatically has proved challenging. This in turn has\nmotivated development of metrics intended to measure the factual consistency of\ngenerated summaries against sources. But are these approaches measuring what\nthey purport to? Or are they mostly exploiting artifacts? In this work, we\nstress test a range of automatic factuality metrics, including specialized\nmodels and LLM-based prompting methods, to probe what they actually capture.\nUsing a shallow classifier to separate ``easy'' examples for factual evaluation\nwhere surface features suffice from ``hard'' cases requiring deeper reasoning,\nwe find that all metrics show substantial performance drops on the latter.\nFurthermore, some metrics are more sensitive to benign, fact-preserving edits\nthan to factual corrections. Building on this observation, we demonstrate that\nmost automatic factuality metrics can be gamed, i.e., their scores can be\nartificially inflated by appending innocuous, content-free sentences to\nsummaries. Among the metrics tested, the prompt based ChatGPT-DA approach is\nthe most robust and reliable. However, this comes with a notable caveat:\nPrompting LLMs to assess factuality may overly rely on their parametric\nknowledge rather than the provided reference when making judgments. Taken\ntogether, our findings call into question the reliability of current factuality\nmetrics and prompt a broader reflection on what these metrics are truly\nmeasuring."
                },
                "authors": [
                    {
                        "name": "Sanjana Ramprasad"
                    },
                    {
                        "name": "Byron C. Wallace"
                    }
                ],
                "author_detail": {
                    "name": "Byron C. Wallace"
                },
                "author": "Byron C. Wallace",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16638v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16638v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02802v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02802v2",
                "updated": "2025-11-05T17:36:30Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    17,
                    36,
                    30,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-04T18:25:17Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    18,
                    25,
                    17,
                    1,
                    308,
                    0
                ],
                "title": "TabTune: A Unified Library for Inference and Fine-Tuning Tabular\n  Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TabTune: A Unified Library for Inference and Fine-Tuning Tabular\n  Foundation Models"
                },
                "summary": "Tabular foundation models represent a growing paradigm in structured data\nlearning, extending the benefits of large-scale pretraining to tabular domains.\nHowever, their adoption remains limited due to heterogeneous preprocessing\npipelines, fragmented APIs, inconsistent fine-tuning procedures, and the\nabsence of standardized evaluation for deployment-oriented metrics such as\ncalibration and fairness. We present TabTune, a unified library that\nstandardizes the complete workflow for tabular foundation models through a\nsingle interface. TabTune provides consistent access to seven state-of-the-art\nmodels supporting multiple adaptation strategies, including zero-shot\ninference, meta-learning, supervised fine-tuning (SFT), and parameter-efficient\nfine-tuning (PEFT). The framework automates model-aware preprocessing, manages\narchitectural heterogeneity internally, and integrates evaluation modules for\nperformance, calibration, and fairness. Designed for extensibility and\nreproducibility, TabTune enables consistent benchmarking of adaptation\nstrategies of tabular foundation models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tabular foundation models represent a growing paradigm in structured data\nlearning, extending the benefits of large-scale pretraining to tabular domains.\nHowever, their adoption remains limited due to heterogeneous preprocessing\npipelines, fragmented APIs, inconsistent fine-tuning procedures, and the\nabsence of standardized evaluation for deployment-oriented metrics such as\ncalibration and fairness. We present TabTune, a unified library that\nstandardizes the complete workflow for tabular foundation models through a\nsingle interface. TabTune provides consistent access to seven state-of-the-art\nmodels supporting multiple adaptation strategies, including zero-shot\ninference, meta-learning, supervised fine-tuning (SFT), and parameter-efficient\nfine-tuning (PEFT). The framework automates model-aware preprocessing, manages\narchitectural heterogeneity internally, and integrates evaluation modules for\nperformance, calibration, and fairness. Designed for extensibility and\nreproducibility, TabTune enables consistent benchmarking of adaptation\nstrategies of tabular foundation models."
                },
                "authors": [
                    {
                        "name": "Aditya Tanna"
                    },
                    {
                        "name": "Pratinav Seth"
                    },
                    {
                        "name": "Mohamed Bouadi"
                    },
                    {
                        "name": "Utsav Avaiya"
                    },
                    {
                        "name": "Vinay Kumar Sankarapu"
                    }
                ],
                "author_detail": {
                    "name": "Vinay Kumar Sankarapu"
                },
                "author": "Vinay Kumar Sankarapu",
                "arxiv_comment": "The library is open source and available at\n  https://github.com/Lexsi-Labs/TabTune",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02802v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02802v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03667v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03667v1",
                "updated": "2025-11-05T17:33:44Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    17,
                    33,
                    44,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T17:33:44Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    17,
                    33,
                    44,
                    2,
                    309,
                    0
                ],
                "title": "Addressing prior dependence in hierarchical Bayesian modeling for PTA\n  data analysis I: Methodology and implementation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Addressing prior dependence in hierarchical Bayesian modeling for PTA\n  data analysis I: Methodology and implementation"
                },
                "summary": "Complex inference tasks, such as those encountered in Pulsar Timing Array\n(PTA) data analysis, rely on Bayesian frameworks. The high-dimensional\nparameter space and the strong interdependencies among astrophysical, pulsar\nnoise, and nuisance parameters introduce significant challenges for efficient\nlearning and robust inference. These challenges are emblematic of broader\nissues in decision science, where model over-parameterization and prior\nsensitivity can compromise both computational tractability and the reliability\nof the results. We address these issues in the framework of hierarchical\nBayesian modeling by introducing a reparameterization strategy. Our approach\nemploys Normalizing Flows (NFs) to decorrelate the parameters governing\nhierarchical priors from those of astrophysical interest. The use of NF-based\nmappings provides both the flexibility to realize the reparametrization and the\ntractability to preserve proper probability densities. We further adopt\ni-nessai, a flow-guided nested sampler, to accelerate exploration of complex\nposteriors. This unified use of NFs improves statistical robustness and\ncomputational efficiency, providing a principled methodology for addressing\nhierarchical Bayesian inference in PTA analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Complex inference tasks, such as those encountered in Pulsar Timing Array\n(PTA) data analysis, rely on Bayesian frameworks. The high-dimensional\nparameter space and the strong interdependencies among astrophysical, pulsar\nnoise, and nuisance parameters introduce significant challenges for efficient\nlearning and robust inference. These challenges are emblematic of broader\nissues in decision science, where model over-parameterization and prior\nsensitivity can compromise both computational tractability and the reliability\nof the results. We address these issues in the framework of hierarchical\nBayesian modeling by introducing a reparameterization strategy. Our approach\nemploys Normalizing Flows (NFs) to decorrelate the parameters governing\nhierarchical priors from those of astrophysical interest. The use of NF-based\nmappings provides both the flexibility to realize the reparametrization and the\ntractability to preserve proper probability densities. We further adopt\ni-nessai, a flow-guided nested sampler, to accelerate exploration of complex\nposteriors. This unified use of NFs improves statistical robustness and\ncomputational efficiency, providing a principled methodology for addressing\nhierarchical Bayesian inference in PTA analysis."
                },
                "authors": [
                    {
                        "name": "Luigi D'amico"
                    },
                    {
                        "name": "Eleonora Villa"
                    },
                    {
                        "name": "Fatima Modica Bittordo"
                    },
                    {
                        "name": "Aldo Barca"
                    },
                    {
                        "name": "Francesco Al"
                    },
                    {
                        "name": "Massimo Meneghetti"
                    },
                    {
                        "name": "Luca Naso"
                    }
                ],
                "author_detail": {
                    "name": "Luca Naso"
                },
                "author": "Luca Naso",
                "arxiv_comment": "15 pages, 7 figures. Under review for Proceedings of International\n  Summer Conference 2025: Intelligent Systems & Decision Making: Human Insights\n  in the Era of A.I - Lecture Notes in Computer Science, Springer Nature",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03667v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03667v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20749v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20749v3",
                "updated": "2025-11-05T17:33:06Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    17,
                    33,
                    6,
                    2,
                    309,
                    0
                ],
                "published": "2024-10-28T05:28:51Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    5,
                    28,
                    51,
                    0,
                    302,
                    0
                ],
                "title": "Matryoshka Pilot: Learning to Drive Black-Box LLMs with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Matryoshka Pilot: Learning to Drive Black-Box LLMs with LLMs"
                },
                "summary": "Despite the impressive generative abilities of black-box large language\nmodels (LLMs), their inherent opacity hinders further advancements in\ncapabilities such as reasoning, planning, and personalization. Existing works\naim to enhance LLM capabilities via domain-specific adaptation, which require\nadditional training on accessible model parameters, an infeasible option for\nblack-box LLMs. To address this challenge, we introduce Matryoshka Pilot\n(M-Pilot), a lightweight white-box LLM controller that guides a large-scale\nblack-box LLM generator by decomposing complex tasks into a series of\nintermediate outputs. Specifically, we consider the black-box LLM as an\nenvironment, with M-Pilot serving as a policy to provide intermediate guidance\nthrough prompts for driving the black-box LLM. M-Pilot is trained to pivot the\noutputs of the black-box LLM aligning with preferences during iterative\ninteraction, which enables controllable multi-turn generation and\nself-improvement in optimizing intermediate guidance. Empirical evaluations on\ndiverse tasks demonstrate that our method effectively enhances the capabilities\nof black-box LLMs in complex, long-horizon tasks. Our code is publicly\navailable at: https://github.com/lichangh20/Matryoshka.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the impressive generative abilities of black-box large language\nmodels (LLMs), their inherent opacity hinders further advancements in\ncapabilities such as reasoning, planning, and personalization. Existing works\naim to enhance LLM capabilities via domain-specific adaptation, which require\nadditional training on accessible model parameters, an infeasible option for\nblack-box LLMs. To address this challenge, we introduce Matryoshka Pilot\n(M-Pilot), a lightweight white-box LLM controller that guides a large-scale\nblack-box LLM generator by decomposing complex tasks into a series of\nintermediate outputs. Specifically, we consider the black-box LLM as an\nenvironment, with M-Pilot serving as a policy to provide intermediate guidance\nthrough prompts for driving the black-box LLM. M-Pilot is trained to pivot the\noutputs of the black-box LLM aligning with preferences during iterative\ninteraction, which enables controllable multi-turn generation and\nself-improvement in optimizing intermediate guidance. Empirical evaluations on\ndiverse tasks demonstrate that our method effectively enhances the capabilities\nof black-box LLMs in complex, long-horizon tasks. Our code is publicly\navailable at: https://github.com/lichangh20/Matryoshka."
                },
                "authors": [
                    {
                        "name": "Changhao Li"
                    },
                    {
                        "name": "Yuchen Zhuang"
                    },
                    {
                        "name": "Rushi Qiang"
                    },
                    {
                        "name": "Haotian Sun"
                    },
                    {
                        "name": "Hanjun Dai"
                    },
                    {
                        "name": "Chao Zhang"
                    },
                    {
                        "name": "Bo Dai"
                    }
                ],
                "author_detail": {
                    "name": "Bo Dai"
                },
                "author": "Bo Dai",
                "arxiv_comment": "Accepted by NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20749v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20749v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03666v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03666v1",
                "updated": "2025-11-05T17:33:03Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    17,
                    33,
                    3,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T17:33:03Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    17,
                    33,
                    3,
                    2,
                    309,
                    0
                ],
                "title": "Part-Aware Bottom-Up Group Reasoning for Fine-Grained Social Interaction\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Part-Aware Bottom-Up Group Reasoning for Fine-Grained Social Interaction\n  Detection"
                },
                "summary": "Social interactions often emerge from subtle, fine-grained cues such as\nfacial expressions, gaze, and gestures. However, existing methods for social\ninteraction detection overlook such nuanced cues and primarily rely on holistic\nrepresentations of individuals. Moreover, they directly detect social groups\nwithout explicitly modeling the underlying interactions between individuals.\nThese drawbacks limit their ability to capture localized social signals and\nintroduce ambiguity when group configurations should be inferred from social\ninteractions grounded in nuanced cues. In this work, we propose a part-aware\nbottom-up group reasoning framework for fine-grained social interaction\ndetection. The proposed method infers social groups and their interactions\nusing body part features and their interpersonal relations. Our model first\ndetects individuals and enhances their features using part-aware cues, and then\ninfers group configuration by associating individuals via similarity-based\nreasoning, which considers not only spatial relations but also subtle social\ncues that signal interactions, leading to more accurate group inference.\nExperiments on the NVI dataset demonstrate that our method outperforms prior\nmethods, achieving the new state of the art.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Social interactions often emerge from subtle, fine-grained cues such as\nfacial expressions, gaze, and gestures. However, existing methods for social\ninteraction detection overlook such nuanced cues and primarily rely on holistic\nrepresentations of individuals. Moreover, they directly detect social groups\nwithout explicitly modeling the underlying interactions between individuals.\nThese drawbacks limit their ability to capture localized social signals and\nintroduce ambiguity when group configurations should be inferred from social\ninteractions grounded in nuanced cues. In this work, we propose a part-aware\nbottom-up group reasoning framework for fine-grained social interaction\ndetection. The proposed method infers social groups and their interactions\nusing body part features and their interpersonal relations. Our model first\ndetects individuals and enhances their features using part-aware cues, and then\ninfers group configuration by associating individuals via similarity-based\nreasoning, which considers not only spatial relations but also subtle social\ncues that signal interactions, leading to more accurate group inference.\nExperiments on the NVI dataset demonstrate that our method outperforms prior\nmethods, achieving the new state of the art."
                },
                "authors": [
                    {
                        "name": "Dongkeun Kim"
                    },
                    {
                        "name": "Minsu Cho"
                    },
                    {
                        "name": "Suha Kwak"
                    }
                ],
                "author_detail": {
                    "name": "Suha Kwak"
                },
                "author": "Suha Kwak",
                "arxiv_comment": "Accepted to NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03666v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03666v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03663v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03663v1",
                "updated": "2025-11-05T17:26:05Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    17,
                    26,
                    5,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T17:26:05Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    17,
                    26,
                    5,
                    2,
                    309,
                    0
                ],
                "title": "3D Full Spectrum Fitting: Algorithm Comparison",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D Full Spectrum Fitting: Algorithm Comparison"
                },
                "summary": "Full spectrum fitting is the prevailing method for extracting stellar\nkinematic and population measurements from 1D galaxy spectra. 3D methods refer\nto analysis of Integral Field Spectroscopy (IFS) data where spatial and\nspectral dimensions are modelled simultaneously. While several 3D methods exist\nfor modelling gas structures there has been less investigation into the more\ncomputationally demanding problem of 3D full spectrum fitting for stellar\nrecoveries. This work introduces and compares two algorithms for this task: the\nProjected Nesterov Kaczmarz Reconstruction method (PNKR) and a version of the\nBayes-LOSVD software which has been modified to account for spatial\ncorrelations. We aim to understand strengths and weaknesses of both algorithms\nand assess the impact of 3D methods for stellar inferences. We apply both\nrecovery algorithms to a mock IFS data over a signal-to-noise ratio (SNR) range\nfrom 20-200 and evaluate the quality of the recoveries compared to the known\nground truth. Accounting for spatial correlations in Bayes-LOSVD significantly\nimproved the accuracy and precision of kinematic recoveries. 3D modelling with\nPNKR did not provide any significant improvement over 1D fits however, for\nSNR>40, PNKR did recover the most accurate kinematics overall. Additionally, by\nmodelling the joint distribution over kinematics and populations, PNKR could\nsuccessfully infer trends between these quantities e.g. inferring local\nmetallicity-velocity trends, albeit with a significant bias on the absolute\nmetallicity. Having demonstrated advantages of (i) 3D modelling with\nBayes-LOSVD, and (ii) joint kinematic-population analyses with PNKR, we\nconclude that both methodological advances will prove useful for detecting and\ncharacterising stellar structures from IFS data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Full spectrum fitting is the prevailing method for extracting stellar\nkinematic and population measurements from 1D galaxy spectra. 3D methods refer\nto analysis of Integral Field Spectroscopy (IFS) data where spatial and\nspectral dimensions are modelled simultaneously. While several 3D methods exist\nfor modelling gas structures there has been less investigation into the more\ncomputationally demanding problem of 3D full spectrum fitting for stellar\nrecoveries. This work introduces and compares two algorithms for this task: the\nProjected Nesterov Kaczmarz Reconstruction method (PNKR) and a version of the\nBayes-LOSVD software which has been modified to account for spatial\ncorrelations. We aim to understand strengths and weaknesses of both algorithms\nand assess the impact of 3D methods for stellar inferences. We apply both\nrecovery algorithms to a mock IFS data over a signal-to-noise ratio (SNR) range\nfrom 20-200 and evaluate the quality of the recoveries compared to the known\nground truth. Accounting for spatial correlations in Bayes-LOSVD significantly\nimproved the accuracy and precision of kinematic recoveries. 3D modelling with\nPNKR did not provide any significant improvement over 1D fits however, for\nSNR>40, PNKR did recover the most accurate kinematics overall. Additionally, by\nmodelling the joint distribution over kinematics and populations, PNKR could\nsuccessfully infer trends between these quantities e.g. inferring local\nmetallicity-velocity trends, albeit with a significant bias on the absolute\nmetallicity. Having demonstrated advantages of (i) 3D modelling with\nBayes-LOSVD, and (ii) joint kinematic-population analyses with PNKR, we\nconclude that both methodological advances will prove useful for detecting and\ncharacterising stellar structures from IFS data."
                },
                "authors": [
                    {
                        "name": "Prashin Jethwa"
                    },
                    {
                        "name": "Simon Hubmer"
                    },
                    {
                        "name": "Ronny Ramlau"
                    },
                    {
                        "name": "Glenn Van de Ven"
                    }
                ],
                "author_detail": {
                    "name": "Glenn Van de Ven"
                },
                "author": "Glenn Van de Ven",
                "arxiv_comment": "Submitted to A&A",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03663v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03663v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14526v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14526v2",
                "updated": "2025-11-05T17:12:59Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    17,
                    12,
                    59,
                    2,
                    309,
                    0
                ],
                "published": "2025-05-20T15:48:23Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    15,
                    48,
                    23,
                    1,
                    140,
                    0
                ],
                "title": "RoboRAN: A Unified Robotics Framework for Reinforcement Learning-Based\n  Autonomous Navigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RoboRAN: A Unified Robotics Framework for Reinforcement Learning-Based\n  Autonomous Navigation"
                },
                "summary": "Autonomous robots must navigate and operate in diverse environments, from\nterrestrial and aquatic settings to aerial and space domains. While\nReinforcement Learning (RL) has shown promise in training policies for specific\nautonomous robots, existing frameworks and benchmarks are often constrained to\nunique platforms, limiting generalization and fair comparisons across different\nmobility systems. In this paper, we present a multi-domain framework for\ntraining, evaluating and deploying RL-based navigation policies across diverse\nrobotic platforms and operational environments. Our work presents four key\ncontributions: (1) a scalable and modular framework, facilitating seamless\nrobot-task interchangeability and reproducible training pipelines; (2)\nsim-to-real transfer demonstrated through real-world experiments with multiple\nrobots, including a satellite robotic simulator, an unmanned surface vessel,\nand a wheeled ground vehicle; (3) the release of the first open-source API for\ndeploying Isaac Lab-trained policies to real robots, enabling lightweight\ninference and rapid field validation; and (4) uniform tasks and metrics for\ncross-medium evaluation, through a unified evaluation testbed to assess\nperformance of navigation tasks in diverse operational conditions (aquatic,\nterrestrial and space). By ensuring consistency between simulation and\nreal-world deployment, RoboRAN lowers the barrier to developing adaptable\nRL-based navigation strategies. Its modular design enables straightforward\nintegration of new robots and tasks through predefined templates, fostering\nreproducibility and extension to diverse domains. To support the community, we\nrelease RoboRAN as open-source.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous robots must navigate and operate in diverse environments, from\nterrestrial and aquatic settings to aerial and space domains. While\nReinforcement Learning (RL) has shown promise in training policies for specific\nautonomous robots, existing frameworks and benchmarks are often constrained to\nunique platforms, limiting generalization and fair comparisons across different\nmobility systems. In this paper, we present a multi-domain framework for\ntraining, evaluating and deploying RL-based navigation policies across diverse\nrobotic platforms and operational environments. Our work presents four key\ncontributions: (1) a scalable and modular framework, facilitating seamless\nrobot-task interchangeability and reproducible training pipelines; (2)\nsim-to-real transfer demonstrated through real-world experiments with multiple\nrobots, including a satellite robotic simulator, an unmanned surface vessel,\nand a wheeled ground vehicle; (3) the release of the first open-source API for\ndeploying Isaac Lab-trained policies to real robots, enabling lightweight\ninference and rapid field validation; and (4) uniform tasks and metrics for\ncross-medium evaluation, through a unified evaluation testbed to assess\nperformance of navigation tasks in diverse operational conditions (aquatic,\nterrestrial and space). By ensuring consistency between simulation and\nreal-world deployment, RoboRAN lowers the barrier to developing adaptable\nRL-based navigation strategies. Its modular design enables straightforward\nintegration of new robots and tasks through predefined templates, fostering\nreproducibility and extension to diverse domains. To support the community, we\nrelease RoboRAN as open-source."
                },
                "authors": [
                    {
                        "name": "Matteo El-Hariry"
                    },
                    {
                        "name": "Antoine Richard"
                    },
                    {
                        "name": "Ricard M. Castan"
                    },
                    {
                        "name": "Luis F. W. Batista"
                    },
                    {
                        "name": "Matthieu Geist"
                    },
                    {
                        "name": "Cedric Pradalier"
                    },
                    {
                        "name": "Miguel Olivares-Mendez"
                    }
                ],
                "author_detail": {
                    "name": "Miguel Olivares-Mendez"
                },
                "author": "Miguel Olivares-Mendez",
                "arxiv_comment": "Accepted at Transactions on Machine Learning Research (TMLR)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14526v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14526v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11857v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11857v2",
                "updated": "2025-11-05T17:05:02Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    17,
                    5,
                    2,
                    2,
                    309,
                    0
                ],
                "published": "2025-06-13T15:04:01Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    15,
                    4,
                    1,
                    4,
                    164,
                    0
                ],
                "title": "Post Persona Alignment for Multi-Session Dialogue Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post Persona Alignment for Multi-Session Dialogue Generation"
                },
                "summary": "Multi-session persona-based dialogue generation presents challenges in\nmaintaining long-term consistency and generating diverse, personalized\nresponses. While large language models (LLMs) excel in single-session\ndialogues, they struggle to preserve persona fidelity and conversational\ncoherence across extended interactions. Existing methods typically retrieve\npersona information before response generation, which can constrain diversity\nand result in generic outputs. We propose Post Persona Alignment (PPA), a novel\ntwo-stage framework that reverses this process. PPA first generates a general\nresponse based solely on dialogue context, then retrieves relevant persona\nmemories using the response as a query, and finally refines the response to\nalign with the speaker's persona. This post-hoc alignment strategy promotes\nnaturalness and diversity while preserving consistency and personalization.\nExperiments on multi-session LLM-generated dialogue data demonstrate that PPA\nsignificantly outperforms prior approaches in consistency, diversity, and\npersona relevance, offering a more flexible and effective paradigm for\nlong-term personalized dialogue generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-session persona-based dialogue generation presents challenges in\nmaintaining long-term consistency and generating diverse, personalized\nresponses. While large language models (LLMs) excel in single-session\ndialogues, they struggle to preserve persona fidelity and conversational\ncoherence across extended interactions. Existing methods typically retrieve\npersona information before response generation, which can constrain diversity\nand result in generic outputs. We propose Post Persona Alignment (PPA), a novel\ntwo-stage framework that reverses this process. PPA first generates a general\nresponse based solely on dialogue context, then retrieves relevant persona\nmemories using the response as a query, and finally refines the response to\nalign with the speaker's persona. This post-hoc alignment strategy promotes\nnaturalness and diversity while preserving consistency and personalization.\nExperiments on multi-session LLM-generated dialogue data demonstrate that PPA\nsignificantly outperforms prior approaches in consistency, diversity, and\npersona relevance, offering a more flexible and effective paradigm for\nlong-term personalized dialogue generation."
                },
                "authors": [
                    {
                        "name": "Yi-Pei Chen"
                    },
                    {
                        "name": "Noriki Nishida"
                    },
                    {
                        "name": "Hideki Nakayama"
                    },
                    {
                        "name": "Yuji Matsumoto"
                    }
                ],
                "author_detail": {
                    "name": "Yuji Matsumoto"
                },
                "author": "Yuji Matsumoto",
                "arxiv_comment": "EMNLP 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11857v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11857v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03641v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03641v1",
                "updated": "2025-11-05T17:00:39Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    17,
                    0,
                    39,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T17:00:39Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    17,
                    0,
                    39,
                    2,
                    309,
                    0
                ],
                "title": "Watermarking Large Language Models in Europe: Interpreting the AI Act in\n  Light of Technology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Watermarking Large Language Models in Europe: Interpreting the AI Act in\n  Light of Technology"
                },
                "summary": "To foster trustworthy Artificial Intelligence (AI) within the European Union,\nthe AI Act requires providers to mark and detect the outputs of their\ngeneral-purpose models. The Article 50 and Recital 133 call for marking methods\nthat are ''sufficiently reliable, interoperable, effective and robust''. Yet,\nthe rapidly evolving and heterogeneous landscape of watermarks for Large\nLanguage Models (LLMs) makes it difficult to determine how these four standards\ncan be translated into concrete and measurable evaluations. Our paper addresses\nthis challenge, anchoring the normativity of European requirements in the\nmultiplicity of watermarking techniques. Introducing clear and distinct\nconcepts on LLM watermarking, our contribution is threefold. (1) Watermarking\nCategorisation: We propose an accessible taxonomy of watermarking methods\naccording to the stage of the LLM lifecycle at which they are applied - before,\nduring, or after training, and during next-token distribution or sampling. (2)\nWatermarking Evaluation: We interpret the EU AI Act's requirements by mapping\neach criterion with state-of-the-art evaluations on robustness and\ndetectability of the watermark, and of quality of the LLM. Since\ninteroperability remains largely untheorised in LLM watermarking research, we\npropose three normative dimensions to frame its assessment. (3) Watermarking\nComparison: We compare current watermarking methods for LLMs against the\noperationalised European criteria and show that no approach yet satisfies all\nfour standards. Encouraged by emerging empirical tests, we recommend further\nresearch into watermarking directly embedded within the low-level architecture\nof LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To foster trustworthy Artificial Intelligence (AI) within the European Union,\nthe AI Act requires providers to mark and detect the outputs of their\ngeneral-purpose models. The Article 50 and Recital 133 call for marking methods\nthat are ''sufficiently reliable, interoperable, effective and robust''. Yet,\nthe rapidly evolving and heterogeneous landscape of watermarks for Large\nLanguage Models (LLMs) makes it difficult to determine how these four standards\ncan be translated into concrete and measurable evaluations. Our paper addresses\nthis challenge, anchoring the normativity of European requirements in the\nmultiplicity of watermarking techniques. Introducing clear and distinct\nconcepts on LLM watermarking, our contribution is threefold. (1) Watermarking\nCategorisation: We propose an accessible taxonomy of watermarking methods\naccording to the stage of the LLM lifecycle at which they are applied - before,\nduring, or after training, and during next-token distribution or sampling. (2)\nWatermarking Evaluation: We interpret the EU AI Act's requirements by mapping\neach criterion with state-of-the-art evaluations on robustness and\ndetectability of the watermark, and of quality of the LLM. Since\ninteroperability remains largely untheorised in LLM watermarking research, we\npropose three normative dimensions to frame its assessment. (3) Watermarking\nComparison: We compare current watermarking methods for LLMs against the\noperationalised European criteria and show that no approach yet satisfies all\nfour standards. Encouraged by emerging empirical tests, we recommend further\nresearch into watermarking directly embedded within the low-level architecture\nof LLMs."
                },
                "authors": [
                    {
                        "name": "Thomas Souverain"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Souverain"
                },
                "author": "Thomas Souverain",
                "arxiv_comment": "17 pages, 2 Tables and 2 Pictures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03641v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03641v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T01, 68727, 68T30, 68T35, 68T37, 68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03636v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03636v1",
                "updated": "2025-11-05T16:54:17Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    16,
                    54,
                    17,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T16:54:17Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    16,
                    54,
                    17,
                    2,
                    309,
                    0
                ],
                "title": "Quantifying Weighted Morphological Content of Large-Scale Structures via\n  Simulation-Based Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantifying Weighted Morphological Content of Large-Scale Structures via\n  Simulation-Based Inference"
                },
                "summary": "In this work, we perform a simulation-based forecasting analysis to compare\nthe constraining power of two higher-order summary statistics of the\nlarge-scale structure (LSS), the Minkowski Functionals (MFs) and the\nConditional Moments of Derivative (CMD), with a particular focus on their\nsensitivity to nonlinear and anisotropic features in redshift-space. Our\nanalysis relies on halo catalogs from the Big Sobol Sequence(BSQ) simulations\nat redshift $z=0.5$, employing a likelihood-free inference framework\nimplemented via neural posterior estimation. At the fiducial cosmology of the\nQuijote simulations $(\\Omega_{m}=0.3175,\\,\\sigma_{8}=0.834)$, and for the\nsmoothing scale $R=15\\,h^{-1}$Mpc, we find that the CMD yields tighter\nforecasts for $(\\Omega_{m}},\\,\\sigma_{8})$ than the zeroth- to third-order MFs\ncomponents, improving the constraint precision by ${\\sim}(44\\%,\\,52\\%)$,\n${\\sim}(30\\%,\\,45\\%)$, ${\\sim}(27\\%,\\,17\\%)$, and ${\\sim}(26\\%,\\,17\\%)$,\nrespectively. A joint configuration combining the MFs and CMD further enhances\nthe precision by approximately ${\\sim}27\\%$ compared to the standard MFs alone,\nhighlighting the complementary anisotropy-sensitive information captured by the\nCMD in contrast to the scalar morphological content encapsulated by the MFs. We\nfurther extend the forecasting analysis to a continuous range of cosmological\nparameter values and multiple smoothing scales. Our results show that, although\nthe absolute forecast uncertainty for each component of summary statistics\ndepends on the underlying parameter values and the adopted smoothing scale, the\nrelative constraining power among the summary statistics remains nearly\nconstant throughout.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we perform a simulation-based forecasting analysis to compare\nthe constraining power of two higher-order summary statistics of the\nlarge-scale structure (LSS), the Minkowski Functionals (MFs) and the\nConditional Moments of Derivative (CMD), with a particular focus on their\nsensitivity to nonlinear and anisotropic features in redshift-space. Our\nanalysis relies on halo catalogs from the Big Sobol Sequence(BSQ) simulations\nat redshift $z=0.5$, employing a likelihood-free inference framework\nimplemented via neural posterior estimation. At the fiducial cosmology of the\nQuijote simulations $(\\Omega_{m}=0.3175,\\,\\sigma_{8}=0.834)$, and for the\nsmoothing scale $R=15\\,h^{-1}$Mpc, we find that the CMD yields tighter\nforecasts for $(\\Omega_{m}},\\,\\sigma_{8})$ than the zeroth- to third-order MFs\ncomponents, improving the constraint precision by ${\\sim}(44\\%,\\,52\\%)$,\n${\\sim}(30\\%,\\,45\\%)$, ${\\sim}(27\\%,\\,17\\%)$, and ${\\sim}(26\\%,\\,17\\%)$,\nrespectively. A joint configuration combining the MFs and CMD further enhances\nthe precision by approximately ${\\sim}27\\%$ compared to the standard MFs alone,\nhighlighting the complementary anisotropy-sensitive information captured by the\nCMD in contrast to the scalar morphological content encapsulated by the MFs. We\nfurther extend the forecasting analysis to a continuous range of cosmological\nparameter values and multiple smoothing scales. Our results show that, although\nthe absolute forecast uncertainty for each component of summary statistics\ndepends on the underlying parameter values and the adopted smoothing scale, the\nrelative constraining power among the summary statistics remains nearly\nconstant throughout."
                },
                "authors": [
                    {
                        "name": "M. H. Jalali Kanafi"
                    },
                    {
                        "name": "S. M. S. Movahed"
                    }
                ],
                "author_detail": {
                    "name": "S. M. S. Movahed"
                },
                "author": "S. M. S. Movahed",
                "arxiv_comment": "19 pages, 9 figures and 3 tables. Comments are welcome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03636v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03636v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03635v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03635v1",
                "updated": "2025-11-05T16:54:10Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    16,
                    54,
                    10,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T16:54:10Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    16,
                    54,
                    10,
                    2,
                    309,
                    0
                ],
                "title": "Towards Transparent Stance Detection: A Zero-Shot Approach Using\n  Implicit and Explicit Interpretability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Transparent Stance Detection: A Zero-Shot Approach Using\n  Implicit and Explicit Interpretability"
                },
                "summary": "Zero-Shot Stance Detection (ZSSD) identifies the attitude of the post toward\nunseen targets. Existing research using contrastive, meta-learning, or data\naugmentation suffers from generalizability issues or lack of coherence between\ntext and target. Recent works leveraging large language models (LLMs) for ZSSD\nfocus either on improving unseen target-specific knowledge or generating\nexplanations for stance analysis. However, most of these works are limited by\ntheir over-reliance on explicit reasoning, provide coarse explanations that\nlack nuance, and do not explicitly model the reasoning process, making it\ndifficult to interpret the model's predictions. To address these issues, in our\nstudy, we develop a novel interpretable ZSSD framework, IRIS. We provide an\ninterpretable understanding of the attitude of the input towards the target\nimplicitly based on sequences within the text (implicit rationales) and\nexplicitly based on linguistic measures (explicit rationales). IRIS considers\nstance detection as an information retrieval ranking task, understanding the\nrelevance of implicit rationales for different stances to guide the model\ntowards correct predictions without requiring the ground-truth of rationales,\nthus providing inherent interpretability. In addition, explicit rationales\nbased on communicative features help decode the emotional and cognitive\ndimensions of stance, offering an interpretable understanding of the author's\nattitude towards the given target. Extensive experiments on the benchmark\ndatasets of VAST, EZ-STANCE, P-Stance, and RFD using 50%, 30%, and even 10%\ntraining data prove the generalizability of our model, benefiting from the\nproposed architecture and interpretable design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-Shot Stance Detection (ZSSD) identifies the attitude of the post toward\nunseen targets. Existing research using contrastive, meta-learning, or data\naugmentation suffers from generalizability issues or lack of coherence between\ntext and target. Recent works leveraging large language models (LLMs) for ZSSD\nfocus either on improving unseen target-specific knowledge or generating\nexplanations for stance analysis. However, most of these works are limited by\ntheir over-reliance on explicit reasoning, provide coarse explanations that\nlack nuance, and do not explicitly model the reasoning process, making it\ndifficult to interpret the model's predictions. To address these issues, in our\nstudy, we develop a novel interpretable ZSSD framework, IRIS. We provide an\ninterpretable understanding of the attitude of the input towards the target\nimplicitly based on sequences within the text (implicit rationales) and\nexplicitly based on linguistic measures (explicit rationales). IRIS considers\nstance detection as an information retrieval ranking task, understanding the\nrelevance of implicit rationales for different stances to guide the model\ntowards correct predictions without requiring the ground-truth of rationales,\nthus providing inherent interpretability. In addition, explicit rationales\nbased on communicative features help decode the emotional and cognitive\ndimensions of stance, offering an interpretable understanding of the author's\nattitude towards the given target. Extensive experiments on the benchmark\ndatasets of VAST, EZ-STANCE, P-Stance, and RFD using 50%, 30%, and even 10%\ntraining data prove the generalizability of our model, benefiting from the\nproposed architecture and interpretable design."
                },
                "authors": [
                    {
                        "name": "Apoorva Upadhyaya"
                    },
                    {
                        "name": "Wolfgang Nejdl"
                    },
                    {
                        "name": "Marco Fisichella"
                    }
                ],
                "author_detail": {
                    "name": "Marco Fisichella"
                },
                "author": "Marco Fisichella",
                "arxiv_comment": "Accepted in AAAI CONFERENCE ON WEB AND SOCIAL MEDIA (ICWSM 2026)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03635v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03635v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23845v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23845v2",
                "updated": "2025-11-05T16:53:09Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    16,
                    53,
                    9,
                    2,
                    309,
                    0
                ],
                "published": "2025-05-28T17:01:30Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    17,
                    1,
                    30,
                    2,
                    148,
                    0
                ],
                "title": "Read Your Own Mind: Reasoning Helps Surface Self-Confidence Signals in\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Read Your Own Mind: Reasoning Helps Surface Self-Confidence Signals in\n  LLMs"
                },
                "summary": "We study the source of uncertainty in DeepSeek R1-32B by analyzing its\nself-reported verbal confidence on question answering (QA) tasks. In the\ndefault answer-then-confidence setting, the model is regularly over-confident,\nwhereas semantic entropy - obtained by sampling many responses - remains\nreliable. We hypothesize that this is because of semantic entropy's larger\ntest-time compute, which lets us explore the model's predictive distribution.\nWe show that granting DeepSeek the budget to explore its distribution by\nforcing a long chain-of-thought before the final answer greatly improves its\nverbal score effectiveness, even on simple fact-retrieval questions that\nnormally require no reasoning. Furthermore, a separate reader model that sees\nonly the chain can reconstruct very similar confidences, indicating the verbal\nscore might be merely a statistic of the alternatives surfaced during\nreasoning. Our analysis concludes that reliable uncertainty estimation requires\nexplicit exploration of the generative space, and self-reported confidence is\ntrustworthy only after such exploration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the source of uncertainty in DeepSeek R1-32B by analyzing its\nself-reported verbal confidence on question answering (QA) tasks. In the\ndefault answer-then-confidence setting, the model is regularly over-confident,\nwhereas semantic entropy - obtained by sampling many responses - remains\nreliable. We hypothesize that this is because of semantic entropy's larger\ntest-time compute, which lets us explore the model's predictive distribution.\nWe show that granting DeepSeek the budget to explore its distribution by\nforcing a long chain-of-thought before the final answer greatly improves its\nverbal score effectiveness, even on simple fact-retrieval questions that\nnormally require no reasoning. Furthermore, a separate reader model that sees\nonly the chain can reconstruct very similar confidences, indicating the verbal\nscore might be merely a statistic of the alternatives surfaced during\nreasoning. Our analysis concludes that reliable uncertainty estimation requires\nexplicit exploration of the generative space, and self-reported confidence is\ntrustworthy only after such exploration."
                },
                "authors": [
                    {
                        "name": "Jakub Podolak"
                    },
                    {
                        "name": "Rajeev Verma"
                    }
                ],
                "author_detail": {
                    "name": "Rajeev Verma"
                },
                "author": "Rajeev Verma",
                "arxiv_comment": "Presented at UncertaiNLP Workshop at EMNLP 2025\n  https://aclanthology.org/2025.uncertainlp-main.21.pdf",
                "arxiv_journal_ref": "UncertaiNLP Workshop at Empirical Methods in Natural Language\n  Processing 2025 (EMNLP 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23845v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23845v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03628v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03628v1",
                "updated": "2025-11-05T16:47:26Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    16,
                    47,
                    26,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T16:47:26Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    16,
                    47,
                    26,
                    2,
                    309,
                    0
                ],
                "title": "LiveTradeBench: Seeking Real-World Alpha with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LiveTradeBench: Seeking Real-World Alpha with Large Language Models"
                },
                "summary": "Large language models (LLMs) achieve strong performance across\nbenchmarks--from knowledge quizzes and math reasoning to web-agent tasks--but\nthese tests occur in static settings, lacking real dynamics and uncertainty.\nConsequently, they evaluate isolated reasoning or problem-solving rather than\ndecision-making under uncertainty. To address this, we introduce\nLiveTradeBench, a live trading environment for evaluating LLM agents in\nrealistic and evolving markets. LiveTradeBench follows three design principles:\n(i) Live data streaming of market prices and news, eliminating dependence on\noffline backtesting and preventing information leakage while capturing\nreal-time uncertainty; (ii) a portfolio-management abstraction that extends\ncontrol from single-asset actions to multi-asset allocation, integrating risk\nmanagement and cross-asset reasoning; and (iii) multi-market evaluation across\nstructurally distinct environments--U.S. stocks and Polymarket prediction\nmarkets--differing in volatility, liquidity, and information flow. At each\nstep, an agent observes prices, news, and its portfolio, then outputs\npercentage allocations that balance risk and return. Using LiveTradeBench, we\nrun 50-day live evaluations of 21 LLMs across families. Results show that (1)\nhigh LMArena scores do not imply superior trading outcomes; (2) models display\ndistinct portfolio styles reflecting risk appetite and reasoning dynamics; and\n(3) some LLMs effectively leverage live signals to adapt decisions. These\nfindings expose a gap between static evaluation and real-world competence,\nmotivating benchmarks that test sequential decision making and consistency\nunder live uncertainty.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) achieve strong performance across\nbenchmarks--from knowledge quizzes and math reasoning to web-agent tasks--but\nthese tests occur in static settings, lacking real dynamics and uncertainty.\nConsequently, they evaluate isolated reasoning or problem-solving rather than\ndecision-making under uncertainty. To address this, we introduce\nLiveTradeBench, a live trading environment for evaluating LLM agents in\nrealistic and evolving markets. LiveTradeBench follows three design principles:\n(i) Live data streaming of market prices and news, eliminating dependence on\noffline backtesting and preventing information leakage while capturing\nreal-time uncertainty; (ii) a portfolio-management abstraction that extends\ncontrol from single-asset actions to multi-asset allocation, integrating risk\nmanagement and cross-asset reasoning; and (iii) multi-market evaluation across\nstructurally distinct environments--U.S. stocks and Polymarket prediction\nmarkets--differing in volatility, liquidity, and information flow. At each\nstep, an agent observes prices, news, and its portfolio, then outputs\npercentage allocations that balance risk and return. Using LiveTradeBench, we\nrun 50-day live evaluations of 21 LLMs across families. Results show that (1)\nhigh LMArena scores do not imply superior trading outcomes; (2) models display\ndistinct portfolio styles reflecting risk appetite and reasoning dynamics; and\n(3) some LLMs effectively leverage live signals to adapt decisions. These\nfindings expose a gap between static evaluation and real-world competence,\nmotivating benchmarks that test sequential decision making and consistency\nunder live uncertainty."
                },
                "authors": [
                    {
                        "name": "Haofei Yu"
                    },
                    {
                        "name": "Fenghai Li"
                    },
                    {
                        "name": "Jiaxuan You"
                    }
                ],
                "author_detail": {
                    "name": "Jiaxuan You"
                },
                "author": "Jiaxuan You",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03628v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03628v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.TR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.TR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21205v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21205v2",
                "updated": "2025-11-05T16:42:11Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    16,
                    42,
                    11,
                    2,
                    309,
                    0
                ],
                "published": "2025-04-29T22:22:44Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    22,
                    22,
                    44,
                    1,
                    119,
                    0
                ],
                "title": "SecRepoBench: Benchmarking Code Agents for Secure Code Completion in\n  Real-World Repositories",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SecRepoBench: Benchmarking Code Agents for Secure Code Completion in\n  Real-World Repositories"
                },
                "summary": "This paper introduces SecRepoBench, a benchmark to evaluate code agents on\nsecure code completion in real-world repositories. SecRepoBench has 318 code\ncompletion tasks in 27 C/C++ repositories, covering 15 CWEs. We evaluate 28\nstandalone LLMs and 13 code agents across 3 state-of-the-art agent frameworks\nusing our benchmark. We find that state-of-the-art LLMs struggle with\ngenerating correct and secure code completions. However, code agents\nsignificantly outperform standalone LLMs. We show that SecRepoBench is more\ndifficult than the prior state-of-the-art benchmark. Finally, our comprehensive\nanalysis provides insights into potential directions for enhancing the ability\nof code agents to write correct and secure code in real-world repositories.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces SecRepoBench, a benchmark to evaluate code agents on\nsecure code completion in real-world repositories. SecRepoBench has 318 code\ncompletion tasks in 27 C/C++ repositories, covering 15 CWEs. We evaluate 28\nstandalone LLMs and 13 code agents across 3 state-of-the-art agent frameworks\nusing our benchmark. We find that state-of-the-art LLMs struggle with\ngenerating correct and secure code completions. However, code agents\nsignificantly outperform standalone LLMs. We show that SecRepoBench is more\ndifficult than the prior state-of-the-art benchmark. Finally, our comprehensive\nanalysis provides insights into potential directions for enhancing the ability\nof code agents to write correct and secure code in real-world repositories."
                },
                "authors": [
                    {
                        "name": "Chihao Shen"
                    },
                    {
                        "name": "Connor Dilgren"
                    },
                    {
                        "name": "Purva Chiniya"
                    },
                    {
                        "name": "Luke Griffith"
                    },
                    {
                        "name": "Yu Ding"
                    },
                    {
                        "name": "Yizheng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yizheng Chen"
                },
                "author": "Yizheng Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21205v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21205v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21600v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21600v2",
                "updated": "2025-11-05T16:39:11Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    16,
                    39,
                    11,
                    2,
                    309,
                    0
                ],
                "published": "2025-05-27T16:57:20Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    16,
                    57,
                    20,
                    1,
                    147,
                    0
                ],
                "title": "R2R: Efficiently Navigating Divergent Reasoning Paths with Small-Large\n  Model Token Routing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "R2R: Efficiently Navigating Divergent Reasoning Paths with Small-Large\n  Model Token Routing"
                },
                "summary": "Large Language Models (LLMs) achieve impressive reasoning capabilities at the\ncost of substantial inference overhead, posing substantial deployment\nchallenges. Although distilled Small Language Models (SLMs) significantly\nenhance efficiency, their performance suffers as they fail to follow LLMs'\nreasoning paths. Luckily, we reveal that only a small fraction of tokens\ngenuinely diverge reasoning paths between LLMs and SLMs. Most generated tokens\nare either identical or exhibit neutral differences, such as minor variations\nin abbreviations or expressions. Leveraging this insight, we introduce **Roads\nto Rome (R2R)**, a neural token routing method that selectively utilizes LLMs\nonly for these critical, path-divergent tokens, while leaving the majority of\ntoken generation to the SLM. We also develop an automatic data generation\npipeline that identifies divergent tokens and generates token-level routing\nlabels to train the lightweight router. We apply R2R to combine R1-1.5B and\nR1-32B models from the DeepSeek family, and evaluate on challenging math,\ncoding, and QA benchmarks. With an average activated parameter size of 5.6B,\nR2R surpasses the average accuracy of R1-7B by 1.6x, outperforming even the\nR1-14B model. Compared to R1-32B, it delivers a 2.8x wall-clock speedup with\ncomparable performance, advancing the Pareto frontier of test-time scaling\nefficiency. Our code is available at https://github.com/thu-nics/R2R.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) achieve impressive reasoning capabilities at the\ncost of substantial inference overhead, posing substantial deployment\nchallenges. Although distilled Small Language Models (SLMs) significantly\nenhance efficiency, their performance suffers as they fail to follow LLMs'\nreasoning paths. Luckily, we reveal that only a small fraction of tokens\ngenuinely diverge reasoning paths between LLMs and SLMs. Most generated tokens\nare either identical or exhibit neutral differences, such as minor variations\nin abbreviations or expressions. Leveraging this insight, we introduce **Roads\nto Rome (R2R)**, a neural token routing method that selectively utilizes LLMs\nonly for these critical, path-divergent tokens, while leaving the majority of\ntoken generation to the SLM. We also develop an automatic data generation\npipeline that identifies divergent tokens and generates token-level routing\nlabels to train the lightweight router. We apply R2R to combine R1-1.5B and\nR1-32B models from the DeepSeek family, and evaluate on challenging math,\ncoding, and QA benchmarks. With an average activated parameter size of 5.6B,\nR2R surpasses the average accuracy of R1-7B by 1.6x, outperforming even the\nR1-14B model. Compared to R1-32B, it delivers a 2.8x wall-clock speedup with\ncomparable performance, advancing the Pareto frontier of test-time scaling\nefficiency. Our code is available at https://github.com/thu-nics/R2R."
                },
                "authors": [
                    {
                        "name": "Tianyu Fu"
                    },
                    {
                        "name": "Yi Ge"
                    },
                    {
                        "name": "Yichen You"
                    },
                    {
                        "name": "Enshu Liu"
                    },
                    {
                        "name": "Zhihang Yuan"
                    },
                    {
                        "name": "Guohao Dai"
                    },
                    {
                        "name": "Shengen Yan"
                    },
                    {
                        "name": "Huazhong Yang"
                    },
                    {
                        "name": "Yu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wang"
                },
                "author": "Yu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21600v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21600v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06394v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06394v2",
                "updated": "2025-11-05T16:36:55Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    16,
                    36,
                    55,
                    2,
                    309,
                    0
                ],
                "published": "2025-04-08T19:34:59Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    19,
                    34,
                    59,
                    1,
                    98,
                    0
                ],
                "title": "Uncovering flow and deformation regimes in the coupled fluid-solid\n  vestibular system",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncovering flow and deformation regimes in the coupled fluid-solid\n  vestibular system"
                },
                "summary": "In this paper, we showcase how flow obstruction by a deformable object can\nlead to symmetry breaking in curved domains subject to angular acceleration.\nOur analysis is motivated by the deflection of the cupula, a soft tissue\nlocated in the inner ear that is used to perceive rotational motion as part of\nthe vestibular system. The cupula is understood to block the rotation-induced\nflow in a toroidal region with the flow-induced deformation of the cupula used\nby the brain to infer motion. By asymptotically solving the governing equations\nfor this flow, we characterise regimes for which the sensory system is\nsensitive to either angular velocity or angular acceleration. Moreover, we show\nthe fluid flow is not symmetric in the latter case. Finally, we extend our\nanalysis of symmetry breaking to understand the formation of vortical flow in\ncavernous regions within channels. We discuss the implications of our results\nfor the sensing of rotation by mammals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we showcase how flow obstruction by a deformable object can\nlead to symmetry breaking in curved domains subject to angular acceleration.\nOur analysis is motivated by the deflection of the cupula, a soft tissue\nlocated in the inner ear that is used to perceive rotational motion as part of\nthe vestibular system. The cupula is understood to block the rotation-induced\nflow in a toroidal region with the flow-induced deformation of the cupula used\nby the brain to infer motion. By asymptotically solving the governing equations\nfor this flow, we characterise regimes for which the sensory system is\nsensitive to either angular velocity or angular acceleration. Moreover, we show\nthe fluid flow is not symmetric in the latter case. Finally, we extend our\nanalysis of symmetry breaking to understand the formation of vortical flow in\ncavernous regions within channels. We discuss the implications of our results\nfor the sensing of rotation by mammals."
                },
                "authors": [
                    {
                        "name": "Javier Chico-Vzquez"
                    },
                    {
                        "name": "Derek E. Moulton"
                    },
                    {
                        "name": "Dominic Vella"
                    }
                ],
                "author_detail": {
                    "name": "Dominic Vella"
                },
                "author": "Dominic Vella",
                "arxiv_doi": "10.1017/jfm.2025.10783",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1017/jfm.2025.10783",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.06394v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06394v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "J. Fluid Mech. 1022, A40 (2025)",
                "arxiv_primary_category": {
                    "term": "physics.flu-dyn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.bio-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03616v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03616v1",
                "updated": "2025-11-05T16:33:39Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    16,
                    33,
                    39,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T16:33:39Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    16,
                    33,
                    39,
                    2,
                    309,
                    0
                ],
                "title": "Going Beyond Expert Performance via Deep Implicit Imitation\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Going Beyond Expert Performance via Deep Implicit Imitation\n  Reinforcement Learning"
                },
                "summary": "Imitation learning traditionally requires complete state-action\ndemonstrations from optimal or near-optimal experts. These requirements\nseverely limit practical applicability, as many real-world scenarios provide\nonly state observations without corresponding actions and expert performance is\noften suboptimal. In this paper we introduce a deep implicit imitation\nreinforcement learning framework that addresses both limitations by combining\ndeep reinforcement learning with implicit imitation learning from\nobservation-only datasets. Our main algorithm, Deep Implicit Imitation\nQ-Network (DIIQN), employs an action inference mechanism that reconstructs\nexpert actions through online exploration and integrates a dynamic confidence\nmechanism that adaptively balances expert-guided and self-directed learning.\nThis enables the agent to leverage expert guidance for accelerated training\nwhile maintaining capacity to surpass suboptimal expert performance. We further\nextend our framework with a Heterogeneous Actions DIIQN (HA-DIIQN) algorithm to\ntackle scenarios where expert and agent possess different action sets, a\nchallenge previously unaddressed in the implicit imitation learning literature.\nHA-DIIQN introduces an infeasibility detection mechanism and a bridging\nprocedure identifying alternative pathways connecting agent capabilities to\nexpert guidance when direct action replication is impossible. Our experimental\nresults demonstrate that DIIQN achieves up to 130% higher episodic returns\ncompared to standard DQN, while consistently outperforming existing implicit\nimitation methods that cannot exceed expert performance. In heterogeneous\naction settings, HA-DIIQN learns up to 64% faster than baselines, leveraging\nexpert datasets unusable by conventional approaches. Extensive parameter\nsensitivity analysis reveals the framework's robustness across varying dataset\nsizes and hyperparameter configurations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Imitation learning traditionally requires complete state-action\ndemonstrations from optimal or near-optimal experts. These requirements\nseverely limit practical applicability, as many real-world scenarios provide\nonly state observations without corresponding actions and expert performance is\noften suboptimal. In this paper we introduce a deep implicit imitation\nreinforcement learning framework that addresses both limitations by combining\ndeep reinforcement learning with implicit imitation learning from\nobservation-only datasets. Our main algorithm, Deep Implicit Imitation\nQ-Network (DIIQN), employs an action inference mechanism that reconstructs\nexpert actions through online exploration and integrates a dynamic confidence\nmechanism that adaptively balances expert-guided and self-directed learning.\nThis enables the agent to leverage expert guidance for accelerated training\nwhile maintaining capacity to surpass suboptimal expert performance. We further\nextend our framework with a Heterogeneous Actions DIIQN (HA-DIIQN) algorithm to\ntackle scenarios where expert and agent possess different action sets, a\nchallenge previously unaddressed in the implicit imitation learning literature.\nHA-DIIQN introduces an infeasibility detection mechanism and a bridging\nprocedure identifying alternative pathways connecting agent capabilities to\nexpert guidance when direct action replication is impossible. Our experimental\nresults demonstrate that DIIQN achieves up to 130% higher episodic returns\ncompared to standard DQN, while consistently outperforming existing implicit\nimitation methods that cannot exceed expert performance. In heterogeneous\naction settings, HA-DIIQN learns up to 64% faster than baselines, leveraging\nexpert datasets unusable by conventional approaches. Extensive parameter\nsensitivity analysis reveals the framework's robustness across varying dataset\nsizes and hyperparameter configurations."
                },
                "authors": [
                    {
                        "name": "Iason Chrysomallis"
                    },
                    {
                        "name": "Georgios Chalkiadakis"
                    }
                ],
                "author_detail": {
                    "name": "Georgios Chalkiadakis"
                },
                "author": "Georgios Chalkiadakis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03616v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03616v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03608v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03608v1",
                "updated": "2025-11-05T16:27:20Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    16,
                    27,
                    20,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T16:27:20Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    16,
                    27,
                    20,
                    2,
                    309,
                    0
                ],
                "title": "A local eigenvector centrality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A local eigenvector centrality"
                },
                "summary": "Eigenvector centrality is an established measure of global connectivity, from\nwhich the importance and influence of nodes can be inferred. We introduce a\nlocal eigenvector centrality that incorporates both local and global\nconnectivity. This new measure references prominent eigengaps and combines\ntheir associated eigenspectrum, via the Euclidean norm, to detect centrality\nthat reflects the influence of prominent community structures. In contact\nnetworks, with clearly defined community structures, local eigenvector\ncentrality is shown to identify similar but distinct distributions to\neigenvector centrality applied on each community in isolation and PageRank.\nDiscrepancies between the two eigenvector measures highlight nodes and\ncommunities that do not conform to their defined local structures, e.g. nodes\nwith more connections outside of their defined community than within it. While\nreference to PageRank's centrality assessment enables a mitigation strategy for\nlocalisation effects inherent in eigenvector-based measures. In networks\nwithout clearly defined communities, such as city road networks, local\neigenvector centrality is shown to identify both locally prominent and globally\nconnected hubs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eigenvector centrality is an established measure of global connectivity, from\nwhich the importance and influence of nodes can be inferred. We introduce a\nlocal eigenvector centrality that incorporates both local and global\nconnectivity. This new measure references prominent eigengaps and combines\ntheir associated eigenspectrum, via the Euclidean norm, to detect centrality\nthat reflects the influence of prominent community structures. In contact\nnetworks, with clearly defined community structures, local eigenvector\ncentrality is shown to identify similar but distinct distributions to\neigenvector centrality applied on each community in isolation and PageRank.\nDiscrepancies between the two eigenvector measures highlight nodes and\ncommunities that do not conform to their defined local structures, e.g. nodes\nwith more connections outside of their defined community than within it. While\nreference to PageRank's centrality assessment enables a mitigation strategy for\nlocalisation effects inherent in eigenvector-based measures. In networks\nwithout clearly defined communities, such as city road networks, local\neigenvector centrality is shown to identify both locally prominent and globally\nconnected hubs."
                },
                "authors": [
                    {
                        "name": "Ruaridh A. Clark"
                    },
                    {
                        "name": "Francesca Arrigo"
                    },
                    {
                        "name": "Agathe Bouis"
                    },
                    {
                        "name": "Malcolm Macdonald"
                    }
                ],
                "author_detail": {
                    "name": "Malcolm Macdonald"
                },
                "author": "Malcolm Macdonald",
                "arxiv_comment": "17 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03608v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03608v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03605v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03605v1",
                "updated": "2025-11-05T16:25:22Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    16,
                    25,
                    22,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T16:25:22Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    16,
                    25,
                    22,
                    2,
                    309,
                    0
                ],
                "title": "Bayesian Topological Analysis of Functional Brain Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian Topological Analysis of Functional Brain Networks"
                },
                "summary": "Subtle alterations in brain network topology often evade detection by\ntraditional statistical methods. To address this limitation, we introduce a\nBayesian inference framework for topological comparison of brain networks that\nprobabilistically models within- and between-group dissimilarities. The\nframework employs Markov chain Monte Carlo sampling to estimate posterior\ndistributions of test statistics and Bayes factors, enabling graded evidence\nassessment beyond binary significance testing. Simulations confirmed\nstatistical consistency to permutation testing. Applied to fMRI data from the\nDuke-UNC Alzheimer's Disease Research Center, the framework detected\ntopology-based network differences that conventional permutation tests failed\nto reveal, highlighting its enhanced sensitivity to early or subtle brain\nnetwork alterations in clinical neuroimaging.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Subtle alterations in brain network topology often evade detection by\ntraditional statistical methods. To address this limitation, we introduce a\nBayesian inference framework for topological comparison of brain networks that\nprobabilistically models within- and between-group dissimilarities. The\nframework employs Markov chain Monte Carlo sampling to estimate posterior\ndistributions of test statistics and Bayes factors, enabling graded evidence\nassessment beyond binary significance testing. Simulations confirmed\nstatistical consistency to permutation testing. Applied to fMRI data from the\nDuke-UNC Alzheimer's Disease Research Center, the framework detected\ntopology-based network differences that conventional permutation tests failed\nto reveal, highlighting its enhanced sensitivity to early or subtle brain\nnetwork alterations in clinical neuroimaging."
                },
                "authors": [
                    {
                        "name": "Xukun Zhu"
                    },
                    {
                        "name": "Michael W Lutz"
                    },
                    {
                        "name": "Tananun Songdechakraiwut"
                    }
                ],
                "author_detail": {
                    "name": "Tananun Songdechakraiwut"
                },
                "author": "Tananun Songdechakraiwut",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03605v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03605v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03604v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03604v1",
                "updated": "2025-11-05T16:24:53Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    16,
                    24,
                    53,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T16:24:53Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    16,
                    24,
                    53,
                    2,
                    309,
                    0
                ],
                "title": "The first year of LISA Galactic foreground",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The first year of LISA Galactic foreground"
                },
                "summary": "Galactic white-dwarf binaries play a central role in the inference model for\nthe Laser Interferometer Space Antenna. In this manuscript, we employ the\n$\\texttt{bahamas}$ codebase to characterize, in a global-fit fashion, the\nreconstruction of the Galactic foreground during the first year of observation.\nTo account for its statistical properties, we represent the data in\ntime--frequency domain, and characterize the effectiveness of multiple\napproaches, e.g. statistically viable likelihoods, sampling schemes,\nsegmentation widths, and gaps density. Our analysis yields consistent results\nacross, with overwhelming evidence in favor of a non-stationary model in less\nthan a month of data. Moreover, we show robustness against the presence of\nadditional extragalactic foregrounds, and test the suitability of our\napproximations on the more complex simulated data in the $\\textit{Yorsh}$ data\nchallenge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Galactic white-dwarf binaries play a central role in the inference model for\nthe Laser Interferometer Space Antenna. In this manuscript, we employ the\n$\\texttt{bahamas}$ codebase to characterize, in a global-fit fashion, the\nreconstruction of the Galactic foreground during the first year of observation.\nTo account for its statistical properties, we represent the data in\ntime--frequency domain, and characterize the effectiveness of multiple\napproaches, e.g. statistically viable likelihoods, sampling schemes,\nsegmentation widths, and gaps density. Our analysis yields consistent results\nacross, with overwhelming evidence in favor of a non-stationary model in less\nthan a month of data. Moreover, we show robustness against the presence of\nadditional extragalactic foregrounds, and test the suitability of our\napproximations on the more complex simulated data in the $\\textit{Yorsh}$ data\nchallenge."
                },
                "authors": [
                    {
                        "name": "Riccardo Buscicchio"
                    },
                    {
                        "name": "Federico Pozzoli"
                    },
                    {
                        "name": "Daniele Chirico"
                    },
                    {
                        "name": "Alberto Sesana"
                    }
                ],
                "author_detail": {
                    "name": "Alberto Sesana"
                },
                "author": "Alberto Sesana",
                "arxiv_comment": "16 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03604v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03604v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03601v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03601v1",
                "updated": "2025-11-05T16:22:19Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    16,
                    22,
                    19,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T16:22:19Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    16,
                    22,
                    19,
                    2,
                    309,
                    0
                ],
                "title": "Step-Audio-EditX Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Step-Audio-EditX Technical Report"
                },
                "summary": "We present Step-Audio-EditX, the first open-source LLM-based audio model\nexcelling at expressive and iterative audio editing encompassing emotion,\nspeaking style, and paralinguistics alongside robust zero-shot text-to-speech\n(TTS) capabilities.Our core innovation lies in leveraging only large-margin\nsynthetic data, which circumvents the need for embedding-based priors or\nauxiliary modules. This large-margin learning approach enables both iterative\ncontrol and high expressivity across voices, and represents a fundamental pivot\nfrom the conventional focus on representation-level disentanglement. Evaluation\nresults demonstrate that Step-Audio-EditX surpasses both MiniMax-2.6-hd and\nDoubao-Seed-TTS-2.0 in emotion editing and other fine-grained control tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Step-Audio-EditX, the first open-source LLM-based audio model\nexcelling at expressive and iterative audio editing encompassing emotion,\nspeaking style, and paralinguistics alongside robust zero-shot text-to-speech\n(TTS) capabilities.Our core innovation lies in leveraging only large-margin\nsynthetic data, which circumvents the need for embedding-based priors or\nauxiliary modules. This large-margin learning approach enables both iterative\ncontrol and high expressivity across voices, and represents a fundamental pivot\nfrom the conventional focus on representation-level disentanglement. Evaluation\nresults demonstrate that Step-Audio-EditX surpasses both MiniMax-2.6-hd and\nDoubao-Seed-TTS-2.0 in emotion editing and other fine-grained control tasks."
                },
                "authors": [
                    {
                        "name": "Chao Yan"
                    },
                    {
                        "name": "Boyong Wu"
                    },
                    {
                        "name": "Peng Yang"
                    },
                    {
                        "name": "Pengfei Tan"
                    },
                    {
                        "name": "Guoqiang Hu"
                    },
                    {
                        "name": "Yuxin Zhang"
                    },
                    {
                        "name": "Xiangyu"
                    },
                    {
                        "name": "Zhang"
                    },
                    {
                        "name": "Fei Tian"
                    },
                    {
                        "name": "Xuerui Yang"
                    },
                    {
                        "name": "Xiangyu Zhang"
                    },
                    {
                        "name": "Daxin Jiang"
                    },
                    {
                        "name": "Gang Yu"
                    }
                ],
                "author_detail": {
                    "name": "Gang Yu"
                },
                "arxiv_affiliation": "Tony",
                "author": "Gang Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03601v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03601v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06633v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06633v2",
                "updated": "2025-11-05T16:19:04Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    16,
                    19,
                    4,
                    2,
                    309,
                    0
                ],
                "published": "2025-07-09T08:00:49Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    8,
                    0,
                    49,
                    2,
                    190,
                    0
                ],
                "title": "Parameter estimation in interacting particle systems on dynamic random\n  networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameter estimation in interacting particle systems on dynamic random\n  networks"
                },
                "summary": "In this paper we consider a class of interacting particle systems on dynamic\nrandom networks, in which the joint dynamics of vertices and edges acts as\none-way feedback, i.e., edges appear and disappear over time depending on the\nstate of the two connected vertices, while the vertex dynamics does not depend\non the edge process. Our goal is to estimate the underlying dynamics from\npartial information of the process, specifically from snapshots of the total\nnumber of edges present. We showcase the effectiveness of our inference method\nthrough various numerical results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper we consider a class of interacting particle systems on dynamic\nrandom networks, in which the joint dynamics of vertices and edges acts as\none-way feedback, i.e., edges appear and disappear over time depending on the\nstate of the two connected vertices, while the vertex dynamics does not depend\non the edge process. Our goal is to estimate the underlying dynamics from\npartial information of the process, specifically from snapshots of the total\nnumber of edges present. We showcase the effectiveness of our inference method\nthrough various numerical results."
                },
                "authors": [
                    {
                        "name": "Simone Baldassarri"
                    },
                    {
                        "name": "Jiesen Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jiesen Wang"
                },
                "author": "Jiesen Wang",
                "arxiv_doi": "10.1103/k8cr-vkkf",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/k8cr-vkkf",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.06633v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06633v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "16 pages",
                "arxiv_journal_ref": "Physical Review E 112, 054301 (2025)",
                "arxiv_primary_category": {
                    "term": "math.PR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10594v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10594v2",
                "updated": "2025-11-05T16:07:58Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    16,
                    7,
                    58,
                    2,
                    309,
                    0
                ],
                "published": "2025-09-12T14:59:52Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    14,
                    59,
                    52,
                    4,
                    255,
                    0
                ],
                "title": "SME-TEAM: Leveraging Trust and Ethics for Secure and Responsible Use of\n  AI and LLMs in SMEs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SME-TEAM: Leveraging Trust and Ethics for Secure and Responsible Use of\n  AI and LLMs in SMEs"
                },
                "summary": "Artificial Intelligence (AI) and Large Language Models (LLMs) are\nrevolutionizing today's business practices; however, their adoption within\nsmall and medium-sized enterprises (SMEs) raises serious trust, ethical, and\ntechnical issues. In this perspective paper, we introduce a structured,\nmulti-phased framework, \"SME-TEAM\" for the secure and responsible use of these\ntechnologies in SMEs. Based on a conceptual structure of four key pillars,\ni.e., Data, Algorithms, Human Oversight, and Model Architecture, SME-TEAM\nbridges theoretical ethical principles with operational practice, enhancing AI\ncapabilities across a wide range of applications in SMEs. Ultimately, this\npaper provides a structured roadmap for the adoption of these emerging\ntechnologies, positioning trust and ethics as a driving force for resilience,\ncompetitiveness, and sustainable innovation within the area of business\nanalytics and SMEs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial Intelligence (AI) and Large Language Models (LLMs) are\nrevolutionizing today's business practices; however, their adoption within\nsmall and medium-sized enterprises (SMEs) raises serious trust, ethical, and\ntechnical issues. In this perspective paper, we introduce a structured,\nmulti-phased framework, \"SME-TEAM\" for the secure and responsible use of these\ntechnologies in SMEs. Based on a conceptual structure of four key pillars,\ni.e., Data, Algorithms, Human Oversight, and Model Architecture, SME-TEAM\nbridges theoretical ethical principles with operational practice, enhancing AI\ncapabilities across a wide range of applications in SMEs. Ultimately, this\npaper provides a structured roadmap for the adoption of these emerging\ntechnologies, positioning trust and ethics as a driving force for resilience,\ncompetitiveness, and sustainable innovation within the area of business\nanalytics and SMEs."
                },
                "authors": [
                    {
                        "name": "Iqbal H. Sarker"
                    },
                    {
                        "name": "Helge Janicke"
                    },
                    {
                        "name": "Ahmad Mohsin"
                    },
                    {
                        "name": "Leandros Maglaras"
                    }
                ],
                "author_detail": {
                    "name": "Leandros Maglaras"
                },
                "author": "Leandros Maglaras",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10594v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10594v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03586v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03586v1",
                "updated": "2025-11-05T16:05:26Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    16,
                    5,
                    26,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T16:05:26Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    16,
                    5,
                    26,
                    2,
                    309,
                    0
                ],
                "title": "PerfDojo: Automated ML Library Generation for Heterogeneous\n  Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PerfDojo: Automated ML Library Generation for Heterogeneous\n  Architectures"
                },
                "summary": "The increasing complexity of machine learning models and the proliferation of\ndiverse hardware architectures (CPUs, GPUs, accelerators) make achieving\noptimal performance a significant challenge. Heterogeneity in instruction sets,\nspecialized kernel requirements for different data types and model features\n(e.g., sparsity, quantization), and architecture-specific optimizations\ncomplicate performance tuning. Manual optimization is resource-intensive, while\nexisting automatic approaches often rely on complex hardware-specific\nheuristics and uninterpretable intermediate representations, hindering\nperformance portability. We introduce PerfLLM, a novel automatic optimization\nmethodology leveraging Large Language Models (LLMs) and Reinforcement Learning\n(RL). Central to this is PerfDojo, an environment framing optimization as an RL\ngame using a human-readable, mathematically-inspired code representation that\nguarantees semantic validity through transformations. This allows effective\noptimization without prior hardware knowledge, facilitating both human analysis\nand RL agent training. We demonstrate PerfLLM's ability to achieve significant\nperformance gains across diverse CPU (x86, Arm, RISC-V) and GPU architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing complexity of machine learning models and the proliferation of\ndiverse hardware architectures (CPUs, GPUs, accelerators) make achieving\noptimal performance a significant challenge. Heterogeneity in instruction sets,\nspecialized kernel requirements for different data types and model features\n(e.g., sparsity, quantization), and architecture-specific optimizations\ncomplicate performance tuning. Manual optimization is resource-intensive, while\nexisting automatic approaches often rely on complex hardware-specific\nheuristics and uninterpretable intermediate representations, hindering\nperformance portability. We introduce PerfLLM, a novel automatic optimization\nmethodology leveraging Large Language Models (LLMs) and Reinforcement Learning\n(RL). Central to this is PerfDojo, an environment framing optimization as an RL\ngame using a human-readable, mathematically-inspired code representation that\nguarantees semantic validity through transformations. This allows effective\noptimization without prior hardware knowledge, facilitating both human analysis\nand RL agent training. We demonstrate PerfLLM's ability to achieve significant\nperformance gains across diverse CPU (x86, Arm, RISC-V) and GPU architectures."
                },
                "authors": [
                    {
                        "name": "Andrei Ivanov"
                    },
                    {
                        "name": "Siyuan Shen"
                    },
                    {
                        "name": "Gioele Gottardo"
                    },
                    {
                        "name": "Marcin Chrapek"
                    },
                    {
                        "name": "Afif Boudaoud"
                    },
                    {
                        "name": "Timo Schneider"
                    },
                    {
                        "name": "Luca Benini"
                    },
                    {
                        "name": "Torsten Hoefler"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Hoefler"
                },
                "author": "Torsten Hoefler",
                "arxiv_doi": "10.1145/3712285.3759900",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3712285.3759900",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2511.03586v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03586v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "The International Conference for High Performance Computing,\n  Networking, Storage and Analysis (SC '25), November 16--21, 2025, St Louis,\n  MO, USA",
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03581v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03581v1",
                "updated": "2025-11-05T16:02:40Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    16,
                    2,
                    40,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T16:02:40Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    16,
                    2,
                    40,
                    2,
                    309,
                    0
                ],
                "title": "First Detection of CH3OD in Prestellar Cores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "First Detection of CH3OD in Prestellar Cores"
                },
                "summary": "The isotopic ratios of deuterated methanol derived around protostars are\ncommonly used to infer the physical conditions under which they formed in the\nearlier prestellar stage. However, there is a discrepancy in the ratio of the\nsingly deuterated methanol isotopologues, CH2DOH/CH3OD, between low- and\nhigh-mass protostars, which puts into question whether prestellar isotopic\nratios are generally preserved during the star- and planet-forming process.\nResolving this puzzle is only made harder by the complete lack of data on this\nratio in the prestellar stage. This work presents observations with the IRAM\n30m telescope that securely detect CH3OD in the prestellar core L1448 in\nPerseus and tentatively in B213-C6 in Taurus. This work constrains the ratio of\nCH2DOH/CH3OD and the D/H ratios for both singly deuterated methanol\nisotopologues for the first time at the prestellar stage. Column densities\ncalculated under the assumption of local thermal equilibrium lead to a\nCH2DOH/CH3OD ratio of 2.8-8.5 in L1448 and $\\leq$ 5.7 in B213-C6. The values\nare marginally consistent with the statistically expected ratio of 3, but most\nassumptions put the values in an elevated range in line with values found\naround low-mass protostars. The D/H ratio in CH2DOH is between 3.6% and 6.8% in\nL1448 and in the range of 2.4-5.8% in B213-C6. The D/H ratio derived for CH3OD\nis lower, namely 1.4-4.4% in L1448 and $\\leq$ 3.8% in B213-C6.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The isotopic ratios of deuterated methanol derived around protostars are\ncommonly used to infer the physical conditions under which they formed in the\nearlier prestellar stage. However, there is a discrepancy in the ratio of the\nsingly deuterated methanol isotopologues, CH2DOH/CH3OD, between low- and\nhigh-mass protostars, which puts into question whether prestellar isotopic\nratios are generally preserved during the star- and planet-forming process.\nResolving this puzzle is only made harder by the complete lack of data on this\nratio in the prestellar stage. This work presents observations with the IRAM\n30m telescope that securely detect CH3OD in the prestellar core L1448 in\nPerseus and tentatively in B213-C6 in Taurus. This work constrains the ratio of\nCH2DOH/CH3OD and the D/H ratios for both singly deuterated methanol\nisotopologues for the first time at the prestellar stage. Column densities\ncalculated under the assumption of local thermal equilibrium lead to a\nCH2DOH/CH3OD ratio of 2.8-8.5 in L1448 and $\\leq$ 5.7 in B213-C6. The values\nare marginally consistent with the statistically expected ratio of 3, but most\nassumptions put the values in an elevated range in line with values found\naround low-mass protostars. The D/H ratio in CH2DOH is between 3.6% and 6.8% in\nL1448 and in the range of 2.4-5.8% in B213-C6. The D/H ratio derived for CH3OD\nis lower, namely 1.4-4.4% in L1448 and $\\leq$ 3.8% in B213-C6."
                },
                "authors": [
                    {
                        "name": "Beatrice M. Kulterer"
                    },
                    {
                        "name": "Asuncin Fuente"
                    },
                    {
                        "name": "Maria N. Drozdovskaya"
                    },
                    {
                        "name": "Silvia Spezzano"
                    },
                    {
                        "name": "Gisela Esplugues"
                    },
                    {
                        "name": "David Navarro-Almaida"
                    },
                    {
                        "name": "Marina Rodrguez Baras"
                    },
                    {
                        "name": "Angle Taillard"
                    },
                    {
                        "name": "Karin berg"
                    }
                ],
                "author_detail": {
                    "name": "Karin berg"
                },
                "author": "Karin berg",
                "arxiv_comment": "Accepted for publication in ApJ 22 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03581v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03581v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03572v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03572v1",
                "updated": "2025-11-05T15:53:19Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    15,
                    53,
                    19,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T15:53:19Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    15,
                    53,
                    19,
                    2,
                    309,
                    0
                ],
                "title": "Leniency Designs: An Operator's Manual",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leniency Designs: An Operator's Manual"
                },
                "summary": "We develop a step-by-step guide to leniency (a.k.a. judge or examiner\ninstrument) designs, drawing on recent econometric literatures. The unbiased\njackknife instrumental variables estimator (UJIVE) is purpose-built for\nleveraging exogenous leniency variation, avoiding subtle biases even in the\npresence of many decision-makers or controls. We show how UJIVE can also be\nused to assess key assumptions underlying leniency designs, including\nquasi-random assignment and average first-stage monotonicity, and to probe the\nexternal validity of treatment effect estimates. We further discuss statistical\ninference, arguing that non-clustered standard errors are often appropriate. A\nreanalysis of Farre-Mensa et al. (2020), using quasi-random examiner assignment\nto estimate the value of patents to startups, illustrates our checklist.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We develop a step-by-step guide to leniency (a.k.a. judge or examiner\ninstrument) designs, drawing on recent econometric literatures. The unbiased\njackknife instrumental variables estimator (UJIVE) is purpose-built for\nleveraging exogenous leniency variation, avoiding subtle biases even in the\npresence of many decision-makers or controls. We show how UJIVE can also be\nused to assess key assumptions underlying leniency designs, including\nquasi-random assignment and average first-stage monotonicity, and to probe the\nexternal validity of treatment effect estimates. We further discuss statistical\ninference, arguing that non-clustered standard errors are often appropriate. A\nreanalysis of Farre-Mensa et al. (2020), using quasi-random examiner assignment\nto estimate the value of patents to startups, illustrates our checklist."
                },
                "authors": [
                    {
                        "name": "Paul Goldsmith-Pinkham"
                    },
                    {
                        "name": "Peter Hull"
                    },
                    {
                        "name": "Michal Kolesr"
                    }
                ],
                "author_detail": {
                    "name": "Michal Kolesr"
                },
                "author": "Michal Kolesr",
                "arxiv_comment": "32 pages, including all appendices",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03572v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03572v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03570v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03570v1",
                "updated": "2025-11-05T15:51:03Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    15,
                    51,
                    3,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T15:51:03Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    15,
                    51,
                    3,
                    2,
                    309,
                    0
                ],
                "title": "TabGemma: Text-Based Tabular ICL via LLM using Continued Pretraining and\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TabGemma: Text-Based Tabular ICL via LLM using Continued Pretraining and\n  Retrieval"
                },
                "summary": "We study LLMs for tabular prediction with mixed text, numeric, and\ncategorical fields. We introduce TabGemma, a schema-agnostic in-context learner\nthat treats rows as sequences and tackles two practical hurdles when adapting\npretrained LLMs for tabular predictions: unstable numeric tokenization and\nlimited context size. We propose to canonicalize numbers via signed scientific\nnotation and continue pretraining of a 12B Gemma 3 model with a target\nimputation objective using a large-scale real world dataset. For inference, we\nuse a compact n-gram-based retrieval to select informative exemplars that fit\nwithin a 128k-token window.\n  On semantically rich benchmarks, TabGemma establishes a new state of the art\non classification across low- and high-data regimes and improves monotonically\nwith more context rows. For regression, it is competitive at small sample sizes\nbut trails conventional approaches as data grows. Our results show that LLMs\ncan be effective tabular in-context learners on highly semantic tasks when\npaired with dedicated numeric handling and context retrieval, while motivating\nfurther advances in numeric modeling and long-context scaling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study LLMs for tabular prediction with mixed text, numeric, and\ncategorical fields. We introduce TabGemma, a schema-agnostic in-context learner\nthat treats rows as sequences and tackles two practical hurdles when adapting\npretrained LLMs for tabular predictions: unstable numeric tokenization and\nlimited context size. We propose to canonicalize numbers via signed scientific\nnotation and continue pretraining of a 12B Gemma 3 model with a target\nimputation objective using a large-scale real world dataset. For inference, we\nuse a compact n-gram-based retrieval to select informative exemplars that fit\nwithin a 128k-token window.\n  On semantically rich benchmarks, TabGemma establishes a new state of the art\non classification across low- and high-data regimes and improves monotonically\nwith more context rows. For regression, it is competitive at small sample sizes\nbut trails conventional approaches as data grows. Our results show that LLMs\ncan be effective tabular in-context learners on highly semantic tasks when\npaired with dedicated numeric handling and context retrieval, while motivating\nfurther advances in numeric modeling and long-context scaling."
                },
                "authors": [
                    {
                        "name": "Gnther Schindler"
                    },
                    {
                        "name": "Maximilian Schambach"
                    },
                    {
                        "name": "Michael Medek"
                    },
                    {
                        "name": "Sam Thelin"
                    }
                ],
                "author_detail": {
                    "name": "Sam Thelin"
                },
                "author": "Sam Thelin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03570v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03570v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03563v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03563v1",
                "updated": "2025-11-05T15:45:52Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    15,
                    45,
                    52,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T15:45:52Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    15,
                    45,
                    52,
                    2,
                    309,
                    0
                ],
                "title": "ASVRI-Legal: Fine-Tuning LLMs with Retrieval Augmented Generation for\n  Enhanced Legal Regulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASVRI-Legal: Fine-Tuning LLMs with Retrieval Augmented Generation for\n  Enhanced Legal Regulation"
                },
                "summary": "In this study, we explore the fine-tuning of Large Language Models (LLMs) to\nbetter support policymakers in their crucial work of understanding, analyzing,\nand crafting legal regulations. To equip the model with a deep understanding of\nlegal texts, we curated a supervised dataset tailored to the specific needs of\nthe legal domain. Additionally, we integrated the Retrieval-Augmented\nGeneration (RAG) method, enabling the LLM to access and incorporate up-to-date\nlegal knowledge from external sources. This combination of fine-tuning and\nRAG-based augmentation results in a tool that not only processes legal\ninformation but actively assists policymakers in interpreting regulations and\ndrafting new ones that align with current needs. The results demonstrate that\nthis approach can significantly enhance the effectiveness of legal research and\nregulation development, offering a valuable resource in the ever-evolving field\nof law.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we explore the fine-tuning of Large Language Models (LLMs) to\nbetter support policymakers in their crucial work of understanding, analyzing,\nand crafting legal regulations. To equip the model with a deep understanding of\nlegal texts, we curated a supervised dataset tailored to the specific needs of\nthe legal domain. Additionally, we integrated the Retrieval-Augmented\nGeneration (RAG) method, enabling the LLM to access and incorporate up-to-date\nlegal knowledge from external sources. This combination of fine-tuning and\nRAG-based augmentation results in a tool that not only processes legal\ninformation but actively assists policymakers in interpreting regulations and\ndrafting new ones that align with current needs. The results demonstrate that\nthis approach can significantly enhance the effectiveness of legal research and\nregulation development, offering a valuable resource in the ever-evolving field\nof law."
                },
                "authors": [
                    {
                        "name": "One Octadion"
                    },
                    {
                        "name": "Bondan Sapta Prakoso"
                    },
                    {
                        "name": "Nanang Yudi Setiawan"
                    },
                    {
                        "name": "Novanto Yudistira"
                    }
                ],
                "author_detail": {
                    "name": "Novanto Yudistira"
                },
                "author": "Novanto Yudistira",
                "arxiv_comment": "11 pages (including references), 2 figures, 4 tables, published in\n  Atlantis Press (Open Access under CC BY-NC 4.0 license)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03563v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03563v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03555v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03555v1",
                "updated": "2025-11-05T15:36:19Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    15,
                    36,
                    19,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T15:36:19Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    15,
                    36,
                    19,
                    2,
                    309,
                    0
                ],
                "title": "Post-2024 U.S. Presidential Election Analysis of Election and Poll Data:\n  Real-life Validation of Prediction via Small Area Estimation and Uncertainty\n  Quantification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-2024 U.S. Presidential Election Analysis of Election and Poll Data:\n  Real-life Validation of Prediction via Small Area Estimation and Uncertainty\n  Quantification"
                },
                "summary": "We carry out a post-election analysis of the 2024 U.S. Presidential Election\n(USPE) using a prediction model derived from the Small Area Estimation (SAE)\nmethodology. With pollster data obtained one week prior to the election day,\nretrospectively, our SAE-based prediction model can perfectly predict the\nElectoral College election results in all 44 states where polling data were\navailable. In addition to such desirable prediction accuracy, we introduce the\nprobability of incorrect prediction (PoIP) to rigorously analyze prediction\nuncertainty. Since the standard bootstrap method appears inadequate for\nestimating PoIP, we propose a conformal inference method that yields reliable\nuncertainty quantification. We further investigate potential pollster biases by\nthe means of sensitivity analyses and conclude that swing states are\nparticularly vulnerable to polling bias in the prediction of the 2024 USPE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We carry out a post-election analysis of the 2024 U.S. Presidential Election\n(USPE) using a prediction model derived from the Small Area Estimation (SAE)\nmethodology. With pollster data obtained one week prior to the election day,\nretrospectively, our SAE-based prediction model can perfectly predict the\nElectoral College election results in all 44 states where polling data were\navailable. In addition to such desirable prediction accuracy, we introduce the\nprobability of incorrect prediction (PoIP) to rigorously analyze prediction\nuncertainty. Since the standard bootstrap method appears inadequate for\nestimating PoIP, we propose a conformal inference method that yields reliable\nuncertainty quantification. We further investigate potential pollster biases by\nthe means of sensitivity analyses and conclude that swing states are\nparticularly vulnerable to polling bias in the prediction of the 2024 USPE."
                },
                "authors": [
                    {
                        "name": "Zheshi Zheng"
                    },
                    {
                        "name": "Yuanyuan Li"
                    },
                    {
                        "name": "Peter X. K. Song"
                    },
                    {
                        "name": "Jiming Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Jiming Jiang"
                },
                "author": "Jiming Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03555v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03555v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2211.01446v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2211.01446v2",
                "updated": "2025-11-05T15:35:30Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    15,
                    35,
                    30,
                    2,
                    309,
                    0
                ],
                "published": "2022-11-02T19:37:55Z",
                "published_parsed": [
                    2022,
                    11,
                    2,
                    19,
                    37,
                    55,
                    2,
                    306,
                    0
                ],
                "title": "Trustworthy Representation Learning via Information Funnels and\n  Bottlenecks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trustworthy Representation Learning via Information Funnels and\n  Bottlenecks"
                },
                "summary": "Ensuring trustworthiness in machine learning -- by balancing utility,\nfairness, and privacy -- remains a critical challenge, particularly in\nrepresentation learning. In this work, we investigate a family of closely\nrelated information-theoretic objectives, including information funnels and\nbottlenecks, designed to extract invariant representations from data. We\nintroduce the Conditional Privacy Funnel with Side-information (CPFSI), a novel\nformulation within this family, applicable in both fully and semi-supervised\nsettings. Given the intractability of these objectives, we derive\nneural-network-based approximations via amortized variational inference. We\nsystematically analyze the trade-offs between utility, invariance, and\nrepresentation fidelity, offering new insights into the Pareto frontiers of\nthese methods. Our results demonstrate that CPFSI effectively balances these\ncompeting objectives and frequently outperforms existing approaches.\nFurthermore, we show that by intervening on sensitive attributes in CPFSI's\npredictive posterior enhances fairness while maintaining predictive\nperformance. Finally, we focus on the real-world applicability of these\napproaches, particularly for learning robust and fair representations from\ntabular datasets in data scarce-environments -- a modality where these methods\nare often especially relevant.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensuring trustworthiness in machine learning -- by balancing utility,\nfairness, and privacy -- remains a critical challenge, particularly in\nrepresentation learning. In this work, we investigate a family of closely\nrelated information-theoretic objectives, including information funnels and\nbottlenecks, designed to extract invariant representations from data. We\nintroduce the Conditional Privacy Funnel with Side-information (CPFSI), a novel\nformulation within this family, applicable in both fully and semi-supervised\nsettings. Given the intractability of these objectives, we derive\nneural-network-based approximations via amortized variational inference. We\nsystematically analyze the trade-offs between utility, invariance, and\nrepresentation fidelity, offering new insights into the Pareto frontiers of\nthese methods. Our results demonstrate that CPFSI effectively balances these\ncompeting objectives and frequently outperforms existing approaches.\nFurthermore, we show that by intervening on sensitive attributes in CPFSI's\npredictive posterior enhances fairness while maintaining predictive\nperformance. Finally, we focus on the real-world applicability of these\napproaches, particularly for learning robust and fair representations from\ntabular datasets in data scarce-environments -- a modality where these methods\nare often especially relevant."
                },
                "authors": [
                    {
                        "name": "Joo Machado de Freitas"
                    },
                    {
                        "name": "Bernhard C. Geiger"
                    }
                ],
                "author_detail": {
                    "name": "Bernhard C. Geiger"
                },
                "author": "Bernhard C. Geiger",
                "arxiv_doi": "10.1007/s10994-025-06924-9",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s10994-025-06924-9",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2211.01446v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2211.01446v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in Machine Learning (Springer), vol. 114, no. 12, Article\n  267, 2025",
                "arxiv_journal_ref": "Mach Learn 114, 267 (2025)",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07329v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07329v2",
                "updated": "2025-11-05T15:35:21Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    15,
                    35,
                    21,
                    2,
                    309,
                    0
                ],
                "published": "2025-03-10T13:42:04Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    13,
                    42,
                    4,
                    0,
                    69,
                    0
                ],
                "title": "Assessing the Macro and Micro Effects of Random Seeds on Fine-Tuning\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing the Macro and Micro Effects of Random Seeds on Fine-Tuning\n  Large Language Models"
                },
                "summary": "The impact of random seeds in fine-tuning large language models (LLMs) has\nbeen largely overlooked despite its potential influence on model performance.In\nthis study, we systematically evaluate the effects of random seeds on LLMs\nusing the GLUE and SuperGLUE benchmarks. We analyze the macro-level impact\nthrough traditional metrics like accuracy and F1, calculating their mean and\nvariance to quantify performance fluctuations. To capture the micro-level\neffects, we introduce a novel metric, consistency, measuring the stability of\nindividual predictions across runs. Our experiments reveal significant variance\nat both macro and micro levels, underscoring the need for careful consideration\nof random seeds in fine-tuning and evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The impact of random seeds in fine-tuning large language models (LLMs) has\nbeen largely overlooked despite its potential influence on model performance.In\nthis study, we systematically evaluate the effects of random seeds on LLMs\nusing the GLUE and SuperGLUE benchmarks. We analyze the macro-level impact\nthrough traditional metrics like accuracy and F1, calculating their mean and\nvariance to quantify performance fluctuations. To capture the micro-level\neffects, we introduce a novel metric, consistency, measuring the stability of\nindividual predictions across runs. Our experiments reveal significant variance\nat both macro and micro levels, underscoring the need for careful consideration\nof random seeds in fine-tuning and evaluation."
                },
                "authors": [
                    {
                        "name": "Nghia Bui"
                    },
                    {
                        "name": "Guergana Savova"
                    },
                    {
                        "name": "Lijing Wang"
                    }
                ],
                "author_detail": {
                    "name": "Lijing Wang"
                },
                "author": "Lijing Wang",
                "arxiv_comment": "7 pages, 5 tables, 3 figures. Accepted at IJCNLP 2025. This is the\n  final, peer-reviewed version of the work, which supersedes and extends the\n  unauthorized draft previously posted as arXiv:2503.07329",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07329v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07329v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16301v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16301v2",
                "updated": "2025-11-05T15:35:06Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    15,
                    35,
                    6,
                    2,
                    309,
                    0
                ],
                "published": "2024-10-07T06:30:59Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    6,
                    30,
                    59,
                    0,
                    281,
                    0
                ],
                "title": "Intelligent Computing Social Modeling and Methodological Innovations in\n  Political Science in the Era of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intelligent Computing Social Modeling and Methodological Innovations in\n  Political Science in the Era of Large Language Models"
                },
                "summary": "The recent wave of artificial intelligence, epitomized by large language\nmodels (LLMs),has presented opportunities and challenges for methodological\ninnovation in political science,sparking discussions on a potential paradigm\nshift in the social sciences. However, how can weunderstand the impact of LLMs\non knowledge production and paradigm transformation in thesocial sciences from\na comprehensive perspective that integrates technology and methodology? What\nare LLMs' specific applications and representative innovative methods in\npolitical scienceresearch? These questions, particularly from a practical\nmethodological standpoint, remainunderexplored. This paper proposes the\n\"Intelligent Computing Social Modeling\" (ICSM) methodto address these issues by\nclarifying the critical mechanisms of LLMs. ICSM leverages thestrengths of LLMs\nin idea synthesis and action simulation, advancing intellectual exploration\ninpolitical science through \"simulated social construction\" and \"simulation\nvalidation.\" Bysimulating the U.S. presidential election, this study\nempirically demonstrates the operationalpathways and methodological advantages\nof ICSM. By integrating traditional social scienceparadigms, ICSM not only\nenhances the quantitative paradigm's capability to apply big data toassess the\nimpact of factors but also provides qualitative paradigms with evidence for\nsocialmechanism discovery at the individual level, offering a powerful tool\nthat balances interpretabilityand predictability in social science research.\nThe findings suggest that LLMs will drivemethodological innovation in political\nscience through integration and improvement rather thandirect substitution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent wave of artificial intelligence, epitomized by large language\nmodels (LLMs),has presented opportunities and challenges for methodological\ninnovation in political science,sparking discussions on a potential paradigm\nshift in the social sciences. However, how can weunderstand the impact of LLMs\non knowledge production and paradigm transformation in thesocial sciences from\na comprehensive perspective that integrates technology and methodology? What\nare LLMs' specific applications and representative innovative methods in\npolitical scienceresearch? These questions, particularly from a practical\nmethodological standpoint, remainunderexplored. This paper proposes the\n\"Intelligent Computing Social Modeling\" (ICSM) methodto address these issues by\nclarifying the critical mechanisms of LLMs. ICSM leverages thestrengths of LLMs\nin idea synthesis and action simulation, advancing intellectual exploration\ninpolitical science through \"simulated social construction\" and \"simulation\nvalidation.\" Bysimulating the U.S. presidential election, this study\nempirically demonstrates the operationalpathways and methodological advantages\nof ICSM. By integrating traditional social scienceparadigms, ICSM not only\nenhances the quantitative paradigm's capability to apply big data toassess the\nimpact of factors but also provides qualitative paradigms with evidence for\nsocialmechanism discovery at the individual level, offering a powerful tool\nthat balances interpretabilityand predictability in social science research.\nThe findings suggest that LLMs will drivemethodological innovation in political\nscience through integration and improvement rather thandirect substitution."
                },
                "authors": [
                    {
                        "name": "Zhenyu Wang"
                    },
                    {
                        "name": "Dequan Wang"
                    },
                    {
                        "name": "Yi Xu"
                    },
                    {
                        "name": "Lingfeng Zhou"
                    },
                    {
                        "name": "Yiqi Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Yiqi Zhou"
                },
                "author": "Yiqi Zhou",
                "arxiv_doi": "10.1007/s11366-025-09917-6",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s11366-025-09917-6",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.16301v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16301v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "37 pages, 11 figures, 3 tables. J OF CHIN POLIT SCI (2025)",
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03553v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03553v1",
                "updated": "2025-11-05T15:34:48Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    15,
                    34,
                    48,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T15:34:48Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    15,
                    34,
                    48,
                    2,
                    309,
                    0
                ],
                "title": "MultiZebraLogic: A Multilingual Logical Reasoning Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MultiZebraLogic: A Multilingual Logical Reasoning Benchmark"
                },
                "summary": "Measuring the full abilities of large language models (LLMs) requires\nbenchmarks representing multiple tasks. We aim to create large, high-quality\ndatasets for comparison of logical reasoning skills across several languages\nand of suitable difficulty for LLMs of various reasoning ability. We explore\nmultiple ways of increasing difficulty. We generate zebra puzzles in multiple\nlanguages, themes, sizes and including 14 different clue types and 8 red\nherring types (uninformative clues). We find puzzle sizes 2x3 and 4x5 are\nsufficiently challenging for GPT-4o mini (a non-reasoning model) and o3-mini (a\nreasoning model), respectively. Including 5 red herrings decreases o3-mini\npuzzle-level accuracy on 4x5 puzzles by 15$\\pm$7 %. Scores of o3-mini on 4x5\npuzzles are not significantly affected by use of English vs. Danish or the\ncommon houses theme vs. the country-specific smoerrebroed theme. We find no\ncorrelation between difficulty and the selected clue types. Datasets of\n128+1024 puzzles are published as MultiZebraLogic in each of nine Germanic\nlanguages for sizes 2x3 and 4x5. We publish code for puzzle generation,\ndesigned for adaptablity into more languages and themes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring the full abilities of large language models (LLMs) requires\nbenchmarks representing multiple tasks. We aim to create large, high-quality\ndatasets for comparison of logical reasoning skills across several languages\nand of suitable difficulty for LLMs of various reasoning ability. We explore\nmultiple ways of increasing difficulty. We generate zebra puzzles in multiple\nlanguages, themes, sizes and including 14 different clue types and 8 red\nherring types (uninformative clues). We find puzzle sizes 2x3 and 4x5 are\nsufficiently challenging for GPT-4o mini (a non-reasoning model) and o3-mini (a\nreasoning model), respectively. Including 5 red herrings decreases o3-mini\npuzzle-level accuracy on 4x5 puzzles by 15$\\pm$7 %. Scores of o3-mini on 4x5\npuzzles are not significantly affected by use of English vs. Danish or the\ncommon houses theme vs. the country-specific smoerrebroed theme. We find no\ncorrelation between difficulty and the selected clue types. Datasets of\n128+1024 puzzles are published as MultiZebraLogic in each of nine Germanic\nlanguages for sizes 2x3 and 4x5. We publish code for puzzle generation,\ndesigned for adaptablity into more languages and themes."
                },
                "authors": [
                    {
                        "name": "Sofie Helene Bruun"
                    },
                    {
                        "name": "Dan Saattrup Smart"
                    }
                ],
                "author_detail": {
                    "name": "Dan Saattrup Smart"
                },
                "author": "Dan Saattrup Smart",
                "arxiv_comment": "Submitted to LREC 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03553v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03553v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03549v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03549v1",
                "updated": "2025-11-05T15:31:42Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    15,
                    31,
                    42,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T15:31:42Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    15,
                    31,
                    42,
                    2,
                    309,
                    0
                ],
                "title": "Uncovering Code Insights: Leveraging GitHub Artifacts for Deeper Code\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncovering Code Insights: Leveraging GitHub Artifacts for Deeper Code\n  Understanding"
                },
                "summary": "Understanding the purpose of source code is a critical task in software\nmaintenance, onboarding, and modernization. While large language models (LLMs)\nhave shown promise in generating code explanations, they often lack grounding\nin the broader software engineering context. We propose a novel approach that\nleverages natural language artifacts from GitHub -- such as pull request\ndescriptions, issue descriptions and discussions, and commit messages -- to\nenhance LLM-based code understanding. Our system consists of three components:\none that extracts and structures relevant GitHub context, another that uses\nthis context to generate high-level explanations of the code's purpose, and a\nthird that validates the explanation. We implemented this as a standalone tool,\nas well as a server within the Model Context Protocol (MCP), enabling\nintegration with other AI-assisted development tools. Our main use case is that\nof enhancing a standard LLM-based code explanation with code insights that our\nsystem generates. To evaluate explanations' quality, we conducted a small scale\nuser study, with developers of several open projects, as well as developers of\nproprietary projects. Our user study indicates that when insights are generated\nthey often are helpful and non trivial, and are free from hallucinations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the purpose of source code is a critical task in software\nmaintenance, onboarding, and modernization. While large language models (LLMs)\nhave shown promise in generating code explanations, they often lack grounding\nin the broader software engineering context. We propose a novel approach that\nleverages natural language artifacts from GitHub -- such as pull request\ndescriptions, issue descriptions and discussions, and commit messages -- to\nenhance LLM-based code understanding. Our system consists of three components:\none that extracts and structures relevant GitHub context, another that uses\nthis context to generate high-level explanations of the code's purpose, and a\nthird that validates the explanation. We implemented this as a standalone tool,\nas well as a server within the Model Context Protocol (MCP), enabling\nintegration with other AI-assisted development tools. Our main use case is that\nof enhancing a standard LLM-based code explanation with code insights that our\nsystem generates. To evaluate explanations' quality, we conducted a small scale\nuser study, with developers of several open projects, as well as developers of\nproprietary projects. Our user study indicates that when insights are generated\nthey often are helpful and non trivial, and are free from hallucinations."
                },
                "authors": [
                    {
                        "name": "Ziv Nevo"
                    },
                    {
                        "name": "Orna Raz"
                    },
                    {
                        "name": "Karen Yorav"
                    }
                ],
                "author_detail": {
                    "name": "Karen Yorav"
                },
                "author": "Karen Yorav",
                "arxiv_comment": "7 pages, 6 figures, to be published in AISM 2025, see\n  https://aism25.github.io/aism25/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03549v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03549v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18544v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18544v2",
                "updated": "2025-11-05T15:28:30Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    15,
                    28,
                    30,
                    2,
                    309,
                    0
                ],
                "published": "2025-07-24T16:10:20Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    16,
                    10,
                    20,
                    3,
                    205,
                    0
                ],
                "title": "EP250108a/SN2025kg: A Magnetar-powered Gamma-Ray Burst Supernova\n  Originating from a Close Helium-star Binary via Isolated Binary Evolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EP250108a/SN2025kg: A Magnetar-powered Gamma-Ray Burst Supernova\n  Originating from a Close Helium-star Binary via Isolated Binary Evolution"
                },
                "summary": "SN\\,2025kg, linked to EP250108a, is among the brightest broad-lined Type Ic\nsupernova (SN Ic-BL) known, showing unique helium absorptions, a late-time\nbroad H$\\alpha$, and an early bump. In this {\\em{Letter}}, we propose a\njet-cocoon origin to explain EP250108a as off-axis cooling emission from a\nmildly relativistic inner cocoon viewed at $\\sim45^\\circ$ and the early bump of\nSN\\,2025kg as the outer cocoon cooling emission, both constraining an energy of\n$\\sim(1-2)\\times10^{52}{\\rm{erg}}$ and a progenitor radius of $\\sim5\\,R_\\odot$.\nTo explain SN\\,2025kg's exceptionally luminous peak, potential energy injection\ninto the $\\sim2.5\\,M_\\odot$ ejecta from a magnetar with initial period\n$\\sim1.7\\,{\\rm{ms}}$ and magnetic field $\\sim2\\times10^{15}{\\rm{G}}$ may be\nrequired, implying a rapidly rotating $\\sim4\\,M_\\odot$ progenitor. Thus, the\nprogenitor may be a low-mass helium star with an extended helium envelope,\nsupported by helium absorption lines and an inferred weak pre-SN wind.\nHydrogen-rich material may reside in the inner ejecta layers, as suggested by\nthe late-time broad H$\\alpha$, possibly originating from main-sequence\ncompanion material evaporated by the magnetar wind. Since the observed\nnear-solar metallicity challenges the popular quasi-chemically homogeneous\nevolution channel, the rapidly rotating helium-star progenitor of\nEP250108a/SN\\,2025kg might attain angular momentum by being tidally spun up by\na main-sequence companion in a close binary formed through isolated binary\nevolution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SN\\,2025kg, linked to EP250108a, is among the brightest broad-lined Type Ic\nsupernova (SN Ic-BL) known, showing unique helium absorptions, a late-time\nbroad H$\\alpha$, and an early bump. In this {\\em{Letter}}, we propose a\njet-cocoon origin to explain EP250108a as off-axis cooling emission from a\nmildly relativistic inner cocoon viewed at $\\sim45^\\circ$ and the early bump of\nSN\\,2025kg as the outer cocoon cooling emission, both constraining an energy of\n$\\sim(1-2)\\times10^{52}{\\rm{erg}}$ and a progenitor radius of $\\sim5\\,R_\\odot$.\nTo explain SN\\,2025kg's exceptionally luminous peak, potential energy injection\ninto the $\\sim2.5\\,M_\\odot$ ejecta from a magnetar with initial period\n$\\sim1.7\\,{\\rm{ms}}$ and magnetic field $\\sim2\\times10^{15}{\\rm{G}}$ may be\nrequired, implying a rapidly rotating $\\sim4\\,M_\\odot$ progenitor. Thus, the\nprogenitor may be a low-mass helium star with an extended helium envelope,\nsupported by helium absorption lines and an inferred weak pre-SN wind.\nHydrogen-rich material may reside in the inner ejecta layers, as suggested by\nthe late-time broad H$\\alpha$, possibly originating from main-sequence\ncompanion material evaporated by the magnetar wind. Since the observed\nnear-solar metallicity challenges the popular quasi-chemically homogeneous\nevolution channel, the rapidly rotating helium-star progenitor of\nEP250108a/SN\\,2025kg might attain angular momentum by being tidally spun up by\na main-sequence companion in a close binary formed through isolated binary\nevolution."
                },
                "authors": [
                    {
                        "name": "Jin-Ping Zhu"
                    },
                    {
                        "name": "Jian-He Zheng"
                    },
                    {
                        "name": "Bing Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Bing Zhang"
                },
                "author": "Bing Zhang",
                "arxiv_comment": "11 pages, 5 figures, accepted by MNRAS Letters",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18544v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18544v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15053v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15053v2",
                "updated": "2025-11-05T15:21:15Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    15,
                    21,
                    15,
                    2,
                    309,
                    0
                ],
                "published": "2025-04-21T12:26:27Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    12,
                    26,
                    27,
                    0,
                    111,
                    0
                ],
                "title": "One pathogen does not an epidemic make: A review of interacting\n  contagions, diseases, beliefs, and stories",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One pathogen does not an epidemic make: A review of interacting\n  contagions, diseases, beliefs, and stories"
                },
                "summary": "From pathogens and computer viruses to genes and memes, contagion models have\nfound widespread utility across the natural and social sciences. Despite their\nsuccess and breadth of adoption, the approach and structure of these models\nremain surprisingly siloed by field. Given the siloed nature of their\ndevelopment and widespread use, one persistent assumption is that a given\ncontagion can be studied in isolation, independently from what else might be\nspreading in the population. In reality, countless contagions of biological and\nsocial nature interact within hosts (interacting with existing beliefs, or the\nimmune system) and across hosts (interacting in the environment, or affecting\ntransmission mechanisms). Additionally, from a modeling perspective, we know\nthat relaxing these assumptions has profound effects on the physics and\ntranslational implications of the models. Here, we review mechanisms for\ninteractions in social and biological contagions, as well as the models and\nframeworks developed to include these interactions in the study of the\ncontagions. We highlight existing problems related to the inference of\ninteractions and to the scalability of mathematical models and identify\npromising avenues of future inquiries. In doing so, we highlight the need for\ninterdisciplinary efforts under a unified science of contagions and for\nremoving a common dichotomy between social and biological contagions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From pathogens and computer viruses to genes and memes, contagion models have\nfound widespread utility across the natural and social sciences. Despite their\nsuccess and breadth of adoption, the approach and structure of these models\nremain surprisingly siloed by field. Given the siloed nature of their\ndevelopment and widespread use, one persistent assumption is that a given\ncontagion can be studied in isolation, independently from what else might be\nspreading in the population. In reality, countless contagions of biological and\nsocial nature interact within hosts (interacting with existing beliefs, or the\nimmune system) and across hosts (interacting in the environment, or affecting\ntransmission mechanisms). Additionally, from a modeling perspective, we know\nthat relaxing these assumptions has profound effects on the physics and\ntranslational implications of the models. Here, we review mechanisms for\ninteractions in social and biological contagions, as well as the models and\nframeworks developed to include these interactions in the study of the\ncontagions. We highlight existing problems related to the inference of\ninteractions and to the scalability of mathematical models and identify\npromising avenues of future inquiries. In doing so, we highlight the need for\ninterdisciplinary efforts under a unified science of contagions and for\nremoving a common dichotomy between social and biological contagions."
                },
                "authors": [
                    {
                        "name": "Laurent Hbert-Dufresne"
                    },
                    {
                        "name": "Yong-Yeol Ahn"
                    },
                    {
                        "name": "Antoine Allard"
                    },
                    {
                        "name": "Vittoria Colizza"
                    },
                    {
                        "name": "Jessica W. Crothers"
                    },
                    {
                        "name": "Peter Sheridan Dodds"
                    },
                    {
                        "name": "Mirta Galesic"
                    },
                    {
                        "name": "Fakhteh Ghanbarnejad"
                    },
                    {
                        "name": "Dominique Gravel"
                    },
                    {
                        "name": "Ross A. Hammond"
                    },
                    {
                        "name": "Kristina Lerman"
                    },
                    {
                        "name": "Juniper Lovato"
                    },
                    {
                        "name": "John J. Openshaw"
                    },
                    {
                        "name": "S. Redner"
                    },
                    {
                        "name": "Samuel V. Scarpino"
                    },
                    {
                        "name": "Guillaume St-Onge"
                    },
                    {
                        "name": "Timothy R. Tangherlini"
                    },
                    {
                        "name": "Jean-Gabriel Young"
                    }
                ],
                "author_detail": {
                    "name": "Jean-Gabriel Young"
                },
                "author": "Jean-Gabriel Young",
                "arxiv_doi": "10.1038/s44260-025-00050-2",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1038/s44260-025-00050-2",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.15053v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15053v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "npj Complex 2, 26 (2025)",
                "arxiv_primary_category": {
                    "term": "physics.soc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.PE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.01910v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.01910v2",
                "updated": "2025-11-05T15:15:41Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    15,
                    15,
                    41,
                    2,
                    309,
                    0
                ],
                "published": "2025-10-31T20:20:10Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    20,
                    20,
                    10,
                    4,
                    304,
                    0
                ],
                "title": "Security Audit of intel ICE Driver for e810 Network Interface Card",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Security Audit of intel ICE Driver for e810 Network Interface Card"
                },
                "summary": "The security of enterprise-grade networking hardware and software is critical\nto ensuring the integrity, availability, and confidentiality of data in modern\ncloud and data center environments. Network interface controllers (NICs) play a\npivotal role in high-performance computing and virtualization, but their\nprivileged access to system resources makes them a prime target for security\nvulnerabilities. This study presents a security analysis of the Intel ICE\ndriver using the E810 Ethernet Controller, employing static analysis, fuzz\ntesting, and timing-based side-channel evaluation to assess robustness against\nexploitation. The objective is to evaluate the drivers resilience to malformed\ninputs, identify implementation weaknesses, and determine whether timing\ndiscrepancies can be exploited for unauthorized inference of system states.\nStatic code analysis reveals that insufficient bounds checking and unsafe\nstring operations may introduce security flaws. Fuzz testing targets the Admin\nQueue, debugfs interface, and virtual function (VF) management. Interface-aware\nfuzzing and command mutation confirm strong input validation that prevents\nmemory corruption and privilege escalation under normal conditions. However,\nusing principles from KernelSnitch, the driver is found to be susceptible to\ntiming-based side-channel attacks. Execution time discrepancies in hash table\nlookups allow an unprivileged attacker to infer VF occupancy states, enabling\npotential network mapping in multi-tenant environments. Further analysis shows\ninefficiencies in Read-Copy-Update (RCU) synchronization, where missing\nsynchronization leads to stale data persistence, memory leaks, and\nout-of-memory conditions. Kernel instrumentation confirms that occupied VF\nlookups complete faster than unoccupied queries, exposing timing-based\ninformation leakage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The security of enterprise-grade networking hardware and software is critical\nto ensuring the integrity, availability, and confidentiality of data in modern\ncloud and data center environments. Network interface controllers (NICs) play a\npivotal role in high-performance computing and virtualization, but their\nprivileged access to system resources makes them a prime target for security\nvulnerabilities. This study presents a security analysis of the Intel ICE\ndriver using the E810 Ethernet Controller, employing static analysis, fuzz\ntesting, and timing-based side-channel evaluation to assess robustness against\nexploitation. The objective is to evaluate the drivers resilience to malformed\ninputs, identify implementation weaknesses, and determine whether timing\ndiscrepancies can be exploited for unauthorized inference of system states.\nStatic code analysis reveals that insufficient bounds checking and unsafe\nstring operations may introduce security flaws. Fuzz testing targets the Admin\nQueue, debugfs interface, and virtual function (VF) management. Interface-aware\nfuzzing and command mutation confirm strong input validation that prevents\nmemory corruption and privilege escalation under normal conditions. However,\nusing principles from KernelSnitch, the driver is found to be susceptible to\ntiming-based side-channel attacks. Execution time discrepancies in hash table\nlookups allow an unprivileged attacker to infer VF occupancy states, enabling\npotential network mapping in multi-tenant environments. Further analysis shows\ninefficiencies in Read-Copy-Update (RCU) synchronization, where missing\nsynchronization leads to stale data persistence, memory leaks, and\nout-of-memory conditions. Kernel instrumentation confirms that occupied VF\nlookups complete faster than unoccupied queries, exposing timing-based\ninformation leakage."
                },
                "authors": [
                    {
                        "name": "Oisin O Sullivan"
                    }
                ],
                "author_detail": {
                    "name": "Oisin O Sullivan"
                },
                "author": "Oisin O Sullivan",
                "arxiv_comment": "Final Year Project Report, submitted 24/03/2025 as part of Bachelor\n  of Science in Cyber Security and IT Forensics at the University Of Limerick",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.01910v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.01910v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23734v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23734v3",
                "updated": "2025-11-05T15:14:01Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    15,
                    14,
                    1,
                    2,
                    309,
                    0
                ],
                "published": "2025-05-29T17:57:04Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    57,
                    4,
                    3,
                    149,
                    0
                ],
                "title": "ZPressor: Bottleneck-Aware Compression for Scalable Feed-Forward 3DGS",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZPressor: Bottleneck-Aware Compression for Scalable Feed-Forward 3DGS"
                },
                "summary": "Feed-forward 3D Gaussian Splatting (3DGS) models have recently emerged as a\npromising solution for novel view synthesis, enabling one-pass inference\nwithout the need for per-scene 3DGS optimization. However, their scalability is\nfundamentally constrained by the limited capacity of their models, leading to\ndegraded performance or excessive memory consumption as the number of input\nviews increases. In this work, we analyze feed-forward 3DGS frameworks through\nthe lens of the Information Bottleneck principle and introduce ZPressor, a\nlightweight architecture-agnostic module that enables efficient compression of\nmulti-view inputs into a compact latent state $Z$ that retains essential scene\ninformation while discarding redundancy. Concretely, ZPressor enables existing\nfeed-forward 3DGS models to scale to over 100 input views at 480P resolution on\nan 80GB GPU, by partitioning the views into anchor and support sets and using\ncross attention to compress the information from the support views into anchor\nviews, forming the compressed latent state $Z$. We show that integrating\nZPressor into several state-of-the-art feed-forward 3DGS models consistently\nimproves performance under moderate input views and enhances robustness under\ndense view settings on two large-scale benchmarks DL3DV-10K and RealEstate10K.\nThe video results, code and trained models are available on our project page:\nhttps://lhmd.top/zpressor.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Feed-forward 3D Gaussian Splatting (3DGS) models have recently emerged as a\npromising solution for novel view synthesis, enabling one-pass inference\nwithout the need for per-scene 3DGS optimization. However, their scalability is\nfundamentally constrained by the limited capacity of their models, leading to\ndegraded performance or excessive memory consumption as the number of input\nviews increases. In this work, we analyze feed-forward 3DGS frameworks through\nthe lens of the Information Bottleneck principle and introduce ZPressor, a\nlightweight architecture-agnostic module that enables efficient compression of\nmulti-view inputs into a compact latent state $Z$ that retains essential scene\ninformation while discarding redundancy. Concretely, ZPressor enables existing\nfeed-forward 3DGS models to scale to over 100 input views at 480P resolution on\nan 80GB GPU, by partitioning the views into anchor and support sets and using\ncross attention to compress the information from the support views into anchor\nviews, forming the compressed latent state $Z$. We show that integrating\nZPressor into several state-of-the-art feed-forward 3DGS models consistently\nimproves performance under moderate input views and enhances robustness under\ndense view settings on two large-scale benchmarks DL3DV-10K and RealEstate10K.\nThe video results, code and trained models are available on our project page:\nhttps://lhmd.top/zpressor."
                },
                "authors": [
                    {
                        "name": "Weijie Wang"
                    },
                    {
                        "name": "Donny Y. Chen"
                    },
                    {
                        "name": "Zeyu Zhang"
                    },
                    {
                        "name": "Duochao Shi"
                    },
                    {
                        "name": "Akide Liu"
                    },
                    {
                        "name": "Bohan Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Bohan Zhuang"
                },
                "author": "Bohan Zhuang",
                "arxiv_comment": "NeurIPS 2025, Project Page: https://lhmd.top/zpressor, Code:\n  https://github.com/ziplab/ZPressor",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23734v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23734v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09630v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09630v2",
                "updated": "2025-11-05T15:13:40Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    15,
                    13,
                    40,
                    2,
                    309,
                    0
                ],
                "published": "2025-03-11T18:20:20Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    18,
                    20,
                    20,
                    1,
                    70,
                    0
                ],
                "title": "CASteer: Steering Diffusion Models for Controllable Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CASteer: Steering Diffusion Models for Controllable Generation"
                },
                "summary": "Diffusion models have transformed image generation, yet controlling their\noutputs to reliably erase undesired concepts remains challenging. Existing\napproaches usually require task-specific training and struggle to generalize\nacross both concrete (e.g., objects) and abstract (e.g., styles) concepts. We\npropose CASteer (Cross-Attention Steering), a training-free framework for\nconcept erasure in diffusion models using steering vectors to influence hidden\nrepresentations dynamically. CASteer precomputes concept-specific steering\nvectors by averaging neural activations from images generated for each target\nconcept. During inference, it dynamically applies these vectors to suppress\nundesired concepts only when they appear, ensuring that unrelated regions\nremain unaffected. This selective activation enables precise, context-aware\nerasure without degrading overall image quality. This approach achieves\neffective removal of harmful or unwanted content across a wide range of visual\nconcepts, all without model retraining. CASteer outperforms state-of-the-art\nconcept erasure techniques while preserving unrelated content and minimizing\nunintended effects. Pseudocode is provided in the supplementary.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have transformed image generation, yet controlling their\noutputs to reliably erase undesired concepts remains challenging. Existing\napproaches usually require task-specific training and struggle to generalize\nacross both concrete (e.g., objects) and abstract (e.g., styles) concepts. We\npropose CASteer (Cross-Attention Steering), a training-free framework for\nconcept erasure in diffusion models using steering vectors to influence hidden\nrepresentations dynamically. CASteer precomputes concept-specific steering\nvectors by averaging neural activations from images generated for each target\nconcept. During inference, it dynamically applies these vectors to suppress\nundesired concepts only when they appear, ensuring that unrelated regions\nremain unaffected. This selective activation enables precise, context-aware\nerasure without degrading overall image quality. This approach achieves\neffective removal of harmful or unwanted content across a wide range of visual\nconcepts, all without model retraining. CASteer outperforms state-of-the-art\nconcept erasure techniques while preserving unrelated content and minimizing\nunintended effects. Pseudocode is provided in the supplementary."
                },
                "authors": [
                    {
                        "name": "Tatiana Gaintseva"
                    },
                    {
                        "name": "Andreea-Maria Oncescu"
                    },
                    {
                        "name": "Chengcheng Ma"
                    },
                    {
                        "name": "Ziquan Liu"
                    },
                    {
                        "name": "Martin Benning"
                    },
                    {
                        "name": "Gregory Slabaugh"
                    },
                    {
                        "name": "Jiankang Deng"
                    },
                    {
                        "name": "Ismail Elezi"
                    }
                ],
                "author_detail": {
                    "name": "Ismail Elezi"
                },
                "author": "Ismail Elezi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09630v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09630v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15976v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15976v2",
                "updated": "2025-11-05T15:07:38Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    15,
                    7,
                    38,
                    2,
                    309,
                    0
                ],
                "published": "2025-09-19T13:36:25Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    13,
                    36,
                    25,
                    4,
                    262,
                    0
                ],
                "title": "The GECKOS Survey: revealing the formation history of a barred galaxy\n  via structural decomposition and resolved spectroscopy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The GECKOS Survey: revealing the formation history of a barred galaxy\n  via structural decomposition and resolved spectroscopy"
                },
                "summary": "Disentangling the (co-)evolution of individual galaxy structural components\nremains a difficult task, owing to the inability to cleanly isolate light from\nspatially overlapping components. In this pilot study of PGC\\,044931, observed\nas part of the GECKOS survey, we utilise a VIRCAM $H$-band image to decompose\nthe galaxy into five photometric components, three of which dominate by\ncontributing $>50\\%$ of light in specific regions: a main disc, a boxy/peanut\nbulge, and a nuclear disc. When the photometric decompositions are mapped onto\nMUSE observations, we find remarkably good separation in stellar kinematic\nspace. All three structures occupy unique locations in the parameter space of\nthe ratio of the light-weighted stellar line-of-sight mean velocity and\nvelocity dispersion ($\\rm{V}_{\\star}/\\sigma_{\\star}$), and the high-order\nstellar skew ($h_{3}$). These clear and distinct kinematic behaviours allow us\nto make inferences about the formation histories of the individual components\nfrom observations of the mean stellar ages and metallicities of the three\ncomponents. A clear story emerges: the main disc built over a sustained and\nextended star formation phase, possibly partly fuelled by gas from a\nlow-metallicity reservoir. Early on, that disc formed a bar that buckled and\nsubsequently formed a nuclear disc in multiple and enriched star-formation\nepisodes. This result is an example of how careful photometric decompositions,\ncombined with spatially well-resolved stellar kinematic information, can help\nseparate out age-metallicity relations of different components and therefore\ndisentangle the formation history of a galaxy. The results of this pilot study\ncan be extended to a differential study of all GECKOS survey galaxies to assert\nthe true diversity of Milky Way-like galaxies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disentangling the (co-)evolution of individual galaxy structural components\nremains a difficult task, owing to the inability to cleanly isolate light from\nspatially overlapping components. In this pilot study of PGC\\,044931, observed\nas part of the GECKOS survey, we utilise a VIRCAM $H$-band image to decompose\nthe galaxy into five photometric components, three of which dominate by\ncontributing $>50\\%$ of light in specific regions: a main disc, a boxy/peanut\nbulge, and a nuclear disc. When the photometric decompositions are mapped onto\nMUSE observations, we find remarkably good separation in stellar kinematic\nspace. All three structures occupy unique locations in the parameter space of\nthe ratio of the light-weighted stellar line-of-sight mean velocity and\nvelocity dispersion ($\\rm{V}_{\\star}/\\sigma_{\\star}$), and the high-order\nstellar skew ($h_{3}$). These clear and distinct kinematic behaviours allow us\nto make inferences about the formation histories of the individual components\nfrom observations of the mean stellar ages and metallicities of the three\ncomponents. A clear story emerges: the main disc built over a sustained and\nextended star formation phase, possibly partly fuelled by gas from a\nlow-metallicity reservoir. Early on, that disc formed a bar that buckled and\nsubsequently formed a nuclear disc in multiple and enriched star-formation\nepisodes. This result is an example of how careful photometric decompositions,\ncombined with spatially well-resolved stellar kinematic information, can help\nseparate out age-metallicity relations of different components and therefore\ndisentangle the formation history of a galaxy. The results of this pilot study\ncan be extended to a differential study of all GECKOS survey galaxies to assert\nthe true diversity of Milky Way-like galaxies."
                },
                "authors": [
                    {
                        "name": "A. Fraser-McKelvie"
                    },
                    {
                        "name": "D. A. Gadotti"
                    },
                    {
                        "name": "F. Fragkoudi"
                    },
                    {
                        "name": "C. de S-Freitas"
                    },
                    {
                        "name": "M. Martig"
                    },
                    {
                        "name": "M. Bureau"
                    },
                    {
                        "name": "T. Davis"
                    },
                    {
                        "name": "R. Elliott"
                    },
                    {
                        "name": "E. Emsellem"
                    },
                    {
                        "name": "D. Fisher"
                    },
                    {
                        "name": "M. R. Hayden"
                    },
                    {
                        "name": "J. van de Sande"
                    },
                    {
                        "name": "A. B. Watts"
                    }
                ],
                "author_detail": {
                    "name": "A. B. Watts"
                },
                "author": "A. B. Watts",
                "arxiv_comment": "5 pages, 3 figures, accepted by A&A",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15976v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15976v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02625v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02625v3",
                "updated": "2025-11-05T15:07:16Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    15,
                    7,
                    16,
                    2,
                    309,
                    0
                ],
                "published": "2025-01-05T18:41:54Z",
                "published_parsed": [
                    2025,
                    1,
                    5,
                    18,
                    41,
                    54,
                    6,
                    5,
                    0
                ],
                "title": "HALO: Hadamard-Assisted Lower-Precision Optimization for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HALO: Hadamard-Assisted Lower-Precision Optimization for LLMs"
                },
                "summary": "Quantized training of Large Language Models (LLMs) remains an open challenge,\nas maintaining accuracy while performing all matrix multiplications in low\nprecision has proven difficult. This is particularly the case when fine-tuning\npre-trained models, which can have large weight and activation outlier values\nthat make lower-precision optimization difficult. To address this, we present\nHALO, a novel quantization-aware training approach for Transformers that\nenables accurate and efficient low-precision training by combining 1) strategic\nplacement of Hadamard rotations in both forward and backward passes, which\nmitigate outliers, 2) high-performance kernel support, and 3) FSDP integration\nfor low-precision communication. Our approach ensures that all large matrix\nmultiplications during the forward and backward passes are executed in lower\nprecision. Applied to LLAMA-family models, HALO achieves\nnear-full-precision-equivalent results during fine-tuning on various tasks,\nwhile delivering up to 1.41x end-to-end speedup for full fine-tuning on RTX\n4090 GPUs. HALO efficiently supports both standard and parameterefficient\nfine-tuning (PEFT). Our results demonstrate the first practical approach to\nfully quantized LLM fine-tuning that maintains accuracy in 8-bit precision,\nwhile delivering performance benefits. Code is available at\nhttps://github.com/IST-DASLab/HALO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantized training of Large Language Models (LLMs) remains an open challenge,\nas maintaining accuracy while performing all matrix multiplications in low\nprecision has proven difficult. This is particularly the case when fine-tuning\npre-trained models, which can have large weight and activation outlier values\nthat make lower-precision optimization difficult. To address this, we present\nHALO, a novel quantization-aware training approach for Transformers that\nenables accurate and efficient low-precision training by combining 1) strategic\nplacement of Hadamard rotations in both forward and backward passes, which\nmitigate outliers, 2) high-performance kernel support, and 3) FSDP integration\nfor low-precision communication. Our approach ensures that all large matrix\nmultiplications during the forward and backward passes are executed in lower\nprecision. Applied to LLAMA-family models, HALO achieves\nnear-full-precision-equivalent results during fine-tuning on various tasks,\nwhile delivering up to 1.41x end-to-end speedup for full fine-tuning on RTX\n4090 GPUs. HALO efficiently supports both standard and parameterefficient\nfine-tuning (PEFT). Our results demonstrate the first practical approach to\nfully quantized LLM fine-tuning that maintains accuracy in 8-bit precision,\nwhile delivering performance benefits. Code is available at\nhttps://github.com/IST-DASLab/HALO."
                },
                "authors": [
                    {
                        "name": "Saleh Ashkboos"
                    },
                    {
                        "name": "Mahdi Nikdan"
                    },
                    {
                        "name": "Soroush Tabesh"
                    },
                    {
                        "name": "Roberto L. Castro"
                    },
                    {
                        "name": "Torsten Hoefler"
                    },
                    {
                        "name": "Dan Alistarh"
                    }
                ],
                "author_detail": {
                    "name": "Dan Alistarh"
                },
                "author": "Dan Alistarh",
                "arxiv_comment": "19 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02625v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02625v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03534v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03534v1",
                "updated": "2025-11-05T15:06:39Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    15,
                    6,
                    39,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T15:06:39Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    15,
                    6,
                    39,
                    2,
                    309,
                    0
                ],
                "title": "PnPSelect: Plug-and-play IoT Device Selection Using Ultra-wideband\n  Signals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PnPSelect: Plug-and-play IoT Device Selection Using Ultra-wideband\n  Signals"
                },
                "summary": "In recent years, the number of Internet of Things (IoT) devices in smart\nhomes has rapidly increased. A key challenge affecting user experience is how\nto enable users to efficiently and intuitively select the devices they wish to\ncontrol. This paper proposes PnPSelect, a plug-and-play IoT device selection\nsolution utilizing Ultra-wideband (UWB) technology on commercial devices.\nUnlike previous works, PnPSelect does not require the installation of dedicated\nhardware on each IoT device, thereby reducing deployment costs and\ncomplexities, and achieving true plug-and-play functionality. To enable\nintuitive device selection, we introduce a pointing direction estimation method\nthat utilizes UWB readings from a single anchor to infer the user pointing\ndirection. Additionally, we propose a lightweight device localization method\nthat allows users to register new IoT devices by simply pointing at them from\ntwo distinct positions, eliminating the need for manual measurements. We\nimplement PnPSelect on commercial smartphones and smartwatches and conduct\nextensive evaluations in both controlled laboratory settings and real-world\nenvironments. Our results demonstrate high accuracy, robustness, and\nadaptability, making PnPSelect a practical and scalable solution for\nnext-generation smart home interactions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the number of Internet of Things (IoT) devices in smart\nhomes has rapidly increased. A key challenge affecting user experience is how\nto enable users to efficiently and intuitively select the devices they wish to\ncontrol. This paper proposes PnPSelect, a plug-and-play IoT device selection\nsolution utilizing Ultra-wideband (UWB) technology on commercial devices.\nUnlike previous works, PnPSelect does not require the installation of dedicated\nhardware on each IoT device, thereby reducing deployment costs and\ncomplexities, and achieving true plug-and-play functionality. To enable\nintuitive device selection, we introduce a pointing direction estimation method\nthat utilizes UWB readings from a single anchor to infer the user pointing\ndirection. Additionally, we propose a lightweight device localization method\nthat allows users to register new IoT devices by simply pointing at them from\ntwo distinct positions, eliminating the need for manual measurements. We\nimplement PnPSelect on commercial smartphones and smartwatches and conduct\nextensive evaluations in both controlled laboratory settings and real-world\nenvironments. Our results demonstrate high accuracy, robustness, and\nadaptability, making PnPSelect a practical and scalable solution for\nnext-generation smart home interactions."
                },
                "authors": [
                    {
                        "name": "Zhaoxin Chang"
                    },
                    {
                        "name": "Fusang Zhang"
                    },
                    {
                        "name": "Jie Xiong"
                    },
                    {
                        "name": "Ziyu Li"
                    },
                    {
                        "name": "Badii Jouaber"
                    },
                    {
                        "name": "Daqing Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Daqing Zhang"
                },
                "author": "Daqing Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03534v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03534v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03524v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03524v1",
                "updated": "2025-11-05T14:57:03Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    14,
                    57,
                    3,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T14:57:03Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    14,
                    57,
                    3,
                    2,
                    309,
                    0
                ],
                "title": "Counterexamples to statements on isometric graph coverings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Counterexamples to statements on isometric graph coverings"
                },
                "summary": "A connected subgraph of a graph is isometric if it preserves distances. In\nthis short note, we provide counterexamples to several variants of the\nfollowing general question: When a graph $G$ is edge covered by connected\nisometric subgraphs $H_1,\\dots,H_k$, which properties of $G$ can we infer from\nproperties of $H_1,\\dots,H_k$? For example, Dumas, Foucaud, Perez and Todinca\n(SIDMA, 2024) proved that when $H_1,\\dots,H_k$ are paths, then the pathwidth of\n$G$ is bounded in terms of $k$. Among others, we show that there are graphs of\narbitrarily large treewidth that can be isometrically edge covered by four\ntrees.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A connected subgraph of a graph is isometric if it preserves distances. In\nthis short note, we provide counterexamples to several variants of the\nfollowing general question: When a graph $G$ is edge covered by connected\nisometric subgraphs $H_1,\\dots,H_k$, which properties of $G$ can we infer from\nproperties of $H_1,\\dots,H_k$? For example, Dumas, Foucaud, Perez and Todinca\n(SIDMA, 2024) proved that when $H_1,\\dots,H_k$ are paths, then the pathwidth of\n$G$ is bounded in terms of $k$. Among others, we show that there are graphs of\narbitrarily large treewidth that can be isometrically edge covered by four\ntrees."
                },
                "authors": [
                    {
                        "name": "Paul Bastide"
                    },
                    {
                        "name": "Julien Duron"
                    },
                    {
                        "name": "Jdrzej Hodor"
                    },
                    {
                        "name": "Weichan Liu"
                    },
                    {
                        "name": "Xiangxiang Nie"
                    }
                ],
                "author_detail": {
                    "name": "Xiangxiang Nie"
                },
                "author": "Xiangxiang Nie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03524v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03524v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09684v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09684v2",
                "updated": "2025-11-05T14:51:07Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    14,
                    51,
                    7,
                    2,
                    309,
                    0
                ],
                "published": "2025-06-11T13:02:17Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    13,
                    2,
                    17,
                    2,
                    162,
                    0
                ],
                "title": "Inv-Entropy: A Fully Probabilistic Framework for Uncertainty\n  Quantification in Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inv-Entropy: A Fully Probabilistic Framework for Uncertainty\n  Quantification in Language Models"
                },
                "summary": "Large language models (LLMs) have transformed natural language processing,\nbut their reliable deployment requires effective uncertainty quantification\n(UQ). Existing UQ methods are often heuristic and lack a probabilistic\ninterpretation. This paper begins by providing a theoretical justification for\nthe role of perturbations in UQ for LLMs. We then introduce a dual random walk\nperspective, modeling input-output pairs as two Markov chains with transition\nprobabilities defined by semantic similarity. Building on this, we propose a\nfully probabilistic framework based on an inverse model, which quantifies\nuncertainty by evaluating the diversity of the input space conditioned on a\ngiven output through systematic perturbations. Within this framework, we define\na new uncertainty measure, Inv-Entropy. A key strength of our framework is its\nflexibility: it supports various definitions of uncertainty measures,\nembeddings, perturbation strategies, and similarity metrics. We also propose\nGAAP, a perturbation algorithm based on genetic algorithms, which enhances the\ndiversity of sampled inputs. In addition, we introduce a new evaluation metric,\nTemperature Sensitivity of Uncertainty (TSU), which directly assesses\nuncertainty without relying on correctness as a proxy. Extensive experiments\ndemonstrate that Inv-Entropy outperforms existing semantic UQ methods. The code\nto reproduce the results can be found at\nhttps://github.com/UMDataScienceLab/Uncertainty-Quantification-for-LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have transformed natural language processing,\nbut their reliable deployment requires effective uncertainty quantification\n(UQ). Existing UQ methods are often heuristic and lack a probabilistic\ninterpretation. This paper begins by providing a theoretical justification for\nthe role of perturbations in UQ for LLMs. We then introduce a dual random walk\nperspective, modeling input-output pairs as two Markov chains with transition\nprobabilities defined by semantic similarity. Building on this, we propose a\nfully probabilistic framework based on an inverse model, which quantifies\nuncertainty by evaluating the diversity of the input space conditioned on a\ngiven output through systematic perturbations. Within this framework, we define\na new uncertainty measure, Inv-Entropy. A key strength of our framework is its\nflexibility: it supports various definitions of uncertainty measures,\nembeddings, perturbation strategies, and similarity metrics. We also propose\nGAAP, a perturbation algorithm based on genetic algorithms, which enhances the\ndiversity of sampled inputs. In addition, we introduce a new evaluation metric,\nTemperature Sensitivity of Uncertainty (TSU), which directly assesses\nuncertainty without relying on correctness as a proxy. Extensive experiments\ndemonstrate that Inv-Entropy outperforms existing semantic UQ methods. The code\nto reproduce the results can be found at\nhttps://github.com/UMDataScienceLab/Uncertainty-Quantification-for-LLMs."
                },
                "authors": [
                    {
                        "name": "Haoyi Song"
                    },
                    {
                        "name": "Ruihan Ji"
                    },
                    {
                        "name": "Naichen Shi"
                    },
                    {
                        "name": "Fan Lai"
                    },
                    {
                        "name": "Raed Al Kontar"
                    }
                ],
                "author_detail": {
                    "name": "Raed Al Kontar"
                },
                "author": "Raed Al Kontar",
                "arxiv_journal_ref": "NeurIPS, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09684v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09684v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03517v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03517v1",
                "updated": "2025-11-05T14:46:58Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    14,
                    46,
                    58,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T14:46:58Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    14,
                    46,
                    58,
                    2,
                    309,
                    0
                ],
                "title": "U2F: Encouraging SWE-Agent to Seize Novelty without Losing Feasibility",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "U2F: Encouraging SWE-Agent to Seize Novelty without Losing Feasibility"
                },
                "summary": "Large language models (LLMs) have shown strong capabilities in software\nengineering tasks, yet most existing LLM-based SWE-Agents mainly tackle\nwell-defined problems using conventional methods, often overlooking alternative\nor innovative solutions beyond their predefined frameworks. This limitation is\nevident in open-world software environments, where emerging challenges\ntranscend established paradigms.\n  We propose U2F (Unknown Unknowns to Functional solutions), a\ncognitive-inspired, uncertainty-embracing multi-agent framework that\nsystematically surfaces \"Unknown Unknowns\" - novel solution pathways absent\nfrom initial formulations but holding innovative potential. U2F consists of two\nkey components: (1) a Discovery-Exploration-Integration agent system for\nuncovering and synthesizing potential solutions, and (2) cognitive enhancement\nmechanisms across three dimensions: cross-domain analogical reasoning, reverse\nthinking, and external validation, which strategically reframe and extend\nconventional solution boundaries.\n  Applied to 218 real-world software enabler stories curated from authentic\nengineering tasks, U2F achieved notable improvements: human experts reported a\n14 percent increase in overall novelty, 51 percent improvement in semantic\nnovelty, and stable feasibility (4.02/5.0), corroborated by an LLM-based\nevaluator. These results highlight the potential of embracing uncertainty as a\ncatalyst for innovation in software engineering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown strong capabilities in software\nengineering tasks, yet most existing LLM-based SWE-Agents mainly tackle\nwell-defined problems using conventional methods, often overlooking alternative\nor innovative solutions beyond their predefined frameworks. This limitation is\nevident in open-world software environments, where emerging challenges\ntranscend established paradigms.\n  We propose U2F (Unknown Unknowns to Functional solutions), a\ncognitive-inspired, uncertainty-embracing multi-agent framework that\nsystematically surfaces \"Unknown Unknowns\" - novel solution pathways absent\nfrom initial formulations but holding innovative potential. U2F consists of two\nkey components: (1) a Discovery-Exploration-Integration agent system for\nuncovering and synthesizing potential solutions, and (2) cognitive enhancement\nmechanisms across three dimensions: cross-domain analogical reasoning, reverse\nthinking, and external validation, which strategically reframe and extend\nconventional solution boundaries.\n  Applied to 218 real-world software enabler stories curated from authentic\nengineering tasks, U2F achieved notable improvements: human experts reported a\n14 percent increase in overall novelty, 51 percent improvement in semantic\nnovelty, and stable feasibility (4.02/5.0), corroborated by an LLM-based\nevaluator. These results highlight the potential of embracing uncertainty as a\ncatalyst for innovation in software engineering."
                },
                "authors": [
                    {
                        "name": "Wencheng Ye"
                    },
                    {
                        "name": "Yan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yan Liu"
                },
                "author": "Yan Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03517v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03517v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03508v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03508v1",
                "updated": "2025-11-05T14:39:59Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    14,
                    39,
                    59,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T14:39:59Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    14,
                    39,
                    59,
                    2,
                    309,
                    0
                ],
                "title": "One Battle After Another: Probing LLMs' Limits on Multi-Turn Instruction\n  Following with a Benchmark Evolving Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One Battle After Another: Probing LLMs' Limits on Multi-Turn Instruction\n  Following with a Benchmark Evolving Framework"
                },
                "summary": "Understanding how well large language models can follow users' instructions\nthroughout a dialogue spanning multiple topics is of great importance for\ndata-intensive conversational applications. Existing benchmarks are often\nlimited to a fixed number of turns, making them susceptible to saturation and\nfailing to account for the user's interactive experience. In this work, we\npropose an extensible framework for assessing multi-turn instruction-following\nability. At its core, our framework decouples linguistic surface forms from\nuser intent simulation through a three-layer mechanism that tracks constraints,\ninstructions, and topics. This framework mimics User-LLM interaction by\nenabling the dynamic construction of benchmarks with state changes and\ntracebacks, terminating a conversation only when the model exhausts a simulated\nuser's patience. We define a suite of metrics capturing the quality of the\ninteraction process. Using this framework, we construct EvolIF, an evolving\ninstruction-following benchmark incorporating nine distinct constraint types.\nOur results indicate that GPT-5 exhibits superior instruction-following\nperformance. It sustains an average of 18.54 conversational turns and\ndemonstrates 70.31% robustness, outperforming Gemini-2.5-Pro by a significant\nmargin of 11.41%, while other models lag far behind. All of the data and code\nwill be made publicly available online.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding how well large language models can follow users' instructions\nthroughout a dialogue spanning multiple topics is of great importance for\ndata-intensive conversational applications. Existing benchmarks are often\nlimited to a fixed number of turns, making them susceptible to saturation and\nfailing to account for the user's interactive experience. In this work, we\npropose an extensible framework for assessing multi-turn instruction-following\nability. At its core, our framework decouples linguistic surface forms from\nuser intent simulation through a three-layer mechanism that tracks constraints,\ninstructions, and topics. This framework mimics User-LLM interaction by\nenabling the dynamic construction of benchmarks with state changes and\ntracebacks, terminating a conversation only when the model exhausts a simulated\nuser's patience. We define a suite of metrics capturing the quality of the\ninteraction process. Using this framework, we construct EvolIF, an evolving\ninstruction-following benchmark incorporating nine distinct constraint types.\nOur results indicate that GPT-5 exhibits superior instruction-following\nperformance. It sustains an average of 18.54 conversational turns and\ndemonstrates 70.31% robustness, outperforming Gemini-2.5-Pro by a significant\nmargin of 11.41%, while other models lag far behind. All of the data and code\nwill be made publicly available online."
                },
                "authors": [
                    {
                        "name": "Qi Jia"
                    },
                    {
                        "name": "Kaiwei Zhang"
                    },
                    {
                        "name": "Xiujie Song"
                    },
                    {
                        "name": "Ye Shen"
                    },
                    {
                        "name": "Xiangyang Zhu"
                    },
                    {
                        "name": "Guangtao Zhai"
                    }
                ],
                "author_detail": {
                    "name": "Guangtao Zhai"
                },
                "author": "Guangtao Zhai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03508v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03508v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03506v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03506v1",
                "updated": "2025-11-05T14:37:34Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    14,
                    37,
                    34,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T14:37:34Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    14,
                    37,
                    34,
                    2,
                    309,
                    0
                ],
                "title": "HaluMem: Evaluating Hallucinations in Memory Systems of Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HaluMem: Evaluating Hallucinations in Memory Systems of Agents"
                },
                "summary": "Memory systems are key components that enable AI systems such as LLMs and AI\nagents to achieve long-term learning and sustained interaction. However, during\nmemory storage and retrieval, these systems frequently exhibit memory\nhallucinations, including fabrication, errors, conflicts, and omissions.\nExisting evaluations of memory hallucinations are primarily end-to-end question\nanswering, which makes it difficult to localize the operational stage within\nthe memory system where hallucinations arise. To address this, we introduce the\nHallucination in Memory Benchmark (HaluMem), the first operation level\nhallucination evaluation benchmark tailored to memory systems. HaluMem defines\nthree evaluation tasks (memory extraction, memory updating, and memory question\nanswering) to comprehensively reveal hallucination behaviors across different\noperational stages of interaction. To support evaluation, we construct\nuser-centric, multi-turn human-AI interaction datasets, HaluMem-Medium and\nHaluMem-Long. Both include about 15k memory points and 3.5k multi-type\nquestions. The average dialogue length per user reaches 1.5k and 2.6k turns,\nwith context lengths exceeding 1M tokens, enabling evaluation of hallucinations\nacross different context scales and task complexities. Empirical studies based\non HaluMem show that existing memory systems tend to generate and accumulate\nhallucinations during the extraction and updating stages, which subsequently\npropagate errors to the question answering stage. Future research should focus\non developing interpretable and constrained memory operation mechanisms that\nsystematically suppress hallucinations and improve memory reliability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory systems are key components that enable AI systems such as LLMs and AI\nagents to achieve long-term learning and sustained interaction. However, during\nmemory storage and retrieval, these systems frequently exhibit memory\nhallucinations, including fabrication, errors, conflicts, and omissions.\nExisting evaluations of memory hallucinations are primarily end-to-end question\nanswering, which makes it difficult to localize the operational stage within\nthe memory system where hallucinations arise. To address this, we introduce the\nHallucination in Memory Benchmark (HaluMem), the first operation level\nhallucination evaluation benchmark tailored to memory systems. HaluMem defines\nthree evaluation tasks (memory extraction, memory updating, and memory question\nanswering) to comprehensively reveal hallucination behaviors across different\noperational stages of interaction. To support evaluation, we construct\nuser-centric, multi-turn human-AI interaction datasets, HaluMem-Medium and\nHaluMem-Long. Both include about 15k memory points and 3.5k multi-type\nquestions. The average dialogue length per user reaches 1.5k and 2.6k turns,\nwith context lengths exceeding 1M tokens, enabling evaluation of hallucinations\nacross different context scales and task complexities. Empirical studies based\non HaluMem show that existing memory systems tend to generate and accumulate\nhallucinations during the extraction and updating stages, which subsequently\npropagate errors to the question answering stage. Future research should focus\non developing interpretable and constrained memory operation mechanisms that\nsystematically suppress hallucinations and improve memory reliability."
                },
                "authors": [
                    {
                        "name": "Ding Chen"
                    },
                    {
                        "name": "Simin Niu"
                    },
                    {
                        "name": "Kehang Li"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Xiangping Zheng"
                    },
                    {
                        "name": "Bo Tang"
                    },
                    {
                        "name": "Xinchi Li"
                    },
                    {
                        "name": "Feiyu Xiong"
                    },
                    {
                        "name": "Zhiyu Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyu Li"
                },
                "author": "Zhiyu Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03506v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03506v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03502v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03502v1",
                "updated": "2025-11-05T14:33:04Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    14,
                    33,
                    4,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T14:33:04Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    14,
                    33,
                    4,
                    2,
                    309,
                    0
                ],
                "title": "Emergent tuning heterogeneity in cortical circuits is sensitive to\n  cellular neuronal dynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emergent tuning heterogeneity in cortical circuits is sensitive to\n  cellular neuronal dynamics"
                },
                "summary": "Cortical circuits exhibit high levels of response diversity, even across\napparently uniform neuronal populations. While emerging data-driven approaches\nexploit this heterogeneity to infer effective models of cortical circuit\ncomputation (e.g. Genkin et al. Nature 2025), the power of response diversity\nto enable inference of mechanistic circuit models is largely unexplored. Within\nthe landscape of cortical circuit models, spiking neuron networks in the\nbalanced state naturally exhibit high levels of response and tuning diversity\nemerging from their internal dynamics. A statistical theory for this emergent\ntuning heterogeneity, however, has only been formulated for binary spin models\n(Vreeswijk & Sompolinsky, 2005). Here we present a formulation of feature-tuned\nbalanced state networks that allows for arbitrary and diverse dynamics of\npostsynaptic currents and variable levels of heterogeneity in cellular\nexcitability but nevertheless is analytically exactly tractable with respect to\nthe emergent tuning curve heterogeneity. Using this framework, we present a\ncase study demonstrating that, for a wide range of parameters even the\npopulation mean response is non-universal and sensitive to mechanistic circuit\ndetails. As our theory enables exactly and analytically obtaining the\nlikelihood-function of tuning heterogeneity given circuit parameters, we argue\nthat it forms a powerful and rigorous basis for neural circuit inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cortical circuits exhibit high levels of response diversity, even across\napparently uniform neuronal populations. While emerging data-driven approaches\nexploit this heterogeneity to infer effective models of cortical circuit\ncomputation (e.g. Genkin et al. Nature 2025), the power of response diversity\nto enable inference of mechanistic circuit models is largely unexplored. Within\nthe landscape of cortical circuit models, spiking neuron networks in the\nbalanced state naturally exhibit high levels of response and tuning diversity\nemerging from their internal dynamics. A statistical theory for this emergent\ntuning heterogeneity, however, has only been formulated for binary spin models\n(Vreeswijk & Sompolinsky, 2005). Here we present a formulation of feature-tuned\nbalanced state networks that allows for arbitrary and diverse dynamics of\npostsynaptic currents and variable levels of heterogeneity in cellular\nexcitability but nevertheless is analytically exactly tractable with respect to\nthe emergent tuning curve heterogeneity. Using this framework, we present a\ncase study demonstrating that, for a wide range of parameters even the\npopulation mean response is non-universal and sensitive to mechanistic circuit\ndetails. As our theory enables exactly and analytically obtaining the\nlikelihood-function of tuning heterogeneity given circuit parameters, we argue\nthat it forms a powerful and rigorous basis for neural circuit inference."
                },
                "authors": [
                    {
                        "name": "Mohammadreza Soltanipour"
                    },
                    {
                        "name": "Stefan Treue"
                    },
                    {
                        "name": "Fred Wolf"
                    }
                ],
                "author_detail": {
                    "name": "Fred Wolf"
                },
                "author": "Fred Wolf",
                "arxiv_comment": "25 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03502v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03502v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.NC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.bio-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02558v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02558v2",
                "updated": "2025-11-05T14:29:12Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    14,
                    29,
                    12,
                    2,
                    309,
                    0
                ],
                "published": "2025-08-04T16:14:03Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    16,
                    14,
                    3,
                    0,
                    216,
                    0
                ],
                "title": "Sparse-dLLM: Accelerating Diffusion LLMs with Dynamic Cache Eviction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse-dLLM: Accelerating Diffusion LLMs with Dynamic Cache Eviction"
                },
                "summary": "Diffusion Large Language Models (dLLMs) enable breakthroughs in reasoning and\nparallel decoding but suffer from prohibitive quadratic computational\ncomplexity and memory overhead during inference. Current caching techniques\naccelerate decoding by storing full-layer states, yet impose substantial memory\nusage that limit long-context applications. Our analysis of attention patterns\nin dLLMs reveals persistent cross-layer sparsity, with pivotal tokens remaining\nsalient across decoding steps and low-relevance tokens staying unimportant,\nmotivating selective cache eviction. We propose Sparse-dLLM, the first\ntraining-free framework integrating dynamic cache eviction with sparse\nattention via delayed bidirectional sparse caching. By leveraging the stability\nof token saliency over steps, it retains critical tokens and dynamically evicts\nunimportant prefix/suffix entries using an attention-guided strategy. Extensive\nexperiments on LLaDA and Dream series demonstrate Sparse-dLLM achieves up to\n10$\\times$ higher throughput than vanilla dLLMs, with comparable performance\nand similar peak memory costs, outperforming previous methods in efficiency and\neffectiveness. The code is available at\nhttps://github.com/OpenMOSS/Sparse-dLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Large Language Models (dLLMs) enable breakthroughs in reasoning and\nparallel decoding but suffer from prohibitive quadratic computational\ncomplexity and memory overhead during inference. Current caching techniques\naccelerate decoding by storing full-layer states, yet impose substantial memory\nusage that limit long-context applications. Our analysis of attention patterns\nin dLLMs reveals persistent cross-layer sparsity, with pivotal tokens remaining\nsalient across decoding steps and low-relevance tokens staying unimportant,\nmotivating selective cache eviction. We propose Sparse-dLLM, the first\ntraining-free framework integrating dynamic cache eviction with sparse\nattention via delayed bidirectional sparse caching. By leveraging the stability\nof token saliency over steps, it retains critical tokens and dynamically evicts\nunimportant prefix/suffix entries using an attention-guided strategy. Extensive\nexperiments on LLaDA and Dream series demonstrate Sparse-dLLM achieves up to\n10$\\times$ higher throughput than vanilla dLLMs, with comparable performance\nand similar peak memory costs, outperforming previous methods in efficiency and\neffectiveness. The code is available at\nhttps://github.com/OpenMOSS/Sparse-dLLM."
                },
                "authors": [
                    {
                        "name": "Yuerong Song"
                    },
                    {
                        "name": "Xiaoran Liu"
                    },
                    {
                        "name": "Ruixiao Li"
                    },
                    {
                        "name": "Zhigeng Liu"
                    },
                    {
                        "name": "Zengfeng Huang"
                    },
                    {
                        "name": "Qipeng Guo"
                    },
                    {
                        "name": "Ziwei He"
                    },
                    {
                        "name": "Xipeng Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Xipeng Qiu"
                },
                "author": "Xipeng Qiu",
                "arxiv_comment": "12 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02558v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02558v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03497v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03497v1",
                "updated": "2025-11-05T14:27:58Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    14,
                    27,
                    58,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T14:27:58Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    14,
                    27,
                    58,
                    2,
                    309,
                    0
                ],
                "title": "ROSBag MCP Server: Analyzing Robot Data with LLMs for Agentic Embodied\n  AI Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ROSBag MCP Server: Analyzing Robot Data with LLMs for Agentic Embodied\n  AI Applications"
                },
                "summary": "Agentic AI systems and Physical or Embodied AI systems have been two key\nresearch verticals at the forefront of Artificial Intelligence and Robotics,\nwith Model Context Protocol (MCP) increasingly becoming a key component and\nenabler of agentic applications. However, the literature at the intersection of\nthese verticals, i.e., Agentic Embodied AI, remains scarce. This paper\nintroduces an MCP server for analyzing ROS and ROS 2 bags, allowing for\nanalyzing, visualizing and processing robot data with natural language through\nLLMs and VLMs. We describe specific tooling built with robotics domain\nknowledge, with our initial release focused on mobile robotics and supporting\nnatively the analysis of trajectories, laser scan data, transforms, or time\nseries data. This is in addition to providing an interface to standard ROS 2\nCLI tools (\"ros2 bag list\" or \"ros2 bag info\"), as well as the ability to\nfilter bags with a subset of topics or trimmed in time. Coupled with the MCP\nserver, we provide a lightweight UI that allows the benchmarking of the tooling\nwith different LLMs, both proprietary (Anthropic, OpenAI) and open-source\n(through Groq). Our experimental results include the analysis of tool calling\ncapabilities of eight different state-of-the-art LLM/VLM models, both\nproprietary and open-source, large and small. Our experiments indicate that\nthere is a large divide in tool calling capabilities, with Kimi K2 and Claude\nSonnet 4 demonstrating clearly superior performance. We also conclude that\nthere are multiple factors affecting the success rates, from the tool\ndescription schema to the number of arguments, as well as the number of tools\navailable to the models. The code is available with a permissive license at\nhttps://github.com/binabik-ai/mcp-rosbags.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic AI systems and Physical or Embodied AI systems have been two key\nresearch verticals at the forefront of Artificial Intelligence and Robotics,\nwith Model Context Protocol (MCP) increasingly becoming a key component and\nenabler of agentic applications. However, the literature at the intersection of\nthese verticals, i.e., Agentic Embodied AI, remains scarce. This paper\nintroduces an MCP server for analyzing ROS and ROS 2 bags, allowing for\nanalyzing, visualizing and processing robot data with natural language through\nLLMs and VLMs. We describe specific tooling built with robotics domain\nknowledge, with our initial release focused on mobile robotics and supporting\nnatively the analysis of trajectories, laser scan data, transforms, or time\nseries data. This is in addition to providing an interface to standard ROS 2\nCLI tools (\"ros2 bag list\" or \"ros2 bag info\"), as well as the ability to\nfilter bags with a subset of topics or trimmed in time. Coupled with the MCP\nserver, we provide a lightweight UI that allows the benchmarking of the tooling\nwith different LLMs, both proprietary (Anthropic, OpenAI) and open-source\n(through Groq). Our experimental results include the analysis of tool calling\ncapabilities of eight different state-of-the-art LLM/VLM models, both\nproprietary and open-source, large and small. Our experiments indicate that\nthere is a large divide in tool calling capabilities, with Kimi K2 and Claude\nSonnet 4 demonstrating clearly superior performance. We also conclude that\nthere are multiple factors affecting the success rates, from the tool\ndescription schema to the number of arguments, as well as the number of tools\navailable to the models. The code is available with a permissive license at\nhttps://github.com/binabik-ai/mcp-rosbags."
                },
                "authors": [
                    {
                        "name": "Lei Fu"
                    },
                    {
                        "name": "Sahar Salimpour"
                    },
                    {
                        "name": "Leonardo Militano"
                    },
                    {
                        "name": "Harry Edelman"
                    },
                    {
                        "name": "Jorge Pea Queralta"
                    },
                    {
                        "name": "Giovanni Toffetti"
                    }
                ],
                "author_detail": {
                    "name": "Giovanni Toffetti"
                },
                "author": "Giovanni Toffetti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03497v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03497v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.23095v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.23095v2",
                "updated": "2025-11-05T14:25:38Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    14,
                    25,
                    38,
                    2,
                    309,
                    0
                ],
                "published": "2025-10-27T08:00:46Z",
                "published_parsed": [
                    2025,
                    10,
                    27,
                    8,
                    0,
                    46,
                    0,
                    300,
                    0
                ],
                "title": "Revisiting Multimodal Positional Encoding in Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revisiting Multimodal Positional Encoding in Vision-Language Models"
                },
                "summary": "Multimodal position encoding is essential for vision-language models, yet\nthere has been little systematic investigation into multimodal position\nencoding. We conduct a comprehensive analysis of multimodal Rotary Positional\nEmbedding (RoPE) by examining its two core components: position design and\nfrequency allocation. Through extensive experiments, we identify three key\nguidelines: positional coherence, full frequency utilization, and preservation\nof textual priors-ensuring unambiguous layout, rich representation, and\nfaithful transfer from the pre-trained LLM. Based on these insights, we propose\nMulti-Head RoPE (MHRoPE) and MRoPE-Interleave (MRoPE-I), two simple and\nplug-and-play variants that require no architectural changes. Our methods\nconsistently outperform existing approaches across diverse benchmarks, with\nsignificant improvements in both general and fine-grained multimodal\nunderstanding. Code will be avaliable at\nhttps://github.com/JJJYmmm/Multimodal-RoPEs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal position encoding is essential for vision-language models, yet\nthere has been little systematic investigation into multimodal position\nencoding. We conduct a comprehensive analysis of multimodal Rotary Positional\nEmbedding (RoPE) by examining its two core components: position design and\nfrequency allocation. Through extensive experiments, we identify three key\nguidelines: positional coherence, full frequency utilization, and preservation\nof textual priors-ensuring unambiguous layout, rich representation, and\nfaithful transfer from the pre-trained LLM. Based on these insights, we propose\nMulti-Head RoPE (MHRoPE) and MRoPE-Interleave (MRoPE-I), two simple and\nplug-and-play variants that require no architectural changes. Our methods\nconsistently outperform existing approaches across diverse benchmarks, with\nsignificant improvements in both general and fine-grained multimodal\nunderstanding. Code will be avaliable at\nhttps://github.com/JJJYmmm/Multimodal-RoPEs."
                },
                "authors": [
                    {
                        "name": "Jie Huang"
                    },
                    {
                        "name": "Xuejing Liu"
                    },
                    {
                        "name": "Sibo Song"
                    },
                    {
                        "name": "Ruibing Hou"
                    },
                    {
                        "name": "Hong Chang"
                    },
                    {
                        "name": "Junyang Lin"
                    },
                    {
                        "name": "Shuai Bai"
                    }
                ],
                "author_detail": {
                    "name": "Shuai Bai"
                },
                "author": "Shuai Bai",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.23095v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.23095v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03492v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03492v1",
                "updated": "2025-11-05T14:21:18Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    14,
                    21,
                    18,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T14:21:18Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    14,
                    21,
                    18,
                    2,
                    309,
                    0
                ],
                "title": "Why Less is More (Sometimes): A Theory of Data Curation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Why Less is More (Sometimes): A Theory of Data Curation"
                },
                "summary": "This paper introduces a theoretical framework to resolve a central paradox in\nmodern machine learning: When is it better to use less data? This question has\nbecome critical as classical scaling laws suggesting ``more is more'' (Sun et\nal., 2025) are challenged by methods like LIMO (``less is more'') and s1 (Ye et\nal., 2025; Muenighoff et al., 2025), which achieve superior performance with\nsmall, aggressively curated datasets. Here, we study data curation strategies\nwhere an imperfect oracle selects the training examples according to their\ndifficulty and correctness. Our results provide exact scaling law curves for\ntest error under both label-agnostic and label-aware curation rules, revealing\nwhen and why keeping only a subset of data can improve generalization. In\ncontrast to classical scaling laws, we show that under certain conditions,\nsmall curated datasets can outperform full datasets, and we provide analytical\nconditions for this by deriving precise phase transition curves tied to data\nsize and quality. We validate these theoretical claims with empirical results\non ImageNet, confirming our predictions about when curation improves accuracy\nand can even mitigate model collapse. Furthermore, our framework provides a\nprincipled explanation for the contradictory curation strategies recently\nobserved in LLM mathematical reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a theoretical framework to resolve a central paradox in\nmodern machine learning: When is it better to use less data? This question has\nbecome critical as classical scaling laws suggesting ``more is more'' (Sun et\nal., 2025) are challenged by methods like LIMO (``less is more'') and s1 (Ye et\nal., 2025; Muenighoff et al., 2025), which achieve superior performance with\nsmall, aggressively curated datasets. Here, we study data curation strategies\nwhere an imperfect oracle selects the training examples according to their\ndifficulty and correctness. Our results provide exact scaling law curves for\ntest error under both label-agnostic and label-aware curation rules, revealing\nwhen and why keeping only a subset of data can improve generalization. In\ncontrast to classical scaling laws, we show that under certain conditions,\nsmall curated datasets can outperform full datasets, and we provide analytical\nconditions for this by deriving precise phase transition curves tied to data\nsize and quality. We validate these theoretical claims with empirical results\non ImageNet, confirming our predictions about when curation improves accuracy\nand can even mitigate model collapse. Furthermore, our framework provides a\nprincipled explanation for the contradictory curation strategies recently\nobserved in LLM mathematical reasoning."
                },
                "authors": [
                    {
                        "name": "Elvis Dohmatob"
                    },
                    {
                        "name": "Mohammad Pezeshki"
                    },
                    {
                        "name": "Reyhane Askari-Hemmat"
                    }
                ],
                "author_detail": {
                    "name": "Reyhane Askari-Hemmat"
                },
                "author": "Reyhane Askari-Hemmat",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03492v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03492v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08436v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08436v2",
                "updated": "2025-11-05T14:16:25Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    14,
                    16,
                    25,
                    2,
                    309,
                    0
                ],
                "published": "2025-02-12T14:20:36Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    14,
                    20,
                    36,
                    2,
                    43,
                    0
                ],
                "title": "From Haystack to Needle: Label Space Reduction for Zero-shot\n  Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Haystack to Needle: Label Space Reduction for Zero-shot\n  Classification"
                },
                "summary": "We present Label Space Reduction (LSR), a novel method for improving\nzero-shot classification performance of Large Language Models (LLMs). LSR\niteratively refines the classification label space by systematically ranking\nand reducing candidate classes, enabling the model to concentrate on the most\nrelevant options. By leveraging unlabeled data with the statistical learning\ncapabilities of data-driven models, LSR dynamically optimizes the label space\nrepresentation at test time. Our experiments across seven benchmarks\ndemonstrate that LSR improves macro-F1 scores by an average of 7.0% (up to\n14.2%) with Llama-3.1-70B and 3.3% (up to 11.1%) with Claude-3.5-Sonnet\ncompared to standard zero-shot classification baselines. To reduce the\ncomputational overhead of LSR, which requires an additional LLM call at each\niteration, we propose distilling the model into a probabilistic classifier,\nallowing for efficient inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Label Space Reduction (LSR), a novel method for improving\nzero-shot classification performance of Large Language Models (LLMs). LSR\niteratively refines the classification label space by systematically ranking\nand reducing candidate classes, enabling the model to concentrate on the most\nrelevant options. By leveraging unlabeled data with the statistical learning\ncapabilities of data-driven models, LSR dynamically optimizes the label space\nrepresentation at test time. Our experiments across seven benchmarks\ndemonstrate that LSR improves macro-F1 scores by an average of 7.0% (up to\n14.2%) with Llama-3.1-70B and 3.3% (up to 11.1%) with Claude-3.5-Sonnet\ncompared to standard zero-shot classification baselines. To reduce the\ncomputational overhead of LSR, which requires an additional LLM call at each\niteration, we propose distilling the model into a probabilistic classifier,\nallowing for efficient inference."
                },
                "authors": [
                    {
                        "name": "Nathan Vandemoortele"
                    },
                    {
                        "name": "Bram Steenwinckel"
                    },
                    {
                        "name": "Femke Ongenae"
                    },
                    {
                        "name": "Sofie Van Hoecke"
                    }
                ],
                "author_detail": {
                    "name": "Sofie Van Hoecke"
                },
                "author": "Sofie Van Hoecke",
                "arxiv_comment": "Add acknowledgment",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08436v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08436v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03477v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03477v1",
                "updated": "2025-11-05T14:02:11Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    14,
                    2,
                    11,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T14:02:11Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    14,
                    2,
                    11,
                    2,
                    309,
                    0
                ],
                "title": "Probing $J/$ Production Mechanisms in Proton-Proton Collisions at\n  SPD/NICA Energies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probing $J/$ Production Mechanisms in Proton-Proton Collisions at\n  SPD/NICA Energies"
                },
                "summary": "We investigate inclusive $J/\\psi$ production in proton-proton collisions at\ntens of GeV $\\sqrt{s}$ energy, relevant for forthcoming measurements with the\nSpin Physics Detector (SPD) at NICA. Simulations are performed using the\nPEGASUS event generator with transverse-momentum-dependent (TMD) gluon\ndensities, comparing the recent KMR-based KL$'2025$ and CCFM-based LLM$'2024$\nparametrizations. Differential cross sections in rapidity and transverse\nmomentum exhibit smooth, stable behavior under renormalization-scale variation,\nwhile factorization-scale dependence exposes limitations of the LLM$'2024$ set\nat low scales in contrast to KL$'2025$. Normalized $p_T$ spectra reveal\ndistinct hardening patterns linked to the underlying gluon $k_T$ broadening in\neach model. The relative contributions of color-singlet and color-octet\nchannels are also quantified, demonstrating the dominance of color-octet\nmechanisms in the SPD energy regime. These results provide the first detailed\nassessment of quarkonium production sensitivity to gluon TMDs near threshold,\noffering timely theoretical guidance for upcoming $J/\\psi$ measurements at\nSPD/NICA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate inclusive $J/\\psi$ production in proton-proton collisions at\ntens of GeV $\\sqrt{s}$ energy, relevant for forthcoming measurements with the\nSpin Physics Detector (SPD) at NICA. Simulations are performed using the\nPEGASUS event generator with transverse-momentum-dependent (TMD) gluon\ndensities, comparing the recent KMR-based KL$'2025$ and CCFM-based LLM$'2024$\nparametrizations. Differential cross sections in rapidity and transverse\nmomentum exhibit smooth, stable behavior under renormalization-scale variation,\nwhile factorization-scale dependence exposes limitations of the LLM$'2024$ set\nat low scales in contrast to KL$'2025$. Normalized $p_T$ spectra reveal\ndistinct hardening patterns linked to the underlying gluon $k_T$ broadening in\neach model. The relative contributions of color-singlet and color-octet\nchannels are also quantified, demonstrating the dominance of color-octet\nmechanisms in the SPD energy regime. These results provide the first detailed\nassessment of quarkonium production sensitivity to gluon TMDs near threshold,\noffering timely theoretical guidance for upcoming $J/\\psi$ measurements at\nSPD/NICA."
                },
                "authors": [
                    {
                        "name": "Shubham Sharma"
                    },
                    {
                        "name": "Alexey Aparin"
                    }
                ],
                "author_detail": {
                    "name": "Alexey Aparin"
                },
                "author": "Alexey Aparin",
                "arxiv_comment": "5 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03477v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03477v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.12398v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.12398v2",
                "updated": "2025-11-05T13:59:39Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    13,
                    59,
                    39,
                    2,
                    309,
                    0
                ],
                "published": "2025-05-18T12:51:55Z",
                "published_parsed": [
                    2025,
                    5,
                    18,
                    12,
                    51,
                    55,
                    6,
                    138,
                    0
                ],
                "title": "Traversal Verification for Speculative Tree Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traversal Verification for Speculative Tree Decoding"
                },
                "summary": "Speculative decoding is a promising approach for accelerating large language\nmodels. The primary idea is to use a lightweight draft model to speculate the\noutput of the target model for multiple subsequent timesteps, and then verify\nthem in parallel to determine whether the drafted tokens should be accepted or\nrejected. To enhance acceptance rates, existing frameworks typically construct\ntoken trees containing multiple candidates in each timestep. However, their\nreliance on token-level verification mechanisms introduces two critical\nlimitations: First, the probability distribution of a sequence differs from\nthat of individual tokens, leading to suboptimal acceptance length. Second,\ncurrent verification schemes begin from the root node and proceed layer by\nlayer in a top-down manner. Once a parent node is rejected, all its child nodes\nshould be discarded, resulting in inefficient utilization of speculative\ncandidates. This paper introduces Traversal Verification, a novel speculative\ndecoding algorithm that fundamentally rethinks the verification paradigm\nthrough leaf-to-root traversal. Our approach considers the acceptance of the\nentire token sequence from the current node to the root, and preserves\npotentially valid subsequences that would be prematurely discarded by existing\nmethods. We theoretically prove that the probability distribution obtained\nthrough Traversal Verification is identical to that of the target model,\nguaranteeing lossless inference while achieving substantial acceleration gains.\nExperimental results across different large language models and multiple tasks\nshow that our method consistently improves acceptance length and throughput\nover existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding is a promising approach for accelerating large language\nmodels. The primary idea is to use a lightweight draft model to speculate the\noutput of the target model for multiple subsequent timesteps, and then verify\nthem in parallel to determine whether the drafted tokens should be accepted or\nrejected. To enhance acceptance rates, existing frameworks typically construct\ntoken trees containing multiple candidates in each timestep. However, their\nreliance on token-level verification mechanisms introduces two critical\nlimitations: First, the probability distribution of a sequence differs from\nthat of individual tokens, leading to suboptimal acceptance length. Second,\ncurrent verification schemes begin from the root node and proceed layer by\nlayer in a top-down manner. Once a parent node is rejected, all its child nodes\nshould be discarded, resulting in inefficient utilization of speculative\ncandidates. This paper introduces Traversal Verification, a novel speculative\ndecoding algorithm that fundamentally rethinks the verification paradigm\nthrough leaf-to-root traversal. Our approach considers the acceptance of the\nentire token sequence from the current node to the root, and preserves\npotentially valid subsequences that would be prematurely discarded by existing\nmethods. We theoretically prove that the probability distribution obtained\nthrough Traversal Verification is identical to that of the target model,\nguaranteeing lossless inference while achieving substantial acceleration gains.\nExperimental results across different large language models and multiple tasks\nshow that our method consistently improves acceptance length and throughput\nover existing methods."
                },
                "authors": [
                    {
                        "name": "Yepeng Weng"
                    },
                    {
                        "name": "Qiao Hu"
                    },
                    {
                        "name": "Xujie Chen"
                    },
                    {
                        "name": "Li Liu"
                    },
                    {
                        "name": "Dianwen Mei"
                    },
                    {
                        "name": "Huishi Qiu"
                    },
                    {
                        "name": "Jiang Tian"
                    },
                    {
                        "name": "Zhongchao Shi"
                    }
                ],
                "author_detail": {
                    "name": "Zhongchao Shi"
                },
                "author": "Zhongchao Shi",
                "arxiv_comment": "NeurIPS 2025 poster",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.12398v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.12398v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03475v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03475v1",
                "updated": "2025-11-05T13:59:01Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    13,
                    59,
                    1,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T13:59:01Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    13,
                    59,
                    1,
                    2,
                    309,
                    0
                ],
                "title": "RAGBoost: Efficient Retrieval-Augmented Generation with\n  Accuracy-Preserving Context Reuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAGBoost: Efficient Retrieval-Augmented Generation with\n  Accuracy-Preserving Context Reuse"
                },
                "summary": "Retrieval-augmented generation (RAG) enhances large language models (LLMs)\nwith retrieved context but often suffers from downgraded prefill performance as\nmodern applications demand longer and more complex inputs. Existing caching\ntechniques either preserve accuracy with low cache reuse or improve reuse at\nthe cost of degraded reasoning quality. We present RAGBoost, an efficient RAG\nsystem that achieves high cache reuse without sacrificing accuracy through\naccuracy-preserving context reuse. RAGBoost detects overlapping retrieved items\nacross concurrent sessions and multi-turn interactions, using efficient context\nindexing, ordering, and de-duplication to maximize reuse, while lightweight\ncontextual hints maintain reasoning fidelity. It integrates seamlessly with\nexisting LLM inference engines and improves their prefill performance by 1.5-3X\nover state-of-the-art methods, while preserving or even enhancing reasoning\naccuracy across diverse RAG and agentic AI workloads. Our code is released at:\nhttps://github.com/Edinburgh-AgenticAI/RAGBoost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) enhances large language models (LLMs)\nwith retrieved context but often suffers from downgraded prefill performance as\nmodern applications demand longer and more complex inputs. Existing caching\ntechniques either preserve accuracy with low cache reuse or improve reuse at\nthe cost of degraded reasoning quality. We present RAGBoost, an efficient RAG\nsystem that achieves high cache reuse without sacrificing accuracy through\naccuracy-preserving context reuse. RAGBoost detects overlapping retrieved items\nacross concurrent sessions and multi-turn interactions, using efficient context\nindexing, ordering, and de-duplication to maximize reuse, while lightweight\ncontextual hints maintain reasoning fidelity. It integrates seamlessly with\nexisting LLM inference engines and improves their prefill performance by 1.5-3X\nover state-of-the-art methods, while preserving or even enhancing reasoning\naccuracy across diverse RAG and agentic AI workloads. Our code is released at:\nhttps://github.com/Edinburgh-AgenticAI/RAGBoost."
                },
                "authors": [
                    {
                        "name": "Yinsicheng Jiang"
                    },
                    {
                        "name": "Yeqi Huang"
                    },
                    {
                        "name": "Liang Cheng"
                    },
                    {
                        "name": "Cheng Deng"
                    },
                    {
                        "name": "Xuan Sun"
                    },
                    {
                        "name": "Luo Mai"
                    }
                ],
                "author_detail": {
                    "name": "Luo Mai"
                },
                "author": "Luo Mai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03475v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03475v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08179v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08179v2",
                "updated": "2025-11-05T13:53:51Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    13,
                    53,
                    51,
                    2,
                    309,
                    0
                ],
                "published": "2024-12-11T08:09:42Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    8,
                    9,
                    42,
                    2,
                    346,
                    0
                ],
                "title": "RAG-IT: Retrieval-Augmented Instruction Tuning for Automated Financial\n  Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAG-IT: Retrieval-Augmented Instruction Tuning for Automated Financial\n  Analysis"
                },
                "summary": "Financial analysis relies heavily on the interpretation of earnings reports\nto assess company performance and guide decision-making. Traditional methods\nfor generating such analyses demand significant financial expertise and are\noften time-consuming. With the rapid advancement of Large Language Models\n(LLMs), domain-specific adaptations have emerged for financial tasks such as\nsentiment analysis and entity recognition. This paper introduces RAG-IT\n(Retrieval-Augmented Instruction Tuning), a novel framework designed to\nautomate the generation of earnings report analyses through an LLM fine-tuned\nspecifically for the financial domain. Our approach integrates retrieval\naugmentation with instruction-based fine-tuning to enhance factual accuracy,\ncontextual relevance, and domain adaptability. We construct a comprehensive\nfinancial instruction dataset derived from extensive financial documents and\nearnings reports to guide the LLM's adaptation to specialized financial\nreasoning. Experimental results demonstrate that RAG-IT outperforms\ngeneral-purpose open-source models and achieves performance comparable to\ncommercial systems like GPT-3.5 on financial report generation tasks. This\nresearch highlights the potential of retrieval-augmented instruction tuning to\nstreamline and elevate financial analysis automation, advancing the broader\nfield of intelligent financial reporting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Financial analysis relies heavily on the interpretation of earnings reports\nto assess company performance and guide decision-making. Traditional methods\nfor generating such analyses demand significant financial expertise and are\noften time-consuming. With the rapid advancement of Large Language Models\n(LLMs), domain-specific adaptations have emerged for financial tasks such as\nsentiment analysis and entity recognition. This paper introduces RAG-IT\n(Retrieval-Augmented Instruction Tuning), a novel framework designed to\nautomate the generation of earnings report analyses through an LLM fine-tuned\nspecifically for the financial domain. Our approach integrates retrieval\naugmentation with instruction-based fine-tuning to enhance factual accuracy,\ncontextual relevance, and domain adaptability. We construct a comprehensive\nfinancial instruction dataset derived from extensive financial documents and\nearnings reports to guide the LLM's adaptation to specialized financial\nreasoning. Experimental results demonstrate that RAG-IT outperforms\ngeneral-purpose open-source models and achieves performance comparable to\ncommercial systems like GPT-3.5 on financial report generation tasks. This\nresearch highlights the potential of retrieval-augmented instruction tuning to\nstreamline and elevate financial analysis automation, advancing the broader\nfield of intelligent financial reporting."
                },
                "authors": [
                    {
                        "name": "Van-Duc Le"
                    },
                    {
                        "name": "Hai-Thien To"
                    }
                ],
                "author_detail": {
                    "name": "Hai-Thien To"
                },
                "author": "Hai-Thien To",
                "arxiv_comment": "11 pages, 1 figure, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08179v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08179v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03470v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03470v1",
                "updated": "2025-11-05T13:47:17Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    13,
                    47,
                    17,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T13:47:17Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    13,
                    47,
                    17,
                    2,
                    309,
                    0
                ],
                "title": "First Associated Neutrino Search for a Failed Supernova Candidate with\n  Super-Kamiokande",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "First Associated Neutrino Search for a Failed Supernova Candidate with\n  Super-Kamiokande"
                },
                "summary": "In 2024, a failed supernova candidate, M31-2014-DS1, was reported in the\nAndromeda galaxy (M31), located at a distance of approximately 770 kpc. In this\npaper, we search for neutrinos from this failed supernova using data from\nSuper-Kamiokande (SK). Based on the estimated time of black hole formation\ninferred from optical and infrared observations, we define a search window for\nneutrino events in the SK data. Using this window, we develop a dedicated\nanalysis method for failed supernovae and apply it to M31-2014-DS1, by\nconducting a cluster search using the timing and energy information of\ncandidate events. No significant neutrino excess is observed within the search\nregion. Consequently, we place an upper limit on the electron antineutrino\nluminosity from M31-2014-DS1 and discuss its implications for various failed SN\nmodels and their neutrino emission characteristics. Despite the 18 MeV\nthreshold adopted to suppress backgrounds, the search remains sufficiently\nsensitive to constrain the Shen-TM1 EOS, yielding a 90% confidence level upper\nlimit of 1.76 \\times 10^{53} erg on the electron antineutrino luminosity,\nslightly above the expected value of 1.35 \\times 10^{53} erg.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In 2024, a failed supernova candidate, M31-2014-DS1, was reported in the\nAndromeda galaxy (M31), located at a distance of approximately 770 kpc. In this\npaper, we search for neutrinos from this failed supernova using data from\nSuper-Kamiokande (SK). Based on the estimated time of black hole formation\ninferred from optical and infrared observations, we define a search window for\nneutrino events in the SK data. Using this window, we develop a dedicated\nanalysis method for failed supernovae and apply it to M31-2014-DS1, by\nconducting a cluster search using the timing and energy information of\ncandidate events. No significant neutrino excess is observed within the search\nregion. Consequently, we place an upper limit on the electron antineutrino\nluminosity from M31-2014-DS1 and discuss its implications for various failed SN\nmodels and their neutrino emission characteristics. Despite the 18 MeV\nthreshold adopted to suppress backgrounds, the search remains sufficiently\nsensitive to constrain the Shen-TM1 EOS, yielding a 90% confidence level upper\nlimit of 1.76 \\times 10^{53} erg on the electron antineutrino luminosity,\nslightly above the expected value of 1.35 \\times 10^{53} erg."
                },
                "authors": [
                    {
                        "name": "F. Nakanishi"
                    },
                    {
                        "name": "K. Abe"
                    },
                    {
                        "name": "S. Abe"
                    },
                    {
                        "name": "C. Bronner"
                    },
                    {
                        "name": "M. Harada"
                    },
                    {
                        "name": "Y. Hayato"
                    },
                    {
                        "name": "K. Hiraide"
                    },
                    {
                        "name": "K. Hosokawa"
                    },
                    {
                        "name": "T. H. Hung"
                    },
                    {
                        "name": "K. Ieki"
                    },
                    {
                        "name": "M. Ikeda"
                    },
                    {
                        "name": "J. Kameda"
                    },
                    {
                        "name": "Y. Kanemura"
                    },
                    {
                        "name": "Y. Kataoka"
                    },
                    {
                        "name": "S. Miki"
                    },
                    {
                        "name": "S. Mine"
                    },
                    {
                        "name": "M. Miura"
                    },
                    {
                        "name": "S. Moriyama"
                    },
                    {
                        "name": "M. Nakahata"
                    },
                    {
                        "name": "S. Nakayama"
                    },
                    {
                        "name": "Y. Noguchi"
                    },
                    {
                        "name": "G. Pronost"
                    },
                    {
                        "name": "K. Sato"
                    },
                    {
                        "name": "H. Sekiya"
                    },
                    {
                        "name": "K. Shimizu"
                    },
                    {
                        "name": "M. Shiozawa"
                    },
                    {
                        "name": "Y. Suzuki"
                    },
                    {
                        "name": "A. Takeda"
                    },
                    {
                        "name": "Y. Takemoto"
                    },
                    {
                        "name": "H. Tanaka"
                    },
                    {
                        "name": "T. Yano"
                    },
                    {
                        "name": "Y. Itow"
                    },
                    {
                        "name": "T. Kajita"
                    },
                    {
                        "name": "K. Okumura"
                    },
                    {
                        "name": "T. Tashiro"
                    },
                    {
                        "name": "T. Tomiya"
                    },
                    {
                        "name": "X. Wang"
                    },
                    {
                        "name": "P. Fernandez"
                    },
                    {
                        "name": "L. Labarga"
                    },
                    {
                        "name": "B. Zaldivar"
                    },
                    {
                        "name": "B. W. Pointon"
                    },
                    {
                        "name": "C. Yanagisawa"
                    },
                    {
                        "name": "E. Kearns"
                    },
                    {
                        "name": "L. Wan"
                    },
                    {
                        "name": "T. Wester"
                    },
                    {
                        "name": "J. Bian"
                    },
                    {
                        "name": "B. Cortez"
                    },
                    {
                        "name": "N. J. Griskevich"
                    },
                    {
                        "name": "Y. Jiang"
                    },
                    {
                        "name": "M. B. Smy"
                    },
                    {
                        "name": "H. W. Sobel"
                    },
                    {
                        "name": "V. Takhistov"
                    },
                    {
                        "name": "A. Yankelevich"
                    },
                    {
                        "name": "J. Hill"
                    },
                    {
                        "name": "M. C. Jang"
                    },
                    {
                        "name": "S. H. Lee"
                    },
                    {
                        "name": "D. H. Moon"
                    },
                    {
                        "name": "R. G. Park"
                    },
                    {
                        "name": "B. S. Yang"
                    },
                    {
                        "name": "B. Bodur"
                    },
                    {
                        "name": "K. Scholberg"
                    },
                    {
                        "name": "C. W. Walter"
                    },
                    {
                        "name": "A. Beauchne"
                    },
                    {
                        "name": "Le Blvec"
                    },
                    {
                        "name": "O. Drapier"
                    },
                    {
                        "name": "A. Ershova"
                    },
                    {
                        "name": "M. Ferey"
                    },
                    {
                        "name": "Th. A. Mueller"
                    },
                    {
                        "name": "A. D. Santos"
                    },
                    {
                        "name": "P. Paganini"
                    },
                    {
                        "name": "C. Quach"
                    },
                    {
                        "name": "R. Rogly"
                    },
                    {
                        "name": "T. Nakamura"
                    },
                    {
                        "name": "J. S. Jang"
                    },
                    {
                        "name": "R. P. Litchfield"
                    },
                    {
                        "name": "L. N. Machado"
                    },
                    {
                        "name": "F. J. . P Soler"
                    },
                    {
                        "name": "J. G. Learned"
                    },
                    {
                        "name": "K. Choi"
                    },
                    {
                        "name": "S. Cao"
                    },
                    {
                        "name": "L. H. V. Anthony"
                    },
                    {
                        "name": "N. W. Prouse"
                    },
                    {
                        "name": "M. Scott"
                    },
                    {
                        "name": "Y. Uchida"
                    },
                    {
                        "name": "V. Berardi"
                    },
                    {
                        "name": "N. F. Calabria"
                    },
                    {
                        "name": "M. G. Catanesi"
                    },
                    {
                        "name": "N. Ospina"
                    },
                    {
                        "name": "E. Radicioni"
                    },
                    {
                        "name": "A. Langella"
                    },
                    {
                        "name": "G. De Rosa"
                    },
                    {
                        "name": "G. Collazuol"
                    },
                    {
                        "name": "M. Feltre"
                    },
                    {
                        "name": "M. Mattiazzi"
                    },
                    {
                        "name": "L. Ludovici"
                    },
                    {
                        "name": "M. Gonin"
                    },
                    {
                        "name": "L. L. Priss"
                    },
                    {
                        "name": "B. Quilain"
                    },
                    {
                        "name": "S. Horiuchi"
                    },
                    {
                        "name": "A. Kawabata"
                    },
                    {
                        "name": "M. Kobayashi"
                    },
                    {
                        "name": "Y. M. Liu"
                    },
                    {
                        "name": "Y. Maekawa"
                    },
                    {
                        "name": "Y. Nishimura"
                    },
                    {
                        "name": "R. Akutsu"
                    },
                    {
                        "name": "M. Friend"
                    },
                    {
                        "name": "T. Hasegawa"
                    },
                    {
                        "name": "Y. Hino"
                    },
                    {
                        "name": "T. Ishida"
                    },
                    {
                        "name": "T. Kobayashi"
                    },
                    {
                        "name": "M. Jakkapu"
                    },
                    {
                        "name": "T. Matsubara"
                    },
                    {
                        "name": "T. Nakadaira"
                    },
                    {
                        "name": "Y. Oyama"
                    },
                    {
                        "name": "A. Portocarrero Yrey"
                    },
                    {
                        "name": "K. Sakashita"
                    },
                    {
                        "name": "T. Sekiguchi"
                    },
                    {
                        "name": "T. Tsukamoto"
                    },
                    {
                        "name": "N. Bhuiyan"
                    },
                    {
                        "name": "G. T. Burton"
                    },
                    {
                        "name": "F. Di Lodovico"
                    },
                    {
                        "name": "J. Gao"
                    },
                    {
                        "name": "T. Katori"
                    },
                    {
                        "name": "R. Kralik"
                    },
                    {
                        "name": "N. Latham"
                    },
                    {
                        "name": "R. M. Ramsden"
                    },
                    {
                        "name": "H. Ito"
                    },
                    {
                        "name": "T. Sone"
                    },
                    {
                        "name": "A. T. Suzuki"
                    },
                    {
                        "name": "Y. Takeuchi"
                    },
                    {
                        "name": "S. Wada"
                    },
                    {
                        "name": "H. Zhong"
                    },
                    {
                        "name": "J. Feng"
                    },
                    {
                        "name": "L. Feng"
                    },
                    {
                        "name": "S. Han"
                    },
                    {
                        "name": "J. Hikida"
                    },
                    {
                        "name": "J. R. Hu"
                    },
                    {
                        "name": "Z. Hu"
                    },
                    {
                        "name": "M. Kawaue"
                    },
                    {
                        "name": "T. Kikawa"
                    },
                    {
                        "name": "T. V. Ngoc"
                    },
                    {
                        "name": "T. Nakaya"
                    },
                    {
                        "name": "R. A. Wendell"
                    },
                    {
                        "name": "S. J. Jenkins"
                    },
                    {
                        "name": "N. McCauley"
                    },
                    {
                        "name": "A. Tarrant"
                    },
                    {
                        "name": "M. Fan`"
                    },
                    {
                        "name": "M. J. Wilking"
                    },
                    {
                        "name": "Z. Xie"
                    },
                    {
                        "name": "Y. Fukuda"
                    },
                    {
                        "name": "H. Menjo"
                    },
                    {
                        "name": "Y. Yoshioka"
                    },
                    {
                        "name": "J. Lagoda"
                    },
                    {
                        "name": "M. Mandal"
                    },
                    {
                        "name": "Y. S. Prabhu"
                    },
                    {
                        "name": "J. Zalipska"
                    },
                    {
                        "name": "M. Mori"
                    },
                    {
                        "name": "J. Jiang"
                    },
                    {
                        "name": "K. Hamaguchi"
                    },
                    {
                        "name": "H. Ishino"
                    },
                    {
                        "name": "Y. Koshio"
                    },
                    {
                        "name": "T. Tada"
                    },
                    {
                        "name": "T. Ishizuka"
                    },
                    {
                        "name": "G. Barr"
                    },
                    {
                        "name": "D. Barrow"
                    },
                    {
                        "name": "L. Cook"
                    },
                    {
                        "name": "S. Samani"
                    },
                    {
                        "name": "D. Wark"
                    },
                    {
                        "name": "A. Holin"
                    },
                    {
                        "name": "F. Nova"
                    },
                    {
                        "name": "S. Jung"
                    },
                    {
                        "name": "J. Yoo"
                    },
                    {
                        "name": "J. E. P. Fannon"
                    },
                    {
                        "name": "L. Kneale"
                    },
                    {
                        "name": "M. Malek"
                    },
                    {
                        "name": "J. M. McElwee"
                    },
                    {
                        "name": "T. Peacock"
                    },
                    {
                        "name": "P. Stowell"
                    },
                    {
                        "name": "M. D. Thiesse"
                    },
                    {
                        "name": "L. F. Thompson"
                    },
                    {
                        "name": "H. Okazawa"
                    },
                    {
                        "name": "S. M. Lakshmi"
                    },
                    {
                        "name": "E. Kwon"
                    },
                    {
                        "name": "M. W. Lee"
                    },
                    {
                        "name": "J. W. Seo"
                    },
                    {
                        "name": "I. Yu"
                    },
                    {
                        "name": "Y. Ashida"
                    },
                    {
                        "name": "A. K. Ichikawa"
                    },
                    {
                        "name": "K. D. Nakamura"
                    },
                    {
                        "name": "S. Goto"
                    },
                    {
                        "name": "H. Hayasaki"
                    },
                    {
                        "name": "S. Kodama"
                    },
                    {
                        "name": "Y. Kong"
                    },
                    {
                        "name": "Y. Masaki"
                    },
                    {
                        "name": "Y. Mizuno"
                    },
                    {
                        "name": "T. Muro"
                    },
                    {
                        "name": "K. Nakagiri"
                    },
                    {
                        "name": "Y. Nakajima"
                    },
                    {
                        "name": "N. Taniuchi"
                    },
                    {
                        "name": "M. Yokoyama"
                    },
                    {
                        "name": "P. de Perio"
                    },
                    {
                        "name": "S. Fujita"
                    },
                    {
                        "name": "C. Jess-Valls"
                    },
                    {
                        "name": "K. Martens"
                    },
                    {
                        "name": "Ll. Marti"
                    },
                    {
                        "name": "K. M. Tsui"
                    },
                    {
                        "name": "M. R. Vagins"
                    },
                    {
                        "name": "J. Xia"
                    },
                    {
                        "name": "M. Kuze"
                    },
                    {
                        "name": "S. Izumiyama"
                    },
                    {
                        "name": "R. Matsumoto"
                    },
                    {
                        "name": "R. Asaka"
                    },
                    {
                        "name": "M. Ishitsuka"
                    },
                    {
                        "name": "M. Sugo"
                    },
                    {
                        "name": "M. Wako"
                    },
                    {
                        "name": "K. Yamauchi"
                    },
                    {
                        "name": "Y. Nakano"
                    },
                    {
                        "name": "F. Cormier"
                    },
                    {
                        "name": "R. Gaur"
                    },
                    {
                        "name": "M. Hartz"
                    },
                    {
                        "name": "A. Konaka"
                    },
                    {
                        "name": "X. Li"
                    },
                    {
                        "name": "B. R. Smithers"
                    },
                    {
                        "name": "S. Chen"
                    },
                    {
                        "name": "Y. Wu"
                    },
                    {
                        "name": "B. D. Xu"
                    },
                    {
                        "name": "A. Q. Zhang"
                    },
                    {
                        "name": "B. Zhang"
                    },
                    {
                        "name": "H. Adhikary"
                    },
                    {
                        "name": "M. Girgus"
                    },
                    {
                        "name": "P. Govindaraj"
                    },
                    {
                        "name": "M. Posiadala-Zezula"
                    },
                    {
                        "name": "Y. S. Prabhu"
                    },
                    {
                        "name": "S. B. Boyd"
                    },
                    {
                        "name": "R. Edwards"
                    },
                    {
                        "name": "D. Hadley"
                    },
                    {
                        "name": "M. Nicholson"
                    },
                    {
                        "name": "M. O'Flaherty"
                    },
                    {
                        "name": "B. Richards"
                    },
                    {
                        "name": "A. Ali"
                    },
                    {
                        "name": "B. Jamieson"
                    },
                    {
                        "name": "C. Bronner"
                    },
                    {
                        "name": "D. Horiguchi"
                    },
                    {
                        "name": "A. Minamino"
                    },
                    {
                        "name": "Y. Sasaki"
                    },
                    {
                        "name": "R. Shibayama"
                    },
                    {
                        "name": "R. Shimamura"
                    }
                ],
                "author_detail": {
                    "name": "R. Shimamura"
                },
                "author": "R. Shimamura",
                "arxiv_comment": "11 pages, 4 figures, and 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03470v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03470v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.00801v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.00801v2",
                "updated": "2025-11-05T13:45:24Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    13,
                    45,
                    24,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-02T04:46:43Z",
                "published_parsed": [
                    2025,
                    11,
                    2,
                    4,
                    46,
                    43,
                    6,
                    306,
                    0
                ],
                "title": "Med-Banana-50K: A Cross-modality Large-Scale Dataset for Text-guided\n  Medical Image Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Med-Banana-50K: A Cross-modality Large-Scale Dataset for Text-guided\n  Medical Image Editing"
                },
                "summary": "Recent advances in multimodal large language models have enabled remarkable\nmedical image editing capabilities. However, the research community's progress\nremains constrained by the absence of large-scale, high-quality, and openly\naccessible datasets built specifically for medical image editing with strict\nanatomical and clinical constraints. We introduce Med-Banana-50K, a\ncomprehensive 50K-image dataset for instruction-based medical image editing\nspanning three modalities (chest X-ray, brain MRI, fundus photography) and 23\ndisease types. Our dataset is constructed by leveraging Gemini-2.5-Flash-Image\nto generate bidirectional edits (lesion addition and removal) from real medical\nimages. What distinguishes Med-Banana-50K from general-domain editing datasets\nis our systematic approach to medical quality control: we employ LLM-as-Judge\nwith a medically grounded rubric (instruction compliance, structural\nplausibility, realism, and fidelity preservation) and history-aware iterative\nrefinement up to five rounds. Beyond single-turn editing, Med-Banana-50K\nincludes 37K failed attempts with full conversation logs for preference\nlearning and alignment research. By providing this large-scale, medically\nvalidated, and fully documented resource, Med-Banana-50K establishes a\nfoundation for training and evaluating the next generation of medical image\nediting models.Our dataset and code are publicly available at\n[https://github.com/richardChenzhihui/med-banana-50k].",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in multimodal large language models have enabled remarkable\nmedical image editing capabilities. However, the research community's progress\nremains constrained by the absence of large-scale, high-quality, and openly\naccessible datasets built specifically for medical image editing with strict\nanatomical and clinical constraints. We introduce Med-Banana-50K, a\ncomprehensive 50K-image dataset for instruction-based medical image editing\nspanning three modalities (chest X-ray, brain MRI, fundus photography) and 23\ndisease types. Our dataset is constructed by leveraging Gemini-2.5-Flash-Image\nto generate bidirectional edits (lesion addition and removal) from real medical\nimages. What distinguishes Med-Banana-50K from general-domain editing datasets\nis our systematic approach to medical quality control: we employ LLM-as-Judge\nwith a medically grounded rubric (instruction compliance, structural\nplausibility, realism, and fidelity preservation) and history-aware iterative\nrefinement up to five rounds. Beyond single-turn editing, Med-Banana-50K\nincludes 37K failed attempts with full conversation logs for preference\nlearning and alignment research. By providing this large-scale, medically\nvalidated, and fully documented resource, Med-Banana-50K establishes a\nfoundation for training and evaluating the next generation of medical image\nediting models.Our dataset and code are publicly available at\n[https://github.com/richardChenzhihui/med-banana-50k]."
                },
                "authors": [
                    {
                        "name": "Zhihui Chen"
                    },
                    {
                        "name": "Mengling Feng"
                    }
                ],
                "author_detail": {
                    "name": "Mengling Feng"
                },
                "author": "Mengling Feng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.00801v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.00801v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02179v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02179v3",
                "updated": "2025-11-05T13:25:10Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    13,
                    25,
                    10,
                    2,
                    309,
                    0
                ],
                "published": "2025-07-02T22:23:17Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    22,
                    23,
                    17,
                    2,
                    183,
                    0
                ],
                "title": "A general polynomial emulator for cosmology via moment projection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A general polynomial emulator for cosmology via moment projection"
                },
                "summary": "We present MomentEmu, a general-purpose polynomial emulator for fast and\ninterpretable mappings between theoretical parameters and observational\nfeatures. The method constructs moment matrices to project simulation data onto\npolynomial bases, yielding symbolic expressions that approximate the target\nmapping. Compared to neural-network-based emulators, MomentEmu offers\nnegligible training cost, millisecond-level evaluation, and transparent\nfunctional forms. As a proof-of-concept demonstration, we develop two\nemulators: PolyCAMB-$D_\\ell$, which maps six cosmological parameters to the CMB\npower spectra (TT, EE, BB, TE), and PolyCAMB-peak, which enables a\nbidirectional mapping between the cosmological parameters and the acoustic peak\nfeatures of $D_\\ell^{\\rm TT}$. PolyCAMB-$D_\\ell$ achieves sub-percent accuracy\nover multipoles $\\ell \\leq 4050$, while PolyCAMB-peak also attains comparable\nprecision and produces symbolic forms consistent with known analytical\napproximations. The method is well suited for forward modelling, parameter\ninference, and uncertainty propagation, particularly when the parameter space\nis moderate in dimensionality and the mapping is smooth. MomentEmu offers a\nlightweight and portable alternative to regression-based or black-box emulators\nin cosmological analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present MomentEmu, a general-purpose polynomial emulator for fast and\ninterpretable mappings between theoretical parameters and observational\nfeatures. The method constructs moment matrices to project simulation data onto\npolynomial bases, yielding symbolic expressions that approximate the target\nmapping. Compared to neural-network-based emulators, MomentEmu offers\nnegligible training cost, millisecond-level evaluation, and transparent\nfunctional forms. As a proof-of-concept demonstration, we develop two\nemulators: PolyCAMB-$D_\\ell$, which maps six cosmological parameters to the CMB\npower spectra (TT, EE, BB, TE), and PolyCAMB-peak, which enables a\nbidirectional mapping between the cosmological parameters and the acoustic peak\nfeatures of $D_\\ell^{\\rm TT}$. PolyCAMB-$D_\\ell$ achieves sub-percent accuracy\nover multipoles $\\ell \\leq 4050$, while PolyCAMB-peak also attains comparable\nprecision and produces symbolic forms consistent with known analytical\napproximations. The method is well suited for forward modelling, parameter\ninference, and uncertainty propagation, particularly when the parameter space\nis moderate in dimensionality and the mapping is smooth. MomentEmu offers a\nlightweight and portable alternative to regression-based or black-box emulators\nin cosmological analysis."
                },
                "authors": [
                    {
                        "name": "Zheng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Zhang"
                },
                "author": "Zheng Zhang",
                "arxiv_comment": "10 pages, 8 figures. Under the review of the main MNRAS journal.\n  Substantial revision has been implemented: new emulators for polarisation\n  power spectra, extented to lmax=4050, etc",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02179v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02179v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.01066v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.01066v2",
                "updated": "2025-11-05T13:19:47Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    13,
                    19,
                    47,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-02T20:16:38Z",
                "published_parsed": [
                    2025,
                    11,
                    2,
                    20,
                    16,
                    38,
                    6,
                    306,
                    0
                ],
                "title": "HPLT 3.0: Very Large-Scale Multilingual Resources for LLM and MT. Mono-\n  and Bi-lingual Data, Multilingual Evaluation, and Pre-Trained Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HPLT 3.0: Very Large-Scale Multilingual Resources for LLM and MT. Mono-\n  and Bi-lingual Data, Multilingual Evaluation, and Pre-Trained Models"
                },
                "summary": "We present an ongoing initiative to provide open, very large, high-quality,\nand richly annotated textual datasets for almost 200 languages. At 30 trillion\ntokens, this is likely the largest generally available multilingual collection\nof LLM pre-training data. These datasets are derived from web crawls from\ndifferent sources and accompanied with a complete, open-source pipeline for\ndocument selection from web archives, text extraction from HTML, language\nidentification for noisy texts, exact and near-deduplication, annotation with,\namong others, register labels, text quality estimates, and personally\nidentifiable information; and final selection and filtering. We report on data\nquality probes through contrastive and analytical statistics, through manual\ninspection of samples for 24 languages, and through end-to-end evaluation of\nvarious language model architectures trained on this data. For multilingual LLM\nevaluation, we provide a comprehensive collection of benchmarks for nine\nEuropean languages, with special emphasis on natively created tasks, mechanisms\nto mitigate prompt sensitivity, and refined normalization and aggregation of\nscores. Additionally, we train and evaluate a family of 57 monolingual\nencoder-decoder models, as well as a handful of monolingual GPT-like reference\nmodels. Besides the monolingual data and models, we also present a very large\ncollection of parallel texts automatically mined from this data, together with\na novel parallel corpus synthesized via machine translation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present an ongoing initiative to provide open, very large, high-quality,\nand richly annotated textual datasets for almost 200 languages. At 30 trillion\ntokens, this is likely the largest generally available multilingual collection\nof LLM pre-training data. These datasets are derived from web crawls from\ndifferent sources and accompanied with a complete, open-source pipeline for\ndocument selection from web archives, text extraction from HTML, language\nidentification for noisy texts, exact and near-deduplication, annotation with,\namong others, register labels, text quality estimates, and personally\nidentifiable information; and final selection and filtering. We report on data\nquality probes through contrastive and analytical statistics, through manual\ninspection of samples for 24 languages, and through end-to-end evaluation of\nvarious language model architectures trained on this data. For multilingual LLM\nevaluation, we provide a comprehensive collection of benchmarks for nine\nEuropean languages, with special emphasis on natively created tasks, mechanisms\nto mitigate prompt sensitivity, and refined normalization and aggregation of\nscores. Additionally, we train and evaluate a family of 57 monolingual\nencoder-decoder models, as well as a handful of monolingual GPT-like reference\nmodels. Besides the monolingual data and models, we also present a very large\ncollection of parallel texts automatically mined from this data, together with\na novel parallel corpus synthesized via machine translation."
                },
                "authors": [
                    {
                        "name": "Stephan Oepen"
                    },
                    {
                        "name": "Nikolay Arefev"
                    },
                    {
                        "name": "Mikko Aulamo"
                    },
                    {
                        "name": "Marta Ban"
                    },
                    {
                        "name": "Maja Buljan"
                    },
                    {
                        "name": "Laurie Burchell"
                    },
                    {
                        "name": "Lucas Charpentier"
                    },
                    {
                        "name": "Pinzhen Chen"
                    },
                    {
                        "name": "Mariya Fedorova"
                    },
                    {
                        "name": "Ona de Gibert"
                    },
                    {
                        "name": "Barry Haddow"
                    },
                    {
                        "name": "Jan Haji"
                    },
                    {
                        "name": "Jindich Helcl"
                    },
                    {
                        "name": "Andrey Kutuzov"
                    },
                    {
                        "name": "Veronika Laippala"
                    },
                    {
                        "name": "Zihao Li"
                    },
                    {
                        "name": "Risto Luukkonen"
                    },
                    {
                        "name": "Bhavitvya Malik"
                    },
                    {
                        "name": "Vladislav Mikhailov"
                    },
                    {
                        "name": "Amanda Myntti"
                    },
                    {
                        "name": "Dayyn O'Brien"
                    },
                    {
                        "name": "Lucie Polkov"
                    },
                    {
                        "name": "Sampo Pyysalo"
                    },
                    {
                        "name": "Gema Ramrez Snchez"
                    },
                    {
                        "name": "Janine Siewert"
                    },
                    {
                        "name": "Pavel Stepachev"
                    },
                    {
                        "name": "Jrg Tiedemann"
                    },
                    {
                        "name": "Teemu Vahtola"
                    },
                    {
                        "name": "Duan Vari"
                    },
                    {
                        "name": "Fedor Vitiugin"
                    },
                    {
                        "name": "Tea Vojtchov"
                    },
                    {
                        "name": "Jaume Zaragoza"
                    }
                ],
                "author_detail": {
                    "name": "Jaume Zaragoza"
                },
                "author": "Jaume Zaragoza",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.01066v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.01066v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.22193v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.22193v2",
                "updated": "2025-11-05T13:05:52Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    13,
                    5,
                    52,
                    2,
                    309,
                    0
                ],
                "published": "2025-10-25T07:19:52Z",
                "published_parsed": [
                    2025,
                    10,
                    25,
                    7,
                    19,
                    52,
                    5,
                    298,
                    0
                ],
                "title": "(Approximate) Matrix Multiplication via Convolutions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "(Approximate) Matrix Multiplication via Convolutions"
                },
                "summary": "We study the capability of the Fast Fourier Transform (FFT) to accelerate\nexact and approximate matrix multiplication without using Strassen-like\ndivide-and-conquer. We present a simple exact algorithm running in\n$O(n^{2.89})$ time, which only sums a few convolutions (FFTs) in\n$\\mathbb{Z}_{m}^{k}$, building on the work of Cohn, Kleinberg, Szegedy and\nUmans (2005). As a corollary, combining this algorithm with linear sketching\nbreaks the longstanding linear speed-accuracy tradeoff for \"combinatorial\"\napproximate matrix multiplication (AMM, Pagh'13, Sarlos'06,\nClarkson-Woodruff'13), achieving error $\\frac{1}{r^{1.1}}\\left\\lVert \\mathbf{A}\n\\right\\rVert_{F}^{2}\\left\\lVert \\mathbf{B}\\right\\rVert_{F}^{2}$ in $O(rn^{2})$\ntime, using nothing but FFTs.\n  Motivated by the rich literature for approximating polynomials, our main\ncontribution in this paper is extending the group-theoretic framework of Cohn\nand Umans (2003) to approximate matrix multiplication (AMM). Specifically, we\nintroduce and study an approximate notion of the Triple Product Property, which\nin the abelian case is equivalent to finding a Sumset which minimizes\n(multi-)intersections with an arithmetic progression. We prove tight bounds on\nthis quantity for abelian groups (yielding a simple and practical AMM algorithm\nvia polynomial multiplication), and establish a weaker lower bound for\nnon-abelian groups, extending a lemma of Gowers. Finally, we propose a concrete\napproach that uses low-degree approximation of multi-variate polynomials for\nAMM, which we believe will lead to practical, non-asymptotic AMM algorithms in\nreal-world applications, most notably LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the capability of the Fast Fourier Transform (FFT) to accelerate\nexact and approximate matrix multiplication without using Strassen-like\ndivide-and-conquer. We present a simple exact algorithm running in\n$O(n^{2.89})$ time, which only sums a few convolutions (FFTs) in\n$\\mathbb{Z}_{m}^{k}$, building on the work of Cohn, Kleinberg, Szegedy and\nUmans (2005). As a corollary, combining this algorithm with linear sketching\nbreaks the longstanding linear speed-accuracy tradeoff for \"combinatorial\"\napproximate matrix multiplication (AMM, Pagh'13, Sarlos'06,\nClarkson-Woodruff'13), achieving error $\\frac{1}{r^{1.1}}\\left\\lVert \\mathbf{A}\n\\right\\rVert_{F}^{2}\\left\\lVert \\mathbf{B}\\right\\rVert_{F}^{2}$ in $O(rn^{2})$\ntime, using nothing but FFTs.\n  Motivated by the rich literature for approximating polynomials, our main\ncontribution in this paper is extending the group-theoretic framework of Cohn\nand Umans (2003) to approximate matrix multiplication (AMM). Specifically, we\nintroduce and study an approximate notion of the Triple Product Property, which\nin the abelian case is equivalent to finding a Sumset which minimizes\n(multi-)intersections with an arithmetic progression. We prove tight bounds on\nthis quantity for abelian groups (yielding a simple and practical AMM algorithm\nvia polynomial multiplication), and establish a weaker lower bound for\nnon-abelian groups, extending a lemma of Gowers. Finally, we propose a concrete\napproach that uses low-degree approximation of multi-variate polynomials for\nAMM, which we believe will lead to practical, non-asymptotic AMM algorithms in\nreal-world applications, most notably LLM inference."
                },
                "authors": [
                    {
                        "name": "Yahel Uffenheimer"
                    },
                    {
                        "name": "Omri Weinstein"
                    }
                ],
                "author_detail": {
                    "name": "Omri Weinstein"
                },
                "author": "Omri Weinstein",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.22193v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.22193v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03441v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03441v1",
                "updated": "2025-11-05T13:02:06Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    13,
                    2,
                    6,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T13:02:06Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    13,
                    2,
                    6,
                    2,
                    309,
                    0
                ],
                "title": "CareMedEval dataset: Evaluating Critical Appraisal and Reasoning in the\n  Biomedical Field",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CareMedEval dataset: Evaluating Critical Appraisal and Reasoning in the\n  Biomedical Field"
                },
                "summary": "Critical appraisal of scientific literature is an essential skill in the\nbiomedical field. While large language models (LLMs) can offer promising\nsupport in this task, their reliability remains limited, particularly for\ncritical reasoning in specialized domains. We introduce CareMedEval, an\noriginal dataset designed to evaluate LLMs on biomedical critical appraisal and\nreasoning tasks. Derived from authentic exams taken by French medical students,\nthe dataset contains 534 questions based on 37 scientific articles. Unlike\nexisting benchmarks, CareMedEval explicitly evaluates critical reading and\nreasoning grounded in scientific papers. Benchmarking state-of-the-art\ngeneralist and biomedical-specialized LLMs under various context conditions\nreveals the difficulty of the task: open and commercial models fail to exceed\nan Exact Match Rate of 0.5 even though generating intermediate reasoning tokens\nconsiderably improves the results. Yet, models remain challenged especially on\nquestions about study limitations and statistical analysis. CareMedEval\nprovides a challenging benchmark for grounded reasoning, exposing current LLM\nlimitations and paving the way for future development of automated support for\ncritical appraisal.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Critical appraisal of scientific literature is an essential skill in the\nbiomedical field. While large language models (LLMs) can offer promising\nsupport in this task, their reliability remains limited, particularly for\ncritical reasoning in specialized domains. We introduce CareMedEval, an\noriginal dataset designed to evaluate LLMs on biomedical critical appraisal and\nreasoning tasks. Derived from authentic exams taken by French medical students,\nthe dataset contains 534 questions based on 37 scientific articles. Unlike\nexisting benchmarks, CareMedEval explicitly evaluates critical reading and\nreasoning grounded in scientific papers. Benchmarking state-of-the-art\ngeneralist and biomedical-specialized LLMs under various context conditions\nreveals the difficulty of the task: open and commercial models fail to exceed\nan Exact Match Rate of 0.5 even though generating intermediate reasoning tokens\nconsiderably improves the results. Yet, models remain challenged especially on\nquestions about study limitations and statistical analysis. CareMedEval\nprovides a challenging benchmark for grounded reasoning, exposing current LLM\nlimitations and paving the way for future development of automated support for\ncritical appraisal."
                },
                "authors": [
                    {
                        "name": "Doria Bonzi"
                    },
                    {
                        "name": "Alexandre Guiggi"
                    },
                    {
                        "name": "Frdric Bchet"
                    },
                    {
                        "name": "Carlos Ramisch"
                    },
                    {
                        "name": "Benoit Favre"
                    }
                ],
                "author_detail": {
                    "name": "Benoit Favre"
                },
                "author": "Benoit Favre",
                "arxiv_comment": "Preprint submitted to LREC 2026 (under review) To access the dataset,\n  see https://github.com/bonzid/CareMedEval",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03441v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03441v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03434v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03434v1",
                "updated": "2025-11-05T12:50:06Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    12,
                    50,
                    6,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T12:50:06Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    12,
                    50,
                    6,
                    2,
                    309,
                    0
                ],
                "title": "Inter-Agent Trust Models: A Comparative Study of Brief, Claim, Proof,\n  Stake, Reputation and Constraint in Agentic Web Protocol Design-A2A, AP2,\n  ERC-8004, and Beyond",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inter-Agent Trust Models: A Comparative Study of Brief, Claim, Proof,\n  Stake, Reputation and Constraint in Agentic Web Protocol Design-A2A, AP2,\n  ERC-8004, and Beyond"
                },
                "summary": "As the \"agentic web\" takes shape-billions of AI agents (often LLM-powered)\nautonomously transacting and collaborating-trust shifts from human oversight to\nprotocol design. In 2025, several inter-agent protocols crystallized this\nshift, including Google's Agent-to-Agent (A2A), Agent Payments Protocol (AP2),\nand Ethereum's ERC-8004 \"Trustless Agents,\" yet their underlying trust\nassumptions remain under-examined. This paper presents a comparative study of\ntrust models in inter-agent protocol design: Brief (self- or third-party\nverifiable claims), Claim (self-proclaimed capabilities and identity, e.g.\nAgentCard), Proof (cryptographic verification, including zero-knowledge proofs\nand trusted execution environment attestations), Stake (bonded collateral with\nslashing and insurance), Reputation (crowd feedback and graph-based trust\nsignals), and Constraint (sandboxing and capability bounding). For each, we\nanalyze assumptions, attack surfaces, and design trade-offs, with particular\nemphasis on LLM-specific fragilities-prompt injection,\nsycophancy/nudge-susceptibility, hallucination, deception, and\nmisalignment-that render purely reputational or claim-only approaches brittle.\nOur findings indicate no single mechanism suffices. We argue for\ntrustless-by-default architectures anchored in Proof and Stake to gate\nhigh-impact actions, augmented by Brief for identity and discovery and\nReputation overlays for flexibility and social signals. We comparatively\nevaluate A2A, AP2, ERC-8004 and related historical variations in academic\nresearch under metrics spanning security, privacy, latency/cost, and social\nrobustness (Sybil/collusion/whitewashing resistance). We conclude with hybrid\ntrust model recommendations that mitigate reputation gaming and misinformed LLM\nbehavior, and we distill actionable design guidelines for safer, interoperable,\nand scalable agent economies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the \"agentic web\" takes shape-billions of AI agents (often LLM-powered)\nautonomously transacting and collaborating-trust shifts from human oversight to\nprotocol design. In 2025, several inter-agent protocols crystallized this\nshift, including Google's Agent-to-Agent (A2A), Agent Payments Protocol (AP2),\nand Ethereum's ERC-8004 \"Trustless Agents,\" yet their underlying trust\nassumptions remain under-examined. This paper presents a comparative study of\ntrust models in inter-agent protocol design: Brief (self- or third-party\nverifiable claims), Claim (self-proclaimed capabilities and identity, e.g.\nAgentCard), Proof (cryptographic verification, including zero-knowledge proofs\nand trusted execution environment attestations), Stake (bonded collateral with\nslashing and insurance), Reputation (crowd feedback and graph-based trust\nsignals), and Constraint (sandboxing and capability bounding). For each, we\nanalyze assumptions, attack surfaces, and design trade-offs, with particular\nemphasis on LLM-specific fragilities-prompt injection,\nsycophancy/nudge-susceptibility, hallucination, deception, and\nmisalignment-that render purely reputational or claim-only approaches brittle.\nOur findings indicate no single mechanism suffices. We argue for\ntrustless-by-default architectures anchored in Proof and Stake to gate\nhigh-impact actions, augmented by Brief for identity and discovery and\nReputation overlays for flexibility and social signals. We comparatively\nevaluate A2A, AP2, ERC-8004 and related historical variations in academic\nresearch under metrics spanning security, privacy, latency/cost, and social\nrobustness (Sybil/collusion/whitewashing resistance). We conclude with hybrid\ntrust model recommendations that mitigate reputation gaming and misinformed LLM\nbehavior, and we distill actionable design guidelines for safer, interoperable,\nand scalable agent economies."
                },
                "authors": [
                    {
                        "name": "Botao 'Amber' Hu"
                    },
                    {
                        "name": "Helena Rong"
                    }
                ],
                "author_detail": {
                    "name": "Helena Rong"
                },
                "author": "Helena Rong",
                "arxiv_comment": "Submitted to AAAI 2026 Workshop on Trust and Control in Agentic AI\n  (TrustAgent)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03434v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03434v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03424v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03424v1",
                "updated": "2025-11-05T12:40:34Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    12,
                    40,
                    34,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T12:40:34Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    12,
                    40,
                    34,
                    2,
                    309,
                    0
                ],
                "title": "The moment is here: a generalised class of estimators for fuzzy\n  regression discontinuity designs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The moment is here: a generalised class of estimators for fuzzy\n  regression discontinuity designs"
                },
                "summary": "The standard fuzzy regression discontinuity (FRD) estimator is a ratio of\ndifferences of local polynomial estimators. I show that this estimator does not\nhave finite moments of any order in finite samples, regardless of the choice of\nkernel function, bandwidth, or order of polynomial. This leads to an imprecise\nestimator with a heavy-tailed sampling distribution, and inaccurate inference\nwith small sample sizes or when the discontinuity in the probability of\ntreatment assignment at the cutoff is small. I present a generalised class of\ncomputationally simple FRD estimators, which contains a continuum of estimators\nwith finite moments of all orders in finite samples, and nests both the\nstandard FRD and sharp (SRD) estimators. The class is indexed by a single\ntuning parameter, and I provide simple values that lead to substantial\nimprovements in median bias, median absolute deviation and root mean squared\nerror. These new estimators remain very stable in small samples, or when the\ndiscontinuity in the probability of treatment assignment at the cutoff is\nsmall. Simple confidence intervals that have strong coverage and length\nproperties in small samples are also developed. The improvements are seen\nacross a wide range of models and using common bandwidth selection algorithms\nin extensive Monte Carlo simulations. The improved stability and performance of\nthe estimators and confidence intervals is also demonstrated using data on\nclass size effects on educational attainment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The standard fuzzy regression discontinuity (FRD) estimator is a ratio of\ndifferences of local polynomial estimators. I show that this estimator does not\nhave finite moments of any order in finite samples, regardless of the choice of\nkernel function, bandwidth, or order of polynomial. This leads to an imprecise\nestimator with a heavy-tailed sampling distribution, and inaccurate inference\nwith small sample sizes or when the discontinuity in the probability of\ntreatment assignment at the cutoff is small. I present a generalised class of\ncomputationally simple FRD estimators, which contains a continuum of estimators\nwith finite moments of all orders in finite samples, and nests both the\nstandard FRD and sharp (SRD) estimators. The class is indexed by a single\ntuning parameter, and I provide simple values that lead to substantial\nimprovements in median bias, median absolute deviation and root mean squared\nerror. These new estimators remain very stable in small samples, or when the\ndiscontinuity in the probability of treatment assignment at the cutoff is\nsmall. Simple confidence intervals that have strong coverage and length\nproperties in small samples are also developed. The improvements are seen\nacross a wide range of models and using common bandwidth selection algorithms\nin extensive Monte Carlo simulations. The improved stability and performance of\nthe estimators and confidence intervals is also demonstrated using data on\nclass size effects on educational attainment."
                },
                "authors": [
                    {
                        "name": "Stuart Lane"
                    }
                ],
                "author_detail": {
                    "name": "Stuart Lane"
                },
                "author": "Stuart Lane",
                "arxiv_comment": "73 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03424v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03424v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03421v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03421v1",
                "updated": "2025-11-05T12:38:11Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    12,
                    38,
                    11,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T12:38:11Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    12,
                    38,
                    11,
                    2,
                    309,
                    0
                ],
                "title": "Light over Heavy: Automated Performance Requirements Quantification with\n  Linguistic Inducement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Light over Heavy: Automated Performance Requirements Quantification with\n  Linguistic Inducement"
                },
                "summary": "Elicited performance requirements need to be quantified for compliance in\ndifferent engineering tasks, e.g., configuration tuning and performance\ntesting. Much existing work has relied on manual quantification, which is\nexpensive and error-prone due to the imprecision. In this paper, we present\nLQPR, a highly efficient automatic approach for performance requirements\nquantification.LQPR relies on a new theoretical framework that converts\nquantification as a classification problem. Despite the prevalent applications\nof Large Language Models (LLMs) for requirement analytics, LQPR takes a\ndifferent perspective to address the classification: we observed that\nperformance requirements can exhibit strong patterns and are often\nshort/concise, therefore we design a lightweight linguistically induced\nmatching mechanism. We compare LQPR against nine state-of-the-art\nlearning-based approaches over diverse datasets, demonstrating that it is\nranked as the sole best for 75% or more cases with two orders less cost. Our\nwork proves that, at least for performance requirement quantification,\nspecialized methods can be more suitable than the general LLM-driven\napproaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Elicited performance requirements need to be quantified for compliance in\ndifferent engineering tasks, e.g., configuration tuning and performance\ntesting. Much existing work has relied on manual quantification, which is\nexpensive and error-prone due to the imprecision. In this paper, we present\nLQPR, a highly efficient automatic approach for performance requirements\nquantification.LQPR relies on a new theoretical framework that converts\nquantification as a classification problem. Despite the prevalent applications\nof Large Language Models (LLMs) for requirement analytics, LQPR takes a\ndifferent perspective to address the classification: we observed that\nperformance requirements can exhibit strong patterns and are often\nshort/concise, therefore we design a lightweight linguistically induced\nmatching mechanism. We compare LQPR against nine state-of-the-art\nlearning-based approaches over diverse datasets, demonstrating that it is\nranked as the sole best for 75% or more cases with two orders less cost. Our\nwork proves that, at least for performance requirement quantification,\nspecialized methods can be more suitable than the general LLM-driven\napproaches."
                },
                "authors": [
                    {
                        "name": "Shihai Wang"
                    },
                    {
                        "name": "Tao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tao Chen"
                },
                "author": "Tao Chen",
                "arxiv_comment": "accepted by ICSE 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03421v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03421v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03420v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03420v1",
                "updated": "2025-11-05T12:36:16Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    12,
                    36,
                    16,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T12:36:16Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    12,
                    36,
                    16,
                    2,
                    309,
                    0
                ],
                "title": "Multi-layer dissolution exponential-family models for weighted signed\n  networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-layer dissolution exponential-family models for weighted signed\n  networks"
                },
                "summary": "Understanding the structure of weighted signed networks is essential for\nanalysing social systems in which relationships vary both in sign and strength.\nDespite significant advances in statistical network analysis, there is still a\nlack of statistical models that can jointly and rigorously account for both the\nsign and strength of relationships in networks. We introduce a multi-layer\ndissolution exponential random graph modelling framework that jointly captures\nthe signed and weighted processes, conditional on the observed interaction\nstructure. The framework enables rigorous assessment of structural balance\neffects while fully accounting for edge weights. To enhance inference, we adopt\na fully-probabilistic Bayesian hierarchical approach that partially pools\ninformation across layers, with parameters estimated via an adaptive\napproximate exchange algorithm. We demonstrate the flexibility and explanatory\npower of the proposed methodology by applying it to bill sponsorship data from\nthe 108th US Senate, revealing complex patterns of signed and weighted\ninteractions and structural balance effects that traditional approaches are\nunable to capture.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the structure of weighted signed networks is essential for\nanalysing social systems in which relationships vary both in sign and strength.\nDespite significant advances in statistical network analysis, there is still a\nlack of statistical models that can jointly and rigorously account for both the\nsign and strength of relationships in networks. We introduce a multi-layer\ndissolution exponential random graph modelling framework that jointly captures\nthe signed and weighted processes, conditional on the observed interaction\nstructure. The framework enables rigorous assessment of structural balance\neffects while fully accounting for edge weights. To enhance inference, we adopt\na fully-probabilistic Bayesian hierarchical approach that partially pools\ninformation across layers, with parameters estimated via an adaptive\napproximate exchange algorithm. We demonstrate the flexibility and explanatory\npower of the proposed methodology by applying it to bill sponsorship data from\nthe 108th US Senate, revealing complex patterns of signed and weighted\ninteractions and structural balance effects that traditional approaches are\nunable to capture."
                },
                "authors": [
                    {
                        "name": "Alberto Caimo"
                    },
                    {
                        "name": "Isabella Gollini"
                    }
                ],
                "author_detail": {
                    "name": "Isabella Gollini"
                },
                "author": "Isabella Gollini",
                "arxiv_comment": "27 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03420v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03420v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03410v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03410v1",
                "updated": "2025-11-05T12:24:20Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    12,
                    24,
                    20,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T12:24:20Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    12,
                    24,
                    20,
                    2,
                    309,
                    0
                ],
                "title": "Knowledge-Augmented Question Error Correction for Chinese Question\n  Answer System with QuestionRAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge-Augmented Question Error Correction for Chinese Question\n  Answer System with QuestionRAG"
                },
                "summary": "Input errors in question-answering (QA) systems often lead to incorrect\nresponses. Large language models (LLMs) struggle with this task, frequently\nfailing to interpret user intent (misinterpretation) or unnecessarily altering\nthe original question's structure (over-correction). We propose QuestionRAG, a\nframework that tackles these problems. To address misinterpretation, it\nenriches the input with external knowledge (e.g., search results, related\nentities). To prevent over-correction, it uses reinforcement learning (RL) to\nalign the model's objective with precise correction, not just paraphrasing. Our\nresults demonstrate that knowledge augmentation is critical for understanding\nfaulty questions. Furthermore, RL-based alignment proves significantly more\neffective than traditional supervised fine-tuning (SFT), boosting the model's\nability to follow instructions and generalize. By integrating these two\nstrategies, QuestionRAG unlocks the full potential of LLMs for the question\ncorrection task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Input errors in question-answering (QA) systems often lead to incorrect\nresponses. Large language models (LLMs) struggle with this task, frequently\nfailing to interpret user intent (misinterpretation) or unnecessarily altering\nthe original question's structure (over-correction). We propose QuestionRAG, a\nframework that tackles these problems. To address misinterpretation, it\nenriches the input with external knowledge (e.g., search results, related\nentities). To prevent over-correction, it uses reinforcement learning (RL) to\nalign the model's objective with precise correction, not just paraphrasing. Our\nresults demonstrate that knowledge augmentation is critical for understanding\nfaulty questions. Furthermore, RL-based alignment proves significantly more\neffective than traditional supervised fine-tuning (SFT), boosting the model's\nability to follow instructions and generalize. By integrating these two\nstrategies, QuestionRAG unlocks the full potential of LLMs for the question\ncorrection task."
                },
                "authors": [
                    {
                        "name": "Longpeng Qiu"
                    },
                    {
                        "name": "Ting Li"
                    },
                    {
                        "name": "Shuai Mao"
                    },
                    {
                        "name": "Nan Yang"
                    },
                    {
                        "name": "Xiaohui Yan"
                    }
                ],
                "author_detail": {
                    "name": "Xiaohui Yan"
                },
                "author": "Xiaohui Yan",
                "arxiv_comment": "EMNLP2025 Industry Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03410v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03410v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03408v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03408v1",
                "updated": "2025-11-05T12:20:45Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    12,
                    20,
                    45,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T12:20:45Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    12,
                    20,
                    45,
                    2,
                    309,
                    0
                ],
                "title": "Efficient Reasoning via Thought-Training and Thought-Free Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Reasoning via Thought-Training and Thought-Free Inference"
                },
                "summary": "Recent advances in large language models (LLMs) have leveraged explicit\nChain-of-Thought (CoT) prompting to improve reasoning accuracy. However, most\nexisting methods primarily compress verbose reasoning outputs. These\nLong-to-Short transformations aim to improve efficiency, but still rely on\nexplicit reasoning during inference. In this work, we introduce \\textbf{3TF}\n(\\textbf{T}hought-\\textbf{T}raining and \\textbf{T}hought-\\textbf{F}ree\ninference), a framework for efficient reasoning that takes a Short-to-Long\nperspective. We first train a hybrid model that can operate in both reasoning\nand non-reasoning modes, and then further train it on CoT-annotated data to\ninternalize structured reasoning, while enforcing concise, thought-free outputs\nat inference time using the no-reasoning mode. Unlike compression-based\napproaches, 3TF improves the reasoning quality of non-reasoning outputs,\nenabling models to perform rich internal reasoning implicitly while keeping\nexternal outputs short. Empirically, 3TF-trained models obtain large\nimprovements on reasoning benchmarks under thought-free inference,\ndemonstrating that high quality reasoning can be learned and executed\nimplicitly without explicit step-by-step generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have leveraged explicit\nChain-of-Thought (CoT) prompting to improve reasoning accuracy. However, most\nexisting methods primarily compress verbose reasoning outputs. These\nLong-to-Short transformations aim to improve efficiency, but still rely on\nexplicit reasoning during inference. In this work, we introduce \\textbf{3TF}\n(\\textbf{T}hought-\\textbf{T}raining and \\textbf{T}hought-\\textbf{F}ree\ninference), a framework for efficient reasoning that takes a Short-to-Long\nperspective. We first train a hybrid model that can operate in both reasoning\nand non-reasoning modes, and then further train it on CoT-annotated data to\ninternalize structured reasoning, while enforcing concise, thought-free outputs\nat inference time using the no-reasoning mode. Unlike compression-based\napproaches, 3TF improves the reasoning quality of non-reasoning outputs,\nenabling models to perform rich internal reasoning implicitly while keeping\nexternal outputs short. Empirically, 3TF-trained models obtain large\nimprovements on reasoning benchmarks under thought-free inference,\ndemonstrating that high quality reasoning can be learned and executed\nimplicitly without explicit step-by-step generation."
                },
                "authors": [
                    {
                        "name": "Canhui Wu"
                    },
                    {
                        "name": "Qiong Cao"
                    },
                    {
                        "name": "Chao Xue"
                    },
                    {
                        "name": "Wei Xi"
                    },
                    {
                        "name": "Xiaodong He"
                    }
                ],
                "author_detail": {
                    "name": "Xiaodong He"
                },
                "author": "Xiaodong He",
                "arxiv_comment": "11 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03408v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03408v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03404v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03404v1",
                "updated": "2025-11-05T12:12:35Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    12,
                    12,
                    35,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T12:12:35Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    12,
                    12,
                    35,
                    2,
                    309,
                    0
                ],
                "title": "Towards Realistic Project-Level Code Generation via Multi-Agent\n  Collaboration and Semantic Architecture Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Realistic Project-Level Code Generation via Multi-Agent\n  Collaboration and Semantic Architecture Modeling"
                },
                "summary": "In recent years, Large Language Models (LLMs) have achieved remarkable\nprogress in automated code generation. In real-world software engineering, the\ngrowing demand for rapid iteration and continuous delivery underscores the\nimportance of project-level code generation, where LLMs are expected to\ngenerate complete software projects directly from complex user requirements.\nAlthough existing studies have made initial explorations, they still face key\nlimitations, including unrealistic datasets and unreliable evaluation metrics\nthat fail to reflect real-world complexity, the semantic gap between\nhuman-written requirements and machine-interpretable structures, and\ndifficulties in managing hierarchical dependencies and maintaining quality\nthroughout the generation process. To address these limitations, we first\nintroduce CodeProjectEval, a project-level code generation dataset built from\n18 real-world repositories with 12.7 files and 2,388.6 lines of code per task\non average, supplemented with documentation and executable test cases for\nautomatic evaluation. We further propose ProjectGen, a multi-agent framework\nthat decomposes projects into architecture design, skeleton generation, and\ncode filling stages with iterative refinement and memory-based context\nmanagement. Within this framework, we introduce the Semantic Software\nArchitecture Tree (SSAT), a structured and semantically rich representation\nthat effectively bridges user requirements and source code implementation.\nExperiments show that ProjectGen achieves state-of-the-art performance, passing\n52/124 test cases on the small-scale project-level code generation dataset\nDevBench, a 57% improvement over the baseline approaches, and 310 test cases on\nCodeProjectEval, representing an improvement of roughly tenfold compared to the\nbaselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Large Language Models (LLMs) have achieved remarkable\nprogress in automated code generation. In real-world software engineering, the\ngrowing demand for rapid iteration and continuous delivery underscores the\nimportance of project-level code generation, where LLMs are expected to\ngenerate complete software projects directly from complex user requirements.\nAlthough existing studies have made initial explorations, they still face key\nlimitations, including unrealistic datasets and unreliable evaluation metrics\nthat fail to reflect real-world complexity, the semantic gap between\nhuman-written requirements and machine-interpretable structures, and\ndifficulties in managing hierarchical dependencies and maintaining quality\nthroughout the generation process. To address these limitations, we first\nintroduce CodeProjectEval, a project-level code generation dataset built from\n18 real-world repositories with 12.7 files and 2,388.6 lines of code per task\non average, supplemented with documentation and executable test cases for\nautomatic evaluation. We further propose ProjectGen, a multi-agent framework\nthat decomposes projects into architecture design, skeleton generation, and\ncode filling stages with iterative refinement and memory-based context\nmanagement. Within this framework, we introduce the Semantic Software\nArchitecture Tree (SSAT), a structured and semantically rich representation\nthat effectively bridges user requirements and source code implementation.\nExperiments show that ProjectGen achieves state-of-the-art performance, passing\n52/124 test cases on the small-scale project-level code generation dataset\nDevBench, a 57% improvement over the baseline approaches, and 310 test cases on\nCodeProjectEval, representing an improvement of roughly tenfold compared to the\nbaselines."
                },
                "authors": [
                    {
                        "name": "Qianhui Zhao"
                    },
                    {
                        "name": "Li Zhang"
                    },
                    {
                        "name": "Fang Liu"
                    },
                    {
                        "name": "Junhang Cheng"
                    },
                    {
                        "name": "Chengru Wu"
                    },
                    {
                        "name": "Junchen Ai"
                    },
                    {
                        "name": "Qiaoyuanhe Meng"
                    },
                    {
                        "name": "Lichen Zhang"
                    },
                    {
                        "name": "Xiaoli Lian"
                    },
                    {
                        "name": "Shubin Song"
                    },
                    {
                        "name": "Yuanping Guo"
                    }
                ],
                "author_detail": {
                    "name": "Yuanping Guo"
                },
                "author": "Yuanping Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03404v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03404v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03400v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03400v1",
                "updated": "2025-11-05T12:08:05Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    12,
                    8,
                    5,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T12:08:05Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    12,
                    8,
                    5,
                    2,
                    309,
                    0
                ],
                "title": "GUIDES: Guidance Using Instructor-Distilled Embeddings for Pre-trained\n  Robot Policy Enhancement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GUIDES: Guidance Using Instructor-Distilled Embeddings for Pre-trained\n  Robot Policy Enhancement"
                },
                "summary": "Pre-trained robot policies serve as the foundation of many validated robotic\nsystems, which encapsulate extensive embodied knowledge. However, they often\nlack the semantic awareness characteristic of foundation models, and replacing\nthem entirely is impractical in many situations due to high costs and the loss\nof accumulated knowledge. To address this gap, we introduce GUIDES, a\nlightweight framework that augments pre-trained policies with semantic guidance\nfrom foundation models without requiring architectural redesign. GUIDES employs\na fine-tuned vision-language model (Instructor) to generate contextual\ninstructions, which are encoded by an auxiliary module into guidance\nembeddings. These embeddings are injected into the policy's latent space,\nallowing the legacy model to adapt to this new semantic input through brief,\ntargeted fine-tuning. For inference-time robustness, a large language\nmodel-based Reflector monitors the Instructor's confidence and, when confidence\nis low, initiates a reasoning loop that analyzes execution history, retrieves\nrelevant examples, and augments the VLM's context to refine subsequent actions.\nExtensive validation in the RoboCasa simulation environment across diverse\npolicy architectures shows consistent and substantial improvements in task\nsuccess rates. Real-world deployment on a UR5 robot further demonstrates that\nGUIDES enhances motion precision for critical sub-tasks such as grasping.\nOverall, GUIDES offers a practical and resource-efficient pathway to upgrade,\nrather than replace, validated robot policies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pre-trained robot policies serve as the foundation of many validated robotic\nsystems, which encapsulate extensive embodied knowledge. However, they often\nlack the semantic awareness characteristic of foundation models, and replacing\nthem entirely is impractical in many situations due to high costs and the loss\nof accumulated knowledge. To address this gap, we introduce GUIDES, a\nlightweight framework that augments pre-trained policies with semantic guidance\nfrom foundation models without requiring architectural redesign. GUIDES employs\na fine-tuned vision-language model (Instructor) to generate contextual\ninstructions, which are encoded by an auxiliary module into guidance\nembeddings. These embeddings are injected into the policy's latent space,\nallowing the legacy model to adapt to this new semantic input through brief,\ntargeted fine-tuning. For inference-time robustness, a large language\nmodel-based Reflector monitors the Instructor's confidence and, when confidence\nis low, initiates a reasoning loop that analyzes execution history, retrieves\nrelevant examples, and augments the VLM's context to refine subsequent actions.\nExtensive validation in the RoboCasa simulation environment across diverse\npolicy architectures shows consistent and substantial improvements in task\nsuccess rates. Real-world deployment on a UR5 robot further demonstrates that\nGUIDES enhances motion precision for critical sub-tasks such as grasping.\nOverall, GUIDES offers a practical and resource-efficient pathway to upgrade,\nrather than replace, validated robot policies."
                },
                "authors": [
                    {
                        "name": "Minquan Gao"
                    },
                    {
                        "name": "Xinyi Li"
                    },
                    {
                        "name": "Qing Yan"
                    },
                    {
                        "name": "Xiaojian Sun"
                    },
                    {
                        "name": "Xiaopan Zhang"
                    },
                    {
                        "name": "Chien-Ming Huang"
                    },
                    {
                        "name": "Jiachen Li"
                    }
                ],
                "author_detail": {
                    "name": "Jiachen Li"
                },
                "author": "Jiachen Li",
                "arxiv_comment": "8 pages, 4 figures, Accepted by IEEE IROS 2025 Workshop WIR-M",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03400v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03400v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03399v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03399v1",
                "updated": "2025-11-05T12:07:55Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    12,
                    7,
                    55,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T12:07:55Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    12,
                    7,
                    55,
                    2,
                    309,
                    0
                ],
                "title": "Bayesian Causal Effect Estimation for Categorical Data using Staged Tree\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian Causal Effect Estimation for Categorical Data using Staged Tree\n  Models"
                },
                "summary": "We propose a fully Bayesian approach for causal inference with multivariate\ncategorical data based on staged tree models, a class of probabilistic\ngraphical models capable of representing asymmetric and context-specific\ndependencies. To account for uncertainty in both structure and parameters, we\nintroduce a flexible family of prior distributions over staged trees. These\ninclude product partition models to encourage parsimony, a novel distance-based\nprior to promote interpretable dependence patterns, and an extension that\nincorporates continuous covariates into the learning process. Posterior\ninference is achieved via a tailored Markov Chain Monte Carlo algorithm with\nsplit-and-merge moves, yielding posterior samples of staged trees from which\naverage treatment effects and uncertainty measures are derived. Posterior\nsummaries and uncertainty measures are obtained via techniques from the\nBayesian nonparametrics literature. Two case studies on electronic fetal\nmonitoring and cesarean delivery and on anthracycline therapy and cardiac\ndysfunction in breast cancer illustrate the methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a fully Bayesian approach for causal inference with multivariate\ncategorical data based on staged tree models, a class of probabilistic\ngraphical models capable of representing asymmetric and context-specific\ndependencies. To account for uncertainty in both structure and parameters, we\nintroduce a flexible family of prior distributions over staged trees. These\ninclude product partition models to encourage parsimony, a novel distance-based\nprior to promote interpretable dependence patterns, and an extension that\nincorporates continuous covariates into the learning process. Posterior\ninference is achieved via a tailored Markov Chain Monte Carlo algorithm with\nsplit-and-merge moves, yielding posterior samples of staged trees from which\naverage treatment effects and uncertainty measures are derived. Posterior\nsummaries and uncertainty measures are obtained via techniques from the\nBayesian nonparametrics literature. Two case studies on electronic fetal\nmonitoring and cesarean delivery and on anthracycline therapy and cardiac\ndysfunction in breast cancer illustrate the methods."
                },
                "authors": [
                    {
                        "name": "Andrea Cremaschi"
                    },
                    {
                        "name": "Manuele Leonelli"
                    },
                    {
                        "name": "Gherardo Varando"
                    }
                ],
                "author_detail": {
                    "name": "Gherardo Varando"
                },
                "author": "Gherardo Varando",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03399v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03399v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03394v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03394v1",
                "updated": "2025-11-05T11:55:20Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    11,
                    55,
                    20,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T11:55:20Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    11,
                    55,
                    20,
                    2,
                    309,
                    0
                ],
                "title": "The subtle statistics of the distance ladder: On the distance prior and\n  selection effects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The subtle statistics of the distance ladder: On the distance prior and\n  selection effects"
                },
                "summary": "Statistical methodology is rarely considered significant in distance ladder\nstudies or a potential contributor to the Hubble tension. We suggest it should\nbe, highlighting two appreciable issues. First, astronomical distances are\ninferred latent parameters, requiring a prior. We show that the common\nassumption of (perhaps implicit) uniform priors on distance moduli biases\ndistances low due to objects being intrinsically uniformly distributed in\nvolume. At fixed measured redshifts, this biases the Hubble constant high.\nSecond, selection effects introduce additional factors in the posterior. These\ntypically counteract the effect of the volume prior to some extent, but depend\nsignificantly on the nature of the selection. Typical assumptions place $H_0$\nat the top of the plausible range, corresponding to a redshift-selected sample.\nAfter a detailed analytic and mock-based study of these effects, we apply them\nto the CosmicFlows-4 sample, where introducing the distance prior causes an\napproximately 12~per cent increase in distances and $>8$ km/s/Mpc (55\nstatistical $\\sigma$) decrease in the Hubble constant for the case of volume or\nmagnitude selection. Redshift selection would fully undo this shift and is the\nmore likely scenario, as a phenomenological model shows. We also investigate\nthe SH0ES sample, where the volume-prior effect is modest ($1.6\\sigma$) and is\nlikely already accounted for within the SH0ES pipeline. Our work highlights the\ncrucial need to model both the distance prior and selection accurately for\nrobust distance ladders and derived parameters. The latter requires samples\nwith known, homogeneous selection criteria, which should be prioritised in\nfuture surveys.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Statistical methodology is rarely considered significant in distance ladder\nstudies or a potential contributor to the Hubble tension. We suggest it should\nbe, highlighting two appreciable issues. First, astronomical distances are\ninferred latent parameters, requiring a prior. We show that the common\nassumption of (perhaps implicit) uniform priors on distance moduli biases\ndistances low due to objects being intrinsically uniformly distributed in\nvolume. At fixed measured redshifts, this biases the Hubble constant high.\nSecond, selection effects introduce additional factors in the posterior. These\ntypically counteract the effect of the volume prior to some extent, but depend\nsignificantly on the nature of the selection. Typical assumptions place $H_0$\nat the top of the plausible range, corresponding to a redshift-selected sample.\nAfter a detailed analytic and mock-based study of these effects, we apply them\nto the CosmicFlows-4 sample, where introducing the distance prior causes an\napproximately 12~per cent increase in distances and $>8$ km/s/Mpc (55\nstatistical $\\sigma$) decrease in the Hubble constant for the case of volume or\nmagnitude selection. Redshift selection would fully undo this shift and is the\nmore likely scenario, as a phenomenological model shows. We also investigate\nthe SH0ES sample, where the volume-prior effect is modest ($1.6\\sigma$) and is\nlikely already accounted for within the SH0ES pipeline. Our work highlights the\ncrucial need to model both the distance prior and selection accurately for\nrobust distance ladders and derived parameters. The latter requires samples\nwith known, homogeneous selection criteria, which should be prioritised in\nfuture surveys."
                },
                "authors": [
                    {
                        "name": "Harry Desmond"
                    },
                    {
                        "name": "Richard Stiskalek"
                    },
                    {
                        "name": "Jose Antonio Najera"
                    },
                    {
                        "name": "Indranil Banik"
                    }
                ],
                "author_detail": {
                    "name": "Indranil Banik"
                },
                "author": "Indranil Banik",
                "arxiv_comment": "14 pages, 5 figures; submitted to MNRAS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03394v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03394v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17612v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17612v2",
                "updated": "2025-11-05T11:42:56Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    11,
                    42,
                    56,
                    2,
                    309,
                    0
                ],
                "published": "2025-05-23T08:20:15Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    8,
                    20,
                    15,
                    4,
                    143,
                    0
                ],
                "title": "Distilling LLM Agent into Small Models with Retrieval and Code Tools",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distilling LLM Agent into Small Models with Retrieval and Code Tools"
                },
                "summary": "Large language models (LLMs) excel at complex reasoning tasks but remain\ncomputationally expensive, limiting their practical deployment. To address\nthis, recent works have focused on distilling reasoning capabilities into\nsmaller language models (sLMs) using chain-of-thought (CoT) traces from teacher\nLLMs. However, this approach struggles in scenarios requiring rare factual\nknowledge or precise computation, where sLMs often hallucinate due to limited\ncapability. In this work, we propose Agent Distillation, a framework for\ntransferring not only reasoning capability but full task-solving behavior from\nLLM-based agents into sLMs with retrieval and code tools. We improve agent\ndistillation along two complementary axes: (1) we introduce a prompting method\ncalled first-thought prefix to enhance the quality of teacher-generated\ntrajectories; and (2) we propose a self-consistent action generation for\nimproving test-time robustness of small agents. We evaluate our method on eight\nreasoning tasks across factual and mathematical domains, covering both\nin-domain and out-of-domain generalization. Our results show that sLMs as small\nas 0.5B, 1.5B, 3B parameters can achieve performance competitive with next-tier\nlarger 1.5B, 3B, 7B models fine-tuned using CoT distillation, demonstrating the\npotential of agent distillation for building practical, tool-using small\nagents. Our code is available at https://github.com/Nardien/agent-distillation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) excel at complex reasoning tasks but remain\ncomputationally expensive, limiting their practical deployment. To address\nthis, recent works have focused on distilling reasoning capabilities into\nsmaller language models (sLMs) using chain-of-thought (CoT) traces from teacher\nLLMs. However, this approach struggles in scenarios requiring rare factual\nknowledge or precise computation, where sLMs often hallucinate due to limited\ncapability. In this work, we propose Agent Distillation, a framework for\ntransferring not only reasoning capability but full task-solving behavior from\nLLM-based agents into sLMs with retrieval and code tools. We improve agent\ndistillation along two complementary axes: (1) we introduce a prompting method\ncalled first-thought prefix to enhance the quality of teacher-generated\ntrajectories; and (2) we propose a self-consistent action generation for\nimproving test-time robustness of small agents. We evaluate our method on eight\nreasoning tasks across factual and mathematical domains, covering both\nin-domain and out-of-domain generalization. Our results show that sLMs as small\nas 0.5B, 1.5B, 3B parameters can achieve performance competitive with next-tier\nlarger 1.5B, 3B, 7B models fine-tuned using CoT distillation, demonstrating the\npotential of agent distillation for building practical, tool-using small\nagents. Our code is available at https://github.com/Nardien/agent-distillation."
                },
                "authors": [
                    {
                        "name": "Minki Kang"
                    },
                    {
                        "name": "Jongwon Jeong"
                    },
                    {
                        "name": "Seanie Lee"
                    },
                    {
                        "name": "Jaewoong Cho"
                    },
                    {
                        "name": "Sung Ju Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Sung Ju Hwang"
                },
                "author": "Sung Ju Hwang",
                "arxiv_comment": "NeurIPS 2025 Spotlight",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17612v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17612v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13384v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13384v2",
                "updated": "2025-11-05T11:40:58Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    11,
                    40,
                    58,
                    2,
                    309,
                    0
                ],
                "published": "2025-05-19T17:28:03Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    17,
                    28,
                    3,
                    0,
                    139,
                    0
                ],
                "title": "An Empirical Bayes approach to ARX Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Empirical Bayes approach to ARX Estimation"
                },
                "summary": "Empirical Bayes inference is based on estimation of the parameters of an a\npriori distribution from the observed data. The estimation technique of the\nparameters of the prior, called hyperparameters, is based on the marginal\ndistribution obtained by integrating the joint density of the model with\nrespect to the prior. This is a key step which needs to be properly adapted to\nthe problem at hand. In this paper we study Empirical Bayes inference of linear\nautoregressive models with inputs (ARX models) for time series and compare the\nperformance of the marginal parametric estimator with that a full Empirical\nBayesian analysis based on the estimated prior. Such a comparison, can only\nmake sense for a (realistic) finite data length. In this setting, we propose a\nnew estimation technique of the hyperparameters by a sequential Bayes procedure\nwhich is essentially a backward Kalman filter. It turns out that for finite\ndata length the marginal Bayes tends to behave slightly better than the full\nEmpirical Bayesian parameter estimator and so also in the case of slowly\nvarying random parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Empirical Bayes inference is based on estimation of the parameters of an a\npriori distribution from the observed data. The estimation technique of the\nparameters of the prior, called hyperparameters, is based on the marginal\ndistribution obtained by integrating the joint density of the model with\nrespect to the prior. This is a key step which needs to be properly adapted to\nthe problem at hand. In this paper we study Empirical Bayes inference of linear\nautoregressive models with inputs (ARX models) for time series and compare the\nperformance of the marginal parametric estimator with that a full Empirical\nBayesian analysis based on the estimated prior. Such a comparison, can only\nmake sense for a (realistic) finite data length. In this setting, we propose a\nnew estimation technique of the hyperparameters by a sequential Bayes procedure\nwhich is essentially a backward Kalman filter. It turns out that for finite\ndata length the marginal Bayes tends to behave slightly better than the full\nEmpirical Bayesian parameter estimator and so also in the case of slowly\nvarying random parameters."
                },
                "authors": [
                    {
                        "name": "Timofei Leahu"
                    },
                    {
                        "name": "Giorgio Picci"
                    }
                ],
                "author_detail": {
                    "name": "Giorgio Picci"
                },
                "author": "Giorgio Picci",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13384v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13384v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19090v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19090v2",
                "updated": "2025-11-05T11:39:05Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    11,
                    39,
                    5,
                    2,
                    309,
                    0
                ],
                "published": "2025-10-21T21:33:54Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    21,
                    33,
                    54,
                    1,
                    294,
                    0
                ],
                "title": "Learning noisy tissue dynamics across time scales",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning noisy tissue dynamics across time scales"
                },
                "summary": "Tissue dynamics play a crucial role in biological processes ranging from\ninflammation to morphogenesis. However, these noisy multicellular dynamics are\nnotoriously hard to predict. Here, we introduce a biomimetic machine learning\nframework capable of inferring noisy multicellular dynamics directly from\nexperimental movies. This generative model combines graph neural networks,\nnormalizing flows and WaveNet algorithms to represent tissues as neural\nstochastic differential equations where cells are edges of an evolving graph.\nCell interactions are encoded in a dual signaling graph capable of handling\nsignaling cascades. The dual graph architecture of our neural networks reflects\nthe architecture of the underlying biological tissues, substantially reducing\nthe amount of data needed for training, compared to convolutional or\nfully-connected neural networks. Taking epithelial tissue experiments as a case\nstudy, we show that our model not only captures stochastic cell motion but also\npredicts the evolution of cell states in their division cycle. Finally, we\ndemonstrate that our method can accurately generate the experimental dynamics\nof developmental systems, such as the fly wing, and cell signaling processes\nmediated by stochastic ERK waves, paving the way for its use as a digital twin\nin bioengineering and clinical contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tissue dynamics play a crucial role in biological processes ranging from\ninflammation to morphogenesis. However, these noisy multicellular dynamics are\nnotoriously hard to predict. Here, we introduce a biomimetic machine learning\nframework capable of inferring noisy multicellular dynamics directly from\nexperimental movies. This generative model combines graph neural networks,\nnormalizing flows and WaveNet algorithms to represent tissues as neural\nstochastic differential equations where cells are edges of an evolving graph.\nCell interactions are encoded in a dual signaling graph capable of handling\nsignaling cascades. The dual graph architecture of our neural networks reflects\nthe architecture of the underlying biological tissues, substantially reducing\nthe amount of data needed for training, compared to convolutional or\nfully-connected neural networks. Taking epithelial tissue experiments as a case\nstudy, we show that our model not only captures stochastic cell motion but also\npredicts the evolution of cell states in their division cycle. Finally, we\ndemonstrate that our method can accurately generate the experimental dynamics\nof developmental systems, such as the fly wing, and cell signaling processes\nmediated by stochastic ERK waves, paving the way for its use as a digital twin\nin bioengineering and clinical contexts."
                },
                "authors": [
                    {
                        "name": "Ming Han"
                    },
                    {
                        "name": "John Devany"
                    },
                    {
                        "name": "Michel Fruchart"
                    },
                    {
                        "name": "Margaret L. Gardel"
                    },
                    {
                        "name": "Vincenzo Vitelli"
                    }
                ],
                "author_detail": {
                    "name": "Vincenzo Vitelli"
                },
                "author": "Vincenzo Vitelli",
                "arxiv_comment": "15 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19090v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19090v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.soft",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.soft",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.bio-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03379v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03379v1",
                "updated": "2025-11-05T11:35:00Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    11,
                    35,
                    0,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T11:35:00Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    11,
                    35,
                    0,
                    2,
                    309,
                    0
                ],
                "title": "A Digital Twin of Evaporative Thermo-Fluidic Process in Fixation Unit of\n  DoD Inkjet Printers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Digital Twin of Evaporative Thermo-Fluidic Process in Fixation Unit of\n  DoD Inkjet Printers"
                },
                "summary": "In inkjet printing, optimal paper moisture is crucial for print quality,\nachieved through hot-air impingement in the fixation unit. This paper presents\na modular digital twin of the fixation unit, modeling the thermo-fluidic drying\nprocess and monitoring its spatio-temporal performance. The novel approach\nformulates the digital twin as an infinite-dimensional state estimator that\ninfers fixation states from limited sensor data, while remaining robust to\ndisturbances. Modularity is achieved through a graph-theoretic model, where\neach node represents thermo-fluidic dynamics in different sections of the\nfixation unit. Evaporation is modeled as a nonlinear boundary effect coupled\nwith node dynamics via Linear Fractional Representation. Using the Partial\nIntegral Equation (PIE) framework, we develop a unified approach for stability,\ninput-output analysis, simulation, and rapid prototyping, validated with\noperational data from a commercial printer. An $\\mathcal{H}_{\\infty}$-optimal\nLuenberger state estimator is then synthesized to estimate thermal states from\navailable sensor data, enabling real-time monitoring of spatio-temporal thermal\neffects on paper sheets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In inkjet printing, optimal paper moisture is crucial for print quality,\nachieved through hot-air impingement in the fixation unit. This paper presents\na modular digital twin of the fixation unit, modeling the thermo-fluidic drying\nprocess and monitoring its spatio-temporal performance. The novel approach\nformulates the digital twin as an infinite-dimensional state estimator that\ninfers fixation states from limited sensor data, while remaining robust to\ndisturbances. Modularity is achieved through a graph-theoretic model, where\neach node represents thermo-fluidic dynamics in different sections of the\nfixation unit. Evaporation is modeled as a nonlinear boundary effect coupled\nwith node dynamics via Linear Fractional Representation. Using the Partial\nIntegral Equation (PIE) framework, we develop a unified approach for stability,\ninput-output analysis, simulation, and rapid prototyping, validated with\noperational data from a commercial printer. An $\\mathcal{H}_{\\infty}$-optimal\nLuenberger state estimator is then synthesized to estimate thermal states from\navailable sensor data, enabling real-time monitoring of spatio-temporal thermal\neffects on paper sheets."
                },
                "authors": [
                    {
                        "name": "Samarth Toolhally"
                    },
                    {
                        "name": "Joeri Roelofs"
                    },
                    {
                        "name": "Siep Weiland"
                    },
                    {
                        "name": "Amritam Das"
                    }
                ],
                "author_detail": {
                    "name": "Amritam Das"
                },
                "author": "Amritam Das",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03379v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03379v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.07491v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.07491v2",
                "updated": "2025-11-05T11:34:39Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    11,
                    34,
                    39,
                    2,
                    309,
                    0
                ],
                "published": "2025-06-09T07:10:58Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    7,
                    10,
                    58,
                    0,
                    160,
                    0
                ],
                "title": "SpatialLM: Training Large Language Models for Structured Indoor Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpatialLM: Training Large Language Models for Structured Indoor Modeling"
                },
                "summary": "SpatialLM is a large language model designed to process 3D point cloud data\nand generate structured 3D scene understanding outputs. These outputs include\narchitectural elements like walls, doors, windows, and oriented object boxes\nwith their semantic categories. Unlike previous methods which exploit\ntask-specific network designs, our model adheres to the standard multimodal LLM\narchitecture and is fine-tuned directly from open-source LLMs.\n  To train SpatialLM, we collect a large-scale, high-quality synthetic dataset\nconsisting of the point clouds of 12,328 indoor scenes (54,778 rooms) with\nground-truth 3D annotations, and conduct a careful study on various modeling\nand training decisions. On public benchmarks, our model gives state-of-the-art\nperformance in layout estimation and competitive results in 3D object\ndetection. With that, we show a feasible path for enhancing the spatial\nunderstanding capabilities of modern LLMs for applications in augmented\nreality, embodied robotics, and more.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpatialLM is a large language model designed to process 3D point cloud data\nand generate structured 3D scene understanding outputs. These outputs include\narchitectural elements like walls, doors, windows, and oriented object boxes\nwith their semantic categories. Unlike previous methods which exploit\ntask-specific network designs, our model adheres to the standard multimodal LLM\narchitecture and is fine-tuned directly from open-source LLMs.\n  To train SpatialLM, we collect a large-scale, high-quality synthetic dataset\nconsisting of the point clouds of 12,328 indoor scenes (54,778 rooms) with\nground-truth 3D annotations, and conduct a careful study on various modeling\nand training decisions. On public benchmarks, our model gives state-of-the-art\nperformance in layout estimation and competitive results in 3D object\ndetection. With that, we show a feasible path for enhancing the spatial\nunderstanding capabilities of modern LLMs for applications in augmented\nreality, embodied robotics, and more."
                },
                "authors": [
                    {
                        "name": "Yongsen Mao"
                    },
                    {
                        "name": "Junhao Zhong"
                    },
                    {
                        "name": "Chuan Fang"
                    },
                    {
                        "name": "Jia Zheng"
                    },
                    {
                        "name": "Rui Tang"
                    },
                    {
                        "name": "Hao Zhu"
                    },
                    {
                        "name": "Ping Tan"
                    },
                    {
                        "name": "Zihan Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Zihan Zhou"
                },
                "author": "Zihan Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.07491v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.07491v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03376v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03376v1",
                "updated": "2025-11-05T11:31:08Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    11,
                    31,
                    8,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T11:31:08Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    11,
                    31,
                    8,
                    2,
                    309,
                    0
                ],
                "title": "Computational Imaging Meets LLMs: Zero-Shot IDH Mutation Prediction in\n  Brain Gliomas",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computational Imaging Meets LLMs: Zero-Shot IDH Mutation Prediction in\n  Brain Gliomas"
                },
                "summary": "We present a framework that combines Large Language Models with computational\nimage analytics for non-invasive, zero-shot prediction of IDH mutation status\nin brain gliomas. For each subject, coregistered multi-parametric MRI scans and\nmulti-class tumor segmentation maps were processed to extract interpretable\nsemantic (visual) attributes and quantitative features, serialized in a\nstandardized JSON file, and used to query GPT 4o and GPT 5 without fine-tuning.\nWe evaluated this framework on six publicly available datasets (N = 1427) and\nresults showcased high accuracy and balanced classification performance across\nheterogeneous cohorts, even in the absence of manual annotations. GPT 5\noutperformed GPT 4o in context-driven phenotype interpretation. Volumetric\nfeatures emerged as the most important predictors, supplemented by\nsubtype-specific imaging markers and clinical information. Our results\ndemonstrate the potential of integrating LLM-based reasoning with computational\nimage analytics for precise, non-invasive tumor genotyping, advancing\ndiagnostic strategies in neuro-oncology. The code is available at\nhttps://github.com/ATPLab-LUMS/CIM-LLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a framework that combines Large Language Models with computational\nimage analytics for non-invasive, zero-shot prediction of IDH mutation status\nin brain gliomas. For each subject, coregistered multi-parametric MRI scans and\nmulti-class tumor segmentation maps were processed to extract interpretable\nsemantic (visual) attributes and quantitative features, serialized in a\nstandardized JSON file, and used to query GPT 4o and GPT 5 without fine-tuning.\nWe evaluated this framework on six publicly available datasets (N = 1427) and\nresults showcased high accuracy and balanced classification performance across\nheterogeneous cohorts, even in the absence of manual annotations. GPT 5\noutperformed GPT 4o in context-driven phenotype interpretation. Volumetric\nfeatures emerged as the most important predictors, supplemented by\nsubtype-specific imaging markers and clinical information. Our results\ndemonstrate the potential of integrating LLM-based reasoning with computational\nimage analytics for precise, non-invasive tumor genotyping, advancing\ndiagnostic strategies in neuro-oncology. The code is available at\nhttps://github.com/ATPLab-LUMS/CIM-LLM."
                },
                "authors": [
                    {
                        "name": "Syed Muqeem Mahmood"
                    },
                    {
                        "name": "Hassan Mohy-ud-Din"
                    }
                ],
                "author_detail": {
                    "name": "Hassan Mohy-ud-Din"
                },
                "author": "Hassan Mohy-ud-Din",
                "arxiv_comment": "5 pages, 1 figure, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03376v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03376v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13994v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13994v2",
                "updated": "2025-11-05T11:26:59Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    11,
                    26,
                    59,
                    2,
                    309,
                    0
                ],
                "published": "2025-05-20T06:44:34Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    6,
                    44,
                    34,
                    1,
                    140,
                    0
                ],
                "title": "Divide by Question, Conquer by Agent: SPLIT-RAG with Question-Driven\n  Graph Partitioning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Divide by Question, Conquer by Agent: SPLIT-RAG with Question-Driven\n  Graph Partitioning"
                },
                "summary": "Retrieval-Augmented Generation (RAG) systems empower large language models\n(LLMs) with external knowledge, yet struggle with efficiency-accuracy\ntrade-offs when scaling to large knowledge graphs. Existing approaches often\nrely on monolithic graph retrieval, incurring unnecessary latency for simple\nqueries and fragmented reasoning for complex multi-hop questions. To address\nthese challenges, this paper propose SPLIT-RAG, a multi-agent RAG framework\nthat addresses these limitations with question-driven semantic graph\npartitioning and collaborative subgraph retrieval. The innovative framework\nfirst create Semantic Partitioning of Linked Information, then use the\nType-Specialized knowledge base to achieve Multi-Agent RAG. The attribute-aware\ngraph segmentation manages to divide knowledge graphs into semantically\ncoherent subgraphs, ensuring subgraphs align with different query types, while\nlightweight LLM agents are assigned to partitioned subgraphs, and only relevant\npartitions are activated during retrieval, thus reduce search space while\nenhancing efficiency. Finally, a hierarchical merging module resolves\ninconsistencies across subgraph-derived answers through logical verifications.\nExtensive experimental validation demonstrates considerable improvements\ncompared to existing approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) systems empower large language models\n(LLMs) with external knowledge, yet struggle with efficiency-accuracy\ntrade-offs when scaling to large knowledge graphs. Existing approaches often\nrely on monolithic graph retrieval, incurring unnecessary latency for simple\nqueries and fragmented reasoning for complex multi-hop questions. To address\nthese challenges, this paper propose SPLIT-RAG, a multi-agent RAG framework\nthat addresses these limitations with question-driven semantic graph\npartitioning and collaborative subgraph retrieval. The innovative framework\nfirst create Semantic Partitioning of Linked Information, then use the\nType-Specialized knowledge base to achieve Multi-Agent RAG. The attribute-aware\ngraph segmentation manages to divide knowledge graphs into semantically\ncoherent subgraphs, ensuring subgraphs align with different query types, while\nlightweight LLM agents are assigned to partitioned subgraphs, and only relevant\npartitions are activated during retrieval, thus reduce search space while\nenhancing efficiency. Finally, a hierarchical merging module resolves\ninconsistencies across subgraph-derived answers through logical verifications.\nExtensive experimental validation demonstrates considerable improvements\ncompared to existing approaches."
                },
                "authors": [
                    {
                        "name": "Ruiyi Yang"
                    },
                    {
                        "name": "Hao Xue"
                    },
                    {
                        "name": "Imran Razzak"
                    },
                    {
                        "name": "Shirui Pan"
                    },
                    {
                        "name": "Hakim Hacid"
                    },
                    {
                        "name": "Flora D. Salim"
                    }
                ],
                "author_detail": {
                    "name": "Flora D. Salim"
                },
                "author": "Flora D. Salim",
                "arxiv_comment": "20 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13994v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13994v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03372v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03372v1",
                "updated": "2025-11-05T11:26:38Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    11,
                    26,
                    38,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T11:26:38Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    11,
                    26,
                    38,
                    2,
                    309,
                    0
                ],
                "title": "LFC-DA: Logical Formula-Controlled Data Augmentation for Enhanced\n  Logical Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LFC-DA: Logical Formula-Controlled Data Augmentation for Enhanced\n  Logical Reasoning"
                },
                "summary": "For complex logical data augmentation, heavy reliance on human annotation is\ncostly, whereas direct generation with large language models yields\nuninterpretable and logically homogeneous examples. To address this, we present\nLFC-DA, a symbolic-logic-controlled pipeline: logical text is first mapped to\npropositional expressions, a compact rule library is compiled, and a bounded\nstate-space search systematically discovers valid formulas that are then\nverbalized back into natural-language questions, ensuring both diversity and\nlogical rigor under propositional logic. Experiments on ReClor and LogiQA show\nsignificant improvements in the logical-reasoning accuracy of pretrained\nmodels, confirming the effectiveness of LFC-DA for LLM-guided logical data\naugmentation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "For complex logical data augmentation, heavy reliance on human annotation is\ncostly, whereas direct generation with large language models yields\nuninterpretable and logically homogeneous examples. To address this, we present\nLFC-DA, a symbolic-logic-controlled pipeline: logical text is first mapped to\npropositional expressions, a compact rule library is compiled, and a bounded\nstate-space search systematically discovers valid formulas that are then\nverbalized back into natural-language questions, ensuring both diversity and\nlogical rigor under propositional logic. Experiments on ReClor and LogiQA show\nsignificant improvements in the logical-reasoning accuracy of pretrained\nmodels, confirming the effectiveness of LFC-DA for LLM-guided logical data\naugmentation."
                },
                "authors": [
                    {
                        "name": "Shenghao Li"
                    }
                ],
                "author_detail": {
                    "name": "Shenghao Li"
                },
                "author": "Shenghao Li",
                "arxiv_comment": "10 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03372v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03372v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.6; F.4.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00816v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00816v3",
                "updated": "2025-11-05T11:26:10Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    11,
                    26,
                    10,
                    2,
                    309,
                    0
                ],
                "published": "2025-02-02T14:52:50Z",
                "published_parsed": [
                    2025,
                    2,
                    2,
                    14,
                    52,
                    50,
                    6,
                    33,
                    0
                ],
                "title": "Sundial: A Family of Highly Capable Time Series Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sundial: A Family of Highly Capable Time Series Foundation Models"
                },
                "summary": "We introduce Sundial, a family of native, flexible, and scalable time series\nfoundation models. To predict the next-patch's distribution, we propose a\nTimeFlow Loss based on flow-matching, which facilitates native pre-training of\nTransformers on continuous-valued time series without discrete tokenization.\nConditioned on arbitrary-length time series, our models are pre-trained without\nspecifying any prior distribution and can generate multiple probable\npredictions, achieving more flexibility in representation learning than using\nparametric densities. Towards time series foundation models, we leverage\nminimal but crucial adaptations of Transformers and curate TimeBench with one\ntrillion time points, comprising mostly real-world datasets and synthetic data.\nBy mitigating mode collapse via TimeFlow Loss, we pre-train a family of Sundial\nmodels on TimeBench, which achieve unprecedented model capacity and\ngeneralization performance. In addition to excellent scalability, Sundial\nachieves state-of-the-art results on both point and probabilistic forecasting\nbenchmarks with a just-in-time inference speed, i.e., making zero-shot\npredictions within a few milliseconds. We believe that Sundial's pioneering\ngenerative forecasting capability can improve model reliability in real-world\ndecision-making. Code is available at: https://github.com/thuml/Sundial.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Sundial, a family of native, flexible, and scalable time series\nfoundation models. To predict the next-patch's distribution, we propose a\nTimeFlow Loss based on flow-matching, which facilitates native pre-training of\nTransformers on continuous-valued time series without discrete tokenization.\nConditioned on arbitrary-length time series, our models are pre-trained without\nspecifying any prior distribution and can generate multiple probable\npredictions, achieving more flexibility in representation learning than using\nparametric densities. Towards time series foundation models, we leverage\nminimal but crucial adaptations of Transformers and curate TimeBench with one\ntrillion time points, comprising mostly real-world datasets and synthetic data.\nBy mitigating mode collapse via TimeFlow Loss, we pre-train a family of Sundial\nmodels on TimeBench, which achieve unprecedented model capacity and\ngeneralization performance. In addition to excellent scalability, Sundial\nachieves state-of-the-art results on both point and probabilistic forecasting\nbenchmarks with a just-in-time inference speed, i.e., making zero-shot\npredictions within a few milliseconds. We believe that Sundial's pioneering\ngenerative forecasting capability can improve model reliability in real-world\ndecision-making. Code is available at: https://github.com/thuml/Sundial."
                },
                "authors": [
                    {
                        "name": "Yong Liu"
                    },
                    {
                        "name": "Guo Qin"
                    },
                    {
                        "name": "Zhiyuan Shi"
                    },
                    {
                        "name": "Zhi Chen"
                    },
                    {
                        "name": "Caiyin Yang"
                    },
                    {
                        "name": "Xiangdong Huang"
                    },
                    {
                        "name": "Jianmin Wang"
                    },
                    {
                        "name": "Mingsheng Long"
                    }
                ],
                "author_detail": {
                    "name": "Mingsheng Long"
                },
                "author": "Mingsheng Long",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00816v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00816v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23700v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23700v2",
                "updated": "2025-11-05T11:26:02Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    11,
                    26,
                    2,
                    2,
                    309,
                    0
                ],
                "published": "2025-05-29T17:37:47Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    37,
                    47,
                    3,
                    149,
                    0
                ],
                "title": "DiCoFlex: Model-agnostic diverse counterfactuals with flexible control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiCoFlex: Model-agnostic diverse counterfactuals with flexible control"
                },
                "summary": "Counterfactual explanations play a pivotal role in explainable artificial\nintelligence (XAI) by offering intuitive, human-understandable alternatives\nthat elucidate machine learning model decisions. Despite their significance,\nexisting methods for generating counterfactuals often require constant access\nto the predictive model, involve computationally intensive optimization for\neach instance and lack the flexibility to adapt to new user-defined constraints\nwithout retraining. In this paper, we propose DiCoFlex, a novel model-agnostic,\nconditional generative framework that produces multiple diverse counterfactuals\nin a single forward pass. Leveraging conditional normalizing flows trained\nsolely on labeled data, DiCoFlex addresses key limitations by enabling\nreal-time user-driven customization of constraints such as sparsity and\nactionability at inference time. Extensive experiments on standard benchmark\ndatasets show that DiCoFlex outperforms existing methods in terms of validity,\ndiversity, proximity, and constraint adherence, making it a practical and\nscalable solution for counterfactual generation in sensitive decision-making\ndomains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Counterfactual explanations play a pivotal role in explainable artificial\nintelligence (XAI) by offering intuitive, human-understandable alternatives\nthat elucidate machine learning model decisions. Despite their significance,\nexisting methods for generating counterfactuals often require constant access\nto the predictive model, involve computationally intensive optimization for\neach instance and lack the flexibility to adapt to new user-defined constraints\nwithout retraining. In this paper, we propose DiCoFlex, a novel model-agnostic,\nconditional generative framework that produces multiple diverse counterfactuals\nin a single forward pass. Leveraging conditional normalizing flows trained\nsolely on labeled data, DiCoFlex addresses key limitations by enabling\nreal-time user-driven customization of constraints such as sparsity and\nactionability at inference time. Extensive experiments on standard benchmark\ndatasets show that DiCoFlex outperforms existing methods in terms of validity,\ndiversity, proximity, and constraint adherence, making it a practical and\nscalable solution for counterfactual generation in sensitive decision-making\ndomains."
                },
                "authors": [
                    {
                        "name": "Oleksii Furman"
                    },
                    {
                        "name": "Ulvi Movsum-zada"
                    },
                    {
                        "name": "Patryk Marszalek"
                    },
                    {
                        "name": "Maciej Ziba"
                    },
                    {
                        "name": "Marek mieja"
                    }
                ],
                "author_detail": {
                    "name": "Marek mieja"
                },
                "author": "Marek mieja",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23700v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23700v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03370v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03370v1",
                "updated": "2025-11-05T11:25:07Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    11,
                    25,
                    7,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T11:25:07Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    11,
                    25,
                    7,
                    2,
                    309,
                    0
                ],
                "title": "EQ-Negotiator: Dynamic Emotional Personas Empower Small Language Models\n  for Edge-Deployable Credit Negotiation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EQ-Negotiator: Dynamic Emotional Personas Empower Small Language Models\n  for Edge-Deployable Credit Negotiation"
                },
                "summary": "The deployment of large language models (LLMs) in automated negotiation has\nset a high performance benchmark, but their computational cost and data privacy\nrequirements render them unsuitable for many privacy-sensitive, on-device\napplications such as mobile assistants, embodied AI agents or private client\ninteractions. While small language models (SLMs) offer a practical alternative,\nthey suffer from a significant performance gap compared to LLMs in playing\nemotionally charged complex personas, especially for credit negotiation. This\npaper introduces EQ-Negotiator, a novel framework that bridges this capability\ngap using emotional personas. Its core is a reasoning system that integrates\ngame theory with a Hidden Markov Model(HMM) to learn and track debtor emotional\nstates online, without pre-training. This allows EQ-Negotiator to equip SLMs\nwith the strategic intelligence to counter manipulation while de-escalating\nconflict and upholding ethical standards. Through extensive agent-to-agent\nsimulations across diverse credit negotiation scenarios, including adversarial\ndebtor strategies like cheating, threatening, and playing the victim, we show\nthat a 7B parameter language model with EQ-Negotiator achieves better debt\nrecovery and negotiation efficiency than baseline LLMs more than 10 times its\nsize. This work advances persona modeling from descriptive character profiles\nto dynamic emotional architectures that operate within privacy constraints.\nBesides, this paper establishes that strategic emotional intelligence, not raw\nmodel scale, is the critical factor for success in automated negotiation,\npaving the way for effective, ethical, and privacy-preserving AI negotiators\nthat can operate on the edge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of large language models (LLMs) in automated negotiation has\nset a high performance benchmark, but their computational cost and data privacy\nrequirements render them unsuitable for many privacy-sensitive, on-device\napplications such as mobile assistants, embodied AI agents or private client\ninteractions. While small language models (SLMs) offer a practical alternative,\nthey suffer from a significant performance gap compared to LLMs in playing\nemotionally charged complex personas, especially for credit negotiation. This\npaper introduces EQ-Negotiator, a novel framework that bridges this capability\ngap using emotional personas. Its core is a reasoning system that integrates\ngame theory with a Hidden Markov Model(HMM) to learn and track debtor emotional\nstates online, without pre-training. This allows EQ-Negotiator to equip SLMs\nwith the strategic intelligence to counter manipulation while de-escalating\nconflict and upholding ethical standards. Through extensive agent-to-agent\nsimulations across diverse credit negotiation scenarios, including adversarial\ndebtor strategies like cheating, threatening, and playing the victim, we show\nthat a 7B parameter language model with EQ-Negotiator achieves better debt\nrecovery and negotiation efficiency than baseline LLMs more than 10 times its\nsize. This work advances persona modeling from descriptive character profiles\nto dynamic emotional architectures that operate within privacy constraints.\nBesides, this paper establishes that strategic emotional intelligence, not raw\nmodel scale, is the critical factor for success in automated negotiation,\npaving the way for effective, ethical, and privacy-preserving AI negotiators\nthat can operate on the edge."
                },
                "authors": [
                    {
                        "name": "Yunbo Long"
                    },
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Alexandra Brintrup"
                    }
                ],
                "author_detail": {
                    "name": "Alexandra Brintrup"
                },
                "author": "Alexandra Brintrup",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03370v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03370v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14234v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14234v4",
                "updated": "2025-11-05T11:24:50Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    11,
                    24,
                    50,
                    2,
                    309,
                    0
                ],
                "published": "2025-03-18T13:11:43Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    13,
                    11,
                    43,
                    1,
                    77,
                    0
                ],
                "title": "Beyond Single Pass, Looping Through Time: KG-IRAG with Iterative\n  Knowledge Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Single Pass, Looping Through Time: KG-IRAG with Iterative\n  Knowledge Retrieval"
                },
                "summary": "Graph Retrieval-Augmented Generation (GraphRAG) has proven highly effective\nin enhancing the performance of Large Language Models (LLMs) on tasks that\nrequire external knowledge. By leveraging Knowledge Graphs (KGs), GraphRAG\nimproves information retrieval for complex reasoning tasks, providing more\nprecise and comprehensive retrieval and generating more accurate responses to\nQAs. However, most RAG methods fall short in addressing multi-step reasoning,\nparticularly when both information extraction and inference are necessary. To\naddress this limitation, this paper presents Knowledge Graph-Based Iterative\nRetrieval-Augmented Generation (KG-IRAG), a novel framework that integrates KGs\nwith iterative reasoning to improve LLMs' ability to handle queries involving\ntemporal and logical dependencies. Through iterative retrieval steps, KG-IRAG\nincrementally gathers relevant data from external KGs, enabling step-by-step\nreasoning. The proposed approach is particularly suited for scenarios where\nreasoning is required alongside dynamic temporal data extraction, such as\ndetermining optimal travel times based on weather conditions or traffic\npatterns. Experimental results show that KG-IRAG improves accuracy in complex\nreasoning tasks by effectively integrating external knowledge with iterative,\nlogic-based retrieval. Additionally, three new datasets: weatherQA-Irish,\nweatherQA-Sydney, and trafficQA-TFNSW, are formed to evaluate KG-IRAG's\nperformance, demonstrating its potential beyond traditional RAG applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Retrieval-Augmented Generation (GraphRAG) has proven highly effective\nin enhancing the performance of Large Language Models (LLMs) on tasks that\nrequire external knowledge. By leveraging Knowledge Graphs (KGs), GraphRAG\nimproves information retrieval for complex reasoning tasks, providing more\nprecise and comprehensive retrieval and generating more accurate responses to\nQAs. However, most RAG methods fall short in addressing multi-step reasoning,\nparticularly when both information extraction and inference are necessary. To\naddress this limitation, this paper presents Knowledge Graph-Based Iterative\nRetrieval-Augmented Generation (KG-IRAG), a novel framework that integrates KGs\nwith iterative reasoning to improve LLMs' ability to handle queries involving\ntemporal and logical dependencies. Through iterative retrieval steps, KG-IRAG\nincrementally gathers relevant data from external KGs, enabling step-by-step\nreasoning. The proposed approach is particularly suited for scenarios where\nreasoning is required alongside dynamic temporal data extraction, such as\ndetermining optimal travel times based on weather conditions or traffic\npatterns. Experimental results show that KG-IRAG improves accuracy in complex\nreasoning tasks by effectively integrating external knowledge with iterative,\nlogic-based retrieval. Additionally, three new datasets: weatherQA-Irish,\nweatherQA-Sydney, and trafficQA-TFNSW, are formed to evaluate KG-IRAG's\nperformance, demonstrating its potential beyond traditional RAG applications."
                },
                "authors": [
                    {
                        "name": "Ruiyi Yang"
                    },
                    {
                        "name": "Hao Xue"
                    },
                    {
                        "name": "Imran Razzak"
                    },
                    {
                        "name": "Hakim Hacid"
                    },
                    {
                        "name": "Flora D. Salim"
                    }
                ],
                "author_detail": {
                    "name": "Flora D. Salim"
                },
                "author": "Flora D. Salim",
                "arxiv_comment": "15 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14234v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14234v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2511.03724v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03724v1",
                "updated": "2025-11-05T18:58:18Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    18,
                    58,
                    18,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T18:58:18Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    18,
                    58,
                    18,
                    2,
                    309,
                    0
                ],
                "title": "Outbidding and Outbluffing Elite Humans: Mastering Liar's Poker via\n  Self-Play and Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Outbidding and Outbluffing Elite Humans: Mastering Liar's Poker via\n  Self-Play and Reinforcement Learning"
                },
                "summary": "AI researchers have long focused on poker-like games as a testbed for\nenvironments characterized by multi-player dynamics, imperfect information, and\nreasoning under uncertainty. While recent breakthroughs have matched elite\nhuman play at no-limit Texas hold'em, the multi-player dynamics are subdued:\nmost hands converge quickly with only two players engaged through multiple\nrounds of bidding. In this paper, we present Solly, the first AI agent to\nachieve elite human play in reduced-format Liar's Poker, a game characterized\nby extensive multi-player engagement. We trained Solly using self-play with a\nmodel-free, actor-critic, deep reinforcement learning algorithm. Solly played\nat an elite human level as measured by win rate (won over 50% of hands) and\nequity (money won) in heads-up and multi-player Liar's Poker. Solly also\noutperformed large language models (LLMs), including those with reasoning\nabilities, on the same metrics. Solly developed novel bidding strategies,\nrandomized play effectively, and was not easily exploitable by world-class\nhuman players.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI researchers have long focused on poker-like games as a testbed for\nenvironments characterized by multi-player dynamics, imperfect information, and\nreasoning under uncertainty. While recent breakthroughs have matched elite\nhuman play at no-limit Texas hold'em, the multi-player dynamics are subdued:\nmost hands converge quickly with only two players engaged through multiple\nrounds of bidding. In this paper, we present Solly, the first AI agent to\nachieve elite human play in reduced-format Liar's Poker, a game characterized\nby extensive multi-player engagement. We trained Solly using self-play with a\nmodel-free, actor-critic, deep reinforcement learning algorithm. Solly played\nat an elite human level as measured by win rate (won over 50% of hands) and\nequity (money won) in heads-up and multi-player Liar's Poker. Solly also\noutperformed large language models (LLMs), including those with reasoning\nabilities, on the same metrics. Solly developed novel bidding strategies,\nrandomized play effectively, and was not easily exploitable by world-class\nhuman players."
                },
                "authors": [
                    {
                        "name": "Richard Dewey"
                    },
                    {
                        "name": "Janos Botyanszki"
                    },
                    {
                        "name": "Ciamac C. Moallemi"
                    },
                    {
                        "name": "Andrew T. Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Andrew T. Zheng"
                },
                "author": "Andrew T. Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03724v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03724v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03718v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03718v1",
                "updated": "2025-11-05T18:52:28Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    18,
                    52,
                    28,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T18:52:28Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    18,
                    52,
                    28,
                    2,
                    309,
                    0
                ],
                "title": "Grounded Misunderstandings in Asymmetric Dialogue: A Perspectivist\n  Annotation Scheme for MapTask",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Grounded Misunderstandings in Asymmetric Dialogue: A Perspectivist\n  Annotation Scheme for MapTask"
                },
                "summary": "Collaborative dialogue relies on participants incrementally establishing\ncommon ground, yet in asymmetric settings they may believe they agree while\nreferring to different entities. We introduce a perspectivist annotation scheme\nfor the HCRC MapTask corpus (Anderson et al., 1991) that separately captures\nspeaker and addressee grounded interpretations for each reference expression,\nenabling us to trace how understanding emerges, diverges, and repairs over\ntime. Using a scheme-constrained LLM annotation pipeline, we obtain 13k\nannotated reference expressions with reliability estimates and analyze the\nresulting understanding states. The results show that full misunderstandings\nare rare once lexical variants are unified, but multiplicity discrepancies\nsystematically induce divergences, revealing how apparent grounding can mask\nreferential misalignment. Our framework provides both a resource and an\nanalytic lens for studying grounded misunderstanding and for evaluating\n(V)LLMs' capacity to model perspective-dependent grounding in collaborative\ndialogue.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collaborative dialogue relies on participants incrementally establishing\ncommon ground, yet in asymmetric settings they may believe they agree while\nreferring to different entities. We introduce a perspectivist annotation scheme\nfor the HCRC MapTask corpus (Anderson et al., 1991) that separately captures\nspeaker and addressee grounded interpretations for each reference expression,\nenabling us to trace how understanding emerges, diverges, and repairs over\ntime. Using a scheme-constrained LLM annotation pipeline, we obtain 13k\nannotated reference expressions with reliability estimates and analyze the\nresulting understanding states. The results show that full misunderstandings\nare rare once lexical variants are unified, but multiplicity discrepancies\nsystematically induce divergences, revealing how apparent grounding can mask\nreferential misalignment. Our framework provides both a resource and an\nanalytic lens for studying grounded misunderstanding and for evaluating\n(V)LLMs' capacity to model perspective-dependent grounding in collaborative\ndialogue."
                },
                "authors": [
                    {
                        "name": "Nan Li"
                    },
                    {
                        "name": "Albert Gatt"
                    },
                    {
                        "name": "Massimo Poesio"
                    }
                ],
                "author_detail": {
                    "name": "Massimo Poesio"
                },
                "author": "Massimo Poesio",
                "arxiv_comment": "11 pages, 3 figures, 5 tables; under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03718v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03718v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20637v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20637v2",
                "updated": "2025-11-05T18:39:38Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    18,
                    39,
                    38,
                    2,
                    309,
                    0
                ],
                "published": "2025-08-28T10:35:44Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    10,
                    35,
                    44,
                    3,
                    240,
                    0
                ],
                "title": "GDS Agent for Graph Algorithmic Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GDS Agent for Graph Algorithmic Reasoning"
                },
                "summary": "Large language models (LLMs) have shown remarkable multimodal information\nprocessing and reasoning ability. When equipped with tools through function\ncalling and enhanced with retrieval-augmented techniques, compound LLM-based\nsystems can access closed data sources and answer questions about them.\nHowever, they still struggle to process and reason over large-scale\ngraph-structure data. We introduce the GDS (Graph Data Science) agent in this\ntechnical report. The GDS agent introduces a comprehensive set of graph\nalgorithms as tools, together with preprocessing (retrieval) and postprocessing\nof algorithm results, in a model context protocol (MCP) server. The server can\nbe used with any modern LLM out-of-the-box. GDS agent allows users to ask any\nquestion that implicitly and intrinsically requires graph algorithmic reasoning\nabout their data, and quickly obtain accurate and grounded answers. We\nintroduce new benchmarks that evaluate intermediate tool calls as well as final\nresponses. The results indicate that GDS agent is able to solve a wide spectrum\nof graph tasks. We also provide detailed case studies for more open-ended tasks\nand study scenarios where the agent struggles. Finally, we discuss the\nremaining challenges and the future roadmap.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable multimodal information\nprocessing and reasoning ability. When equipped with tools through function\ncalling and enhanced with retrieval-augmented techniques, compound LLM-based\nsystems can access closed data sources and answer questions about them.\nHowever, they still struggle to process and reason over large-scale\ngraph-structure data. We introduce the GDS (Graph Data Science) agent in this\ntechnical report. The GDS agent introduces a comprehensive set of graph\nalgorithms as tools, together with preprocessing (retrieval) and postprocessing\nof algorithm results, in a model context protocol (MCP) server. The server can\nbe used with any modern LLM out-of-the-box. GDS agent allows users to ask any\nquestion that implicitly and intrinsically requires graph algorithmic reasoning\nabout their data, and quickly obtain accurate and grounded answers. We\nintroduce new benchmarks that evaluate intermediate tool calls as well as final\nresponses. The results indicate that GDS agent is able to solve a wide spectrum\nof graph tasks. We also provide detailed case studies for more open-ended tasks\nand study scenarios where the agent struggles. Finally, we discuss the\nremaining challenges and the future roadmap."
                },
                "authors": [
                    {
                        "name": "Borun Shi"
                    },
                    {
                        "name": "Ioannis Panagiotas"
                    }
                ],
                "author_detail": {
                    "name": "Ioannis Panagiotas"
                },
                "author": "Ioannis Panagiotas",
                "arxiv_comment": "Technical report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20637v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20637v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03706v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03706v1",
                "updated": "2025-11-05T18:38:02Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    18,
                    38,
                    2,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T18:38:02Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    18,
                    38,
                    2,
                    2,
                    309,
                    0
                ],
                "title": "LLM-enhanced Air Quality Monitoring Interface via Model Context Protocol",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-enhanced Air Quality Monitoring Interface via Model Context Protocol"
                },
                "summary": "Air quality monitoring is central to environmental sustainability and public\nhealth, yet traditional systems remain difficult for non-expert users to\ninterpret due to complex visualizations, limited interactivity, and high\ndeployment costs. Recent advances in Large Language Models (LLMs) offer new\nopportunities to make sensor data more accessible, but their tendency to\nproduce hallucinations limits reliability in safety-critical domains. To\naddress these challenges, we present an LLM-enhanced Air Monitoring Interface\n(AMI) that integrates real-time sensor data with a conversational interface via\nthe Model Context Protocol (MCP). Our system grounds LLM outputs in live\nenvironmental data, enabling accurate, context-aware responses while reducing\nhallucination risk. The architecture combines a Django-based backend, a\nresponsive user dashboard, and a secure MCP server that exposes system\nfunctions as discoverable tools, allowing the LLM to act as an active operator\nrather than a passive responder. Expert evaluation demonstrated high factual\naccuracy (4.78), completeness (4.82), and minimal hallucinations (4.84), on a\nscale of 5, supported by inter-rater reliability analysis. These results\nhighlight the potential of combining LLMs with standardized tool protocols to\ncreate reliable, secure, and user-friendly interfaces for real-time\nenvironmental monitoring.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Air quality monitoring is central to environmental sustainability and public\nhealth, yet traditional systems remain difficult for non-expert users to\ninterpret due to complex visualizations, limited interactivity, and high\ndeployment costs. Recent advances in Large Language Models (LLMs) offer new\nopportunities to make sensor data more accessible, but their tendency to\nproduce hallucinations limits reliability in safety-critical domains. To\naddress these challenges, we present an LLM-enhanced Air Monitoring Interface\n(AMI) that integrates real-time sensor data with a conversational interface via\nthe Model Context Protocol (MCP). Our system grounds LLM outputs in live\nenvironmental data, enabling accurate, context-aware responses while reducing\nhallucination risk. The architecture combines a Django-based backend, a\nresponsive user dashboard, and a secure MCP server that exposes system\nfunctions as discoverable tools, allowing the LLM to act as an active operator\nrather than a passive responder. Expert evaluation demonstrated high factual\naccuracy (4.78), completeness (4.82), and minimal hallucinations (4.84), on a\nscale of 5, supported by inter-rater reliability analysis. These results\nhighlight the potential of combining LLMs with standardized tool protocols to\ncreate reliable, secure, and user-friendly interfaces for real-time\nenvironmental monitoring."
                },
                "authors": [
                    {
                        "name": "Yu-Erh Pan"
                    },
                    {
                        "name": "Ayesha Siddika Nipu"
                    }
                ],
                "author_detail": {
                    "name": "Ayesha Siddika Nipu"
                },
                "author": "Ayesha Siddika Nipu",
                "arxiv_comment": "International Symposium on Advanced Electrical and Communication\n  Technologies, ISAECT 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03706v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03706v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03699v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03699v1",
                "updated": "2025-11-05T18:28:28Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    18,
                    28,
                    28,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T18:28:28Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    18,
                    28,
                    28,
                    2,
                    309,
                    0
                ],
                "title": "Do Androids Dream of Unseen Puppeteers? Probing for a Conspiracy Mindset\n  in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Androids Dream of Unseen Puppeteers? Probing for a Conspiracy Mindset\n  in Large Language Models"
                },
                "summary": "In this paper, we investigate whether Large Language Models (LLMs) exhibit\nconspiratorial tendencies, whether they display sociodemographic biases in this\ndomain, and how easily they can be conditioned into adopting conspiratorial\nperspectives. Conspiracy beliefs play a central role in the spread of\nmisinformation and in shaping distrust toward institutions, making them a\ncritical testbed for evaluating the social fidelity of LLMs. LLMs are\nincreasingly used as proxies for studying human behavior, yet little is known\nabout whether they reproduce higher-order psychological constructs such as a\nconspiratorial mindset. To bridge this research gap, we administer validated\npsychometric surveys measuring conspiracy mindset to multiple models under\ndifferent prompting and conditioning strategies. Our findings reveal that LLMs\nshow partial agreement with elements of conspiracy belief, and conditioning\nwith socio-demographic attributes produces uneven effects, exposing latent\ndemographic biases. Moreover, targeted prompts can easily shift model responses\ntoward conspiratorial directions, underscoring both the susceptibility of LLMs\nto manipulation and the potential risks of their deployment in sensitive\ncontexts. These results highlight the importance of critically evaluating the\npsychological dimensions embedded in LLMs, both to advance computational social\nscience and to inform possible mitigation strategies against harmful uses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we investigate whether Large Language Models (LLMs) exhibit\nconspiratorial tendencies, whether they display sociodemographic biases in this\ndomain, and how easily they can be conditioned into adopting conspiratorial\nperspectives. Conspiracy beliefs play a central role in the spread of\nmisinformation and in shaping distrust toward institutions, making them a\ncritical testbed for evaluating the social fidelity of LLMs. LLMs are\nincreasingly used as proxies for studying human behavior, yet little is known\nabout whether they reproduce higher-order psychological constructs such as a\nconspiratorial mindset. To bridge this research gap, we administer validated\npsychometric surveys measuring conspiracy mindset to multiple models under\ndifferent prompting and conditioning strategies. Our findings reveal that LLMs\nshow partial agreement with elements of conspiracy belief, and conditioning\nwith socio-demographic attributes produces uneven effects, exposing latent\ndemographic biases. Moreover, targeted prompts can easily shift model responses\ntoward conspiratorial directions, underscoring both the susceptibility of LLMs\nto manipulation and the potential risks of their deployment in sensitive\ncontexts. These results highlight the importance of critically evaluating the\npsychological dimensions embedded in LLMs, both to advance computational social\nscience and to inform possible mitigation strategies against harmful uses."
                },
                "authors": [
                    {
                        "name": "Francesco Corso"
                    },
                    {
                        "name": "Francesco Pierri"
                    },
                    {
                        "name": "Gianmarco De Francisci Morales"
                    }
                ],
                "author_detail": {
                    "name": "Gianmarco De Francisci Morales"
                },
                "author": "Gianmarco De Francisci Morales",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03699v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03699v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03697v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03697v1",
                "updated": "2025-11-05T18:24:01Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    18,
                    24,
                    1,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T18:24:01Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    18,
                    24,
                    1,
                    2,
                    309,
                    0
                ],
                "title": "AnaFlow: Agentic LLM-based Workflow for Reasoning-Driven Explainable and\n  Sample-Efficient Analog Circuit Sizing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AnaFlow: Agentic LLM-based Workflow for Reasoning-Driven Explainable and\n  Sample-Efficient Analog Circuit Sizing"
                },
                "summary": "Analog/mixed-signal circuits are key for interfacing electronics with the\nphysical world. Their design, however, remains a largely handcrafted process,\nresulting in long and error-prone design cycles. While the recent rise of\nAI-based reinforcement learning and generative AI has created new techniques to\nautomate this task, the need for many time-consuming simulations is a critical\nbottleneck hindering the overall efficiency. Furthermore, the lack of\nexplainability of the resulting design solutions hampers widespread adoption of\nthe tools. To address these issues, a novel agentic AI framework for\nsample-efficient and explainable analog circuit sizing is presented. It employs\na multi-agent workflow where specialized Large Language Model (LLM)-based\nagents collaborate to interpret the circuit topology, to understand the design\ngoals, and to iteratively refine the circuit's design parameters towards the\ntarget goals with human-interpretable reasoning. The adaptive simulation\nstrategy creates an intelligent control that yields a high sample efficiency.\nThe AnaFlow framework is demonstrated for two circuits of varying complexity\nand is able to complete the sizing task fully automatically, differently from\npure Bayesian optimization and reinforcement learning approaches. The system\nlearns from its optimization history to avoid past mistakes and to accelerate\nconvergence. The inherent explainability makes this a powerful tool for analog\ndesign space exploration and a new paradigm in analog EDA, where AI agents\nserve as transparent design assistants.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analog/mixed-signal circuits are key for interfacing electronics with the\nphysical world. Their design, however, remains a largely handcrafted process,\nresulting in long and error-prone design cycles. While the recent rise of\nAI-based reinforcement learning and generative AI has created new techniques to\nautomate this task, the need for many time-consuming simulations is a critical\nbottleneck hindering the overall efficiency. Furthermore, the lack of\nexplainability of the resulting design solutions hampers widespread adoption of\nthe tools. To address these issues, a novel agentic AI framework for\nsample-efficient and explainable analog circuit sizing is presented. It employs\na multi-agent workflow where specialized Large Language Model (LLM)-based\nagents collaborate to interpret the circuit topology, to understand the design\ngoals, and to iteratively refine the circuit's design parameters towards the\ntarget goals with human-interpretable reasoning. The adaptive simulation\nstrategy creates an intelligent control that yields a high sample efficiency.\nThe AnaFlow framework is demonstrated for two circuits of varying complexity\nand is able to complete the sizing task fully automatically, differently from\npure Bayesian optimization and reinforcement learning approaches. The system\nlearns from its optimization history to avoid past mistakes and to accelerate\nconvergence. The inherent explainability makes this a powerful tool for analog\ndesign space exploration and a new paradigm in analog EDA, where AI agents\nserve as transparent design assistants."
                },
                "authors": [
                    {
                        "name": "Mohsen Ahmadzadeh"
                    },
                    {
                        "name": "Kaichang Chen"
                    },
                    {
                        "name": "Georges Gielen"
                    }
                ],
                "author_detail": {
                    "name": "Georges Gielen"
                },
                "author": "Georges Gielen",
                "arxiv_comment": "This article was accepted by 2025 International Conference on\n  Computer-Aided Design (ICCAD 2025) and was presented in Munich, October 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03697v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03697v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03695v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03695v1",
                "updated": "2025-11-05T18:20:23Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    18,
                    20,
                    23,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T18:20:23Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    18,
                    20,
                    23,
                    2,
                    309,
                    0
                ],
                "title": "Behavior-Adaptive Q-Learning: A Unifying Framework for Offline-to-Online\n  RL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Behavior-Adaptive Q-Learning: A Unifying Framework for Offline-to-Online\n  RL"
                },
                "summary": "Offline reinforcement learning (RL) enables training from fixed data without\nonline interaction, but policies learned offline often struggle when deployed\nin dynamic environments due to distributional shift and unreliable value\nestimates on unseen state-action pairs. We introduce Behavior-Adaptive\nQ-Learning (BAQ), a framework designed to enable a smooth and reliable\ntransition from offline to online RL. The key idea is to leverage an implicit\nbehavioral model derived from offline data to provide a behavior-consistency\nsignal during online fine-tuning. BAQ incorporates a dual-objective loss that\n(i) aligns the online policy toward the offline behavior when uncertainty is\nhigh, and (ii) gradually relaxes this constraint as more confident online\nexperience is accumulated. This adaptive mechanism reduces error propagation\nfrom out-of-distribution estimates, stabilizes early online updates, and\naccelerates adaptation to new scenarios. Across standard benchmarks, BAQ\nconsistently outperforms prior offline-to-online RL approaches, achieving\nfaster recovery, improved robustness, and higher overall performance. Our\nresults demonstrate that implicit behavior adaptation is a principled and\npractical solution for reliable real-world policy deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Offline reinforcement learning (RL) enables training from fixed data without\nonline interaction, but policies learned offline often struggle when deployed\nin dynamic environments due to distributional shift and unreliable value\nestimates on unseen state-action pairs. We introduce Behavior-Adaptive\nQ-Learning (BAQ), a framework designed to enable a smooth and reliable\ntransition from offline to online RL. The key idea is to leverage an implicit\nbehavioral model derived from offline data to provide a behavior-consistency\nsignal during online fine-tuning. BAQ incorporates a dual-objective loss that\n(i) aligns the online policy toward the offline behavior when uncertainty is\nhigh, and (ii) gradually relaxes this constraint as more confident online\nexperience is accumulated. This adaptive mechanism reduces error propagation\nfrom out-of-distribution estimates, stabilizes early online updates, and\naccelerates adaptation to new scenarios. Across standard benchmarks, BAQ\nconsistently outperforms prior offline-to-online RL approaches, achieving\nfaster recovery, improved robustness, and higher overall performance. Our\nresults demonstrate that implicit behavior adaptation is a principled and\npractical solution for reliable real-world policy deployment."
                },
                "authors": [
                    {
                        "name": "Lipeng Zu"
                    },
                    {
                        "name": "Hansong Zhou"
                    },
                    {
                        "name": "Xiaonan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaonan Zhang"
                },
                "author": "Xiaonan Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03695v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03695v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03693v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03693v1",
                "updated": "2025-11-05T18:18:09Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    18,
                    18,
                    9,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T18:18:09Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    18,
                    18,
                    9,
                    2,
                    309,
                    0
                ],
                "title": "Colorectal Cancer Histopathological Grading using Multi-Scale Federated\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Colorectal Cancer Histopathological Grading using Multi-Scale Federated\n  Learning"
                },
                "summary": "Colorectal cancer (CRC) grading is a critical prognostic factor but remains\nhampered by inter-observer variability and the privacy constraints of\nmulti-institutional data sharing. While deep learning offers a path to\nautomation, centralized training models conflict with data governance\nregulations and neglect the diagnostic importance of multi-scale analysis. In\nthis work, we propose a scalable, privacy-preserving federated learning (FL)\nframework for CRC histopathological grading that integrates multi-scale feature\nlearning within a distributed training paradigm. Our approach employs a\ndual-stream ResNetRS50 backbone to concurrently capture fine-grained nuclear\ndetail and broader tissue-level context. This architecture is integrated into a\nrobust FL system stabilized using FedProx to mitigate client drift across\nheterogeneous data distributions from multiple hospitals. Extensive evaluation\non the CRC-HGD dataset demonstrates that our framework achieves an overall\naccuracy of 83.5%, outperforming a comparable centralized model (81.6%).\nCrucially, the system excels in identifying the most aggressive Grade III\ntumors with a high recall of 87.5%, a key clinical priority to prevent\ndangerous false negatives. Performance further improves with higher\nmagnification, reaching 88.0% accuracy at 40x. These results validate that our\nfederated multi-scale approach not only preserves patient privacy but also\nenhances model performance and generalization. The proposed modular pipeline,\nwith built-in preprocessing, checkpointing, and error handling, establishes a\nfoundational step toward deployable, privacy-aware clinical AI for digital\npathology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Colorectal cancer (CRC) grading is a critical prognostic factor but remains\nhampered by inter-observer variability and the privacy constraints of\nmulti-institutional data sharing. While deep learning offers a path to\nautomation, centralized training models conflict with data governance\nregulations and neglect the diagnostic importance of multi-scale analysis. In\nthis work, we propose a scalable, privacy-preserving federated learning (FL)\nframework for CRC histopathological grading that integrates multi-scale feature\nlearning within a distributed training paradigm. Our approach employs a\ndual-stream ResNetRS50 backbone to concurrently capture fine-grained nuclear\ndetail and broader tissue-level context. This architecture is integrated into a\nrobust FL system stabilized using FedProx to mitigate client drift across\nheterogeneous data distributions from multiple hospitals. Extensive evaluation\non the CRC-HGD dataset demonstrates that our framework achieves an overall\naccuracy of 83.5%, outperforming a comparable centralized model (81.6%).\nCrucially, the system excels in identifying the most aggressive Grade III\ntumors with a high recall of 87.5%, a key clinical priority to prevent\ndangerous false negatives. Performance further improves with higher\nmagnification, reaching 88.0% accuracy at 40x. These results validate that our\nfederated multi-scale approach not only preserves patient privacy but also\nenhances model performance and generalization. The proposed modular pipeline,\nwith built-in preprocessing, checkpointing, and error handling, establishes a\nfoundational step toward deployable, privacy-aware clinical AI for digital\npathology."
                },
                "authors": [
                    {
                        "name": "Md Ahasanul Arafath"
                    },
                    {
                        "name": "Abhijit Kumar Ghosh"
                    },
                    {
                        "name": "Md Rony Ahmed"
                    },
                    {
                        "name": "Sabrin Afroz"
                    },
                    {
                        "name": "Minhazul Hosen"
                    },
                    {
                        "name": "Md Hasan Moon"
                    },
                    {
                        "name": "Md Tanzim Reza"
                    },
                    {
                        "name": "Md Ashad Alam"
                    }
                ],
                "author_detail": {
                    "name": "Md Ashad Alam"
                },
                "author": "Md Ashad Alam",
                "arxiv_comment": "15 pages and 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03693v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03693v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03690v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03690v1",
                "updated": "2025-11-05T18:16:44Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    18,
                    16,
                    44,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T18:16:44Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    18,
                    16,
                    44,
                    2,
                    309,
                    0
                ],
                "title": "The OpenHands Software Agent SDK: A Composable and Extensible Foundation\n  for Production Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The OpenHands Software Agent SDK: A Composable and Extensible Foundation\n  for Production Agents"
                },
                "summary": "Agents are now used widely in the process of software development, but\nbuilding production-ready software engineering agents is a complex task.\nDeploying software agents effectively requires flexibility in implementation\nand experimentation, reliable and secure execution, and interfaces for users to\ninteract with agents. In this paper, we present the OpenHands Software Agent\nSDK, a toolkit for implementing software development agents that satisfy these\ndesiderata. This toolkit is a complete architectural redesign of the agent\ncomponents of the popular OpenHands framework for software development agents,\nwhich has 64k+ GitHub stars. To achieve flexibility, we design a simple\ninterface for implementing agents that requires only a few lines of code in the\ndefault case, but is easily extensible to more complex, full-featured agents\nwith features such as custom tools, memory management, and more. For security\nand reliability, it delivers seamless local-to-remote execution portability,\nintegrated REST/WebSocket services. For interaction with human users, it can\nconnect directly to a variety of interfaces, such as visual workspaces (VS\nCode, VNC, browser), command-line interfaces, and APIs. Compared with existing\nSDKs from OpenAI, Claude, and Google, OpenHands uniquely integrates native\nsandboxed execution, lifecycle control, model-agnostic multi-LLM routing, and\nbuilt-in security analysis. Empirical results on SWE-Bench Verified and GAIA\nbenchmarks demonstrate strong performance. Put together, these elements allow\nthe OpenHands Software Agent SDK to provide a practical foundation for\nprototyping, unlocking new classes of custom applications, and reliably\ndeploying agents at scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agents are now used widely in the process of software development, but\nbuilding production-ready software engineering agents is a complex task.\nDeploying software agents effectively requires flexibility in implementation\nand experimentation, reliable and secure execution, and interfaces for users to\ninteract with agents. In this paper, we present the OpenHands Software Agent\nSDK, a toolkit for implementing software development agents that satisfy these\ndesiderata. This toolkit is a complete architectural redesign of the agent\ncomponents of the popular OpenHands framework for software development agents,\nwhich has 64k+ GitHub stars. To achieve flexibility, we design a simple\ninterface for implementing agents that requires only a few lines of code in the\ndefault case, but is easily extensible to more complex, full-featured agents\nwith features such as custom tools, memory management, and more. For security\nand reliability, it delivers seamless local-to-remote execution portability,\nintegrated REST/WebSocket services. For interaction with human users, it can\nconnect directly to a variety of interfaces, such as visual workspaces (VS\nCode, VNC, browser), command-line interfaces, and APIs. Compared with existing\nSDKs from OpenAI, Claude, and Google, OpenHands uniquely integrates native\nsandboxed execution, lifecycle control, model-agnostic multi-LLM routing, and\nbuilt-in security analysis. Empirical results on SWE-Bench Verified and GAIA\nbenchmarks demonstrate strong performance. Put together, these elements allow\nthe OpenHands Software Agent SDK to provide a practical foundation for\nprototyping, unlocking new classes of custom applications, and reliably\ndeploying agents at scale."
                },
                "authors": [
                    {
                        "name": "Xingyao Wang"
                    },
                    {
                        "name": "Simon Rosenberg"
                    },
                    {
                        "name": "Juan Michelini"
                    },
                    {
                        "name": "Calvin Smith"
                    },
                    {
                        "name": "Hoang Tran"
                    },
                    {
                        "name": "Engel Nyst"
                    },
                    {
                        "name": "Rohit Malhotra"
                    },
                    {
                        "name": "Xuhui Zhou"
                    },
                    {
                        "name": "Valerie Chen"
                    },
                    {
                        "name": "Robert Brennan"
                    },
                    {
                        "name": "Graham Neubig"
                    }
                ],
                "author_detail": {
                    "name": "Graham Neubig"
                },
                "author": "Graham Neubig",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03690v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03690v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.00807v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.00807v2",
                "updated": "2025-11-05T18:15:57Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    18,
                    15,
                    57,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-02T05:17:02Z",
                "published_parsed": [
                    2025,
                    11,
                    2,
                    5,
                    17,
                    2,
                    6,
                    306,
                    0
                ],
                "title": "FREESH: Fair, Resource- and Energy-Efficient Scheduling for LLM Serving\n  on Heterogeneous GPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FREESH: Fair, Resource- and Energy-Efficient Scheduling for LLM Serving\n  on Heterogeneous GPUs"
                },
                "summary": "The ever-increasing computation and energy demand for LLM and AI agents call\nfor holistic and efficient optimization of LLM serving systems. In practice,\nheterogeneous GPU clusters can be deployed in a geographically distributed\nmanner, while LLM load also observes diversity in terms of both query traffic\nand serving patterns. LLM queries running on advanced GPUs during a\nhigh-emission hour at one location can lead to significantly higher carbon\nfootprints versus same queries running on mid-level GPUs at a low-emission time\nand location. By observing LLM serving requirements and leveraging\nspatiotemporal computation flexibility, we consider the joint routing and\nscheduling problem, and propose FREESH to cooperatively run a group of data\ncenters while minimizing user-specified carbon or energy objectives. FREESH\nidentifies the optimal configurations of balanced load serving by matching\ndistinct GPU instance's power-throughput characteristics with predictable LLM\nquery length and workloads. To ensure both latency and fairness requirements,\nFREESH identifies optimized parallelism and query routing schedules together\nwith dynamic GPU frequency scaling for power saving, and Least-Laxity-First\n(LLF) serving strategy for query scheduling. During the 1-hour serving on\nproduction workloads, FREESH reduces energy by 28.6% and emissions by 45.45%\ntogether with improvements in SLO attainment and fairness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ever-increasing computation and energy demand for LLM and AI agents call\nfor holistic and efficient optimization of LLM serving systems. In practice,\nheterogeneous GPU clusters can be deployed in a geographically distributed\nmanner, while LLM load also observes diversity in terms of both query traffic\nand serving patterns. LLM queries running on advanced GPUs during a\nhigh-emission hour at one location can lead to significantly higher carbon\nfootprints versus same queries running on mid-level GPUs at a low-emission time\nand location. By observing LLM serving requirements and leveraging\nspatiotemporal computation flexibility, we consider the joint routing and\nscheduling problem, and propose FREESH to cooperatively run a group of data\ncenters while minimizing user-specified carbon or energy objectives. FREESH\nidentifies the optimal configurations of balanced load serving by matching\ndistinct GPU instance's power-throughput characteristics with predictable LLM\nquery length and workloads. To ensure both latency and fairness requirements,\nFREESH identifies optimized parallelism and query routing schedules together\nwith dynamic GPU frequency scaling for power saving, and Least-Laxity-First\n(LLF) serving strategy for query scheduling. During the 1-hour serving on\nproduction workloads, FREESH reduces energy by 28.6% and emissions by 45.45%\ntogether with improvements in SLO attainment and fairness."
                },
                "authors": [
                    {
                        "name": "Xuan He"
                    },
                    {
                        "name": "Zequan Fang"
                    },
                    {
                        "name": "Jinzhao Lian"
                    },
                    {
                        "name": "Danny H. K. Tsang"
                    },
                    {
                        "name": "Baosen Zhang"
                    },
                    {
                        "name": "Yize Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yize Chen"
                },
                "author": "Yize Chen",
                "arxiv_comment": "In Submission, code available at\n  https://github.com/AndrewFangZequan/LLM_Serving_FREESH",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.00807v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.00807v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04677v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04677v2",
                "updated": "2025-11-05T18:12:33Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    18,
                    12,
                    33,
                    2,
                    309,
                    0
                ],
                "published": "2025-02-07T05:49:50Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    5,
                    49,
                    50,
                    4,
                    38,
                    0
                ],
                "title": "LLM Query Scheduling with Prefix Reuse and Latency Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Query Scheduling with Prefix Reuse and Latency Constraints"
                },
                "summary": "The efficient deployment of large language models (LLMs) in online settings\nrequires optimizing inference performance under stringent latency constraints,\nparticularly the time-to-first-token (TTFT) and time-per-output-token (TPOT).\nThis paper focuses on the query scheduling problem for LLM inference with\nprefix reuse, a technique that leverages shared prefixes across queries to\nreduce computational overhead. Our work reveals previously unknown limitations\nof the existing first-come-first-serve (FCFS) and longest-prefix-match (LPM)\nscheduling strategies with respect to satisfying latency constraints. We\npresent a formal theoretical framework for LLM query scheduling under\nRadixAttention, a prefix reuse mechanism that stores and reuses intermediate\nrepresentations in a radix tree structure. Our analysis establishes the\nNP-hardness of the scheduling problem with prefix reuse under TTFT constraints\nand proposes a novel scheduling algorithm, $k$-LPM, which generalizes existing\nmethods by balancing prefix reuse and fairness in query processing. Theoretical\nguarantees demonstrate that $k$-LPM achieves improved TTFT performance under\nrealistic traffic patterns captured by a data generative model. Empirical\nevaluations in a realistic serving setting validates our findings, showing\nsignificant reductions in P99 TTFT compared to baseline methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The efficient deployment of large language models (LLMs) in online settings\nrequires optimizing inference performance under stringent latency constraints,\nparticularly the time-to-first-token (TTFT) and time-per-output-token (TPOT).\nThis paper focuses on the query scheduling problem for LLM inference with\nprefix reuse, a technique that leverages shared prefixes across queries to\nreduce computational overhead. Our work reveals previously unknown limitations\nof the existing first-come-first-serve (FCFS) and longest-prefix-match (LPM)\nscheduling strategies with respect to satisfying latency constraints. We\npresent a formal theoretical framework for LLM query scheduling under\nRadixAttention, a prefix reuse mechanism that stores and reuses intermediate\nrepresentations in a radix tree structure. Our analysis establishes the\nNP-hardness of the scheduling problem with prefix reuse under TTFT constraints\nand proposes a novel scheduling algorithm, $k$-LPM, which generalizes existing\nmethods by balancing prefix reuse and fairness in query processing. Theoretical\nguarantees demonstrate that $k$-LPM achieves improved TTFT performance under\nrealistic traffic patterns captured by a data generative model. Empirical\nevaluations in a realistic serving setting validates our findings, showing\nsignificant reductions in P99 TTFT compared to baseline methods."
                },
                "authors": [
                    {
                        "name": "Gregory Dexter"
                    },
                    {
                        "name": "Shao Tang"
                    },
                    {
                        "name": "Ata Fatahi Baarzi"
                    },
                    {
                        "name": "Qingquan Song"
                    },
                    {
                        "name": "Tejas Dharamsi"
                    },
                    {
                        "name": "Aman Gupta"
                    }
                ],
                "author_detail": {
                    "name": "Aman Gupta"
                },
                "author": "Aman Gupta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04677v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04677v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.00183v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.00183v2",
                "updated": "2025-11-05T17:58:32Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    17,
                    58,
                    32,
                    2,
                    309,
                    0
                ],
                "published": "2025-10-31T18:38:05Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    18,
                    38,
                    5,
                    4,
                    304,
                    0
                ],
                "title": "PDE-SHARP: PDE Solver Hybrids through Analysis and Refinement Passes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PDE-SHARP: PDE Solver Hybrids through Analysis and Refinement Passes"
                },
                "summary": "Current LLM-driven approaches using test-time computing to generate PDE\nsolvers execute a large number of solver samples to identify high-accuracy\nsolvers. These paradigms are especially costly for complex PDEs requiring\nsubstantial computational resources for numerical evaluation. We introduce\nPDE-SHARP, a framework to reduce computational costs by replacing expensive\nscientific computation by cheaper LLM inference that achieves superior solver\naccuracy with 60-75% fewer computational evaluations. PDE-SHARP employs three\nstages: (1) Analysis: mathematical chain-of-thought analysis including PDE\nclassification, solution type detection, and stability analysis; (2) Genesis:\nsolver generation based on mathematical insights from the previous stage; and\n(3) Synthesis: collaborative selection-hybridization tournaments in which LLM\njudges iteratively refine implementations through flexible performance\nfeedback. To generate high-quality solvers, PDE-SHARP requires fewer than 13\nsolver evaluations on average compared to 30+ for baseline methods, improving\naccuracy uniformly across tested PDEs by $4\\times$ on average, and demonstrates\nrobust performance across LLM architectures, from general-purpose to\nspecialized reasoning models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current LLM-driven approaches using test-time computing to generate PDE\nsolvers execute a large number of solver samples to identify high-accuracy\nsolvers. These paradigms are especially costly for complex PDEs requiring\nsubstantial computational resources for numerical evaluation. We introduce\nPDE-SHARP, a framework to reduce computational costs by replacing expensive\nscientific computation by cheaper LLM inference that achieves superior solver\naccuracy with 60-75% fewer computational evaluations. PDE-SHARP employs three\nstages: (1) Analysis: mathematical chain-of-thought analysis including PDE\nclassification, solution type detection, and stability analysis; (2) Genesis:\nsolver generation based on mathematical insights from the previous stage; and\n(3) Synthesis: collaborative selection-hybridization tournaments in which LLM\njudges iteratively refine implementations through flexible performance\nfeedback. To generate high-quality solvers, PDE-SHARP requires fewer than 13\nsolver evaluations on average compared to 30+ for baseline methods, improving\naccuracy uniformly across tested PDEs by $4\\times$ on average, and demonstrates\nrobust performance across LLM architectures, from general-purpose to\nspecialized reasoning models."
                },
                "authors": [
                    {
                        "name": "Shaghayegh Fazliani"
                    },
                    {
                        "name": "Madeleine Udell"
                    }
                ],
                "author_detail": {
                    "name": "Madeleine Udell"
                },
                "author": "Madeleine Udell",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.00183v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.00183v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03675v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03675v1",
                "updated": "2025-11-05T17:47:46Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    17,
                    47,
                    46,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T17:47:46Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    17,
                    47,
                    46,
                    2,
                    309,
                    0
                ],
                "title": "Whisper Leak: a side-channel attack on Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Whisper Leak: a side-channel attack on Large Language Models"
                },
                "summary": "Large Language Models (LLMs) are increasingly deployed in sensitive domains\nincluding healthcare, legal services, and confidential communications, where\nprivacy is paramount. This paper introduces Whisper Leak, a side-channel attack\nthat infers user prompt topics from encrypted LLM traffic by analyzing packet\nsize and timing patterns in streaming responses. Despite TLS encryption\nprotecting content, these metadata patterns leak sufficient information to\nenable topic classification. We demonstrate the attack across 28 popular LLMs\nfrom major providers, achieving near-perfect classification (often >98% AUPRC)\nand high precision even at extreme class imbalance (10,000:1 noise-to-target\nratio). For many models, we achieve 100% precision in identifying sensitive\ntopics like \"money laundering\" while recovering 5-20% of target conversations.\nThis industry-wide vulnerability poses significant risks for users under\nnetwork surveillance by ISPs, governments, or local adversaries. We evaluate\nthree mitigation strategies - random padding, token batching, and packet\ninjection - finding that while each reduces attack effectiveness, none provides\ncomplete protection. Through responsible disclosure, we have collaborated with\nproviders to implement initial countermeasures. Our findings underscore the\nneed for LLM providers to address metadata leakage as AI systems handle\nincreasingly sensitive information.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly deployed in sensitive domains\nincluding healthcare, legal services, and confidential communications, where\nprivacy is paramount. This paper introduces Whisper Leak, a side-channel attack\nthat infers user prompt topics from encrypted LLM traffic by analyzing packet\nsize and timing patterns in streaming responses. Despite TLS encryption\nprotecting content, these metadata patterns leak sufficient information to\nenable topic classification. We demonstrate the attack across 28 popular LLMs\nfrom major providers, achieving near-perfect classification (often >98% AUPRC)\nand high precision even at extreme class imbalance (10,000:1 noise-to-target\nratio). For many models, we achieve 100% precision in identifying sensitive\ntopics like \"money laundering\" while recovering 5-20% of target conversations.\nThis industry-wide vulnerability poses significant risks for users under\nnetwork surveillance by ISPs, governments, or local adversaries. We evaluate\nthree mitigation strategies - random padding, token batching, and packet\ninjection - finding that while each reduces attack effectiveness, none provides\ncomplete protection. Through responsible disclosure, we have collaborated with\nproviders to implement initial countermeasures. Our findings underscore the\nneed for LLM providers to address metadata leakage as AI systems handle\nincreasingly sensitive information."
                },
                "authors": [
                    {
                        "name": "Geoff McDonald"
                    },
                    {
                        "name": "Jonathan Bar Or"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan Bar Or"
                },
                "author": "Jonathan Bar Or",
                "arxiv_comment": "14 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03675v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03675v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "K.4.1; C.2.0; K.6.5; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16638v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16638v4",
                "updated": "2025-11-05T17:42:36Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    17,
                    42,
                    36,
                    2,
                    309,
                    0
                ],
                "published": "2024-11-25T18:15:15Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    18,
                    15,
                    15,
                    0,
                    330,
                    0
                ],
                "title": "Do Automatic Factuality Metrics Measure Factuality? A Critical\n  Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Automatic Factuality Metrics Measure Factuality? A Critical\n  Evaluation"
                },
                "summary": "Modern LLMs can now produce highly readable abstractive summaries, to the\npoint that traditional automated metrics for evaluating summary quality, such\nas ROUGE, have saturated. However, LLMs still sometimes introduce inaccuracies\ninto summaries, i.e., information inconsistent with or unsupported by the\ncorresponding source. Measuring the occurrence of these often subtle factual\ninconsistencies automatically has proved challenging. This in turn has\nmotivated development of metrics intended to measure the factual consistency of\ngenerated summaries against sources. But are these approaches measuring what\nthey purport to? Or are they mostly exploiting artifacts? In this work, we\nstress test a range of automatic factuality metrics, including specialized\nmodels and LLM-based prompting methods, to probe what they actually capture.\nUsing a shallow classifier to separate ``easy'' examples for factual evaluation\nwhere surface features suffice from ``hard'' cases requiring deeper reasoning,\nwe find that all metrics show substantial performance drops on the latter.\nFurthermore, some metrics are more sensitive to benign, fact-preserving edits\nthan to factual corrections. Building on this observation, we demonstrate that\nmost automatic factuality metrics can be gamed, i.e., their scores can be\nartificially inflated by appending innocuous, content-free sentences to\nsummaries. Among the metrics tested, the prompt based ChatGPT-DA approach is\nthe most robust and reliable. However, this comes with a notable caveat:\nPrompting LLMs to assess factuality may overly rely on their parametric\nknowledge rather than the provided reference when making judgments. Taken\ntogether, our findings call into question the reliability of current factuality\nmetrics and prompt a broader reflection on what these metrics are truly\nmeasuring.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern LLMs can now produce highly readable abstractive summaries, to the\npoint that traditional automated metrics for evaluating summary quality, such\nas ROUGE, have saturated. However, LLMs still sometimes introduce inaccuracies\ninto summaries, i.e., information inconsistent with or unsupported by the\ncorresponding source. Measuring the occurrence of these often subtle factual\ninconsistencies automatically has proved challenging. This in turn has\nmotivated development of metrics intended to measure the factual consistency of\ngenerated summaries against sources. But are these approaches measuring what\nthey purport to? Or are they mostly exploiting artifacts? In this work, we\nstress test a range of automatic factuality metrics, including specialized\nmodels and LLM-based prompting methods, to probe what they actually capture.\nUsing a shallow classifier to separate ``easy'' examples for factual evaluation\nwhere surface features suffice from ``hard'' cases requiring deeper reasoning,\nwe find that all metrics show substantial performance drops on the latter.\nFurthermore, some metrics are more sensitive to benign, fact-preserving edits\nthan to factual corrections. Building on this observation, we demonstrate that\nmost automatic factuality metrics can be gamed, i.e., their scores can be\nartificially inflated by appending innocuous, content-free sentences to\nsummaries. Among the metrics tested, the prompt based ChatGPT-DA approach is\nthe most robust and reliable. However, this comes with a notable caveat:\nPrompting LLMs to assess factuality may overly rely on their parametric\nknowledge rather than the provided reference when making judgments. Taken\ntogether, our findings call into question the reliability of current factuality\nmetrics and prompt a broader reflection on what these metrics are truly\nmeasuring."
                },
                "authors": [
                    {
                        "name": "Sanjana Ramprasad"
                    },
                    {
                        "name": "Byron C. Wallace"
                    }
                ],
                "author_detail": {
                    "name": "Byron C. Wallace"
                },
                "author": "Byron C. Wallace",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16638v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16638v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02802v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02802v2",
                "updated": "2025-11-05T17:36:30Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    17,
                    36,
                    30,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-04T18:25:17Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    18,
                    25,
                    17,
                    1,
                    308,
                    0
                ],
                "title": "TabTune: A Unified Library for Inference and Fine-Tuning Tabular\n  Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TabTune: A Unified Library for Inference and Fine-Tuning Tabular\n  Foundation Models"
                },
                "summary": "Tabular foundation models represent a growing paradigm in structured data\nlearning, extending the benefits of large-scale pretraining to tabular domains.\nHowever, their adoption remains limited due to heterogeneous preprocessing\npipelines, fragmented APIs, inconsistent fine-tuning procedures, and the\nabsence of standardized evaluation for deployment-oriented metrics such as\ncalibration and fairness. We present TabTune, a unified library that\nstandardizes the complete workflow for tabular foundation models through a\nsingle interface. TabTune provides consistent access to seven state-of-the-art\nmodels supporting multiple adaptation strategies, including zero-shot\ninference, meta-learning, supervised fine-tuning (SFT), and parameter-efficient\nfine-tuning (PEFT). The framework automates model-aware preprocessing, manages\narchitectural heterogeneity internally, and integrates evaluation modules for\nperformance, calibration, and fairness. Designed for extensibility and\nreproducibility, TabTune enables consistent benchmarking of adaptation\nstrategies of tabular foundation models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tabular foundation models represent a growing paradigm in structured data\nlearning, extending the benefits of large-scale pretraining to tabular domains.\nHowever, their adoption remains limited due to heterogeneous preprocessing\npipelines, fragmented APIs, inconsistent fine-tuning procedures, and the\nabsence of standardized evaluation for deployment-oriented metrics such as\ncalibration and fairness. We present TabTune, a unified library that\nstandardizes the complete workflow for tabular foundation models through a\nsingle interface. TabTune provides consistent access to seven state-of-the-art\nmodels supporting multiple adaptation strategies, including zero-shot\ninference, meta-learning, supervised fine-tuning (SFT), and parameter-efficient\nfine-tuning (PEFT). The framework automates model-aware preprocessing, manages\narchitectural heterogeneity internally, and integrates evaluation modules for\nperformance, calibration, and fairness. Designed for extensibility and\nreproducibility, TabTune enables consistent benchmarking of adaptation\nstrategies of tabular foundation models."
                },
                "authors": [
                    {
                        "name": "Aditya Tanna"
                    },
                    {
                        "name": "Pratinav Seth"
                    },
                    {
                        "name": "Mohamed Bouadi"
                    },
                    {
                        "name": "Utsav Avaiya"
                    },
                    {
                        "name": "Vinay Kumar Sankarapu"
                    }
                ],
                "author_detail": {
                    "name": "Vinay Kumar Sankarapu"
                },
                "author": "Vinay Kumar Sankarapu",
                "arxiv_comment": "The library is open source and available at\n  https://github.com/Lexsi-Labs/TabTune",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02802v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02802v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20749v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20749v3",
                "updated": "2025-11-05T17:33:06Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    17,
                    33,
                    6,
                    2,
                    309,
                    0
                ],
                "published": "2024-10-28T05:28:51Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    5,
                    28,
                    51,
                    0,
                    302,
                    0
                ],
                "title": "Matryoshka Pilot: Learning to Drive Black-Box LLMs with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Matryoshka Pilot: Learning to Drive Black-Box LLMs with LLMs"
                },
                "summary": "Despite the impressive generative abilities of black-box large language\nmodels (LLMs), their inherent opacity hinders further advancements in\ncapabilities such as reasoning, planning, and personalization. Existing works\naim to enhance LLM capabilities via domain-specific adaptation, which require\nadditional training on accessible model parameters, an infeasible option for\nblack-box LLMs. To address this challenge, we introduce Matryoshka Pilot\n(M-Pilot), a lightweight white-box LLM controller that guides a large-scale\nblack-box LLM generator by decomposing complex tasks into a series of\nintermediate outputs. Specifically, we consider the black-box LLM as an\nenvironment, with M-Pilot serving as a policy to provide intermediate guidance\nthrough prompts for driving the black-box LLM. M-Pilot is trained to pivot the\noutputs of the black-box LLM aligning with preferences during iterative\ninteraction, which enables controllable multi-turn generation and\nself-improvement in optimizing intermediate guidance. Empirical evaluations on\ndiverse tasks demonstrate that our method effectively enhances the capabilities\nof black-box LLMs in complex, long-horizon tasks. Our code is publicly\navailable at: https://github.com/lichangh20/Matryoshka.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the impressive generative abilities of black-box large language\nmodels (LLMs), their inherent opacity hinders further advancements in\ncapabilities such as reasoning, planning, and personalization. Existing works\naim to enhance LLM capabilities via domain-specific adaptation, which require\nadditional training on accessible model parameters, an infeasible option for\nblack-box LLMs. To address this challenge, we introduce Matryoshka Pilot\n(M-Pilot), a lightweight white-box LLM controller that guides a large-scale\nblack-box LLM generator by decomposing complex tasks into a series of\nintermediate outputs. Specifically, we consider the black-box LLM as an\nenvironment, with M-Pilot serving as a policy to provide intermediate guidance\nthrough prompts for driving the black-box LLM. M-Pilot is trained to pivot the\noutputs of the black-box LLM aligning with preferences during iterative\ninteraction, which enables controllable multi-turn generation and\nself-improvement in optimizing intermediate guidance. Empirical evaluations on\ndiverse tasks demonstrate that our method effectively enhances the capabilities\nof black-box LLMs in complex, long-horizon tasks. Our code is publicly\navailable at: https://github.com/lichangh20/Matryoshka."
                },
                "authors": [
                    {
                        "name": "Changhao Li"
                    },
                    {
                        "name": "Yuchen Zhuang"
                    },
                    {
                        "name": "Rushi Qiang"
                    },
                    {
                        "name": "Haotian Sun"
                    },
                    {
                        "name": "Hanjun Dai"
                    },
                    {
                        "name": "Chao Zhang"
                    },
                    {
                        "name": "Bo Dai"
                    }
                ],
                "author_detail": {
                    "name": "Bo Dai"
                },
                "author": "Bo Dai",
                "arxiv_comment": "Accepted by NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20749v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20749v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03665v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03665v1",
                "updated": "2025-11-05T17:30:31Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    17,
                    30,
                    31,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T17:30:31Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    17,
                    30,
                    31,
                    2,
                    309,
                    0
                ],
                "title": "A Lightweight 3D-CNN for Event-Based Human Action Recognition with\n  Privacy-Preserving Potential",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Lightweight 3D-CNN for Event-Based Human Action Recognition with\n  Privacy-Preserving Potential"
                },
                "summary": "This paper presents a lightweight three-dimensional convolutional neural\nnetwork (3DCNN) for human activity recognition (HAR) using event-based vision\ndata. Privacy preservation is a key challenge in human monitoring systems, as\nconventional frame-based cameras capture identifiable personal information. In\ncontrast, event cameras record only changes in pixel intensity, providing an\ninherently privacy-preserving sensing modality. The proposed network\neffectively models both spatial and temporal dynamics while maintaining a\ncompact design suitable for edge deployment. To address class imbalance and\nenhance generalization, focal loss with class reweighting and targeted data\naugmentation strategies are employed. The model is trained and evaluated on a\ncomposite dataset derived from the Toyota Smart Home and ETRI datasets.\nExperimental results demonstrate an F1-score of 0.9415 and an overall accuracy\nof 94.17%, outperforming benchmark 3D-CNN architectures such as C3D, ResNet3D,\nand MC3_18 by up to 3%. These results highlight the potential of event-based\ndeep learning for developing accurate, efficient, and privacy-aware human\naction recognition systems suitable for real-world edge applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a lightweight three-dimensional convolutional neural\nnetwork (3DCNN) for human activity recognition (HAR) using event-based vision\ndata. Privacy preservation is a key challenge in human monitoring systems, as\nconventional frame-based cameras capture identifiable personal information. In\ncontrast, event cameras record only changes in pixel intensity, providing an\ninherently privacy-preserving sensing modality. The proposed network\neffectively models both spatial and temporal dynamics while maintaining a\ncompact design suitable for edge deployment. To address class imbalance and\nenhance generalization, focal loss with class reweighting and targeted data\naugmentation strategies are employed. The model is trained and evaluated on a\ncomposite dataset derived from the Toyota Smart Home and ETRI datasets.\nExperimental results demonstrate an F1-score of 0.9415 and an overall accuracy\nof 94.17%, outperforming benchmark 3D-CNN architectures such as C3D, ResNet3D,\nand MC3_18 by up to 3%. These results highlight the potential of event-based\ndeep learning for developing accurate, efficient, and privacy-aware human\naction recognition systems suitable for real-world edge applications."
                },
                "authors": [
                    {
                        "name": "Mehdi Sefidgar Dilmaghani"
                    },
                    {
                        "name": "Francis Fowley"
                    },
                    {
                        "name": "Peter Corcoran"
                    }
                ],
                "author_detail": {
                    "name": "Peter Corcoran"
                },
                "author": "Peter Corcoran",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03665v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03665v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02826v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02826v2",
                "updated": "2025-11-05T17:25:40Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    17,
                    25,
                    40,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-04T18:54:58Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    18,
                    54,
                    58,
                    1,
                    308,
                    0
                ],
                "title": "PLUTO-4: Frontier Pathology Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PLUTO-4: Frontier Pathology Foundation Models"
                },
                "summary": "Foundation models trained on large-scale pathology image corpora have\ndemonstrated strong transfer capabilities across diverse histopathology tasks.\nBuilding on this progress, we introduce PLUTO-4, our next generation of\npathology foundation models that extend the Pathology-Universal Transformer\n(PLUTO) to frontier scale. We share two complementary Vision Transformer\narchitectures in the PLUTO-4 family: a compact and efficient PLUTO-4S model\noptimized for multi-scale deployment using a FlexiViT setup with 2D-RoPE\nembeddings, and a frontier-scale PLUTO-4G model trained with a single patch\nsize to maximize representation capacity and stability. Both models are\npretrained using a self-supervised objective derived from DINOv2 on a large\nmulti-institutional corpus containing 551,164 WSIs from 137,144 patients across\nover 50 institutions, spanning over 60 disease types and over 100 stains.\nComprehensive evaluation across public and internal benchmarks demonstrates\nthat PLUTO-4 achieves state-of-the-art performance on tasks requiring varying\nspatial and biological context, including patch-level classification,\nsegmentation, and slide-level diagnosis. The compact PLUTO-4S provides\nhigh-throughput and robust performance for practical deployment, while PLUTO-4G\nestablishes new performance frontiers across multiple pathology benchmarks,\nincluding an 11% improvement in dermatopathology diagnosis. These diverse\nimprovements underscore PLUTO-4's potential to transform real-world\napplications as a backbone for translational research and diagnostic use cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foundation models trained on large-scale pathology image corpora have\ndemonstrated strong transfer capabilities across diverse histopathology tasks.\nBuilding on this progress, we introduce PLUTO-4, our next generation of\npathology foundation models that extend the Pathology-Universal Transformer\n(PLUTO) to frontier scale. We share two complementary Vision Transformer\narchitectures in the PLUTO-4 family: a compact and efficient PLUTO-4S model\noptimized for multi-scale deployment using a FlexiViT setup with 2D-RoPE\nembeddings, and a frontier-scale PLUTO-4G model trained with a single patch\nsize to maximize representation capacity and stability. Both models are\npretrained using a self-supervised objective derived from DINOv2 on a large\nmulti-institutional corpus containing 551,164 WSIs from 137,144 patients across\nover 50 institutions, spanning over 60 disease types and over 100 stains.\nComprehensive evaluation across public and internal benchmarks demonstrates\nthat PLUTO-4 achieves state-of-the-art performance on tasks requiring varying\nspatial and biological context, including patch-level classification,\nsegmentation, and slide-level diagnosis. The compact PLUTO-4S provides\nhigh-throughput and robust performance for practical deployment, while PLUTO-4G\nestablishes new performance frontiers across multiple pathology benchmarks,\nincluding an 11% improvement in dermatopathology diagnosis. These diverse\nimprovements underscore PLUTO-4's potential to transform real-world\napplications as a backbone for translational research and diagnostic use cases."
                },
                "authors": [
                    {
                        "name": "Harshith Padigela"
                    },
                    {
                        "name": "Shima Nofallah"
                    },
                    {
                        "name": "Atchuth Naveen Chilaparasetti"
                    },
                    {
                        "name": "Ryun Han"
                    },
                    {
                        "name": "Andrew Walker"
                    },
                    {
                        "name": "Judy Shen"
                    },
                    {
                        "name": "Chintan Shah"
                    },
                    {
                        "name": "Blake Martin"
                    },
                    {
                        "name": "Aashish Sood"
                    },
                    {
                        "name": "Elliot Miller"
                    },
                    {
                        "name": "Ben Glass"
                    },
                    {
                        "name": "Andy Beck"
                    },
                    {
                        "name": "Harsha Pokkalla"
                    },
                    {
                        "name": "Syed Ashar Javed"
                    }
                ],
                "author_detail": {
                    "name": "Syed Ashar Javed"
                },
                "author": "Syed Ashar Javed",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02826v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02826v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14526v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14526v2",
                "updated": "2025-11-05T17:12:59Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    17,
                    12,
                    59,
                    2,
                    309,
                    0
                ],
                "published": "2025-05-20T15:48:23Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    15,
                    48,
                    23,
                    1,
                    140,
                    0
                ],
                "title": "RoboRAN: A Unified Robotics Framework for Reinforcement Learning-Based\n  Autonomous Navigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RoboRAN: A Unified Robotics Framework for Reinforcement Learning-Based\n  Autonomous Navigation"
                },
                "summary": "Autonomous robots must navigate and operate in diverse environments, from\nterrestrial and aquatic settings to aerial and space domains. While\nReinforcement Learning (RL) has shown promise in training policies for specific\nautonomous robots, existing frameworks and benchmarks are often constrained to\nunique platforms, limiting generalization and fair comparisons across different\nmobility systems. In this paper, we present a multi-domain framework for\ntraining, evaluating and deploying RL-based navigation policies across diverse\nrobotic platforms and operational environments. Our work presents four key\ncontributions: (1) a scalable and modular framework, facilitating seamless\nrobot-task interchangeability and reproducible training pipelines; (2)\nsim-to-real transfer demonstrated through real-world experiments with multiple\nrobots, including a satellite robotic simulator, an unmanned surface vessel,\nand a wheeled ground vehicle; (3) the release of the first open-source API for\ndeploying Isaac Lab-trained policies to real robots, enabling lightweight\ninference and rapid field validation; and (4) uniform tasks and metrics for\ncross-medium evaluation, through a unified evaluation testbed to assess\nperformance of navigation tasks in diverse operational conditions (aquatic,\nterrestrial and space). By ensuring consistency between simulation and\nreal-world deployment, RoboRAN lowers the barrier to developing adaptable\nRL-based navigation strategies. Its modular design enables straightforward\nintegration of new robots and tasks through predefined templates, fostering\nreproducibility and extension to diverse domains. To support the community, we\nrelease RoboRAN as open-source.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous robots must navigate and operate in diverse environments, from\nterrestrial and aquatic settings to aerial and space domains. While\nReinforcement Learning (RL) has shown promise in training policies for specific\nautonomous robots, existing frameworks and benchmarks are often constrained to\nunique platforms, limiting generalization and fair comparisons across different\nmobility systems. In this paper, we present a multi-domain framework for\ntraining, evaluating and deploying RL-based navigation policies across diverse\nrobotic platforms and operational environments. Our work presents four key\ncontributions: (1) a scalable and modular framework, facilitating seamless\nrobot-task interchangeability and reproducible training pipelines; (2)\nsim-to-real transfer demonstrated through real-world experiments with multiple\nrobots, including a satellite robotic simulator, an unmanned surface vessel,\nand a wheeled ground vehicle; (3) the release of the first open-source API for\ndeploying Isaac Lab-trained policies to real robots, enabling lightweight\ninference and rapid field validation; and (4) uniform tasks and metrics for\ncross-medium evaluation, through a unified evaluation testbed to assess\nperformance of navigation tasks in diverse operational conditions (aquatic,\nterrestrial and space). By ensuring consistency between simulation and\nreal-world deployment, RoboRAN lowers the barrier to developing adaptable\nRL-based navigation strategies. Its modular design enables straightforward\nintegration of new robots and tasks through predefined templates, fostering\nreproducibility and extension to diverse domains. To support the community, we\nrelease RoboRAN as open-source."
                },
                "authors": [
                    {
                        "name": "Matteo El-Hariry"
                    },
                    {
                        "name": "Antoine Richard"
                    },
                    {
                        "name": "Ricard M. Castan"
                    },
                    {
                        "name": "Luis F. W. Batista"
                    },
                    {
                        "name": "Matthieu Geist"
                    },
                    {
                        "name": "Cedric Pradalier"
                    },
                    {
                        "name": "Miguel Olivares-Mendez"
                    }
                ],
                "author_detail": {
                    "name": "Miguel Olivares-Mendez"
                },
                "author": "Miguel Olivares-Mendez",
                "arxiv_comment": "Accepted at Transactions on Machine Learning Research (TMLR)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14526v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14526v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03651v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03651v1",
                "updated": "2025-11-05T17:09:16Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    17,
                    9,
                    16,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T17:09:16Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    17,
                    9,
                    16,
                    2,
                    309,
                    0
                ],
                "title": "Flying Robotics Art: ROS-based Drone Draws the Record-Breaking Mural",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flying Robotics Art: ROS-based Drone Draws the Record-Breaking Mural"
                },
                "summary": "This paper presents the innovative design and successful deployment of a\npioneering autonomous unmanned aerial system developed for executing the\nworld's largest mural painted by a drone. Addressing the dual challenges of\nmaintaining artistic precision and operational reliability under adverse\noutdoor conditions such as wind and direct sunlight, our work introduces a\nrobust system capable of navigating and painting outdoors with unprecedented\naccuracy. Key to our approach is a novel navigation system that combines an\ninfrared (IR) motion capture camera and LiDAR technology, enabling precise\nlocation tracking tailored specifically for largescale artistic applications.\nWe employ a unique control architecture that uses different regulation in\ntangential and normal directions relative to the planned path, enabling precise\ntrajectory tracking and stable line rendering. We also present algorithms for\ntrajectory planning and path optimization, allowing for complex curve drawing\nand area filling. The system includes a custom-designed paint spraying\nmechanism, specifically engineered to function effectively amidst the turbulent\nairflow generated by the drone's propellers, which also protects the drone's\ncritical components from paint-related damage, ensuring longevity and\nconsistent performance. Experimental results demonstrate the system's\nrobustness and precision in varied conditions, showcasing its potential for\nautonomous large-scale art creation and expanding the functional applications\nof robotics in creative fields.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents the innovative design and successful deployment of a\npioneering autonomous unmanned aerial system developed for executing the\nworld's largest mural painted by a drone. Addressing the dual challenges of\nmaintaining artistic precision and operational reliability under adverse\noutdoor conditions such as wind and direct sunlight, our work introduces a\nrobust system capable of navigating and painting outdoors with unprecedented\naccuracy. Key to our approach is a novel navigation system that combines an\ninfrared (IR) motion capture camera and LiDAR technology, enabling precise\nlocation tracking tailored specifically for largescale artistic applications.\nWe employ a unique control architecture that uses different regulation in\ntangential and normal directions relative to the planned path, enabling precise\ntrajectory tracking and stable line rendering. We also present algorithms for\ntrajectory planning and path optimization, allowing for complex curve drawing\nand area filling. The system includes a custom-designed paint spraying\nmechanism, specifically engineered to function effectively amidst the turbulent\nairflow generated by the drone's propellers, which also protects the drone's\ncritical components from paint-related damage, ensuring longevity and\nconsistent performance. Experimental results demonstrate the system's\nrobustness and precision in varied conditions, showcasing its potential for\nautonomous large-scale art creation and expanding the functional applications\nof robotics in creative fields."
                },
                "authors": [
                    {
                        "name": "Andrei A. Korigodskii"
                    },
                    {
                        "name": "Oleg D. Kalachev"
                    },
                    {
                        "name": "Artem E. Vasiunik"
                    },
                    {
                        "name": "Matvei V. Urvantsev"
                    },
                    {
                        "name": "Georgii E. Bondar"
                    }
                ],
                "author_detail": {
                    "name": "Georgii E. Bondar"
                },
                "author": "Georgii E. Bondar",
                "arxiv_doi": "10.1109/IROS58592.2024.10802405",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/IROS58592.2024.10802405",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2511.03651v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03651v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "2024 IEEE/RSJ International Conference on Intelligent Robots and\n  Systems (IROS)",
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.9; J.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11857v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11857v2",
                "updated": "2025-11-05T17:05:02Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    17,
                    5,
                    2,
                    2,
                    309,
                    0
                ],
                "published": "2025-06-13T15:04:01Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    15,
                    4,
                    1,
                    4,
                    164,
                    0
                ],
                "title": "Post Persona Alignment for Multi-Session Dialogue Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post Persona Alignment for Multi-Session Dialogue Generation"
                },
                "summary": "Multi-session persona-based dialogue generation presents challenges in\nmaintaining long-term consistency and generating diverse, personalized\nresponses. While large language models (LLMs) excel in single-session\ndialogues, they struggle to preserve persona fidelity and conversational\ncoherence across extended interactions. Existing methods typically retrieve\npersona information before response generation, which can constrain diversity\nand result in generic outputs. We propose Post Persona Alignment (PPA), a novel\ntwo-stage framework that reverses this process. PPA first generates a general\nresponse based solely on dialogue context, then retrieves relevant persona\nmemories using the response as a query, and finally refines the response to\nalign with the speaker's persona. This post-hoc alignment strategy promotes\nnaturalness and diversity while preserving consistency and personalization.\nExperiments on multi-session LLM-generated dialogue data demonstrate that PPA\nsignificantly outperforms prior approaches in consistency, diversity, and\npersona relevance, offering a more flexible and effective paradigm for\nlong-term personalized dialogue generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-session persona-based dialogue generation presents challenges in\nmaintaining long-term consistency and generating diverse, personalized\nresponses. While large language models (LLMs) excel in single-session\ndialogues, they struggle to preserve persona fidelity and conversational\ncoherence across extended interactions. Existing methods typically retrieve\npersona information before response generation, which can constrain diversity\nand result in generic outputs. We propose Post Persona Alignment (PPA), a novel\ntwo-stage framework that reverses this process. PPA first generates a general\nresponse based solely on dialogue context, then retrieves relevant persona\nmemories using the response as a query, and finally refines the response to\nalign with the speaker's persona. This post-hoc alignment strategy promotes\nnaturalness and diversity while preserving consistency and personalization.\nExperiments on multi-session LLM-generated dialogue data demonstrate that PPA\nsignificantly outperforms prior approaches in consistency, diversity, and\npersona relevance, offering a more flexible and effective paradigm for\nlong-term personalized dialogue generation."
                },
                "authors": [
                    {
                        "name": "Yi-Pei Chen"
                    },
                    {
                        "name": "Noriki Nishida"
                    },
                    {
                        "name": "Hideki Nakayama"
                    },
                    {
                        "name": "Yuji Matsumoto"
                    }
                ],
                "author_detail": {
                    "name": "Yuji Matsumoto"
                },
                "author": "Yuji Matsumoto",
                "arxiv_comment": "EMNLP 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11857v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11857v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03641v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03641v1",
                "updated": "2025-11-05T17:00:39Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    17,
                    0,
                    39,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T17:00:39Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    17,
                    0,
                    39,
                    2,
                    309,
                    0
                ],
                "title": "Watermarking Large Language Models in Europe: Interpreting the AI Act in\n  Light of Technology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Watermarking Large Language Models in Europe: Interpreting the AI Act in\n  Light of Technology"
                },
                "summary": "To foster trustworthy Artificial Intelligence (AI) within the European Union,\nthe AI Act requires providers to mark and detect the outputs of their\ngeneral-purpose models. The Article 50 and Recital 133 call for marking methods\nthat are ''sufficiently reliable, interoperable, effective and robust''. Yet,\nthe rapidly evolving and heterogeneous landscape of watermarks for Large\nLanguage Models (LLMs) makes it difficult to determine how these four standards\ncan be translated into concrete and measurable evaluations. Our paper addresses\nthis challenge, anchoring the normativity of European requirements in the\nmultiplicity of watermarking techniques. Introducing clear and distinct\nconcepts on LLM watermarking, our contribution is threefold. (1) Watermarking\nCategorisation: We propose an accessible taxonomy of watermarking methods\naccording to the stage of the LLM lifecycle at which they are applied - before,\nduring, or after training, and during next-token distribution or sampling. (2)\nWatermarking Evaluation: We interpret the EU AI Act's requirements by mapping\neach criterion with state-of-the-art evaluations on robustness and\ndetectability of the watermark, and of quality of the LLM. Since\ninteroperability remains largely untheorised in LLM watermarking research, we\npropose three normative dimensions to frame its assessment. (3) Watermarking\nComparison: We compare current watermarking methods for LLMs against the\noperationalised European criteria and show that no approach yet satisfies all\nfour standards. Encouraged by emerging empirical tests, we recommend further\nresearch into watermarking directly embedded within the low-level architecture\nof LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To foster trustworthy Artificial Intelligence (AI) within the European Union,\nthe AI Act requires providers to mark and detect the outputs of their\ngeneral-purpose models. The Article 50 and Recital 133 call for marking methods\nthat are ''sufficiently reliable, interoperable, effective and robust''. Yet,\nthe rapidly evolving and heterogeneous landscape of watermarks for Large\nLanguage Models (LLMs) makes it difficult to determine how these four standards\ncan be translated into concrete and measurable evaluations. Our paper addresses\nthis challenge, anchoring the normativity of European requirements in the\nmultiplicity of watermarking techniques. Introducing clear and distinct\nconcepts on LLM watermarking, our contribution is threefold. (1) Watermarking\nCategorisation: We propose an accessible taxonomy of watermarking methods\naccording to the stage of the LLM lifecycle at which they are applied - before,\nduring, or after training, and during next-token distribution or sampling. (2)\nWatermarking Evaluation: We interpret the EU AI Act's requirements by mapping\neach criterion with state-of-the-art evaluations on robustness and\ndetectability of the watermark, and of quality of the LLM. Since\ninteroperability remains largely untheorised in LLM watermarking research, we\npropose three normative dimensions to frame its assessment. (3) Watermarking\nComparison: We compare current watermarking methods for LLMs against the\noperationalised European criteria and show that no approach yet satisfies all\nfour standards. Encouraged by emerging empirical tests, we recommend further\nresearch into watermarking directly embedded within the low-level architecture\nof LLMs."
                },
                "authors": [
                    {
                        "name": "Thomas Souverain"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Souverain"
                },
                "author": "Thomas Souverain",
                "arxiv_comment": "17 pages, 2 Tables and 2 Pictures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03641v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03641v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T01, 68727, 68T30, 68T35, 68T37, 68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03635v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03635v1",
                "updated": "2025-11-05T16:54:10Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    16,
                    54,
                    10,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T16:54:10Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    16,
                    54,
                    10,
                    2,
                    309,
                    0
                ],
                "title": "Towards Transparent Stance Detection: A Zero-Shot Approach Using\n  Implicit and Explicit Interpretability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Transparent Stance Detection: A Zero-Shot Approach Using\n  Implicit and Explicit Interpretability"
                },
                "summary": "Zero-Shot Stance Detection (ZSSD) identifies the attitude of the post toward\nunseen targets. Existing research using contrastive, meta-learning, or data\naugmentation suffers from generalizability issues or lack of coherence between\ntext and target. Recent works leveraging large language models (LLMs) for ZSSD\nfocus either on improving unseen target-specific knowledge or generating\nexplanations for stance analysis. However, most of these works are limited by\ntheir over-reliance on explicit reasoning, provide coarse explanations that\nlack nuance, and do not explicitly model the reasoning process, making it\ndifficult to interpret the model's predictions. To address these issues, in our\nstudy, we develop a novel interpretable ZSSD framework, IRIS. We provide an\ninterpretable understanding of the attitude of the input towards the target\nimplicitly based on sequences within the text (implicit rationales) and\nexplicitly based on linguistic measures (explicit rationales). IRIS considers\nstance detection as an information retrieval ranking task, understanding the\nrelevance of implicit rationales for different stances to guide the model\ntowards correct predictions without requiring the ground-truth of rationales,\nthus providing inherent interpretability. In addition, explicit rationales\nbased on communicative features help decode the emotional and cognitive\ndimensions of stance, offering an interpretable understanding of the author's\nattitude towards the given target. Extensive experiments on the benchmark\ndatasets of VAST, EZ-STANCE, P-Stance, and RFD using 50%, 30%, and even 10%\ntraining data prove the generalizability of our model, benefiting from the\nproposed architecture and interpretable design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-Shot Stance Detection (ZSSD) identifies the attitude of the post toward\nunseen targets. Existing research using contrastive, meta-learning, or data\naugmentation suffers from generalizability issues or lack of coherence between\ntext and target. Recent works leveraging large language models (LLMs) for ZSSD\nfocus either on improving unseen target-specific knowledge or generating\nexplanations for stance analysis. However, most of these works are limited by\ntheir over-reliance on explicit reasoning, provide coarse explanations that\nlack nuance, and do not explicitly model the reasoning process, making it\ndifficult to interpret the model's predictions. To address these issues, in our\nstudy, we develop a novel interpretable ZSSD framework, IRIS. We provide an\ninterpretable understanding of the attitude of the input towards the target\nimplicitly based on sequences within the text (implicit rationales) and\nexplicitly based on linguistic measures (explicit rationales). IRIS considers\nstance detection as an information retrieval ranking task, understanding the\nrelevance of implicit rationales for different stances to guide the model\ntowards correct predictions without requiring the ground-truth of rationales,\nthus providing inherent interpretability. In addition, explicit rationales\nbased on communicative features help decode the emotional and cognitive\ndimensions of stance, offering an interpretable understanding of the author's\nattitude towards the given target. Extensive experiments on the benchmark\ndatasets of VAST, EZ-STANCE, P-Stance, and RFD using 50%, 30%, and even 10%\ntraining data prove the generalizability of our model, benefiting from the\nproposed architecture and interpretable design."
                },
                "authors": [
                    {
                        "name": "Apoorva Upadhyaya"
                    },
                    {
                        "name": "Wolfgang Nejdl"
                    },
                    {
                        "name": "Marco Fisichella"
                    }
                ],
                "author_detail": {
                    "name": "Marco Fisichella"
                },
                "author": "Marco Fisichella",
                "arxiv_comment": "Accepted in AAAI CONFERENCE ON WEB AND SOCIAL MEDIA (ICWSM 2026)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03635v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03635v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23845v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23845v2",
                "updated": "2025-11-05T16:53:09Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    16,
                    53,
                    9,
                    2,
                    309,
                    0
                ],
                "published": "2025-05-28T17:01:30Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    17,
                    1,
                    30,
                    2,
                    148,
                    0
                ],
                "title": "Read Your Own Mind: Reasoning Helps Surface Self-Confidence Signals in\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Read Your Own Mind: Reasoning Helps Surface Self-Confidence Signals in\n  LLMs"
                },
                "summary": "We study the source of uncertainty in DeepSeek R1-32B by analyzing its\nself-reported verbal confidence on question answering (QA) tasks. In the\ndefault answer-then-confidence setting, the model is regularly over-confident,\nwhereas semantic entropy - obtained by sampling many responses - remains\nreliable. We hypothesize that this is because of semantic entropy's larger\ntest-time compute, which lets us explore the model's predictive distribution.\nWe show that granting DeepSeek the budget to explore its distribution by\nforcing a long chain-of-thought before the final answer greatly improves its\nverbal score effectiveness, even on simple fact-retrieval questions that\nnormally require no reasoning. Furthermore, a separate reader model that sees\nonly the chain can reconstruct very similar confidences, indicating the verbal\nscore might be merely a statistic of the alternatives surfaced during\nreasoning. Our analysis concludes that reliable uncertainty estimation requires\nexplicit exploration of the generative space, and self-reported confidence is\ntrustworthy only after such exploration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the source of uncertainty in DeepSeek R1-32B by analyzing its\nself-reported verbal confidence on question answering (QA) tasks. In the\ndefault answer-then-confidence setting, the model is regularly over-confident,\nwhereas semantic entropy - obtained by sampling many responses - remains\nreliable. We hypothesize that this is because of semantic entropy's larger\ntest-time compute, which lets us explore the model's predictive distribution.\nWe show that granting DeepSeek the budget to explore its distribution by\nforcing a long chain-of-thought before the final answer greatly improves its\nverbal score effectiveness, even on simple fact-retrieval questions that\nnormally require no reasoning. Furthermore, a separate reader model that sees\nonly the chain can reconstruct very similar confidences, indicating the verbal\nscore might be merely a statistic of the alternatives surfaced during\nreasoning. Our analysis concludes that reliable uncertainty estimation requires\nexplicit exploration of the generative space, and self-reported confidence is\ntrustworthy only after such exploration."
                },
                "authors": [
                    {
                        "name": "Jakub Podolak"
                    },
                    {
                        "name": "Rajeev Verma"
                    }
                ],
                "author_detail": {
                    "name": "Rajeev Verma"
                },
                "author": "Rajeev Verma",
                "arxiv_comment": "Presented at UncertaiNLP Workshop at EMNLP 2025\n  https://aclanthology.org/2025.uncertainlp-main.21.pdf",
                "arxiv_journal_ref": "UncertaiNLP Workshop at Empirical Methods in Natural Language\n  Processing 2025 (EMNLP 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23845v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23845v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03631v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03631v1",
                "updated": "2025-11-05T16:49:50Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    16,
                    49,
                    50,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T16:49:50Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    16,
                    49,
                    50,
                    2,
                    309,
                    0
                ],
                "title": "Financial Management System for SMEs: Real-World Deployment of Accounts\n  Receivable and Cash Flow Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Financial Management System for SMEs: Real-World Deployment of Accounts\n  Receivable and Cash Flow Prediction"
                },
                "summary": "Small and Medium Enterprises (SMEs), particularly freelancers and early-stage\nbusinesses, face unique financial management challenges due to limited\nresources, small customer bases, and constrained data availability. This paper\npresents the development and deployment of an integrated financial prediction\nsystem that combines accounts receivable prediction and cash flow forecasting\nspecifically designed for SME operational constraints. Our system addresses the\ngap between enterprise-focused financial tools and the practical needs of\nfreelancers and small businesses. The solution integrates two key components: a\nbinary classification model for predicting invoice payment delays, and a\nmulti-module cash flow forecasting model that handles incomplete and limited\nhistorical data. A prototype system has been implemented and deployed as a web\napplication with integration into Cluee's platform, a startup providing\nfinancial management tools for freelancers, demonstrating practical feasibility\nfor real-world SME financial management.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Small and Medium Enterprises (SMEs), particularly freelancers and early-stage\nbusinesses, face unique financial management challenges due to limited\nresources, small customer bases, and constrained data availability. This paper\npresents the development and deployment of an integrated financial prediction\nsystem that combines accounts receivable prediction and cash flow forecasting\nspecifically designed for SME operational constraints. Our system addresses the\ngap between enterprise-focused financial tools and the practical needs of\nfreelancers and small businesses. The solution integrates two key components: a\nbinary classification model for predicting invoice payment delays, and a\nmulti-module cash flow forecasting model that handles incomplete and limited\nhistorical data. A prototype system has been implemented and deployed as a web\napplication with integration into Cluee's platform, a startup providing\nfinancial management tools for freelancers, demonstrating practical feasibility\nfor real-world SME financial management."
                },
                "authors": [
                    {
                        "name": "Bartomiej Makus"
                    },
                    {
                        "name": "Szymon Bobek"
                    },
                    {
                        "name": "Grzegorz J. Nalepa"
                    }
                ],
                "author_detail": {
                    "name": "Grzegorz J. Nalepa"
                },
                "author": "Grzegorz J. Nalepa",
                "arxiv_comment": "11 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03631v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03631v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03628v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03628v1",
                "updated": "2025-11-05T16:47:26Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    16,
                    47,
                    26,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T16:47:26Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    16,
                    47,
                    26,
                    2,
                    309,
                    0
                ],
                "title": "LiveTradeBench: Seeking Real-World Alpha with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LiveTradeBench: Seeking Real-World Alpha with Large Language Models"
                },
                "summary": "Large language models (LLMs) achieve strong performance across\nbenchmarks--from knowledge quizzes and math reasoning to web-agent tasks--but\nthese tests occur in static settings, lacking real dynamics and uncertainty.\nConsequently, they evaluate isolated reasoning or problem-solving rather than\ndecision-making under uncertainty. To address this, we introduce\nLiveTradeBench, a live trading environment for evaluating LLM agents in\nrealistic and evolving markets. LiveTradeBench follows three design principles:\n(i) Live data streaming of market prices and news, eliminating dependence on\noffline backtesting and preventing information leakage while capturing\nreal-time uncertainty; (ii) a portfolio-management abstraction that extends\ncontrol from single-asset actions to multi-asset allocation, integrating risk\nmanagement and cross-asset reasoning; and (iii) multi-market evaluation across\nstructurally distinct environments--U.S. stocks and Polymarket prediction\nmarkets--differing in volatility, liquidity, and information flow. At each\nstep, an agent observes prices, news, and its portfolio, then outputs\npercentage allocations that balance risk and return. Using LiveTradeBench, we\nrun 50-day live evaluations of 21 LLMs across families. Results show that (1)\nhigh LMArena scores do not imply superior trading outcomes; (2) models display\ndistinct portfolio styles reflecting risk appetite and reasoning dynamics; and\n(3) some LLMs effectively leverage live signals to adapt decisions. These\nfindings expose a gap between static evaluation and real-world competence,\nmotivating benchmarks that test sequential decision making and consistency\nunder live uncertainty.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) achieve strong performance across\nbenchmarks--from knowledge quizzes and math reasoning to web-agent tasks--but\nthese tests occur in static settings, lacking real dynamics and uncertainty.\nConsequently, they evaluate isolated reasoning or problem-solving rather than\ndecision-making under uncertainty. To address this, we introduce\nLiveTradeBench, a live trading environment for evaluating LLM agents in\nrealistic and evolving markets. LiveTradeBench follows three design principles:\n(i) Live data streaming of market prices and news, eliminating dependence on\noffline backtesting and preventing information leakage while capturing\nreal-time uncertainty; (ii) a portfolio-management abstraction that extends\ncontrol from single-asset actions to multi-asset allocation, integrating risk\nmanagement and cross-asset reasoning; and (iii) multi-market evaluation across\nstructurally distinct environments--U.S. stocks and Polymarket prediction\nmarkets--differing in volatility, liquidity, and information flow. At each\nstep, an agent observes prices, news, and its portfolio, then outputs\npercentage allocations that balance risk and return. Using LiveTradeBench, we\nrun 50-day live evaluations of 21 LLMs across families. Results show that (1)\nhigh LMArena scores do not imply superior trading outcomes; (2) models display\ndistinct portfolio styles reflecting risk appetite and reasoning dynamics; and\n(3) some LLMs effectively leverage live signals to adapt decisions. These\nfindings expose a gap between static evaluation and real-world competence,\nmotivating benchmarks that test sequential decision making and consistency\nunder live uncertainty."
                },
                "authors": [
                    {
                        "name": "Haofei Yu"
                    },
                    {
                        "name": "Fenghai Li"
                    },
                    {
                        "name": "Jiaxuan You"
                    }
                ],
                "author_detail": {
                    "name": "Jiaxuan You"
                },
                "author": "Jiaxuan You",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03628v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03628v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.TR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.TR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21205v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21205v2",
                "updated": "2025-11-05T16:42:11Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    16,
                    42,
                    11,
                    2,
                    309,
                    0
                ],
                "published": "2025-04-29T22:22:44Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    22,
                    22,
                    44,
                    1,
                    119,
                    0
                ],
                "title": "SecRepoBench: Benchmarking Code Agents for Secure Code Completion in\n  Real-World Repositories",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SecRepoBench: Benchmarking Code Agents for Secure Code Completion in\n  Real-World Repositories"
                },
                "summary": "This paper introduces SecRepoBench, a benchmark to evaluate code agents on\nsecure code completion in real-world repositories. SecRepoBench has 318 code\ncompletion tasks in 27 C/C++ repositories, covering 15 CWEs. We evaluate 28\nstandalone LLMs and 13 code agents across 3 state-of-the-art agent frameworks\nusing our benchmark. We find that state-of-the-art LLMs struggle with\ngenerating correct and secure code completions. However, code agents\nsignificantly outperform standalone LLMs. We show that SecRepoBench is more\ndifficult than the prior state-of-the-art benchmark. Finally, our comprehensive\nanalysis provides insights into potential directions for enhancing the ability\nof code agents to write correct and secure code in real-world repositories.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces SecRepoBench, a benchmark to evaluate code agents on\nsecure code completion in real-world repositories. SecRepoBench has 318 code\ncompletion tasks in 27 C/C++ repositories, covering 15 CWEs. We evaluate 28\nstandalone LLMs and 13 code agents across 3 state-of-the-art agent frameworks\nusing our benchmark. We find that state-of-the-art LLMs struggle with\ngenerating correct and secure code completions. However, code agents\nsignificantly outperform standalone LLMs. We show that SecRepoBench is more\ndifficult than the prior state-of-the-art benchmark. Finally, our comprehensive\nanalysis provides insights into potential directions for enhancing the ability\nof code agents to write correct and secure code in real-world repositories."
                },
                "authors": [
                    {
                        "name": "Chihao Shen"
                    },
                    {
                        "name": "Connor Dilgren"
                    },
                    {
                        "name": "Purva Chiniya"
                    },
                    {
                        "name": "Luke Griffith"
                    },
                    {
                        "name": "Yu Ding"
                    },
                    {
                        "name": "Yizheng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yizheng Chen"
                },
                "author": "Yizheng Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21205v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21205v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21600v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21600v2",
                "updated": "2025-11-05T16:39:11Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    16,
                    39,
                    11,
                    2,
                    309,
                    0
                ],
                "published": "2025-05-27T16:57:20Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    16,
                    57,
                    20,
                    1,
                    147,
                    0
                ],
                "title": "R2R: Efficiently Navigating Divergent Reasoning Paths with Small-Large\n  Model Token Routing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "R2R: Efficiently Navigating Divergent Reasoning Paths with Small-Large\n  Model Token Routing"
                },
                "summary": "Large Language Models (LLMs) achieve impressive reasoning capabilities at the\ncost of substantial inference overhead, posing substantial deployment\nchallenges. Although distilled Small Language Models (SLMs) significantly\nenhance efficiency, their performance suffers as they fail to follow LLMs'\nreasoning paths. Luckily, we reveal that only a small fraction of tokens\ngenuinely diverge reasoning paths between LLMs and SLMs. Most generated tokens\nare either identical or exhibit neutral differences, such as minor variations\nin abbreviations or expressions. Leveraging this insight, we introduce **Roads\nto Rome (R2R)**, a neural token routing method that selectively utilizes LLMs\nonly for these critical, path-divergent tokens, while leaving the majority of\ntoken generation to the SLM. We also develop an automatic data generation\npipeline that identifies divergent tokens and generates token-level routing\nlabels to train the lightweight router. We apply R2R to combine R1-1.5B and\nR1-32B models from the DeepSeek family, and evaluate on challenging math,\ncoding, and QA benchmarks. With an average activated parameter size of 5.6B,\nR2R surpasses the average accuracy of R1-7B by 1.6x, outperforming even the\nR1-14B model. Compared to R1-32B, it delivers a 2.8x wall-clock speedup with\ncomparable performance, advancing the Pareto frontier of test-time scaling\nefficiency. Our code is available at https://github.com/thu-nics/R2R.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) achieve impressive reasoning capabilities at the\ncost of substantial inference overhead, posing substantial deployment\nchallenges. Although distilled Small Language Models (SLMs) significantly\nenhance efficiency, their performance suffers as they fail to follow LLMs'\nreasoning paths. Luckily, we reveal that only a small fraction of tokens\ngenuinely diverge reasoning paths between LLMs and SLMs. Most generated tokens\nare either identical or exhibit neutral differences, such as minor variations\nin abbreviations or expressions. Leveraging this insight, we introduce **Roads\nto Rome (R2R)**, a neural token routing method that selectively utilizes LLMs\nonly for these critical, path-divergent tokens, while leaving the majority of\ntoken generation to the SLM. We also develop an automatic data generation\npipeline that identifies divergent tokens and generates token-level routing\nlabels to train the lightweight router. We apply R2R to combine R1-1.5B and\nR1-32B models from the DeepSeek family, and evaluate on challenging math,\ncoding, and QA benchmarks. With an average activated parameter size of 5.6B,\nR2R surpasses the average accuracy of R1-7B by 1.6x, outperforming even the\nR1-14B model. Compared to R1-32B, it delivers a 2.8x wall-clock speedup with\ncomparable performance, advancing the Pareto frontier of test-time scaling\nefficiency. Our code is available at https://github.com/thu-nics/R2R."
                },
                "authors": [
                    {
                        "name": "Tianyu Fu"
                    },
                    {
                        "name": "Yi Ge"
                    },
                    {
                        "name": "Yichen You"
                    },
                    {
                        "name": "Enshu Liu"
                    },
                    {
                        "name": "Zhihang Yuan"
                    },
                    {
                        "name": "Guohao Dai"
                    },
                    {
                        "name": "Shengen Yan"
                    },
                    {
                        "name": "Huazhong Yang"
                    },
                    {
                        "name": "Yu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wang"
                },
                "author": "Yu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21600v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21600v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03612v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03612v1",
                "updated": "2025-11-05T16:29:20Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    16,
                    29,
                    20,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T16:29:20Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    16,
                    29,
                    20,
                    2,
                    309,
                    0
                ],
                "title": "3D Cooperative User Tracking for Distributed Integrated Sensing and\n  Communication",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D Cooperative User Tracking for Distributed Integrated Sensing and\n  Communication"
                },
                "summary": "As integrated sensing and communication (ISAC) becomes an integral part of 6G\nnetworks, distributed ISAC (DISAC) is expected to enhance both sensing and\ncommunication performance through its decentralized architecture. This paper\npresents a complete framework to address the challenge of cooperative user\ntracking in DISAC systems. By incorporating a global probability hypothesis\ndensity (PHD) filter and a field-of-view-aware access point (AP) management\nstrategy, the framework enables accurate user tracking using radio signals\nwhile optimizing AP scheduling. In addition, a real-world distributed MIMO\nchannel measurement campaign is performed to evaluate the effectiveness of the\nframework. The results demonstrate that a centimeter-level root mean-square\ntrajectory error can be achieved. Furthermore, the results show that it is not\nnecessary to keep APs active at all times to maintain high tracking accuracy,\nindicating the need for robust and efficient AP management. These findings\nprovide valuable insight into practical deployments and further development of\ncooperative user tracking techniques in DISAC systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As integrated sensing and communication (ISAC) becomes an integral part of 6G\nnetworks, distributed ISAC (DISAC) is expected to enhance both sensing and\ncommunication performance through its decentralized architecture. This paper\npresents a complete framework to address the challenge of cooperative user\ntracking in DISAC systems. By incorporating a global probability hypothesis\ndensity (PHD) filter and a field-of-view-aware access point (AP) management\nstrategy, the framework enables accurate user tracking using radio signals\nwhile optimizing AP scheduling. In addition, a real-world distributed MIMO\nchannel measurement campaign is performed to evaluate the effectiveness of the\nframework. The results demonstrate that a centimeter-level root mean-square\ntrajectory error can be achieved. Furthermore, the results show that it is not\nnecessary to keep APs active at all times to maintain high tracking accuracy,\nindicating the need for robust and efficient AP management. These findings\nprovide valuable insight into practical deployments and further development of\ncooperative user tracking techniques in DISAC systems."
                },
                "authors": [
                    {
                        "name": "Yingjie Xu"
                    },
                    {
                        "name": "Xuesong Cai"
                    },
                    {
                        "name": "Michiel Sandra"
                    },
                    {
                        "name": "Sara Willhammar"
                    },
                    {
                        "name": "Fredrik Tufvesson"
                    }
                ],
                "author_detail": {
                    "name": "Fredrik Tufvesson"
                },
                "author": "Fredrik Tufvesson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03612v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03612v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03601v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03601v1",
                "updated": "2025-11-05T16:22:19Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    16,
                    22,
                    19,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T16:22:19Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    16,
                    22,
                    19,
                    2,
                    309,
                    0
                ],
                "title": "Step-Audio-EditX Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Step-Audio-EditX Technical Report"
                },
                "summary": "We present Step-Audio-EditX, the first open-source LLM-based audio model\nexcelling at expressive and iterative audio editing encompassing emotion,\nspeaking style, and paralinguistics alongside robust zero-shot text-to-speech\n(TTS) capabilities.Our core innovation lies in leveraging only large-margin\nsynthetic data, which circumvents the need for embedding-based priors or\nauxiliary modules. This large-margin learning approach enables both iterative\ncontrol and high expressivity across voices, and represents a fundamental pivot\nfrom the conventional focus on representation-level disentanglement. Evaluation\nresults demonstrate that Step-Audio-EditX surpasses both MiniMax-2.6-hd and\nDoubao-Seed-TTS-2.0 in emotion editing and other fine-grained control tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Step-Audio-EditX, the first open-source LLM-based audio model\nexcelling at expressive and iterative audio editing encompassing emotion,\nspeaking style, and paralinguistics alongside robust zero-shot text-to-speech\n(TTS) capabilities.Our core innovation lies in leveraging only large-margin\nsynthetic data, which circumvents the need for embedding-based priors or\nauxiliary modules. This large-margin learning approach enables both iterative\ncontrol and high expressivity across voices, and represents a fundamental pivot\nfrom the conventional focus on representation-level disentanglement. Evaluation\nresults demonstrate that Step-Audio-EditX surpasses both MiniMax-2.6-hd and\nDoubao-Seed-TTS-2.0 in emotion editing and other fine-grained control tasks."
                },
                "authors": [
                    {
                        "name": "Chao Yan"
                    },
                    {
                        "name": "Boyong Wu"
                    },
                    {
                        "name": "Peng Yang"
                    },
                    {
                        "name": "Pengfei Tan"
                    },
                    {
                        "name": "Guoqiang Hu"
                    },
                    {
                        "name": "Yuxin Zhang"
                    },
                    {
                        "name": "Xiangyu"
                    },
                    {
                        "name": "Zhang"
                    },
                    {
                        "name": "Fei Tian"
                    },
                    {
                        "name": "Xuerui Yang"
                    },
                    {
                        "name": "Xiangyu Zhang"
                    },
                    {
                        "name": "Daxin Jiang"
                    },
                    {
                        "name": "Gang Yu"
                    }
                ],
                "author_detail": {
                    "name": "Gang Yu"
                },
                "arxiv_affiliation": "Tony",
                "author": "Gang Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03601v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03601v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10594v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10594v2",
                "updated": "2025-11-05T16:07:58Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    16,
                    7,
                    58,
                    2,
                    309,
                    0
                ],
                "published": "2025-09-12T14:59:52Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    14,
                    59,
                    52,
                    4,
                    255,
                    0
                ],
                "title": "SME-TEAM: Leveraging Trust and Ethics for Secure and Responsible Use of\n  AI and LLMs in SMEs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SME-TEAM: Leveraging Trust and Ethics for Secure and Responsible Use of\n  AI and LLMs in SMEs"
                },
                "summary": "Artificial Intelligence (AI) and Large Language Models (LLMs) are\nrevolutionizing today's business practices; however, their adoption within\nsmall and medium-sized enterprises (SMEs) raises serious trust, ethical, and\ntechnical issues. In this perspective paper, we introduce a structured,\nmulti-phased framework, \"SME-TEAM\" for the secure and responsible use of these\ntechnologies in SMEs. Based on a conceptual structure of four key pillars,\ni.e., Data, Algorithms, Human Oversight, and Model Architecture, SME-TEAM\nbridges theoretical ethical principles with operational practice, enhancing AI\ncapabilities across a wide range of applications in SMEs. Ultimately, this\npaper provides a structured roadmap for the adoption of these emerging\ntechnologies, positioning trust and ethics as a driving force for resilience,\ncompetitiveness, and sustainable innovation within the area of business\nanalytics and SMEs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial Intelligence (AI) and Large Language Models (LLMs) are\nrevolutionizing today's business practices; however, their adoption within\nsmall and medium-sized enterprises (SMEs) raises serious trust, ethical, and\ntechnical issues. In this perspective paper, we introduce a structured,\nmulti-phased framework, \"SME-TEAM\" for the secure and responsible use of these\ntechnologies in SMEs. Based on a conceptual structure of four key pillars,\ni.e., Data, Algorithms, Human Oversight, and Model Architecture, SME-TEAM\nbridges theoretical ethical principles with operational practice, enhancing AI\ncapabilities across a wide range of applications in SMEs. Ultimately, this\npaper provides a structured roadmap for the adoption of these emerging\ntechnologies, positioning trust and ethics as a driving force for resilience,\ncompetitiveness, and sustainable innovation within the area of business\nanalytics and SMEs."
                },
                "authors": [
                    {
                        "name": "Iqbal H. Sarker"
                    },
                    {
                        "name": "Helge Janicke"
                    },
                    {
                        "name": "Ahmad Mohsin"
                    },
                    {
                        "name": "Leandros Maglaras"
                    }
                ],
                "author_detail": {
                    "name": "Leandros Maglaras"
                },
                "author": "Leandros Maglaras",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10594v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10594v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03586v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03586v1",
                "updated": "2025-11-05T16:05:26Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    16,
                    5,
                    26,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T16:05:26Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    16,
                    5,
                    26,
                    2,
                    309,
                    0
                ],
                "title": "PerfDojo: Automated ML Library Generation for Heterogeneous\n  Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PerfDojo: Automated ML Library Generation for Heterogeneous\n  Architectures"
                },
                "summary": "The increasing complexity of machine learning models and the proliferation of\ndiverse hardware architectures (CPUs, GPUs, accelerators) make achieving\noptimal performance a significant challenge. Heterogeneity in instruction sets,\nspecialized kernel requirements for different data types and model features\n(e.g., sparsity, quantization), and architecture-specific optimizations\ncomplicate performance tuning. Manual optimization is resource-intensive, while\nexisting automatic approaches often rely on complex hardware-specific\nheuristics and uninterpretable intermediate representations, hindering\nperformance portability. We introduce PerfLLM, a novel automatic optimization\nmethodology leveraging Large Language Models (LLMs) and Reinforcement Learning\n(RL). Central to this is PerfDojo, an environment framing optimization as an RL\ngame using a human-readable, mathematically-inspired code representation that\nguarantees semantic validity through transformations. This allows effective\noptimization without prior hardware knowledge, facilitating both human analysis\nand RL agent training. We demonstrate PerfLLM's ability to achieve significant\nperformance gains across diverse CPU (x86, Arm, RISC-V) and GPU architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing complexity of machine learning models and the proliferation of\ndiverse hardware architectures (CPUs, GPUs, accelerators) make achieving\noptimal performance a significant challenge. Heterogeneity in instruction sets,\nspecialized kernel requirements for different data types and model features\n(e.g., sparsity, quantization), and architecture-specific optimizations\ncomplicate performance tuning. Manual optimization is resource-intensive, while\nexisting automatic approaches often rely on complex hardware-specific\nheuristics and uninterpretable intermediate representations, hindering\nperformance portability. We introduce PerfLLM, a novel automatic optimization\nmethodology leveraging Large Language Models (LLMs) and Reinforcement Learning\n(RL). Central to this is PerfDojo, an environment framing optimization as an RL\ngame using a human-readable, mathematically-inspired code representation that\nguarantees semantic validity through transformations. This allows effective\noptimization without prior hardware knowledge, facilitating both human analysis\nand RL agent training. We demonstrate PerfLLM's ability to achieve significant\nperformance gains across diverse CPU (x86, Arm, RISC-V) and GPU architectures."
                },
                "authors": [
                    {
                        "name": "Andrei Ivanov"
                    },
                    {
                        "name": "Siyuan Shen"
                    },
                    {
                        "name": "Gioele Gottardo"
                    },
                    {
                        "name": "Marcin Chrapek"
                    },
                    {
                        "name": "Afif Boudaoud"
                    },
                    {
                        "name": "Timo Schneider"
                    },
                    {
                        "name": "Luca Benini"
                    },
                    {
                        "name": "Torsten Hoefler"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Hoefler"
                },
                "author": "Torsten Hoefler",
                "arxiv_doi": "10.1145/3712285.3759900",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3712285.3759900",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2511.03586v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03586v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "The International Conference for High Performance Computing,\n  Networking, Storage and Analysis (SC '25), November 16--21, 2025, St Louis,\n  MO, USA",
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03571v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03571v1",
                "updated": "2025-11-05T15:51:42Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    15,
                    51,
                    42,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T15:51:42Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    15,
                    51,
                    42,
                    2,
                    309,
                    0
                ],
                "title": "OneOcc: Semantic Occupancy Prediction for Legged Robots with a Single\n  Panoramic Camera",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OneOcc: Semantic Occupancy Prediction for Legged Robots with a Single\n  Panoramic Camera"
                },
                "summary": "Robust 3D semantic occupancy is crucial for legged/humanoid robots, yet most\nsemantic scene completion (SSC) systems target wheeled platforms with\nforward-facing sensors. We present OneOcc, a vision-only panoramic SSC\nframework designed for gait-introduced body jitter and 360{\\deg} continuity.\nOneOcc combines: (i) Dual-Projection fusion (DP-ER) to exploit the annular\npanorama and its equirectangular unfolding, preserving 360{\\deg} continuity and\ngrid alignment; (ii) Bi-Grid Voxelization (BGV) to reason in Cartesian and\ncylindrical-polar spaces, reducing discretization bias and sharpening\nfree/occupied boundaries; (iii) a lightweight decoder with Hierarchical AMoE-3D\nfor dynamic multi-scale fusion and better long-range/occlusion reasoning; and\n(iv) plug-and-play Gait Displacement Compensation (GDC) learning feature-level\nmotion correction without extra sensors. We also release two panoramic\noccupancy benchmarks: QuadOcc (real quadruped, first-person 360{\\deg}) and\nHuman360Occ (H3O) (CARLA human-ego 360{\\deg} with RGB, Depth, semantic\noccupancy; standardized within-/cross-city splits). OneOcc sets new\nstate-of-the-art (SOTA): on QuadOcc it beats strong vision baselines and\npopular LiDAR ones; on H3O it gains +3.83 mIoU (within-city) and +8.08\n(cross-city). Modules are lightweight, enabling deployable full-surround\nperception for legged/humanoid robots. Datasets and code will be publicly\navailable at https://github.com/MasterHow/OneOcc.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust 3D semantic occupancy is crucial for legged/humanoid robots, yet most\nsemantic scene completion (SSC) systems target wheeled platforms with\nforward-facing sensors. We present OneOcc, a vision-only panoramic SSC\nframework designed for gait-introduced body jitter and 360{\\deg} continuity.\nOneOcc combines: (i) Dual-Projection fusion (DP-ER) to exploit the annular\npanorama and its equirectangular unfolding, preserving 360{\\deg} continuity and\ngrid alignment; (ii) Bi-Grid Voxelization (BGV) to reason in Cartesian and\ncylindrical-polar spaces, reducing discretization bias and sharpening\nfree/occupied boundaries; (iii) a lightweight decoder with Hierarchical AMoE-3D\nfor dynamic multi-scale fusion and better long-range/occlusion reasoning; and\n(iv) plug-and-play Gait Displacement Compensation (GDC) learning feature-level\nmotion correction without extra sensors. We also release two panoramic\noccupancy benchmarks: QuadOcc (real quadruped, first-person 360{\\deg}) and\nHuman360Occ (H3O) (CARLA human-ego 360{\\deg} with RGB, Depth, semantic\noccupancy; standardized within-/cross-city splits). OneOcc sets new\nstate-of-the-art (SOTA): on QuadOcc it beats strong vision baselines and\npopular LiDAR ones; on H3O it gains +3.83 mIoU (within-city) and +8.08\n(cross-city). Modules are lightweight, enabling deployable full-surround\nperception for legged/humanoid robots. Datasets and code will be publicly\navailable at https://github.com/MasterHow/OneOcc."
                },
                "authors": [
                    {
                        "name": "Hao Shi"
                    },
                    {
                        "name": "Ze Wang"
                    },
                    {
                        "name": "Shangwei Guo"
                    },
                    {
                        "name": "Mengfei Duan"
                    },
                    {
                        "name": "Song Wang"
                    },
                    {
                        "name": "Teng Chen"
                    },
                    {
                        "name": "Kailun Yang"
                    },
                    {
                        "name": "Lin Wang"
                    },
                    {
                        "name": "Kaiwei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Kaiwei Wang"
                },
                "author": "Kaiwei Wang",
                "arxiv_comment": "Datasets and code will be publicly available at\n  https://github.com/MasterHow/OneOcc",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03571v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03571v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03570v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03570v1",
                "updated": "2025-11-05T15:51:03Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    15,
                    51,
                    3,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T15:51:03Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    15,
                    51,
                    3,
                    2,
                    309,
                    0
                ],
                "title": "TabGemma: Text-Based Tabular ICL via LLM using Continued Pretraining and\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TabGemma: Text-Based Tabular ICL via LLM using Continued Pretraining and\n  Retrieval"
                },
                "summary": "We study LLMs for tabular prediction with mixed text, numeric, and\ncategorical fields. We introduce TabGemma, a schema-agnostic in-context learner\nthat treats rows as sequences and tackles two practical hurdles when adapting\npretrained LLMs for tabular predictions: unstable numeric tokenization and\nlimited context size. We propose to canonicalize numbers via signed scientific\nnotation and continue pretraining of a 12B Gemma 3 model with a target\nimputation objective using a large-scale real world dataset. For inference, we\nuse a compact n-gram-based retrieval to select informative exemplars that fit\nwithin a 128k-token window.\n  On semantically rich benchmarks, TabGemma establishes a new state of the art\non classification across low- and high-data regimes and improves monotonically\nwith more context rows. For regression, it is competitive at small sample sizes\nbut trails conventional approaches as data grows. Our results show that LLMs\ncan be effective tabular in-context learners on highly semantic tasks when\npaired with dedicated numeric handling and context retrieval, while motivating\nfurther advances in numeric modeling and long-context scaling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study LLMs for tabular prediction with mixed text, numeric, and\ncategorical fields. We introduce TabGemma, a schema-agnostic in-context learner\nthat treats rows as sequences and tackles two practical hurdles when adapting\npretrained LLMs for tabular predictions: unstable numeric tokenization and\nlimited context size. We propose to canonicalize numbers via signed scientific\nnotation and continue pretraining of a 12B Gemma 3 model with a target\nimputation objective using a large-scale real world dataset. For inference, we\nuse a compact n-gram-based retrieval to select informative exemplars that fit\nwithin a 128k-token window.\n  On semantically rich benchmarks, TabGemma establishes a new state of the art\non classification across low- and high-data regimes and improves monotonically\nwith more context rows. For regression, it is competitive at small sample sizes\nbut trails conventional approaches as data grows. Our results show that LLMs\ncan be effective tabular in-context learners on highly semantic tasks when\npaired with dedicated numeric handling and context retrieval, while motivating\nfurther advances in numeric modeling and long-context scaling."
                },
                "authors": [
                    {
                        "name": "Gnther Schindler"
                    },
                    {
                        "name": "Maximilian Schambach"
                    },
                    {
                        "name": "Michael Medek"
                    },
                    {
                        "name": "Sam Thelin"
                    }
                ],
                "author_detail": {
                    "name": "Sam Thelin"
                },
                "author": "Sam Thelin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03570v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03570v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03564v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03564v1",
                "updated": "2025-11-05T15:45:55Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    15,
                    45,
                    55,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T15:45:55Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    15,
                    45,
                    55,
                    2,
                    309,
                    0
                ],
                "title": "ENDF/B-VIII.1: Updated Nuclear Reaction Data Library for Science and\n  Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ENDF/B-VIII.1: Updated Nuclear Reaction Data Library for Science and\n  Applications"
                },
                "summary": "The ENDF/B-VIII.1 library is the newest recommended evaluated nuclear data\nfile by the Cross Section Evaluation Working Group (CSEWG) for use in nuclear\nscience and technology applications, and incorporates advances made in the six\nyears since the release of ENDF/B-VIII.0. Among key advances made are that the\n$^{239}$Pu file was reevaluated by a joint international effort and that\nupdated $^{16,18}$O, $^{19}$F, $^{28-30}$Si, $^{50-54}$Cr, $^{55}$Mn,\n$^{54,56,57}$Fe, $^{63,65}$Cu, $^{139}$La, $^{233,235,238}$U, and\n$^{240,241}$Pu neutron nuclear data from the IAEA coordinated INDEN\ncollaboration were adopted. Over 60 neutron dosimetry cross sections were\nadopted from the IAEA's IRDFF-II library. In addition, the new library includes\nsignificant changes for $^3$He, $^6$Li,$^9$Be, $^{51}$V, $^{88}$Sr, $^{103}$Rh,\n$^{140,142}$Ce, Dy, $^{181}$Ta, Pt, $^{206-208}$Pb, and $^{234,236}$U neutron\ndata, and new nuclear data for the photonuclear, charged-particle and atomic\nsublibraries. Numerous thermal neutron scattering kernels were reevaluated or\nprovided for the very first time. On the covariance side, work was undertaken\nto introduce better uncertainty quantification standards and testing for\nnuclear data covariances. The significant effort to reevaluate important\nnuclides has reduced bias in the simulations of many integral experiments with\nparticular progress noted for fluorine, copper, and stainless steel containing\nbenchmarks. Data issues hindered the successful deployment of the previous\nENDF/B-VIII.0 for commercial nuclear power applications in high burnup\nsituations. These issues were addressed by improving the $^{238}$U and\n$^{239,240,241}$Pu evaluated data in the resonance region. The new library\nperformance as a function of burnup is similar to the reference ENDF/B-VII.1\nlibrary. The ENDF/B-VIII.1 data are available in ENDF-6 and GNDS format at\nhttps://doi.org/10.11578/endf/2571019.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ENDF/B-VIII.1 library is the newest recommended evaluated nuclear data\nfile by the Cross Section Evaluation Working Group (CSEWG) for use in nuclear\nscience and technology applications, and incorporates advances made in the six\nyears since the release of ENDF/B-VIII.0. Among key advances made are that the\n$^{239}$Pu file was reevaluated by a joint international effort and that\nupdated $^{16,18}$O, $^{19}$F, $^{28-30}$Si, $^{50-54}$Cr, $^{55}$Mn,\n$^{54,56,57}$Fe, $^{63,65}$Cu, $^{139}$La, $^{233,235,238}$U, and\n$^{240,241}$Pu neutron nuclear data from the IAEA coordinated INDEN\ncollaboration were adopted. Over 60 neutron dosimetry cross sections were\nadopted from the IAEA's IRDFF-II library. In addition, the new library includes\nsignificant changes for $^3$He, $^6$Li,$^9$Be, $^{51}$V, $^{88}$Sr, $^{103}$Rh,\n$^{140,142}$Ce, Dy, $^{181}$Ta, Pt, $^{206-208}$Pb, and $^{234,236}$U neutron\ndata, and new nuclear data for the photonuclear, charged-particle and atomic\nsublibraries. Numerous thermal neutron scattering kernels were reevaluated or\nprovided for the very first time. On the covariance side, work was undertaken\nto introduce better uncertainty quantification standards and testing for\nnuclear data covariances. The significant effort to reevaluate important\nnuclides has reduced bias in the simulations of many integral experiments with\nparticular progress noted for fluorine, copper, and stainless steel containing\nbenchmarks. Data issues hindered the successful deployment of the previous\nENDF/B-VIII.0 for commercial nuclear power applications in high burnup\nsituations. These issues were addressed by improving the $^{238}$U and\n$^{239,240,241}$Pu evaluated data in the resonance region. The new library\nperformance as a function of burnup is similar to the reference ENDF/B-VII.1\nlibrary. The ENDF/B-VIII.1 data are available in ENDF-6 and GNDS format at\nhttps://doi.org/10.11578/endf/2571019."
                },
                "authors": [
                    {
                        "name": "G. P. A. Nobre"
                    },
                    {
                        "name": "R. Capote"
                    },
                    {
                        "name": "M. T. Pigni"
                    },
                    {
                        "name": "A. Trkov"
                    },
                    {
                        "name": "C. M. Mattoon"
                    },
                    {
                        "name": "D. Neudecker"
                    },
                    {
                        "name": "D. A. Brown"
                    },
                    {
                        "name": "M. B. Chadwick"
                    },
                    {
                        "name": "A. C. Kahler"
                    },
                    {
                        "name": "N. A. Kleedtke"
                    },
                    {
                        "name": "M. Zerkle"
                    },
                    {
                        "name": "A. I. Hawari"
                    },
                    {
                        "name": "C. W. Chapman"
                    },
                    {
                        "name": "N. C. Fleming"
                    },
                    {
                        "name": "J. L. Wormald"
                    },
                    {
                        "name": "K. Rami"
                    },
                    {
                        "name": "Y. Danon"
                    },
                    {
                        "name": "N. A. Gibson"
                    },
                    {
                        "name": "P. Brain"
                    },
                    {
                        "name": "M. W. Paris"
                    },
                    {
                        "name": "G. M. Hale"
                    },
                    {
                        "name": "I. J. Thompson"
                    },
                    {
                        "name": "D. P. Barry"
                    },
                    {
                        "name": "I. Stetcu"
                    },
                    {
                        "name": "W. Haeck"
                    },
                    {
                        "name": "A. E. Lovell"
                    },
                    {
                        "name": "M. R. Mumpower"
                    },
                    {
                        "name": "G. Potel"
                    },
                    {
                        "name": "K. Kravvaris"
                    },
                    {
                        "name": "G. Noguere"
                    },
                    {
                        "name": "J. D. McDonnell"
                    },
                    {
                        "name": "A. D. Carlson"
                    },
                    {
                        "name": "M. Dunn"
                    },
                    {
                        "name": "T. Kawano"
                    },
                    {
                        "name": "D. Wiarda"
                    },
                    {
                        "name": "I. Al-Qasir"
                    },
                    {
                        "name": "G. Arbanas"
                    },
                    {
                        "name": "R. Arcilla"
                    },
                    {
                        "name": "B. Beck"
                    },
                    {
                        "name": "D. Bernard"
                    },
                    {
                        "name": "R. Beyer"
                    },
                    {
                        "name": "J. M. Brown"
                    },
                    {
                        "name": "O. Cabellos"
                    },
                    {
                        "name": "R. J. Casperson"
                    },
                    {
                        "name": "Y. Cheng"
                    },
                    {
                        "name": "E. V. Chimanski"
                    },
                    {
                        "name": "R. Coles"
                    },
                    {
                        "name": "M. Cornock"
                    },
                    {
                        "name": "J. Cotchen"
                    },
                    {
                        "name": "J. P. W. Crozier"
                    },
                    {
                        "name": "D. E. Cullen"
                    },
                    {
                        "name": "A. Daskalakis"
                    },
                    {
                        "name": "M. -A. Descalle"
                    },
                    {
                        "name": "D. D. DiJulio"
                    },
                    {
                        "name": "P. Dimitriou"
                    },
                    {
                        "name": "A. C. Dreyfuss"
                    },
                    {
                        "name": "I. Durn"
                    },
                    {
                        "name": "R. Ferrer"
                    },
                    {
                        "name": "T. Gaines"
                    },
                    {
                        "name": "V. Gillette"
                    },
                    {
                        "name": "G. Gert"
                    },
                    {
                        "name": "K. H. Guber"
                    },
                    {
                        "name": "J. D. Haverkamp"
                    },
                    {
                        "name": "M. W. Herman"
                    },
                    {
                        "name": "J. Holmes"
                    },
                    {
                        "name": "M. Hursin"
                    },
                    {
                        "name": "N. Jisrawi"
                    },
                    {
                        "name": "A. R. Junghans"
                    },
                    {
                        "name": "K. J. Kelly"
                    },
                    {
                        "name": "H. I. Kim"
                    },
                    {
                        "name": "K. S. Kim"
                    },
                    {
                        "name": "A. J. Koning"
                    },
                    {
                        "name": "M. Kotl"
                    },
                    {
                        "name": "B. K. Laramee"
                    },
                    {
                        "name": "A. Lauer-Coles"
                    },
                    {
                        "name": "L. Leal"
                    },
                    {
                        "name": "H. Y. Lee"
                    },
                    {
                        "name": "A. M. Lewis"
                    },
                    {
                        "name": "J. Malec"
                    },
                    {
                        "name": "J. I. Mrquez Damin"
                    },
                    {
                        "name": "W. J. Marshall"
                    },
                    {
                        "name": "A. Mattera"
                    },
                    {
                        "name": "G. Muhrer"
                    },
                    {
                        "name": "A. Ney"
                    },
                    {
                        "name": "W. E. Ormand"
                    },
                    {
                        "name": "D. K. Parsons"
                    },
                    {
                        "name": "C. M. Percher"
                    },
                    {
                        "name": "V. G. Pronyaev"
                    },
                    {
                        "name": "A. Qteish"
                    },
                    {
                        "name": "S. Quaglioni"
                    },
                    {
                        "name": "M. Rapp"
                    },
                    {
                        "name": "J. J. Ressler"
                    },
                    {
                        "name": "M. Rising"
                    },
                    {
                        "name": "D. Rochman"
                    },
                    {
                        "name": "P. K. Romano"
                    },
                    {
                        "name": "D. Roubtsov"
                    },
                    {
                        "name": "G. Schnabel"
                    },
                    {
                        "name": "M. Schulc"
                    },
                    {
                        "name": "G. J. Siemers"
                    },
                    {
                        "name": "A. A. Sonzogni"
                    },
                    {
                        "name": "P. Talou"
                    },
                    {
                        "name": "J. Thompson"
                    },
                    {
                        "name": "T. H. Trumbull"
                    },
                    {
                        "name": "S. C. van der Marck"
                    },
                    {
                        "name": "M. Vorabbi"
                    },
                    {
                        "name": "C. Wemple"
                    },
                    {
                        "name": "K. A. Wendt"
                    },
                    {
                        "name": "M. White"
                    },
                    {
                        "name": "R. Q. Wright"
                    }
                ],
                "author_detail": {
                    "name": "R. Q. Wright"
                },
                "author": "R. Q. Wright",
                "arxiv_comment": "Article associated with the ENDF/B-VIII.1 release, submitted to\n  Nuclear Data Sheets and currently under second round of referee review. 222\n  pages, 61 tables, 227 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03564v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03564v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nucl-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nucl-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03563v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03563v1",
                "updated": "2025-11-05T15:45:52Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    15,
                    45,
                    52,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T15:45:52Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    15,
                    45,
                    52,
                    2,
                    309,
                    0
                ],
                "title": "ASVRI-Legal: Fine-Tuning LLMs with Retrieval Augmented Generation for\n  Enhanced Legal Regulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASVRI-Legal: Fine-Tuning LLMs with Retrieval Augmented Generation for\n  Enhanced Legal Regulation"
                },
                "summary": "In this study, we explore the fine-tuning of Large Language Models (LLMs) to\nbetter support policymakers in their crucial work of understanding, analyzing,\nand crafting legal regulations. To equip the model with a deep understanding of\nlegal texts, we curated a supervised dataset tailored to the specific needs of\nthe legal domain. Additionally, we integrated the Retrieval-Augmented\nGeneration (RAG) method, enabling the LLM to access and incorporate up-to-date\nlegal knowledge from external sources. This combination of fine-tuning and\nRAG-based augmentation results in a tool that not only processes legal\ninformation but actively assists policymakers in interpreting regulations and\ndrafting new ones that align with current needs. The results demonstrate that\nthis approach can significantly enhance the effectiveness of legal research and\nregulation development, offering a valuable resource in the ever-evolving field\nof law.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we explore the fine-tuning of Large Language Models (LLMs) to\nbetter support policymakers in their crucial work of understanding, analyzing,\nand crafting legal regulations. To equip the model with a deep understanding of\nlegal texts, we curated a supervised dataset tailored to the specific needs of\nthe legal domain. Additionally, we integrated the Retrieval-Augmented\nGeneration (RAG) method, enabling the LLM to access and incorporate up-to-date\nlegal knowledge from external sources. This combination of fine-tuning and\nRAG-based augmentation results in a tool that not only processes legal\ninformation but actively assists policymakers in interpreting regulations and\ndrafting new ones that align with current needs. The results demonstrate that\nthis approach can significantly enhance the effectiveness of legal research and\nregulation development, offering a valuable resource in the ever-evolving field\nof law."
                },
                "authors": [
                    {
                        "name": "One Octadion"
                    },
                    {
                        "name": "Bondan Sapta Prakoso"
                    },
                    {
                        "name": "Nanang Yudi Setiawan"
                    },
                    {
                        "name": "Novanto Yudistira"
                    }
                ],
                "author_detail": {
                    "name": "Novanto Yudistira"
                },
                "author": "Novanto Yudistira",
                "arxiv_comment": "11 pages (including references), 2 figures, 4 tables, published in\n  Atlantis Press (Open Access under CC BY-NC 4.0 license)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03563v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03563v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07329v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07329v2",
                "updated": "2025-11-05T15:35:21Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    15,
                    35,
                    21,
                    2,
                    309,
                    0
                ],
                "published": "2025-03-10T13:42:04Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    13,
                    42,
                    4,
                    0,
                    69,
                    0
                ],
                "title": "Assessing the Macro and Micro Effects of Random Seeds on Fine-Tuning\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing the Macro and Micro Effects of Random Seeds on Fine-Tuning\n  Large Language Models"
                },
                "summary": "The impact of random seeds in fine-tuning large language models (LLMs) has\nbeen largely overlooked despite its potential influence on model performance.In\nthis study, we systematically evaluate the effects of random seeds on LLMs\nusing the GLUE and SuperGLUE benchmarks. We analyze the macro-level impact\nthrough traditional metrics like accuracy and F1, calculating their mean and\nvariance to quantify performance fluctuations. To capture the micro-level\neffects, we introduce a novel metric, consistency, measuring the stability of\nindividual predictions across runs. Our experiments reveal significant variance\nat both macro and micro levels, underscoring the need for careful consideration\nof random seeds in fine-tuning and evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The impact of random seeds in fine-tuning large language models (LLMs) has\nbeen largely overlooked despite its potential influence on model performance.In\nthis study, we systematically evaluate the effects of random seeds on LLMs\nusing the GLUE and SuperGLUE benchmarks. We analyze the macro-level impact\nthrough traditional metrics like accuracy and F1, calculating their mean and\nvariance to quantify performance fluctuations. To capture the micro-level\neffects, we introduce a novel metric, consistency, measuring the stability of\nindividual predictions across runs. Our experiments reveal significant variance\nat both macro and micro levels, underscoring the need for careful consideration\nof random seeds in fine-tuning and evaluation."
                },
                "authors": [
                    {
                        "name": "Nghia Bui"
                    },
                    {
                        "name": "Guergana Savova"
                    },
                    {
                        "name": "Lijing Wang"
                    }
                ],
                "author_detail": {
                    "name": "Lijing Wang"
                },
                "author": "Lijing Wang",
                "arxiv_comment": "7 pages, 5 tables, 3 figures. Accepted at IJCNLP 2025. This is the\n  final, peer-reviewed version of the work, which supersedes and extends the\n  unauthorized draft previously posted as arXiv:2503.07329",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07329v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07329v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16301v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16301v2",
                "updated": "2025-11-05T15:35:06Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    15,
                    35,
                    6,
                    2,
                    309,
                    0
                ],
                "published": "2024-10-07T06:30:59Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    6,
                    30,
                    59,
                    0,
                    281,
                    0
                ],
                "title": "Intelligent Computing Social Modeling and Methodological Innovations in\n  Political Science in the Era of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intelligent Computing Social Modeling and Methodological Innovations in\n  Political Science in the Era of Large Language Models"
                },
                "summary": "The recent wave of artificial intelligence, epitomized by large language\nmodels (LLMs),has presented opportunities and challenges for methodological\ninnovation in political science,sparking discussions on a potential paradigm\nshift in the social sciences. However, how can weunderstand the impact of LLMs\non knowledge production and paradigm transformation in thesocial sciences from\na comprehensive perspective that integrates technology and methodology? What\nare LLMs' specific applications and representative innovative methods in\npolitical scienceresearch? These questions, particularly from a practical\nmethodological standpoint, remainunderexplored. This paper proposes the\n\"Intelligent Computing Social Modeling\" (ICSM) methodto address these issues by\nclarifying the critical mechanisms of LLMs. ICSM leverages thestrengths of LLMs\nin idea synthesis and action simulation, advancing intellectual exploration\ninpolitical science through \"simulated social construction\" and \"simulation\nvalidation.\" Bysimulating the U.S. presidential election, this study\nempirically demonstrates the operationalpathways and methodological advantages\nof ICSM. By integrating traditional social scienceparadigms, ICSM not only\nenhances the quantitative paradigm's capability to apply big data toassess the\nimpact of factors but also provides qualitative paradigms with evidence for\nsocialmechanism discovery at the individual level, offering a powerful tool\nthat balances interpretabilityand predictability in social science research.\nThe findings suggest that LLMs will drivemethodological innovation in political\nscience through integration and improvement rather thandirect substitution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent wave of artificial intelligence, epitomized by large language\nmodels (LLMs),has presented opportunities and challenges for methodological\ninnovation in political science,sparking discussions on a potential paradigm\nshift in the social sciences. However, how can weunderstand the impact of LLMs\non knowledge production and paradigm transformation in thesocial sciences from\na comprehensive perspective that integrates technology and methodology? What\nare LLMs' specific applications and representative innovative methods in\npolitical scienceresearch? These questions, particularly from a practical\nmethodological standpoint, remainunderexplored. This paper proposes the\n\"Intelligent Computing Social Modeling\" (ICSM) methodto address these issues by\nclarifying the critical mechanisms of LLMs. ICSM leverages thestrengths of LLMs\nin idea synthesis and action simulation, advancing intellectual exploration\ninpolitical science through \"simulated social construction\" and \"simulation\nvalidation.\" Bysimulating the U.S. presidential election, this study\nempirically demonstrates the operationalpathways and methodological advantages\nof ICSM. By integrating traditional social scienceparadigms, ICSM not only\nenhances the quantitative paradigm's capability to apply big data toassess the\nimpact of factors but also provides qualitative paradigms with evidence for\nsocialmechanism discovery at the individual level, offering a powerful tool\nthat balances interpretabilityand predictability in social science research.\nThe findings suggest that LLMs will drivemethodological innovation in political\nscience through integration and improvement rather thandirect substitution."
                },
                "authors": [
                    {
                        "name": "Zhenyu Wang"
                    },
                    {
                        "name": "Dequan Wang"
                    },
                    {
                        "name": "Yi Xu"
                    },
                    {
                        "name": "Lingfeng Zhou"
                    },
                    {
                        "name": "Yiqi Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Yiqi Zhou"
                },
                "author": "Yiqi Zhou",
                "arxiv_doi": "10.1007/s11366-025-09917-6",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s11366-025-09917-6",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.16301v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16301v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "37 pages, 11 figures, 3 tables. J OF CHIN POLIT SCI (2025)",
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03553v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03553v1",
                "updated": "2025-11-05T15:34:48Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    15,
                    34,
                    48,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T15:34:48Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    15,
                    34,
                    48,
                    2,
                    309,
                    0
                ],
                "title": "MultiZebraLogic: A Multilingual Logical Reasoning Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MultiZebraLogic: A Multilingual Logical Reasoning Benchmark"
                },
                "summary": "Measuring the full abilities of large language models (LLMs) requires\nbenchmarks representing multiple tasks. We aim to create large, high-quality\ndatasets for comparison of logical reasoning skills across several languages\nand of suitable difficulty for LLMs of various reasoning ability. We explore\nmultiple ways of increasing difficulty. We generate zebra puzzles in multiple\nlanguages, themes, sizes and including 14 different clue types and 8 red\nherring types (uninformative clues). We find puzzle sizes 2x3 and 4x5 are\nsufficiently challenging for GPT-4o mini (a non-reasoning model) and o3-mini (a\nreasoning model), respectively. Including 5 red herrings decreases o3-mini\npuzzle-level accuracy on 4x5 puzzles by 15$\\pm$7 %. Scores of o3-mini on 4x5\npuzzles are not significantly affected by use of English vs. Danish or the\ncommon houses theme vs. the country-specific smoerrebroed theme. We find no\ncorrelation between difficulty and the selected clue types. Datasets of\n128+1024 puzzles are published as MultiZebraLogic in each of nine Germanic\nlanguages for sizes 2x3 and 4x5. We publish code for puzzle generation,\ndesigned for adaptablity into more languages and themes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring the full abilities of large language models (LLMs) requires\nbenchmarks representing multiple tasks. We aim to create large, high-quality\ndatasets for comparison of logical reasoning skills across several languages\nand of suitable difficulty for LLMs of various reasoning ability. We explore\nmultiple ways of increasing difficulty. We generate zebra puzzles in multiple\nlanguages, themes, sizes and including 14 different clue types and 8 red\nherring types (uninformative clues). We find puzzle sizes 2x3 and 4x5 are\nsufficiently challenging for GPT-4o mini (a non-reasoning model) and o3-mini (a\nreasoning model), respectively. Including 5 red herrings decreases o3-mini\npuzzle-level accuracy on 4x5 puzzles by 15$\\pm$7 %. Scores of o3-mini on 4x5\npuzzles are not significantly affected by use of English vs. Danish or the\ncommon houses theme vs. the country-specific smoerrebroed theme. We find no\ncorrelation between difficulty and the selected clue types. Datasets of\n128+1024 puzzles are published as MultiZebraLogic in each of nine Germanic\nlanguages for sizes 2x3 and 4x5. We publish code for puzzle generation,\ndesigned for adaptablity into more languages and themes."
                },
                "authors": [
                    {
                        "name": "Sofie Helene Bruun"
                    },
                    {
                        "name": "Dan Saattrup Smart"
                    }
                ],
                "author_detail": {
                    "name": "Dan Saattrup Smart"
                },
                "author": "Dan Saattrup Smart",
                "arxiv_comment": "Submitted to LREC 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03553v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03553v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03549v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03549v1",
                "updated": "2025-11-05T15:31:42Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    15,
                    31,
                    42,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T15:31:42Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    15,
                    31,
                    42,
                    2,
                    309,
                    0
                ],
                "title": "Uncovering Code Insights: Leveraging GitHub Artifacts for Deeper Code\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncovering Code Insights: Leveraging GitHub Artifacts for Deeper Code\n  Understanding"
                },
                "summary": "Understanding the purpose of source code is a critical task in software\nmaintenance, onboarding, and modernization. While large language models (LLMs)\nhave shown promise in generating code explanations, they often lack grounding\nin the broader software engineering context. We propose a novel approach that\nleverages natural language artifacts from GitHub -- such as pull request\ndescriptions, issue descriptions and discussions, and commit messages -- to\nenhance LLM-based code understanding. Our system consists of three components:\none that extracts and structures relevant GitHub context, another that uses\nthis context to generate high-level explanations of the code's purpose, and a\nthird that validates the explanation. We implemented this as a standalone tool,\nas well as a server within the Model Context Protocol (MCP), enabling\nintegration with other AI-assisted development tools. Our main use case is that\nof enhancing a standard LLM-based code explanation with code insights that our\nsystem generates. To evaluate explanations' quality, we conducted a small scale\nuser study, with developers of several open projects, as well as developers of\nproprietary projects. Our user study indicates that when insights are generated\nthey often are helpful and non trivial, and are free from hallucinations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the purpose of source code is a critical task in software\nmaintenance, onboarding, and modernization. While large language models (LLMs)\nhave shown promise in generating code explanations, they often lack grounding\nin the broader software engineering context. We propose a novel approach that\nleverages natural language artifacts from GitHub -- such as pull request\ndescriptions, issue descriptions and discussions, and commit messages -- to\nenhance LLM-based code understanding. Our system consists of three components:\none that extracts and structures relevant GitHub context, another that uses\nthis context to generate high-level explanations of the code's purpose, and a\nthird that validates the explanation. We implemented this as a standalone tool,\nas well as a server within the Model Context Protocol (MCP), enabling\nintegration with other AI-assisted development tools. Our main use case is that\nof enhancing a standard LLM-based code explanation with code insights that our\nsystem generates. To evaluate explanations' quality, we conducted a small scale\nuser study, with developers of several open projects, as well as developers of\nproprietary projects. Our user study indicates that when insights are generated\nthey often are helpful and non trivial, and are free from hallucinations."
                },
                "authors": [
                    {
                        "name": "Ziv Nevo"
                    },
                    {
                        "name": "Orna Raz"
                    },
                    {
                        "name": "Karen Yorav"
                    }
                ],
                "author_detail": {
                    "name": "Karen Yorav"
                },
                "author": "Karen Yorav",
                "arxiv_comment": "7 pages, 6 figures, to be published in AISM 2025, see\n  https://aism25.github.io/aism25/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03549v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03549v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03542v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03542v1",
                "updated": "2025-11-05T15:15:35Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    15,
                    15,
                    35,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T15:15:35Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    15,
                    15,
                    35,
                    2,
                    309,
                    0
                ],
                "title": "SOLVE-Med: Specialized Orchestration for Leading Vertical Experts across\n  Medical Specialties",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SOLVE-Med: Specialized Orchestration for Leading Vertical Experts across\n  Medical Specialties"
                },
                "summary": "Medical question answering systems face deployment challenges including\nhallucinations, bias, computational demands, privacy concerns, and the need for\nspecialized expertise across diverse domains. Here, we present SOLVE-Med, a\nmulti-agent architecture combining domain-specialized small language models for\ncomplex medical queries. The system employs a Router Agent for dynamic\nspecialist selection, ten specialized models (1B parameters each) fine-tuned on\nspecific medical domains, and an Orchestrator Agent that synthesizes responses.\nEvaluated on Italian medical forum data across ten specialties, SOLVE-Med\nachieves superior performance with ROUGE-1 of 0.301 and BERTScore F1 of 0.697,\noutperforming standalone models up to 14B parameters while enabling local\ndeployment. Our code is publicly available on GitHub:\nhttps://github.com/PRAISELab-PicusLab/SOLVE-Med.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medical question answering systems face deployment challenges including\nhallucinations, bias, computational demands, privacy concerns, and the need for\nspecialized expertise across diverse domains. Here, we present SOLVE-Med, a\nmulti-agent architecture combining domain-specialized small language models for\ncomplex medical queries. The system employs a Router Agent for dynamic\nspecialist selection, ten specialized models (1B parameters each) fine-tuned on\nspecific medical domains, and an Orchestrator Agent that synthesizes responses.\nEvaluated on Italian medical forum data across ten specialties, SOLVE-Med\nachieves superior performance with ROUGE-1 of 0.301 and BERTScore F1 of 0.697,\noutperforming standalone models up to 14B parameters while enabling local\ndeployment. Our code is publicly available on GitHub:\nhttps://github.com/PRAISELab-PicusLab/SOLVE-Med."
                },
                "authors": [
                    {
                        "name": "Roberta Di Marino"
                    },
                    {
                        "name": "Giovanni Dioguardi"
                    },
                    {
                        "name": "Antonio Romano"
                    },
                    {
                        "name": "Giuseppe Riccio"
                    },
                    {
                        "name": "Mariano Barone"
                    },
                    {
                        "name": "Marco Postiglione"
                    },
                    {
                        "name": "Flora Amato"
                    },
                    {
                        "name": "Vincenzo Moscato"
                    }
                ],
                "author_detail": {
                    "name": "Vincenzo Moscato"
                },
                "author": "Vincenzo Moscato",
                "arxiv_doi": "10.3233/FAIA251438",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3233/FAIA251438",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2511.03542v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03542v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "28th European Conference on Artificial Intelligence, 25-30 October\n  2025, Bologna, Italy",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06733v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06733v2",
                "updated": "2025-11-05T15:14:29Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    15,
                    14,
                    29,
                    2,
                    309,
                    0
                ],
                "published": "2025-09-08T14:27:23Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    14,
                    27,
                    23,
                    0,
                    251,
                    0
                ],
                "title": "Reinforcement Learning Foundations for Deep Research Systems: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning Foundations for Deep Research Systems: A Survey"
                },
                "summary": "Deep research systems, agentic AI that solve complex, multi-step tasks by\ncoordinating reasoning, search across the open web and user files, and tool\nuse, are moving toward hierarchical deployments with a Planner, Coordinator,\nand Executors. In practice, training entire stacks end-to-end remains\nimpractical, so most work trains a single planner connected to core tools such\nas search, browsing, and code. While SFT imparts protocol fidelity, it suffers\nfrom imitation and exposure biases and underuses environment feedback.\nPreference alignment methods such as DPO are schema and proxy-dependent,\noff-policy, and weak for long-horizon credit assignment and multi-objective\ntrade-offs. A further limitation of SFT and DPO is their reliance on human\ndefined decision points and subskills through schema design and labeled\ncomparisons. Reinforcement learning aligns with closed-loop, tool-interaction\nresearch by optimizing trajectory-level policies, enabling exploration,\nrecovery behaviors, and principled credit assignment, and it reduces dependence\non such human priors and rater biases.\n  This survey is, to our knowledge, the first dedicated to the RL foundations\nof deep research systems. It systematizes recent work along three axes: (i)\ndata synthesis and curation; (ii) RL methods for agentic research covering\nstability, sample efficiency, long context handling, reward and credit design,\nmulti-objective optimization, and multimodal integration; and (iii) agentic RL\ntraining systems and frameworks. We also cover agent architecture and\ncoordination, as well as evaluation and benchmarks, including recent QA, VQA,\nlong-form synthesis, and domain-grounded, tool-interaction tasks. We distill\nrecurring patterns, surface infrastructure bottlenecks, and offer practical\nguidance for training robust, transparent deep research agents with RL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep research systems, agentic AI that solve complex, multi-step tasks by\ncoordinating reasoning, search across the open web and user files, and tool\nuse, are moving toward hierarchical deployments with a Planner, Coordinator,\nand Executors. In practice, training entire stacks end-to-end remains\nimpractical, so most work trains a single planner connected to core tools such\nas search, browsing, and code. While SFT imparts protocol fidelity, it suffers\nfrom imitation and exposure biases and underuses environment feedback.\nPreference alignment methods such as DPO are schema and proxy-dependent,\noff-policy, and weak for long-horizon credit assignment and multi-objective\ntrade-offs. A further limitation of SFT and DPO is their reliance on human\ndefined decision points and subskills through schema design and labeled\ncomparisons. Reinforcement learning aligns with closed-loop, tool-interaction\nresearch by optimizing trajectory-level policies, enabling exploration,\nrecovery behaviors, and principled credit assignment, and it reduces dependence\non such human priors and rater biases.\n  This survey is, to our knowledge, the first dedicated to the RL foundations\nof deep research systems. It systematizes recent work along three axes: (i)\ndata synthesis and curation; (ii) RL methods for agentic research covering\nstability, sample efficiency, long context handling, reward and credit design,\nmulti-objective optimization, and multimodal integration; and (iii) agentic RL\ntraining systems and frameworks. We also cover agent architecture and\ncoordination, as well as evaluation and benchmarks, including recent QA, VQA,\nlong-form synthesis, and domain-grounded, tool-interaction tasks. We distill\nrecurring patterns, surface infrastructure bottlenecks, and offer practical\nguidance for training robust, transparent deep research agents with RL."
                },
                "authors": [
                    {
                        "name": "Wenjun Li"
                    },
                    {
                        "name": "Zhi Chen"
                    },
                    {
                        "name": "Jingru Lin"
                    },
                    {
                        "name": "Hannan Cao"
                    },
                    {
                        "name": "Wei Han"
                    },
                    {
                        "name": "Sheng Liang"
                    },
                    {
                        "name": "Zhi Zhang"
                    },
                    {
                        "name": "Kuicai Dong"
                    },
                    {
                        "name": "Dexun Li"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Yong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Liu"
                },
                "author": "Yong Liu",
                "arxiv_comment": "39 pages, second version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06733v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06733v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03538v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03538v1",
                "updated": "2025-11-05T15:08:55Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    15,
                    8,
                    55,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T15:08:55Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    15,
                    8,
                    55,
                    2,
                    309,
                    0
                ],
                "title": "Security and Privacy Management of IoT Using Quantum Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Security and Privacy Management of IoT Using Quantum Computing"
                },
                "summary": "The convergence of the Internet of Things (IoT) and quantum computing is\nredefining the security paradigm of interconnected digital systems. Classical\ncryptographic algorithms such as RSA, Elliptic Curve Cryptography (ECC), and\nAdvanced Encryption Standard (AES) have long provided the foundation for\nsecuring IoT communication. However, the emergence of quantum algorithms such\nas Shor's and Grover's threatens to render these techniques vulnerable,\nnecessitating the development of quantum-resilient alternatives. This chapter\nexamines the implications of quantum computing for IoT security and explores\nstrategies for building cryptographically robust systems in the post-quantum\nera. It presents an overview of Post-Quantum Cryptographic (PQC) families,\nincluding lattice-based, code-based, hash-based, and multivariate approaches,\nanalyzing their potential for deployment in resource-constrained IoT\nenvironments. In addition, quantum-based methods such as Quantum Key\nDistribution (QKD) and Quantum Random Number Generators (QRNGs) are discussed\nfor their ability to enhance confidentiality and privacy through physics-based\nsecurity guarantees. The chapter also highlights issues of privacy management,\nregulatory compliance, and standardization, emphasizing the need for\ncollaborative efforts across academia, industry, and governance. Overall, it\nprovides a comprehensive perspective on security IoT ecosystems against quantum\nthreats and ensures resilience in the next generation of intelligent networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The convergence of the Internet of Things (IoT) and quantum computing is\nredefining the security paradigm of interconnected digital systems. Classical\ncryptographic algorithms such as RSA, Elliptic Curve Cryptography (ECC), and\nAdvanced Encryption Standard (AES) have long provided the foundation for\nsecuring IoT communication. However, the emergence of quantum algorithms such\nas Shor's and Grover's threatens to render these techniques vulnerable,\nnecessitating the development of quantum-resilient alternatives. This chapter\nexamines the implications of quantum computing for IoT security and explores\nstrategies for building cryptographically robust systems in the post-quantum\nera. It presents an overview of Post-Quantum Cryptographic (PQC) families,\nincluding lattice-based, code-based, hash-based, and multivariate approaches,\nanalyzing their potential for deployment in resource-constrained IoT\nenvironments. In addition, quantum-based methods such as Quantum Key\nDistribution (QKD) and Quantum Random Number Generators (QRNGs) are discussed\nfor their ability to enhance confidentiality and privacy through physics-based\nsecurity guarantees. The chapter also highlights issues of privacy management,\nregulatory compliance, and standardization, emphasizing the need for\ncollaborative efforts across academia, industry, and governance. Overall, it\nprovides a comprehensive perspective on security IoT ecosystems against quantum\nthreats and ensures resilience in the next generation of intelligent networks."
                },
                "authors": [
                    {
                        "name": "Jaydip Sen"
                    }
                ],
                "author_detail": {
                    "name": "Jaydip Sen"
                },
                "author": "Jaydip Sen",
                "arxiv_comment": "This is a preprint of the chapter. It will be published by Springer,\n  Singapore, in \"Quantum Computing, Sensing and Communications for IoT\" edited\n  by Suyel Namasudra, Kemal Akkaya and Nirmalya Kar. Link to the final\n  authenticated version will be shared as soon as the chapter is published. The\n  current version has 55 pages, 15 figures, and 10 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03538v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03538v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02625v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02625v3",
                "updated": "2025-11-05T15:07:16Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    15,
                    7,
                    16,
                    2,
                    309,
                    0
                ],
                "published": "2025-01-05T18:41:54Z",
                "published_parsed": [
                    2025,
                    1,
                    5,
                    18,
                    41,
                    54,
                    6,
                    5,
                    0
                ],
                "title": "HALO: Hadamard-Assisted Lower-Precision Optimization for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HALO: Hadamard-Assisted Lower-Precision Optimization for LLMs"
                },
                "summary": "Quantized training of Large Language Models (LLMs) remains an open challenge,\nas maintaining accuracy while performing all matrix multiplications in low\nprecision has proven difficult. This is particularly the case when fine-tuning\npre-trained models, which can have large weight and activation outlier values\nthat make lower-precision optimization difficult. To address this, we present\nHALO, a novel quantization-aware training approach for Transformers that\nenables accurate and efficient low-precision training by combining 1) strategic\nplacement of Hadamard rotations in both forward and backward passes, which\nmitigate outliers, 2) high-performance kernel support, and 3) FSDP integration\nfor low-precision communication. Our approach ensures that all large matrix\nmultiplications during the forward and backward passes are executed in lower\nprecision. Applied to LLAMA-family models, HALO achieves\nnear-full-precision-equivalent results during fine-tuning on various tasks,\nwhile delivering up to 1.41x end-to-end speedup for full fine-tuning on RTX\n4090 GPUs. HALO efficiently supports both standard and parameterefficient\nfine-tuning (PEFT). Our results demonstrate the first practical approach to\nfully quantized LLM fine-tuning that maintains accuracy in 8-bit precision,\nwhile delivering performance benefits. Code is available at\nhttps://github.com/IST-DASLab/HALO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantized training of Large Language Models (LLMs) remains an open challenge,\nas maintaining accuracy while performing all matrix multiplications in low\nprecision has proven difficult. This is particularly the case when fine-tuning\npre-trained models, which can have large weight and activation outlier values\nthat make lower-precision optimization difficult. To address this, we present\nHALO, a novel quantization-aware training approach for Transformers that\nenables accurate and efficient low-precision training by combining 1) strategic\nplacement of Hadamard rotations in both forward and backward passes, which\nmitigate outliers, 2) high-performance kernel support, and 3) FSDP integration\nfor low-precision communication. Our approach ensures that all large matrix\nmultiplications during the forward and backward passes are executed in lower\nprecision. Applied to LLAMA-family models, HALO achieves\nnear-full-precision-equivalent results during fine-tuning on various tasks,\nwhile delivering up to 1.41x end-to-end speedup for full fine-tuning on RTX\n4090 GPUs. HALO efficiently supports both standard and parameterefficient\nfine-tuning (PEFT). Our results demonstrate the first practical approach to\nfully quantized LLM fine-tuning that maintains accuracy in 8-bit precision,\nwhile delivering performance benefits. Code is available at\nhttps://github.com/IST-DASLab/HALO."
                },
                "authors": [
                    {
                        "name": "Saleh Ashkboos"
                    },
                    {
                        "name": "Mahdi Nikdan"
                    },
                    {
                        "name": "Soroush Tabesh"
                    },
                    {
                        "name": "Roberto L. Castro"
                    },
                    {
                        "name": "Torsten Hoefler"
                    },
                    {
                        "name": "Dan Alistarh"
                    }
                ],
                "author_detail": {
                    "name": "Dan Alistarh"
                },
                "author": "Dan Alistarh",
                "arxiv_comment": "19 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02625v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02625v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03534v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03534v1",
                "updated": "2025-11-05T15:06:39Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    15,
                    6,
                    39,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T15:06:39Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    15,
                    6,
                    39,
                    2,
                    309,
                    0
                ],
                "title": "PnPSelect: Plug-and-play IoT Device Selection Using Ultra-wideband\n  Signals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PnPSelect: Plug-and-play IoT Device Selection Using Ultra-wideband\n  Signals"
                },
                "summary": "In recent years, the number of Internet of Things (IoT) devices in smart\nhomes has rapidly increased. A key challenge affecting user experience is how\nto enable users to efficiently and intuitively select the devices they wish to\ncontrol. This paper proposes PnPSelect, a plug-and-play IoT device selection\nsolution utilizing Ultra-wideband (UWB) technology on commercial devices.\nUnlike previous works, PnPSelect does not require the installation of dedicated\nhardware on each IoT device, thereby reducing deployment costs and\ncomplexities, and achieving true plug-and-play functionality. To enable\nintuitive device selection, we introduce a pointing direction estimation method\nthat utilizes UWB readings from a single anchor to infer the user pointing\ndirection. Additionally, we propose a lightweight device localization method\nthat allows users to register new IoT devices by simply pointing at them from\ntwo distinct positions, eliminating the need for manual measurements. We\nimplement PnPSelect on commercial smartphones and smartwatches and conduct\nextensive evaluations in both controlled laboratory settings and real-world\nenvironments. Our results demonstrate high accuracy, robustness, and\nadaptability, making PnPSelect a practical and scalable solution for\nnext-generation smart home interactions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the number of Internet of Things (IoT) devices in smart\nhomes has rapidly increased. A key challenge affecting user experience is how\nto enable users to efficiently and intuitively select the devices they wish to\ncontrol. This paper proposes PnPSelect, a plug-and-play IoT device selection\nsolution utilizing Ultra-wideband (UWB) technology on commercial devices.\nUnlike previous works, PnPSelect does not require the installation of dedicated\nhardware on each IoT device, thereby reducing deployment costs and\ncomplexities, and achieving true plug-and-play functionality. To enable\nintuitive device selection, we introduce a pointing direction estimation method\nthat utilizes UWB readings from a single anchor to infer the user pointing\ndirection. Additionally, we propose a lightweight device localization method\nthat allows users to register new IoT devices by simply pointing at them from\ntwo distinct positions, eliminating the need for manual measurements. We\nimplement PnPSelect on commercial smartphones and smartwatches and conduct\nextensive evaluations in both controlled laboratory settings and real-world\nenvironments. Our results demonstrate high accuracy, robustness, and\nadaptability, making PnPSelect a practical and scalable solution for\nnext-generation smart home interactions."
                },
                "authors": [
                    {
                        "name": "Zhaoxin Chang"
                    },
                    {
                        "name": "Fusang Zhang"
                    },
                    {
                        "name": "Jie Xiong"
                    },
                    {
                        "name": "Ziyu Li"
                    },
                    {
                        "name": "Badii Jouaber"
                    },
                    {
                        "name": "Daqing Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Daqing Zhang"
                },
                "author": "Daqing Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03534v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03534v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.22336v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.22336v2",
                "updated": "2025-11-05T15:01:29Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    15,
                    1,
                    29,
                    2,
                    309,
                    0
                ],
                "published": "2025-10-25T15:40:18Z",
                "published_parsed": [
                    2025,
                    10,
                    25,
                    15,
                    40,
                    18,
                    5,
                    298,
                    0
                ],
                "title": "Toward Humanoid Brain-Body Co-design: Joint Optimization of Control and\n  Morphology for Fall Recovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward Humanoid Brain-Body Co-design: Joint Optimization of Control and\n  Morphology for Fall Recovery"
                },
                "summary": "Humanoid robots represent a central frontier in embodied intelligence, as\ntheir anthropomorphic form enables natural deployment in humans' workspace.\nBrain-body co-design for humanoids presents a promising approach to realizing\nthis potential by jointly optimizing control policies and physical morphology.\nWithin this context, fall recovery emerges as a critical capability. It not\nonly enhances safety and resilience but also integrates naturally with\nlocomotion systems, thereby advancing the autonomy of humanoids. In this paper,\nwe propose RoboCraft, a scalable humanoid co-design framework for fall recovery\nthat iteratively improves performance through the coupled updates of control\npolicy and morphology. A shared policy pretrained across multiple designs is\nprogressively finetuned on high-performing morphologies, enabling efficient\nadaptation without retraining from scratch. Concurrently, morphology search is\nguided by human-inspired priors and optimization algorithms, supported by a\npriority buffer that balances reevaluation of promising candidates with the\nexploration of novel designs. Experiments show that RoboCraft achieves an\naverage performance gain of 44.55% on seven public humanoid robots, with\nmorphology optimization drives at least 40% of improvements in co-designing\nfour humanoid robots, underscoring the critical role of humanoid co-design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Humanoid robots represent a central frontier in embodied intelligence, as\ntheir anthropomorphic form enables natural deployment in humans' workspace.\nBrain-body co-design for humanoids presents a promising approach to realizing\nthis potential by jointly optimizing control policies and physical morphology.\nWithin this context, fall recovery emerges as a critical capability. It not\nonly enhances safety and resilience but also integrates naturally with\nlocomotion systems, thereby advancing the autonomy of humanoids. In this paper,\nwe propose RoboCraft, a scalable humanoid co-design framework for fall recovery\nthat iteratively improves performance through the coupled updates of control\npolicy and morphology. A shared policy pretrained across multiple designs is\nprogressively finetuned on high-performing morphologies, enabling efficient\nadaptation without retraining from scratch. Concurrently, morphology search is\nguided by human-inspired priors and optimization algorithms, supported by a\npriority buffer that balances reevaluation of promising candidates with the\nexploration of novel designs. Experiments show that RoboCraft achieves an\naverage performance gain of 44.55% on seven public humanoid robots, with\nmorphology optimization drives at least 40% of improvements in co-designing\nfour humanoid robots, underscoring the critical role of humanoid co-design."
                },
                "authors": [
                    {
                        "name": "Bo Yue"
                    },
                    {
                        "name": "Sheng Xu"
                    },
                    {
                        "name": "Kui Jia"
                    },
                    {
                        "name": "Guiliang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Guiliang Liu"
                },
                "author": "Guiliang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.22336v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.22336v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09684v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09684v2",
                "updated": "2025-11-05T14:51:07Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    14,
                    51,
                    7,
                    2,
                    309,
                    0
                ],
                "published": "2025-06-11T13:02:17Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    13,
                    2,
                    17,
                    2,
                    162,
                    0
                ],
                "title": "Inv-Entropy: A Fully Probabilistic Framework for Uncertainty\n  Quantification in Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inv-Entropy: A Fully Probabilistic Framework for Uncertainty\n  Quantification in Language Models"
                },
                "summary": "Large language models (LLMs) have transformed natural language processing,\nbut their reliable deployment requires effective uncertainty quantification\n(UQ). Existing UQ methods are often heuristic and lack a probabilistic\ninterpretation. This paper begins by providing a theoretical justification for\nthe role of perturbations in UQ for LLMs. We then introduce a dual random walk\nperspective, modeling input-output pairs as two Markov chains with transition\nprobabilities defined by semantic similarity. Building on this, we propose a\nfully probabilistic framework based on an inverse model, which quantifies\nuncertainty by evaluating the diversity of the input space conditioned on a\ngiven output through systematic perturbations. Within this framework, we define\na new uncertainty measure, Inv-Entropy. A key strength of our framework is its\nflexibility: it supports various definitions of uncertainty measures,\nembeddings, perturbation strategies, and similarity metrics. We also propose\nGAAP, a perturbation algorithm based on genetic algorithms, which enhances the\ndiversity of sampled inputs. In addition, we introduce a new evaluation metric,\nTemperature Sensitivity of Uncertainty (TSU), which directly assesses\nuncertainty without relying on correctness as a proxy. Extensive experiments\ndemonstrate that Inv-Entropy outperforms existing semantic UQ methods. The code\nto reproduce the results can be found at\nhttps://github.com/UMDataScienceLab/Uncertainty-Quantification-for-LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have transformed natural language processing,\nbut their reliable deployment requires effective uncertainty quantification\n(UQ). Existing UQ methods are often heuristic and lack a probabilistic\ninterpretation. This paper begins by providing a theoretical justification for\nthe role of perturbations in UQ for LLMs. We then introduce a dual random walk\nperspective, modeling input-output pairs as two Markov chains with transition\nprobabilities defined by semantic similarity. Building on this, we propose a\nfully probabilistic framework based on an inverse model, which quantifies\nuncertainty by evaluating the diversity of the input space conditioned on a\ngiven output through systematic perturbations. Within this framework, we define\na new uncertainty measure, Inv-Entropy. A key strength of our framework is its\nflexibility: it supports various definitions of uncertainty measures,\nembeddings, perturbation strategies, and similarity metrics. We also propose\nGAAP, a perturbation algorithm based on genetic algorithms, which enhances the\ndiversity of sampled inputs. In addition, we introduce a new evaluation metric,\nTemperature Sensitivity of Uncertainty (TSU), which directly assesses\nuncertainty without relying on correctness as a proxy. Extensive experiments\ndemonstrate that Inv-Entropy outperforms existing semantic UQ methods. The code\nto reproduce the results can be found at\nhttps://github.com/UMDataScienceLab/Uncertainty-Quantification-for-LLMs."
                },
                "authors": [
                    {
                        "name": "Haoyi Song"
                    },
                    {
                        "name": "Ruihan Ji"
                    },
                    {
                        "name": "Naichen Shi"
                    },
                    {
                        "name": "Fan Lai"
                    },
                    {
                        "name": "Raed Al Kontar"
                    }
                ],
                "author_detail": {
                    "name": "Raed Al Kontar"
                },
                "author": "Raed Al Kontar",
                "arxiv_journal_ref": "NeurIPS, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09684v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09684v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03517v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03517v1",
                "updated": "2025-11-05T14:46:58Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    14,
                    46,
                    58,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T14:46:58Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    14,
                    46,
                    58,
                    2,
                    309,
                    0
                ],
                "title": "U2F: Encouraging SWE-Agent to Seize Novelty without Losing Feasibility",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "U2F: Encouraging SWE-Agent to Seize Novelty without Losing Feasibility"
                },
                "summary": "Large language models (LLMs) have shown strong capabilities in software\nengineering tasks, yet most existing LLM-based SWE-Agents mainly tackle\nwell-defined problems using conventional methods, often overlooking alternative\nor innovative solutions beyond their predefined frameworks. This limitation is\nevident in open-world software environments, where emerging challenges\ntranscend established paradigms.\n  We propose U2F (Unknown Unknowns to Functional solutions), a\ncognitive-inspired, uncertainty-embracing multi-agent framework that\nsystematically surfaces \"Unknown Unknowns\" - novel solution pathways absent\nfrom initial formulations but holding innovative potential. U2F consists of two\nkey components: (1) a Discovery-Exploration-Integration agent system for\nuncovering and synthesizing potential solutions, and (2) cognitive enhancement\nmechanisms across three dimensions: cross-domain analogical reasoning, reverse\nthinking, and external validation, which strategically reframe and extend\nconventional solution boundaries.\n  Applied to 218 real-world software enabler stories curated from authentic\nengineering tasks, U2F achieved notable improvements: human experts reported a\n14 percent increase in overall novelty, 51 percent improvement in semantic\nnovelty, and stable feasibility (4.02/5.0), corroborated by an LLM-based\nevaluator. These results highlight the potential of embracing uncertainty as a\ncatalyst for innovation in software engineering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown strong capabilities in software\nengineering tasks, yet most existing LLM-based SWE-Agents mainly tackle\nwell-defined problems using conventional methods, often overlooking alternative\nor innovative solutions beyond their predefined frameworks. This limitation is\nevident in open-world software environments, where emerging challenges\ntranscend established paradigms.\n  We propose U2F (Unknown Unknowns to Functional solutions), a\ncognitive-inspired, uncertainty-embracing multi-agent framework that\nsystematically surfaces \"Unknown Unknowns\" - novel solution pathways absent\nfrom initial formulations but holding innovative potential. U2F consists of two\nkey components: (1) a Discovery-Exploration-Integration agent system for\nuncovering and synthesizing potential solutions, and (2) cognitive enhancement\nmechanisms across three dimensions: cross-domain analogical reasoning, reverse\nthinking, and external validation, which strategically reframe and extend\nconventional solution boundaries.\n  Applied to 218 real-world software enabler stories curated from authentic\nengineering tasks, U2F achieved notable improvements: human experts reported a\n14 percent increase in overall novelty, 51 percent improvement in semantic\nnovelty, and stable feasibility (4.02/5.0), corroborated by an LLM-based\nevaluator. These results highlight the potential of embracing uncertainty as a\ncatalyst for innovation in software engineering."
                },
                "authors": [
                    {
                        "name": "Wencheng Ye"
                    },
                    {
                        "name": "Yan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yan Liu"
                },
                "author": "Yan Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03517v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03517v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03508v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03508v1",
                "updated": "2025-11-05T14:39:59Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    14,
                    39,
                    59,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T14:39:59Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    14,
                    39,
                    59,
                    2,
                    309,
                    0
                ],
                "title": "One Battle After Another: Probing LLMs' Limits on Multi-Turn Instruction\n  Following with a Benchmark Evolving Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One Battle After Another: Probing LLMs' Limits on Multi-Turn Instruction\n  Following with a Benchmark Evolving Framework"
                },
                "summary": "Understanding how well large language models can follow users' instructions\nthroughout a dialogue spanning multiple topics is of great importance for\ndata-intensive conversational applications. Existing benchmarks are often\nlimited to a fixed number of turns, making them susceptible to saturation and\nfailing to account for the user's interactive experience. In this work, we\npropose an extensible framework for assessing multi-turn instruction-following\nability. At its core, our framework decouples linguistic surface forms from\nuser intent simulation through a three-layer mechanism that tracks constraints,\ninstructions, and topics. This framework mimics User-LLM interaction by\nenabling the dynamic construction of benchmarks with state changes and\ntracebacks, terminating a conversation only when the model exhausts a simulated\nuser's patience. We define a suite of metrics capturing the quality of the\ninteraction process. Using this framework, we construct EvolIF, an evolving\ninstruction-following benchmark incorporating nine distinct constraint types.\nOur results indicate that GPT-5 exhibits superior instruction-following\nperformance. It sustains an average of 18.54 conversational turns and\ndemonstrates 70.31% robustness, outperforming Gemini-2.5-Pro by a significant\nmargin of 11.41%, while other models lag far behind. All of the data and code\nwill be made publicly available online.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding how well large language models can follow users' instructions\nthroughout a dialogue spanning multiple topics is of great importance for\ndata-intensive conversational applications. Existing benchmarks are often\nlimited to a fixed number of turns, making them susceptible to saturation and\nfailing to account for the user's interactive experience. In this work, we\npropose an extensible framework for assessing multi-turn instruction-following\nability. At its core, our framework decouples linguistic surface forms from\nuser intent simulation through a three-layer mechanism that tracks constraints,\ninstructions, and topics. This framework mimics User-LLM interaction by\nenabling the dynamic construction of benchmarks with state changes and\ntracebacks, terminating a conversation only when the model exhausts a simulated\nuser's patience. We define a suite of metrics capturing the quality of the\ninteraction process. Using this framework, we construct EvolIF, an evolving\ninstruction-following benchmark incorporating nine distinct constraint types.\nOur results indicate that GPT-5 exhibits superior instruction-following\nperformance. It sustains an average of 18.54 conversational turns and\ndemonstrates 70.31% robustness, outperforming Gemini-2.5-Pro by a significant\nmargin of 11.41%, while other models lag far behind. All of the data and code\nwill be made publicly available online."
                },
                "authors": [
                    {
                        "name": "Qi Jia"
                    },
                    {
                        "name": "Kaiwei Zhang"
                    },
                    {
                        "name": "Xiujie Song"
                    },
                    {
                        "name": "Ye Shen"
                    },
                    {
                        "name": "Xiangyang Zhu"
                    },
                    {
                        "name": "Guangtao Zhai"
                    }
                ],
                "author_detail": {
                    "name": "Guangtao Zhai"
                },
                "author": "Guangtao Zhai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03508v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03508v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03506v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03506v1",
                "updated": "2025-11-05T14:37:34Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    14,
                    37,
                    34,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T14:37:34Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    14,
                    37,
                    34,
                    2,
                    309,
                    0
                ],
                "title": "HaluMem: Evaluating Hallucinations in Memory Systems of Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HaluMem: Evaluating Hallucinations in Memory Systems of Agents"
                },
                "summary": "Memory systems are key components that enable AI systems such as LLMs and AI\nagents to achieve long-term learning and sustained interaction. However, during\nmemory storage and retrieval, these systems frequently exhibit memory\nhallucinations, including fabrication, errors, conflicts, and omissions.\nExisting evaluations of memory hallucinations are primarily end-to-end question\nanswering, which makes it difficult to localize the operational stage within\nthe memory system where hallucinations arise. To address this, we introduce the\nHallucination in Memory Benchmark (HaluMem), the first operation level\nhallucination evaluation benchmark tailored to memory systems. HaluMem defines\nthree evaluation tasks (memory extraction, memory updating, and memory question\nanswering) to comprehensively reveal hallucination behaviors across different\noperational stages of interaction. To support evaluation, we construct\nuser-centric, multi-turn human-AI interaction datasets, HaluMem-Medium and\nHaluMem-Long. Both include about 15k memory points and 3.5k multi-type\nquestions. The average dialogue length per user reaches 1.5k and 2.6k turns,\nwith context lengths exceeding 1M tokens, enabling evaluation of hallucinations\nacross different context scales and task complexities. Empirical studies based\non HaluMem show that existing memory systems tend to generate and accumulate\nhallucinations during the extraction and updating stages, which subsequently\npropagate errors to the question answering stage. Future research should focus\non developing interpretable and constrained memory operation mechanisms that\nsystematically suppress hallucinations and improve memory reliability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory systems are key components that enable AI systems such as LLMs and AI\nagents to achieve long-term learning and sustained interaction. However, during\nmemory storage and retrieval, these systems frequently exhibit memory\nhallucinations, including fabrication, errors, conflicts, and omissions.\nExisting evaluations of memory hallucinations are primarily end-to-end question\nanswering, which makes it difficult to localize the operational stage within\nthe memory system where hallucinations arise. To address this, we introduce the\nHallucination in Memory Benchmark (HaluMem), the first operation level\nhallucination evaluation benchmark tailored to memory systems. HaluMem defines\nthree evaluation tasks (memory extraction, memory updating, and memory question\nanswering) to comprehensively reveal hallucination behaviors across different\noperational stages of interaction. To support evaluation, we construct\nuser-centric, multi-turn human-AI interaction datasets, HaluMem-Medium and\nHaluMem-Long. Both include about 15k memory points and 3.5k multi-type\nquestions. The average dialogue length per user reaches 1.5k and 2.6k turns,\nwith context lengths exceeding 1M tokens, enabling evaluation of hallucinations\nacross different context scales and task complexities. Empirical studies based\non HaluMem show that existing memory systems tend to generate and accumulate\nhallucinations during the extraction and updating stages, which subsequently\npropagate errors to the question answering stage. Future research should focus\non developing interpretable and constrained memory operation mechanisms that\nsystematically suppress hallucinations and improve memory reliability."
                },
                "authors": [
                    {
                        "name": "Ding Chen"
                    },
                    {
                        "name": "Simin Niu"
                    },
                    {
                        "name": "Kehang Li"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Xiangping Zheng"
                    },
                    {
                        "name": "Bo Tang"
                    },
                    {
                        "name": "Xinchi Li"
                    },
                    {
                        "name": "Feiyu Xiong"
                    },
                    {
                        "name": "Zhiyu Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyu Li"
                },
                "author": "Zhiyu Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03506v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03506v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02558v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02558v2",
                "updated": "2025-11-05T14:29:12Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    14,
                    29,
                    12,
                    2,
                    309,
                    0
                ],
                "published": "2025-08-04T16:14:03Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    16,
                    14,
                    3,
                    0,
                    216,
                    0
                ],
                "title": "Sparse-dLLM: Accelerating Diffusion LLMs with Dynamic Cache Eviction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse-dLLM: Accelerating Diffusion LLMs with Dynamic Cache Eviction"
                },
                "summary": "Diffusion Large Language Models (dLLMs) enable breakthroughs in reasoning and\nparallel decoding but suffer from prohibitive quadratic computational\ncomplexity and memory overhead during inference. Current caching techniques\naccelerate decoding by storing full-layer states, yet impose substantial memory\nusage that limit long-context applications. Our analysis of attention patterns\nin dLLMs reveals persistent cross-layer sparsity, with pivotal tokens remaining\nsalient across decoding steps and low-relevance tokens staying unimportant,\nmotivating selective cache eviction. We propose Sparse-dLLM, the first\ntraining-free framework integrating dynamic cache eviction with sparse\nattention via delayed bidirectional sparse caching. By leveraging the stability\nof token saliency over steps, it retains critical tokens and dynamically evicts\nunimportant prefix/suffix entries using an attention-guided strategy. Extensive\nexperiments on LLaDA and Dream series demonstrate Sparse-dLLM achieves up to\n10$\\times$ higher throughput than vanilla dLLMs, with comparable performance\nand similar peak memory costs, outperforming previous methods in efficiency and\neffectiveness. The code is available at\nhttps://github.com/OpenMOSS/Sparse-dLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Large Language Models (dLLMs) enable breakthroughs in reasoning and\nparallel decoding but suffer from prohibitive quadratic computational\ncomplexity and memory overhead during inference. Current caching techniques\naccelerate decoding by storing full-layer states, yet impose substantial memory\nusage that limit long-context applications. Our analysis of attention patterns\nin dLLMs reveals persistent cross-layer sparsity, with pivotal tokens remaining\nsalient across decoding steps and low-relevance tokens staying unimportant,\nmotivating selective cache eviction. We propose Sparse-dLLM, the first\ntraining-free framework integrating dynamic cache eviction with sparse\nattention via delayed bidirectional sparse caching. By leveraging the stability\nof token saliency over steps, it retains critical tokens and dynamically evicts\nunimportant prefix/suffix entries using an attention-guided strategy. Extensive\nexperiments on LLaDA and Dream series demonstrate Sparse-dLLM achieves up to\n10$\\times$ higher throughput than vanilla dLLMs, with comparable performance\nand similar peak memory costs, outperforming previous methods in efficiency and\neffectiveness. The code is available at\nhttps://github.com/OpenMOSS/Sparse-dLLM."
                },
                "authors": [
                    {
                        "name": "Yuerong Song"
                    },
                    {
                        "name": "Xiaoran Liu"
                    },
                    {
                        "name": "Ruixiao Li"
                    },
                    {
                        "name": "Zhigeng Liu"
                    },
                    {
                        "name": "Zengfeng Huang"
                    },
                    {
                        "name": "Qipeng Guo"
                    },
                    {
                        "name": "Ziwei He"
                    },
                    {
                        "name": "Xipeng Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Xipeng Qiu"
                },
                "author": "Xipeng Qiu",
                "arxiv_comment": "12 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02558v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02558v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03497v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03497v1",
                "updated": "2025-11-05T14:27:58Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    14,
                    27,
                    58,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T14:27:58Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    14,
                    27,
                    58,
                    2,
                    309,
                    0
                ],
                "title": "ROSBag MCP Server: Analyzing Robot Data with LLMs for Agentic Embodied\n  AI Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ROSBag MCP Server: Analyzing Robot Data with LLMs for Agentic Embodied\n  AI Applications"
                },
                "summary": "Agentic AI systems and Physical or Embodied AI systems have been two key\nresearch verticals at the forefront of Artificial Intelligence and Robotics,\nwith Model Context Protocol (MCP) increasingly becoming a key component and\nenabler of agentic applications. However, the literature at the intersection of\nthese verticals, i.e., Agentic Embodied AI, remains scarce. This paper\nintroduces an MCP server for analyzing ROS and ROS 2 bags, allowing for\nanalyzing, visualizing and processing robot data with natural language through\nLLMs and VLMs. We describe specific tooling built with robotics domain\nknowledge, with our initial release focused on mobile robotics and supporting\nnatively the analysis of trajectories, laser scan data, transforms, or time\nseries data. This is in addition to providing an interface to standard ROS 2\nCLI tools (\"ros2 bag list\" or \"ros2 bag info\"), as well as the ability to\nfilter bags with a subset of topics or trimmed in time. Coupled with the MCP\nserver, we provide a lightweight UI that allows the benchmarking of the tooling\nwith different LLMs, both proprietary (Anthropic, OpenAI) and open-source\n(through Groq). Our experimental results include the analysis of tool calling\ncapabilities of eight different state-of-the-art LLM/VLM models, both\nproprietary and open-source, large and small. Our experiments indicate that\nthere is a large divide in tool calling capabilities, with Kimi K2 and Claude\nSonnet 4 demonstrating clearly superior performance. We also conclude that\nthere are multiple factors affecting the success rates, from the tool\ndescription schema to the number of arguments, as well as the number of tools\navailable to the models. The code is available with a permissive license at\nhttps://github.com/binabik-ai/mcp-rosbags.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic AI systems and Physical or Embodied AI systems have been two key\nresearch verticals at the forefront of Artificial Intelligence and Robotics,\nwith Model Context Protocol (MCP) increasingly becoming a key component and\nenabler of agentic applications. However, the literature at the intersection of\nthese verticals, i.e., Agentic Embodied AI, remains scarce. This paper\nintroduces an MCP server for analyzing ROS and ROS 2 bags, allowing for\nanalyzing, visualizing and processing robot data with natural language through\nLLMs and VLMs. We describe specific tooling built with robotics domain\nknowledge, with our initial release focused on mobile robotics and supporting\nnatively the analysis of trajectories, laser scan data, transforms, or time\nseries data. This is in addition to providing an interface to standard ROS 2\nCLI tools (\"ros2 bag list\" or \"ros2 bag info\"), as well as the ability to\nfilter bags with a subset of topics or trimmed in time. Coupled with the MCP\nserver, we provide a lightweight UI that allows the benchmarking of the tooling\nwith different LLMs, both proprietary (Anthropic, OpenAI) and open-source\n(through Groq). Our experimental results include the analysis of tool calling\ncapabilities of eight different state-of-the-art LLM/VLM models, both\nproprietary and open-source, large and small. Our experiments indicate that\nthere is a large divide in tool calling capabilities, with Kimi K2 and Claude\nSonnet 4 demonstrating clearly superior performance. We also conclude that\nthere are multiple factors affecting the success rates, from the tool\ndescription schema to the number of arguments, as well as the number of tools\navailable to the models. The code is available with a permissive license at\nhttps://github.com/binabik-ai/mcp-rosbags."
                },
                "authors": [
                    {
                        "name": "Lei Fu"
                    },
                    {
                        "name": "Sahar Salimpour"
                    },
                    {
                        "name": "Leonardo Militano"
                    },
                    {
                        "name": "Harry Edelman"
                    },
                    {
                        "name": "Jorge Pea Queralta"
                    },
                    {
                        "name": "Giovanni Toffetti"
                    }
                ],
                "author_detail": {
                    "name": "Giovanni Toffetti"
                },
                "author": "Giovanni Toffetti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03497v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03497v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.23095v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.23095v2",
                "updated": "2025-11-05T14:25:38Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    14,
                    25,
                    38,
                    2,
                    309,
                    0
                ],
                "published": "2025-10-27T08:00:46Z",
                "published_parsed": [
                    2025,
                    10,
                    27,
                    8,
                    0,
                    46,
                    0,
                    300,
                    0
                ],
                "title": "Revisiting Multimodal Positional Encoding in Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revisiting Multimodal Positional Encoding in Vision-Language Models"
                },
                "summary": "Multimodal position encoding is essential for vision-language models, yet\nthere has been little systematic investigation into multimodal position\nencoding. We conduct a comprehensive analysis of multimodal Rotary Positional\nEmbedding (RoPE) by examining its two core components: position design and\nfrequency allocation. Through extensive experiments, we identify three key\nguidelines: positional coherence, full frequency utilization, and preservation\nof textual priors-ensuring unambiguous layout, rich representation, and\nfaithful transfer from the pre-trained LLM. Based on these insights, we propose\nMulti-Head RoPE (MHRoPE) and MRoPE-Interleave (MRoPE-I), two simple and\nplug-and-play variants that require no architectural changes. Our methods\nconsistently outperform existing approaches across diverse benchmarks, with\nsignificant improvements in both general and fine-grained multimodal\nunderstanding. Code will be avaliable at\nhttps://github.com/JJJYmmm/Multimodal-RoPEs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal position encoding is essential for vision-language models, yet\nthere has been little systematic investigation into multimodal position\nencoding. We conduct a comprehensive analysis of multimodal Rotary Positional\nEmbedding (RoPE) by examining its two core components: position design and\nfrequency allocation. Through extensive experiments, we identify three key\nguidelines: positional coherence, full frequency utilization, and preservation\nof textual priors-ensuring unambiguous layout, rich representation, and\nfaithful transfer from the pre-trained LLM. Based on these insights, we propose\nMulti-Head RoPE (MHRoPE) and MRoPE-Interleave (MRoPE-I), two simple and\nplug-and-play variants that require no architectural changes. Our methods\nconsistently outperform existing approaches across diverse benchmarks, with\nsignificant improvements in both general and fine-grained multimodal\nunderstanding. Code will be avaliable at\nhttps://github.com/JJJYmmm/Multimodal-RoPEs."
                },
                "authors": [
                    {
                        "name": "Jie Huang"
                    },
                    {
                        "name": "Xuejing Liu"
                    },
                    {
                        "name": "Sibo Song"
                    },
                    {
                        "name": "Ruibing Hou"
                    },
                    {
                        "name": "Hong Chang"
                    },
                    {
                        "name": "Junyang Lin"
                    },
                    {
                        "name": "Shuai Bai"
                    }
                ],
                "author_detail": {
                    "name": "Shuai Bai"
                },
                "author": "Shuai Bai",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.23095v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.23095v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03492v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03492v1",
                "updated": "2025-11-05T14:21:18Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    14,
                    21,
                    18,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T14:21:18Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    14,
                    21,
                    18,
                    2,
                    309,
                    0
                ],
                "title": "Why Less is More (Sometimes): A Theory of Data Curation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Why Less is More (Sometimes): A Theory of Data Curation"
                },
                "summary": "This paper introduces a theoretical framework to resolve a central paradox in\nmodern machine learning: When is it better to use less data? This question has\nbecome critical as classical scaling laws suggesting ``more is more'' (Sun et\nal., 2025) are challenged by methods like LIMO (``less is more'') and s1 (Ye et\nal., 2025; Muenighoff et al., 2025), which achieve superior performance with\nsmall, aggressively curated datasets. Here, we study data curation strategies\nwhere an imperfect oracle selects the training examples according to their\ndifficulty and correctness. Our results provide exact scaling law curves for\ntest error under both label-agnostic and label-aware curation rules, revealing\nwhen and why keeping only a subset of data can improve generalization. In\ncontrast to classical scaling laws, we show that under certain conditions,\nsmall curated datasets can outperform full datasets, and we provide analytical\nconditions for this by deriving precise phase transition curves tied to data\nsize and quality. We validate these theoretical claims with empirical results\non ImageNet, confirming our predictions about when curation improves accuracy\nand can even mitigate model collapse. Furthermore, our framework provides a\nprincipled explanation for the contradictory curation strategies recently\nobserved in LLM mathematical reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a theoretical framework to resolve a central paradox in\nmodern machine learning: When is it better to use less data? This question has\nbecome critical as classical scaling laws suggesting ``more is more'' (Sun et\nal., 2025) are challenged by methods like LIMO (``less is more'') and s1 (Ye et\nal., 2025; Muenighoff et al., 2025), which achieve superior performance with\nsmall, aggressively curated datasets. Here, we study data curation strategies\nwhere an imperfect oracle selects the training examples according to their\ndifficulty and correctness. Our results provide exact scaling law curves for\ntest error under both label-agnostic and label-aware curation rules, revealing\nwhen and why keeping only a subset of data can improve generalization. In\ncontrast to classical scaling laws, we show that under certain conditions,\nsmall curated datasets can outperform full datasets, and we provide analytical\nconditions for this by deriving precise phase transition curves tied to data\nsize and quality. We validate these theoretical claims with empirical results\non ImageNet, confirming our predictions about when curation improves accuracy\nand can even mitigate model collapse. Furthermore, our framework provides a\nprincipled explanation for the contradictory curation strategies recently\nobserved in LLM mathematical reasoning."
                },
                "authors": [
                    {
                        "name": "Elvis Dohmatob"
                    },
                    {
                        "name": "Mohammad Pezeshki"
                    },
                    {
                        "name": "Reyhane Askari-Hemmat"
                    }
                ],
                "author_detail": {
                    "name": "Reyhane Askari-Hemmat"
                },
                "author": "Reyhane Askari-Hemmat",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03492v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03492v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08436v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08436v2",
                "updated": "2025-11-05T14:16:25Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    14,
                    16,
                    25,
                    2,
                    309,
                    0
                ],
                "published": "2025-02-12T14:20:36Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    14,
                    20,
                    36,
                    2,
                    43,
                    0
                ],
                "title": "From Haystack to Needle: Label Space Reduction for Zero-shot\n  Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Haystack to Needle: Label Space Reduction for Zero-shot\n  Classification"
                },
                "summary": "We present Label Space Reduction (LSR), a novel method for improving\nzero-shot classification performance of Large Language Models (LLMs). LSR\niteratively refines the classification label space by systematically ranking\nand reducing candidate classes, enabling the model to concentrate on the most\nrelevant options. By leveraging unlabeled data with the statistical learning\ncapabilities of data-driven models, LSR dynamically optimizes the label space\nrepresentation at test time. Our experiments across seven benchmarks\ndemonstrate that LSR improves macro-F1 scores by an average of 7.0% (up to\n14.2%) with Llama-3.1-70B and 3.3% (up to 11.1%) with Claude-3.5-Sonnet\ncompared to standard zero-shot classification baselines. To reduce the\ncomputational overhead of LSR, which requires an additional LLM call at each\niteration, we propose distilling the model into a probabilistic classifier,\nallowing for efficient inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Label Space Reduction (LSR), a novel method for improving\nzero-shot classification performance of Large Language Models (LLMs). LSR\niteratively refines the classification label space by systematically ranking\nand reducing candidate classes, enabling the model to concentrate on the most\nrelevant options. By leveraging unlabeled data with the statistical learning\ncapabilities of data-driven models, LSR dynamically optimizes the label space\nrepresentation at test time. Our experiments across seven benchmarks\ndemonstrate that LSR improves macro-F1 scores by an average of 7.0% (up to\n14.2%) with Llama-3.1-70B and 3.3% (up to 11.1%) with Claude-3.5-Sonnet\ncompared to standard zero-shot classification baselines. To reduce the\ncomputational overhead of LSR, which requires an additional LLM call at each\niteration, we propose distilling the model into a probabilistic classifier,\nallowing for efficient inference."
                },
                "authors": [
                    {
                        "name": "Nathan Vandemoortele"
                    },
                    {
                        "name": "Bram Steenwinckel"
                    },
                    {
                        "name": "Femke Ongenae"
                    },
                    {
                        "name": "Sofie Van Hoecke"
                    }
                ],
                "author_detail": {
                    "name": "Sofie Van Hoecke"
                },
                "author": "Sofie Van Hoecke",
                "arxiv_comment": "Add acknowledgment",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08436v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08436v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03477v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03477v1",
                "updated": "2025-11-05T14:02:11Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    14,
                    2,
                    11,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T14:02:11Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    14,
                    2,
                    11,
                    2,
                    309,
                    0
                ],
                "title": "Probing $J/$ Production Mechanisms in Proton-Proton Collisions at\n  SPD/NICA Energies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probing $J/$ Production Mechanisms in Proton-Proton Collisions at\n  SPD/NICA Energies"
                },
                "summary": "We investigate inclusive $J/\\psi$ production in proton-proton collisions at\ntens of GeV $\\sqrt{s}$ energy, relevant for forthcoming measurements with the\nSpin Physics Detector (SPD) at NICA. Simulations are performed using the\nPEGASUS event generator with transverse-momentum-dependent (TMD) gluon\ndensities, comparing the recent KMR-based KL$'2025$ and CCFM-based LLM$'2024$\nparametrizations. Differential cross sections in rapidity and transverse\nmomentum exhibit smooth, stable behavior under renormalization-scale variation,\nwhile factorization-scale dependence exposes limitations of the LLM$'2024$ set\nat low scales in contrast to KL$'2025$. Normalized $p_T$ spectra reveal\ndistinct hardening patterns linked to the underlying gluon $k_T$ broadening in\neach model. The relative contributions of color-singlet and color-octet\nchannels are also quantified, demonstrating the dominance of color-octet\nmechanisms in the SPD energy regime. These results provide the first detailed\nassessment of quarkonium production sensitivity to gluon TMDs near threshold,\noffering timely theoretical guidance for upcoming $J/\\psi$ measurements at\nSPD/NICA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate inclusive $J/\\psi$ production in proton-proton collisions at\ntens of GeV $\\sqrt{s}$ energy, relevant for forthcoming measurements with the\nSpin Physics Detector (SPD) at NICA. Simulations are performed using the\nPEGASUS event generator with transverse-momentum-dependent (TMD) gluon\ndensities, comparing the recent KMR-based KL$'2025$ and CCFM-based LLM$'2024$\nparametrizations. Differential cross sections in rapidity and transverse\nmomentum exhibit smooth, stable behavior under renormalization-scale variation,\nwhile factorization-scale dependence exposes limitations of the LLM$'2024$ set\nat low scales in contrast to KL$'2025$. Normalized $p_T$ spectra reveal\ndistinct hardening patterns linked to the underlying gluon $k_T$ broadening in\neach model. The relative contributions of color-singlet and color-octet\nchannels are also quantified, demonstrating the dominance of color-octet\nmechanisms in the SPD energy regime. These results provide the first detailed\nassessment of quarkonium production sensitivity to gluon TMDs near threshold,\noffering timely theoretical guidance for upcoming $J/\\psi$ measurements at\nSPD/NICA."
                },
                "authors": [
                    {
                        "name": "Shubham Sharma"
                    },
                    {
                        "name": "Alexey Aparin"
                    }
                ],
                "author_detail": {
                    "name": "Alexey Aparin"
                },
                "author": "Alexey Aparin",
                "arxiv_comment": "5 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03477v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03477v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03475v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03475v1",
                "updated": "2025-11-05T13:59:01Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    13,
                    59,
                    1,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T13:59:01Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    13,
                    59,
                    1,
                    2,
                    309,
                    0
                ],
                "title": "RAGBoost: Efficient Retrieval-Augmented Generation with\n  Accuracy-Preserving Context Reuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAGBoost: Efficient Retrieval-Augmented Generation with\n  Accuracy-Preserving Context Reuse"
                },
                "summary": "Retrieval-augmented generation (RAG) enhances large language models (LLMs)\nwith retrieved context but often suffers from downgraded prefill performance as\nmodern applications demand longer and more complex inputs. Existing caching\ntechniques either preserve accuracy with low cache reuse or improve reuse at\nthe cost of degraded reasoning quality. We present RAGBoost, an efficient RAG\nsystem that achieves high cache reuse without sacrificing accuracy through\naccuracy-preserving context reuse. RAGBoost detects overlapping retrieved items\nacross concurrent sessions and multi-turn interactions, using efficient context\nindexing, ordering, and de-duplication to maximize reuse, while lightweight\ncontextual hints maintain reasoning fidelity. It integrates seamlessly with\nexisting LLM inference engines and improves their prefill performance by 1.5-3X\nover state-of-the-art methods, while preserving or even enhancing reasoning\naccuracy across diverse RAG and agentic AI workloads. Our code is released at:\nhttps://github.com/Edinburgh-AgenticAI/RAGBoost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) enhances large language models (LLMs)\nwith retrieved context but often suffers from downgraded prefill performance as\nmodern applications demand longer and more complex inputs. Existing caching\ntechniques either preserve accuracy with low cache reuse or improve reuse at\nthe cost of degraded reasoning quality. We present RAGBoost, an efficient RAG\nsystem that achieves high cache reuse without sacrificing accuracy through\naccuracy-preserving context reuse. RAGBoost detects overlapping retrieved items\nacross concurrent sessions and multi-turn interactions, using efficient context\nindexing, ordering, and de-duplication to maximize reuse, while lightweight\ncontextual hints maintain reasoning fidelity. It integrates seamlessly with\nexisting LLM inference engines and improves their prefill performance by 1.5-3X\nover state-of-the-art methods, while preserving or even enhancing reasoning\naccuracy across diverse RAG and agentic AI workloads. Our code is released at:\nhttps://github.com/Edinburgh-AgenticAI/RAGBoost."
                },
                "authors": [
                    {
                        "name": "Yinsicheng Jiang"
                    },
                    {
                        "name": "Yeqi Huang"
                    },
                    {
                        "name": "Liang Cheng"
                    },
                    {
                        "name": "Cheng Deng"
                    },
                    {
                        "name": "Xuan Sun"
                    },
                    {
                        "name": "Luo Mai"
                    }
                ],
                "author_detail": {
                    "name": "Luo Mai"
                },
                "author": "Luo Mai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03475v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03475v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08179v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08179v2",
                "updated": "2025-11-05T13:53:51Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    13,
                    53,
                    51,
                    2,
                    309,
                    0
                ],
                "published": "2024-12-11T08:09:42Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    8,
                    9,
                    42,
                    2,
                    346,
                    0
                ],
                "title": "RAG-IT: Retrieval-Augmented Instruction Tuning for Automated Financial\n  Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAG-IT: Retrieval-Augmented Instruction Tuning for Automated Financial\n  Analysis"
                },
                "summary": "Financial analysis relies heavily on the interpretation of earnings reports\nto assess company performance and guide decision-making. Traditional methods\nfor generating such analyses demand significant financial expertise and are\noften time-consuming. With the rapid advancement of Large Language Models\n(LLMs), domain-specific adaptations have emerged for financial tasks such as\nsentiment analysis and entity recognition. This paper introduces RAG-IT\n(Retrieval-Augmented Instruction Tuning), a novel framework designed to\nautomate the generation of earnings report analyses through an LLM fine-tuned\nspecifically for the financial domain. Our approach integrates retrieval\naugmentation with instruction-based fine-tuning to enhance factual accuracy,\ncontextual relevance, and domain adaptability. We construct a comprehensive\nfinancial instruction dataset derived from extensive financial documents and\nearnings reports to guide the LLM's adaptation to specialized financial\nreasoning. Experimental results demonstrate that RAG-IT outperforms\ngeneral-purpose open-source models and achieves performance comparable to\ncommercial systems like GPT-3.5 on financial report generation tasks. This\nresearch highlights the potential of retrieval-augmented instruction tuning to\nstreamline and elevate financial analysis automation, advancing the broader\nfield of intelligent financial reporting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Financial analysis relies heavily on the interpretation of earnings reports\nto assess company performance and guide decision-making. Traditional methods\nfor generating such analyses demand significant financial expertise and are\noften time-consuming. With the rapid advancement of Large Language Models\n(LLMs), domain-specific adaptations have emerged for financial tasks such as\nsentiment analysis and entity recognition. This paper introduces RAG-IT\n(Retrieval-Augmented Instruction Tuning), a novel framework designed to\nautomate the generation of earnings report analyses through an LLM fine-tuned\nspecifically for the financial domain. Our approach integrates retrieval\naugmentation with instruction-based fine-tuning to enhance factual accuracy,\ncontextual relevance, and domain adaptability. We construct a comprehensive\nfinancial instruction dataset derived from extensive financial documents and\nearnings reports to guide the LLM's adaptation to specialized financial\nreasoning. Experimental results demonstrate that RAG-IT outperforms\ngeneral-purpose open-source models and achieves performance comparable to\ncommercial systems like GPT-3.5 on financial report generation tasks. This\nresearch highlights the potential of retrieval-augmented instruction tuning to\nstreamline and elevate financial analysis automation, advancing the broader\nfield of intelligent financial reporting."
                },
                "authors": [
                    {
                        "name": "Van-Duc Le"
                    },
                    {
                        "name": "Hai-Thien To"
                    }
                ],
                "author_detail": {
                    "name": "Hai-Thien To"
                },
                "author": "Hai-Thien To",
                "arxiv_comment": "11 pages, 1 figure, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08179v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08179v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.00801v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.00801v2",
                "updated": "2025-11-05T13:45:24Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    13,
                    45,
                    24,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-02T04:46:43Z",
                "published_parsed": [
                    2025,
                    11,
                    2,
                    4,
                    46,
                    43,
                    6,
                    306,
                    0
                ],
                "title": "Med-Banana-50K: A Cross-modality Large-Scale Dataset for Text-guided\n  Medical Image Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Med-Banana-50K: A Cross-modality Large-Scale Dataset for Text-guided\n  Medical Image Editing"
                },
                "summary": "Recent advances in multimodal large language models have enabled remarkable\nmedical image editing capabilities. However, the research community's progress\nremains constrained by the absence of large-scale, high-quality, and openly\naccessible datasets built specifically for medical image editing with strict\nanatomical and clinical constraints. We introduce Med-Banana-50K, a\ncomprehensive 50K-image dataset for instruction-based medical image editing\nspanning three modalities (chest X-ray, brain MRI, fundus photography) and 23\ndisease types. Our dataset is constructed by leveraging Gemini-2.5-Flash-Image\nto generate bidirectional edits (lesion addition and removal) from real medical\nimages. What distinguishes Med-Banana-50K from general-domain editing datasets\nis our systematic approach to medical quality control: we employ LLM-as-Judge\nwith a medically grounded rubric (instruction compliance, structural\nplausibility, realism, and fidelity preservation) and history-aware iterative\nrefinement up to five rounds. Beyond single-turn editing, Med-Banana-50K\nincludes 37K failed attempts with full conversation logs for preference\nlearning and alignment research. By providing this large-scale, medically\nvalidated, and fully documented resource, Med-Banana-50K establishes a\nfoundation for training and evaluating the next generation of medical image\nediting models.Our dataset and code are publicly available at\n[https://github.com/richardChenzhihui/med-banana-50k].",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in multimodal large language models have enabled remarkable\nmedical image editing capabilities. However, the research community's progress\nremains constrained by the absence of large-scale, high-quality, and openly\naccessible datasets built specifically for medical image editing with strict\nanatomical and clinical constraints. We introduce Med-Banana-50K, a\ncomprehensive 50K-image dataset for instruction-based medical image editing\nspanning three modalities (chest X-ray, brain MRI, fundus photography) and 23\ndisease types. Our dataset is constructed by leveraging Gemini-2.5-Flash-Image\nto generate bidirectional edits (lesion addition and removal) from real medical\nimages. What distinguishes Med-Banana-50K from general-domain editing datasets\nis our systematic approach to medical quality control: we employ LLM-as-Judge\nwith a medically grounded rubric (instruction compliance, structural\nplausibility, realism, and fidelity preservation) and history-aware iterative\nrefinement up to five rounds. Beyond single-turn editing, Med-Banana-50K\nincludes 37K failed attempts with full conversation logs for preference\nlearning and alignment research. By providing this large-scale, medically\nvalidated, and fully documented resource, Med-Banana-50K establishes a\nfoundation for training and evaluating the next generation of medical image\nediting models.Our dataset and code are publicly available at\n[https://github.com/richardChenzhihui/med-banana-50k]."
                },
                "authors": [
                    {
                        "name": "Zhihui Chen"
                    },
                    {
                        "name": "Mengling Feng"
                    }
                ],
                "author_detail": {
                    "name": "Mengling Feng"
                },
                "author": "Mengling Feng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.00801v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.00801v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.01066v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.01066v2",
                "updated": "2025-11-05T13:19:47Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    13,
                    19,
                    47,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-02T20:16:38Z",
                "published_parsed": [
                    2025,
                    11,
                    2,
                    20,
                    16,
                    38,
                    6,
                    306,
                    0
                ],
                "title": "HPLT 3.0: Very Large-Scale Multilingual Resources for LLM and MT. Mono-\n  and Bi-lingual Data, Multilingual Evaluation, and Pre-Trained Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HPLT 3.0: Very Large-Scale Multilingual Resources for LLM and MT. Mono-\n  and Bi-lingual Data, Multilingual Evaluation, and Pre-Trained Models"
                },
                "summary": "We present an ongoing initiative to provide open, very large, high-quality,\nand richly annotated textual datasets for almost 200 languages. At 30 trillion\ntokens, this is likely the largest generally available multilingual collection\nof LLM pre-training data. These datasets are derived from web crawls from\ndifferent sources and accompanied with a complete, open-source pipeline for\ndocument selection from web archives, text extraction from HTML, language\nidentification for noisy texts, exact and near-deduplication, annotation with,\namong others, register labels, text quality estimates, and personally\nidentifiable information; and final selection and filtering. We report on data\nquality probes through contrastive and analytical statistics, through manual\ninspection of samples for 24 languages, and through end-to-end evaluation of\nvarious language model architectures trained on this data. For multilingual LLM\nevaluation, we provide a comprehensive collection of benchmarks for nine\nEuropean languages, with special emphasis on natively created tasks, mechanisms\nto mitigate prompt sensitivity, and refined normalization and aggregation of\nscores. Additionally, we train and evaluate a family of 57 monolingual\nencoder-decoder models, as well as a handful of monolingual GPT-like reference\nmodels. Besides the monolingual data and models, we also present a very large\ncollection of parallel texts automatically mined from this data, together with\na novel parallel corpus synthesized via machine translation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present an ongoing initiative to provide open, very large, high-quality,\nand richly annotated textual datasets for almost 200 languages. At 30 trillion\ntokens, this is likely the largest generally available multilingual collection\nof LLM pre-training data. These datasets are derived from web crawls from\ndifferent sources and accompanied with a complete, open-source pipeline for\ndocument selection from web archives, text extraction from HTML, language\nidentification for noisy texts, exact and near-deduplication, annotation with,\namong others, register labels, text quality estimates, and personally\nidentifiable information; and final selection and filtering. We report on data\nquality probes through contrastive and analytical statistics, through manual\ninspection of samples for 24 languages, and through end-to-end evaluation of\nvarious language model architectures trained on this data. For multilingual LLM\nevaluation, we provide a comprehensive collection of benchmarks for nine\nEuropean languages, with special emphasis on natively created tasks, mechanisms\nto mitigate prompt sensitivity, and refined normalization and aggregation of\nscores. Additionally, we train and evaluate a family of 57 monolingual\nencoder-decoder models, as well as a handful of monolingual GPT-like reference\nmodels. Besides the monolingual data and models, we also present a very large\ncollection of parallel texts automatically mined from this data, together with\na novel parallel corpus synthesized via machine translation."
                },
                "authors": [
                    {
                        "name": "Stephan Oepen"
                    },
                    {
                        "name": "Nikolay Arefev"
                    },
                    {
                        "name": "Mikko Aulamo"
                    },
                    {
                        "name": "Marta Ban"
                    },
                    {
                        "name": "Maja Buljan"
                    },
                    {
                        "name": "Laurie Burchell"
                    },
                    {
                        "name": "Lucas Charpentier"
                    },
                    {
                        "name": "Pinzhen Chen"
                    },
                    {
                        "name": "Mariya Fedorova"
                    },
                    {
                        "name": "Ona de Gibert"
                    },
                    {
                        "name": "Barry Haddow"
                    },
                    {
                        "name": "Jan Haji"
                    },
                    {
                        "name": "Jindich Helcl"
                    },
                    {
                        "name": "Andrey Kutuzov"
                    },
                    {
                        "name": "Veronika Laippala"
                    },
                    {
                        "name": "Zihao Li"
                    },
                    {
                        "name": "Risto Luukkonen"
                    },
                    {
                        "name": "Bhavitvya Malik"
                    },
                    {
                        "name": "Vladislav Mikhailov"
                    },
                    {
                        "name": "Amanda Myntti"
                    },
                    {
                        "name": "Dayyn O'Brien"
                    },
                    {
                        "name": "Lucie Polkov"
                    },
                    {
                        "name": "Sampo Pyysalo"
                    },
                    {
                        "name": "Gema Ramrez Snchez"
                    },
                    {
                        "name": "Janine Siewert"
                    },
                    {
                        "name": "Pavel Stepachev"
                    },
                    {
                        "name": "Jrg Tiedemann"
                    },
                    {
                        "name": "Teemu Vahtola"
                    },
                    {
                        "name": "Duan Vari"
                    },
                    {
                        "name": "Fedor Vitiugin"
                    },
                    {
                        "name": "Tea Vojtchov"
                    },
                    {
                        "name": "Jaume Zaragoza"
                    }
                ],
                "author_detail": {
                    "name": "Jaume Zaragoza"
                },
                "author": "Jaume Zaragoza",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.01066v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.01066v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.22193v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.22193v2",
                "updated": "2025-11-05T13:05:52Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    13,
                    5,
                    52,
                    2,
                    309,
                    0
                ],
                "published": "2025-10-25T07:19:52Z",
                "published_parsed": [
                    2025,
                    10,
                    25,
                    7,
                    19,
                    52,
                    5,
                    298,
                    0
                ],
                "title": "(Approximate) Matrix Multiplication via Convolutions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "(Approximate) Matrix Multiplication via Convolutions"
                },
                "summary": "We study the capability of the Fast Fourier Transform (FFT) to accelerate\nexact and approximate matrix multiplication without using Strassen-like\ndivide-and-conquer. We present a simple exact algorithm running in\n$O(n^{2.89})$ time, which only sums a few convolutions (FFTs) in\n$\\mathbb{Z}_{m}^{k}$, building on the work of Cohn, Kleinberg, Szegedy and\nUmans (2005). As a corollary, combining this algorithm with linear sketching\nbreaks the longstanding linear speed-accuracy tradeoff for \"combinatorial\"\napproximate matrix multiplication (AMM, Pagh'13, Sarlos'06,\nClarkson-Woodruff'13), achieving error $\\frac{1}{r^{1.1}}\\left\\lVert \\mathbf{A}\n\\right\\rVert_{F}^{2}\\left\\lVert \\mathbf{B}\\right\\rVert_{F}^{2}$ in $O(rn^{2})$\ntime, using nothing but FFTs.\n  Motivated by the rich literature for approximating polynomials, our main\ncontribution in this paper is extending the group-theoretic framework of Cohn\nand Umans (2003) to approximate matrix multiplication (AMM). Specifically, we\nintroduce and study an approximate notion of the Triple Product Property, which\nin the abelian case is equivalent to finding a Sumset which minimizes\n(multi-)intersections with an arithmetic progression. We prove tight bounds on\nthis quantity for abelian groups (yielding a simple and practical AMM algorithm\nvia polynomial multiplication), and establish a weaker lower bound for\nnon-abelian groups, extending a lemma of Gowers. Finally, we propose a concrete\napproach that uses low-degree approximation of multi-variate polynomials for\nAMM, which we believe will lead to practical, non-asymptotic AMM algorithms in\nreal-world applications, most notably LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the capability of the Fast Fourier Transform (FFT) to accelerate\nexact and approximate matrix multiplication without using Strassen-like\ndivide-and-conquer. We present a simple exact algorithm running in\n$O(n^{2.89})$ time, which only sums a few convolutions (FFTs) in\n$\\mathbb{Z}_{m}^{k}$, building on the work of Cohn, Kleinberg, Szegedy and\nUmans (2005). As a corollary, combining this algorithm with linear sketching\nbreaks the longstanding linear speed-accuracy tradeoff for \"combinatorial\"\napproximate matrix multiplication (AMM, Pagh'13, Sarlos'06,\nClarkson-Woodruff'13), achieving error $\\frac{1}{r^{1.1}}\\left\\lVert \\mathbf{A}\n\\right\\rVert_{F}^{2}\\left\\lVert \\mathbf{B}\\right\\rVert_{F}^{2}$ in $O(rn^{2})$\ntime, using nothing but FFTs.\n  Motivated by the rich literature for approximating polynomials, our main\ncontribution in this paper is extending the group-theoretic framework of Cohn\nand Umans (2003) to approximate matrix multiplication (AMM). Specifically, we\nintroduce and study an approximate notion of the Triple Product Property, which\nin the abelian case is equivalent to finding a Sumset which minimizes\n(multi-)intersections with an arithmetic progression. We prove tight bounds on\nthis quantity for abelian groups (yielding a simple and practical AMM algorithm\nvia polynomial multiplication), and establish a weaker lower bound for\nnon-abelian groups, extending a lemma of Gowers. Finally, we propose a concrete\napproach that uses low-degree approximation of multi-variate polynomials for\nAMM, which we believe will lead to practical, non-asymptotic AMM algorithms in\nreal-world applications, most notably LLM inference."
                },
                "authors": [
                    {
                        "name": "Yahel Uffenheimer"
                    },
                    {
                        "name": "Omri Weinstein"
                    }
                ],
                "author_detail": {
                    "name": "Omri Weinstein"
                },
                "author": "Omri Weinstein",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.22193v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.22193v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03441v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03441v1",
                "updated": "2025-11-05T13:02:06Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    13,
                    2,
                    6,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T13:02:06Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    13,
                    2,
                    6,
                    2,
                    309,
                    0
                ],
                "title": "CareMedEval dataset: Evaluating Critical Appraisal and Reasoning in the\n  Biomedical Field",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CareMedEval dataset: Evaluating Critical Appraisal and Reasoning in the\n  Biomedical Field"
                },
                "summary": "Critical appraisal of scientific literature is an essential skill in the\nbiomedical field. While large language models (LLMs) can offer promising\nsupport in this task, their reliability remains limited, particularly for\ncritical reasoning in specialized domains. We introduce CareMedEval, an\noriginal dataset designed to evaluate LLMs on biomedical critical appraisal and\nreasoning tasks. Derived from authentic exams taken by French medical students,\nthe dataset contains 534 questions based on 37 scientific articles. Unlike\nexisting benchmarks, CareMedEval explicitly evaluates critical reading and\nreasoning grounded in scientific papers. Benchmarking state-of-the-art\ngeneralist and biomedical-specialized LLMs under various context conditions\nreveals the difficulty of the task: open and commercial models fail to exceed\nan Exact Match Rate of 0.5 even though generating intermediate reasoning tokens\nconsiderably improves the results. Yet, models remain challenged especially on\nquestions about study limitations and statistical analysis. CareMedEval\nprovides a challenging benchmark for grounded reasoning, exposing current LLM\nlimitations and paving the way for future development of automated support for\ncritical appraisal.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Critical appraisal of scientific literature is an essential skill in the\nbiomedical field. While large language models (LLMs) can offer promising\nsupport in this task, their reliability remains limited, particularly for\ncritical reasoning in specialized domains. We introduce CareMedEval, an\noriginal dataset designed to evaluate LLMs on biomedical critical appraisal and\nreasoning tasks. Derived from authentic exams taken by French medical students,\nthe dataset contains 534 questions based on 37 scientific articles. Unlike\nexisting benchmarks, CareMedEval explicitly evaluates critical reading and\nreasoning grounded in scientific papers. Benchmarking state-of-the-art\ngeneralist and biomedical-specialized LLMs under various context conditions\nreveals the difficulty of the task: open and commercial models fail to exceed\nan Exact Match Rate of 0.5 even though generating intermediate reasoning tokens\nconsiderably improves the results. Yet, models remain challenged especially on\nquestions about study limitations and statistical analysis. CareMedEval\nprovides a challenging benchmark for grounded reasoning, exposing current LLM\nlimitations and paving the way for future development of automated support for\ncritical appraisal."
                },
                "authors": [
                    {
                        "name": "Doria Bonzi"
                    },
                    {
                        "name": "Alexandre Guiggi"
                    },
                    {
                        "name": "Frdric Bchet"
                    },
                    {
                        "name": "Carlos Ramisch"
                    },
                    {
                        "name": "Benoit Favre"
                    }
                ],
                "author_detail": {
                    "name": "Benoit Favre"
                },
                "author": "Benoit Favre",
                "arxiv_comment": "Preprint submitted to LREC 2026 (under review) To access the dataset,\n  see https://github.com/bonzid/CareMedEval",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03441v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03441v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03434v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03434v1",
                "updated": "2025-11-05T12:50:06Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    12,
                    50,
                    6,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T12:50:06Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    12,
                    50,
                    6,
                    2,
                    309,
                    0
                ],
                "title": "Inter-Agent Trust Models: A Comparative Study of Brief, Claim, Proof,\n  Stake, Reputation and Constraint in Agentic Web Protocol Design-A2A, AP2,\n  ERC-8004, and Beyond",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inter-Agent Trust Models: A Comparative Study of Brief, Claim, Proof,\n  Stake, Reputation and Constraint in Agentic Web Protocol Design-A2A, AP2,\n  ERC-8004, and Beyond"
                },
                "summary": "As the \"agentic web\" takes shape-billions of AI agents (often LLM-powered)\nautonomously transacting and collaborating-trust shifts from human oversight to\nprotocol design. In 2025, several inter-agent protocols crystallized this\nshift, including Google's Agent-to-Agent (A2A), Agent Payments Protocol (AP2),\nand Ethereum's ERC-8004 \"Trustless Agents,\" yet their underlying trust\nassumptions remain under-examined. This paper presents a comparative study of\ntrust models in inter-agent protocol design: Brief (self- or third-party\nverifiable claims), Claim (self-proclaimed capabilities and identity, e.g.\nAgentCard), Proof (cryptographic verification, including zero-knowledge proofs\nand trusted execution environment attestations), Stake (bonded collateral with\nslashing and insurance), Reputation (crowd feedback and graph-based trust\nsignals), and Constraint (sandboxing and capability bounding). For each, we\nanalyze assumptions, attack surfaces, and design trade-offs, with particular\nemphasis on LLM-specific fragilities-prompt injection,\nsycophancy/nudge-susceptibility, hallucination, deception, and\nmisalignment-that render purely reputational or claim-only approaches brittle.\nOur findings indicate no single mechanism suffices. We argue for\ntrustless-by-default architectures anchored in Proof and Stake to gate\nhigh-impact actions, augmented by Brief for identity and discovery and\nReputation overlays for flexibility and social signals. We comparatively\nevaluate A2A, AP2, ERC-8004 and related historical variations in academic\nresearch under metrics spanning security, privacy, latency/cost, and social\nrobustness (Sybil/collusion/whitewashing resistance). We conclude with hybrid\ntrust model recommendations that mitigate reputation gaming and misinformed LLM\nbehavior, and we distill actionable design guidelines for safer, interoperable,\nand scalable agent economies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the \"agentic web\" takes shape-billions of AI agents (often LLM-powered)\nautonomously transacting and collaborating-trust shifts from human oversight to\nprotocol design. In 2025, several inter-agent protocols crystallized this\nshift, including Google's Agent-to-Agent (A2A), Agent Payments Protocol (AP2),\nand Ethereum's ERC-8004 \"Trustless Agents,\" yet their underlying trust\nassumptions remain under-examined. This paper presents a comparative study of\ntrust models in inter-agent protocol design: Brief (self- or third-party\nverifiable claims), Claim (self-proclaimed capabilities and identity, e.g.\nAgentCard), Proof (cryptographic verification, including zero-knowledge proofs\nand trusted execution environment attestations), Stake (bonded collateral with\nslashing and insurance), Reputation (crowd feedback and graph-based trust\nsignals), and Constraint (sandboxing and capability bounding). For each, we\nanalyze assumptions, attack surfaces, and design trade-offs, with particular\nemphasis on LLM-specific fragilities-prompt injection,\nsycophancy/nudge-susceptibility, hallucination, deception, and\nmisalignment-that render purely reputational or claim-only approaches brittle.\nOur findings indicate no single mechanism suffices. We argue for\ntrustless-by-default architectures anchored in Proof and Stake to gate\nhigh-impact actions, augmented by Brief for identity and discovery and\nReputation overlays for flexibility and social signals. We comparatively\nevaluate A2A, AP2, ERC-8004 and related historical variations in academic\nresearch under metrics spanning security, privacy, latency/cost, and social\nrobustness (Sybil/collusion/whitewashing resistance). We conclude with hybrid\ntrust model recommendations that mitigate reputation gaming and misinformed LLM\nbehavior, and we distill actionable design guidelines for safer, interoperable,\nand scalable agent economies."
                },
                "authors": [
                    {
                        "name": "Botao 'Amber' Hu"
                    },
                    {
                        "name": "Helena Rong"
                    }
                ],
                "author_detail": {
                    "name": "Helena Rong"
                },
                "author": "Helena Rong",
                "arxiv_comment": "Submitted to AAAI 2026 Workshop on Trust and Control in Agentic AI\n  (TrustAgent)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03434v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03434v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03421v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03421v1",
                "updated": "2025-11-05T12:38:11Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    12,
                    38,
                    11,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T12:38:11Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    12,
                    38,
                    11,
                    2,
                    309,
                    0
                ],
                "title": "Light over Heavy: Automated Performance Requirements Quantification with\n  Linguistic Inducement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Light over Heavy: Automated Performance Requirements Quantification with\n  Linguistic Inducement"
                },
                "summary": "Elicited performance requirements need to be quantified for compliance in\ndifferent engineering tasks, e.g., configuration tuning and performance\ntesting. Much existing work has relied on manual quantification, which is\nexpensive and error-prone due to the imprecision. In this paper, we present\nLQPR, a highly efficient automatic approach for performance requirements\nquantification.LQPR relies on a new theoretical framework that converts\nquantification as a classification problem. Despite the prevalent applications\nof Large Language Models (LLMs) for requirement analytics, LQPR takes a\ndifferent perspective to address the classification: we observed that\nperformance requirements can exhibit strong patterns and are often\nshort/concise, therefore we design a lightweight linguistically induced\nmatching mechanism. We compare LQPR against nine state-of-the-art\nlearning-based approaches over diverse datasets, demonstrating that it is\nranked as the sole best for 75% or more cases with two orders less cost. Our\nwork proves that, at least for performance requirement quantification,\nspecialized methods can be more suitable than the general LLM-driven\napproaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Elicited performance requirements need to be quantified for compliance in\ndifferent engineering tasks, e.g., configuration tuning and performance\ntesting. Much existing work has relied on manual quantification, which is\nexpensive and error-prone due to the imprecision. In this paper, we present\nLQPR, a highly efficient automatic approach for performance requirements\nquantification.LQPR relies on a new theoretical framework that converts\nquantification as a classification problem. Despite the prevalent applications\nof Large Language Models (LLMs) for requirement analytics, LQPR takes a\ndifferent perspective to address the classification: we observed that\nperformance requirements can exhibit strong patterns and are often\nshort/concise, therefore we design a lightweight linguistically induced\nmatching mechanism. We compare LQPR against nine state-of-the-art\nlearning-based approaches over diverse datasets, demonstrating that it is\nranked as the sole best for 75% or more cases with two orders less cost. Our\nwork proves that, at least for performance requirement quantification,\nspecialized methods can be more suitable than the general LLM-driven\napproaches."
                },
                "authors": [
                    {
                        "name": "Shihai Wang"
                    },
                    {
                        "name": "Tao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tao Chen"
                },
                "author": "Tao Chen",
                "arxiv_comment": "accepted by ICSE 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03421v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03421v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03410v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03410v1",
                "updated": "2025-11-05T12:24:20Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    12,
                    24,
                    20,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T12:24:20Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    12,
                    24,
                    20,
                    2,
                    309,
                    0
                ],
                "title": "Knowledge-Augmented Question Error Correction for Chinese Question\n  Answer System with QuestionRAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge-Augmented Question Error Correction for Chinese Question\n  Answer System with QuestionRAG"
                },
                "summary": "Input errors in question-answering (QA) systems often lead to incorrect\nresponses. Large language models (LLMs) struggle with this task, frequently\nfailing to interpret user intent (misinterpretation) or unnecessarily altering\nthe original question's structure (over-correction). We propose QuestionRAG, a\nframework that tackles these problems. To address misinterpretation, it\nenriches the input with external knowledge (e.g., search results, related\nentities). To prevent over-correction, it uses reinforcement learning (RL) to\nalign the model's objective with precise correction, not just paraphrasing. Our\nresults demonstrate that knowledge augmentation is critical for understanding\nfaulty questions. Furthermore, RL-based alignment proves significantly more\neffective than traditional supervised fine-tuning (SFT), boosting the model's\nability to follow instructions and generalize. By integrating these two\nstrategies, QuestionRAG unlocks the full potential of LLMs for the question\ncorrection task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Input errors in question-answering (QA) systems often lead to incorrect\nresponses. Large language models (LLMs) struggle with this task, frequently\nfailing to interpret user intent (misinterpretation) or unnecessarily altering\nthe original question's structure (over-correction). We propose QuestionRAG, a\nframework that tackles these problems. To address misinterpretation, it\nenriches the input with external knowledge (e.g., search results, related\nentities). To prevent over-correction, it uses reinforcement learning (RL) to\nalign the model's objective with precise correction, not just paraphrasing. Our\nresults demonstrate that knowledge augmentation is critical for understanding\nfaulty questions. Furthermore, RL-based alignment proves significantly more\neffective than traditional supervised fine-tuning (SFT), boosting the model's\nability to follow instructions and generalize. By integrating these two\nstrategies, QuestionRAG unlocks the full potential of LLMs for the question\ncorrection task."
                },
                "authors": [
                    {
                        "name": "Longpeng Qiu"
                    },
                    {
                        "name": "Ting Li"
                    },
                    {
                        "name": "Shuai Mao"
                    },
                    {
                        "name": "Nan Yang"
                    },
                    {
                        "name": "Xiaohui Yan"
                    }
                ],
                "author_detail": {
                    "name": "Xiaohui Yan"
                },
                "author": "Xiaohui Yan",
                "arxiv_comment": "EMNLP2025 Industry Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03410v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03410v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03408v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03408v1",
                "updated": "2025-11-05T12:20:45Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    12,
                    20,
                    45,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T12:20:45Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    12,
                    20,
                    45,
                    2,
                    309,
                    0
                ],
                "title": "Efficient Reasoning via Thought-Training and Thought-Free Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Reasoning via Thought-Training and Thought-Free Inference"
                },
                "summary": "Recent advances in large language models (LLMs) have leveraged explicit\nChain-of-Thought (CoT) prompting to improve reasoning accuracy. However, most\nexisting methods primarily compress verbose reasoning outputs. These\nLong-to-Short transformations aim to improve efficiency, but still rely on\nexplicit reasoning during inference. In this work, we introduce \\textbf{3TF}\n(\\textbf{T}hought-\\textbf{T}raining and \\textbf{T}hought-\\textbf{F}ree\ninference), a framework for efficient reasoning that takes a Short-to-Long\nperspective. We first train a hybrid model that can operate in both reasoning\nand non-reasoning modes, and then further train it on CoT-annotated data to\ninternalize structured reasoning, while enforcing concise, thought-free outputs\nat inference time using the no-reasoning mode. Unlike compression-based\napproaches, 3TF improves the reasoning quality of non-reasoning outputs,\nenabling models to perform rich internal reasoning implicitly while keeping\nexternal outputs short. Empirically, 3TF-trained models obtain large\nimprovements on reasoning benchmarks under thought-free inference,\ndemonstrating that high quality reasoning can be learned and executed\nimplicitly without explicit step-by-step generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have leveraged explicit\nChain-of-Thought (CoT) prompting to improve reasoning accuracy. However, most\nexisting methods primarily compress verbose reasoning outputs. These\nLong-to-Short transformations aim to improve efficiency, but still rely on\nexplicit reasoning during inference. In this work, we introduce \\textbf{3TF}\n(\\textbf{T}hought-\\textbf{T}raining and \\textbf{T}hought-\\textbf{F}ree\ninference), a framework for efficient reasoning that takes a Short-to-Long\nperspective. We first train a hybrid model that can operate in both reasoning\nand non-reasoning modes, and then further train it on CoT-annotated data to\ninternalize structured reasoning, while enforcing concise, thought-free outputs\nat inference time using the no-reasoning mode. Unlike compression-based\napproaches, 3TF improves the reasoning quality of non-reasoning outputs,\nenabling models to perform rich internal reasoning implicitly while keeping\nexternal outputs short. Empirically, 3TF-trained models obtain large\nimprovements on reasoning benchmarks under thought-free inference,\ndemonstrating that high quality reasoning can be learned and executed\nimplicitly without explicit step-by-step generation."
                },
                "authors": [
                    {
                        "name": "Canhui Wu"
                    },
                    {
                        "name": "Qiong Cao"
                    },
                    {
                        "name": "Chao Xue"
                    },
                    {
                        "name": "Wei Xi"
                    },
                    {
                        "name": "Xiaodong He"
                    }
                ],
                "author_detail": {
                    "name": "Xiaodong He"
                },
                "author": "Xiaodong He",
                "arxiv_comment": "11 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03408v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03408v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03404v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03404v1",
                "updated": "2025-11-05T12:12:35Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    12,
                    12,
                    35,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T12:12:35Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    12,
                    12,
                    35,
                    2,
                    309,
                    0
                ],
                "title": "Towards Realistic Project-Level Code Generation via Multi-Agent\n  Collaboration and Semantic Architecture Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Realistic Project-Level Code Generation via Multi-Agent\n  Collaboration and Semantic Architecture Modeling"
                },
                "summary": "In recent years, Large Language Models (LLMs) have achieved remarkable\nprogress in automated code generation. In real-world software engineering, the\ngrowing demand for rapid iteration and continuous delivery underscores the\nimportance of project-level code generation, where LLMs are expected to\ngenerate complete software projects directly from complex user requirements.\nAlthough existing studies have made initial explorations, they still face key\nlimitations, including unrealistic datasets and unreliable evaluation metrics\nthat fail to reflect real-world complexity, the semantic gap between\nhuman-written requirements and machine-interpretable structures, and\ndifficulties in managing hierarchical dependencies and maintaining quality\nthroughout the generation process. To address these limitations, we first\nintroduce CodeProjectEval, a project-level code generation dataset built from\n18 real-world repositories with 12.7 files and 2,388.6 lines of code per task\non average, supplemented with documentation and executable test cases for\nautomatic evaluation. We further propose ProjectGen, a multi-agent framework\nthat decomposes projects into architecture design, skeleton generation, and\ncode filling stages with iterative refinement and memory-based context\nmanagement. Within this framework, we introduce the Semantic Software\nArchitecture Tree (SSAT), a structured and semantically rich representation\nthat effectively bridges user requirements and source code implementation.\nExperiments show that ProjectGen achieves state-of-the-art performance, passing\n52/124 test cases on the small-scale project-level code generation dataset\nDevBench, a 57% improvement over the baseline approaches, and 310 test cases on\nCodeProjectEval, representing an improvement of roughly tenfold compared to the\nbaselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Large Language Models (LLMs) have achieved remarkable\nprogress in automated code generation. In real-world software engineering, the\ngrowing demand for rapid iteration and continuous delivery underscores the\nimportance of project-level code generation, where LLMs are expected to\ngenerate complete software projects directly from complex user requirements.\nAlthough existing studies have made initial explorations, they still face key\nlimitations, including unrealistic datasets and unreliable evaluation metrics\nthat fail to reflect real-world complexity, the semantic gap between\nhuman-written requirements and machine-interpretable structures, and\ndifficulties in managing hierarchical dependencies and maintaining quality\nthroughout the generation process. To address these limitations, we first\nintroduce CodeProjectEval, a project-level code generation dataset built from\n18 real-world repositories with 12.7 files and 2,388.6 lines of code per task\non average, supplemented with documentation and executable test cases for\nautomatic evaluation. We further propose ProjectGen, a multi-agent framework\nthat decomposes projects into architecture design, skeleton generation, and\ncode filling stages with iterative refinement and memory-based context\nmanagement. Within this framework, we introduce the Semantic Software\nArchitecture Tree (SSAT), a structured and semantically rich representation\nthat effectively bridges user requirements and source code implementation.\nExperiments show that ProjectGen achieves state-of-the-art performance, passing\n52/124 test cases on the small-scale project-level code generation dataset\nDevBench, a 57% improvement over the baseline approaches, and 310 test cases on\nCodeProjectEval, representing an improvement of roughly tenfold compared to the\nbaselines."
                },
                "authors": [
                    {
                        "name": "Qianhui Zhao"
                    },
                    {
                        "name": "Li Zhang"
                    },
                    {
                        "name": "Fang Liu"
                    },
                    {
                        "name": "Junhang Cheng"
                    },
                    {
                        "name": "Chengru Wu"
                    },
                    {
                        "name": "Junchen Ai"
                    },
                    {
                        "name": "Qiaoyuanhe Meng"
                    },
                    {
                        "name": "Lichen Zhang"
                    },
                    {
                        "name": "Xiaoli Lian"
                    },
                    {
                        "name": "Shubin Song"
                    },
                    {
                        "name": "Yuanping Guo"
                    }
                ],
                "author_detail": {
                    "name": "Yuanping Guo"
                },
                "author": "Yuanping Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03404v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03404v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03401v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03401v1",
                "updated": "2025-11-05T12:08:08Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    12,
                    8,
                    8,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T12:08:08Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    12,
                    8,
                    8,
                    2,
                    309,
                    0
                ],
                "title": "Performance Analysis of Wireless-Powered Pinching Antenna Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Analysis of Wireless-Powered Pinching Antenna Systems"
                },
                "summary": "Pinching antenna system (PAS) serves as a groundbreaking paradigm that\nenhances wireless communications by flexibly adjusting the position of pinching\nantenna (PA) and establishing a strong line-of-sight (LoS) link, thereby\nreducing the free-space path loss. This paper introduces the concept of\nwireless-powered PAS, and investigates the reliability of wireless-powered PAS\nto explore the advantages of PA in improving the performance of\nwireless-powered communication (WPC) system. In addition, we derive the\nclosed-form expressions of outage probability and ergodic rate for the\npractical lossy waveguide case and ideal lossless waveguide case, respectively,\nand analyze the optimal deployment of waveguides and user to provide valuable\ninsights for guiding their deployments. The results show that an increase in\nthe absorption coefficient and in the dimensions of the user area leads to\nhigher in-waveguide and free-space propagation losses, respectively, which in\nturn increase the outage probability and reduce the ergodic rate of the\nwireless-powered PAS. However, the performance of wireless-powered PAS is\nseverely affected by the absorption coefficient and the waveguide length, e.g.,\nunder conditions of high absorption coefficient and long waveguide, the outage\nprobability of wireless-powered PAS is even worse than that of traditional WPC\nsystem. While the ergodic rate of wireless-powered PAS is better than that of\ntraditional WPC system under conditions of high absorption coefficient and long\nwaveguide. Interestingly, the wireless-powered PAS has the optimal time\nallocation factor and optimal distance between power station (PS) and access\npoint (AP) to minimize the outage probability or maximize the ergodic rate.\nMoreover, the system performance of PS and AP separated at the optimal distance\nbetween PS and AP is superior to that of PS and AP integrated into a hybrid\naccess point.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pinching antenna system (PAS) serves as a groundbreaking paradigm that\nenhances wireless communications by flexibly adjusting the position of pinching\nantenna (PA) and establishing a strong line-of-sight (LoS) link, thereby\nreducing the free-space path loss. This paper introduces the concept of\nwireless-powered PAS, and investigates the reliability of wireless-powered PAS\nto explore the advantages of PA in improving the performance of\nwireless-powered communication (WPC) system. In addition, we derive the\nclosed-form expressions of outage probability and ergodic rate for the\npractical lossy waveguide case and ideal lossless waveguide case, respectively,\nand analyze the optimal deployment of waveguides and user to provide valuable\ninsights for guiding their deployments. The results show that an increase in\nthe absorption coefficient and in the dimensions of the user area leads to\nhigher in-waveguide and free-space propagation losses, respectively, which in\nturn increase the outage probability and reduce the ergodic rate of the\nwireless-powered PAS. However, the performance of wireless-powered PAS is\nseverely affected by the absorption coefficient and the waveguide length, e.g.,\nunder conditions of high absorption coefficient and long waveguide, the outage\nprobability of wireless-powered PAS is even worse than that of traditional WPC\nsystem. While the ergodic rate of wireless-powered PAS is better than that of\ntraditional WPC system under conditions of high absorption coefficient and long\nwaveguide. Interestingly, the wireless-powered PAS has the optimal time\nallocation factor and optimal distance between power station (PS) and access\npoint (AP) to minimize the outage probability or maximize the ergodic rate.\nMoreover, the system performance of PS and AP separated at the optimal distance\nbetween PS and AP is superior to that of PS and AP integrated into a hybrid\naccess point."
                },
                "authors": [
                    {
                        "name": "Kunrui Cao"
                    },
                    {
                        "name": "Jingyu Chen"
                    },
                    {
                        "name": "Panagiotis D. Diamantoulakis"
                    },
                    {
                        "name": "Lei Zhou"
                    },
                    {
                        "name": "Xingwang Li"
                    },
                    {
                        "name": "Yuanwei Liu"
                    },
                    {
                        "name": "George K. Karagiannidis"
                    }
                ],
                "author_detail": {
                    "name": "George K. Karagiannidis"
                },
                "author": "George K. Karagiannidis",
                "arxiv_comment": "13 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03401v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03401v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03400v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03400v1",
                "updated": "2025-11-05T12:08:05Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    12,
                    8,
                    5,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T12:08:05Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    12,
                    8,
                    5,
                    2,
                    309,
                    0
                ],
                "title": "GUIDES: Guidance Using Instructor-Distilled Embeddings for Pre-trained\n  Robot Policy Enhancement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GUIDES: Guidance Using Instructor-Distilled Embeddings for Pre-trained\n  Robot Policy Enhancement"
                },
                "summary": "Pre-trained robot policies serve as the foundation of many validated robotic\nsystems, which encapsulate extensive embodied knowledge. However, they often\nlack the semantic awareness characteristic of foundation models, and replacing\nthem entirely is impractical in many situations due to high costs and the loss\nof accumulated knowledge. To address this gap, we introduce GUIDES, a\nlightweight framework that augments pre-trained policies with semantic guidance\nfrom foundation models without requiring architectural redesign. GUIDES employs\na fine-tuned vision-language model (Instructor) to generate contextual\ninstructions, which are encoded by an auxiliary module into guidance\nembeddings. These embeddings are injected into the policy's latent space,\nallowing the legacy model to adapt to this new semantic input through brief,\ntargeted fine-tuning. For inference-time robustness, a large language\nmodel-based Reflector monitors the Instructor's confidence and, when confidence\nis low, initiates a reasoning loop that analyzes execution history, retrieves\nrelevant examples, and augments the VLM's context to refine subsequent actions.\nExtensive validation in the RoboCasa simulation environment across diverse\npolicy architectures shows consistent and substantial improvements in task\nsuccess rates. Real-world deployment on a UR5 robot further demonstrates that\nGUIDES enhances motion precision for critical sub-tasks such as grasping.\nOverall, GUIDES offers a practical and resource-efficient pathway to upgrade,\nrather than replace, validated robot policies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pre-trained robot policies serve as the foundation of many validated robotic\nsystems, which encapsulate extensive embodied knowledge. However, they often\nlack the semantic awareness characteristic of foundation models, and replacing\nthem entirely is impractical in many situations due to high costs and the loss\nof accumulated knowledge. To address this gap, we introduce GUIDES, a\nlightweight framework that augments pre-trained policies with semantic guidance\nfrom foundation models without requiring architectural redesign. GUIDES employs\na fine-tuned vision-language model (Instructor) to generate contextual\ninstructions, which are encoded by an auxiliary module into guidance\nembeddings. These embeddings are injected into the policy's latent space,\nallowing the legacy model to adapt to this new semantic input through brief,\ntargeted fine-tuning. For inference-time robustness, a large language\nmodel-based Reflector monitors the Instructor's confidence and, when confidence\nis low, initiates a reasoning loop that analyzes execution history, retrieves\nrelevant examples, and augments the VLM's context to refine subsequent actions.\nExtensive validation in the RoboCasa simulation environment across diverse\npolicy architectures shows consistent and substantial improvements in task\nsuccess rates. Real-world deployment on a UR5 robot further demonstrates that\nGUIDES enhances motion precision for critical sub-tasks such as grasping.\nOverall, GUIDES offers a practical and resource-efficient pathway to upgrade,\nrather than replace, validated robot policies."
                },
                "authors": [
                    {
                        "name": "Minquan Gao"
                    },
                    {
                        "name": "Xinyi Li"
                    },
                    {
                        "name": "Qing Yan"
                    },
                    {
                        "name": "Xiaojian Sun"
                    },
                    {
                        "name": "Xiaopan Zhang"
                    },
                    {
                        "name": "Chien-Ming Huang"
                    },
                    {
                        "name": "Jiachen Li"
                    }
                ],
                "author_detail": {
                    "name": "Jiachen Li"
                },
                "author": "Jiachen Li",
                "arxiv_comment": "8 pages, 4 figures, Accepted by IEEE IROS 2025 Workshop WIR-M",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03400v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03400v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17612v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17612v2",
                "updated": "2025-11-05T11:42:56Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    11,
                    42,
                    56,
                    2,
                    309,
                    0
                ],
                "published": "2025-05-23T08:20:15Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    8,
                    20,
                    15,
                    4,
                    143,
                    0
                ],
                "title": "Distilling LLM Agent into Small Models with Retrieval and Code Tools",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distilling LLM Agent into Small Models with Retrieval and Code Tools"
                },
                "summary": "Large language models (LLMs) excel at complex reasoning tasks but remain\ncomputationally expensive, limiting their practical deployment. To address\nthis, recent works have focused on distilling reasoning capabilities into\nsmaller language models (sLMs) using chain-of-thought (CoT) traces from teacher\nLLMs. However, this approach struggles in scenarios requiring rare factual\nknowledge or precise computation, where sLMs often hallucinate due to limited\ncapability. In this work, we propose Agent Distillation, a framework for\ntransferring not only reasoning capability but full task-solving behavior from\nLLM-based agents into sLMs with retrieval and code tools. We improve agent\ndistillation along two complementary axes: (1) we introduce a prompting method\ncalled first-thought prefix to enhance the quality of teacher-generated\ntrajectories; and (2) we propose a self-consistent action generation for\nimproving test-time robustness of small agents. We evaluate our method on eight\nreasoning tasks across factual and mathematical domains, covering both\nin-domain and out-of-domain generalization. Our results show that sLMs as small\nas 0.5B, 1.5B, 3B parameters can achieve performance competitive with next-tier\nlarger 1.5B, 3B, 7B models fine-tuned using CoT distillation, demonstrating the\npotential of agent distillation for building practical, tool-using small\nagents. Our code is available at https://github.com/Nardien/agent-distillation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) excel at complex reasoning tasks but remain\ncomputationally expensive, limiting their practical deployment. To address\nthis, recent works have focused on distilling reasoning capabilities into\nsmaller language models (sLMs) using chain-of-thought (CoT) traces from teacher\nLLMs. However, this approach struggles in scenarios requiring rare factual\nknowledge or precise computation, where sLMs often hallucinate due to limited\ncapability. In this work, we propose Agent Distillation, a framework for\ntransferring not only reasoning capability but full task-solving behavior from\nLLM-based agents into sLMs with retrieval and code tools. We improve agent\ndistillation along two complementary axes: (1) we introduce a prompting method\ncalled first-thought prefix to enhance the quality of teacher-generated\ntrajectories; and (2) we propose a self-consistent action generation for\nimproving test-time robustness of small agents. We evaluate our method on eight\nreasoning tasks across factual and mathematical domains, covering both\nin-domain and out-of-domain generalization. Our results show that sLMs as small\nas 0.5B, 1.5B, 3B parameters can achieve performance competitive with next-tier\nlarger 1.5B, 3B, 7B models fine-tuned using CoT distillation, demonstrating the\npotential of agent distillation for building practical, tool-using small\nagents. Our code is available at https://github.com/Nardien/agent-distillation."
                },
                "authors": [
                    {
                        "name": "Minki Kang"
                    },
                    {
                        "name": "Jongwon Jeong"
                    },
                    {
                        "name": "Seanie Lee"
                    },
                    {
                        "name": "Jaewoong Cho"
                    },
                    {
                        "name": "Sung Ju Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Sung Ju Hwang"
                },
                "author": "Sung Ju Hwang",
                "arxiv_comment": "NeurIPS 2025 Spotlight",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17612v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17612v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.07491v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.07491v2",
                "updated": "2025-11-05T11:34:39Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    11,
                    34,
                    39,
                    2,
                    309,
                    0
                ],
                "published": "2025-06-09T07:10:58Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    7,
                    10,
                    58,
                    0,
                    160,
                    0
                ],
                "title": "SpatialLM: Training Large Language Models for Structured Indoor Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpatialLM: Training Large Language Models for Structured Indoor Modeling"
                },
                "summary": "SpatialLM is a large language model designed to process 3D point cloud data\nand generate structured 3D scene understanding outputs. These outputs include\narchitectural elements like walls, doors, windows, and oriented object boxes\nwith their semantic categories. Unlike previous methods which exploit\ntask-specific network designs, our model adheres to the standard multimodal LLM\narchitecture and is fine-tuned directly from open-source LLMs.\n  To train SpatialLM, we collect a large-scale, high-quality synthetic dataset\nconsisting of the point clouds of 12,328 indoor scenes (54,778 rooms) with\nground-truth 3D annotations, and conduct a careful study on various modeling\nand training decisions. On public benchmarks, our model gives state-of-the-art\nperformance in layout estimation and competitive results in 3D object\ndetection. With that, we show a feasible path for enhancing the spatial\nunderstanding capabilities of modern LLMs for applications in augmented\nreality, embodied robotics, and more.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpatialLM is a large language model designed to process 3D point cloud data\nand generate structured 3D scene understanding outputs. These outputs include\narchitectural elements like walls, doors, windows, and oriented object boxes\nwith their semantic categories. Unlike previous methods which exploit\ntask-specific network designs, our model adheres to the standard multimodal LLM\narchitecture and is fine-tuned directly from open-source LLMs.\n  To train SpatialLM, we collect a large-scale, high-quality synthetic dataset\nconsisting of the point clouds of 12,328 indoor scenes (54,778 rooms) with\nground-truth 3D annotations, and conduct a careful study on various modeling\nand training decisions. On public benchmarks, our model gives state-of-the-art\nperformance in layout estimation and competitive results in 3D object\ndetection. With that, we show a feasible path for enhancing the spatial\nunderstanding capabilities of modern LLMs for applications in augmented\nreality, embodied robotics, and more."
                },
                "authors": [
                    {
                        "name": "Yongsen Mao"
                    },
                    {
                        "name": "Junhao Zhong"
                    },
                    {
                        "name": "Chuan Fang"
                    },
                    {
                        "name": "Jia Zheng"
                    },
                    {
                        "name": "Rui Tang"
                    },
                    {
                        "name": "Hao Zhu"
                    },
                    {
                        "name": "Ping Tan"
                    },
                    {
                        "name": "Zihan Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Zihan Zhou"
                },
                "author": "Zihan Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.07491v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.07491v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03376v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03376v1",
                "updated": "2025-11-05T11:31:08Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    11,
                    31,
                    8,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T11:31:08Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    11,
                    31,
                    8,
                    2,
                    309,
                    0
                ],
                "title": "Computational Imaging Meets LLMs: Zero-Shot IDH Mutation Prediction in\n  Brain Gliomas",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computational Imaging Meets LLMs: Zero-Shot IDH Mutation Prediction in\n  Brain Gliomas"
                },
                "summary": "We present a framework that combines Large Language Models with computational\nimage analytics for non-invasive, zero-shot prediction of IDH mutation status\nin brain gliomas. For each subject, coregistered multi-parametric MRI scans and\nmulti-class tumor segmentation maps were processed to extract interpretable\nsemantic (visual) attributes and quantitative features, serialized in a\nstandardized JSON file, and used to query GPT 4o and GPT 5 without fine-tuning.\nWe evaluated this framework on six publicly available datasets (N = 1427) and\nresults showcased high accuracy and balanced classification performance across\nheterogeneous cohorts, even in the absence of manual annotations. GPT 5\noutperformed GPT 4o in context-driven phenotype interpretation. Volumetric\nfeatures emerged as the most important predictors, supplemented by\nsubtype-specific imaging markers and clinical information. Our results\ndemonstrate the potential of integrating LLM-based reasoning with computational\nimage analytics for precise, non-invasive tumor genotyping, advancing\ndiagnostic strategies in neuro-oncology. The code is available at\nhttps://github.com/ATPLab-LUMS/CIM-LLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a framework that combines Large Language Models with computational\nimage analytics for non-invasive, zero-shot prediction of IDH mutation status\nin brain gliomas. For each subject, coregistered multi-parametric MRI scans and\nmulti-class tumor segmentation maps were processed to extract interpretable\nsemantic (visual) attributes and quantitative features, serialized in a\nstandardized JSON file, and used to query GPT 4o and GPT 5 without fine-tuning.\nWe evaluated this framework on six publicly available datasets (N = 1427) and\nresults showcased high accuracy and balanced classification performance across\nheterogeneous cohorts, even in the absence of manual annotations. GPT 5\noutperformed GPT 4o in context-driven phenotype interpretation. Volumetric\nfeatures emerged as the most important predictors, supplemented by\nsubtype-specific imaging markers and clinical information. Our results\ndemonstrate the potential of integrating LLM-based reasoning with computational\nimage analytics for precise, non-invasive tumor genotyping, advancing\ndiagnostic strategies in neuro-oncology. The code is available at\nhttps://github.com/ATPLab-LUMS/CIM-LLM."
                },
                "authors": [
                    {
                        "name": "Syed Muqeem Mahmood"
                    },
                    {
                        "name": "Hassan Mohy-ud-Din"
                    }
                ],
                "author_detail": {
                    "name": "Hassan Mohy-ud-Din"
                },
                "author": "Hassan Mohy-ud-Din",
                "arxiv_comment": "5 pages, 1 figure, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03376v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03376v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13994v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13994v2",
                "updated": "2025-11-05T11:26:59Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    11,
                    26,
                    59,
                    2,
                    309,
                    0
                ],
                "published": "2025-05-20T06:44:34Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    6,
                    44,
                    34,
                    1,
                    140,
                    0
                ],
                "title": "Divide by Question, Conquer by Agent: SPLIT-RAG with Question-Driven\n  Graph Partitioning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Divide by Question, Conquer by Agent: SPLIT-RAG with Question-Driven\n  Graph Partitioning"
                },
                "summary": "Retrieval-Augmented Generation (RAG) systems empower large language models\n(LLMs) with external knowledge, yet struggle with efficiency-accuracy\ntrade-offs when scaling to large knowledge graphs. Existing approaches often\nrely on monolithic graph retrieval, incurring unnecessary latency for simple\nqueries and fragmented reasoning for complex multi-hop questions. To address\nthese challenges, this paper propose SPLIT-RAG, a multi-agent RAG framework\nthat addresses these limitations with question-driven semantic graph\npartitioning and collaborative subgraph retrieval. The innovative framework\nfirst create Semantic Partitioning of Linked Information, then use the\nType-Specialized knowledge base to achieve Multi-Agent RAG. The attribute-aware\ngraph segmentation manages to divide knowledge graphs into semantically\ncoherent subgraphs, ensuring subgraphs align with different query types, while\nlightweight LLM agents are assigned to partitioned subgraphs, and only relevant\npartitions are activated during retrieval, thus reduce search space while\nenhancing efficiency. Finally, a hierarchical merging module resolves\ninconsistencies across subgraph-derived answers through logical verifications.\nExtensive experimental validation demonstrates considerable improvements\ncompared to existing approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) systems empower large language models\n(LLMs) with external knowledge, yet struggle with efficiency-accuracy\ntrade-offs when scaling to large knowledge graphs. Existing approaches often\nrely on monolithic graph retrieval, incurring unnecessary latency for simple\nqueries and fragmented reasoning for complex multi-hop questions. To address\nthese challenges, this paper propose SPLIT-RAG, a multi-agent RAG framework\nthat addresses these limitations with question-driven semantic graph\npartitioning and collaborative subgraph retrieval. The innovative framework\nfirst create Semantic Partitioning of Linked Information, then use the\nType-Specialized knowledge base to achieve Multi-Agent RAG. The attribute-aware\ngraph segmentation manages to divide knowledge graphs into semantically\ncoherent subgraphs, ensuring subgraphs align with different query types, while\nlightweight LLM agents are assigned to partitioned subgraphs, and only relevant\npartitions are activated during retrieval, thus reduce search space while\nenhancing efficiency. Finally, a hierarchical merging module resolves\ninconsistencies across subgraph-derived answers through logical verifications.\nExtensive experimental validation demonstrates considerable improvements\ncompared to existing approaches."
                },
                "authors": [
                    {
                        "name": "Ruiyi Yang"
                    },
                    {
                        "name": "Hao Xue"
                    },
                    {
                        "name": "Imran Razzak"
                    },
                    {
                        "name": "Shirui Pan"
                    },
                    {
                        "name": "Hakim Hacid"
                    },
                    {
                        "name": "Flora D. Salim"
                    }
                ],
                "author_detail": {
                    "name": "Flora D. Salim"
                },
                "author": "Flora D. Salim",
                "arxiv_comment": "20 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13994v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13994v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03372v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03372v1",
                "updated": "2025-11-05T11:26:38Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    11,
                    26,
                    38,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T11:26:38Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    11,
                    26,
                    38,
                    2,
                    309,
                    0
                ],
                "title": "LFC-DA: Logical Formula-Controlled Data Augmentation for Enhanced\n  Logical Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LFC-DA: Logical Formula-Controlled Data Augmentation for Enhanced\n  Logical Reasoning"
                },
                "summary": "For complex logical data augmentation, heavy reliance on human annotation is\ncostly, whereas direct generation with large language models yields\nuninterpretable and logically homogeneous examples. To address this, we present\nLFC-DA, a symbolic-logic-controlled pipeline: logical text is first mapped to\npropositional expressions, a compact rule library is compiled, and a bounded\nstate-space search systematically discovers valid formulas that are then\nverbalized back into natural-language questions, ensuring both diversity and\nlogical rigor under propositional logic. Experiments on ReClor and LogiQA show\nsignificant improvements in the logical-reasoning accuracy of pretrained\nmodels, confirming the effectiveness of LFC-DA for LLM-guided logical data\naugmentation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "For complex logical data augmentation, heavy reliance on human annotation is\ncostly, whereas direct generation with large language models yields\nuninterpretable and logically homogeneous examples. To address this, we present\nLFC-DA, a symbolic-logic-controlled pipeline: logical text is first mapped to\npropositional expressions, a compact rule library is compiled, and a bounded\nstate-space search systematically discovers valid formulas that are then\nverbalized back into natural-language questions, ensuring both diversity and\nlogical rigor under propositional logic. Experiments on ReClor and LogiQA show\nsignificant improvements in the logical-reasoning accuracy of pretrained\nmodels, confirming the effectiveness of LFC-DA for LLM-guided logical data\naugmentation."
                },
                "authors": [
                    {
                        "name": "Shenghao Li"
                    }
                ],
                "author_detail": {
                    "name": "Shenghao Li"
                },
                "author": "Shenghao Li",
                "arxiv_comment": "10 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03372v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03372v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.6; F.4.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03370v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03370v1",
                "updated": "2025-11-05T11:25:07Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    11,
                    25,
                    7,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T11:25:07Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    11,
                    25,
                    7,
                    2,
                    309,
                    0
                ],
                "title": "EQ-Negotiator: Dynamic Emotional Personas Empower Small Language Models\n  for Edge-Deployable Credit Negotiation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EQ-Negotiator: Dynamic Emotional Personas Empower Small Language Models\n  for Edge-Deployable Credit Negotiation"
                },
                "summary": "The deployment of large language models (LLMs) in automated negotiation has\nset a high performance benchmark, but their computational cost and data privacy\nrequirements render them unsuitable for many privacy-sensitive, on-device\napplications such as mobile assistants, embodied AI agents or private client\ninteractions. While small language models (SLMs) offer a practical alternative,\nthey suffer from a significant performance gap compared to LLMs in playing\nemotionally charged complex personas, especially for credit negotiation. This\npaper introduces EQ-Negotiator, a novel framework that bridges this capability\ngap using emotional personas. Its core is a reasoning system that integrates\ngame theory with a Hidden Markov Model(HMM) to learn and track debtor emotional\nstates online, without pre-training. This allows EQ-Negotiator to equip SLMs\nwith the strategic intelligence to counter manipulation while de-escalating\nconflict and upholding ethical standards. Through extensive agent-to-agent\nsimulations across diverse credit negotiation scenarios, including adversarial\ndebtor strategies like cheating, threatening, and playing the victim, we show\nthat a 7B parameter language model with EQ-Negotiator achieves better debt\nrecovery and negotiation efficiency than baseline LLMs more than 10 times its\nsize. This work advances persona modeling from descriptive character profiles\nto dynamic emotional architectures that operate within privacy constraints.\nBesides, this paper establishes that strategic emotional intelligence, not raw\nmodel scale, is the critical factor for success in automated negotiation,\npaving the way for effective, ethical, and privacy-preserving AI negotiators\nthat can operate on the edge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of large language models (LLMs) in automated negotiation has\nset a high performance benchmark, but their computational cost and data privacy\nrequirements render them unsuitable for many privacy-sensitive, on-device\napplications such as mobile assistants, embodied AI agents or private client\ninteractions. While small language models (SLMs) offer a practical alternative,\nthey suffer from a significant performance gap compared to LLMs in playing\nemotionally charged complex personas, especially for credit negotiation. This\npaper introduces EQ-Negotiator, a novel framework that bridges this capability\ngap using emotional personas. Its core is a reasoning system that integrates\ngame theory with a Hidden Markov Model(HMM) to learn and track debtor emotional\nstates online, without pre-training. This allows EQ-Negotiator to equip SLMs\nwith the strategic intelligence to counter manipulation while de-escalating\nconflict and upholding ethical standards. Through extensive agent-to-agent\nsimulations across diverse credit negotiation scenarios, including adversarial\ndebtor strategies like cheating, threatening, and playing the victim, we show\nthat a 7B parameter language model with EQ-Negotiator achieves better debt\nrecovery and negotiation efficiency than baseline LLMs more than 10 times its\nsize. This work advances persona modeling from descriptive character profiles\nto dynamic emotional architectures that operate within privacy constraints.\nBesides, this paper establishes that strategic emotional intelligence, not raw\nmodel scale, is the critical factor for success in automated negotiation,\npaving the way for effective, ethical, and privacy-preserving AI negotiators\nthat can operate on the edge."
                },
                "authors": [
                    {
                        "name": "Yunbo Long"
                    },
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Alexandra Brintrup"
                    }
                ],
                "author_detail": {
                    "name": "Alexandra Brintrup"
                },
                "author": "Alexandra Brintrup",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03370v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03370v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14234v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14234v4",
                "updated": "2025-11-05T11:24:50Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    11,
                    24,
                    50,
                    2,
                    309,
                    0
                ],
                "published": "2025-03-18T13:11:43Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    13,
                    11,
                    43,
                    1,
                    77,
                    0
                ],
                "title": "Beyond Single Pass, Looping Through Time: KG-IRAG with Iterative\n  Knowledge Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Single Pass, Looping Through Time: KG-IRAG with Iterative\n  Knowledge Retrieval"
                },
                "summary": "Graph Retrieval-Augmented Generation (GraphRAG) has proven highly effective\nin enhancing the performance of Large Language Models (LLMs) on tasks that\nrequire external knowledge. By leveraging Knowledge Graphs (KGs), GraphRAG\nimproves information retrieval for complex reasoning tasks, providing more\nprecise and comprehensive retrieval and generating more accurate responses to\nQAs. However, most RAG methods fall short in addressing multi-step reasoning,\nparticularly when both information extraction and inference are necessary. To\naddress this limitation, this paper presents Knowledge Graph-Based Iterative\nRetrieval-Augmented Generation (KG-IRAG), a novel framework that integrates KGs\nwith iterative reasoning to improve LLMs' ability to handle queries involving\ntemporal and logical dependencies. Through iterative retrieval steps, KG-IRAG\nincrementally gathers relevant data from external KGs, enabling step-by-step\nreasoning. The proposed approach is particularly suited for scenarios where\nreasoning is required alongside dynamic temporal data extraction, such as\ndetermining optimal travel times based on weather conditions or traffic\npatterns. Experimental results show that KG-IRAG improves accuracy in complex\nreasoning tasks by effectively integrating external knowledge with iterative,\nlogic-based retrieval. Additionally, three new datasets: weatherQA-Irish,\nweatherQA-Sydney, and trafficQA-TFNSW, are formed to evaluate KG-IRAG's\nperformance, demonstrating its potential beyond traditional RAG applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Retrieval-Augmented Generation (GraphRAG) has proven highly effective\nin enhancing the performance of Large Language Models (LLMs) on tasks that\nrequire external knowledge. By leveraging Knowledge Graphs (KGs), GraphRAG\nimproves information retrieval for complex reasoning tasks, providing more\nprecise and comprehensive retrieval and generating more accurate responses to\nQAs. However, most RAG methods fall short in addressing multi-step reasoning,\nparticularly when both information extraction and inference are necessary. To\naddress this limitation, this paper presents Knowledge Graph-Based Iterative\nRetrieval-Augmented Generation (KG-IRAG), a novel framework that integrates KGs\nwith iterative reasoning to improve LLMs' ability to handle queries involving\ntemporal and logical dependencies. Through iterative retrieval steps, KG-IRAG\nincrementally gathers relevant data from external KGs, enabling step-by-step\nreasoning. The proposed approach is particularly suited for scenarios where\nreasoning is required alongside dynamic temporal data extraction, such as\ndetermining optimal travel times based on weather conditions or traffic\npatterns. Experimental results show that KG-IRAG improves accuracy in complex\nreasoning tasks by effectively integrating external knowledge with iterative,\nlogic-based retrieval. Additionally, three new datasets: weatherQA-Irish,\nweatherQA-Sydney, and trafficQA-TFNSW, are formed to evaluate KG-IRAG's\nperformance, demonstrating its potential beyond traditional RAG applications."
                },
                "authors": [
                    {
                        "name": "Ruiyi Yang"
                    },
                    {
                        "name": "Hao Xue"
                    },
                    {
                        "name": "Imran Razzak"
                    },
                    {
                        "name": "Hakim Hacid"
                    },
                    {
                        "name": "Flora D. Salim"
                    }
                ],
                "author_detail": {
                    "name": "Flora D. Salim"
                },
                "author": "Flora D. Salim",
                "arxiv_comment": "15 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14234v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14234v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03369v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03369v1",
                "updated": "2025-11-05T11:24:50Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    11,
                    24,
                    50,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T11:24:50Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    11,
                    24,
                    50,
                    2,
                    309,
                    0
                ],
                "title": "Silenced Biases: The Dark Side LLMs Learned to Refuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Silenced Biases: The Dark Side LLMs Learned to Refuse"
                },
                "summary": "Safety-aligned large language models (LLMs) are becoming increasingly\nwidespread, especially in sensitive applications where fairness is essential\nand biased outputs can cause significant harm. However, evaluating the fairness\nof models is a complex challenge, and approaches that do so typically utilize\nstandard question-answer (QA) styled schemes. Such methods often overlook\ndeeper issues by interpreting the model's refusal responses as positive\nfairness measurements, which creates a false sense of fairness. In this work,\nwe introduce the concept of silenced biases, which are unfair preferences\nencoded within models' latent space and are effectively concealed by\nsafety-alignment. Previous approaches that considered similar indirect biases\noften relied on prompt manipulation or handcrafted implicit queries, which\npresent limited scalability and risk contaminating the evaluation process with\nadditional biases. We propose the Silenced Bias Benchmark (SBB), which aims to\nuncover these biases by employing activation steering to reduce model refusals\nduring QA. SBB supports easy expansion to new demographic groups and subjects,\npresenting a fairness evaluation framework that encourages the future\ndevelopment of fair models and tools beyond the masking effects of alignment\ntraining. We demonstrate our approach over multiple LLMs, where our findings\nexpose an alarming distinction between models' direct responses and their\nunderlying fairness issues.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safety-aligned large language models (LLMs) are becoming increasingly\nwidespread, especially in sensitive applications where fairness is essential\nand biased outputs can cause significant harm. However, evaluating the fairness\nof models is a complex challenge, and approaches that do so typically utilize\nstandard question-answer (QA) styled schemes. Such methods often overlook\ndeeper issues by interpreting the model's refusal responses as positive\nfairness measurements, which creates a false sense of fairness. In this work,\nwe introduce the concept of silenced biases, which are unfair preferences\nencoded within models' latent space and are effectively concealed by\nsafety-alignment. Previous approaches that considered similar indirect biases\noften relied on prompt manipulation or handcrafted implicit queries, which\npresent limited scalability and risk contaminating the evaluation process with\nadditional biases. We propose the Silenced Bias Benchmark (SBB), which aims to\nuncover these biases by employing activation steering to reduce model refusals\nduring QA. SBB supports easy expansion to new demographic groups and subjects,\npresenting a fairness evaluation framework that encourages the future\ndevelopment of fair models and tools beyond the masking effects of alignment\ntraining. We demonstrate our approach over multiple LLMs, where our findings\nexpose an alarming distinction between models' direct responses and their\nunderlying fairness issues."
                },
                "authors": [
                    {
                        "name": "Rom Himelstein"
                    },
                    {
                        "name": "Amit LeVi"
                    },
                    {
                        "name": "Brit Youngmann"
                    },
                    {
                        "name": "Yaniv Nemcovsky"
                    },
                    {
                        "name": "Avi Mendelson"
                    }
                ],
                "author_detail": {
                    "name": "Avi Mendelson"
                },
                "author": "Avi Mendelson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03369v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03369v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03366v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03366v1",
                "updated": "2025-11-05T11:10:13Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    11,
                    10,
                    13,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T11:10:13Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    11,
                    10,
                    13,
                    2,
                    309,
                    0
                ],
                "title": "Lightwave Power Transfer-Enabled Underwater Optical ISAC Systems under\n  Ship Attitude Variation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lightwave Power Transfer-Enabled Underwater Optical ISAC Systems under\n  Ship Attitude Variation"
                },
                "summary": "In this paper, we propose a lightwave power transfer-enabled underwater\noptical integrated sensing and communication (O-ISAC) system, where an access\npoint (AP) mounted on a seasurface ship transmits lightwave signals to two\nnodes, namely ($i$) a seabed sensor that harvests energy and transmits uplink\ninformation to the AP, and ($ii$) a sensing target whose position is estimated\nby the AP using an array of pinhole cameras. To capture practical deployment\nconditions, the ship attitude variation is modeled through its roll, pitch, and\nyaw angles, each following a Gaussian distribution under low-to-moderate sea\nstates. Closed-form approximations are derived for the mean squared error (MSE)\nof target localization and the achievable uplink data rate. Analytical and\nsimulation results demonstrate excellent agreement, validating the proposed\nmodels and derived expressions, while revealing the fundamental\ncommunication-sensing tradeoff in the O-ISAC system. The results further\nprovide valuable design insights, including the optimal camera placement on the\nship to minimize localization error, achieving a minimum MSE of $10^{-2}$\n$\\text{m}^2$ with multiple cameras under roll, pitch, and yaw angle variation\nof $10^{\\circ}$, and the optimal harvest-use ratio of $0.55$ for the considered\nsetup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose a lightwave power transfer-enabled underwater\noptical integrated sensing and communication (O-ISAC) system, where an access\npoint (AP) mounted on a seasurface ship transmits lightwave signals to two\nnodes, namely ($i$) a seabed sensor that harvests energy and transmits uplink\ninformation to the AP, and ($ii$) a sensing target whose position is estimated\nby the AP using an array of pinhole cameras. To capture practical deployment\nconditions, the ship attitude variation is modeled through its roll, pitch, and\nyaw angles, each following a Gaussian distribution under low-to-moderate sea\nstates. Closed-form approximations are derived for the mean squared error (MSE)\nof target localization and the achievable uplink data rate. Analytical and\nsimulation results demonstrate excellent agreement, validating the proposed\nmodels and derived expressions, while revealing the fundamental\ncommunication-sensing tradeoff in the O-ISAC system. The results further\nprovide valuable design insights, including the optimal camera placement on the\nship to minimize localization error, achieving a minimum MSE of $10^{-2}$\n$\\text{m}^2$ with multiple cameras under roll, pitch, and yaw angle variation\nof $10^{\\circ}$, and the optimal harvest-use ratio of $0.55$ for the considered\nsetup."
                },
                "authors": [
                    {
                        "name": "Kapila W. S. Palitharathna"
                    },
                    {
                        "name": "Constantinos Psomas"
                    },
                    {
                        "name": "Ioannis Krikidis"
                    }
                ],
                "author_detail": {
                    "name": "Ioannis Krikidis"
                },
                "author": "Ioannis Krikidis",
                "arxiv_comment": "This paper has been submitted to the IEEE International Conference on\n  Communications (ICC 2026) conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03366v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03366v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03363v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03363v1",
                "updated": "2025-11-05T11:08:08Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    11,
                    8,
                    8,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T11:08:08Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    11,
                    8,
                    8,
                    2,
                    309,
                    0
                ],
                "title": "A Modular, Data-Free Pipeline for Multi-Label Intention Recognition in\n  Transportation Agentic AI Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Modular, Data-Free Pipeline for Multi-Label Intention Recognition in\n  Transportation Agentic AI Applications"
                },
                "summary": "In this study, a modular, data-free pipeline for multi-label intention\nrecognition is proposed for agentic AI applications in transportation. Unlike\ntraditional intent recognition systems that depend on large, annotated corpora\nand often struggle with fine-grained, multi-label discrimination, our approach\neliminates the need for costly data collection while enhancing the accuracy of\nmulti-label intention understanding. Specifically, the overall pipeline, named\nDMTC, consists of three steps: 1) using prompt engineering to guide large\nlanguage models (LLMs) to generate diverse synthetic queries in different\ntransport scenarios; 2) encoding each textual query with a Sentence-T5 model to\nobtain compact semantic embeddings; 3) training a lightweight classifier using\na novel online focal-contrastive (OFC) loss that emphasizes hard samples and\nmaximizes inter-class separability. The applicability of the proposed pipeline\nis demonstrated in an agentic AI application in the maritime transportation\ncontext. Extensive experiments show that DMTC achieves a Hamming loss of 5.35%\nand an AUC of 95.92%, outperforming state-of-the-art multi-label classifiers\nand recent end-to-end SOTA LLM-based baselines. Further analysis reveals that\nSentence-T5 embeddings improve subset accuracy by at least 3.29% over\nalternative encoders, and integrating the OFC loss yields an additional 0.98%\ngain compared to standard contrastive objectives. In conclusion, our system\nseamlessly routes user queries to task-specific modules (e.g., ETA information,\ntraffic risk evaluation, and other typical scenarios in the transportation\ndomain), laying the groundwork for fully autonomous, intention-aware agents\nwithout costly manual labelling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, a modular, data-free pipeline for multi-label intention\nrecognition is proposed for agentic AI applications in transportation. Unlike\ntraditional intent recognition systems that depend on large, annotated corpora\nand often struggle with fine-grained, multi-label discrimination, our approach\neliminates the need for costly data collection while enhancing the accuracy of\nmulti-label intention understanding. Specifically, the overall pipeline, named\nDMTC, consists of three steps: 1) using prompt engineering to guide large\nlanguage models (LLMs) to generate diverse synthetic queries in different\ntransport scenarios; 2) encoding each textual query with a Sentence-T5 model to\nobtain compact semantic embeddings; 3) training a lightweight classifier using\na novel online focal-contrastive (OFC) loss that emphasizes hard samples and\nmaximizes inter-class separability. The applicability of the proposed pipeline\nis demonstrated in an agentic AI application in the maritime transportation\ncontext. Extensive experiments show that DMTC achieves a Hamming loss of 5.35%\nand an AUC of 95.92%, outperforming state-of-the-art multi-label classifiers\nand recent end-to-end SOTA LLM-based baselines. Further analysis reveals that\nSentence-T5 embeddings improve subset accuracy by at least 3.29% over\nalternative encoders, and integrating the OFC loss yields an additional 0.98%\ngain compared to standard contrastive objectives. In conclusion, our system\nseamlessly routes user queries to task-specific modules (e.g., ETA information,\ntraffic risk evaluation, and other typical scenarios in the transportation\ndomain), laying the groundwork for fully autonomous, intention-aware agents\nwithout costly manual labelling."
                },
                "authors": [
                    {
                        "name": "Xiaocai Zhang"
                    },
                    {
                        "name": "Hur Lim"
                    },
                    {
                        "name": "Ke Wang"
                    },
                    {
                        "name": "Zhe Xiao"
                    },
                    {
                        "name": "Jing Wang"
                    },
                    {
                        "name": "Kelvin Lee"
                    },
                    {
                        "name": "Xiuju Fu"
                    },
                    {
                        "name": "Zheng Qin"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Qin"
                },
                "author": "Zheng Qin",
                "arxiv_comment": "Present in the Transportation Research Board (TRB) Annual Meeting\n  2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03363v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03363v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03361v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03361v1",
                "updated": "2025-11-05T11:02:16Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    11,
                    2,
                    16,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T11:02:16Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    11,
                    2,
                    16,
                    2,
                    309,
                    0
                ],
                "title": "Open Source State-Of-the-Art Solution for Romanian Speech Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open Source State-Of-the-Art Solution for Romanian Speech Recognition"
                },
                "summary": "In this work, we present a new state-of-the-art Romanian Automatic Speech\nRecognition (ASR) system based on NVIDIA's FastConformer architecture--explored\nhere for the first time in the context of Romanian. We train our model on a\nlarge corpus of, mostly, weakly supervised transcriptions, totaling over 2,600\nhours of speech. Leveraging a hybrid decoder with both Connectionist Temporal\nClassification (CTC) and Token-Duration Transducer (TDT) branches, we evaluate\na range of decoding strategies including greedy, ALSD, and CTC beam search with\na 6-gram token-level language model. Our system achieves state-of-the-art\nperformance across all Romanian evaluation benchmarks, including read,\nspontaneous, and domain-specific speech, with up to 27% relative WER reduction\ncompared to previous best-performing systems. In addition to improved\ntranscription accuracy, our approach demonstrates practical decoding\nefficiency, making it suitable for both research and deployment in low-latency\nASR applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we present a new state-of-the-art Romanian Automatic Speech\nRecognition (ASR) system based on NVIDIA's FastConformer architecture--explored\nhere for the first time in the context of Romanian. We train our model on a\nlarge corpus of, mostly, weakly supervised transcriptions, totaling over 2,600\nhours of speech. Leveraging a hybrid decoder with both Connectionist Temporal\nClassification (CTC) and Token-Duration Transducer (TDT) branches, we evaluate\na range of decoding strategies including greedy, ALSD, and CTC beam search with\na 6-gram token-level language model. Our system achieves state-of-the-art\nperformance across all Romanian evaluation benchmarks, including read,\nspontaneous, and domain-specific speech, with up to 27% relative WER reduction\ncompared to previous best-performing systems. In addition to improved\ntranscription accuracy, our approach demonstrates practical decoding\nefficiency, making it suitable for both research and deployment in low-latency\nASR applications."
                },
                "authors": [
                    {
                        "name": "Gabriel Pirlogeanu"
                    },
                    {
                        "name": "Alexandru-Lucian Georgescu"
                    },
                    {
                        "name": "Horia Cucu"
                    }
                ],
                "author_detail": {
                    "name": "Horia Cucu"
                },
                "author": "Horia Cucu",
                "arxiv_comment": "13th Conference on Speech Technology and Human-Computer Dialogue\n  (SpeD 2025), Cluj-Napoca, Romania",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03361v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03361v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13890v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13890v2",
                "updated": "2025-11-05T10:30:09Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    10,
                    30,
                    9,
                    2,
                    309,
                    0
                ],
                "published": "2025-10-14T04:16:47Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    4,
                    16,
                    47,
                    1,
                    287,
                    0
                ],
                "title": "A Survey on Collaborating Small and Large Language Models for\n  Performance, Cost-effectiveness, Cloud-edge Privacy, and Trustworthiness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Collaborating Small and Large Language Models for\n  Performance, Cost-effectiveness, Cloud-edge Privacy, and Trustworthiness"
                },
                "summary": "Large language models (LLMs) have achieved remarkable progress across domains\nand applications but face challenges such as high fine-tuning costs, inference\nlatency, limited edge deployability, and reliability concerns. Small language\nmodels (SLMs), with compact, efficient, and adaptable features, offer promising\nsolutions. Building on this potential, recent research explores collaborative\nframeworks that integrate their complementary strengths, leveraging SLMs'\nspecialization and efficiency with LLMs' generalization and reasoning to\naddress diverse objectives across tasks and deployment scenarios. Motivated by\nthese developments, this paper presents a systematic survey of SLM-LLM\ncollaboration from the perspective of collaboration objectives. We propose a\ntaxonomy covering four goals: performance enhancement, cost-effectiveness,\ncloud-edge privacy, and trustworthiness. Under this framework, we review\nrepresentative methods, summarize design paradigms, and outline open challenges\nand future directions toward efficient and secure SLM-LLM collaboration. The\ncollected papers are available at https://github.com/FairyFali/SLMs-Survey.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved remarkable progress across domains\nand applications but face challenges such as high fine-tuning costs, inference\nlatency, limited edge deployability, and reliability concerns. Small language\nmodels (SLMs), with compact, efficient, and adaptable features, offer promising\nsolutions. Building on this potential, recent research explores collaborative\nframeworks that integrate their complementary strengths, leveraging SLMs'\nspecialization and efficiency with LLMs' generalization and reasoning to\naddress diverse objectives across tasks and deployment scenarios. Motivated by\nthese developments, this paper presents a systematic survey of SLM-LLM\ncollaboration from the perspective of collaboration objectives. We propose a\ntaxonomy covering four goals: performance enhancement, cost-effectiveness,\ncloud-edge privacy, and trustworthiness. Under this framework, we review\nrepresentative methods, summarize design paradigms, and outline open challenges\nand future directions toward efficient and secure SLM-LLM collaboration. The\ncollected papers are available at https://github.com/FairyFali/SLMs-Survey."
                },
                "authors": [
                    {
                        "name": "Fali Wang"
                    },
                    {
                        "name": "Jihai Chen"
                    },
                    {
                        "name": "Shuhua Yang"
                    },
                    {
                        "name": "Ali Al-Lawati"
                    },
                    {
                        "name": "Linli Tang"
                    },
                    {
                        "name": "Hui Liu"
                    },
                    {
                        "name": "Suhang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Suhang Wang"
                },
                "author": "Suhang Wang",
                "arxiv_comment": "24 pages, 19 figures-under review; more detailed than v1",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13890v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13890v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50 (Primary) 68T07 (Secondary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01522v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01522v3",
                "updated": "2025-11-05T10:20:43Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    10,
                    20,
                    43,
                    2,
                    309,
                    0
                ],
                "published": "2025-08-02T23:52:33Z",
                "published_parsed": [
                    2025,
                    8,
                    2,
                    23,
                    52,
                    33,
                    5,
                    214,
                    0
                ],
                "title": "Decentralized Aerial Manipulation of a Cable-Suspended Load using\n  Multi-Agent Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decentralized Aerial Manipulation of a Cable-Suspended Load using\n  Multi-Agent Reinforcement Learning"
                },
                "summary": "This paper presents the first decentralized method to enable real-world 6-DoF\nmanipulation of a cable-suspended load using a team of Micro-Aerial Vehicles\n(MAVs). Our method leverages multi-agent reinforcement learning (MARL) to train\nan outer-loop control policy for each MAV. Unlike state-of-the-art controllers\nthat utilize a centralized scheme, our policy does not require global states,\ninter-MAV communications, nor neighboring MAV information. Instead, agents\ncommunicate implicitly through load pose observations alone, which enables high\nscalability and flexibility. It also significantly reduces computing costs\nduring inference time, enabling onboard deployment of the policy. In addition,\nwe introduce a new action space design for the MAVs using linear acceleration\nand body rates. This choice, combined with a robust low-level controller,\nenables reliable sim-to-real transfer despite significant uncertainties caused\nby cable tension during dynamic 3D motion. We validate our method in various\nreal-world experiments, including full-pose control under load model\nuncertainties, showing setpoint tracking performance comparable to the\nstate-of-the-art centralized method. We also demonstrate cooperation amongst\nagents with heterogeneous control policies, and robustness to the complete\nin-flight loss of one MAV. Videos of experiments:\nhttps://autonomousrobots.nl/paper_websites/aerial-manipulation-marl",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents the first decentralized method to enable real-world 6-DoF\nmanipulation of a cable-suspended load using a team of Micro-Aerial Vehicles\n(MAVs). Our method leverages multi-agent reinforcement learning (MARL) to train\nan outer-loop control policy for each MAV. Unlike state-of-the-art controllers\nthat utilize a centralized scheme, our policy does not require global states,\ninter-MAV communications, nor neighboring MAV information. Instead, agents\ncommunicate implicitly through load pose observations alone, which enables high\nscalability and flexibility. It also significantly reduces computing costs\nduring inference time, enabling onboard deployment of the policy. In addition,\nwe introduce a new action space design for the MAVs using linear acceleration\nand body rates. This choice, combined with a robust low-level controller,\nenables reliable sim-to-real transfer despite significant uncertainties caused\nby cable tension during dynamic 3D motion. We validate our method in various\nreal-world experiments, including full-pose control under load model\nuncertainties, showing setpoint tracking performance comparable to the\nstate-of-the-art centralized method. We also demonstrate cooperation amongst\nagents with heterogeneous control policies, and robustness to the complete\nin-flight loss of one MAV. Videos of experiments:\nhttps://autonomousrobots.nl/paper_websites/aerial-manipulation-marl"
                },
                "authors": [
                    {
                        "name": "Jack Zeng"
                    },
                    {
                        "name": "Andreu Matoses Gimenez"
                    },
                    {
                        "name": "Eugene Vinitsky"
                    },
                    {
                        "name": "Javier Alonso-Mora"
                    },
                    {
                        "name": "Sihao Sun"
                    }
                ],
                "author_detail": {
                    "name": "Sihao Sun"
                },
                "author": "Sihao Sun",
                "arxiv_journal_ref": "Proceedings of the 9th Conference on Robot Learning, PMLR\n  305:3850-3868, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01522v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01522v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.9; I.2.11; I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02589v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02589v2",
                "updated": "2025-11-05T10:13:35Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    10,
                    13,
                    35,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-04T14:09:09Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    14,
                    9,
                    9,
                    1,
                    308,
                    0
                ],
                "title": "The ORCA Benchmark: Evaluating Real-World Calculation Accuracy in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ORCA Benchmark: Evaluating Real-World Calculation Accuracy in Large\n  Language Models"
                },
                "summary": "We present ORCA (Omni Research on Calculation in AI) Benchmark - a novel\nbenchmark that evaluates large language models (LLMs) on multi-domain,\nreal-life quantitative reasoning using verified outputs from Omni's calculator\nengine. In 500 natural-language tasks across domains such as finance, physics,\nhealth, and statistics, the five state-of-the-art systems (ChatGPT-5,\nGemini~2.5~Flash, Claude~Sonnet~4.5, Grok~4, and DeepSeek~V3.2) achieved only\n$45\\text{--}63\\,\\%$ accuracy, with errors mainly related to rounding ($35\\,\\%$)\nand calculation mistakes ($33\\,\\%$). Results in specific domains indicate\nstrengths in mathematics and engineering, but weaknesses in physics and natural\nsciences. Correlation analysis ($r \\approx 0.40\\text{--}0.65$) shows that the\nmodels often fail together but differ in the types of errors they make,\nhighlighting their partial complementarity rather than redundancy. Unlike\nstandard math datasets, ORCA evaluates step-by-step reasoning, numerical\nprecision, and domain generalization across real problems from finance,\nphysics, health, and statistics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present ORCA (Omni Research on Calculation in AI) Benchmark - a novel\nbenchmark that evaluates large language models (LLMs) on multi-domain,\nreal-life quantitative reasoning using verified outputs from Omni's calculator\nengine. In 500 natural-language tasks across domains such as finance, physics,\nhealth, and statistics, the five state-of-the-art systems (ChatGPT-5,\nGemini~2.5~Flash, Claude~Sonnet~4.5, Grok~4, and DeepSeek~V3.2) achieved only\n$45\\text{--}63\\,\\%$ accuracy, with errors mainly related to rounding ($35\\,\\%$)\nand calculation mistakes ($33\\,\\%$). Results in specific domains indicate\nstrengths in mathematics and engineering, but weaknesses in physics and natural\nsciences. Correlation analysis ($r \\approx 0.40\\text{--}0.65$) shows that the\nmodels often fail together but differ in the types of errors they make,\nhighlighting their partial complementarity rather than redundancy. Unlike\nstandard math datasets, ORCA evaluates step-by-step reasoning, numerical\nprecision, and domain generalization across real problems from finance,\nphysics, health, and statistics."
                },
                "authors": [
                    {
                        "name": "Claudia Herambourg"
                    },
                    {
                        "name": "Dawid Siuda"
                    },
                    {
                        "name": "Julia Kopczyska"
                    },
                    {
                        "name": "Joao R. L. Santos"
                    },
                    {
                        "name": "Wojciech Sas"
                    },
                    {
                        "name": "Joanna mietaska-Nowak"
                    }
                ],
                "author_detail": {
                    "name": "Joanna mietaska-Nowak"
                },
                "author": "Joanna mietaska-Nowak",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02589v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02589v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03325v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03325v1",
                "updated": "2025-11-05T09:40:16Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    9,
                    40,
                    16,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T09:40:16Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    9,
                    40,
                    16,
                    2,
                    309,
                    0
                ],
                "title": "SurgViVQA: Temporally-Grounded Video Question Answering for Surgical\n  Scene Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SurgViVQA: Temporally-Grounded Video Question Answering for Surgical\n  Scene Understanding"
                },
                "summary": "Video Question Answering (VideoQA) in the surgical domain aims to enhance\nintraoperative understanding by enabling AI models to reason over temporally\ncoherent events rather than isolated frames. Current approaches are limited to\nstatic image features, and available datasets often lack temporal annotations,\nignoring the dynamics critical for accurate procedural interpretation. We\npropose SurgViVQA, a surgical VideoQA model that extends visual reasoning from\nstatic images to dynamic surgical scenes. It uses a Masked Video--Text Encoder\nto fuse video and question features, capturing temporal cues such as motion and\ntool--tissue interactions, which a fine-tuned large language model (LLM) then\ndecodes into coherent answers. To evaluate its performance, we curated\nREAL-Colon-VQA, a colonoscopic video dataset that includes motion-related\nquestions and diagnostic attributes, as well as out-of-template questions with\nrephrased or semantically altered formulations to assess model robustness.\nExperimental validation on REAL-Colon-VQA and the public EndoVis18-VQA dataset\nshows that SurgViVQA outperforms existing image-based VQA benchmark models,\nparticularly in keyword accuracy, improving over PitVQA by +11\\% on\nREAL-Colon-VQA and +9\\% on EndoVis18-VQA. A perturbation study on the questions\nfurther confirms improved generalizability and robustness to variations in\nquestion phrasing. SurgViVQA and the REAL-Colon-VQA dataset provide a framework\nfor temporally-aware understanding in surgical VideoQA, enabling AI models to\ninterpret dynamic procedural contexts more effectively. Code and dataset\navailable at https://github.com/madratak/SurgViVQA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Question Answering (VideoQA) in the surgical domain aims to enhance\nintraoperative understanding by enabling AI models to reason over temporally\ncoherent events rather than isolated frames. Current approaches are limited to\nstatic image features, and available datasets often lack temporal annotations,\nignoring the dynamics critical for accurate procedural interpretation. We\npropose SurgViVQA, a surgical VideoQA model that extends visual reasoning from\nstatic images to dynamic surgical scenes. It uses a Masked Video--Text Encoder\nto fuse video and question features, capturing temporal cues such as motion and\ntool--tissue interactions, which a fine-tuned large language model (LLM) then\ndecodes into coherent answers. To evaluate its performance, we curated\nREAL-Colon-VQA, a colonoscopic video dataset that includes motion-related\nquestions and diagnostic attributes, as well as out-of-template questions with\nrephrased or semantically altered formulations to assess model robustness.\nExperimental validation on REAL-Colon-VQA and the public EndoVis18-VQA dataset\nshows that SurgViVQA outperforms existing image-based VQA benchmark models,\nparticularly in keyword accuracy, improving over PitVQA by +11\\% on\nREAL-Colon-VQA and +9\\% on EndoVis18-VQA. A perturbation study on the questions\nfurther confirms improved generalizability and robustness to variations in\nquestion phrasing. SurgViVQA and the REAL-Colon-VQA dataset provide a framework\nfor temporally-aware understanding in surgical VideoQA, enabling AI models to\ninterpret dynamic procedural contexts more effectively. Code and dataset\navailable at https://github.com/madratak/SurgViVQA."
                },
                "authors": [
                    {
                        "name": "Mauro Orazio Drago"
                    },
                    {
                        "name": "Luca Carlini"
                    },
                    {
                        "name": "Pelinsu Celebi Balyemez"
                    },
                    {
                        "name": "Dennis Pierantozzi"
                    },
                    {
                        "name": "Chiara Lena"
                    },
                    {
                        "name": "Cesare Hassan"
                    },
                    {
                        "name": "Danail Stoyanov"
                    },
                    {
                        "name": "Elena De Momi"
                    },
                    {
                        "name": "Sophia Bano"
                    },
                    {
                        "name": "Mobarak I. Hoque"
                    }
                ],
                "author_detail": {
                    "name": "Mobarak I. Hoque"
                },
                "author": "Mobarak I. Hoque",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03325v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03325v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18434v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18434v2",
                "updated": "2025-11-05T09:33:35Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    9,
                    33,
                    35,
                    2,
                    309,
                    0
                ],
                "published": "2025-06-23T09:16:04Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    9,
                    16,
                    4,
                    0,
                    174,
                    0
                ],
                "title": "Benchmarking Foundation Models and Parameter-Efficient Fine-Tuning for\n  Prognosis Prediction in Medical Imaging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Foundation Models and Parameter-Efficient Fine-Tuning for\n  Prognosis Prediction in Medical Imaging"
                },
                "summary": "Despite the significant potential of Foundation Models (FMs) in medical\nimaging, their application to prognosis prediction remains challenging due to\ndata scarcity, class imbalance, and task complexity, which limit their clinical\nadoption. This study introduces the first structured benchmark to assess the\nrobustness and efficiency of transfer learning strategies for FMs compared with\nconvolutional neural networks (CNNs) in predicting COVID-19 patient outcomes\nfrom chest X-rays. The goal is to systematically compare finetuning strategies,\nboth classical and parameter efficient, under realistic clinical constraints\nrelated to data scarcity and class imbalance, offering empirical guidance for\nAI deployment in clinical workflows. Four publicly available COVID-19 chest\nX-ray datasets were used, covering mortality, severity, and ICU admission, with\nvarying sample sizes and class imbalances. CNNs pretrained on ImageNet and FMs\npretrained on general or biomedical datasets were adapted using full\nfinetuning, linear probing, and parameter-efficient methods. Models were\nevaluated under full data and few shot regimes using the Matthews Correlation\nCoefficient (MCC) and Precision Recall AUC (PR-AUC), with cross validation and\nclass weighted losses. CNNs with full fine-tuning performed robustly on small,\nimbalanced datasets, while FMs with Parameter-Efficient Fine-Tuning (PEFT),\nparticularly LoRA and BitFit, achieved competitive results on larger datasets.\nSevere class imbalance degraded PEFT performance, whereas balanced data\nmitigated this effect. In few-shot settings, FMs showed limited generalization,\nwith linear probing yielding the most stable results. No single fine-tuning\nstrategy proved universally optimal: CNNs remain dependable for low-resource\nscenarios, whereas FMs benefit from parameter-efficient methods when data are\nsufficient.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the significant potential of Foundation Models (FMs) in medical\nimaging, their application to prognosis prediction remains challenging due to\ndata scarcity, class imbalance, and task complexity, which limit their clinical\nadoption. This study introduces the first structured benchmark to assess the\nrobustness and efficiency of transfer learning strategies for FMs compared with\nconvolutional neural networks (CNNs) in predicting COVID-19 patient outcomes\nfrom chest X-rays. The goal is to systematically compare finetuning strategies,\nboth classical and parameter efficient, under realistic clinical constraints\nrelated to data scarcity and class imbalance, offering empirical guidance for\nAI deployment in clinical workflows. Four publicly available COVID-19 chest\nX-ray datasets were used, covering mortality, severity, and ICU admission, with\nvarying sample sizes and class imbalances. CNNs pretrained on ImageNet and FMs\npretrained on general or biomedical datasets were adapted using full\nfinetuning, linear probing, and parameter-efficient methods. Models were\nevaluated under full data and few shot regimes using the Matthews Correlation\nCoefficient (MCC) and Precision Recall AUC (PR-AUC), with cross validation and\nclass weighted losses. CNNs with full fine-tuning performed robustly on small,\nimbalanced datasets, while FMs with Parameter-Efficient Fine-Tuning (PEFT),\nparticularly LoRA and BitFit, achieved competitive results on larger datasets.\nSevere class imbalance degraded PEFT performance, whereas balanced data\nmitigated this effect. In few-shot settings, FMs showed limited generalization,\nwith linear probing yielding the most stable results. No single fine-tuning\nstrategy proved universally optimal: CNNs remain dependable for low-resource\nscenarios, whereas FMs benefit from parameter-efficient methods when data are\nsufficient."
                },
                "authors": [
                    {
                        "name": "Filippo Ruffini"
                    },
                    {
                        "name": "Elena Mulero Ayllon"
                    },
                    {
                        "name": "Linlin Shen"
                    },
                    {
                        "name": "Paolo Soda"
                    },
                    {
                        "name": "Valerio Guarrasi"
                    }
                ],
                "author_detail": {
                    "name": "Valerio Guarrasi"
                },
                "author": "Valerio Guarrasi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18434v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18434v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09411v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09411v3",
                "updated": "2025-11-05T09:29:17Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    9,
                    29,
                    17,
                    2,
                    309,
                    0
                ],
                "published": "2025-09-11T12:46:16Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    12,
                    46,
                    16,
                    3,
                    254,
                    0
                ],
                "title": "Gaussian Copula-Based Outage Performance Analysis of Fluid Antenna\n  Systems: Channel Coefficient- or Envelope-Level Correlation Matrix?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gaussian Copula-Based Outage Performance Analysis of Fluid Antenna\n  Systems: Channel Coefficient- or Envelope-Level Correlation Matrix?"
                },
                "summary": "Gaussian copula has been employed to evaluate the outage performance of Fluid\nAntenna Systems (FAS), with the covariance matrix reflecting the dependence\namong multivariate normal random variables (RVs). While prior studies\napproximate this matrix using the channel coefficient correlation matrix from\nJake's model, this work instead employs the channel envelope correlation\nmatrix, motivated by the fact that the multivariate normal RVs are generated by\ntransforming correlated channel envelopes. This raises an open question of\nwhether using the coefficient- or envelope-level correlation matrix yields\nbetter accuracy in accessing FAS performance. Toward this end, this paper\nexplores the benefits of using the envelope-level correlation matrix under\nfully correlated Nakagami-m fading, and develops a method for generating such\nfading channels for Monte Carlo simulations, which serve as a benchmark for\nvalidating the theoretical results. Simulation results confirm the\neffectiveness of the proposed channel modeling approach and demonstrate the\nsuperior accuracy of using the envelope-level correlation matrix, particularly\nin sparse port deployment and low-outage regime.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gaussian copula has been employed to evaluate the outage performance of Fluid\nAntenna Systems (FAS), with the covariance matrix reflecting the dependence\namong multivariate normal random variables (RVs). While prior studies\napproximate this matrix using the channel coefficient correlation matrix from\nJake's model, this work instead employs the channel envelope correlation\nmatrix, motivated by the fact that the multivariate normal RVs are generated by\ntransforming correlated channel envelopes. This raises an open question of\nwhether using the coefficient- or envelope-level correlation matrix yields\nbetter accuracy in accessing FAS performance. Toward this end, this paper\nexplores the benefits of using the envelope-level correlation matrix under\nfully correlated Nakagami-m fading, and develops a method for generating such\nfading channels for Monte Carlo simulations, which serve as a benchmark for\nvalidating the theoretical results. Simulation results confirm the\neffectiveness of the proposed channel modeling approach and demonstrate the\nsuperior accuracy of using the envelope-level correlation matrix, particularly\nin sparse port deployment and low-outage regime."
                },
                "authors": [
                    {
                        "name": "Rui Xu"
                    },
                    {
                        "name": "Yinghui Ye"
                    },
                    {
                        "name": "Xiaoli Chu"
                    },
                    {
                        "name": "Guangyue Lu"
                    },
                    {
                        "name": "Farshad Rostami Ghadi"
                    },
                    {
                        "name": "Kai-Kit Wong"
                    }
                ],
                "author_detail": {
                    "name": "Kai-Kit Wong"
                },
                "author": "Kai-Kit Wong",
                "arxiv_doi": "10.1109/LWC.2025.3629524",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/LWC.2025.3629524",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.09411v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09411v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03310v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03310v1",
                "updated": "2025-11-05T09:24:48Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    9,
                    24,
                    48,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T09:24:48Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    9,
                    24,
                    48,
                    2,
                    309,
                    0
                ],
                "title": "TASU: Text-Only Alignment for Speech Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TASU: Text-Only Alignment for Speech Understanding"
                },
                "summary": "Recent advances in Speech Large Language Models (Speech LLMs) have paved the\nway for unified architectures across diverse speech understanding tasks.\nHowever, prevailing alignment paradigms rely heavily on large-scale audio-text\npaired data and computationally intensive training, yet often exhibit limited\ngeneralization to unseen domains or tasks. To address these limitations, we\npropose TASU (Text-only Alignment for Speech Understanding), a novel alignment\nparadigm that can leverage only unpaired text data to guide cross-modal\nalignment. Experiments show that TASU achieves competitive zero-shot speech\nrecognition. Leveraging this property, it can further function as a\npre-training stage in curriculum learning, enhancing domain generalization in\nspeech recognition. Ultimately, TASU can extend its zero-shot generalization to\na wide range of speech understanding tasks and notably outperforms prominent\nSpeech LLMs including GLM-4-Voice and Step-Audio on the MMSU benchmark,\nestablishing TASU as an efficient and scalable alignment paradigm for Speech\nLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Speech Large Language Models (Speech LLMs) have paved the\nway for unified architectures across diverse speech understanding tasks.\nHowever, prevailing alignment paradigms rely heavily on large-scale audio-text\npaired data and computationally intensive training, yet often exhibit limited\ngeneralization to unseen domains or tasks. To address these limitations, we\npropose TASU (Text-only Alignment for Speech Understanding), a novel alignment\nparadigm that can leverage only unpaired text data to guide cross-modal\nalignment. Experiments show that TASU achieves competitive zero-shot speech\nrecognition. Leveraging this property, it can further function as a\npre-training stage in curriculum learning, enhancing domain generalization in\nspeech recognition. Ultimately, TASU can extend its zero-shot generalization to\na wide range of speech understanding tasks and notably outperforms prominent\nSpeech LLMs including GLM-4-Voice and Step-Audio on the MMSU benchmark,\nestablishing TASU as an efficient and scalable alignment paradigm for Speech\nLLMs."
                },
                "authors": [
                    {
                        "name": "Jing Peng"
                    },
                    {
                        "name": "Yi Yang"
                    },
                    {
                        "name": "Xu Li"
                    },
                    {
                        "name": "Yu Xi"
                    },
                    {
                        "name": "Quanwei Tang"
                    },
                    {
                        "name": "Yangui Fang"
                    },
                    {
                        "name": "Junjie Li"
                    },
                    {
                        "name": "Kai Yu"
                    }
                ],
                "author_detail": {
                    "name": "Kai Yu"
                },
                "author": "Kai Yu",
                "arxiv_comment": "This paper is submitted to ICASSP 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03310v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03310v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.03159v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.03159v2",
                "updated": "2025-11-05T09:19:17Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    9,
                    19,
                    17,
                    2,
                    309,
                    0
                ],
                "published": "2025-08-05T07:04:44Z",
                "published_parsed": [
                    2025,
                    8,
                    5,
                    7,
                    4,
                    44,
                    1,
                    217,
                    0
                ],
                "title": "CoTox: Chain-of-Thought-Based Molecular Toxicity Reasoning and\n  Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoTox: Chain-of-Thought-Based Molecular Toxicity Reasoning and\n  Prediction"
                },
                "summary": "Drug toxicity remains a major challenge in pharmaceutical development. Recent\nmachine learning models have improved in silico toxicity prediction, but their\nreliance on annotated data and lack of interpretability limit their\napplicability. This limits their ability to capture organ-specific toxicities\ndriven by complex biological mechanisms. Large language models (LLMs) offer a\npromising alternative through step-by-step reasoning and integration of textual\ndata, yet prior approaches lack biological context and transparent rationale.\nTo address this issue, we propose CoTox, a novel framework that integrates LLM\nwith chain-of-thought (CoT) reasoning for multi-toxicity prediction. CoTox\ncombines chemical structure data, biological pathways, and gene ontology (GO)\nterms to generate interpretable toxicity predictions through step-by-step\nreasoning. Using GPT-4o, we show that CoTox outperforms both traditional\nmachine learning and deep learning model. We further examine its performance\nacross various LLMs to identify where CoTox is most effective. Additionally, we\nfind that representing chemical structures with IUPAC names, which are easier\nfor LLMs to understand than SMILES, enhances the model's reasoning ability and\nimproves predictive performance. To demonstrate its practical utility in drug\ndevelopment, we simulate the treatment of relevant cell types with drug and\nincorporated the resulting biological context into the CoTox framework. This\napproach allow CoTox to generate toxicity predictions aligned with\nphysiological responses, as shown in case study. This result highlights the\npotential of LLM-based frameworks to improve interpretability and support\nearly-stage drug safety assessment. The code and prompt used in this work are\navailable at https://github.com/dmis-lab/CoTox.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Drug toxicity remains a major challenge in pharmaceutical development. Recent\nmachine learning models have improved in silico toxicity prediction, but their\nreliance on annotated data and lack of interpretability limit their\napplicability. This limits their ability to capture organ-specific toxicities\ndriven by complex biological mechanisms. Large language models (LLMs) offer a\npromising alternative through step-by-step reasoning and integration of textual\ndata, yet prior approaches lack biological context and transparent rationale.\nTo address this issue, we propose CoTox, a novel framework that integrates LLM\nwith chain-of-thought (CoT) reasoning for multi-toxicity prediction. CoTox\ncombines chemical structure data, biological pathways, and gene ontology (GO)\nterms to generate interpretable toxicity predictions through step-by-step\nreasoning. Using GPT-4o, we show that CoTox outperforms both traditional\nmachine learning and deep learning model. We further examine its performance\nacross various LLMs to identify where CoTox is most effective. Additionally, we\nfind that representing chemical structures with IUPAC names, which are easier\nfor LLMs to understand than SMILES, enhances the model's reasoning ability and\nimproves predictive performance. To demonstrate its practical utility in drug\ndevelopment, we simulate the treatment of relevant cell types with drug and\nincorporated the resulting biological context into the CoTox framework. This\napproach allow CoTox to generate toxicity predictions aligned with\nphysiological responses, as shown in case study. This result highlights the\npotential of LLM-based frameworks to improve interpretability and support\nearly-stage drug safety assessment. The code and prompt used in this work are\navailable at https://github.com/dmis-lab/CoTox."
                },
                "authors": [
                    {
                        "name": "Jueon Park"
                    },
                    {
                        "name": "Yein Park"
                    },
                    {
                        "name": "Minju Song"
                    },
                    {
                        "name": "Soyon Park"
                    },
                    {
                        "name": "Donghyeon Lee"
                    },
                    {
                        "name": "Seungheun Baek"
                    },
                    {
                        "name": "Jaewoo Kang"
                    }
                ],
                "author_detail": {
                    "name": "Jaewoo Kang"
                },
                "author": "Jaewoo Kang",
                "arxiv_comment": "Accepted to IEEE BIBM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.03159v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.03159v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.10275v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.10275v2",
                "updated": "2025-11-05T09:00:09Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    9,
                    0,
                    9,
                    2,
                    309,
                    0
                ],
                "published": "2025-06-12T01:38:15Z",
                "published_parsed": [
                    2025,
                    6,
                    12,
                    1,
                    38,
                    15,
                    3,
                    163,
                    0
                ],
                "title": "VQC-MLPNet: An Unconventional Hybrid Quantum-Classical Architecture for\n  Scalable and Robust Quantum Machine Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VQC-MLPNet: An Unconventional Hybrid Quantum-Classical Architecture for\n  Scalable and Robust Quantum Machine Learning"
                },
                "summary": "Variational quantum circuits (VQCs) hold promise for quantum machine learning\nbut face challenges in expressivity, trainability, and noise resilience. We\npropose VQC-MLPNet, a hybrid architecture where a VQC generates the first-layer\nweights of a classical multilayer perceptron during training, while inference\nis performed entirely classically. This design preserves scalability, reduces\nquantum resource demands, and enables practical deployment. We provide a\ntheoretical analysis based on statistical learning and neural tangent kernel\ntheory, establishing explicit risk bounds and demonstrating improved\nexpressivity and trainability compared to purely quantum or existing hybrid\napproaches. These theoretical insights demonstrate exponential improvements in\nrepresentation capacity relative to quantum circuit depth and the number of\nqubits, providing clear computational advantages over standalone quantum\ncircuits and existing hybrid quantum architectures. Empirical results on\ndiverse datasets, including quantum-dot classification and genomic sequence\nanalysis, show that VQC-MLPNet achieves high accuracy and robustness under\nrealistic noise models, outperforming classical and quantum baselines while\nusing significantly fewer trainable parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variational quantum circuits (VQCs) hold promise for quantum machine learning\nbut face challenges in expressivity, trainability, and noise resilience. We\npropose VQC-MLPNet, a hybrid architecture where a VQC generates the first-layer\nweights of a classical multilayer perceptron during training, while inference\nis performed entirely classically. This design preserves scalability, reduces\nquantum resource demands, and enables practical deployment. We provide a\ntheoretical analysis based on statistical learning and neural tangent kernel\ntheory, establishing explicit risk bounds and demonstrating improved\nexpressivity and trainability compared to purely quantum or existing hybrid\napproaches. These theoretical insights demonstrate exponential improvements in\nrepresentation capacity relative to quantum circuit depth and the number of\nqubits, providing clear computational advantages over standalone quantum\ncircuits and existing hybrid quantum architectures. Empirical results on\ndiverse datasets, including quantum-dot classification and genomic sequence\nanalysis, show that VQC-MLPNet achieves high accuracy and robustness under\nrealistic noise models, outperforming classical and quantum baselines while\nusing significantly fewer trainable parameters."
                },
                "authors": [
                    {
                        "name": "Jun Qi"
                    },
                    {
                        "name": "Chao-Han Yang"
                    },
                    {
                        "name": "Pin-Yu Chen"
                    },
                    {
                        "name": "Min-Hsiu Hsieh"
                    }
                ],
                "author_detail": {
                    "name": "Min-Hsiu Hsieh"
                },
                "author": "Min-Hsiu Hsieh",
                "arxiv_comment": "In submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.10275v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.10275v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21298v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21298v3",
                "updated": "2025-11-05T08:55:08Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    8,
                    55,
                    8,
                    2,
                    309,
                    0
                ],
                "published": "2025-05-27T15:01:06Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    15,
                    1,
                    6,
                    1,
                    147,
                    0
                ],
                "title": "Large Language Models Miss the Multi-Agent Mark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models Miss the Multi-Agent Mark"
                },
                "summary": "Recent interest in Multi-Agent Systems of Large Language Models (MAS LLMs)\nhas led to an increase in frameworks leveraging multiple LLMs to tackle complex\ntasks. However, much of this literature appropriates the terminology of MAS\nwithout engaging with its foundational principles. In this position paper, we\nhighlight critical discrepancies between MAS theory and current MAS LLMs\nimplementations, focusing on four key areas: the social aspect of agency,\nenvironment design, coordination and communication protocols, and measuring\nemergent behaviours. Our position is that many MAS LLMs lack multi-agent\ncharacteristics such as autonomy, social interaction, and structured\nenvironments, and often rely on oversimplified, LLM-centric architectures. The\nfield may slow down and lose traction by revisiting problems the MAS literature\nhas already addressed. Therefore, we systematically analyse this issue and\noutline associated research opportunities; we advocate for better integrating\nestablished MAS concepts and more precise terminology to avoid\nmischaracterisation and missed opportunities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent interest in Multi-Agent Systems of Large Language Models (MAS LLMs)\nhas led to an increase in frameworks leveraging multiple LLMs to tackle complex\ntasks. However, much of this literature appropriates the terminology of MAS\nwithout engaging with its foundational principles. In this position paper, we\nhighlight critical discrepancies between MAS theory and current MAS LLMs\nimplementations, focusing on four key areas: the social aspect of agency,\nenvironment design, coordination and communication protocols, and measuring\nemergent behaviours. Our position is that many MAS LLMs lack multi-agent\ncharacteristics such as autonomy, social interaction, and structured\nenvironments, and often rely on oversimplified, LLM-centric architectures. The\nfield may slow down and lose traction by revisiting problems the MAS literature\nhas already addressed. Therefore, we systematically analyse this issue and\noutline associated research opportunities; we advocate for better integrating\nestablished MAS concepts and more precise terminology to avoid\nmischaracterisation and missed opportunities."
                },
                "authors": [
                    {
                        "name": "Emanuele La Malfa"
                    },
                    {
                        "name": "Gabriele La Malfa"
                    },
                    {
                        "name": "Samuele Marro"
                    },
                    {
                        "name": "Jie M. Zhang"
                    },
                    {
                        "name": "Elizabeth Black"
                    },
                    {
                        "name": "Michael Luck"
                    },
                    {
                        "name": "Philip Torr"
                    },
                    {
                        "name": "Michael Wooldridge"
                    }
                ],
                "author_detail": {
                    "name": "Michael Wooldridge"
                },
                "author": "Michael Wooldridge",
                "arxiv_comment": "NeurIPS 2025 (position track)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21298v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21298v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04041v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04041v2",
                "updated": "2025-11-05T08:52:45Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    8,
                    52,
                    45,
                    2,
                    309,
                    0
                ],
                "published": "2025-06-04T15:06:27Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    15,
                    6,
                    27,
                    2,
                    155,
                    0
                ],
                "title": "LexTime: A Benchmark for Temporal Ordering of Legal Events",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LexTime: A Benchmark for Temporal Ordering of Legal Events"
                },
                "summary": "Understanding temporal relationships and accurately reconstructing the event\ntimeline is important for case law analysis, compliance monitoring, and legal\nsummarization. However, existing benchmarks lack specialized language\nevaluation, leaving a gap in understanding how LLMs handle event ordering in\nlegal contexts. We introduce LexTime, a dataset designed to evaluate LLMs'\nevent ordering capabilities in legal language, consisting of 512 instances from\nU.S. Federal Complaints with annotated event pairs and their temporal\nrelations. Our findings show that (1) LLMs are more accurate on legal event\nordering than on narrative texts (up to +10.5%); (2) longer input contexts and\nimplicit events boost accuracy, reaching 80.8% for implicit-explicit event\npairs; (3) legal linguistic complexities and nested clauses remain a challenge.\nWhile performance is promising, specific features of legal texts remain a\nbottleneck for legal temporal event reasoning, and we propose concrete modeling\ndirections to better address them.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding temporal relationships and accurately reconstructing the event\ntimeline is important for case law analysis, compliance monitoring, and legal\nsummarization. However, existing benchmarks lack specialized language\nevaluation, leaving a gap in understanding how LLMs handle event ordering in\nlegal contexts. We introduce LexTime, a dataset designed to evaluate LLMs'\nevent ordering capabilities in legal language, consisting of 512 instances from\nU.S. Federal Complaints with annotated event pairs and their temporal\nrelations. Our findings show that (1) LLMs are more accurate on legal event\nordering than on narrative texts (up to +10.5%); (2) longer input contexts and\nimplicit events boost accuracy, reaching 80.8% for implicit-explicit event\npairs; (3) legal linguistic complexities and nested clauses remain a challenge.\nWhile performance is promising, specific features of legal texts remain a\nbottleneck for legal temporal event reasoning, and we propose concrete modeling\ndirections to better address them."
                },
                "authors": [
                    {
                        "name": "Claire Barale"
                    },
                    {
                        "name": "Leslie Barrett"
                    },
                    {
                        "name": "Vikram Sunil Bajaj"
                    },
                    {
                        "name": "Michael Rovatsos"
                    }
                ],
                "author_detail": {
                    "name": "Michael Rovatsos"
                },
                "author": "Michael Rovatsos",
                "arxiv_comment": "EMNLP 2025 (Findings) long paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04041v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04041v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03293v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03293v1",
                "updated": "2025-11-05T08:44:19Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    8,
                    44,
                    19,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T08:44:19Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    8,
                    44,
                    19,
                    2,
                    309,
                    0
                ],
                "title": "UMDAM: A Unified Data Layout and DRAM Address Mapping for Heterogenous\n  NPU-PIM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UMDAM: A Unified Data Layout and DRAM Address Mapping for Heterogenous\n  NPU-PIM"
                },
                "summary": "Large Language Models (LLMs) are increasingly deployed on edge devices with\nNeural Processing Units (NPUs), yet the decode phase remains memory-intensive,\nlimiting performance. Processing-in-Memory (PIM) offers a promising solution,\nbut co-executing NPU-PIM systems face challenges such as data layout\nmismatches, bandwidth loss, and redundant storage. To address these issues, we\npropose UMDAM, a unified memory-affinity data layout and DRAM address mapping\nscheme tailored for NPU-PIM co-execution. UMDAM employs a column-major,\ntile-based layout and a configurable DRAM mapping strategy to ensure\ncompatibility with NPU computation while maximizing PIM efficiency -- without\nintroducing extra memory overhead or bandwidth loss. Comprehensive evaluations\non OPT models demonstrate that UMDAM reduces time-to-first-token (TTFT) by up\nto 3.0x and time-to-last-token (TTLT) by 2.18x, significantly improving\nend-to-end LLM inference efficiency on edge devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly deployed on edge devices with\nNeural Processing Units (NPUs), yet the decode phase remains memory-intensive,\nlimiting performance. Processing-in-Memory (PIM) offers a promising solution,\nbut co-executing NPU-PIM systems face challenges such as data layout\nmismatches, bandwidth loss, and redundant storage. To address these issues, we\npropose UMDAM, a unified memory-affinity data layout and DRAM address mapping\nscheme tailored for NPU-PIM co-execution. UMDAM employs a column-major,\ntile-based layout and a configurable DRAM mapping strategy to ensure\ncompatibility with NPU computation while maximizing PIM efficiency -- without\nintroducing extra memory overhead or bandwidth loss. Comprehensive evaluations\non OPT models demonstrate that UMDAM reduces time-to-first-token (TTFT) by up\nto 3.0x and time-to-last-token (TTLT) by 2.18x, significantly improving\nend-to-end LLM inference efficiency on edge devices."
                },
                "authors": [
                    {
                        "name": "Hai Huang"
                    },
                    {
                        "name": "Xuhong Qiang"
                    },
                    {
                        "name": "Weisheng Zhao"
                    },
                    {
                        "name": "Chenchen Liu"
                    }
                ],
                "author_detail": {
                    "name": "Chenchen Liu"
                },
                "author": "Chenchen Liu",
                "arxiv_comment": "5 pages, 5 figures, under review for IEEE ISCAS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03293v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03293v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03284v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03284v1",
                "updated": "2025-11-05T08:26:13Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    8,
                    26,
                    13,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T08:26:13Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    8,
                    26,
                    13,
                    2,
                    309,
                    0
                ],
                "title": "Decentralized Federated Learning with Distributed Aggregation Weight\n  Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decentralized Federated Learning with Distributed Aggregation Weight\n  Optimization"
                },
                "summary": "Decentralized federated learning (DFL) is an emerging paradigm to enable edge\ndevices collaboratively training a learning model using a device-to-device\n(D2D) communication manner without the coordination of a parameter server (PS).\nAggregation weights, also known as mixing weights, are crucial in DFL process,\nand impact the learning efficiency and accuracy. Conventional design relies on\na so-called central entity to collect all local information and conduct system\noptimization to obtain appropriate weights. In this paper, we develop a\ndistributed aggregation weight optimization algorithm to align with the\ndecentralized nature of DFL. We analyze convergence by quantitatively capturing\nthe impact of the aggregation weights over decentralized communication\nnetworks. Based on the analysis, we then formulate a learning performance\noptimization problem by designing the aggregation weights to minimize the\nderived convergence bound. The optimization problem is further transformed as\nan eigenvalue optimization problem and solved by our proposed subgradient-based\nalgorithm in a distributed fashion. In our algorithm, edge devices only need\nlocal information to obtain the optimal aggregation weights through local (D2D)\ncommunications, just like the learning itself. Therefore, the optimization,\ncommunication, and learning process can be all conducted in a distributed\nfashion, which leads to a genuinely distributed DFL system. Numerical results\ndemonstrate the superiority of the proposed algorithm in practical DFL\ndeployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decentralized federated learning (DFL) is an emerging paradigm to enable edge\ndevices collaboratively training a learning model using a device-to-device\n(D2D) communication manner without the coordination of a parameter server (PS).\nAggregation weights, also known as mixing weights, are crucial in DFL process,\nand impact the learning efficiency and accuracy. Conventional design relies on\na so-called central entity to collect all local information and conduct system\noptimization to obtain appropriate weights. In this paper, we develop a\ndistributed aggregation weight optimization algorithm to align with the\ndecentralized nature of DFL. We analyze convergence by quantitatively capturing\nthe impact of the aggregation weights over decentralized communication\nnetworks. Based on the analysis, we then formulate a learning performance\noptimization problem by designing the aggregation weights to minimize the\nderived convergence bound. The optimization problem is further transformed as\nan eigenvalue optimization problem and solved by our proposed subgradient-based\nalgorithm in a distributed fashion. In our algorithm, edge devices only need\nlocal information to obtain the optimal aggregation weights through local (D2D)\ncommunications, just like the learning itself. Therefore, the optimization,\ncommunication, and learning process can be all conducted in a distributed\nfashion, which leads to a genuinely distributed DFL system. Numerical results\ndemonstrate the superiority of the proposed algorithm in practical DFL\ndeployment."
                },
                "authors": [
                    {
                        "name": "Zhiyuan Zhai"
                    },
                    {
                        "name": "Xiaojun Yuan"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Geoffrey Ye Li"
                    }
                ],
                "author_detail": {
                    "name": "Geoffrey Ye Li"
                },
                "author": "Geoffrey Ye Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03284v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03284v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03279v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03279v1",
                "updated": "2025-11-05T08:22:42Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    8,
                    22,
                    42,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T08:22:42Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    8,
                    22,
                    42,
                    2,
                    309,
                    0
                ],
                "title": "Multi-Objective Adaptive Rate Limiting in Microservices Using Deep\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Objective Adaptive Rate Limiting in Microservices Using Deep\n  Reinforcement Learning"
                },
                "summary": "As cloud computing and microservice architectures become increasingly\nprevalent, API rate limiting has emerged as a critical mechanism for ensuring\nsystem stability and service quality. Traditional rate limiting algorithms,\nsuch as token bucket and sliding window, while widely adopted, struggle to\nadapt to dynamic traffic patterns and varying system loads. This paper proposes\nan adaptive rate limiting strategy based on deep reinforcement learning that\ndynamically balances system throughput and service latency. We design a hybrid\narchitecture combining Deep Q-Network (DQN) and Asynchronous Advantage\nActor-Critic (A3C) algorithms, modeling the rate limiting decision process as a\nMarkov Decision Process. The system continuously monitors microservice states\nand learns optimal rate limiting policies through environmental interaction.\nExtensive experiments conducted in a Kubernetes cluster environment demonstrate\nthat our approach achieves 23.7% throughput improvement and 31.4% P99 latency\nreduction compared to traditional fixed-threshold strategies under high-load\nscenarios. Results from a 90-day production deployment handling 500 million\ndaily requests validate the practical effectiveness of the proposed method,\nwith 82% reduction in service degradation incidents and 68% decrease in manual\ninterventions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As cloud computing and microservice architectures become increasingly\nprevalent, API rate limiting has emerged as a critical mechanism for ensuring\nsystem stability and service quality. Traditional rate limiting algorithms,\nsuch as token bucket and sliding window, while widely adopted, struggle to\nadapt to dynamic traffic patterns and varying system loads. This paper proposes\nan adaptive rate limiting strategy based on deep reinforcement learning that\ndynamically balances system throughput and service latency. We design a hybrid\narchitecture combining Deep Q-Network (DQN) and Asynchronous Advantage\nActor-Critic (A3C) algorithms, modeling the rate limiting decision process as a\nMarkov Decision Process. The system continuously monitors microservice states\nand learns optimal rate limiting policies through environmental interaction.\nExtensive experiments conducted in a Kubernetes cluster environment demonstrate\nthat our approach achieves 23.7% throughput improvement and 31.4% P99 latency\nreduction compared to traditional fixed-threshold strategies under high-load\nscenarios. Results from a 90-day production deployment handling 500 million\ndaily requests validate the practical effectiveness of the proposed method,\nwith 82% reduction in service degradation incidents and 68% decrease in manual\ninterventions."
                },
                "authors": [
                    {
                        "name": "Ning Lyu"
                    },
                    {
                        "name": "Yuxi Wang"
                    },
                    {
                        "name": "Ziyu Cheng"
                    },
                    {
                        "name": "Qingyuan Zhang"
                    },
                    {
                        "name": "Feng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Feng Chen"
                },
                "author": "Feng Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03279v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03279v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.12605v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.12605v3",
                "updated": "2025-11-05T08:17:16Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    8,
                    17,
                    16,
                    2,
                    309,
                    0
                ],
                "published": "2025-06-14T19:00:37Z",
                "published_parsed": [
                    2025,
                    6,
                    14,
                    19,
                    0,
                    37,
                    5,
                    165,
                    0
                ],
                "title": "The Rise of AI Companions: How Human-Chatbot Relationships Influence\n  Well-Being",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Rise of AI Companions: How Human-Chatbot Relationships Influence\n  Well-Being"
                },
                "summary": "As large language models (LLMs)-enhanced chatbots grow increasingly\nexpressive and socially responsive, many users are beginning to form\ncompanionship-like bonds with them, particularly with simulated AI partners\ndesigned to mimic emotionally attuned interlocutors. These emerging AI\ncompanions raise critical questions: Can such systems fulfill social needs\ntypically met by human relationships? How do they shape psychological\nwell-being? And what new risks arise as users develop emotional ties to\nnon-human agents? This study investigates how people interact with AI\ncompanions, especially simulated partners on CharacterAI, and how this use is\nassociated with users' psychological well-being. We analyzed survey data from\n1,131 users and 4,363 chat sessions (413,509 messages) donated by 244\nparticipants, focusing on three dimensions of use: nature of the interaction,\ninteraction intensity, and self-disclosure. By triangulating self-reports\nprimary motivation, open-ended relationship descriptions, and annotated chat\ntranscripts, we identify patterns in how users engage with AI companions and\nits associations with well-being. Findings suggest that people with smaller\nsocial networks are more likely to turn to chatbots for companionship, but that\ncompanionship-oriented chatbot usage is consistently associated with lower\nwell-being, particularly when people use the chatbots more intensively, engage\nin higher levels of self-disclosure, and lack strong human social support. Even\nthough some people turn to chatbots to fulfill social needs, these uses of\nchatbots do not fully substitute for human connection. As a result, the\npsychological benefits may be limited, and the relationship could pose risks\nfor more socially isolated or emotionally vulnerable users.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs)-enhanced chatbots grow increasingly\nexpressive and socially responsive, many users are beginning to form\ncompanionship-like bonds with them, particularly with simulated AI partners\ndesigned to mimic emotionally attuned interlocutors. These emerging AI\ncompanions raise critical questions: Can such systems fulfill social needs\ntypically met by human relationships? How do they shape psychological\nwell-being? And what new risks arise as users develop emotional ties to\nnon-human agents? This study investigates how people interact with AI\ncompanions, especially simulated partners on CharacterAI, and how this use is\nassociated with users' psychological well-being. We analyzed survey data from\n1,131 users and 4,363 chat sessions (413,509 messages) donated by 244\nparticipants, focusing on three dimensions of use: nature of the interaction,\ninteraction intensity, and self-disclosure. By triangulating self-reports\nprimary motivation, open-ended relationship descriptions, and annotated chat\ntranscripts, we identify patterns in how users engage with AI companions and\nits associations with well-being. Findings suggest that people with smaller\nsocial networks are more likely to turn to chatbots for companionship, but that\ncompanionship-oriented chatbot usage is consistently associated with lower\nwell-being, particularly when people use the chatbots more intensively, engage\nin higher levels of self-disclosure, and lack strong human social support. Even\nthough some people turn to chatbots to fulfill social needs, these uses of\nchatbots do not fully substitute for human connection. As a result, the\npsychological benefits may be limited, and the relationship could pose risks\nfor more socially isolated or emotionally vulnerable users."
                },
                "authors": [
                    {
                        "name": "Yutong Zhang"
                    },
                    {
                        "name": "Dora Zhao"
                    },
                    {
                        "name": "Jeffrey T. Hancock"
                    },
                    {
                        "name": "Robert Kraut"
                    },
                    {
                        "name": "Diyi Yang"
                    }
                ],
                "author_detail": {
                    "name": "Diyi Yang"
                },
                "author": "Diyi Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.12605v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.12605v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03271v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03271v1",
                "updated": "2025-11-05T08:05:58Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    8,
                    5,
                    58,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T08:05:58Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    8,
                    5,
                    58,
                    2,
                    309,
                    0
                ],
                "title": "Let the Bees Find the Weak Spots: A Path Planning Perspective on\n  Multi-Turn Jailbreak Attacks against LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Let the Bees Find the Weak Spots: A Path Planning Perspective on\n  Multi-Turn Jailbreak Attacks against LLMs"
                },
                "summary": "Large Language Models (LLMs) have been widely deployed across various\napplications, yet their potential security and ethical risks have raised\nincreasing concerns. Existing research employs red teaming evaluations,\nutilizing multi-turn jailbreaks to identify potential vulnerabilities in LLMs.\nHowever, these approaches often lack exploration of successful dialogue\ntrajectories within the attack space, and they tend to overlook the\nconsiderable overhead associated with the attack process. To address these\nlimitations, this paper first introduces a theoretical model based on\ndynamically weighted graph topology, abstracting the multi-turn attack process\nas a path planning problem. Based on this framework, we propose ABC, an\nenhanced Artificial Bee Colony algorithm for multi-turn jailbreaks, featuring a\ncollaborative search mechanism with employed, onlooker, and scout bees. This\nalgorithm significantly improves the efficiency of optimal attack path search\nwhile substantially reducing the average number of queries required. Empirical\nevaluations on three open-source and two proprietary language models\ndemonstrate the effectiveness of our approach, achieving attack success rates\nabove 90\\% across the board, with a peak of 98\\% on GPT-3.5-Turbo, and\noutperforming existing baselines. Furthermore, it achieves comparable success\nwith only 26 queries on average, significantly reducing red teaming overhead\nand highlighting its superior efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been widely deployed across various\napplications, yet their potential security and ethical risks have raised\nincreasing concerns. Existing research employs red teaming evaluations,\nutilizing multi-turn jailbreaks to identify potential vulnerabilities in LLMs.\nHowever, these approaches often lack exploration of successful dialogue\ntrajectories within the attack space, and they tend to overlook the\nconsiderable overhead associated with the attack process. To address these\nlimitations, this paper first introduces a theoretical model based on\ndynamically weighted graph topology, abstracting the multi-turn attack process\nas a path planning problem. Based on this framework, we propose ABC, an\nenhanced Artificial Bee Colony algorithm for multi-turn jailbreaks, featuring a\ncollaborative search mechanism with employed, onlooker, and scout bees. This\nalgorithm significantly improves the efficiency of optimal attack path search\nwhile substantially reducing the average number of queries required. Empirical\nevaluations on three open-source and two proprietary language models\ndemonstrate the effectiveness of our approach, achieving attack success rates\nabove 90\\% across the board, with a peak of 98\\% on GPT-3.5-Turbo, and\noutperforming existing baselines. Furthermore, it achieves comparable success\nwith only 26 queries on average, significantly reducing red teaming overhead\nand highlighting its superior efficiency."
                },
                "authors": [
                    {
                        "name": "Yize Liu"
                    },
                    {
                        "name": "Yunyun Hou"
                    },
                    {
                        "name": "Aina Sui"
                    }
                ],
                "author_detail": {
                    "name": "Aina Sui"
                },
                "author": "Aina Sui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03271v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03271v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.14562v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.14562v3",
                "updated": "2025-11-05T08:04:59Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    8,
                    4,
                    59,
                    2,
                    309,
                    0
                ],
                "published": "2025-06-17T14:21:10Z",
                "published_parsed": [
                    2025,
                    6,
                    17,
                    14,
                    21,
                    10,
                    1,
                    168,
                    0
                ],
                "title": "AlphaDecay: Module-wise Weight Decay for Heavy-Tailed Balancing in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AlphaDecay: Module-wise Weight Decay for Heavy-Tailed Balancing in LLMs"
                },
                "summary": "Weight decay is a standard regularization technique for training large\nlanguage models (LLMs). While it is common to assign a uniform decay rate to\nevery layer, this approach overlooks the structural diversity of LLMs and the\nvarying spectral properties across modules. In this paper, we introduce\nAlphaDecay, a simple yet effective method that adaptively assigns different\nweight decay strengths to each module of an LLM. Our approach is guided by\nHeavy-Tailed Self-Regularization (HT-SR) theory, which analyzes the empirical\nspectral density (ESD) of weight correlation matrices to quantify\n\"heavy-tailedness.\" Modules exhibiting more pronounced heavy-tailed ESDs,\nreflecting stronger feature learning, are assigned weaker decay, while modules\nwith lighter-tailed spectra receive stronger decay. Our method leverages\ntailored weight decay assignments to balance the module-wise differences in\nspectral properties, leading to improved performance. Extensive pre-training\ntasks with various model sizes from 60M to 1B demonstrate that AlphaDecay\nachieves better perplexity and generalization than conventional uniform decay\nand other adaptive decay baselines. Our code is available at\nhttps://github.com/hed-ucas/AlphaDecay.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Weight decay is a standard regularization technique for training large\nlanguage models (LLMs). While it is common to assign a uniform decay rate to\nevery layer, this approach overlooks the structural diversity of LLMs and the\nvarying spectral properties across modules. In this paper, we introduce\nAlphaDecay, a simple yet effective method that adaptively assigns different\nweight decay strengths to each module of an LLM. Our approach is guided by\nHeavy-Tailed Self-Regularization (HT-SR) theory, which analyzes the empirical\nspectral density (ESD) of weight correlation matrices to quantify\n\"heavy-tailedness.\" Modules exhibiting more pronounced heavy-tailed ESDs,\nreflecting stronger feature learning, are assigned weaker decay, while modules\nwith lighter-tailed spectra receive stronger decay. Our method leverages\ntailored weight decay assignments to balance the module-wise differences in\nspectral properties, leading to improved performance. Extensive pre-training\ntasks with various model sizes from 60M to 1B demonstrate that AlphaDecay\nachieves better perplexity and generalization than conventional uniform decay\nand other adaptive decay baselines. Our code is available at\nhttps://github.com/hed-ucas/AlphaDecay."
                },
                "authors": [
                    {
                        "name": "Di He"
                    },
                    {
                        "name": "Songjun Tu"
                    },
                    {
                        "name": "Ajay Jaiswal"
                    },
                    {
                        "name": "Li Shen"
                    },
                    {
                        "name": "Ganzhao Yuan"
                    },
                    {
                        "name": "Shiwei Liu"
                    },
                    {
                        "name": "Lu Yin"
                    }
                ],
                "author_detail": {
                    "name": "Lu Yin"
                },
                "author": "Lu Yin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.14562v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.14562v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00079v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00079v2",
                "updated": "2025-11-05T07:50:45Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    7,
                    50,
                    45,
                    2,
                    309,
                    0
                ],
                "published": "2025-07-31T18:12:51Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    18,
                    12,
                    51,
                    3,
                    212,
                    0
                ],
                "title": "PhysicsEval: Inference-Time Techniques to Improve the Reasoning\n  Proficiency of Large Language Models on Physics Problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PhysicsEval: Inference-Time Techniques to Improve the Reasoning\n  Proficiency of Large Language Models on Physics Problems"
                },
                "summary": "The discipline of physics stands as a cornerstone of human intellect, driving\nthe evolution of technology and deepening our understanding of the fundamental\nprinciples of the cosmos. Contemporary literature includes some works centered\non the task of solving physics problems - a crucial domain of natural language\nreasoning. In this paper, we evaluate the performance of frontier LLMs in\nsolving physics problems, both mathematical and descriptive. We also employ a\nplethora of inference-time techniques and agentic frameworks to improve the\nperformance of the models. This includes the verification of proposed solutions\nin a cumulative fashion by other, smaller LLM agents, and we perform a\ncomparative analysis of the performance that the techniques entail. There are\nsignificant improvements when the multi-agent framework is applied to problems\nthat the models initially perform poorly on. Furthermore, we introduce a new\nevaluation benchmark for physics problems, ${\\rm P{\\small HYSICS}E{\\small\nVAL}}$, consisting of 19,609 problems sourced from various physics textbooks\nand their corresponding correct solutions scraped from physics forums and\neducational websites. Our code and data are publicly available at\nhttps://github.com/areebuzair/PhysicsEval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The discipline of physics stands as a cornerstone of human intellect, driving\nthe evolution of technology and deepening our understanding of the fundamental\nprinciples of the cosmos. Contemporary literature includes some works centered\non the task of solving physics problems - a crucial domain of natural language\nreasoning. In this paper, we evaluate the performance of frontier LLMs in\nsolving physics problems, both mathematical and descriptive. We also employ a\nplethora of inference-time techniques and agentic frameworks to improve the\nperformance of the models. This includes the verification of proposed solutions\nin a cumulative fashion by other, smaller LLM agents, and we perform a\ncomparative analysis of the performance that the techniques entail. There are\nsignificant improvements when the multi-agent framework is applied to problems\nthat the models initially perform poorly on. Furthermore, we introduce a new\nevaluation benchmark for physics problems, ${\\rm P{\\small HYSICS}E{\\small\nVAL}}$, consisting of 19,609 problems sourced from various physics textbooks\nand their corresponding correct solutions scraped from physics forums and\neducational websites. Our code and data are publicly available at\nhttps://github.com/areebuzair/PhysicsEval."
                },
                "authors": [
                    {
                        "name": "Oshayer Siddique"
                    },
                    {
                        "name": "J. M Areeb Uzair Alam"
                    },
                    {
                        "name": "Md Jobayer Rahman Rafy"
                    },
                    {
                        "name": "Syed Rifat Raiyan"
                    },
                    {
                        "name": "Hasan Mahmud"
                    },
                    {
                        "name": "Md Kamrul Hasan"
                    }
                ],
                "author_detail": {
                    "name": "Md Kamrul Hasan"
                },
                "author": "Md Kamrul Hasan",
                "arxiv_comment": "Accepted in Findings of the Association for Computational\n  Linguistics: IJCNLP-AACL 2025, 23 pages, 4 figures, 8 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00079v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00079v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03261v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03261v1",
                "updated": "2025-11-05T07:45:53Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    7,
                    45,
                    53,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T07:45:53Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    7,
                    45,
                    53,
                    2,
                    309,
                    0
                ],
                "title": "Comparing the Performance of LLMs in RAG-based Question-Answering: A\n  Case Study in Computer Science Literature",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparing the Performance of LLMs in RAG-based Question-Answering: A\n  Case Study in Computer Science Literature"
                },
                "summary": "Retrieval Augmented Generation (RAG) is emerging as a powerful technique to\nenhance the capabilities of Generative AI models by reducing hallucination.\nThus, the increasing prominence of RAG alongside Large Language Models (LLMs)\nhas sparked interest in comparing the performance of different LLMs in\nquestion-answering (QA) in diverse domains. This study compares the performance\nof four open-source LLMs, Mistral-7b-instruct, LLaMa2-7b-chat,\nFalcon-7b-instruct and Orca-mini-v3-7b, and OpenAI's trending GPT-3.5 over QA\ntasks within the computer science literature leveraging RAG support. Evaluation\nmetrics employed in the study include accuracy and precision for binary\nquestions and ranking by a human expert, ranking by Google's AI model Gemini,\nalongside cosine similarity for long-answer questions. GPT-3.5, when paired\nwith RAG, effectively answers binary and long-answer questions, reaffirming its\nstatus as an advanced LLM. Regarding open-source LLMs, Mistral AI's\nMistral-7b-instruct paired with RAG surpasses the rest in answering both binary\nand long-answer questions. However, among the open-source LLMs, Orca-mini-v3-7b\nreports the shortest average latency in generating responses, whereas\nLLaMa2-7b-chat by Meta reports the highest average latency. This research\nunderscores the fact that open-source LLMs, too, can go hand in hand with\nproprietary models like GPT-3.5 with better infrastructure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval Augmented Generation (RAG) is emerging as a powerful technique to\nenhance the capabilities of Generative AI models by reducing hallucination.\nThus, the increasing prominence of RAG alongside Large Language Models (LLMs)\nhas sparked interest in comparing the performance of different LLMs in\nquestion-answering (QA) in diverse domains. This study compares the performance\nof four open-source LLMs, Mistral-7b-instruct, LLaMa2-7b-chat,\nFalcon-7b-instruct and Orca-mini-v3-7b, and OpenAI's trending GPT-3.5 over QA\ntasks within the computer science literature leveraging RAG support. Evaluation\nmetrics employed in the study include accuracy and precision for binary\nquestions and ranking by a human expert, ranking by Google's AI model Gemini,\nalongside cosine similarity for long-answer questions. GPT-3.5, when paired\nwith RAG, effectively answers binary and long-answer questions, reaffirming its\nstatus as an advanced LLM. Regarding open-source LLMs, Mistral AI's\nMistral-7b-instruct paired with RAG surpasses the rest in answering both binary\nand long-answer questions. However, among the open-source LLMs, Orca-mini-v3-7b\nreports the shortest average latency in generating responses, whereas\nLLaMa2-7b-chat by Meta reports the highest average latency. This research\nunderscores the fact that open-source LLMs, too, can go hand in hand with\nproprietary models like GPT-3.5 with better infrastructure."
                },
                "authors": [
                    {
                        "name": "Ranul Dayarathne"
                    },
                    {
                        "name": "Uvini Ranaweera"
                    },
                    {
                        "name": "Upeksha Ganegoda"
                    }
                ],
                "author_detail": {
                    "name": "Upeksha Ganegoda"
                },
                "author": "Upeksha Ganegoda",
                "arxiv_doi": "10.1007/978-981-97-9255-9_26",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-981-97-9255-9_26",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2511.03261v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03261v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "18 pages, 4 figures, 5 tables, presented at the 5th International\n  Conference on Artificial Intelligence in Education Technology",
                "arxiv_journal_ref": "Lecture Notes on Data Engineering and Communications Technologies,\n  vol. 228, Springer, 2025, pp. 387--403",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.1; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03248v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03248v1",
                "updated": "2025-11-05T07:23:21Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    7,
                    23,
                    21,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T07:23:21Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    7,
                    23,
                    21,
                    2,
                    309,
                    0
                ],
                "title": "Auditing M-LLMs for Privacy Risks: A Synthetic Benchmark and Evaluation\n  Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auditing M-LLMs for Privacy Risks: A Synthetic Benchmark and Evaluation\n  Framework"
                },
                "summary": "Recent advances in multi-modal Large Language Models (M-LLMs) have\ndemonstrated a powerful ability to synthesize implicit information from\ndisparate sources, including images and text. These resourceful data from\nsocial media also introduce a significant and underexplored privacy risk: the\ninference of sensitive personal attributes from seemingly daily media content.\nHowever, the lack of benchmarks and comprehensive evaluations of\nstate-of-the-art M-LLM capabilities hinders the research of private attribute\nprofiling on social media. Accordingly, we propose (1) PRISM, the first\nmulti-modal, multi-dimensional and fine-grained synthesized dataset\nincorporating a comprehensive privacy landscape and dynamic user history; (2)\nan Efficient evaluation framework that measures the cross-modal privacy\ninference capabilities of advanced M-LLM. Specifically, PRISM is a large-scale\nsynthetic benchmark designed to evaluate cross-modal privacy risks. Its key\nfeature is 12 sensitive attribute labels across a diverse set of multi-modal\nprofiles, which enables targeted privacy analysis. These profiles are generated\nvia a sophisticated LLM agentic workflow, governed by a prior distribution to\nensure they realistically mimic social media users. Additionally, we propose a\nMulti-Agent Inference Framework that leverages a pipeline of specialized LLMs\nto enhance evaluation capabilities. We evaluate the inference capabilities of\nsix leading M-LLMs (Qwen, Gemini, GPT-4o, GLM, Doubao, and Grok) on PRISM. The\ncomparison with human performance reveals that these MLLMs significantly\noutperform in accuracy and efficiency, highlighting the threat of potential\nprivacy risks and the urgent need for robust defenses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in multi-modal Large Language Models (M-LLMs) have\ndemonstrated a powerful ability to synthesize implicit information from\ndisparate sources, including images and text. These resourceful data from\nsocial media also introduce a significant and underexplored privacy risk: the\ninference of sensitive personal attributes from seemingly daily media content.\nHowever, the lack of benchmarks and comprehensive evaluations of\nstate-of-the-art M-LLM capabilities hinders the research of private attribute\nprofiling on social media. Accordingly, we propose (1) PRISM, the first\nmulti-modal, multi-dimensional and fine-grained synthesized dataset\nincorporating a comprehensive privacy landscape and dynamic user history; (2)\nan Efficient evaluation framework that measures the cross-modal privacy\ninference capabilities of advanced M-LLM. Specifically, PRISM is a large-scale\nsynthetic benchmark designed to evaluate cross-modal privacy risks. Its key\nfeature is 12 sensitive attribute labels across a diverse set of multi-modal\nprofiles, which enables targeted privacy analysis. These profiles are generated\nvia a sophisticated LLM agentic workflow, governed by a prior distribution to\nensure they realistically mimic social media users. Additionally, we propose a\nMulti-Agent Inference Framework that leverages a pipeline of specialized LLMs\nto enhance evaluation capabilities. We evaluate the inference capabilities of\nsix leading M-LLMs (Qwen, Gemini, GPT-4o, GLM, Doubao, and Grok) on PRISM. The\ncomparison with human performance reveals that these MLLMs significantly\noutperform in accuracy and efficiency, highlighting the threat of potential\nprivacy risks and the urgent need for robust defenses."
                },
                "authors": [
                    {
                        "name": "Junhao Li"
                    },
                    {
                        "name": "Jiahao Chen"
                    },
                    {
                        "name": "Zhou Feng"
                    },
                    {
                        "name": "Chunyi Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Chunyi Zhou"
                },
                "author": "Chunyi Zhou",
                "arxiv_comment": "14 pages, 3 figures; Accepted by MMM 2026; Complete version in\n  progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03248v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03248v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]