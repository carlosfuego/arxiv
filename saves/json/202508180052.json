[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2508.10875v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10875v1",
                "updated": "2025-08-14T17:47:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    17,
                    47,
                    22,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T17:47:22Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    17,
                    47,
                    22,
                    3,
                    226,
                    0
                ],
                "title": "A Survey on Diffusion Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Diffusion Language Models"
                },
                "summary": "Diffusion Language Models (DLMs) are rapidly emerging as a powerful and\npromising alternative to the dominant autoregressive (AR) paradigm. By\ngenerating tokens in parallel through an iterative denoising process, DLMs\npossess inherent advantages in reducing inference latency and capturing\nbidirectional context, thereby enabling fine-grained control over the\ngeneration process. While achieving a several-fold speed-up, recent\nadvancements have allowed DLMs to show performance comparable to their\nautoregressive counterparts, making them a compelling choice for various\nnatural language processing tasks. In this survey, we provide a holistic\noverview of the current DLM landscape. We trace its evolution and relationship\nwith other paradigms, such as autoregressive and masked language models, and\ncover both foundational principles and state-of-the-art models. Our work offers\nan up-to-date, comprehensive taxonomy and an in-depth analysis of current\ntechniques, from pre-training strategies to advanced post-training methods.\nAnother contribution of this survey is a thorough review of DLM inference\nstrategies and optimizations, including improvements in decoding parallelism,\ncaching mechanisms, and generation quality. We also highlight the latest\napproaches to multimodal extensions of DLMs and delineate their applications\nacross various practical scenarios. Furthermore, our discussion addresses the\nlimitations and challenges of DLMs, including efficiency, long-sequence\nhandling, and infrastructure requirements, while outlining future research\ndirections to sustain progress in this rapidly evolving field. Project GitHub\nis available at https://github.com/VILA-Lab/Awesome-DLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Language Models (DLMs) are rapidly emerging as a powerful and\npromising alternative to the dominant autoregressive (AR) paradigm. By\ngenerating tokens in parallel through an iterative denoising process, DLMs\npossess inherent advantages in reducing inference latency and capturing\nbidirectional context, thereby enabling fine-grained control over the\ngeneration process. While achieving a several-fold speed-up, recent\nadvancements have allowed DLMs to show performance comparable to their\nautoregressive counterparts, making them a compelling choice for various\nnatural language processing tasks. In this survey, we provide a holistic\noverview of the current DLM landscape. We trace its evolution and relationship\nwith other paradigms, such as autoregressive and masked language models, and\ncover both foundational principles and state-of-the-art models. Our work offers\nan up-to-date, comprehensive taxonomy and an in-depth analysis of current\ntechniques, from pre-training strategies to advanced post-training methods.\nAnother contribution of this survey is a thorough review of DLM inference\nstrategies and optimizations, including improvements in decoding parallelism,\ncaching mechanisms, and generation quality. We also highlight the latest\napproaches to multimodal extensions of DLMs and delineate their applications\nacross various practical scenarios. Furthermore, our discussion addresses the\nlimitations and challenges of DLMs, including efficiency, long-sequence\nhandling, and infrastructure requirements, while outlining future research\ndirections to sustain progress in this rapidly evolving field. Project GitHub\nis available at https://github.com/VILA-Lab/Awesome-DLMs."
                },
                "authors": [
                    {
                        "name": "Tianyi Li"
                    },
                    {
                        "name": "Mingda Chen"
                    },
                    {
                        "name": "Bowei Guo"
                    },
                    {
                        "name": "Zhiqiang Shen"
                    }
                ],
                "author_detail": {
                    "name": "Zhiqiang Shen"
                },
                "author": "Zhiqiang Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10875v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10875v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10824v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10824v1",
                "updated": "2025-08-14T16:48:38Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    16,
                    48,
                    38,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T16:48:38Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    16,
                    48,
                    38,
                    3,
                    226,
                    0
                ],
                "title": "Memory-Augmented Transformers: A Systematic Review from Neuroscience\n  Principles to Technical Solutions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory-Augmented Transformers: A Systematic Review from Neuroscience\n  Principles to Technical Solutions"
                },
                "summary": "Memory is fundamental to intelligence, enabling learning, reasoning, and\nadaptability across biological and artificial systems. While Transformer\narchitectures excel at sequence modeling, they face critical limitations in\nlong-range context retention, continual learning, and knowledge integration.\nThis review presents a unified framework bridging neuroscience principles,\nincluding dynamic multi-timescale memory, selective attention, and\nconsolidation, with engineering advances in Memory-Augmented Transformers. We\norganize recent progress through three taxonomic dimensions: functional\nobjectives (context extension, reasoning, knowledge integration, adaptation),\nmemory representations (parameter-encoded, state-based, explicit, hybrid), and\nintegration mechanisms (attention fusion, gated control, associative\nretrieval). Our analysis of core memory operations (reading, writing,\nforgetting, and capacity management) reveals a shift from static caches toward\nadaptive, test-time learning systems. We identify persistent challenges in\nscalability and interference, alongside emerging solutions including\nhierarchical buffering and surprise-gated updates. This synthesis provides a\nroadmap toward cognitively-inspired, lifelong-learning Transformer\narchitectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory is fundamental to intelligence, enabling learning, reasoning, and\nadaptability across biological and artificial systems. While Transformer\narchitectures excel at sequence modeling, they face critical limitations in\nlong-range context retention, continual learning, and knowledge integration.\nThis review presents a unified framework bridging neuroscience principles,\nincluding dynamic multi-timescale memory, selective attention, and\nconsolidation, with engineering advances in Memory-Augmented Transformers. We\norganize recent progress through three taxonomic dimensions: functional\nobjectives (context extension, reasoning, knowledge integration, adaptation),\nmemory representations (parameter-encoded, state-based, explicit, hybrid), and\nintegration mechanisms (attention fusion, gated control, associative\nretrieval). Our analysis of core memory operations (reading, writing,\nforgetting, and capacity management) reveals a shift from static caches toward\nadaptive, test-time learning systems. We identify persistent challenges in\nscalability and interference, alongside emerging solutions including\nhierarchical buffering and surprise-gated updates. This synthesis provides a\nroadmap toward cognitively-inspired, lifelong-learning Transformer\narchitectures."
                },
                "authors": [
                    {
                        "name": "Parsa Omidi"
                    },
                    {
                        "name": "Xingshuai Huang"
                    },
                    {
                        "name": "Axel Laborieux"
                    },
                    {
                        "name": "Bahareh Nikpour"
                    },
                    {
                        "name": "Tianyu Shi"
                    },
                    {
                        "name": "Armaghan Eshaghi"
                    }
                ],
                "author_detail": {
                    "name": "Armaghan Eshaghi"
                },
                "author": "Armaghan Eshaghi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10824v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10824v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13109v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13109v2",
                "updated": "2025-08-14T16:12:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    16,
                    12,
                    44,
                    3,
                    226,
                    0
                ],
                "published": "2025-05-19T13:36:45Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    13,
                    36,
                    45,
                    0,
                    139,
                    0
                ],
                "title": "FreeKV: Boosting KV Cache Retrieval for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FreeKV: Boosting KV Cache Retrieval for Efficient LLM Inference"
                },
                "summary": "Large language models (LLMs) have been widely deployed with rapidly expanding\ncontext windows to support increasingly demanding applications. However, long\ncontexts pose significant deployment challenges, primarily due to the KV cache\nwhose size grows proportionally with context length. While KV cache compression\nmethods are proposed to address this issue, KV dropping methods incur\nconsiderable accuracy loss, and KV retrieval methods suffer from significant\nefficiency bottlenecks. We propose FreeKV, an algorithm-system co-optimization\nframework to enhance KV retrieval efficiency while preserving accuracy. On the\nalgorithm side, FreeKV introduces speculative retrieval to shift the KV\nselection and recall processes out of the critical path, combined with\nfine-grained correction to ensure accuracy. On the system side, FreeKV employs\nhybrid KV layouts across CPU and GPU memory to eliminate fragmented data\ntransfers, and leverages double-buffered streamed recall to further improve\nefficiency. Experiments demonstrate that FreeKV achieves near-lossless accuracy\nacross various scenarios and models, delivering up to 13$\\times$ speedup\ncompared to SOTA KV retrieval methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been widely deployed with rapidly expanding\ncontext windows to support increasingly demanding applications. However, long\ncontexts pose significant deployment challenges, primarily due to the KV cache\nwhose size grows proportionally with context length. While KV cache compression\nmethods are proposed to address this issue, KV dropping methods incur\nconsiderable accuracy loss, and KV retrieval methods suffer from significant\nefficiency bottlenecks. We propose FreeKV, an algorithm-system co-optimization\nframework to enhance KV retrieval efficiency while preserving accuracy. On the\nalgorithm side, FreeKV introduces speculative retrieval to shift the KV\nselection and recall processes out of the critical path, combined with\nfine-grained correction to ensure accuracy. On the system side, FreeKV employs\nhybrid KV layouts across CPU and GPU memory to eliminate fragmented data\ntransfers, and leverages double-buffered streamed recall to further improve\nefficiency. Experiments demonstrate that FreeKV achieves near-lossless accuracy\nacross various scenarios and models, delivering up to 13$\\times$ speedup\ncompared to SOTA KV retrieval methods."
                },
                "authors": [
                    {
                        "name": "Guangda Liu"
                    },
                    {
                        "name": "Chengwei Li"
                    },
                    {
                        "name": "Zhenyu Ning"
                    },
                    {
                        "name": "Minyi Guo"
                    },
                    {
                        "name": "Jieru Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Jieru Zhao"
                },
                "author": "Jieru Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13109v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13109v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18773v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18773v2",
                "updated": "2025-08-14T15:37:43Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    15,
                    37,
                    43,
                    3,
                    226,
                    0
                ],
                "published": "2025-03-24T15:22:41Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    15,
                    22,
                    41,
                    0,
                    83,
                    0
                ],
                "title": "BitDecoding: Unlocking Tensor Cores for Long-Context LLMs with Low-Bit\n  KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BitDecoding: Unlocking Tensor Cores for Long-Context LLMs with Low-Bit\n  KV Cache"
                },
                "summary": "The rise of long-context Large Language Models (LLMs) amplifies memory and\nbandwidth demands during autoregressive decoding, as the Key-Value (KV) cache\ngrows with each generated token. Low-bit KV-cache quantization (e.g., 4-bit or\n2-bit) can reduce memory footprint while preserving accuracy, but existing\nsystems suffer from slow decoding due to their exclusive reliance on CUDA\ncores, neglecting Tensor Cores (the primary source of compute on modern GPUs).\nWe present BitDecoding, a new long-context LLM inference system with a low-bit\nKV cache. BitDecoding enables efficient low-bit KV-cache decoding by\ncooperatively leveraging CUDA cores and Tensor Cores. It introduces methods for\nautomatically inducing optimized layouts to exploit Tensor Cores, along with\nwarp-level parallelization strategies for dequantization. For unified system\nsupport, BitDecoding includes a query transformation module supporting diverse\nattention variants, a quantization kernel that supports both tensor-wise and\nchannel-wise scaling used in various quantization algorithms with high\nperformance, and a dequantization kernel with a software-defined pipeline to\ncoordinate CUDA and Tensor Cores execution for mixed-precision operations.\nEvaluated on RTX 4090, A100, and H100, BitDecoding accelerates decoding by up\nto 7.5x, 4.8x, and 8.9x, respectively, over FP16 FlashDecoding-v2, and\nsurpasses the state-of-the-art low-bit system QServe by up to 4.3x. On\nLLaMA-3.1-8B with a 128K context, BitDecoding reduces single-batch decoding\nlatency by 3x, showing substantial improvements for long-context generation.\nThe code is available at https://github.com/DD-DuDa/BitDecoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of long-context Large Language Models (LLMs) amplifies memory and\nbandwidth demands during autoregressive decoding, as the Key-Value (KV) cache\ngrows with each generated token. Low-bit KV-cache quantization (e.g., 4-bit or\n2-bit) can reduce memory footprint while preserving accuracy, but existing\nsystems suffer from slow decoding due to their exclusive reliance on CUDA\ncores, neglecting Tensor Cores (the primary source of compute on modern GPUs).\nWe present BitDecoding, a new long-context LLM inference system with a low-bit\nKV cache. BitDecoding enables efficient low-bit KV-cache decoding by\ncooperatively leveraging CUDA cores and Tensor Cores. It introduces methods for\nautomatically inducing optimized layouts to exploit Tensor Cores, along with\nwarp-level parallelization strategies for dequantization. For unified system\nsupport, BitDecoding includes a query transformation module supporting diverse\nattention variants, a quantization kernel that supports both tensor-wise and\nchannel-wise scaling used in various quantization algorithms with high\nperformance, and a dequantization kernel with a software-defined pipeline to\ncoordinate CUDA and Tensor Cores execution for mixed-precision operations.\nEvaluated on RTX 4090, A100, and H100, BitDecoding accelerates decoding by up\nto 7.5x, 4.8x, and 8.9x, respectively, over FP16 FlashDecoding-v2, and\nsurpasses the state-of-the-art low-bit system QServe by up to 4.3x. On\nLLaMA-3.1-8B with a 128K context, BitDecoding reduces single-batch decoding\nlatency by 3x, showing substantial improvements for long-context generation.\nThe code is available at https://github.com/DD-DuDa/BitDecoding."
                },
                "authors": [
                    {
                        "name": "Dayou Du"
                    },
                    {
                        "name": "Shijie Cao"
                    },
                    {
                        "name": "Jianyi Cheng"
                    },
                    {
                        "name": "Luo Mai"
                    },
                    {
                        "name": "Ting Cao"
                    },
                    {
                        "name": "Mao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Mao Yang"
                },
                "author": "Mao Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18773v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18773v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10613v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10613v1",
                "updated": "2025-08-14T13:10:43Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    13,
                    10,
                    43,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T13:10:43Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    13,
                    10,
                    43,
                    3,
                    226,
                    0
                ],
                "title": "Routing and Wavelength Assignment with Minimal Attack Radius for QKD\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Routing and Wavelength Assignment with Minimal Attack Radius for QKD\n  Networks"
                },
                "summary": "Quantum Key Distribution (QKD) can distribute keys with guaranteed security\nbut remains susceptible to key exchange interruption due to physical-layer\nthreats, such as high-power jamming attacks. To address this challenge, we\nfirst introduce a novel metric, namely Maximum Number of Affected Requests\n(maxNAR), to quantify the worst-case impact of a single physical-layer attack,\nand then we investigate a new problem of Routing and Wavelength Assignment with\nMinimal Attack Radius (RWA-MAR). We formulate the problem using an Integer\nLinear Programming (ILP) model and propose a scalable heuristic to efficiently\nminimize maxNAR. Our approach incorporates key caching through Quantum Key\nPools (QKPs) to enhance resilience and optimize resource utilization. Moreover,\nwe model the impact of different QKD network architectures, employing Optical\nBypass (OB) for optical switching of quantum channels and Trusted Relay (TR)\nfor secure key forwarding. Moreover, a tunable parameter is designed in the\nheuristic to guide the preference for OB or TR, offering enhanced adaptability\nand dynamic control in diverse network scenarios. Simulation results confirm\nthat our method significantly outperforms the baseline in terms of security and\nscalability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum Key Distribution (QKD) can distribute keys with guaranteed security\nbut remains susceptible to key exchange interruption due to physical-layer\nthreats, such as high-power jamming attacks. To address this challenge, we\nfirst introduce a novel metric, namely Maximum Number of Affected Requests\n(maxNAR), to quantify the worst-case impact of a single physical-layer attack,\nand then we investigate a new problem of Routing and Wavelength Assignment with\nMinimal Attack Radius (RWA-MAR). We formulate the problem using an Integer\nLinear Programming (ILP) model and propose a scalable heuristic to efficiently\nminimize maxNAR. Our approach incorporates key caching through Quantum Key\nPools (QKPs) to enhance resilience and optimize resource utilization. Moreover,\nwe model the impact of different QKD network architectures, employing Optical\nBypass (OB) for optical switching of quantum channels and Trusted Relay (TR)\nfor secure key forwarding. Moreover, a tunable parameter is designed in the\nheuristic to guide the preference for OB or TR, offering enhanced adaptability\nand dynamic control in diverse network scenarios. Simulation results confirm\nthat our method significantly outperforms the baseline in terms of security and\nscalability."
                },
                "authors": [
                    {
                        "name": "Mengyao Li"
                    },
                    {
                        "name": "Qiaolun Zhang"
                    },
                    {
                        "name": "Zongshuai Yang"
                    },
                    {
                        "name": "Stefano Bregni"
                    },
                    {
                        "name": "Alberto Gatto"
                    },
                    {
                        "name": "Raouf Boutaba"
                    },
                    {
                        "name": "Massimo Tornatore"
                    }
                ],
                "author_detail": {
                    "name": "Massimo Tornatore"
                },
                "author": "Massimo Tornatore",
                "arxiv_comment": "6 pages, this paper has been successfully accepted by GLOBECOM2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10613v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10613v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08601v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08601v3",
                "updated": "2025-08-14T10:26:51Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    10,
                    26,
                    51,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-12T03:34:21Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    3,
                    34,
                    21,
                    1,
                    224,
                    0
                ],
                "title": "Yan: Foundational Interactive Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Yan: Foundational Interactive Video Generation"
                },
                "summary": "We present Yan, a foundational framework for interactive video generation,\ncovering the entire pipeline from simulation and generation to editing.\nSpecifically, Yan comprises three core modules. AAA-level Simulation: We design\na highly-compressed, low-latency 3D-VAE coupled with a KV-cache-based\nshift-window denoising inference process, achieving real-time 1080P/60FPS\ninteractive simulation. Multi-Modal Generation: We introduce a hierarchical\nautoregressive caption method that injects game-specific knowledge into\nopen-domain multi-modal video diffusion models (VDMs), then transforming the\nVDM into a frame-wise, action-controllable, real-time infinite interactive\nvideo generator. Notably, when the textual and visual prompts are sourced from\ndifferent domains, the model demonstrates strong generalization, allowing it to\nblend and compose the style and mechanics across domains flexibly according to\nuser prompts. Multi-Granularity Editing: We propose a hybrid model that\nexplicitly disentangles interactive mechanics simulation from visual rendering,\nenabling multi-granularity video content editing during interaction through\ntext. Collectively, Yan offers an integration of these modules, pushing\ninteractive video generation beyond isolated capabilities toward a\ncomprehensive AI-driven interactive creation paradigm, paving the way for the\nnext generation of creative tools, media, and entertainment. The project page\nis: https://greatx3.github.io/Yan/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Yan, a foundational framework for interactive video generation,\ncovering the entire pipeline from simulation and generation to editing.\nSpecifically, Yan comprises three core modules. AAA-level Simulation: We design\na highly-compressed, low-latency 3D-VAE coupled with a KV-cache-based\nshift-window denoising inference process, achieving real-time 1080P/60FPS\ninteractive simulation. Multi-Modal Generation: We introduce a hierarchical\nautoregressive caption method that injects game-specific knowledge into\nopen-domain multi-modal video diffusion models (VDMs), then transforming the\nVDM into a frame-wise, action-controllable, real-time infinite interactive\nvideo generator. Notably, when the textual and visual prompts are sourced from\ndifferent domains, the model demonstrates strong generalization, allowing it to\nblend and compose the style and mechanics across domains flexibly according to\nuser prompts. Multi-Granularity Editing: We propose a hybrid model that\nexplicitly disentangles interactive mechanics simulation from visual rendering,\nenabling multi-granularity video content editing during interaction through\ntext. Collectively, Yan offers an integration of these modules, pushing\ninteractive video generation beyond isolated capabilities toward a\ncomprehensive AI-driven interactive creation paradigm, paving the way for the\nnext generation of creative tools, media, and entertainment. The project page\nis: https://greatx3.github.io/Yan/."
                },
                "authors": [
                    {
                        "name": "Deheng Ye"
                    },
                    {
                        "name": "Fangyun Zhou"
                    },
                    {
                        "name": "Jiacheng Lv"
                    },
                    {
                        "name": "Jianqi Ma"
                    },
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Junyan Lv"
                    },
                    {
                        "name": "Junyou Li"
                    },
                    {
                        "name": "Minwen Deng"
                    },
                    {
                        "name": "Mingyu Yang"
                    },
                    {
                        "name": "Qiang Fu"
                    },
                    {
                        "name": "Wei Yang"
                    },
                    {
                        "name": "Wenkai Lv"
                    },
                    {
                        "name": "Yangbin Yu"
                    },
                    {
                        "name": "Yewen Wang"
                    },
                    {
                        "name": "Yonghang Guan"
                    },
                    {
                        "name": "Zhihao Hu"
                    },
                    {
                        "name": "Zhongbin Fang"
                    },
                    {
                        "name": "Zhongqian Sun"
                    }
                ],
                "author_detail": {
                    "name": "Zhongqian Sun"
                },
                "author": "Zhongqian Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08601v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08601v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08895v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08895v2",
                "updated": "2025-08-14T09:04:56Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    9,
                    4,
                    56,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-12T12:35:55Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    12,
                    35,
                    55,
                    1,
                    224,
                    0
                ],
                "title": "ASPD: Unlocking Adaptive Serial-Parallel Decoding by Exploring Intrinsic\n  Parallelism in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASPD: Unlocking Adaptive Serial-Parallel Decoding by Exploring Intrinsic\n  Parallelism in LLMs"
                },
                "summary": "The increasing scale and complexity of large language models (LLMs) pose\nsignificant inference latency challenges, primarily due to their autoregressive\ndecoding paradigm characterized by the sequential nature of next-token\nprediction. By re-examining the outputs of autoregressive models, we observed\nthat some segments exhibit parallelizable structures, which we term intrinsic\nparallelism. Decoding each parallelizable branch simultaneously (i.e. parallel\ndecoding) can significantly improve the overall inference speed of LLMs. In\nthis paper, we propose an Adaptive Serial-Parallel Decoding (ASPD), which\naddresses two core challenges: automated construction of parallelizable data\nand efficient parallel decoding mechanism. More specifically, we introduce a\nnon-invasive pipeline that automatically extracts and validates parallelizable\nstructures from the responses of autoregressive models. To empower efficient\nadaptive serial-parallel decoding, we implement a Hybrid Decoding Engine which\nenables seamless transitions between serial and parallel decoding modes while\nmaintaining a reusable KV cache, maximizing computational efficiency. Extensive\nevaluations across General Tasks, Retrieval-Augmented Generation, Mathematical\nReasoning, demonstrate that ASPD achieves unprecedented performance in both\neffectiveness and efficiency. Notably, on Vicuna Bench, our method achieves up\nto 3.19x speedup (1.85x on average) while maintaining response quality within\n1% difference compared to autoregressive models, realizing significant\nacceleration without compromising generation quality. Our framework sets a\ngroundbreaking benchmark for efficient LLM parallel inference, paving the way\nfor its deployment in latency-sensitive applications such as AI-powered\ncustomer service bots and answer retrieval engines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing scale and complexity of large language models (LLMs) pose\nsignificant inference latency challenges, primarily due to their autoregressive\ndecoding paradigm characterized by the sequential nature of next-token\nprediction. By re-examining the outputs of autoregressive models, we observed\nthat some segments exhibit parallelizable structures, which we term intrinsic\nparallelism. Decoding each parallelizable branch simultaneously (i.e. parallel\ndecoding) can significantly improve the overall inference speed of LLMs. In\nthis paper, we propose an Adaptive Serial-Parallel Decoding (ASPD), which\naddresses two core challenges: automated construction of parallelizable data\nand efficient parallel decoding mechanism. More specifically, we introduce a\nnon-invasive pipeline that automatically extracts and validates parallelizable\nstructures from the responses of autoregressive models. To empower efficient\nadaptive serial-parallel decoding, we implement a Hybrid Decoding Engine which\nenables seamless transitions between serial and parallel decoding modes while\nmaintaining a reusable KV cache, maximizing computational efficiency. Extensive\nevaluations across General Tasks, Retrieval-Augmented Generation, Mathematical\nReasoning, demonstrate that ASPD achieves unprecedented performance in both\neffectiveness and efficiency. Notably, on Vicuna Bench, our method achieves up\nto 3.19x speedup (1.85x on average) while maintaining response quality within\n1% difference compared to autoregressive models, realizing significant\nacceleration without compromising generation quality. Our framework sets a\ngroundbreaking benchmark for efficient LLM parallel inference, paving the way\nfor its deployment in latency-sensitive applications such as AI-powered\ncustomer service bots and answer retrieval engines."
                },
                "authors": [
                    {
                        "name": "Keyu Chen"
                    },
                    {
                        "name": "Zhifeng Shen"
                    },
                    {
                        "name": "Daohai Yu"
                    },
                    {
                        "name": "Haoqian Wu"
                    },
                    {
                        "name": "Wei Wen"
                    },
                    {
                        "name": "Jianfeng He"
                    },
                    {
                        "name": "Ruizhi Qiao"
                    },
                    {
                        "name": "Xing Sun"
                    }
                ],
                "author_detail": {
                    "name": "Xing Sun"
                },
                "author": "Xing Sun",
                "arxiv_comment": "20 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08895v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08895v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10431v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10431v1",
                "updated": "2025-08-14T08:04:15Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    8,
                    4,
                    15,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T08:04:15Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    8,
                    4,
                    15,
                    3,
                    226,
                    0
                ],
                "title": "Yet Another Mirage of Breaking MIRAGE: Debunking Occupancy-based\n  Side-Channel Attacks on Fully Associative Randomized Caches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Yet Another Mirage of Breaking MIRAGE: Debunking Occupancy-based\n  Side-Channel Attacks on Fully Associative Randomized Caches"
                },
                "summary": "Recent work presented at USENIX Security 2025 claims that occupancy-based\nattacks can recover AES keys from the MIRAGE randomized cache. In this paper,\nwe examine these claims and find that they arise from fundamental modeling\nflaws. Most critically, the authors' simulation of MIRAGE uses a constant seed\nto initialize the random number generator used for global evictions in MIRAGE,\ncausing every AES encryption they trace to evict the same deterministic\nsequence of cache lines. This artificially creates a highly repeatable timing\npattern that is not representative of a realistic implementation of MIRAGE,\nwhere eviction sequences vary randomly between encryptions. When we instead\nrandomize the eviction seed for each run, reflecting realistic operation, the\ncorrelation between AES T-table accesses and attacker runtimes disappears, and\nthe attack fails. These findings show that the reported leakage is an artifact\nof incorrect modeling, and not an actual vulnerability in MIRAGE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work presented at USENIX Security 2025 claims that occupancy-based\nattacks can recover AES keys from the MIRAGE randomized cache. In this paper,\nwe examine these claims and find that they arise from fundamental modeling\nflaws. Most critically, the authors' simulation of MIRAGE uses a constant seed\nto initialize the random number generator used for global evictions in MIRAGE,\ncausing every AES encryption they trace to evict the same deterministic\nsequence of cache lines. This artificially creates a highly repeatable timing\npattern that is not representative of a realistic implementation of MIRAGE,\nwhere eviction sequences vary randomly between encryptions. When we instead\nrandomize the eviction seed for each run, reflecting realistic operation, the\ncorrelation between AES T-table accesses and attacker runtimes disappears, and\nthe attack fails. These findings show that the reported leakage is an artifact\nof incorrect modeling, and not an actual vulnerability in MIRAGE."
                },
                "authors": [
                    {
                        "name": "Chris Cao"
                    },
                    {
                        "name": "Gururaj Saileshwar"
                    }
                ],
                "author_detail": {
                    "name": "Gururaj Saileshwar"
                },
                "author": "Gururaj Saileshwar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10431v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10431v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10424v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10424v1",
                "updated": "2025-08-14T07:54:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    7,
                    54,
                    44,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T07:54:44Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    7,
                    54,
                    44,
                    3,
                    226,
                    0
                ],
                "title": "NanoControl: A Lightweight Framework for Precise and Efficient Control\n  in Diffusion Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NanoControl: A Lightweight Framework for Precise and Efficient Control\n  in Diffusion Transformer"
                },
                "summary": "Diffusion Transformers (DiTs) have demonstrated exceptional capabilities in\ntext-to-image synthesis. However, in the domain of controllable text-to-image\ngeneration using DiTs, most existing methods still rely on the ControlNet\nparadigm originally designed for UNet-based diffusion models. This paradigm\nintroduces significant parameter overhead and increased computational costs. To\naddress these challenges, we propose the Nano Control Diffusion Transformer\n(NanoControl), which employs Flux as the backbone network. Our model achieves\nstate-of-the-art controllable text-to-image generation performance while\nincurring only a 0.024\\% increase in parameter count and a 0.029\\% increase in\nGFLOPs, thus enabling highly efficient controllable generation. Specifically,\nrather than duplicating the DiT backbone for control, we design a LoRA-style\n(low-rank adaptation) control module that directly learns control signals from\nraw conditioning inputs. Furthermore, we introduce a KV-Context Augmentation\nmechanism that integrates condition-specific key-value information into the\nbackbone in a simple yet highly effective manner, facilitating deep fusion of\nconditional features. Extensive benchmark experiments demonstrate that\nNanoControl significantly reduces computational overhead compared to\nconventional control approaches, while maintaining superior generation quality\nand achieving improved controllability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) have demonstrated exceptional capabilities in\ntext-to-image synthesis. However, in the domain of controllable text-to-image\ngeneration using DiTs, most existing methods still rely on the ControlNet\nparadigm originally designed for UNet-based diffusion models. This paradigm\nintroduces significant parameter overhead and increased computational costs. To\naddress these challenges, we propose the Nano Control Diffusion Transformer\n(NanoControl), which employs Flux as the backbone network. Our model achieves\nstate-of-the-art controllable text-to-image generation performance while\nincurring only a 0.024\\% increase in parameter count and a 0.029\\% increase in\nGFLOPs, thus enabling highly efficient controllable generation. Specifically,\nrather than duplicating the DiT backbone for control, we design a LoRA-style\n(low-rank adaptation) control module that directly learns control signals from\nraw conditioning inputs. Furthermore, we introduce a KV-Context Augmentation\nmechanism that integrates condition-specific key-value information into the\nbackbone in a simple yet highly effective manner, facilitating deep fusion of\nconditional features. Extensive benchmark experiments demonstrate that\nNanoControl significantly reduces computational overhead compared to\nconventional control approaches, while maintaining superior generation quality\nand achieving improved controllability."
                },
                "authors": [
                    {
                        "name": "Shanyuan Liu"
                    },
                    {
                        "name": "Jian Zhu"
                    },
                    {
                        "name": "Junda Lu"
                    },
                    {
                        "name": "Yue Gong"
                    },
                    {
                        "name": "Liuzhuozheng Li"
                    },
                    {
                        "name": "Bo Cheng"
                    },
                    {
                        "name": "Yuhang Ma"
                    },
                    {
                        "name": "Liebucha Wu"
                    },
                    {
                        "name": "Xiaoyu Wu"
                    },
                    {
                        "name": "Dawei Leng"
                    },
                    {
                        "name": "Yuhui Yin"
                    }
                ],
                "author_detail": {
                    "name": "Yuhui Yin"
                },
                "author": "Yuhui Yin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10424v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10424v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10395v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10395v1",
                "updated": "2025-08-14T06:52:38Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    6,
                    52,
                    38,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T06:52:38Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    6,
                    52,
                    38,
                    3,
                    226,
                    0
                ],
                "title": "XQuant: Breaking the Memory Wall for LLM Inference with KV Cache\n  Rematerialization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XQuant: Breaking the Memory Wall for LLM Inference with KV Cache\n  Rematerialization"
                },
                "summary": "Although LLM inference has emerged as a critical workload for many downstream\napplications, efficiently inferring LLMs is challenging due to the substantial\nmemory footprint and bandwidth requirements. In parallel, compute capabilities\nhave steadily outpaced both memory capacity and bandwidth over the last few\ndecades, a trend that remains evident in modern GPU hardware and exacerbates\nthe challenge of LLM inference. As such, new algorithms are emerging that trade\nincreased computation for reduced memory operations. To that end, we present\nXQuant, which takes advantage of this trend, enabling an order-of-magnitude\nreduction in memory consumption through low-bit quantization with substantial\naccuracy benefits relative to state-of-the-art KV cache quantization methods.\nWe accomplish this by quantizing and caching the layer input activations X,\ninstead of using standard KV caching, and then rematerializing the Keys and\nValues on-the-fly during inference. This results in an immediate 2$\\times$\nmemory savings compared to KV caching. By applying XQuant, we achieve up to\n$\\sim 7.7\\times$ memory savings with $<0.1$ perplexity degradation compared to\nthe FP16 baseline. Furthermore, our approach leverages the fact that X values\nare similar across layers. Building on this observation, we introduce\nXQuant-CL, which exploits the cross-layer similarity in the X embeddings for\nextreme compression. Across different models, XQuant-CL attains up to\n10$\\times$ memory savings relative to the FP16 baseline with only 0.01\nperplexity degradation, and 12.5$\\times$ memory savings with only $0.1$\nperplexity degradation. XQuant exploits the rapidly increasing compute\ncapabilities of hardware platforms to eliminate the memory bottleneck, while\nsurpassing state-of-the-art KV cache quantization methods and achieving\nnear-FP16 accuracy across a wide range of models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although LLM inference has emerged as a critical workload for many downstream\napplications, efficiently inferring LLMs is challenging due to the substantial\nmemory footprint and bandwidth requirements. In parallel, compute capabilities\nhave steadily outpaced both memory capacity and bandwidth over the last few\ndecades, a trend that remains evident in modern GPU hardware and exacerbates\nthe challenge of LLM inference. As such, new algorithms are emerging that trade\nincreased computation for reduced memory operations. To that end, we present\nXQuant, which takes advantage of this trend, enabling an order-of-magnitude\nreduction in memory consumption through low-bit quantization with substantial\naccuracy benefits relative to state-of-the-art KV cache quantization methods.\nWe accomplish this by quantizing and caching the layer input activations X,\ninstead of using standard KV caching, and then rematerializing the Keys and\nValues on-the-fly during inference. This results in an immediate 2$\\times$\nmemory savings compared to KV caching. By applying XQuant, we achieve up to\n$\\sim 7.7\\times$ memory savings with $<0.1$ perplexity degradation compared to\nthe FP16 baseline. Furthermore, our approach leverages the fact that X values\nare similar across layers. Building on this observation, we introduce\nXQuant-CL, which exploits the cross-layer similarity in the X embeddings for\nextreme compression. Across different models, XQuant-CL attains up to\n10$\\times$ memory savings relative to the FP16 baseline with only 0.01\nperplexity degradation, and 12.5$\\times$ memory savings with only $0.1$\nperplexity degradation. XQuant exploits the rapidly increasing compute\ncapabilities of hardware platforms to eliminate the memory bottleneck, while\nsurpassing state-of-the-art KV cache quantization methods and achieving\nnear-FP16 accuracy across a wide range of models."
                },
                "authors": [
                    {
                        "name": "Aditya Tomar"
                    },
                    {
                        "name": "Coleman Hooper"
                    },
                    {
                        "name": "Minjae Lee"
                    },
                    {
                        "name": "Haocheng Xi"
                    },
                    {
                        "name": "Rishabh Tiwari"
                    },
                    {
                        "name": "Wonjun Kang"
                    },
                    {
                        "name": "Luca Manolache"
                    },
                    {
                        "name": "Michael W. Mahoney"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Amir Gholami"
                    }
                ],
                "author_detail": {
                    "name": "Amir Gholami"
                },
                "author": "Amir Gholami",
                "arxiv_comment": "24 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10395v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10395v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14051v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14051v3",
                "updated": "2025-08-13T17:55:58Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    17,
                    55,
                    58,
                    2,
                    225,
                    0
                ],
                "published": "2025-02-19T19:12:46Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    19,
                    12,
                    46,
                    2,
                    50,
                    0
                ],
                "title": "RocketKV: Accelerating Long-Context LLM Inference via Two-Stage KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RocketKV: Accelerating Long-Context LLM Inference via Two-Stage KV Cache\n  Compression"
                },
                "summary": "Transformer-based Large Language Models rely critically on the KV cache to\nefficiently handle extended contexts during the decode phase. Yet, the size of\nthe KV cache grows proportionally with the input length, burdening both memory\nbandwidth and capacity as decoding progresses. To address this challenge, we\npresent RocketKV, a training-free KV cache compression strategy containing two\nconsecutive stages. In the first stage, it performs coarse-grain permanent KV\ncache eviction on the input sequence tokens. In the second stage, it adopts a\nhybrid sparse attention method to conduct fine-grain top-k sparse attention,\napproximating the attention scores by leveraging both head and sequence\ndimensionality reductions. We show that RocketKV provides a compression ratio\nof up to 400$\\times$, end-to-end speedup of up to 3.7$\\times$ as well as peak\nmemory reduction of up to 32.6% in the decode phase on an NVIDIA A100 GPU\ncompared to the full KV cache baseline, while achieving negligible accuracy\nloss on a variety of long-context tasks. We also propose a variant of RocketKV\nfor multi-turn scenarios, which consistently outperforms other existing methods\nand achieves accuracy nearly on par with an oracle top-k attention scheme. The\nsource code is available here: https://github.com/NVlabs/RocketKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based Large Language Models rely critically on the KV cache to\nefficiently handle extended contexts during the decode phase. Yet, the size of\nthe KV cache grows proportionally with the input length, burdening both memory\nbandwidth and capacity as decoding progresses. To address this challenge, we\npresent RocketKV, a training-free KV cache compression strategy containing two\nconsecutive stages. In the first stage, it performs coarse-grain permanent KV\ncache eviction on the input sequence tokens. In the second stage, it adopts a\nhybrid sparse attention method to conduct fine-grain top-k sparse attention,\napproximating the attention scores by leveraging both head and sequence\ndimensionality reductions. We show that RocketKV provides a compression ratio\nof up to 400$\\times$, end-to-end speedup of up to 3.7$\\times$ as well as peak\nmemory reduction of up to 32.6% in the decode phase on an NVIDIA A100 GPU\ncompared to the full KV cache baseline, while achieving negligible accuracy\nloss on a variety of long-context tasks. We also propose a variant of RocketKV\nfor multi-turn scenarios, which consistently outperforms other existing methods\nand achieves accuracy nearly on par with an oracle top-k attention scheme. The\nsource code is available here: https://github.com/NVlabs/RocketKV."
                },
                "authors": [
                    {
                        "name": "Payman Behnam"
                    },
                    {
                        "name": "Yaosheng Fu"
                    },
                    {
                        "name": "Ritchie Zhao"
                    },
                    {
                        "name": "Po-An Tsai"
                    },
                    {
                        "name": "Zhiding Yu"
                    },
                    {
                        "name": "Alexey Tumanov"
                    }
                ],
                "author_detail": {
                    "name": "Alexey Tumanov"
                },
                "author": "Alexey Tumanov",
                "arxiv_comment": "ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14051v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14051v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09822v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09822v1",
                "updated": "2025-08-13T13:54:51Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    13,
                    54,
                    51,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T13:54:51Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    13,
                    54,
                    51,
                    2,
                    225,
                    0
                ],
                "title": "Physical Autoregressive Model for Robotic Manipulation without Action\n  Pretraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Physical Autoregressive Model for Robotic Manipulation without Action\n  Pretraining"
                },
                "summary": "The scarcity of manipulation data has motivated the use of pretrained large\nmodels from other modalities in robotics. In this work, we build upon\nautoregressive video generation models to propose a Physical Autoregressive\nModel (PAR), where physical tokens combine frames and actions to represent the\njoint evolution of the robot and its environment. PAR leverages the world\nknowledge embedded in video pretraining to understand physical dynamics without\nrequiring action pretraining, enabling accurate video prediction and consistent\naction trajectories. It also adopts a DiT-based de-tokenizer to model frames\nand actions as continuous tokens, mitigating quantization errors and\nfacilitating mutual enhancement. Furthermore, we incorporate a causal mask with\ninverse kinematics, parallel training, and the KV-cache mechanism to further\nimprove performance and efficiency. Experiments on the ManiSkill benchmark show\nthat PAR achieves a 100\\% success rate on the PushCube task, matches the\nperformance of action-pretrained baselines on other tasks, and accurately\npredicts future videos with tightly aligned action trajectories. These findings\nunderscore a promising direction for robotic manipulation by transferring world\nknowledge from autoregressive video pretraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The scarcity of manipulation data has motivated the use of pretrained large\nmodels from other modalities in robotics. In this work, we build upon\nautoregressive video generation models to propose a Physical Autoregressive\nModel (PAR), where physical tokens combine frames and actions to represent the\njoint evolution of the robot and its environment. PAR leverages the world\nknowledge embedded in video pretraining to understand physical dynamics without\nrequiring action pretraining, enabling accurate video prediction and consistent\naction trajectories. It also adopts a DiT-based de-tokenizer to model frames\nand actions as continuous tokens, mitigating quantization errors and\nfacilitating mutual enhancement. Furthermore, we incorporate a causal mask with\ninverse kinematics, parallel training, and the KV-cache mechanism to further\nimprove performance and efficiency. Experiments on the ManiSkill benchmark show\nthat PAR achieves a 100\\% success rate on the PushCube task, matches the\nperformance of action-pretrained baselines on other tasks, and accurately\npredicts future videos with tightly aligned action trajectories. These findings\nunderscore a promising direction for robotic manipulation by transferring world\nknowledge from autoregressive video pretraining."
                },
                "authors": [
                    {
                        "name": "Zijian Song"
                    },
                    {
                        "name": "Sihan Qin"
                    },
                    {
                        "name": "Tianshui Chen"
                    },
                    {
                        "name": "Liang Lin"
                    },
                    {
                        "name": "Guangrun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Guangrun Wang"
                },
                "author": "Guangrun Wang",
                "arxiv_comment": "16 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09822v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09822v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09570v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09570v1",
                "updated": "2025-08-13T07:40:25Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    7,
                    40,
                    25,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T07:40:25Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    7,
                    40,
                    25,
                    2,
                    225,
                    0
                ],
                "title": "Re-thinking Memory-Bound Limitations in CGRAs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Re-thinking Memory-Bound Limitations in CGRAs"
                },
                "summary": "Coarse-Grained Reconfigurable Arrays (CGRAs) are specialized accelerators\ncommonly employed to boost performance in workloads with iterative structures.\nExisting research typically focuses on compiler or architecture optimizations\naimed at improving CGRA performance, energy efficiency, flexibility, and area\nutilization, under the idealistic assumption that kernels can access all data\nfrom Scratchpad Memory (SPM). However, certain complex workloads-particularly\nin fields like graph analytics, irregular database operations, and specialized\nforms of high-performance computing (e.g., unstructured mesh\nsimulations)-exhibit irregular memory access patterns that hinder CGRA\nutilization, sometimes dropping below 1.5%, making the CGRA memory-bound. To\naddress this challenge, we conduct a thorough analysis of the underlying causes\nof performance degradation, then propose a redesigned memory subsystem and\nrefine the memory model. With both microarchitectural and theoretical\noptimization, our solution can effectively manage irregular memory accesses\nthrough CGRA-specific runahead execution mechanism and cache reconfiguration\ntechniques. Our results demonstrate that we can achieve performance comparable\nto the original SPM-only system while requiring only 1.27% of the storage size.\nThe runahead execution mechanism achieves an average 3.04x speedup (up to\n6.91x), with cache reconfiguration technique providing an additional 6.02%\nimprovement, significantly enhancing CGRA performance for irregular memory\naccess patterns.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coarse-Grained Reconfigurable Arrays (CGRAs) are specialized accelerators\ncommonly employed to boost performance in workloads with iterative structures.\nExisting research typically focuses on compiler or architecture optimizations\naimed at improving CGRA performance, energy efficiency, flexibility, and area\nutilization, under the idealistic assumption that kernels can access all data\nfrom Scratchpad Memory (SPM). However, certain complex workloads-particularly\nin fields like graph analytics, irregular database operations, and specialized\nforms of high-performance computing (e.g., unstructured mesh\nsimulations)-exhibit irregular memory access patterns that hinder CGRA\nutilization, sometimes dropping below 1.5%, making the CGRA memory-bound. To\naddress this challenge, we conduct a thorough analysis of the underlying causes\nof performance degradation, then propose a redesigned memory subsystem and\nrefine the memory model. With both microarchitectural and theoretical\noptimization, our solution can effectively manage irregular memory accesses\nthrough CGRA-specific runahead execution mechanism and cache reconfiguration\ntechniques. Our results demonstrate that we can achieve performance comparable\nto the original SPM-only system while requiring only 1.27% of the storage size.\nThe runahead execution mechanism achieves an average 3.04x speedup (up to\n6.91x), with cache reconfiguration technique providing an additional 6.02%\nimprovement, significantly enhancing CGRA performance for irregular memory\naccess patterns."
                },
                "authors": [
                    {
                        "name": "Xiangfeng Liu"
                    },
                    {
                        "name": "Zhe Jiang"
                    },
                    {
                        "name": "Anzhen Zhu"
                    },
                    {
                        "name": "Xiaomeng Han"
                    },
                    {
                        "name": "Mingsong Lyu"
                    },
                    {
                        "name": "Qingxu Deng"
                    },
                    {
                        "name": "Nan Guan"
                    }
                ],
                "author_detail": {
                    "name": "Nan Guan"
                },
                "author": "Nan Guan",
                "arxiv_doi": "10.1145/3760386",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3760386",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.09570v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09570v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "25 pages, 18 figures, CODES+ISSS 2025",
                "arxiv_journal_ref": "ACM Transactions on Embedded Computing Systems 2025",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.3.0; B.6.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00279v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00279v4",
                "updated": "2025-08-13T06:13:36Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    6,
                    13,
                    36,
                    2,
                    225,
                    0
                ],
                "published": "2024-12-31T05:24:30Z",
                "published_parsed": [
                    2024,
                    12,
                    31,
                    5,
                    24,
                    30,
                    1,
                    366,
                    0
                ],
                "title": "Performant Automatic BLAS Offloading on Unified Memory Architecture with\n  OpenMP First-Touch Style Data Movement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performant Automatic BLAS Offloading on Unified Memory Architecture with\n  OpenMP First-Touch Style Data Movement"
                },
                "summary": "BLAS is a fundamental building block of advanced linear algebra libraries and\nmany modern scientific computing applications. GPUs are known for their strong\narithmetic computing capabilities and are highly suited for BLAS operations.\nHowever, porting code to GPUs often requires significant effort, especially for\nlarge, complex codes or legacy codes, even for BLAS-heavy applications. While\nvarious tools exist to automatically offload BLAS to GPUs, they are often\nimpractical due to the high costs associated with mandatory data transfers. The\nadvent of unified memory architectures in recent GPU designs, such as the\nNVIDIA Grace-Hopper, allows cache-coherent memory access across all types of\nmemory for both CPU and GPU, potentially eliminating the bottlenecks faced in\nconventional architectures. This breakthrough paves the way for innovative\napplication developments and porting strategies. Building on our preliminary\nwork demonstrating the potential of automatic *gemm offload, this paper extends\nthe framework to all level-3 BLAS operations and introduces SCILIB-Accel, a\nnovel tool for automatic BLAS offload. SCILIB-Accel leverages the memory\ncoherency in Grace-Hopper and introduces a Device First-Use data movement\npolicy inspired by the OpenMP First-Touch approach in multi-socket CPU\nprogramming, minimizing CPU-GPU data transfers for typical scientific computing\ncodes. Additionally, utilizing dynamic binary instrumentation, the tool\nintercepts BLAS symbols directly from a CPU binary, requiring no code\nmodifications or recompilation. SCILIB-Accel has been evaluated using multiple\nquantum physics codes on up to a few hundred GPU nodes, yielding promising\nspeedups. Notably, for the LSMS method in the MuST suite, a 3x speedup was\nachieved on Grace-Hopper compared to Grace-Grace.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BLAS is a fundamental building block of advanced linear algebra libraries and\nmany modern scientific computing applications. GPUs are known for their strong\narithmetic computing capabilities and are highly suited for BLAS operations.\nHowever, porting code to GPUs often requires significant effort, especially for\nlarge, complex codes or legacy codes, even for BLAS-heavy applications. While\nvarious tools exist to automatically offload BLAS to GPUs, they are often\nimpractical due to the high costs associated with mandatory data transfers. The\nadvent of unified memory architectures in recent GPU designs, such as the\nNVIDIA Grace-Hopper, allows cache-coherent memory access across all types of\nmemory for both CPU and GPU, potentially eliminating the bottlenecks faced in\nconventional architectures. This breakthrough paves the way for innovative\napplication developments and porting strategies. Building on our preliminary\nwork demonstrating the potential of automatic *gemm offload, this paper extends\nthe framework to all level-3 BLAS operations and introduces SCILIB-Accel, a\nnovel tool for automatic BLAS offload. SCILIB-Accel leverages the memory\ncoherency in Grace-Hopper and introduces a Device First-Use data movement\npolicy inspired by the OpenMP First-Touch approach in multi-socket CPU\nprogramming, minimizing CPU-GPU data transfers for typical scientific computing\ncodes. Additionally, utilizing dynamic binary instrumentation, the tool\nintercepts BLAS symbols directly from a CPU binary, requiring no code\nmodifications or recompilation. SCILIB-Accel has been evaluated using multiple\nquantum physics codes on up to a few hundred GPU nodes, yielding promising\nspeedups. Notably, for the LSMS method in the MuST suite, a 3x speedup was\nachieved on Grace-Hopper compared to Grace-Grace."
                },
                "authors": [
                    {
                        "name": "Junjie Li"
                    }
                ],
                "author_detail": {
                    "name": "Junjie Li"
                },
                "author": "Junjie Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00279v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00279v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.09500v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.09500v2",
                "updated": "2025-08-13T04:24:56Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    4,
                    24,
                    56,
                    2,
                    225,
                    0
                ],
                "published": "2025-07-13T05:37:33Z",
                "published_parsed": [
                    2025,
                    7,
                    13,
                    5,
                    37,
                    33,
                    6,
                    194,
                    0
                ],
                "title": "Advancing Reliable Test-Time Adaptation of Vision-Language Models under\n  Visual Variations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Reliable Test-Time Adaptation of Vision-Language Models under\n  Visual Variations"
                },
                "summary": "Vision-language models (VLMs) exhibit remarkable zero-shot capabilities but\nstruggle with distribution shifts in downstream tasks when labeled data is\nunavailable, which has motivated the development of Test-Time Adaptation (TTA)\nto improve VLMs' performance during inference without annotations. Among\nvarious TTA approaches, cache-based methods show promise by preserving\nhistorical knowledge from low-entropy samples in a dynamic cache and fostering\nefficient adaptation. However, these methods face two critical reliability\nchallenges: (1) entropy often becomes unreliable under distribution shifts,\ncausing error accumulation in the cache and degradation in adaptation\nperformance; (2) the final predictions may be unreliable due to inflexible\ndecision boundaries that fail to accommodate large downstream shifts. To\naddress these challenges, we propose a Reliable Test-time Adaptation (ReTA)\nmethod that integrates two complementary strategies to enhance reliability from\ntwo perspectives. First, to mitigate the unreliability of entropy as a sample\nselection criterion for cache construction, we introduce Consistency-aware\nEntropy Reweighting (CER), which incorporates consistency constraints to weight\nentropy during cache updating. While conventional approaches rely solely on low\nentropy for cache prioritization and risk introducing noise, our method\nleverages predictive consistency to maintain a high-quality cache and\nfacilitate more robust adaptation. Second, we present Diversity-driven\nDistribution Calibration (DDC), which models class-wise text embeddings as\nmultivariate Gaussian distributions, enabling adaptive decision boundaries for\nmore accurate predictions across visually diverse content. Extensive\nexperiments demonstrate that ReTA consistently outperforms state-of-the-art\nmethods, particularly under real-world distribution shifts. Code:\nhttps://github.com/Evelyn1ywliang/ReTA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language models (VLMs) exhibit remarkable zero-shot capabilities but\nstruggle with distribution shifts in downstream tasks when labeled data is\nunavailable, which has motivated the development of Test-Time Adaptation (TTA)\nto improve VLMs' performance during inference without annotations. Among\nvarious TTA approaches, cache-based methods show promise by preserving\nhistorical knowledge from low-entropy samples in a dynamic cache and fostering\nefficient adaptation. However, these methods face two critical reliability\nchallenges: (1) entropy often becomes unreliable under distribution shifts,\ncausing error accumulation in the cache and degradation in adaptation\nperformance; (2) the final predictions may be unreliable due to inflexible\ndecision boundaries that fail to accommodate large downstream shifts. To\naddress these challenges, we propose a Reliable Test-time Adaptation (ReTA)\nmethod that integrates two complementary strategies to enhance reliability from\ntwo perspectives. First, to mitigate the unreliability of entropy as a sample\nselection criterion for cache construction, we introduce Consistency-aware\nEntropy Reweighting (CER), which incorporates consistency constraints to weight\nentropy during cache updating. While conventional approaches rely solely on low\nentropy for cache prioritization and risk introducing noise, our method\nleverages predictive consistency to maintain a high-quality cache and\nfacilitate more robust adaptation. Second, we present Diversity-driven\nDistribution Calibration (DDC), which models class-wise text embeddings as\nmultivariate Gaussian distributions, enabling adaptive decision boundaries for\nmore accurate predictions across visually diverse content. Extensive\nexperiments demonstrate that ReTA consistently outperforms state-of-the-art\nmethods, particularly under real-world distribution shifts. Code:\nhttps://github.com/Evelyn1ywliang/ReTA."
                },
                "authors": [
                    {
                        "name": "Yiwen Liang"
                    },
                    {
                        "name": "Hui Chen"
                    },
                    {
                        "name": "Yizhe Xiong"
                    },
                    {
                        "name": "Zihan Zhou"
                    },
                    {
                        "name": "Mengyao Lyu"
                    },
                    {
                        "name": "Zijia Lin"
                    },
                    {
                        "name": "Shuaicheng Niu"
                    },
                    {
                        "name": "Sicheng Zhao"
                    },
                    {
                        "name": "Jungong Han"
                    },
                    {
                        "name": "Guiguang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Guiguang Ding"
                },
                "author": "Guiguang Ding",
                "arxiv_comment": "Accepted at the 33rd ACM International Conference on Multimedia(ACM\n  MM 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.09500v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.09500v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20002v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20002v4",
                "updated": "2025-08-13T04:03:10Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    4,
                    3,
                    10,
                    2,
                    225,
                    0
                ],
                "published": "2024-09-30T06:55:00Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    6,
                    55,
                    0,
                    0,
                    274,
                    0
                ],
                "title": "The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM\n  Serving Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM\n  Serving Systems"
                },
                "summary": "The wide deployment of Large Language Models (LLMs) has given rise to strong\ndemands for optimizing their inference performance. Today's techniques serving\nthis purpose primarily focus on reducing latency and improving throughput\nthrough algorithmic and hardware enhancements, while largely overlooking their\nprivacy side effects, particularly in a multi-user environment. In our\nresearch, for the first time, we discovered a set of new timing side channels\nin LLM systems, arising from shared caches and GPU memory allocations, which\ncan be exploited to infer both confidential system prompts and those issued by\nother users. These vulnerabilities echo security challenges observed in\ntraditional computing systems, highlighting an urgent need to address potential\ninformation leakage in LLM serving infrastructures. In this paper, we report\nnovel attack strategies designed to exploit such timing side channels inherent\nin LLM deployments, specifically targeting the Key-Value (KV) cache and\nsemantic cache widely used to enhance LLM inference performance. Our approach\nleverages timing measurements and classification models to detect cache hits,\nallowing an adversary to infer private prompts with high accuracy. We also\npropose a token-by-token search algorithm to efficiently recover shared prompt\nprefixes in the caches, showing the feasibility of stealing system prompts and\nthose produced by peer users. Our experimental studies on black-box testing of\npopular online LLM services demonstrate that such privacy risks are completely\nrealistic, with significant consequences. Our findings underscore the need for\nrobust mitigation to protect LLM systems against such emerging threats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The wide deployment of Large Language Models (LLMs) has given rise to strong\ndemands for optimizing their inference performance. Today's techniques serving\nthis purpose primarily focus on reducing latency and improving throughput\nthrough algorithmic and hardware enhancements, while largely overlooking their\nprivacy side effects, particularly in a multi-user environment. In our\nresearch, for the first time, we discovered a set of new timing side channels\nin LLM systems, arising from shared caches and GPU memory allocations, which\ncan be exploited to infer both confidential system prompts and those issued by\nother users. These vulnerabilities echo security challenges observed in\ntraditional computing systems, highlighting an urgent need to address potential\ninformation leakage in LLM serving infrastructures. In this paper, we report\nnovel attack strategies designed to exploit such timing side channels inherent\nin LLM deployments, specifically targeting the Key-Value (KV) cache and\nsemantic cache widely used to enhance LLM inference performance. Our approach\nleverages timing measurements and classification models to detect cache hits,\nallowing an adversary to infer private prompts with high accuracy. We also\npropose a token-by-token search algorithm to efficiently recover shared prompt\nprefixes in the caches, showing the feasibility of stealing system prompts and\nthose produced by peer users. Our experimental studies on black-box testing of\npopular online LLM services demonstrate that such privacy risks are completely\nrealistic, with significant consequences. Our findings underscore the need for\nrobust mitigation to protect LLM systems against such emerging threats."
                },
                "authors": [
                    {
                        "name": "Linke Song"
                    },
                    {
                        "name": "Zixuan Pang"
                    },
                    {
                        "name": "Wenhao Wang"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "XiaoFeng Wang"
                    },
                    {
                        "name": "Hongbo Chen"
                    },
                    {
                        "name": "Wei Song"
                    },
                    {
                        "name": "Yier Jin"
                    },
                    {
                        "name": "Dan Meng"
                    },
                    {
                        "name": "Rui Hou"
                    }
                ],
                "author_detail": {
                    "name": "Rui Hou"
                },
                "author": "Rui Hou",
                "arxiv_comment": "This work was submitted for review on Sept. 5, 2024, and the initial\n  version was uploaded to Arxiv on Sept. 30, 2024. The latest version reflects\n  the up-to-date experimental results",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20002v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20002v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09442v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09442v1",
                "updated": "2025-08-13T02:48:25Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    2,
                    48,
                    25,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T02:48:25Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    2,
                    48,
                    25,
                    2,
                    225,
                    0
                ],
                "title": "Shadow in the Cache: Unveiling and Mitigating Privacy Risks of KV-cache\n  in LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shadow in the Cache: Unveiling and Mitigating Privacy Risks of KV-cache\n  in LLM Inference"
                },
                "summary": "The Key-Value (KV) cache, which stores intermediate attention computations\n(Key and Value pairs) to avoid redundant calculations, is a fundamental\nmechanism for accelerating Large Language Model (LLM) inference. However, this\nefficiency optimization introduces significant yet underexplored privacy risks.\nThis paper provides the first comprehensive analysis of these vulnerabilities,\ndemonstrating that an attacker can reconstruct sensitive user inputs directly\nfrom the KV-cache. We design and implement three distinct attack vectors: a\ndirect Inversion Attack, a more broadly applicable and potent Collision Attack,\nand a semantic-based Injection Attack. These methods demonstrate the\npracticality and severity of KV-cache privacy leakage issues. To mitigate this,\nwe propose KV-Cloak, a novel, lightweight, and efficient defense mechanism.\nKV-Cloak uses a reversible matrix-based obfuscation scheme, combined with\noperator fusion, to secure the KV-cache. Our extensive experiments show that\nKV-Cloak effectively thwarts all proposed attacks, reducing reconstruction\nquality to random noise. Crucially, it achieves this robust security with\nvirtually no degradation in model accuracy and minimal performance overhead,\noffering a practical solution for trustworthy LLM deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Key-Value (KV) cache, which stores intermediate attention computations\n(Key and Value pairs) to avoid redundant calculations, is a fundamental\nmechanism for accelerating Large Language Model (LLM) inference. However, this\nefficiency optimization introduces significant yet underexplored privacy risks.\nThis paper provides the first comprehensive analysis of these vulnerabilities,\ndemonstrating that an attacker can reconstruct sensitive user inputs directly\nfrom the KV-cache. We design and implement three distinct attack vectors: a\ndirect Inversion Attack, a more broadly applicable and potent Collision Attack,\nand a semantic-based Injection Attack. These methods demonstrate the\npracticality and severity of KV-cache privacy leakage issues. To mitigate this,\nwe propose KV-Cloak, a novel, lightweight, and efficient defense mechanism.\nKV-Cloak uses a reversible matrix-based obfuscation scheme, combined with\noperator fusion, to secure the KV-cache. Our extensive experiments show that\nKV-Cloak effectively thwarts all proposed attacks, reducing reconstruction\nquality to random noise. Crucially, it achieves this robust security with\nvirtually no degradation in model accuracy and minimal performance overhead,\noffering a practical solution for trustworthy LLM deployment."
                },
                "authors": [
                    {
                        "name": "Zhifan Luo"
                    },
                    {
                        "name": "Shuo Shao"
                    },
                    {
                        "name": "Su Zhang"
                    },
                    {
                        "name": "Lijing Zhou"
                    },
                    {
                        "name": "Yuke Hu"
                    },
                    {
                        "name": "Chenxu Zhao"
                    },
                    {
                        "name": "Zhihao Liu"
                    },
                    {
                        "name": "Zhan Qin"
                    }
                ],
                "author_detail": {
                    "name": "Zhan Qin"
                },
                "author": "Zhan Qin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09442v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09442v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09419v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09419v1",
                "updated": "2025-08-13T01:39:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    1,
                    39,
                    9,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T01:39:09Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    1,
                    39,
                    9,
                    2,
                    225,
                    0
                ],
                "title": "Design and Simulation of 6T SRAM Array",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Design and Simulation of 6T SRAM Array"
                },
                "summary": "Conventional 6T SRAM is used in microprocessors in the cache memory design.\nThe basic 6T SRAM cell and a 6 bit memory array layout are designed in LEdit.\nThe design and analysis of key SRAM components, sense amplifiers, decoders,\nwrite drivers and precharge circuits are also provided. The pulse voltage\nwaveforms generated for read and write operations as well as Q and Qbar nodes\nare simulated in LTSpice. Parasitic capacitances are extracted and their impact\non the waveforms analyzed. Static noise margin, propagation delays, and power\ndissipation are calculated. Comparison of SRAM read and write operational\nperformance using CMOS transistors is made with edge-triggered D flip flops. If\ncertain size area and ratio constraints are satisfied, the 6T cell with CMOS\ntransistors will possess stability, speed, and power efficiency. Both\ntheoretical and simulated results are given.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conventional 6T SRAM is used in microprocessors in the cache memory design.\nThe basic 6T SRAM cell and a 6 bit memory array layout are designed in LEdit.\nThe design and analysis of key SRAM components, sense amplifiers, decoders,\nwrite drivers and precharge circuits are also provided. The pulse voltage\nwaveforms generated for read and write operations as well as Q and Qbar nodes\nare simulated in LTSpice. Parasitic capacitances are extracted and their impact\non the waveforms analyzed. Static noise margin, propagation delays, and power\ndissipation are calculated. Comparison of SRAM read and write operational\nperformance using CMOS transistors is made with edge-triggered D flip flops. If\ncertain size area and ratio constraints are satisfied, the 6T cell with CMOS\ntransistors will possess stability, speed, and power efficiency. Both\ntheoretical and simulated results are given."
                },
                "authors": [
                    {
                        "name": "Justin London"
                    }
                ],
                "author_detail": {
                    "name": "Justin London"
                },
                "author": "Justin London",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09419v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09419v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08744v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08744v2",
                "updated": "2025-08-13T01:39:03Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    1,
                    39,
                    3,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-12T08:39:32Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    8,
                    39,
                    32,
                    1,
                    224,
                    0
                ],
                "title": "Scalable Graph Indexing using GPUs for Approximate Nearest Neighbor\n  Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalable Graph Indexing using GPUs for Approximate Nearest Neighbor\n  Search"
                },
                "summary": "Approximate nearest neighbor search (ANNS) in high-dimensional vector spaces\nhas a wide range of real-world applications. Numerous methods have been\nproposed to handle ANNS efficiently, while graph-based indexes have gained\nprominence due to their high accuracy and efficiency. However, the indexing\noverhead of graph-based indexes remains substantial. With exponential growth in\ndata volume and increasing demands for dynamic index adjustments, this overhead\ncontinues to escalate, posing a critical challenge. In this paper, we introduce\nTagore, a fast library accelerated by GPUs for graph indexing, which has\npowerful capabilities of constructing refinement-based graph indexes such as\nNSG and Vamana. We first introduce GNN-Descent, a GPU-specific algorithm for\nefficient k-Nearest Neighbor (k-NN) graph initialization. GNN-Descent speeds up\nthe similarity comparison by a two-phase descent procedure and enables highly\nparallelized neighbor updates. Next, aiming to support various k-NN graph\npruning strategies, we formulate a universal computing procedure termed CFS and\ndevise two generalized GPU kernels for parallel processing complex dependencies\nin neighbor relationships. For large-scale datasets exceeding GPU memory\ncapacity, we propose an asynchronous GPU-CPU-disk indexing framework with a\ncluster-aware caching mechanism to minimize the I/O pressure on the disk.\nExtensive experiments on 7 real-world datasets exhibit that Tagore achieves\n1.32x-112.79x speedup while maintaining the index quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Approximate nearest neighbor search (ANNS) in high-dimensional vector spaces\nhas a wide range of real-world applications. Numerous methods have been\nproposed to handle ANNS efficiently, while graph-based indexes have gained\nprominence due to their high accuracy and efficiency. However, the indexing\noverhead of graph-based indexes remains substantial. With exponential growth in\ndata volume and increasing demands for dynamic index adjustments, this overhead\ncontinues to escalate, posing a critical challenge. In this paper, we introduce\nTagore, a fast library accelerated by GPUs for graph indexing, which has\npowerful capabilities of constructing refinement-based graph indexes such as\nNSG and Vamana. We first introduce GNN-Descent, a GPU-specific algorithm for\nefficient k-Nearest Neighbor (k-NN) graph initialization. GNN-Descent speeds up\nthe similarity comparison by a two-phase descent procedure and enables highly\nparallelized neighbor updates. Next, aiming to support various k-NN graph\npruning strategies, we formulate a universal computing procedure termed CFS and\ndevise two generalized GPU kernels for parallel processing complex dependencies\nin neighbor relationships. For large-scale datasets exceeding GPU memory\ncapacity, we propose an asynchronous GPU-CPU-disk indexing framework with a\ncluster-aware caching mechanism to minimize the I/O pressure on the disk.\nExtensive experiments on 7 real-world datasets exhibit that Tagore achieves\n1.32x-112.79x speedup while maintaining the index quality."
                },
                "authors": [
                    {
                        "name": "Zhonggen Li"
                    },
                    {
                        "name": "Xiangyu Ke"
                    },
                    {
                        "name": "Yifan Zhu"
                    },
                    {
                        "name": "Bocheng Yu"
                    },
                    {
                        "name": "Baihua Zheng"
                    },
                    {
                        "name": "Yunjun Gao"
                    }
                ],
                "author_detail": {
                    "name": "Yunjun Gao"
                },
                "author": "Yunjun Gao",
                "arxiv_comment": "Accepted at SIGMOD 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08744v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08744v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09262v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09262v1",
                "updated": "2025-08-12T18:05:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    18,
                    5,
                    33,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T18:05:33Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    18,
                    5,
                    33,
                    1,
                    224,
                    0
                ],
                "title": "Harnessing Input-Adaptive Inference for Efficient VLN",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harnessing Input-Adaptive Inference for Efficient VLN"
                },
                "summary": "An emerging paradigm in vision-and-language navigation (VLN) is the use of\nhistory-aware multi-modal transformer models. Given a language instruction,\nthese models process observation and navigation history to predict the most\nappropriate action for an agent. While they have significantly improved\nperformance, the scale of these models can be a bottleneck in practical\nsettings with limited computational resources. In this work, we propose a novel\ninput-adaptive navigation method to enhance VLN model efficiency. We first show\nthat existing input-adaptive mechanisms fail to reduce computations without\nsubstantial performance degradation. To address this, we introduce three\nadaptive algorithms, each deployed at a different level: (1) To improve spatial\nefficiency, we selectively process panoramic views at each observation of an\nagent. (2) To improve intra-model efficiency, we propose importance-based\nadaptive thresholding for the early-exit methods. (3) To improve temporal\nefficiency, we implement a caching mechanism that prevents reprocessing of\nviews previously seen by the agent. In evaluations on seven VLN benchmarks, we\ndemonstrate over a 2$\\times$ reduction in computation across three\noff-the-shelf agents in both standard and continuous environments. Our code is\npublicly available at\nhttps://github.com/secure-ai-systems-group/adaptive-vision-and-language-navigation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An emerging paradigm in vision-and-language navigation (VLN) is the use of\nhistory-aware multi-modal transformer models. Given a language instruction,\nthese models process observation and navigation history to predict the most\nappropriate action for an agent. While they have significantly improved\nperformance, the scale of these models can be a bottleneck in practical\nsettings with limited computational resources. In this work, we propose a novel\ninput-adaptive navigation method to enhance VLN model efficiency. We first show\nthat existing input-adaptive mechanisms fail to reduce computations without\nsubstantial performance degradation. To address this, we introduce three\nadaptive algorithms, each deployed at a different level: (1) To improve spatial\nefficiency, we selectively process panoramic views at each observation of an\nagent. (2) To improve intra-model efficiency, we propose importance-based\nadaptive thresholding for the early-exit methods. (3) To improve temporal\nefficiency, we implement a caching mechanism that prevents reprocessing of\nviews previously seen by the agent. In evaluations on seven VLN benchmarks, we\ndemonstrate over a 2$\\times$ reduction in computation across three\noff-the-shelf agents in both standard and continuous environments. Our code is\npublicly available at\nhttps://github.com/secure-ai-systems-group/adaptive-vision-and-language-navigation."
                },
                "authors": [
                    {
                        "name": "Dongwoo Kang"
                    },
                    {
                        "name": "Akhil Perincherry"
                    },
                    {
                        "name": "Zachary Coalson"
                    },
                    {
                        "name": "Aiden Gabriel"
                    },
                    {
                        "name": "Stefan Lee"
                    },
                    {
                        "name": "Sanghyun Hong"
                    }
                ],
                "author_detail": {
                    "name": "Sanghyun Hong"
                },
                "author": "Sanghyun Hong",
                "arxiv_comment": "Accepted to ICCV 2025 [Poster]",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09262v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09262v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09072v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09072v1",
                "updated": "2025-08-12T16:47:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    16,
                    47,
                    48,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T16:47:48Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    16,
                    47,
                    48,
                    1,
                    224,
                    0
                ],
                "title": "READER: Retrieval-Assisted Drafter for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "READER: Retrieval-Assisted Drafter for Efficient LLM Inference"
                },
                "summary": "Large Language Models (LLMs) generate tokens autoregressively, with each\ntoken depending on the preceding context. This sequential nature makes the\ninference process inherently difficult to accelerate, posing a significant\nchallenge for efficient deployment. In recent years, various methods have been\nproposed to address this issue, with the most effective approaches often\ninvolving the training of additional draft models. In this paper, we introduce\nREADER (Retrieval-Assisted Drafter for Efficient LLM Inference), a novel\nlossless speculative decoding method that enhances model-based approaches by\nleveraging self-repetitions in the text. Our algorithm expands the speculative\ndecoding tree using tokens obtained through statistical search. This work\nfocuses on large batch sizes (>= 8), an underexplored yet important area for\nindustrial applications. We also analyze the key-value (KV) cache size during\nspeculative decoding and propose an optimization to improve performance for\nlarge batches. As a result, READER outperforms existing speculative decoding\nmethods. Notably, READER requires no additional training and can reuse\npre-trained speculator models, increasing the speedup by over 40\\%. Our method\ndemonstrates particularly strong performance on search-based tasks, such as\nretrieval-augmented generation, where we achieve more than 10x speedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) generate tokens autoregressively, with each\ntoken depending on the preceding context. This sequential nature makes the\ninference process inherently difficult to accelerate, posing a significant\nchallenge for efficient deployment. In recent years, various methods have been\nproposed to address this issue, with the most effective approaches often\ninvolving the training of additional draft models. In this paper, we introduce\nREADER (Retrieval-Assisted Drafter for Efficient LLM Inference), a novel\nlossless speculative decoding method that enhances model-based approaches by\nleveraging self-repetitions in the text. Our algorithm expands the speculative\ndecoding tree using tokens obtained through statistical search. This work\nfocuses on large batch sizes (>= 8), an underexplored yet important area for\nindustrial applications. We also analyze the key-value (KV) cache size during\nspeculative decoding and propose an optimization to improve performance for\nlarge batches. As a result, READER outperforms existing speculative decoding\nmethods. Notably, READER requires no additional training and can reuse\npre-trained speculator models, increasing the speedup by over 40\\%. Our method\ndemonstrates particularly strong performance on search-based tasks, such as\nretrieval-augmented generation, where we achieve more than 10x speedup."
                },
                "authors": [
                    {
                        "name": "Maxim Divilkovskiy"
                    },
                    {
                        "name": "Vitaly Malygin"
                    },
                    {
                        "name": "Sergey Zlobin"
                    },
                    {
                        "name": "Sultan Isali"
                    },
                    {
                        "name": "Vasily Kalugin"
                    },
                    {
                        "name": "Stanislav Ilyushin"
                    },
                    {
                        "name": "Nuriza Aitassova"
                    },
                    {
                        "name": "Yi Fei"
                    },
                    {
                        "name": "Zeng Weidi"
                    }
                ],
                "author_detail": {
                    "name": "Zeng Weidi"
                },
                "author": "Zeng Weidi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09072v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09072v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09001v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09001v1",
                "updated": "2025-08-12T15:11:47Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    15,
                    11,
                    47,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T15:11:47Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    15,
                    11,
                    47,
                    1,
                    224,
                    0
                ],
                "title": "Retrospective Sparse Attention for Efficient Long-Context Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrospective Sparse Attention for Efficient Long-Context Generation"
                },
                "summary": "Large Language Models (LLMs) are increasingly deployed in long-context tasks\nsuch as reasoning, code generation, and multi-turn dialogue. However, inference\nover extended contexts is bottlenecked by the Key-Value (KV) cache, whose\nmemory footprint grows linearly with sequence length and dominates latency at\neach decoding step. While recent KV cache compression methods identify and load\nimportant tokens, they focus predominantly on input contexts and fail to\naddress the cumulative attention errors that arise during long decoding. In\nthis paper, we introduce RetroAttention, a novel KV cache update technique that\nretrospectively revises past attention outputs using newly arrived KV entries\nfrom subsequent decoding steps. By maintaining a lightweight output cache,\nRetroAttention enables past queries to efficiently access more relevant\ncontext, while incurring minimal latency overhead. This breaks the\nfixed-attention-output paradigm and allows continual correction of prior\napproximations. Extensive experiments on long-generation benchmarks show that\nRetroAttention consistently outperforms state-of-the-art (SOTA) KV compression\nmethods, increasing effective KV exposure by up to 1.6$\\times$ and accuracy by\nup to 21.9\\%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly deployed in long-context tasks\nsuch as reasoning, code generation, and multi-turn dialogue. However, inference\nover extended contexts is bottlenecked by the Key-Value (KV) cache, whose\nmemory footprint grows linearly with sequence length and dominates latency at\neach decoding step. While recent KV cache compression methods identify and load\nimportant tokens, they focus predominantly on input contexts and fail to\naddress the cumulative attention errors that arise during long decoding. In\nthis paper, we introduce RetroAttention, a novel KV cache update technique that\nretrospectively revises past attention outputs using newly arrived KV entries\nfrom subsequent decoding steps. By maintaining a lightweight output cache,\nRetroAttention enables past queries to efficiently access more relevant\ncontext, while incurring minimal latency overhead. This breaks the\nfixed-attention-output paradigm and allows continual correction of prior\napproximations. Extensive experiments on long-generation benchmarks show that\nRetroAttention consistently outperforms state-of-the-art (SOTA) KV compression\nmethods, increasing effective KV exposure by up to 1.6$\\times$ and accuracy by\nup to 21.9\\%."
                },
                "authors": [
                    {
                        "name": "Seonghwan Choi"
                    },
                    {
                        "name": "Beomseok Kang"
                    },
                    {
                        "name": "Dongwon Jo"
                    },
                    {
                        "name": "Jae-Joon Kim"
                    }
                ],
                "author_detail": {
                    "name": "Jae-Joon Kim"
                },
                "author": "Jae-Joon Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09001v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09001v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08978v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08978v1",
                "updated": "2025-08-12T14:40:36Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    14,
                    40,
                    36,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T14:40:36Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    14,
                    40,
                    36,
                    1,
                    224,
                    0
                ],
                "title": "TaoCache: Structure-Maintained Video Generation Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TaoCache: Structure-Maintained Video Generation Acceleration"
                },
                "summary": "Existing cache-based acceleration methods for video diffusion models\nprimarily skip early or mid denoising steps, which often leads to structural\ndiscrepancies relative to full-timestep generation and can hinder instruction\nfollowing and character consistency. We present TaoCache, a training-free,\nplug-and-play caching strategy that, instead of residual-based caching, adopts\na fixed-point perspective to predict the model's noise output and is\nspecifically effective in late denoising stages. By calibrating cosine\nsimilarities and norm ratios of consecutive noise deltas, TaoCache preserves\nhigh-resolution structure while enabling aggressive skipping. The approach is\northogonal to complementary accelerations such as Pyramid Attention Broadcast\n(PAB) and TeaCache, and it integrates seamlessly into DiT-based frameworks.\nAcross Latte-1, OpenSora-Plan v110, and Wan2.1, TaoCache attains substantially\nhigher visual quality (LPIPS, SSIM, PSNR) than prior caching methods under the\nsame speedups.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing cache-based acceleration methods for video diffusion models\nprimarily skip early or mid denoising steps, which often leads to structural\ndiscrepancies relative to full-timestep generation and can hinder instruction\nfollowing and character consistency. We present TaoCache, a training-free,\nplug-and-play caching strategy that, instead of residual-based caching, adopts\na fixed-point perspective to predict the model's noise output and is\nspecifically effective in late denoising stages. By calibrating cosine\nsimilarities and norm ratios of consecutive noise deltas, TaoCache preserves\nhigh-resolution structure while enabling aggressive skipping. The approach is\northogonal to complementary accelerations such as Pyramid Attention Broadcast\n(PAB) and TeaCache, and it integrates seamlessly into DiT-based frameworks.\nAcross Latte-1, OpenSora-Plan v110, and Wan2.1, TaoCache attains substantially\nhigher visual quality (LPIPS, SSIM, PSNR) than prior caching methods under the\nsame speedups."
                },
                "authors": [
                    {
                        "name": "Zhentao Fan"
                    },
                    {
                        "name": "Zongzuo Wang"
                    },
                    {
                        "name": "Weiwei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Weiwei Zhang"
                },
                "author": "Weiwei Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08978v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08978v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.11488v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.11488v2",
                "updated": "2025-08-12T10:43:55Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    10,
                    43,
                    55,
                    1,
                    224,
                    0
                ],
                "published": "2023-11-30T16:02:04Z",
                "published_parsed": [
                    2023,
                    11,
                    30,
                    16,
                    2,
                    4,
                    3,
                    334,
                    0
                ],
                "title": "Keep Your Friends Close: Leveraging Affinity Groups to Accelerate AI\n  Inference Workflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Keep Your Friends Close: Leveraging Affinity Groups to Accelerate AI\n  Inference Workflows"
                },
                "summary": "AI inference workflows are typically structured as a pipeline or graph of AI\nprograms triggered by events. As events occur, the AIs perform inference or\nclassification tasks under time pressure to respond or take some action.\nStandard techniques that reduce latency in other streaming settings (such as\ncaching and optimization-driven scheduling) are of limited value because AI\ndata access patterns (models, databases) change depending on the triggering\nevent: a significant departure from traditional streaming. In this work, we\npropose a novel affinity grouping mechanism that makes it easier for developers\nto express application-specific data access correlations, enabling coordinated\nmanagement of data objects in server clusters hosting streaming inference\ntasks. Our proposals are thus complementary to other approaches such as caching\nand scheduling. Experiments confirm the limitations of standard techniques,\nwhile showing that the proposed mechanism is able to maintain significantly\nlower latency as workload and scale-out increase, and yet requires only minor\ncode changes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI inference workflows are typically structured as a pipeline or graph of AI\nprograms triggered by events. As events occur, the AIs perform inference or\nclassification tasks under time pressure to respond or take some action.\nStandard techniques that reduce latency in other streaming settings (such as\ncaching and optimization-driven scheduling) are of limited value because AI\ndata access patterns (models, databases) change depending on the triggering\nevent: a significant departure from traditional streaming. In this work, we\npropose a novel affinity grouping mechanism that makes it easier for developers\nto express application-specific data access correlations, enabling coordinated\nmanagement of data objects in server clusters hosting streaming inference\ntasks. Our proposals are thus complementary to other approaches such as caching\nand scheduling. Experiments confirm the limitations of standard techniques,\nwhile showing that the proposed mechanism is able to maintain significantly\nlower latency as workload and scale-out increase, and yet requires only minor\ncode changes."
                },
                "authors": [
                    {
                        "name": "Thiago Garrett"
                    },
                    {
                        "name": "Weijia Song"
                    },
                    {
                        "name": "Roman Vitenberg"
                    },
                    {
                        "name": "Ken Birman"
                    }
                ],
                "author_detail": {
                    "name": "Ken Birman"
                },
                "author": "Ken Birman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.11488v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.11488v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07772v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07772v3",
                "updated": "2025-08-12T05:51:37Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    5,
                    51,
                    37,
                    1,
                    224,
                    0
                ],
                "published": "2024-12-10T18:59:50Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    59,
                    50,
                    1,
                    345,
                    0
                ],
                "title": "From Slow Bidirectional to Fast Autoregressive Video Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Slow Bidirectional to Fast Autoregressive Video Diffusion Models"
                },
                "summary": "Current video diffusion models achieve impressive generation quality but\nstruggle in interactive applications due to bidirectional attention\ndependencies. The generation of a single frame requires the model to process\nthe entire sequence, including the future. We address this limitation by\nadapting a pretrained bidirectional diffusion transformer to an autoregressive\ntransformer that generates frames on-the-fly. To further reduce latency, we\nextend distribution matching distillation (DMD) to videos, distilling 50-step\ndiffusion model into a 4-step generator. To enable stable and high-quality\ndistillation, we introduce a student initialization scheme based on teacher's\nODE trajectories, as well as an asymmetric distillation strategy that\nsupervises a causal student model with a bidirectional teacher. This approach\neffectively mitigates error accumulation in autoregressive generation, allowing\nlong-duration video synthesis despite training on short clips. Our model\nachieves a total score of 84.27 on the VBench-Long benchmark, surpassing all\nprevious video generation models. It enables fast streaming generation of\nhigh-quality videos at 9.4 FPS on a single GPU thanks to KV caching. Our\napproach also enables streaming video-to-video translation, image-to-video, and\ndynamic prompting in a zero-shot manner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current video diffusion models achieve impressive generation quality but\nstruggle in interactive applications due to bidirectional attention\ndependencies. The generation of a single frame requires the model to process\nthe entire sequence, including the future. We address this limitation by\nadapting a pretrained bidirectional diffusion transformer to an autoregressive\ntransformer that generates frames on-the-fly. To further reduce latency, we\nextend distribution matching distillation (DMD) to videos, distilling 50-step\ndiffusion model into a 4-step generator. To enable stable and high-quality\ndistillation, we introduce a student initialization scheme based on teacher's\nODE trajectories, as well as an asymmetric distillation strategy that\nsupervises a causal student model with a bidirectional teacher. This approach\neffectively mitigates error accumulation in autoregressive generation, allowing\nlong-duration video synthesis despite training on short clips. Our model\nachieves a total score of 84.27 on the VBench-Long benchmark, surpassing all\nprevious video generation models. It enables fast streaming generation of\nhigh-quality videos at 9.4 FPS on a single GPU thanks to KV caching. Our\napproach also enables streaming video-to-video translation, image-to-video, and\ndynamic prompting in a zero-shot manner."
                },
                "authors": [
                    {
                        "name": "Tianwei Yin"
                    },
                    {
                        "name": "Qiang Zhang"
                    },
                    {
                        "name": "Richard Zhang"
                    },
                    {
                        "name": "William T. Freeman"
                    },
                    {
                        "name": "Fredo Durand"
                    },
                    {
                        "name": "Eli Shechtman"
                    },
                    {
                        "name": "Xun Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xun Huang"
                },
                "author": "Xun Huang",
                "arxiv_comment": "Project Page: https://causvid.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07772v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07772v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08600v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08600v1",
                "updated": "2025-08-12T03:33:15Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    3,
                    33,
                    15,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T03:33:15Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    3,
                    33,
                    15,
                    1,
                    224,
                    0
                ],
                "title": "Rigorous quantum calculations for atom-molecule chemical reactions in\n  electric fields: from single to multiple partial wave regimes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rigorous quantum calculations for atom-molecule chemical reactions in\n  electric fields: from single to multiple partial wave regimes"
                },
                "summary": "We present an efficient method for rigorous quantum calculations of cross\nsections for atom-molecule reactive scattering in the presence of a dc electric\nfield. The wavefunction of the reaction complex is expanded in an overcomplete\nset of arrangement-dependent Fock-Delves hyperspherical basis functions and the\ninteractions of the reactants and products with electric fields are accounted\nfor in the total angular momentum representation. A significant computational\nchallenge affecting our previously developed approach [Phys. Rev. Lett.\n$\\mathbf{115}$, 023201 (2015)] is addressed by an efficient asymptotic frame\ntransformation between the hyperspherical and Jacobi coordinates in the\npresence of an external field. Using accurate {\\it ab initio} potential energy\nsurfaces, we calculate total and state-resolved cross sections for the chemical\nreactions LiF$(v=1,j=0)$ + H $\\to$ Li + HF($v'=0,j'$) and F + HD$(v=0,j=0)$\n$\\to$ HF + D, DF + H as functions of collision energy and electric field\nstrength. The field dependence of the cross sections for the LiF + H chemical\nreaction exhibits resonance structure mediated by tunneling-driven interactions\nbetween reactants and products. No significant field effects are found for the\nF + HD $\\to$ HF + D, DF + H chemical reaction at 1 Kelvin, even for\nstate-resolved transitions and with field magnitudes reaching 200 kV/cm. Our\ncalculations illustrate the essential role of basis set convergence for the\nproper interpretation of external field effects on chemical reaction dynamics.\nWhile reduced-basis calculations for the F + HD reaction indicate significant\neffects of electric fields on product state distributions, these effects vanish\nwhen the number of total angular momentum basis states is increased.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present an efficient method for rigorous quantum calculations of cross\nsections for atom-molecule reactive scattering in the presence of a dc electric\nfield. The wavefunction of the reaction complex is expanded in an overcomplete\nset of arrangement-dependent Fock-Delves hyperspherical basis functions and the\ninteractions of the reactants and products with electric fields are accounted\nfor in the total angular momentum representation. A significant computational\nchallenge affecting our previously developed approach [Phys. Rev. Lett.\n$\\mathbf{115}$, 023201 (2015)] is addressed by an efficient asymptotic frame\ntransformation between the hyperspherical and Jacobi coordinates in the\npresence of an external field. Using accurate {\\it ab initio} potential energy\nsurfaces, we calculate total and state-resolved cross sections for the chemical\nreactions LiF$(v=1,j=0)$ + H $\\to$ Li + HF($v'=0,j'$) and F + HD$(v=0,j=0)$\n$\\to$ HF + D, DF + H as functions of collision energy and electric field\nstrength. The field dependence of the cross sections for the LiF + H chemical\nreaction exhibits resonance structure mediated by tunneling-driven interactions\nbetween reactants and products. No significant field effects are found for the\nF + HD $\\to$ HF + D, DF + H chemical reaction at 1 Kelvin, even for\nstate-resolved transitions and with field magnitudes reaching 200 kV/cm. Our\ncalculations illustrate the essential role of basis set convergence for the\nproper interpretation of external field effects on chemical reaction dynamics.\nWhile reduced-basis calculations for the F + HD reaction indicate significant\neffects of electric fields on product state distributions, these effects vanish\nwhen the number of total angular momentum basis states is increased."
                },
                "authors": [
                    {
                        "name": "Timur V. Tscherbul"
                    },
                    {
                        "name": "Roman V. Krems"
                    }
                ],
                "author_detail": {
                    "name": "Roman V. Krems"
                },
                "author": "Roman V. Krems",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08600v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08600v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.chem-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.chem-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.atom-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.07675v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.07675v2",
                "updated": "2025-08-12T02:51:12Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    2,
                    51,
                    12,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-11T06:53:27Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    6,
                    53,
                    27,
                    0,
                    223,
                    0
                ],
                "title": "Semantic Caching for Low-Cost LLM Serving: From Offline Learning to\n  Online Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Caching for Low-Cost LLM Serving: From Offline Learning to\n  Online Adaptation"
                },
                "summary": "Large Language Models (LLMs) are revolutionizing how users interact with\ninformation systems, yet their high inference cost poses serious scalability\nand sustainability challenges. Caching inference responses, allowing them to be\nretrieved without another forward pass through the LLM, has emerged as one\npossible solution. Traditional exact-match caching, however, overlooks the\nsemantic similarity between queries, leading to unnecessary recomputation.\nSemantic caching addresses this by retrieving responses based on semantic\nsimilarity, but introduces a fundamentally different cache eviction problem:\none must account for mismatch costs between incoming queries and cached\nresponses. Moreover, key system parameters, such as query arrival probabilities\nand serving costs, are often unknown and must be learned over time. Existing\nsemantic caching methods are largely ad-hoc, lacking theoretical foundations\nand unable to adapt to real-world uncertainty. In this paper, we present a\nprincipled, learning-based framework for semantic cache eviction under unknown\nquery and cost distributions. We formulate both offline optimization and online\nlearning variants of the problem, and develop provably efficient algorithms\nwith state-of-the-art guarantees. We also evaluate our framework on a synthetic\ndataset, showing that our proposed algorithms perform matching or superior\nperformance compared with baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are revolutionizing how users interact with\ninformation systems, yet their high inference cost poses serious scalability\nand sustainability challenges. Caching inference responses, allowing them to be\nretrieved without another forward pass through the LLM, has emerged as one\npossible solution. Traditional exact-match caching, however, overlooks the\nsemantic similarity between queries, leading to unnecessary recomputation.\nSemantic caching addresses this by retrieving responses based on semantic\nsimilarity, but introduces a fundamentally different cache eviction problem:\none must account for mismatch costs between incoming queries and cached\nresponses. Moreover, key system parameters, such as query arrival probabilities\nand serving costs, are often unknown and must be learned over time. Existing\nsemantic caching methods are largely ad-hoc, lacking theoretical foundations\nand unable to adapt to real-world uncertainty. In this paper, we present a\nprincipled, learning-based framework for semantic cache eviction under unknown\nquery and cost distributions. We formulate both offline optimization and online\nlearning variants of the problem, and develop provably efficient algorithms\nwith state-of-the-art guarantees. We also evaluate our framework on a synthetic\ndataset, showing that our proposed algorithms perform matching or superior\nperformance compared with baselines."
                },
                "authors": [
                    {
                        "name": "Xutong Liu"
                    },
                    {
                        "name": "Baran Atalar"
                    },
                    {
                        "name": "Xiangxiang Dai"
                    },
                    {
                        "name": "Jinhang Zuo"
                    },
                    {
                        "name": "Siwei Wang"
                    },
                    {
                        "name": "John C. S. Lui"
                    },
                    {
                        "name": "Wei Chen"
                    },
                    {
                        "name": "Carlee Joe-Wong"
                    }
                ],
                "author_detail": {
                    "name": "Carlee Joe-Wong"
                },
                "author": "Carlee Joe-Wong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.07675v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.07675v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08134v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08134v2",
                "updated": "2025-08-12T02:27:05Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    2,
                    27,
                    5,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-11T16:10:00Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    16,
                    10,
                    0,
                    0,
                    223,
                    0
                ],
                "title": "Follow-Your-Shape: Shape-Aware Image Editing via Trajectory-Guided\n  Region Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Follow-Your-Shape: Shape-Aware Image Editing via Trajectory-Guided\n  Region Control"
                },
                "summary": "While recent flow-based image editing models demonstrate general-purpose\ncapabilities across diverse tasks, they often struggle to specialize in\nchallenging scenarios -- particularly those involving large-scale shape\ntransformations. When performing such structural edits, these methods either\nfail to achieve the intended shape change or inadvertently alter non-target\nregions, resulting in degraded background quality. We propose\nFollow-Your-Shape, a training-free and mask-free framework that supports\nprecise and controllable editing of object shapes while strictly preserving\nnon-target content. Motivated by the divergence between inversion and editing\ntrajectories, we compute a Trajectory Divergence Map (TDM) by comparing\ntoken-wise velocity differences between the inversion and denoising paths. The\nTDM enables precise localization of editable regions and guides a Scheduled KV\nInjection mechanism that ensures stable and faithful editing. To facilitate a\nrigorous evaluation, we introduce ReShapeBench, a new benchmark comprising 120\nnew images and enriched prompt pairs specifically curated for shape-aware\nediting. Experiments demonstrate that our method achieves superior editability\nand visual fidelity, particularly in tasks requiring large-scale shape\nreplacement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While recent flow-based image editing models demonstrate general-purpose\ncapabilities across diverse tasks, they often struggle to specialize in\nchallenging scenarios -- particularly those involving large-scale shape\ntransformations. When performing such structural edits, these methods either\nfail to achieve the intended shape change or inadvertently alter non-target\nregions, resulting in degraded background quality. We propose\nFollow-Your-Shape, a training-free and mask-free framework that supports\nprecise and controllable editing of object shapes while strictly preserving\nnon-target content. Motivated by the divergence between inversion and editing\ntrajectories, we compute a Trajectory Divergence Map (TDM) by comparing\ntoken-wise velocity differences between the inversion and denoising paths. The\nTDM enables precise localization of editable regions and guides a Scheduled KV\nInjection mechanism that ensures stable and faithful editing. To facilitate a\nrigorous evaluation, we introduce ReShapeBench, a new benchmark comprising 120\nnew images and enriched prompt pairs specifically curated for shape-aware\nediting. Experiments demonstrate that our method achieves superior editability\nand visual fidelity, particularly in tasks requiring large-scale shape\nreplacement."
                },
                "authors": [
                    {
                        "name": "Zeqian Long"
                    },
                    {
                        "name": "Mingzhe Zheng"
                    },
                    {
                        "name": "Kunyu Feng"
                    },
                    {
                        "name": "Xinhua Zhang"
                    },
                    {
                        "name": "Hongyu Liu"
                    },
                    {
                        "name": "Harry Yang"
                    },
                    {
                        "name": "Linfeng Zhang"
                    },
                    {
                        "name": "Qifeng Chen"
                    },
                    {
                        "name": "Yue Ma"
                    }
                ],
                "author_detail": {
                    "name": "Yue Ma"
                },
                "author": "Yue Ma",
                "arxiv_comment": "Project webpage is available at https://follow-your-shape.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08134v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08134v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08531v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08531v1",
                "updated": "2025-08-12T00:06:34Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    0,
                    6,
                    34,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T00:06:34Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    0,
                    6,
                    34,
                    1,
                    224,
                    0
                ],
                "title": "Profiling Large Language Model Inference on Apple Silicon: A\n  Quantization Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Profiling Large Language Model Inference on Apple Silicon: A\n  Quantization Perspective"
                },
                "summary": "A systematic understanding of Apple Silicon is lacking in the current\nlandscape of hardware efficiency; research focus is largely centered on\naccelerating GPUs for large-scale training or inference on CUDA devices. This\npaper investigates Apple Silicon's unique memory architecture that offers a\nunified memory integrating CPU and GPU memory and its implications for\non-device LLM inference.\n  We decipher myths about whether Apple Silicon is efficient for on-device\ninference compared to competitors such as NVIDIA GPUs by directly conducting\nlatency and throughput comparison benchmarks. We explain the performance gap\nbetween them through profiling low level hardware metrics - ALU utilization,\nmemory bandwidth, buffer usage, cache residency etc. at runtime. We draw\nseveral insights regarding performance bottlenecks such as dequantization\noverhead, compute throughput and memory bandwidth. We debunk existing false\nclaims regarding large language model inference such as compressing models to\nlower bit precision is a defacto promise for faster inference across all\nhardware platforms. We find that the large unified memory enables Apple Silicon\nto be both cost effective and efficient against NVIDIA GPUs for ultra large\nlanguage models.\n  Our large scale evaluation on 5 hardware testbeds incorporating three Apple\nM-series devices: M2 Ultra, M2 Max and M4 Pro and two NVIDIA GPUs: NVIDIA RTX\nA6000, a multi GPU setup with 2xNVIDIA RTX A6000, 5 model scales ranging from\n8B to 405B parameters and 14 quantization schemes gives an understanding of how\nApple Silicon fits within the paradigm of on-device LLM inference. Our analysis\nreveals multiple resource interdependencies and unexpected findings, while also\nquantifying established insights. To the best of our knowledge, this study\nmakes the first attempt to present a thorough characterization and analysis of\nApple Silicon for on-device inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A systematic understanding of Apple Silicon is lacking in the current\nlandscape of hardware efficiency; research focus is largely centered on\naccelerating GPUs for large-scale training or inference on CUDA devices. This\npaper investigates Apple Silicon's unique memory architecture that offers a\nunified memory integrating CPU and GPU memory and its implications for\non-device LLM inference.\n  We decipher myths about whether Apple Silicon is efficient for on-device\ninference compared to competitors such as NVIDIA GPUs by directly conducting\nlatency and throughput comparison benchmarks. We explain the performance gap\nbetween them through profiling low level hardware metrics - ALU utilization,\nmemory bandwidth, buffer usage, cache residency etc. at runtime. We draw\nseveral insights regarding performance bottlenecks such as dequantization\noverhead, compute throughput and memory bandwidth. We debunk existing false\nclaims regarding large language model inference such as compressing models to\nlower bit precision is a defacto promise for faster inference across all\nhardware platforms. We find that the large unified memory enables Apple Silicon\nto be both cost effective and efficient against NVIDIA GPUs for ultra large\nlanguage models.\n  Our large scale evaluation on 5 hardware testbeds incorporating three Apple\nM-series devices: M2 Ultra, M2 Max and M4 Pro and two NVIDIA GPUs: NVIDIA RTX\nA6000, a multi GPU setup with 2xNVIDIA RTX A6000, 5 model scales ranging from\n8B to 405B parameters and 14 quantization schemes gives an understanding of how\nApple Silicon fits within the paradigm of on-device LLM inference. Our analysis\nreveals multiple resource interdependencies and unexpected findings, while also\nquantifying established insights. To the best of our knowledge, this study\nmakes the first attempt to present a thorough characterization and analysis of\nApple Silicon for on-device inference."
                },
                "authors": [
                    {
                        "name": "Afsara Benazir"
                    },
                    {
                        "name": "Felix Xiaozhu Lin"
                    }
                ],
                "author_detail": {
                    "name": "Felix Xiaozhu Lin"
                },
                "author": "Felix Xiaozhu Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08531v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08531v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08457v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08457v1",
                "updated": "2025-08-11T20:30:31Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    20,
                    30,
                    31,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T20:30:31Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    20,
                    30,
                    31,
                    0,
                    223,
                    0
                ],
                "title": "Architecting Long-Context LLM Acceleration with Packing-Prefetch\n  Scheduler and Ultra-Large Capacity On-Chip Memories",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Architecting Long-Context LLM Acceleration with Packing-Prefetch\n  Scheduler and Ultra-Large Capacity On-Chip Memories"
                },
                "summary": "Long-context Large Language Model (LLM) inference faces increasing compute\nbottlenecks as attention calculations scale with context length, primarily due\nto the growing KV-cache transfer overhead that saturates High Bandwidth Memory\n(HBM). While prefetching techniques mitigate cache misses by fetching KV data\nin advance, their spatial and temporal benefits present new opportunities to\nexploit. This work proposes a packing-prefetch scheduling architecture with\nmonolithic 3D (M3D) back-end-of-line (BEOL) compatible embedded memories with\nultra-large on-chip capacity to accelerate long-context LLM inference. Our\noptimizations demonstrate 8.06x decode speedup and 1.83x overall latency\nreduction on Llama3.1-8B using TPUv6e-like hardware with additional 512MB BEOL\nmemories over the serial execution. Evaluations of multi-request workloads on\nTPU-like architectures show 1.7x-2.4x throughput improvement and 1.5x-2.4x HBM\nbandwidth reduction compared to packing-only methods on Llama3.1-8B and\nLlama3.1-70B models. With the co-design of packing, prefetching, and BEOL\nmemories, our approach alleviates HBM constraints and enables efficient\nlong-context LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context Large Language Model (LLM) inference faces increasing compute\nbottlenecks as attention calculations scale with context length, primarily due\nto the growing KV-cache transfer overhead that saturates High Bandwidth Memory\n(HBM). While prefetching techniques mitigate cache misses by fetching KV data\nin advance, their spatial and temporal benefits present new opportunities to\nexploit. This work proposes a packing-prefetch scheduling architecture with\nmonolithic 3D (M3D) back-end-of-line (BEOL) compatible embedded memories with\nultra-large on-chip capacity to accelerate long-context LLM inference. Our\noptimizations demonstrate 8.06x decode speedup and 1.83x overall latency\nreduction on Llama3.1-8B using TPUv6e-like hardware with additional 512MB BEOL\nmemories over the serial execution. Evaluations of multi-request workloads on\nTPU-like architectures show 1.7x-2.4x throughput improvement and 1.5x-2.4x HBM\nbandwidth reduction compared to packing-only methods on Llama3.1-8B and\nLlama3.1-70B models. With the co-design of packing, prefetching, and BEOL\nmemories, our approach alleviates HBM constraints and enables efficient\nlong-context LLM inference."
                },
                "authors": [
                    {
                        "name": "Ming-Yen Lee"
                    },
                    {
                        "name": "Faaiq Waqar"
                    },
                    {
                        "name": "Hanchen Yang"
                    },
                    {
                        "name": "Muhammed Ahosan Ul Karim"
                    },
                    {
                        "name": "Harsono Simka"
                    },
                    {
                        "name": "Shimeng Yu"
                    }
                ],
                "author_detail": {
                    "name": "Shimeng Yu"
                },
                "author": "Shimeng Yu",
                "arxiv_comment": "7 pages, 8 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08457v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08457v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.1.3; B.3.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08438v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08438v1",
                "updated": "2025-08-11T19:55:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    19,
                    55,
                    44,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T19:55:44Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    19,
                    55,
                    44,
                    0,
                    223,
                    0
                ],
                "title": "Selective KV-Cache Sharing to Mitigate Timing Side-Channels in LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Selective KV-Cache Sharing to Mitigate Timing Side-Channels in LLM\n  Inference"
                },
                "summary": "Global KV-cache sharing has emerged as a key optimization for accelerating\nlarge language model (LLM) inference. However, it exposes a new class of timing\nside-channel attacks, enabling adversaries to infer sensitive user inputs via\nshared cache entries. Existing defenses, such as per-user isolation, eliminate\nleakage but degrade performance by up to 38.9% in time-to-first-token (TTFT),\nmaking them impractical for high-throughput deployment. To address this gap, we\nintroduce SafeKV (Secure and Flexible KV Cache Sharing), a privacy-aware\nKV-cache management framework that selectively shares non-sensitive entries\nwhile confining sensitive content to private caches. SafeKV comprises three\ncomponents: (i) a hybrid, multi-tier detection pipeline that integrates\nrule-based pattern matching, a general-purpose privacy detector, and\ncontext-aware validation; (ii) a unified radix-tree index that manages public\nand private entries across heterogeneous memory tiers (HBM, DRAM, SSD); and\n(iii) entropy-based access monitoring to detect and mitigate residual\ninformation leakage. Our evaluation shows that SafeKV mitigates 94% - 97% of\ntiming-based side-channel attacks. Compared to per-user isolation method,\nSafeKV improves TTFT by up to 40.58% and throughput by up to 2.66X across\ndiverse LLMs and workloads. SafeKV reduces cache-induced TTFT overhead from\n50.41% to 11.74% on Qwen3-235B. By combining fine-grained privacy control with\nhigh cache reuse efficiency, SafeKV reclaims the performance advantages of\nglobal sharing while providing robust runtime privacy guarantees for LLM\ninference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Global KV-cache sharing has emerged as a key optimization for accelerating\nlarge language model (LLM) inference. However, it exposes a new class of timing\nside-channel attacks, enabling adversaries to infer sensitive user inputs via\nshared cache entries. Existing defenses, such as per-user isolation, eliminate\nleakage but degrade performance by up to 38.9% in time-to-first-token (TTFT),\nmaking them impractical for high-throughput deployment. To address this gap, we\nintroduce SafeKV (Secure and Flexible KV Cache Sharing), a privacy-aware\nKV-cache management framework that selectively shares non-sensitive entries\nwhile confining sensitive content to private caches. SafeKV comprises three\ncomponents: (i) a hybrid, multi-tier detection pipeline that integrates\nrule-based pattern matching, a general-purpose privacy detector, and\ncontext-aware validation; (ii) a unified radix-tree index that manages public\nand private entries across heterogeneous memory tiers (HBM, DRAM, SSD); and\n(iii) entropy-based access monitoring to detect and mitigate residual\ninformation leakage. Our evaluation shows that SafeKV mitigates 94% - 97% of\ntiming-based side-channel attacks. Compared to per-user isolation method,\nSafeKV improves TTFT by up to 40.58% and throughput by up to 2.66X across\ndiverse LLMs and workloads. SafeKV reduces cache-induced TTFT overhead from\n50.41% to 11.74% on Qwen3-235B. By combining fine-grained privacy control with\nhigh cache reuse efficiency, SafeKV reclaims the performance advantages of\nglobal sharing while providing robust runtime privacy guarantees for LLM\ninference."
                },
                "authors": [
                    {
                        "name": "Kexin Chu"
                    },
                    {
                        "name": "Zecheng Lin"
                    },
                    {
                        "name": "Dawei Xiang"
                    },
                    {
                        "name": "Zixu Shen"
                    },
                    {
                        "name": "Jianchang Su"
                    },
                    {
                        "name": "Cheng Chu"
                    },
                    {
                        "name": "Yiwei Yang"
                    },
                    {
                        "name": "Wenhui Zhang"
                    },
                    {
                        "name": "Wenfei Wu"
                    },
                    {
                        "name": "Wei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Zhang"
                },
                "author": "Wei Zhang",
                "arxiv_comment": "17 pages,17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08438v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08438v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08081v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08081v1",
                "updated": "2025-08-11T15:28:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    15,
                    28,
                    28,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T15:28:28Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    15,
                    28,
                    28,
                    0,
                    223,
                    0
                ],
                "title": "Numerical computation of linearized KV and the Deligne-Drinfeld and\n  Broadhurst-Kreimer conjectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Numerical computation of linearized KV and the Deligne-Drinfeld and\n  Broadhurst-Kreimer conjectures"
                },
                "summary": "We compute numerically the dimensions of the graded quotients of the\nlinearized Kashiwara-Vergne Lie algebra lkv in low weight, confirming a\nconjecture of Raphael-Schneps in those weights. The Lie algebra lkv appears in\na chain of inclusions of Lie algebras, including also the linearized double\nshuffle Lie algebra and the (depth associated graded of the)\nGrothendieck-Teichm\\\"uller Lie algebra. Hence our computations also allow us to\ncheck the validity of the Deligne-Drinfeld conjecture on the structure of the\nGrothendieck-Teichm\\\"uller group up to weight 29, and (a version of) the the\nBroadhurst-Kreimer conjecture on the number of multiple zeta values for a range\nof weight-depth pairs significantly exceeding the previous bounds. Our\ncomputations also verify a conjecture by Alekseev-Torossian on the\nKashiwara-Vergne Lie algebra up to weight 29.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We compute numerically the dimensions of the graded quotients of the\nlinearized Kashiwara-Vergne Lie algebra lkv in low weight, confirming a\nconjecture of Raphael-Schneps in those weights. The Lie algebra lkv appears in\na chain of inclusions of Lie algebras, including also the linearized double\nshuffle Lie algebra and the (depth associated graded of the)\nGrothendieck-Teichm\\\"uller Lie algebra. Hence our computations also allow us to\ncheck the validity of the Deligne-Drinfeld conjecture on the structure of the\nGrothendieck-Teichm\\\"uller group up to weight 29, and (a version of) the the\nBroadhurst-Kreimer conjecture on the number of multiple zeta values for a range\nof weight-depth pairs significantly exceeding the previous bounds. Our\ncomputations also verify a conjecture by Alekseev-Torossian on the\nKashiwara-Vergne Lie algebra up to weight 29."
                },
                "authors": [
                    {
                        "name": "Florian Naef"
                    },
                    {
                        "name": "Thomas Willwacher"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Willwacher"
                },
                "author": "Thomas Willwacher",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08081v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08081v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.QA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.QA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06923v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06923v2",
                "updated": "2025-08-11T14:15:27Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    14,
                    15,
                    27,
                    0,
                    223,
                    0
                ],
                "published": "2025-03-10T05:09:42Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    5,
                    9,
                    42,
                    0,
                    69,
                    0
                ],
                "title": "From Reusing to Forecasting: Accelerating Diffusion Models with\n  TaylorSeers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Reusing to Forecasting: Accelerating Diffusion Models with\n  TaylorSeers"
                },
                "summary": "Diffusion Transformers (DiT) have revolutionized high-fidelity image and\nvideo synthesis, yet their computational demands remain prohibitive for\nreal-time applications. To solve this problem, feature caching has been\nproposed to accelerate diffusion models by caching the features in the previous\ntimesteps and then reusing them in the following timesteps. However, at\ntimesteps with significant intervals, the feature similarity in diffusion\nmodels decreases substantially, leading to a pronounced increase in errors\nintroduced by feature caching, significantly harming the generation quality. To\nsolve this problem, we propose TaylorSeer, which firstly shows that features of\ndiffusion models at future timesteps can be predicted based on their values at\nprevious timesteps. Based on the fact that features change slowly and\ncontinuously across timesteps, TaylorSeer employs a differential method to\napproximate the higher-order derivatives of features and predict features in\nfuture timesteps with Taylor series expansion. Extensive experiments\ndemonstrate its significant effectiveness in both image and video synthesis,\nespecially in high acceleration ratios. For instance, it achieves an almost\nlossless acceleration of 4.99$\\times$ on FLUX and 5.00$\\times$ on HunyuanVideo\nwithout additional training. On DiT, it achieves $3.41$ lower FID compared with\nprevious SOTA at $4.53$$\\times$ acceleration. %Our code is provided in the\nsupplementary materials and will be made publicly available on GitHub. Our\ncodes have been released in Github:https://github.com/Shenyi-Z/TaylorSeer",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT) have revolutionized high-fidelity image and\nvideo synthesis, yet their computational demands remain prohibitive for\nreal-time applications. To solve this problem, feature caching has been\nproposed to accelerate diffusion models by caching the features in the previous\ntimesteps and then reusing them in the following timesteps. However, at\ntimesteps with significant intervals, the feature similarity in diffusion\nmodels decreases substantially, leading to a pronounced increase in errors\nintroduced by feature caching, significantly harming the generation quality. To\nsolve this problem, we propose TaylorSeer, which firstly shows that features of\ndiffusion models at future timesteps can be predicted based on their values at\nprevious timesteps. Based on the fact that features change slowly and\ncontinuously across timesteps, TaylorSeer employs a differential method to\napproximate the higher-order derivatives of features and predict features in\nfuture timesteps with Taylor series expansion. Extensive experiments\ndemonstrate its significant effectiveness in both image and video synthesis,\nespecially in high acceleration ratios. For instance, it achieves an almost\nlossless acceleration of 4.99$\\times$ on FLUX and 5.00$\\times$ on HunyuanVideo\nwithout additional training. On DiT, it achieves $3.41$ lower FID compared with\nprevious SOTA at $4.53$$\\times$ acceleration. %Our code is provided in the\nsupplementary materials and will be made publicly available on GitHub. Our\ncodes have been released in Github:https://github.com/Shenyi-Z/TaylorSeer"
                },
                "authors": [
                    {
                        "name": "Jiacheng Liu"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Yuanhuiyi Lyu"
                    },
                    {
                        "name": "Junjie Chen"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "15 pages, 14 figures; Accepted by ICCV2025; Mainly focus on feature\n  caching for diffusion transformers acceleration",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06923v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06923v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08343v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08343v1",
                "updated": "2025-08-11T10:47:35Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    10,
                    47,
                    35,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T10:47:35Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    10,
                    47,
                    35,
                    0,
                    223,
                    0
                ],
                "title": "Maximizing GPU Efficiency via Optimal Adapter Caching: An Analytical\n  Approach for Multi-Tenant LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Maximizing GPU Efficiency via Optimal Adapter Caching: An Analytical\n  Approach for Multi-Tenant LLM Serving"
                },
                "summary": "Serving LLM adapters has gained significant attention as an effective\napproach to adapt general-purpose language models to diverse, task-specific use\ncases. However, serving a wide range of adapters introduces several and\nsubstantial overheads, leading to performance degradation and challenges in\noptimal placement. To address these challenges, we present an analytical,\nAI-driven pipeline that accurately determines the optimal allocation of\nadapters in single-node setups. This allocation maximizes performance,\neffectively using GPU resources, while preventing request starvation.\nCrucially, the proposed allocation is given based on current workload patterns.\nThese insights in single-node setups can be leveraged in multi-replica\ndeployments for overall placement, load balancing and server configuration,\nultimately enhancing overall performance and improving resource efficiency. Our\napproach builds on an in-depth analysis of LLM adapter serving, accounting for\noverheads and performance variability, and includes the development of the\nfirst Digital Twin capable of replicating online LLM-adapter serving systems\nwith matching key performance metrics. The experimental results demonstrate\nthat the Digital Twin achieves a SMAPE difference of no more than 5.5% in\nthroughput compared to real results, and the proposed pipeline accurately\npredicts the optimal placement with minimal latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving LLM adapters has gained significant attention as an effective\napproach to adapt general-purpose language models to diverse, task-specific use\ncases. However, serving a wide range of adapters introduces several and\nsubstantial overheads, leading to performance degradation and challenges in\noptimal placement. To address these challenges, we present an analytical,\nAI-driven pipeline that accurately determines the optimal allocation of\nadapters in single-node setups. This allocation maximizes performance,\neffectively using GPU resources, while preventing request starvation.\nCrucially, the proposed allocation is given based on current workload patterns.\nThese insights in single-node setups can be leveraged in multi-replica\ndeployments for overall placement, load balancing and server configuration,\nultimately enhancing overall performance and improving resource efficiency. Our\napproach builds on an in-depth analysis of LLM adapter serving, accounting for\noverheads and performance variability, and includes the development of the\nfirst Digital Twin capable of replicating online LLM-adapter serving systems\nwith matching key performance metrics. The experimental results demonstrate\nthat the Digital Twin achieves a SMAPE difference of no more than 5.5% in\nthroughput compared to real results, and the proposed pipeline accurately\npredicts the optimal placement with minimal latency."
                },
                "authors": [
                    {
                        "name": "Ferran Agullo"
                    },
                    {
                        "name": "Joan Oliveras"
                    },
                    {
                        "name": "Chen Wang"
                    },
                    {
                        "name": "Alberto Gutierrez-Torre"
                    },
                    {
                        "name": "Olivier Tardieu"
                    },
                    {
                        "name": "Alaa Youssef"
                    },
                    {
                        "name": "Jordi Torres"
                    },
                    {
                        "name": "Josep Ll. Berral"
                    }
                ],
                "author_detail": {
                    "name": "Josep Ll. Berral"
                },
                "author": "Josep Ll. Berral",
                "arxiv_comment": "Under review for a computer science conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08343v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08343v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.07811v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.07811v1",
                "updated": "2025-08-11T09:54:45Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    9,
                    54,
                    45,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T09:54:45Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    9,
                    54,
                    45,
                    0,
                    223,
                    0
                ],
                "title": "DiTVR: Zero-Shot Diffusion Transformer for Video Restoration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiTVR: Zero-Shot Diffusion Transformer for Video Restoration"
                },
                "summary": "Video restoration aims to reconstruct high quality video sequences from low\nquality inputs, addressing tasks such as super resolution, denoising, and\ndeblurring. Traditional regression based methods often produce unrealistic\ndetails and require extensive paired datasets, while recent generative\ndiffusion models face challenges in ensuring temporal consistency. We introduce\nDiTVR, a zero shot video restoration framework that couples a diffusion\ntransformer with trajectory aware attention and a wavelet guided, flow\nconsistent sampler. Unlike prior 3D convolutional or frame wise diffusion\napproaches, our attention mechanism aligns tokens along optical flow\ntrajectories, with particular emphasis on vital layers that exhibit the highest\nsensitivity to temporal dynamics. A spatiotemporal neighbour cache dynamically\nselects relevant tokens based on motion correspondences across frames. The flow\nguided sampler injects data consistency only into low-frequency bands,\npreserving high frequency priors while accelerating convergence. DiTVR\nestablishes a new zero shot state of the art on video restoration benchmarks,\ndemonstrating superior temporal consistency and detail preservation while\nremaining robust to flow noise and occlusions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video restoration aims to reconstruct high quality video sequences from low\nquality inputs, addressing tasks such as super resolution, denoising, and\ndeblurring. Traditional regression based methods often produce unrealistic\ndetails and require extensive paired datasets, while recent generative\ndiffusion models face challenges in ensuring temporal consistency. We introduce\nDiTVR, a zero shot video restoration framework that couples a diffusion\ntransformer with trajectory aware attention and a wavelet guided, flow\nconsistent sampler. Unlike prior 3D convolutional or frame wise diffusion\napproaches, our attention mechanism aligns tokens along optical flow\ntrajectories, with particular emphasis on vital layers that exhibit the highest\nsensitivity to temporal dynamics. A spatiotemporal neighbour cache dynamically\nselects relevant tokens based on motion correspondences across frames. The flow\nguided sampler injects data consistency only into low-frequency bands,\npreserving high frequency priors while accelerating convergence. DiTVR\nestablishes a new zero shot state of the art on video restoration benchmarks,\ndemonstrating superior temporal consistency and detail preservation while\nremaining robust to flow noise and occlusions."
                },
                "authors": [
                    {
                        "name": "Sicheng Gao"
                    },
                    {
                        "name": "Nancy Mehta"
                    },
                    {
                        "name": "Zongwei Wu"
                    },
                    {
                        "name": "Radu Timofte"
                    }
                ],
                "author_detail": {
                    "name": "Radu Timofte"
                },
                "author": "Radu Timofte",
                "arxiv_comment": "7 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.07811v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.07811v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20790v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20790v2",
                "updated": "2025-08-11T08:10:21Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    8,
                    10,
                    21,
                    0,
                    223,
                    0
                ],
                "published": "2024-10-28T07:13:25Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    7,
                    13,
                    25,
                    0,
                    302,
                    0
                ],
                "title": "SparseTem: Boosting the Efficiency of CNN-Based Video Encoders by\n  Exploiting Temporal Continuity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparseTem: Boosting the Efficiency of CNN-Based Video Encoders by\n  Exploiting Temporal Continuity"
                },
                "summary": "Deep learning models have become pivotal in the field of video processing and\nis increasingly critical in practical applications such as autonomous driving\nand object detection. Although Vision Transformers (ViTs) have demonstrated\ntheir power, Convolutional Neural Networks (CNNs) remain a highly efficient and\nhigh-performance choice for feature extraction and encoding. However, the\nintensive computational demands of convolution operations hinder its broader\nadoption as a video encoder. Given the inherent temporal continuity in video\nframes, changes between consecutive frames are minimal, allowing for the\nskipping of redundant computations. This technique, which we term as Diff\nComputation, presents two primary challenges. First, Diff Computation requires\nto cache intermediate feature maps to ensure the correctness of non-linear\ncomputations, leading to significant memory consumption. Second, the imbalance\nof sparsity among layers, introduced by Diff Computation, incurs accuracy\ndegradation. To address these issues, we propose a memory-efficient scheduling\nmethod to eliminate memory overhead and an online adjustment mechanism to\nminimize accuracy degradation. We integrate these techniques into our\nframework, SparseTem, to seamlessly support various CNN-based video encoders.\nSparseTem achieves speedup of 1.79x for EfficientDet and 4.72x for CRNN, with\nminimal accuracy drop and no additional memory overhead. Extensive experimental\nresults demonstrate that SparseTem sets a new state-of-the-art by effectively\nutilizing temporal continuity to accelerate CNN-based video encoders.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning models have become pivotal in the field of video processing and\nis increasingly critical in practical applications such as autonomous driving\nand object detection. Although Vision Transformers (ViTs) have demonstrated\ntheir power, Convolutional Neural Networks (CNNs) remain a highly efficient and\nhigh-performance choice for feature extraction and encoding. However, the\nintensive computational demands of convolution operations hinder its broader\nadoption as a video encoder. Given the inherent temporal continuity in video\nframes, changes between consecutive frames are minimal, allowing for the\nskipping of redundant computations. This technique, which we term as Diff\nComputation, presents two primary challenges. First, Diff Computation requires\nto cache intermediate feature maps to ensure the correctness of non-linear\ncomputations, leading to significant memory consumption. Second, the imbalance\nof sparsity among layers, introduced by Diff Computation, incurs accuracy\ndegradation. To address these issues, we propose a memory-efficient scheduling\nmethod to eliminate memory overhead and an online adjustment mechanism to\nminimize accuracy degradation. We integrate these techniques into our\nframework, SparseTem, to seamlessly support various CNN-based video encoders.\nSparseTem achieves speedup of 1.79x for EfficientDet and 4.72x for CRNN, with\nminimal accuracy drop and no additional memory overhead. Extensive experimental\nresults demonstrate that SparseTem sets a new state-of-the-art by effectively\nutilizing temporal continuity to accelerate CNN-based video encoders."
                },
                "authors": [
                    {
                        "name": "Kunyun Wang"
                    },
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Jieru Zhao"
                    },
                    {
                        "name": "Wenchao Ding"
                    },
                    {
                        "name": "Quan Chen"
                    },
                    {
                        "name": "Jingwen Leng"
                    },
                    {
                        "name": "Minyi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Minyi Guo"
                },
                "author": "Minyi Guo",
                "arxiv_comment": "9 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20790v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20790v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14051v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14051v4",
                "updated": "2025-08-11T06:16:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    6,
                    16,
                    52,
                    0,
                    223,
                    0
                ],
                "published": "2025-04-18T19:46:54Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    19,
                    46,
                    54,
                    4,
                    108,
                    0
                ],
                "title": "CAOTE: KV Cache Eviction for LLMs via Attention Output Error-Based Token\n  Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAOTE: KV Cache Eviction for LLMs via Attention Output Error-Based Token\n  Selection"
                },
                "summary": "While long context support of large language models has extended their\nabilities, it also incurs challenges in memory and compute which becomes\ncrucial bottlenecks in resource-restricted devices. Token eviction, a widely\nadopted post-training methodology designed to alleviate the bottlenecks by\nevicting less important tokens from the cache, typically uses attention scores\nas proxy metrics for token importance. However, one major limitation of\nattention score as a token-wise importance metrics is that it lacks the\ninformation about contribution of tokens to the attention output. In this\npaper, we propose a simple eviction criterion based on the contribution of\ncached tokens to attention outputs. Our method, CAOTE, optimizes for eviction\nerror due to token eviction, by seamlessly integrating attention scores and\nvalue vectors. This is the first method which uses value vector information on\ntop of attention-based eviction scores. Additionally, CAOTE can act as a\nmeta-heuristic method with flexible usage with any token eviction method. We\nshow that CAOTE, when combined with the state-of-the-art attention score-based\nmethods, always improves accuracies on the downstream task, indicating the\nimportance of leveraging information from values during token eviction process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While long context support of large language models has extended their\nabilities, it also incurs challenges in memory and compute which becomes\ncrucial bottlenecks in resource-restricted devices. Token eviction, a widely\nadopted post-training methodology designed to alleviate the bottlenecks by\nevicting less important tokens from the cache, typically uses attention scores\nas proxy metrics for token importance. However, one major limitation of\nattention score as a token-wise importance metrics is that it lacks the\ninformation about contribution of tokens to the attention output. In this\npaper, we propose a simple eviction criterion based on the contribution of\ncached tokens to attention outputs. Our method, CAOTE, optimizes for eviction\nerror due to token eviction, by seamlessly integrating attention scores and\nvalue vectors. This is the first method which uses value vector information on\ntop of attention-based eviction scores. Additionally, CAOTE can act as a\nmeta-heuristic method with flexible usage with any token eviction method. We\nshow that CAOTE, when combined with the state-of-the-art attention score-based\nmethods, always improves accuracies on the downstream task, indicating the\nimportance of leveraging information from values during token eviction process."
                },
                "authors": [
                    {
                        "name": "Raghavv Goel"
                    },
                    {
                        "name": "Junyoung Park"
                    },
                    {
                        "name": "Mukul Gagrani"
                    },
                    {
                        "name": "Dalton Jones"
                    },
                    {
                        "name": "Matthew Morse"
                    },
                    {
                        "name": "Harper Langston"
                    },
                    {
                        "name": "Mingu Lee"
                    },
                    {
                        "name": "Chris Lott"
                    }
                ],
                "author_detail": {
                    "name": "Chris Lott"
                },
                "author": "Chris Lott",
                "arxiv_comment": "14 pages, 3 figures, 10 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14051v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14051v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.07570v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.07570v1",
                "updated": "2025-08-11T03:03:34Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    3,
                    3,
                    34,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T03:03:34Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    3,
                    3,
                    34,
                    0,
                    223,
                    0
                ],
                "title": "Adaptive Cache Enhancement for Test-Time Adaptation of Vision-Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Cache Enhancement for Test-Time Adaptation of Vision-Language\n  Models"
                },
                "summary": "Vision-language models (VLMs) exhibit remarkable zero-shot generalization but\nsuffer performance degradation under distribution shifts in downstream tasks,\nparticularly in the absence of labeled data. Test-Time Adaptation (TTA)\naddresses this challenge by enabling online optimization of VLMs during\ninference, eliminating the need for annotated data. Cache-based TTA methods\nexploit historical knowledge by maintaining a dynamic memory cache of\nlow-entropy or high-confidence samples, promoting efficient adaptation to\nout-of-distribution data. Nevertheless, these methods face two critical\nchallenges: (1) unreliable confidence metrics under significant distribution\nshifts, resulting in error accumulation within the cache and degraded\nadaptation performance; and (2) rigid decision boundaries that fail to\naccommodate substantial distributional variations, leading to suboptimal\npredictions. To overcome these limitations, we introduce the Adaptive Cache\nEnhancement (ACE) framework, which constructs a robust cache by selectively\nstoring high-confidence or low-entropy image embeddings per class, guided by\ndynamic, class-specific thresholds initialized from zero-shot statistics and\niteratively refined using an exponential moving average and\nexploration-augmented updates. This approach enables adaptive, class-wise\ndecision boundaries, ensuring robust and accurate predictions across diverse\nvisual distributions. Extensive experiments on 15 diverse benchmark datasets\ndemonstrate that ACE achieves state-of-the-art performance, delivering superior\nrobustness and generalization compared to existing TTA methods in challenging\nout-of-distribution scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language models (VLMs) exhibit remarkable zero-shot generalization but\nsuffer performance degradation under distribution shifts in downstream tasks,\nparticularly in the absence of labeled data. Test-Time Adaptation (TTA)\naddresses this challenge by enabling online optimization of VLMs during\ninference, eliminating the need for annotated data. Cache-based TTA methods\nexploit historical knowledge by maintaining a dynamic memory cache of\nlow-entropy or high-confidence samples, promoting efficient adaptation to\nout-of-distribution data. Nevertheless, these methods face two critical\nchallenges: (1) unreliable confidence metrics under significant distribution\nshifts, resulting in error accumulation within the cache and degraded\nadaptation performance; and (2) rigid decision boundaries that fail to\naccommodate substantial distributional variations, leading to suboptimal\npredictions. To overcome these limitations, we introduce the Adaptive Cache\nEnhancement (ACE) framework, which constructs a robust cache by selectively\nstoring high-confidence or low-entropy image embeddings per class, guided by\ndynamic, class-specific thresholds initialized from zero-shot statistics and\niteratively refined using an exponential moving average and\nexploration-augmented updates. This approach enables adaptive, class-wise\ndecision boundaries, ensuring robust and accurate predictions across diverse\nvisual distributions. Extensive experiments on 15 diverse benchmark datasets\ndemonstrate that ACE achieves state-of-the-art performance, delivering superior\nrobustness and generalization compared to existing TTA methods in challenging\nout-of-distribution scenarios."
                },
                "authors": [
                    {
                        "name": "Khanh-Binh Nguyen"
                    },
                    {
                        "name": "Phuoc-Nguyen Bui"
                    },
                    {
                        "name": "Hyunseung Choo"
                    },
                    {
                        "name": "Duc Thanh Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Duc Thanh Nguyen"
                },
                "author": "Duc Thanh Nguyen",
                "arxiv_comment": "12 pages, Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.07570v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.07570v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09208v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09208v1",
                "updated": "2025-08-10T14:05:36Z",
                "updated_parsed": [
                    2025,
                    8,
                    10,
                    14,
                    5,
                    36,
                    6,
                    222,
                    0
                ],
                "published": "2025-08-10T14:05:36Z",
                "published_parsed": [
                    2025,
                    8,
                    10,
                    14,
                    5,
                    36,
                    6,
                    222,
                    0
                ],
                "title": "CoMoE: Collaborative Optimization of Expert Aggregation and Offloading\n  for MoE-based LLMs at Edge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoMoE: Collaborative Optimization of Expert Aggregation and Offloading\n  for MoE-based LLMs at Edge"
                },
                "summary": "The proliferation of large language models (LLMs) has driven the adoption of\nMixture-of-Experts (MoE) architectures as a promising solution to scale model\ncapacity while controlling computational costs. However, deploying MoE models\nin resource-constrained mobile edge computing environments presents significant\nchallenges due to their large memory footprint and dynamic expert activation\npatterns. To address these challenges, we propose a novel dynamic\nresource-aware collaborative optimization framework that jointly optimizes\nexpert aggregation granularity and offloading strategies based on real-time\ndevice resource states, network conditions, and input characteristics in mobile\nedge environments, denoted as CoMoE. In CoMoE, we first systematically analyze\nexisting expert aggregation techniques, including expert parameter\nmerging,knowledge distillation,and parameter sharing decomposition, identifying\ntheir limitations in dynamic mobile environments.We then investigate expert\noffloading strategies encompassing expert prediction and prefetching, expert\ncaching and scheduling, and multi-tier storage architectures, revealing the\ninterdependencies between routing decisions and offloading performance.The\nCoMoE incorporates adaptive scheduling mechanisms that respond to user mobility\nand varying network conditions, enabling efficient MoE deployment across\nheterogeneous edge devices. Extensive experiments on real mobile edge testbeds\ndemonstrate that CoMoE achieves approximately 70% reduction in memory usage\ncompared to baseline methods, 10.5% lower inference latency than existing\nexpert offloading techniques, while maintaining model performance stability.\nFor large-scale MoE models (e.g,7.4B-parameter Switch-Base-128), the CoMoE\nreduces memory requirements from 15.6GB to 4.7GB, enabling deployment on\nresource-constrained mobile edge devices that previously could only support\nmuch smaller models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of large language models (LLMs) has driven the adoption of\nMixture-of-Experts (MoE) architectures as a promising solution to scale model\ncapacity while controlling computational costs. However, deploying MoE models\nin resource-constrained mobile edge computing environments presents significant\nchallenges due to their large memory footprint and dynamic expert activation\npatterns. To address these challenges, we propose a novel dynamic\nresource-aware collaborative optimization framework that jointly optimizes\nexpert aggregation granularity and offloading strategies based on real-time\ndevice resource states, network conditions, and input characteristics in mobile\nedge environments, denoted as CoMoE. In CoMoE, we first systematically analyze\nexisting expert aggregation techniques, including expert parameter\nmerging,knowledge distillation,and parameter sharing decomposition, identifying\ntheir limitations in dynamic mobile environments.We then investigate expert\noffloading strategies encompassing expert prediction and prefetching, expert\ncaching and scheduling, and multi-tier storage architectures, revealing the\ninterdependencies between routing decisions and offloading performance.The\nCoMoE incorporates adaptive scheduling mechanisms that respond to user mobility\nand varying network conditions, enabling efficient MoE deployment across\nheterogeneous edge devices. Extensive experiments on real mobile edge testbeds\ndemonstrate that CoMoE achieves approximately 70% reduction in memory usage\ncompared to baseline methods, 10.5% lower inference latency than existing\nexpert offloading techniques, while maintaining model performance stability.\nFor large-scale MoE models (e.g,7.4B-parameter Switch-Base-128), the CoMoE\nreduces memory requirements from 15.6GB to 4.7GB, enabling deployment on\nresource-constrained mobile edge devices that previously could only support\nmuch smaller models."
                },
                "authors": [
                    {
                        "name": "Muqing Li"
                    },
                    {
                        "name": "Ning Li"
                    },
                    {
                        "name": "Xin Yuan"
                    },
                    {
                        "name": "Wenchao Xu"
                    },
                    {
                        "name": "Quan Chen"
                    },
                    {
                        "name": "Song Guo"
                    },
                    {
                        "name": "Haijun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Haijun Zhang"
                },
                "author": "Haijun Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09208v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09208v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.14769v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.14769v2",
                "updated": "2025-08-09T11:31:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    9,
                    11,
                    31,
                    44,
                    5,
                    221,
                    0
                ],
                "published": "2025-06-17T17:59:12Z",
                "published_parsed": [
                    2025,
                    6,
                    17,
                    17,
                    59,
                    12,
                    1,
                    168,
                    0
                ],
                "title": "CDP: Towards Robust Autoregressive Visuomotor Policy Learning via Causal\n  Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CDP: Towards Robust Autoregressive Visuomotor Policy Learning via Causal\n  Diffusion"
                },
                "summary": "Diffusion Policy (DP) enables robots to learn complex behaviors by imitating\nexpert demonstrations through action diffusion. However, in practical\napplications, hardware limitations often degrade data quality, while real-time\nconstraints restrict model inference to instantaneous state and scene\nobservations. These limitations seriously reduce the efficacy of learning from\nexpert demonstrations, resulting in failures in object localization, grasp\nplanning, and long-horizon task execution. To address these challenges, we\npropose Causal Diffusion Policy (CDP), a novel transformer-based diffusion\nmodel that enhances action prediction by conditioning on historical action\nsequences, thereby enabling more coherent and context-aware visuomotor policy\nlearning. To further mitigate the computational cost associated with\nautoregressive inference, a caching mechanism is also introduced to store\nattention key-value pairs from previous timesteps, substantially reducing\nredundant computations during execution. Extensive experiments in both\nsimulated and real-world environments, spanning diverse 2D and 3D manipulation\ntasks, demonstrate that CDP uniquely leverages historical action sequences to\nachieve significantly higher accuracy than existing methods. Moreover, even\nwhen faced with degraded input observation quality, CDP maintains remarkable\nprecision by reasoning through temporal continuity, which highlights its\npractical robustness for robotic control under realistic, imperfect conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Policy (DP) enables robots to learn complex behaviors by imitating\nexpert demonstrations through action diffusion. However, in practical\napplications, hardware limitations often degrade data quality, while real-time\nconstraints restrict model inference to instantaneous state and scene\nobservations. These limitations seriously reduce the efficacy of learning from\nexpert demonstrations, resulting in failures in object localization, grasp\nplanning, and long-horizon task execution. To address these challenges, we\npropose Causal Diffusion Policy (CDP), a novel transformer-based diffusion\nmodel that enhances action prediction by conditioning on historical action\nsequences, thereby enabling more coherent and context-aware visuomotor policy\nlearning. To further mitigate the computational cost associated with\nautoregressive inference, a caching mechanism is also introduced to store\nattention key-value pairs from previous timesteps, substantially reducing\nredundant computations during execution. Extensive experiments in both\nsimulated and real-world environments, spanning diverse 2D and 3D manipulation\ntasks, demonstrate that CDP uniquely leverages historical action sequences to\nachieve significantly higher accuracy than existing methods. Moreover, even\nwhen faced with degraded input observation quality, CDP maintains remarkable\nprecision by reasoning through temporal continuity, which highlights its\npractical robustness for robotic control under realistic, imperfect conditions."
                },
                "authors": [
                    {
                        "name": "Jiahua Ma"
                    },
                    {
                        "name": "Yiran Qin"
                    },
                    {
                        "name": "Yixiong Li"
                    },
                    {
                        "name": "Xuanqi Liao"
                    },
                    {
                        "name": "Yulan Guo"
                    },
                    {
                        "name": "Ruimao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ruimao Zhang"
                },
                "author": "Ruimao Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.14769v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.14769v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06937v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06937v1",
                "updated": "2025-08-09T11:06:58Z",
                "updated_parsed": [
                    2025,
                    8,
                    9,
                    11,
                    6,
                    58,
                    5,
                    221,
                    0
                ],
                "published": "2025-08-09T11:06:58Z",
                "published_parsed": [
                    2025,
                    8,
                    9,
                    11,
                    6,
                    58,
                    5,
                    221,
                    0
                ],
                "title": "CannyEdit: Selective Canny Control and Dual-Prompt Guidance for\n  Training-Free Image Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CannyEdit: Selective Canny Control and Dual-Prompt Guidance for\n  Training-Free Image Editing"
                },
                "summary": "Recent advances in text-to-image (T2I) models have enabled training-free\nregional image editing by leveraging the generative priors of foundation\nmodels. However, existing methods struggle to balance text adherence in edited\nregions, context fidelity in unedited areas, and seamless integration of edits.\nWe introduce CannyEdit, a novel training-free framework that addresses these\nchallenges through two key innovations: (1) Selective Canny Control, which\nmasks the structural guidance of Canny ControlNet in user-specified editable\nregions while strictly preserving details of the source images in unedited\nareas via inversion-phase ControlNet information retention. This enables\nprecise, text-driven edits without compromising contextual integrity. (2)\nDual-Prompt Guidance, which combines local prompts for object-specific edits\nwith a global target prompt to maintain coherent scene interactions. On\nreal-world image editing tasks (addition, replacement, removal), CannyEdit\noutperforms prior methods like KV-Edit, achieving a 2.93 to 10.49 percent\nimprovement in the balance of text adherence and context fidelity. In terms of\nediting seamlessness, user studies reveal only 49.2 percent of general users\nand 42.0 percent of AIGC experts identified CannyEdit's results as AI-edited\nwhen paired with real images without edits, versus 76.08 to 89.09 percent for\ncompetitor methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in text-to-image (T2I) models have enabled training-free\nregional image editing by leveraging the generative priors of foundation\nmodels. However, existing methods struggle to balance text adherence in edited\nregions, context fidelity in unedited areas, and seamless integration of edits.\nWe introduce CannyEdit, a novel training-free framework that addresses these\nchallenges through two key innovations: (1) Selective Canny Control, which\nmasks the structural guidance of Canny ControlNet in user-specified editable\nregions while strictly preserving details of the source images in unedited\nareas via inversion-phase ControlNet information retention. This enables\nprecise, text-driven edits without compromising contextual integrity. (2)\nDual-Prompt Guidance, which combines local prompts for object-specific edits\nwith a global target prompt to maintain coherent scene interactions. On\nreal-world image editing tasks (addition, replacement, removal), CannyEdit\noutperforms prior methods like KV-Edit, achieving a 2.93 to 10.49 percent\nimprovement in the balance of text adherence and context fidelity. In terms of\nediting seamlessness, user studies reveal only 49.2 percent of general users\nand 42.0 percent of AIGC experts identified CannyEdit's results as AI-edited\nwhen paired with real images without edits, versus 76.08 to 89.09 percent for\ncompetitor methods."
                },
                "authors": [
                    {
                        "name": "Weiyan Xie"
                    },
                    {
                        "name": "Han Gao"
                    },
                    {
                        "name": "Didan Deng"
                    },
                    {
                        "name": "Kaican Li"
                    },
                    {
                        "name": "April Hua Liu"
                    },
                    {
                        "name": "Yongxiang Huang"
                    },
                    {
                        "name": "Nevin L. Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Nevin L. Zhang"
                },
                "author": "Nevin L. Zhang",
                "arxiv_comment": "Project Page: vaynexie.github.io/CannyEdit/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06937v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06937v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.03974v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.03974v2",
                "updated": "2025-08-09T02:33:21Z",
                "updated_parsed": [
                    2025,
                    8,
                    9,
                    2,
                    33,
                    21,
                    5,
                    221,
                    0
                ],
                "published": "2025-08-05T23:47:34Z",
                "published_parsed": [
                    2025,
                    8,
                    5,
                    23,
                    47,
                    34,
                    1,
                    217,
                    0
                ],
                "title": "Managing Data for Scalable and Interactive Event Sequence Visualization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Managing Data for Scalable and Interactive Event Sequence Visualization"
                },
                "summary": "Parallel event sequences, such as those collected in program execution traces\nand automated manufacturing pipelines, are typically visualized as interactive\nparallel timelines. As the dataset size grows, these charts frequently\nexperience lag during common interactions such as zooming, panning, and\nfiltering. Summarization approaches can improve interaction performance, but at\nthe cost of accuracy in representation. To address this challenge, we introduce\nESeMan (Event Sequence Manager), an event sequence management system designed\nto support interactive rendering of timeline visualizations with tunable\naccuracy. ESeMan employs hierarchical data structures and intelligent caching\nto provide visualizations with only the data necessary to generate accurate\nsummarizations with significantly reduced data fetch time. We evaluate ESeMan's\nquery times against summed area tables, M4 aggregation, and statistical\nsub-sampling on a variety of program execution traces. Our results demonstrate\nESeMan provides better performance, achieving sub-100ms fetch times while\nmaintaining visualization accuracy at the pixel level. We further present our\nbenchmarking harness, enabling future performance evaluations for event\nsequence visualization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parallel event sequences, such as those collected in program execution traces\nand automated manufacturing pipelines, are typically visualized as interactive\nparallel timelines. As the dataset size grows, these charts frequently\nexperience lag during common interactions such as zooming, panning, and\nfiltering. Summarization approaches can improve interaction performance, but at\nthe cost of accuracy in representation. To address this challenge, we introduce\nESeMan (Event Sequence Manager), an event sequence management system designed\nto support interactive rendering of timeline visualizations with tunable\naccuracy. ESeMan employs hierarchical data structures and intelligent caching\nto provide visualizations with only the data necessary to generate accurate\nsummarizations with significantly reduced data fetch time. We evaluate ESeMan's\nquery times against summed area tables, M4 aggregation, and statistical\nsub-sampling on a variety of program execution traces. Our results demonstrate\nESeMan provides better performance, achieving sub-100ms fetch times while\nmaintaining visualization accuracy at the pixel level. We further present our\nbenchmarking harness, enabling future performance evaluations for event\nsequence visualization."
                },
                "authors": [
                    {
                        "name": "Sayef Azad Sakin"
                    },
                    {
                        "name": "Katherine E. Isaacs"
                    }
                ],
                "author_detail": {
                    "name": "Katherine E. Isaacs"
                },
                "author": "Katherine E. Isaacs",
                "arxiv_comment": "The 15th IEEE Workshop on Large Data Analysis and Visualization",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.03974v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.03974v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01216v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01216v2",
                "updated": "2025-08-09T00:12:01Z",
                "updated_parsed": [
                    2025,
                    8,
                    9,
                    0,
                    12,
                    1,
                    5,
                    221,
                    0
                ],
                "published": "2025-07-01T22:27:21Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    22,
                    27,
                    21,
                    1,
                    182,
                    0
                ],
                "title": "PAE MobiLLM: Privacy-Aware and Efficient LLM Fine-Tuning on the Mobile\n  Device via Additive Side-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PAE MobiLLM: Privacy-Aware and Efficient LLM Fine-Tuning on the Mobile\n  Device via Additive Side-Tuning"
                },
                "summary": "There is a huge gap between numerous intriguing applications fostered by\non-device large language model (LLM) fine-tuning (FT) from fresh mobile data\nand the limited resources of a mobile device. While existing server-assisted\nmethods (e.g., split learning or side-tuning) may enable LLM FT on the local\nmobile device, they suffer from heavy communication burdens of activation\ntransmissions, and may disclose data and labels to the server. To address those\nissues, we develop PAE MobiLLM, a a privacy-aware and efficient LLM FT method\nwhich can be deployed on the mobile device via server-assisted additive\nside-tuning. To further accelerate FT convergence and improve computing\nefficiency, PAE MobiLLM integrates activation caching on the server side, which\nallows the server to reuse historical activations and saves the mobile device\nfrom repeatedly computing forward passes for the recurring data samples.\nBesides, to reduce communication cost, PAE MobiLLM develops an activation\nshortcut that transmits only the token involved in the loss calculation instead\nof full activation matrices to guide the side network tuning. Last but not\nleast, PAE MobiLLM introduces the additive adapter side-network design which\nmakes the server train the adapter modules based on device-defined prediction\ndifferences rather than raw ground-truth labels. In this way, the server can\nonly assist device-defined side-network computing, and learn nothing about data\nand labels. Extensive experimental results demonstrate PAE MobiLLM's\nsuperiority.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There is a huge gap between numerous intriguing applications fostered by\non-device large language model (LLM) fine-tuning (FT) from fresh mobile data\nand the limited resources of a mobile device. While existing server-assisted\nmethods (e.g., split learning or side-tuning) may enable LLM FT on the local\nmobile device, they suffer from heavy communication burdens of activation\ntransmissions, and may disclose data and labels to the server. To address those\nissues, we develop PAE MobiLLM, a a privacy-aware and efficient LLM FT method\nwhich can be deployed on the mobile device via server-assisted additive\nside-tuning. To further accelerate FT convergence and improve computing\nefficiency, PAE MobiLLM integrates activation caching on the server side, which\nallows the server to reuse historical activations and saves the mobile device\nfrom repeatedly computing forward passes for the recurring data samples.\nBesides, to reduce communication cost, PAE MobiLLM develops an activation\nshortcut that transmits only the token involved in the loss calculation instead\nof full activation matrices to guide the side network tuning. Last but not\nleast, PAE MobiLLM introduces the additive adapter side-network design which\nmakes the server train the adapter modules based on device-defined prediction\ndifferences rather than raw ground-truth labels. In this way, the server can\nonly assist device-defined side-network computing, and learn nothing about data\nand labels. Extensive experimental results demonstrate PAE MobiLLM's\nsuperiority."
                },
                "authors": [
                    {
                        "name": "Xingke Yang"
                    },
                    {
                        "name": "Liang Li"
                    },
                    {
                        "name": "Zhiyi Wan"
                    },
                    {
                        "name": "Sicong Li"
                    },
                    {
                        "name": "Xiaoqi Qi"
                    },
                    {
                        "name": "Jiang Liu"
                    },
                    {
                        "name": "Tomoaki Ohtsuki"
                    },
                    {
                        "name": "Xin Fu"
                    },
                    {
                        "name": "Miao Pan"
                    }
                ],
                "author_detail": {
                    "name": "Miao Pan"
                },
                "author": "Miao Pan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01216v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01216v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11132v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11132v3",
                "updated": "2025-08-08T18:16:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    18,
                    16,
                    33,
                    4,
                    220,
                    0
                ],
                "published": "2025-03-14T06:49:37Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    6,
                    49,
                    37,
                    4,
                    73,
                    0
                ],
                "title": "X-EcoMLA: Upcycling Pre-Trained Attention into MLA for Efficient and\n  Extreme KV Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "X-EcoMLA: Upcycling Pre-Trained Attention into MLA for Efficient and\n  Extreme KV Compression"
                },
                "summary": "Multi-head latent attention (MLA) is designed to optimize KV cache memory\nthrough low-rank key-value joint compression. Rather than caching keys and\nvalues separately, MLA stores their compressed latent representations, reducing\nmemory overhead while maintaining the performance. While MLA improves memory\nefficiency without compromising language model accuracy, its major limitation\nlies in its integration during the pre-training phase, requiring models to be\ntrained from scratch. This raises a key question: can we use MLA's benefits\nfully or partially in models that have already been pre-trained with different\nattention mechanisms? In this paper, we propose X-EcoMLA to deploy post\ntraining distillation to enable the upcycling of Transformer-based attention\ninto an efficient hybrid MLA variant through lightweight post-training\nadaptation, bypassing the need for extensive pre-training. We demonstrate that\nleveraging the dark knowledge of a well-trained model can enhance training\naccuracy and enable extreme KV cache compression in MLA without compromising\nmodel performance. The experimental results show that our proposed method can\neffectively compress the KV cache while preserving the performance on the\nbenchmarks; specifically, for Llama3.2-1B-Instruct baseline, a 6.4x compression\nachieves the same average score by using only 3.6B training tokens and 70 GPU\nhours on AMD MI300, whereas a 10.6x compression have less than 0.1% average\nscore drop with 7B training tokens and 140 GPU hours. The code for this work is\navailable at https://github.com/AMD-AIG-AIMA/AMD-Hybrid-Models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-head latent attention (MLA) is designed to optimize KV cache memory\nthrough low-rank key-value joint compression. Rather than caching keys and\nvalues separately, MLA stores their compressed latent representations, reducing\nmemory overhead while maintaining the performance. While MLA improves memory\nefficiency without compromising language model accuracy, its major limitation\nlies in its integration during the pre-training phase, requiring models to be\ntrained from scratch. This raises a key question: can we use MLA's benefits\nfully or partially in models that have already been pre-trained with different\nattention mechanisms? In this paper, we propose X-EcoMLA to deploy post\ntraining distillation to enable the upcycling of Transformer-based attention\ninto an efficient hybrid MLA variant through lightweight post-training\nadaptation, bypassing the need for extensive pre-training. We demonstrate that\nleveraging the dark knowledge of a well-trained model can enhance training\naccuracy and enable extreme KV cache compression in MLA without compromising\nmodel performance. The experimental results show that our proposed method can\neffectively compress the KV cache while preserving the performance on the\nbenchmarks; specifically, for Llama3.2-1B-Instruct baseline, a 6.4x compression\nachieves the same average score by using only 3.6B training tokens and 70 GPU\nhours on AMD MI300, whereas a 10.6x compression have less than 0.1% average\nscore drop with 7B training tokens and 140 GPU hours. The code for this work is\navailable at https://github.com/AMD-AIG-AIMA/AMD-Hybrid-Models."
                },
                "authors": [
                    {
                        "name": "Guihong Li"
                    },
                    {
                        "name": "Mehdi Rezagholizadeh"
                    },
                    {
                        "name": "Mingyu Yang"
                    },
                    {
                        "name": "Vikram Appia"
                    },
                    {
                        "name": "Emad Barsoum"
                    }
                ],
                "author_detail": {
                    "name": "Emad Barsoum"
                },
                "author": "Emad Barsoum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11132v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11132v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06447v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06447v1",
                "updated": "2025-08-08T16:42:38Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    16,
                    42,
                    38,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T16:42:38Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    16,
                    42,
                    38,
                    4,
                    220,
                    0
                ],
                "title": "SlimInfer: Accelerating Long-Context LLM Inference via Dynamic Token\n  Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SlimInfer: Accelerating Long-Context LLM Inference via Dynamic Token\n  Pruning"
                },
                "summary": "Long-context inference for Large Language Models (LLMs) is heavily limited by\nhigh computational demands. While several existing methods optimize attention\ncomputation, they still process the full set of hidden states at each layer,\nlimiting overall efficiency. In this work, we propose SlimInfer, an innovative\nframework that aims to accelerate inference by directly pruning less critical\nprompt tokens during the forward pass. Our key insight is an information\ndiffusion phenomenon: As information from critical tokens propagates through\nlayers, it becomes distributed across the entire sequence. This diffusion\nprocess suggests that LLMs can maintain their semantic integrity when excessive\ntokens, even including these critical ones, are pruned in hidden states.\nMotivated by this, SlimInfer introduces a dynamic fine-grained pruning\nmechanism that accurately removes redundant tokens of hidden state at\nintermediate layers. This layer-wise pruning naturally enables an asynchronous\nKV cache manager that prefetches required token blocks without complex\npredictors, reducing both memory usage and I/O costs. Extensive experiments\nshow that SlimInfer can achieve up to $\\mathbf{2.53\\times}$ time-to-first-token\n(TTFT) speedup and $\\mathbf{1.88\\times}$ end-to-end latency reduction for\nLLaMA3.1-8B-Instruct on a single RTX 4090, without sacrificing performance on\nLongBench. Our code will be released upon acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context inference for Large Language Models (LLMs) is heavily limited by\nhigh computational demands. While several existing methods optimize attention\ncomputation, they still process the full set of hidden states at each layer,\nlimiting overall efficiency. In this work, we propose SlimInfer, an innovative\nframework that aims to accelerate inference by directly pruning less critical\nprompt tokens during the forward pass. Our key insight is an information\ndiffusion phenomenon: As information from critical tokens propagates through\nlayers, it becomes distributed across the entire sequence. This diffusion\nprocess suggests that LLMs can maintain their semantic integrity when excessive\ntokens, even including these critical ones, are pruned in hidden states.\nMotivated by this, SlimInfer introduces a dynamic fine-grained pruning\nmechanism that accurately removes redundant tokens of hidden state at\nintermediate layers. This layer-wise pruning naturally enables an asynchronous\nKV cache manager that prefetches required token blocks without complex\npredictors, reducing both memory usage and I/O costs. Extensive experiments\nshow that SlimInfer can achieve up to $\\mathbf{2.53\\times}$ time-to-first-token\n(TTFT) speedup and $\\mathbf{1.88\\times}$ end-to-end latency reduction for\nLLaMA3.1-8B-Instruct on a single RTX 4090, without sacrificing performance on\nLongBench. Our code will be released upon acceptance."
                },
                "authors": [
                    {
                        "name": "Lingkun Long"
                    },
                    {
                        "name": "Rubing Yang"
                    },
                    {
                        "name": "Yushi Huang"
                    },
                    {
                        "name": "Desheng Hui"
                    },
                    {
                        "name": "Ao Zhou"
                    },
                    {
                        "name": "Jianlei Yang"
                    }
                ],
                "author_detail": {
                    "name": "Jianlei Yang"
                },
                "author": "Jianlei Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06447v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06447v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.07467v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.07467v3",
                "updated": "2025-08-08T14:25:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    14,
                    25,
                    24,
                    4,
                    220,
                    0
                ],
                "published": "2024-06-11T17:13:18Z",
                "published_parsed": [
                    2024,
                    6,
                    11,
                    17,
                    13,
                    18,
                    1,
                    163,
                    0
                ],
                "title": "LLM meets ML: Data-efficient Anomaly Detection on Unstable Logs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM meets ML: Data-efficient Anomaly Detection on Unstable Logs"
                },
                "summary": "Most log-based anomaly detectors assume logs are stable, though logs are\noften unstable due to software or environmental changes. Anomaly detection on\nunstable logs (ULAD) is therefore a more realistic, yet under-investigated\nchallenge. Current approaches predominantly employ machine learning (ML)\nmodels, which often require extensive labeled data for training. To mitigate\ndata insufficiency, we propose FlexLog, a novel hybrid approach for ULAD that\ncombines ML models -- decision tree, k-nearest neighbors, and a feedforward\nneural network -- with a Large Language Model (Mistral) through ensemble\nlearning. FlexLog also incorporates a cache and retrieval-augmented generation\n(RAG) to further enhance efficiency and effectiveness. To evaluate FlexLog, we\nconfigured four datasets for \\task, namely ADFA-U, LOGEVOL-U, SynHDFS-U, and\nSYNEVOL-U. FlexLog outperforms all baselines by at least 1.2 percentage points\n(pp) in F1 score while using much less labeled data (62.87 pp reduction). When\ntrained on the same amount of data as the baselines, FlexLog achieves up to a\n13 pp increase in F1 score on ADFA-U across varying training dataset sizes.\nAdditionally, FlexLog maintains inference time under one second per log\nsequence, making it suitable for most applications, except latency-sensitive\nsystems. Further analysis reveals the positive impact of FlexLog's key\ncomponents: cache, RAG and ensemble learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Most log-based anomaly detectors assume logs are stable, though logs are\noften unstable due to software or environmental changes. Anomaly detection on\nunstable logs (ULAD) is therefore a more realistic, yet under-investigated\nchallenge. Current approaches predominantly employ machine learning (ML)\nmodels, which often require extensive labeled data for training. To mitigate\ndata insufficiency, we propose FlexLog, a novel hybrid approach for ULAD that\ncombines ML models -- decision tree, k-nearest neighbors, and a feedforward\nneural network -- with a Large Language Model (Mistral) through ensemble\nlearning. FlexLog also incorporates a cache and retrieval-augmented generation\n(RAG) to further enhance efficiency and effectiveness. To evaluate FlexLog, we\nconfigured four datasets for \\task, namely ADFA-U, LOGEVOL-U, SynHDFS-U, and\nSYNEVOL-U. FlexLog outperforms all baselines by at least 1.2 percentage points\n(pp) in F1 score while using much less labeled data (62.87 pp reduction). When\ntrained on the same amount of data as the baselines, FlexLog achieves up to a\n13 pp increase in F1 score on ADFA-U across varying training dataset sizes.\nAdditionally, FlexLog maintains inference time under one second per log\nsequence, making it suitable for most applications, except latency-sensitive\nsystems. Further analysis reveals the positive impact of FlexLog's key\ncomponents: cache, RAG and ensemble learning."
                },
                "authors": [
                    {
                        "name": "Fatemeh Hadadi"
                    },
                    {
                        "name": "Qinghua Xu"
                    },
                    {
                        "name": "Domenico Bianculli"
                    },
                    {
                        "name": "Lionel Briand"
                    }
                ],
                "author_detail": {
                    "name": "Lionel Briand"
                },
                "author": "Lionel Briand",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.07467v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.07467v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06297v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06297v1",
                "updated": "2025-08-08T13:19:30Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    13,
                    19,
                    30,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T13:19:30Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    13,
                    19,
                    30,
                    4,
                    220,
                    0
                ],
                "title": "KV Cache Compression for Inference Efficiency in LLMs: A Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV Cache Compression for Inference Efficiency in LLMs: A Review"
                },
                "summary": "Withtherapid advancement of large language models (LLMs), the context length\nfor inference has been continuously increasing, leading to an exponential\ngrowth in the demand for Key-Value (KV) caching. This has resulted in a\nsignificant memory bottleneck, limiting the inference efficiency and\nscalability of the models. Therefore, optimizing the KV cache during inference\nis crucial for enhancing performance and efficiency. This review systematically\nexamines current KV cache optimization techniques, including compression\nstrategies such as selective token strategies, quantization, and attention\ncompression. We evaluate the effectiveness, trade-offs, and application\nscenarios of these methods, providing a comprehensive analysis of their impact\non memory usage and inference speed. We focus on identifying the limitations\nand challenges of existing methods, such as compatibility issues with different\nmodels and tasks. Additionally, this review highlights future research\ndirections, including hybrid optimization techniques, adaptive dynamic\nstrategies, and software-hardware co-design. These approaches aim to improve\ninference efficiency and promote the practical application of large language\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Withtherapid advancement of large language models (LLMs), the context length\nfor inference has been continuously increasing, leading to an exponential\ngrowth in the demand for Key-Value (KV) caching. This has resulted in a\nsignificant memory bottleneck, limiting the inference efficiency and\nscalability of the models. Therefore, optimizing the KV cache during inference\nis crucial for enhancing performance and efficiency. This review systematically\nexamines current KV cache optimization techniques, including compression\nstrategies such as selective token strategies, quantization, and attention\ncompression. We evaluate the effectiveness, trade-offs, and application\nscenarios of these methods, providing a comprehensive analysis of their impact\non memory usage and inference speed. We focus on identifying the limitations\nand challenges of existing methods, such as compatibility issues with different\nmodels and tasks. Additionally, this review highlights future research\ndirections, including hybrid optimization techniques, adaptive dynamic\nstrategies, and software-hardware co-design. These approaches aim to improve\ninference efficiency and promote the practical application of large language\nmodels."
                },
                "authors": [
                    {
                        "name": "Yanyu Liu"
                    },
                    {
                        "name": "Jingying Fu"
                    },
                    {
                        "name": "Sixiang Liu"
                    },
                    {
                        "name": "Yitian Zou"
                    },
                    {
                        "name": "You Fu"
                    },
                    {
                        "name": "Jiehan Zhou"
                    },
                    {
                        "name": "Shouhua Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Shouhua Zhang"
                },
                "arxiv_affiliation": "University of Oulu",
                "author": "Shouhua Zhang",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06297v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06297v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06160v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06160v1",
                "updated": "2025-08-08T09:29:37Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    9,
                    29,
                    37,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T09:29:37Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    9,
                    29,
                    37,
                    4,
                    220,
                    0
                ],
                "title": "Fewer Denoising Steps or Cheaper Per-Step Inference: Towards\n  Compute-Optimal Diffusion Model Deployment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fewer Denoising Steps or Cheaper Per-Step Inference: Towards\n  Compute-Optimal Diffusion Model Deployment"
                },
                "summary": "Diffusion models have shown remarkable success across generative tasks, yet\ntheir high computational demands challenge deployment on resource-limited\nplatforms. This paper investigates a critical question for compute-optimal\ndiffusion model deployment: Under a post-training setting without fine-tuning,\nis it more effective to reduce the number of denoising steps or to use a\ncheaper per-step inference? Intuitively, reducing the number of denoising steps\nincreases the variability of the distributions across steps, making the model\nmore sensitive to compression. In contrast, keeping more denoising steps makes\nthe differences smaller, preserving redundancy, and making post-training\ncompression more feasible. To systematically examine this, we propose PostDiff,\na training-free framework for accelerating pre-trained diffusion models by\nreducing redundancy at both the input level and module level in a post-training\nmanner. At the input level, we propose a mixed-resolution denoising scheme\nbased on the insight that reducing generation resolution in early denoising\nsteps can enhance low-frequency components and improve final generation\nfidelity. At the module level, we employ a hybrid module caching strategy to\nreuse computations across denoising steps. Extensive experiments and ablation\nstudies demonstrate that (1) PostDiff can significantly improve the\nfidelity-efficiency trade-off of state-of-the-art diffusion models, and (2) to\nboost efficiency while maintaining decent generation fidelity, reducing\nper-step inference cost is often more effective than reducing the number of\ndenoising steps. Our code is available at\nhttps://github.com/GATECH-EIC/PostDiff.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have shown remarkable success across generative tasks, yet\ntheir high computational demands challenge deployment on resource-limited\nplatforms. This paper investigates a critical question for compute-optimal\ndiffusion model deployment: Under a post-training setting without fine-tuning,\nis it more effective to reduce the number of denoising steps or to use a\ncheaper per-step inference? Intuitively, reducing the number of denoising steps\nincreases the variability of the distributions across steps, making the model\nmore sensitive to compression. In contrast, keeping more denoising steps makes\nthe differences smaller, preserving redundancy, and making post-training\ncompression more feasible. To systematically examine this, we propose PostDiff,\na training-free framework for accelerating pre-trained diffusion models by\nreducing redundancy at both the input level and module level in a post-training\nmanner. At the input level, we propose a mixed-resolution denoising scheme\nbased on the insight that reducing generation resolution in early denoising\nsteps can enhance low-frequency components and improve final generation\nfidelity. At the module level, we employ a hybrid module caching strategy to\nreuse computations across denoising steps. Extensive experiments and ablation\nstudies demonstrate that (1) PostDiff can significantly improve the\nfidelity-efficiency trade-off of state-of-the-art diffusion models, and (2) to\nboost efficiency while maintaining decent generation fidelity, reducing\nper-step inference cost is often more effective than reducing the number of\ndenoising steps. Our code is available at\nhttps://github.com/GATECH-EIC/PostDiff."
                },
                "authors": [
                    {
                        "name": "Zhenbang Du"
                    },
                    {
                        "name": "Yonggan Fu"
                    },
                    {
                        "name": "Lifu Wang"
                    },
                    {
                        "name": "Jiayi Qian"
                    },
                    {
                        "name": "Xiao Luo"
                    },
                    {
                        "name": "Yingyan"
                    },
                    {
                        "name": "Lin"
                    }
                ],
                "author_detail": {
                    "name": "Lin"
                },
                "arxiv_affiliation": "Celine",
                "author": "Lin",
                "arxiv_comment": "Accepted by ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06160v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06160v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06133v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06133v1",
                "updated": "2025-08-08T08:54:21Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    8,
                    54,
                    21,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T08:54:21Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    8,
                    54,
                    21,
                    4,
                    220,
                    0
                ],
                "title": "LLM Serving Optimization with Variable Prefill and Decode Lengths",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Serving Optimization with Variable Prefill and Decode Lengths"
                },
                "summary": "We study the problem of serving LLM (Large Language Model) requests where\neach request has heterogeneous prefill and decode lengths. In LLM serving, the\nprefill length corresponds to the input prompt length, which determines the\ninitial memory usage in the KV cache. The decode length refers to the number of\noutput tokens generated sequentially, with each additional token increasing the\nKV cache memory usage by one unit. Given a set of n requests, our goal is to\nschedule and process them to minimize the total completion time. We show that\nthis problem is NP-hard due to the interplay of batching, placement\nconstraints, precedence relationships, and linearly increasing memory usage. We\nthen analyze commonly used scheduling strategies in practice, such as\nFirst-Come-First-Serve (FCFS) and Shortest-First (SF), and prove that their\ncompetitive ratios scale up sublinearly with the memory limit-a significant\ndrawback in real-world settings where memory demand is large. To address this,\nwe propose a novel algorithm based on a new selection metric that efficiently\nforms batches over time. We prove that this algorithm achieves a constant\ncompetitive ratio. Finally, we develop and evaluate a few algorithm variants\ninspired by this approach, including dynamic programming variants, local search\nmethods, and an LP-based scheduler, demonstrating through comprehensive\nsimulations that they outperform standard baselines while maintaining\ncomputational efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the problem of serving LLM (Large Language Model) requests where\neach request has heterogeneous prefill and decode lengths. In LLM serving, the\nprefill length corresponds to the input prompt length, which determines the\ninitial memory usage in the KV cache. The decode length refers to the number of\noutput tokens generated sequentially, with each additional token increasing the\nKV cache memory usage by one unit. Given a set of n requests, our goal is to\nschedule and process them to minimize the total completion time. We show that\nthis problem is NP-hard due to the interplay of batching, placement\nconstraints, precedence relationships, and linearly increasing memory usage. We\nthen analyze commonly used scheduling strategies in practice, such as\nFirst-Come-First-Serve (FCFS) and Shortest-First (SF), and prove that their\ncompetitive ratios scale up sublinearly with the memory limit-a significant\ndrawback in real-world settings where memory demand is large. To address this,\nwe propose a novel algorithm based on a new selection metric that efficiently\nforms batches over time. We prove that this algorithm achieves a constant\ncompetitive ratio. Finally, we develop and evaluate a few algorithm variants\ninspired by this approach, including dynamic programming variants, local search\nmethods, and an LP-based scheduler, demonstrating through comprehensive\nsimulations that they outperform standard baselines while maintaining\ncomputational efficiency."
                },
                "authors": [
                    {
                        "name": "Meixuan Wang"
                    },
                    {
                        "name": "Yinyu Ye"
                    },
                    {
                        "name": "Zijie Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Zijie Zhou"
                },
                "author": "Zijie Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06133v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06133v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06064v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06064v1",
                "updated": "2025-08-08T06:53:50Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    6,
                    53,
                    50,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T06:53:50Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    6,
                    53,
                    50,
                    4,
                    220,
                    0
                ],
                "title": "A Generic Complete Anytime Beam Search for Optimal Decision Tree",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Generic Complete Anytime Beam Search for Optimal Decision Tree"
                },
                "summary": "Finding an optimal decision tree that minimizes classification error is known\nto be NP-hard. While exact algorithms based on MILP, CP, SAT, or dynamic\nprogramming guarantee optimality, they often suffer from poor anytime behavior\n-- meaning they struggle to find high-quality decision trees quickly when the\nsearch is stopped before completion -- due to unbalanced search space\nexploration. To address this, several anytime extensions of exact methods have\nbeen proposed, such as LDS-DL8.5, Top-k-DL8.5, and Blossom, but they have not\nbeen systematically compared, making it difficult to assess their relative\neffectiveness. In this paper, we propose CA-DL8.5, a generic, complete, and\nanytime beam search algorithm that extends the DL8.5 framework and unifies some\nexisting anytime strategies. In particular, CA-DL8.5 generalizes previous\napproaches LDS-DL8.5 and Top-k-DL8.5, by allowing the integration of various\nheuristics and relaxation mechanisms through a modular design. The algorithm\nreuses DL8.5's efficient branch-and-bound pruning and trie-based caching,\ncombined with a restart-based beam search that gradually relaxes pruning\ncriteria to improve solution quality over time. Our contributions are twofold:\n(1) We introduce this new generic framework for exact and anytime decision tree\nlearning, enabling the incorporation of diverse heuristics and search\nstrategies; (2) We conduct a rigorous empirical comparison of several\ninstantiations of CA-DL8.5 -- based on Purity, Gain, Discrepancy, and Top-k\nheuristics -- using an anytime evaluation metric called the primal gap\nintegral. Experimental results on standard classification benchmarks show that\nCA-DL8.5 using LDS (limited discrepancy) consistently provides the best anytime\nperformance, outperforming both other CA-DL8.5 variants and the Blossom\nalgorithm while maintaining completeness and optimality guarantees.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Finding an optimal decision tree that minimizes classification error is known\nto be NP-hard. While exact algorithms based on MILP, CP, SAT, or dynamic\nprogramming guarantee optimality, they often suffer from poor anytime behavior\n-- meaning they struggle to find high-quality decision trees quickly when the\nsearch is stopped before completion -- due to unbalanced search space\nexploration. To address this, several anytime extensions of exact methods have\nbeen proposed, such as LDS-DL8.5, Top-k-DL8.5, and Blossom, but they have not\nbeen systematically compared, making it difficult to assess their relative\neffectiveness. In this paper, we propose CA-DL8.5, a generic, complete, and\nanytime beam search algorithm that extends the DL8.5 framework and unifies some\nexisting anytime strategies. In particular, CA-DL8.5 generalizes previous\napproaches LDS-DL8.5 and Top-k-DL8.5, by allowing the integration of various\nheuristics and relaxation mechanisms through a modular design. The algorithm\nreuses DL8.5's efficient branch-and-bound pruning and trie-based caching,\ncombined with a restart-based beam search that gradually relaxes pruning\ncriteria to improve solution quality over time. Our contributions are twofold:\n(1) We introduce this new generic framework for exact and anytime decision tree\nlearning, enabling the incorporation of diverse heuristics and search\nstrategies; (2) We conduct a rigorous empirical comparison of several\ninstantiations of CA-DL8.5 -- based on Purity, Gain, Discrepancy, and Top-k\nheuristics -- using an anytime evaluation metric called the primal gap\nintegral. Experimental results on standard classification benchmarks show that\nCA-DL8.5 using LDS (limited discrepancy) consistently provides the best anytime\nperformance, outperforming both other CA-DL8.5 variants and the Blossom\nalgorithm while maintaining completeness and optimality guarantees."
                },
                "authors": [
                    {
                        "name": "Harold Silvre Kiossou"
                    },
                    {
                        "name": "Siegfried Nijssen"
                    },
                    {
                        "name": "Pierre Schaus"
                    }
                ],
                "author_detail": {
                    "name": "Pierre Schaus"
                },
                "author": "Pierre Schaus",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06064v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06064v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2104.13123v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2104.13123v3",
                "updated": "2025-08-08T06:38:01Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    6,
                    38,
                    1,
                    4,
                    220,
                    0
                ],
                "published": "2021-04-27T11:55:54Z",
                "published_parsed": [
                    2021,
                    4,
                    27,
                    11,
                    55,
                    54,
                    1,
                    117,
                    0
                ],
                "title": "Affine Springer fibers and depth zero L-packets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Affine Springer fibers and depth zero L-packets"
                },
                "summary": "Let $G$ be a connected reductive group over a field $F=\\mathbb{F}_q((t))$\nsplitting over $\\overline{\\mathbb{F}}_q((t))$. Following [KV,DR], a tamely\nunramified Langlands parameter $\\lambda:W_F\\to{}^L\nG(\\overline{\\mathbb{Q}}_{\\ell})$ in general position gives rise to a finite set\n$\\Pi_{\\lambda}$ of irreducible admissible representations of $G(F)$, called the\n$L$-packet.\n  The main goal of this work is to provide a geometric description of\ncharacters $\\chi_{\\pi}$ of $\\pi\\in\\Pi_{\\lambda}$ and of their endoscopic linear\ncombinations $\\chi_{\\lambda}^{\\kappa}$ in terms of homology of affine Springer\nfibers, thus establishing an analog of Lusztig conjectures in this case.\nFurthermore, each $\\chi_{\\lambda}^{\\kappa}$ can be described as the trace of\nFrobenius function of a conjugation equivariant perverse sheaf on the loop\ngroup by the sheaf-function correspondence.\n  As another application, we prove that the sum\n$\\chi_{\\lambda}^{st}:=\\sum_{\\pi\\in\\Pi_{\\lambda}}\\chi_{\\pi}$ is stable and show\nthat the $\\chi_{\\lambda}^{st}$'s are compatible with inner twistings. More\ngenerally, we prove that each $\\chi_{\\lambda}^{\\kappa}$ is\n$\\mathcal{E}_{\\lambda,\\kappa}$-stable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Let $G$ be a connected reductive group over a field $F=\\mathbb{F}_q((t))$\nsplitting over $\\overline{\\mathbb{F}}_q((t))$. Following [KV,DR], a tamely\nunramified Langlands parameter $\\lambda:W_F\\to{}^L\nG(\\overline{\\mathbb{Q}}_{\\ell})$ in general position gives rise to a finite set\n$\\Pi_{\\lambda}$ of irreducible admissible representations of $G(F)$, called the\n$L$-packet.\n  The main goal of this work is to provide a geometric description of\ncharacters $\\chi_{\\pi}$ of $\\pi\\in\\Pi_{\\lambda}$ and of their endoscopic linear\ncombinations $\\chi_{\\lambda}^{\\kappa}$ in terms of homology of affine Springer\nfibers, thus establishing an analog of Lusztig conjectures in this case.\nFurthermore, each $\\chi_{\\lambda}^{\\kappa}$ can be described as the trace of\nFrobenius function of a conjugation equivariant perverse sheaf on the loop\ngroup by the sheaf-function correspondence.\n  As another application, we prove that the sum\n$\\chi_{\\lambda}^{st}:=\\sum_{\\pi\\in\\Pi_{\\lambda}}\\chi_{\\pi}$ is stable and show\nthat the $\\chi_{\\lambda}^{st}$'s are compatible with inner twistings. More\ngenerally, we prove that each $\\chi_{\\lambda}^{\\kappa}$ is\n$\\mathcal{E}_{\\lambda,\\kappa}$-stable."
                },
                "authors": [
                    {
                        "name": "Roman Bezrukavnikov"
                    },
                    {
                        "name": "Yakov Varshavsky"
                    }
                ],
                "author_detail": {
                    "name": "Yakov Varshavsky"
                },
                "author": "Yakov Varshavsky",
                "arxiv_comment": "v.3, 96 pages, minor changes in abstract and introduction",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2104.13123v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2104.13123v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.RT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.RT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.AG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "22E50, 22E57",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09192v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09192v1",
                "updated": "2025-08-08T04:51:37Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    4,
                    51,
                    37,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T04:51:37Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    4,
                    51,
                    37,
                    4,
                    220,
                    0
                ],
                "title": "Diffusion LLMs Can Do Faster-Than-AR Inference via Discrete Diffusion\n  Forcing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion LLMs Can Do Faster-Than-AR Inference via Discrete Diffusion\n  Forcing"
                },
                "summary": "Diffusion Large Language Models (dLLMs) have emerged as a promising\nalternative to autoregressive (AR) LLMs for text generation, with the potential\nto decode multiple tokens in a single iteration. However, none of the existing\nopen-source dLLMs have achieved superior inference speed over AR LLMs of\nsimilar size. This paper breaks this barrier based on a simple and effective\nstrategy named discrete diffusion forcing (D2F). D2F equips dLLMs with two key\ncapabilities: (1) block-wise autoregressive generation to enable KV cache\nutilization; (2) prediction of following tokens without requiring completion of\nprior blocks for inter-block parallel decoding. In this way, the vanilla dLLMs\nare refurbished into an AR-diffusion hybrid paradigm for efficient inference.\nD2F can be implemented with an asymmetric distillation process based on\npre-trained dLLMs. We further propose a pipelined parallel decoding algorithm,\nwhich enables a trade-off between efficiency and efficacy. Empirically, D2F\ndLLMs achieve more than $\\mathbf{2.5\\times}$ inference speed than LLaMA3 and\nQwen2.5 on GSM8K. Compared to vanilla dLLMs like LLaDA and Dream, the\nacceleration can be more than $\\mathbf{50\\times}$ while maintaining comparable\noutput quality. The code is available at\nhttps://github.com/zhijie-group/Discrete-Diffusion-Forcing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Large Language Models (dLLMs) have emerged as a promising\nalternative to autoregressive (AR) LLMs for text generation, with the potential\nto decode multiple tokens in a single iteration. However, none of the existing\nopen-source dLLMs have achieved superior inference speed over AR LLMs of\nsimilar size. This paper breaks this barrier based on a simple and effective\nstrategy named discrete diffusion forcing (D2F). D2F equips dLLMs with two key\ncapabilities: (1) block-wise autoregressive generation to enable KV cache\nutilization; (2) prediction of following tokens without requiring completion of\nprior blocks for inter-block parallel decoding. In this way, the vanilla dLLMs\nare refurbished into an AR-diffusion hybrid paradigm for efficient inference.\nD2F can be implemented with an asymmetric distillation process based on\npre-trained dLLMs. We further propose a pipelined parallel decoding algorithm,\nwhich enables a trade-off between efficiency and efficacy. Empirically, D2F\ndLLMs achieve more than $\\mathbf{2.5\\times}$ inference speed than LLaMA3 and\nQwen2.5 on GSM8K. Compared to vanilla dLLMs like LLaDA and Dream, the\nacceleration can be more than $\\mathbf{50\\times}$ while maintaining comparable\noutput quality. The code is available at\nhttps://github.com/zhijie-group/Discrete-Diffusion-Forcing."
                },
                "authors": [
                    {
                        "name": "Xu Wang"
                    },
                    {
                        "name": "Chenkai Xu"
                    },
                    {
                        "name": "Yijie Jin"
                    },
                    {
                        "name": "Jiachun Jin"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Zhijie Deng"
                    }
                ],
                "author_detail": {
                    "name": "Zhijie Deng"
                },
                "author": "Zhijie Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09192v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09192v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05904v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05904v1",
                "updated": "2025-08-07T23:53:31Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    23,
                    53,
                    31,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T23:53:31Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    23,
                    53,
                    31,
                    3,
                    219,
                    0
                ],
                "title": "Snowpark: Performant, Secure, User-Friendly Data Engineering and AI/ML\n  Next To Your Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Snowpark: Performant, Secure, User-Friendly Data Engineering and AI/ML\n  Next To Your Data"
                },
                "summary": "Snowflake revolutionized data analytics with an elastic architecture that\ndecouples compute and storage, enabling scalable solutions supporting data\narchitectures like data lake, data warehouse, data lakehouse, and data mesh.\nBuilding on this foundation, Snowflake has advanced its AI Data Cloud vision by\nintroducing Snowpark, a managed turnkey solution that supports data engineering\nand AI and ML workloads using Python and other programming languages.\n  This paper outlines Snowpark's design objectives towards high performance,\nstrong security and governance, and ease of use. We detail the architecture of\nSnowpark, highlighting its elastic scalability and seamless integration with\nSnowflake core compute infrastructure. This includes leveraging Snowflake\ncontrol plane for distributed computing and employing a secure sandbox for\nisolating Snowflake SQL workloads from Snowpark executions. Additionally, we\npresent core innovations in Snowpark that drive further performance\nenhancements, such as query initialization latency reduction through Python\npackage caching, improved workload scheduling for customized workloads, and\ndata skew management via efficient row redistribution. Finally, we showcase\nreal-world case studies that illustrate Snowpark's efficiency and effectiveness\nfor large-scale data engineering and AI and ML tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Snowflake revolutionized data analytics with an elastic architecture that\ndecouples compute and storage, enabling scalable solutions supporting data\narchitectures like data lake, data warehouse, data lakehouse, and data mesh.\nBuilding on this foundation, Snowflake has advanced its AI Data Cloud vision by\nintroducing Snowpark, a managed turnkey solution that supports data engineering\nand AI and ML workloads using Python and other programming languages.\n  This paper outlines Snowpark's design objectives towards high performance,\nstrong security and governance, and ease of use. We detail the architecture of\nSnowpark, highlighting its elastic scalability and seamless integration with\nSnowflake core compute infrastructure. This includes leveraging Snowflake\ncontrol plane for distributed computing and employing a secure sandbox for\nisolating Snowflake SQL workloads from Snowpark executions. Additionally, we\npresent core innovations in Snowpark that drive further performance\nenhancements, such as query initialization latency reduction through Python\npackage caching, improved workload scheduling for customized workloads, and\ndata skew management via efficient row redistribution. Finally, we showcase\nreal-world case studies that illustrate Snowpark's efficiency and effectiveness\nfor large-scale data engineering and AI and ML tasks."
                },
                "authors": [
                    {
                        "name": "Brandon Baker"
                    },
                    {
                        "name": "Elliott Brossard"
                    },
                    {
                        "name": "Chenwei Xie"
                    },
                    {
                        "name": "Zihao Ye"
                    },
                    {
                        "name": "Deen Liu"
                    },
                    {
                        "name": "Yijun Xie"
                    },
                    {
                        "name": "Arthur Zwiegincew"
                    },
                    {
                        "name": "Nitya Kumar Sharma"
                    },
                    {
                        "name": "Gaurav Jain"
                    },
                    {
                        "name": "Eugene Retunsky"
                    },
                    {
                        "name": "Mike Halcrow"
                    },
                    {
                        "name": "Derek Denny-Brown"
                    },
                    {
                        "name": "Istvan Cseri"
                    },
                    {
                        "name": "Tyler Akidau"
                    },
                    {
                        "name": "Yuxiong He"
                    }
                ],
                "author_detail": {
                    "name": "Yuxiong He"
                },
                "author": "Yuxiong He",
                "arxiv_comment": "12 pages, 6 figures, accepted in ICDCS 2025",
                "arxiv_journal_ref": "Proc. 45th IEEE International Conference on Distributed Computing\n  Systems (ICDCS), Glasgow, UK, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05904v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05904v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05898v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05898v1",
                "updated": "2025-08-07T23:11:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    23,
                    11,
                    33,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T23:11:33Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    23,
                    11,
                    33,
                    3,
                    219,
                    0
                ],
                "title": "ETTA: Efficient Test-Time Adaptation for Vision-Language Models through\n  Dynamic Embedding Updates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ETTA: Efficient Test-Time Adaptation for Vision-Language Models through\n  Dynamic Embedding Updates"
                },
                "summary": "Pretrained vision-language models (VLMs) like CLIP show strong zero-shot\nperformance but struggle with generalization under distribution shifts.\nTest-Time Adaptation (TTA) addresses this by adapting VLMs to unlabeled test\ndata in new domains. While some TTA methods rely on prompt-tuning,\ntraining-free cache-based approaches are preferred for efficiency. However,\ncurrent cache-based TTA models store only a limited set of high-confidence\nsamples, restricting the decision boundary to these samples and ignoring the\ninfluence of other incoming test data. To address this, we propose Efficient\nTest-Time Adaptation (ETTA), introducing a Recursive Updating module that\nintegrates all incoming test samples, progressively refining the decision\nboundary. This strategy mimics an unbounded cache, dynamically updating\ncontextual embeddings for improved accuracy with minimal memory and\ncomputational overhead. ETTA also includes an Adaptive Ensemble module to\nreduce prompt dependency in image-to-text scores by dynamically selecting\noptimal prompts for each class. Furthermore, ETTA adaptively combines scores\nfrom both modules based on confidence levels, leveraging their complementary\nstrengths. Extensive experiments on two benchmarks confirm that ETTA surpasses\nthe state-of-the-art TTA models in computational complexity and accuracy,\nsetting a new standard for effective, efficient test-time adaptation. The code\nhas been released at https://github.com/hamidreza-dastmalchi/ETTA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pretrained vision-language models (VLMs) like CLIP show strong zero-shot\nperformance but struggle with generalization under distribution shifts.\nTest-Time Adaptation (TTA) addresses this by adapting VLMs to unlabeled test\ndata in new domains. While some TTA methods rely on prompt-tuning,\ntraining-free cache-based approaches are preferred for efficiency. However,\ncurrent cache-based TTA models store only a limited set of high-confidence\nsamples, restricting the decision boundary to these samples and ignoring the\ninfluence of other incoming test data. To address this, we propose Efficient\nTest-Time Adaptation (ETTA), introducing a Recursive Updating module that\nintegrates all incoming test samples, progressively refining the decision\nboundary. This strategy mimics an unbounded cache, dynamically updating\ncontextual embeddings for improved accuracy with minimal memory and\ncomputational overhead. ETTA also includes an Adaptive Ensemble module to\nreduce prompt dependency in image-to-text scores by dynamically selecting\noptimal prompts for each class. Furthermore, ETTA adaptively combines scores\nfrom both modules based on confidence levels, leveraging their complementary\nstrengths. Extensive experiments on two benchmarks confirm that ETTA surpasses\nthe state-of-the-art TTA models in computational complexity and accuracy,\nsetting a new standard for effective, efficient test-time adaptation. The code\nhas been released at https://github.com/hamidreza-dastmalchi/ETTA."
                },
                "authors": [
                    {
                        "name": "Hamidreza Dastmalchi"
                    },
                    {
                        "name": "Aijun An"
                    },
                    {
                        "name": "Ali cheraghian"
                    }
                ],
                "author_detail": {
                    "name": "Ali cheraghian"
                },
                "author": "Ali cheraghian",
                "arxiv_comment": "BMVC2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05898v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05898v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10024v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10024v1",
                "updated": "2025-08-07T21:18:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    21,
                    18,
                    52,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T21:18:52Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    21,
                    18,
                    52,
                    3,
                    219,
                    0
                ],
                "title": "RTTC: Reward-Guided Collaborative Test-Time Compute",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RTTC: Reward-Guided Collaborative Test-Time Compute"
                },
                "summary": "Test-Time Compute (TTC) has emerged as a powerful paradigm for enhancing the\nperformance of Large Language Models (LLMs) at inference, leveraging strategies\nsuch as Test-Time Training (TTT) and Retrieval-Augmented Generation (RAG).\nHowever, the optimal adaptation strategy varies across queries, and\nindiscriminate application of TTC strategy incurs substantial computational\noverhead. In this work, we introduce Reward-Guided Test-Time Compute (RTTC), a\nnovel framework that adaptively selects the most effective TTC strategy for\neach query via a pretrained reward model, maximizing downstream accuracy across\ndiverse domains and tasks. RTTC operates in a distributed server-client\narchitecture, retrieving relevant samples from a remote knowledge base and\napplying RAG or lightweight fine-tuning on client devices only when necessary.\nTo further mitigate redundant computation, we propose Query-State Caching,\nwhich enables the efficient reuse of historical query states at both retrieval\nand adaptation levels. Extensive experiments across multiple LLMs and\nbenchmarks demonstrate that RTTC consistently achieves superior accuracy\ncompared to vanilla RAG or TTT, validating the necessity of adaptive,\nreward-guided TTC selection and the potential of RTTC for scalable,\nhigh-performance language model adaptation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-Time Compute (TTC) has emerged as a powerful paradigm for enhancing the\nperformance of Large Language Models (LLMs) at inference, leveraging strategies\nsuch as Test-Time Training (TTT) and Retrieval-Augmented Generation (RAG).\nHowever, the optimal adaptation strategy varies across queries, and\nindiscriminate application of TTC strategy incurs substantial computational\noverhead. In this work, we introduce Reward-Guided Test-Time Compute (RTTC), a\nnovel framework that adaptively selects the most effective TTC strategy for\neach query via a pretrained reward model, maximizing downstream accuracy across\ndiverse domains and tasks. RTTC operates in a distributed server-client\narchitecture, retrieving relevant samples from a remote knowledge base and\napplying RAG or lightweight fine-tuning on client devices only when necessary.\nTo further mitigate redundant computation, we propose Query-State Caching,\nwhich enables the efficient reuse of historical query states at both retrieval\nand adaptation levels. Extensive experiments across multiple LLMs and\nbenchmarks demonstrate that RTTC consistently achieves superior accuracy\ncompared to vanilla RAG or TTT, validating the necessity of adaptive,\nreward-guided TTC selection and the potential of RTTC for scalable,\nhigh-performance language model adaptation."
                },
                "authors": [
                    {
                        "name": "J. Pablo Muoz"
                    },
                    {
                        "name": "Jinjie Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Jinjie Yuan"
                },
                "author": "Jinjie Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10024v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10024v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05211v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05211v1",
                "updated": "2025-08-07T09:47:21Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    9,
                    47,
                    21,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T09:47:21Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    9,
                    47,
                    21,
                    3,
                    219,
                    0
                ],
                "title": "VFlowOpt: A Token Pruning Framework for LMMs with Visual Information\n  Flow-Guided Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VFlowOpt: A Token Pruning Framework for LMMs with Visual Information\n  Flow-Guided Optimization"
                },
                "summary": "Large Multimodal Models (LMMs) excel in visual-language tasks by leveraging\nnumerous visual tokens for fine-grained visual information, but this token\nredundancy results in significant computational costs. Previous research aimed\nat reducing visual tokens during inference typically leverages importance maps\nderived from attention scores among vision-only tokens or vision-language\ntokens to prune tokens across one or multiple pruning stages. Despite this\nprogress, pruning frameworks and strategies remain simplistic and\ninsufficiently explored, often resulting in substantial performance\ndegradation. In this paper, we propose VFlowOpt, a token pruning framework that\nintroduces an importance map derivation process and a progressive pruning\nmodule with a recycling mechanism. The hyperparameters of its pruning strategy\nare further optimized by a visual information flow-guided method. Specifically,\nwe compute an importance map for image tokens based on their attention-derived\ncontext relevance and patch-level information entropy. We then decide which\ntokens to retain or prune and aggregate the pruned ones as recycled tokens to\navoid potential information loss. Finally, we apply a visual information\nflow-guided method that regards the last token in the LMM as the most\nrepresentative signal of text-visual interactions. This method minimizes the\ndiscrepancy between token representations in LMMs with and without pruning,\nthereby enabling superior pruning strategies tailored to different LMMs.\nExperiments demonstrate that VFlowOpt can prune 90% of visual tokens while\nmaintaining comparable performance, leading to an 89% reduction in KV-Cache\nmemory and 3.8 times faster inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Multimodal Models (LMMs) excel in visual-language tasks by leveraging\nnumerous visual tokens for fine-grained visual information, but this token\nredundancy results in significant computational costs. Previous research aimed\nat reducing visual tokens during inference typically leverages importance maps\nderived from attention scores among vision-only tokens or vision-language\ntokens to prune tokens across one or multiple pruning stages. Despite this\nprogress, pruning frameworks and strategies remain simplistic and\ninsufficiently explored, often resulting in substantial performance\ndegradation. In this paper, we propose VFlowOpt, a token pruning framework that\nintroduces an importance map derivation process and a progressive pruning\nmodule with a recycling mechanism. The hyperparameters of its pruning strategy\nare further optimized by a visual information flow-guided method. Specifically,\nwe compute an importance map for image tokens based on their attention-derived\ncontext relevance and patch-level information entropy. We then decide which\ntokens to retain or prune and aggregate the pruned ones as recycled tokens to\navoid potential information loss. Finally, we apply a visual information\nflow-guided method that regards the last token in the LMM as the most\nrepresentative signal of text-visual interactions. This method minimizes the\ndiscrepancy between token representations in LMMs with and without pruning,\nthereby enabling superior pruning strategies tailored to different LMMs.\nExperiments demonstrate that VFlowOpt can prune 90% of visual tokens while\nmaintaining comparable performance, leading to an 89% reduction in KV-Cache\nmemory and 3.8 times faster inference."
                },
                "authors": [
                    {
                        "name": "Sihan Yang"
                    },
                    {
                        "name": "Runsen Xu"
                    },
                    {
                        "name": "Chenhang Cui"
                    },
                    {
                        "name": "Tai Wang"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Jiangmiao Pang"
                    }
                ],
                "author_detail": {
                    "name": "Jiangmiao Pang"
                },
                "author": "Jiangmiao Pang",
                "arxiv_comment": "Accepted by ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05211v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05211v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05091v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05091v1",
                "updated": "2025-08-07T07:19:02Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    7,
                    19,
                    2,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T07:19:02Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    7,
                    19,
                    2,
                    3,
                    219,
                    0
                ],
                "title": "PoseGen: In-Context LoRA Finetuning for Pose-Controllable Long Human\n  Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PoseGen: In-Context LoRA Finetuning for Pose-Controllable Long Human\n  Video Generation"
                },
                "summary": "Generating long, temporally coherent videos with precise control over subject\nidentity and motion is a formidable challenge for current diffusion models,\nwhich often suffer from identity drift and are limited to short clips. We\nintroduce PoseGen, a novel framework that generates arbitrarily long videos of\na specific subject from a single reference image and a driving pose sequence.\nOur core innovation is an in-context LoRA finetuning strategy that injects\nsubject appearance at the token level for identity preservation, while\nsimultaneously conditioning on pose information at the channel level for\nfine-grained motion control. To overcome duration limits, PoseGen pioneers an\ninterleaved segment generation method that seamlessly stitches video clips\ntogether, using a shared KV cache mechanism and a specialized transition\nprocess to ensure background consistency and temporal smoothness. Trained on a\nremarkably small 33-hour video dataset, extensive experiments show that PoseGen\nsignificantly outperforms state-of-the-art methods in identity fidelity, pose\naccuracy, and its unique ability to produce coherent, artifact-free videos of\nunlimited duration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating long, temporally coherent videos with precise control over subject\nidentity and motion is a formidable challenge for current diffusion models,\nwhich often suffer from identity drift and are limited to short clips. We\nintroduce PoseGen, a novel framework that generates arbitrarily long videos of\na specific subject from a single reference image and a driving pose sequence.\nOur core innovation is an in-context LoRA finetuning strategy that injects\nsubject appearance at the token level for identity preservation, while\nsimultaneously conditioning on pose information at the channel level for\nfine-grained motion control. To overcome duration limits, PoseGen pioneers an\ninterleaved segment generation method that seamlessly stitches video clips\ntogether, using a shared KV cache mechanism and a specialized transition\nprocess to ensure background consistency and temporal smoothness. Trained on a\nremarkably small 33-hour video dataset, extensive experiments show that PoseGen\nsignificantly outperforms state-of-the-art methods in identity fidelity, pose\naccuracy, and its unique ability to produce coherent, artifact-free videos of\nunlimited duration."
                },
                "authors": [
                    {
                        "name": "Jingxuan He"
                    },
                    {
                        "name": "Busheng Su"
                    },
                    {
                        "name": "Finn Wong"
                    }
                ],
                "author_detail": {
                    "name": "Finn Wong"
                },
                "author": "Finn Wong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05091v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05091v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05012v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05012v1",
                "updated": "2025-08-07T03:49:56Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    3,
                    49,
                    56,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T03:49:56Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    3,
                    49,
                    56,
                    3,
                    219,
                    0
                ],
                "title": "Making Prompts First-Class Citizens for Adaptive LLM Pipelines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Making Prompts First-Class Citizens for Adaptive LLM Pipelines"
                },
                "summary": "Modern LLM pipelines increasingly resemble data-centric systems: they\nretrieve external context, compose intermediate outputs, validate results, and\nadapt based on runtime feedback. Yet, the central element guiding this process\n-- the prompt -- remains a brittle, opaque string, disconnected from the\nsurrounding dataflow. This disconnect limits reuse, optimization, and runtime\ncontrol.\n  In this paper, we describe our vision and an initial design for SPEAR, a\nlanguage and runtime that fills this prompt management gap by making prompts\nstructured, adaptive, and first-class components of the execution model. SPEAR\nenables (1) runtime prompt refinement -- modifying prompts dynamically in\nresponse to execution-time signals such as confidence, latency, or missing\ncontext; and (2) structured prompt management -- organizing prompt fragments\ninto versioned views with support for introspection and logging.\n  SPEAR defines a prompt algebra that governs how prompts are constructed and\nadapted within a pipeline. It supports multiple refinement modes (manual,\nassisted, and automatic), giving developers a balance between control and\nautomation. By treating prompt logic as structured data, SPEAR enables\noptimizations such as operator fusion, prefix caching, and view reuse.\nPreliminary experiments quantify the behavior of different refinement modes\ncompared to static prompts and agentic retries, as well as the impact of\nprompt-level optimizations such as operator fusion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern LLM pipelines increasingly resemble data-centric systems: they\nretrieve external context, compose intermediate outputs, validate results, and\nadapt based on runtime feedback. Yet, the central element guiding this process\n-- the prompt -- remains a brittle, opaque string, disconnected from the\nsurrounding dataflow. This disconnect limits reuse, optimization, and runtime\ncontrol.\n  In this paper, we describe our vision and an initial design for SPEAR, a\nlanguage and runtime that fills this prompt management gap by making prompts\nstructured, adaptive, and first-class components of the execution model. SPEAR\nenables (1) runtime prompt refinement -- modifying prompts dynamically in\nresponse to execution-time signals such as confidence, latency, or missing\ncontext; and (2) structured prompt management -- organizing prompt fragments\ninto versioned views with support for introspection and logging.\n  SPEAR defines a prompt algebra that governs how prompts are constructed and\nadapted within a pipeline. It supports multiple refinement modes (manual,\nassisted, and automatic), giving developers a balance between control and\nautomation. By treating prompt logic as structured data, SPEAR enables\noptimizations such as operator fusion, prefix caching, and view reuse.\nPreliminary experiments quantify the behavior of different refinement modes\ncompared to static prompts and agentic retries, as well as the impact of\nprompt-level optimizations such as operator fusion."
                },
                "authors": [
                    {
                        "name": "Ugur Cetintemel"
                    },
                    {
                        "name": "Shu Chen"
                    },
                    {
                        "name": "Alexander W. Lee"
                    },
                    {
                        "name": "Deepti Raghavan"
                    }
                ],
                "author_detail": {
                    "name": "Deepti Raghavan"
                },
                "author": "Deepti Raghavan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05012v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05012v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04449v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04449v2",
                "updated": "2025-08-06T16:57:39Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    16,
                    57,
                    39,
                    2,
                    218,
                    0
                ],
                "published": "2024-12-05T18:58:03Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    18,
                    58,
                    3,
                    3,
                    340,
                    0
                ],
                "title": "p-MoD: Building Mixture-of-Depths MLLMs via Progressive Ratio Decay",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "p-MoD: Building Mixture-of-Depths MLLMs via Progressive Ratio Decay"
                },
                "summary": "Despite the remarkable performance of multimodal large language models\n(MLLMs) across diverse tasks, the substantial training and inference costs\nimpede their advancement. In this paper, we propose p-MoD, an efficient MLLM\narchitecture that significantly reduces training and inference costs while\nmaintaining model performance. The majority of computation in MLLMs stems from\nthe overwhelming volume of vision tokens processed by the transformer-based\nLLM. Accordingly, we leverage the Mixture-of-Depths (MoD) mechanism, where each\nLLM layer selects essential vision tokens to process while skipping redundant\nones. However, integrating MoD into MLLMs is non-trivial. To address the\nchallenges of training and inference stability as well as limited training\ndata, we adapt the MoD module with two novel designs: tanh-gated weight\nnormalization (TanhNorm) and symmetric token reweighting (STRing). Moreover, we\nobserve that vision tokens exhibit higher redundancy in deeper layers and thus\ndesign a progressive ratio decay (PRD) strategy, which gradually reduces the\ntoken retention ratio layer by layer, employing a shifted cosine schedule. This\ncrucial design fully unleashes the potential of MoD, significantly boosting the\nefficiency and performance of our models. Extensive experiments on two baseline\nmodels across 15 benchmarks show that our model matches or even surpasses the\nperformance of corresponding baselines, while requiring only 55.6% TFLOPs and\n53.7% KV cache storage during inference, and 77.7% GPU hours during training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the remarkable performance of multimodal large language models\n(MLLMs) across diverse tasks, the substantial training and inference costs\nimpede their advancement. In this paper, we propose p-MoD, an efficient MLLM\narchitecture that significantly reduces training and inference costs while\nmaintaining model performance. The majority of computation in MLLMs stems from\nthe overwhelming volume of vision tokens processed by the transformer-based\nLLM. Accordingly, we leverage the Mixture-of-Depths (MoD) mechanism, where each\nLLM layer selects essential vision tokens to process while skipping redundant\nones. However, integrating MoD into MLLMs is non-trivial. To address the\nchallenges of training and inference stability as well as limited training\ndata, we adapt the MoD module with two novel designs: tanh-gated weight\nnormalization (TanhNorm) and symmetric token reweighting (STRing). Moreover, we\nobserve that vision tokens exhibit higher redundancy in deeper layers and thus\ndesign a progressive ratio decay (PRD) strategy, which gradually reduces the\ntoken retention ratio layer by layer, employing a shifted cosine schedule. This\ncrucial design fully unleashes the potential of MoD, significantly boosting the\nefficiency and performance of our models. Extensive experiments on two baseline\nmodels across 15 benchmarks show that our model matches or even surpasses the\nperformance of corresponding baselines, while requiring only 55.6% TFLOPs and\n53.7% KV cache storage during inference, and 77.7% GPU hours during training."
                },
                "authors": [
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Desen Meng"
                    },
                    {
                        "name": "Zhengming Zhang"
                    },
                    {
                        "name": "Zhenpeng Huang"
                    },
                    {
                        "name": "Tao Wu"
                    },
                    {
                        "name": "Limin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Limin Wang"
                },
                "author": "Limin Wang",
                "arxiv_comment": "Accepted by ICCV 2025; Code released at\n  https://github.com/MCG-NJU/p-MoD",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04449v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04449v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04581v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04581v1",
                "updated": "2025-08-06T16:06:43Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    16,
                    6,
                    43,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T16:06:43Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    16,
                    6,
                    43,
                    2,
                    218,
                    0
                ],
                "title": "Share Your Attention: Transformer Weight Sharing via Matrix-based\n  Dictionary Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Share Your Attention: Transformer Weight Sharing via Matrix-based\n  Dictionary Learning"
                },
                "summary": "Large language models (LLMs) have revolutionized AI applications, yet their\nhigh computational and memory demands hinder their widespread deployment.\nExisting compression techniques focus on intra-block optimizations (e.g.\nlow-rank approximation, attention head pruning), while the repetitive layered\nstructure of transformers implies significant inter-block redundancy - a\ndimension largely unexplored beyond key-value (KV) caching. Inspired by\ndictionary learning in CNNs, we propose a framework for structured weight\nsharing across transformer layers. Our approach decomposes attention projection\nmatrices into shared dictionary atoms, reducing the attention module's\nparameters by 66.7% while achieving on-par performance. Unlike complex methods\nrequiring distillation or architectural changes, MASA (Matrix Atom Sharing in\nAttention) operates as a drop-in replacement - trained with standard optimizers\n- and represents each layer's weights as linear combinations of shared matrix\natoms. Experiments across scales (100M-700M parameters) show that MASA achieves\nbetter benchmark accuracy and perplexity than grouped-query attention (GQA),\nlow-rank baselines and recently proposed Repeat-all-over/Sequential sharing at\ncomparable parameter budgets. Ablation studies confirm robustness to the\ndictionary size and the efficacy of shared representations in capturing\ncross-layer statistical regularities. Extending to Vision Transformers (ViT),\nMASA matches performance metrics on image classification and detection tasks\nwith 66.7% fewer attention parameters. By combining dictionary learning\nstrategies with transformer efficiency, MASA offers a scalable blueprint for\nparameter-efficient models without sacrificing performance. Finally, we\ninvestigate the possibility of employing MASA on pretrained LLMs to reduce\ntheir number of parameters without experiencing any significant drop in their\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have revolutionized AI applications, yet their\nhigh computational and memory demands hinder their widespread deployment.\nExisting compression techniques focus on intra-block optimizations (e.g.\nlow-rank approximation, attention head pruning), while the repetitive layered\nstructure of transformers implies significant inter-block redundancy - a\ndimension largely unexplored beyond key-value (KV) caching. Inspired by\ndictionary learning in CNNs, we propose a framework for structured weight\nsharing across transformer layers. Our approach decomposes attention projection\nmatrices into shared dictionary atoms, reducing the attention module's\nparameters by 66.7% while achieving on-par performance. Unlike complex methods\nrequiring distillation or architectural changes, MASA (Matrix Atom Sharing in\nAttention) operates as a drop-in replacement - trained with standard optimizers\n- and represents each layer's weights as linear combinations of shared matrix\natoms. Experiments across scales (100M-700M parameters) show that MASA achieves\nbetter benchmark accuracy and perplexity than grouped-query attention (GQA),\nlow-rank baselines and recently proposed Repeat-all-over/Sequential sharing at\ncomparable parameter budgets. Ablation studies confirm robustness to the\ndictionary size and the efficacy of shared representations in capturing\ncross-layer statistical regularities. Extending to Vision Transformers (ViT),\nMASA matches performance metrics on image classification and detection tasks\nwith 66.7% fewer attention parameters. By combining dictionary learning\nstrategies with transformer efficiency, MASA offers a scalable blueprint for\nparameter-efficient models without sacrificing performance. Finally, we\ninvestigate the possibility of employing MASA on pretrained LLMs to reduce\ntheir number of parameters without experiencing any significant drop in their\nperformance."
                },
                "authors": [
                    {
                        "name": "Magauiya Zhussip"
                    },
                    {
                        "name": "Dmitriy Shopkhoev"
                    },
                    {
                        "name": "Ammar Ali"
                    },
                    {
                        "name": "Stamatios Lefkimmiatis"
                    }
                ],
                "author_detail": {
                    "name": "Stamatios Lefkimmiatis"
                },
                "author": "Stamatios Lefkimmiatis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04581v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04581v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.01516v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.01516v3",
                "updated": "2025-08-06T15:38:06Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    15,
                    38,
                    6,
                    2,
                    218,
                    0
                ],
                "published": "2023-05-02T15:27:16Z",
                "published_parsed": [
                    2023,
                    5,
                    2,
                    15,
                    27,
                    16,
                    1,
                    122,
                    0
                ],
                "title": "From FASTER to F2: Evolving Concurrent Key-Value Store Designs for Large\n  Skewed Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From FASTER to F2: Evolving Concurrent Key-Value Store Designs for Large\n  Skewed Workloads"
                },
                "summary": "Modern large-scale services such as search engines, messaging platforms, and\nserverless functions, rely on key-value (KV) stores to maintain high\nperformance at scale. When such services are deployed in constrained memory\nenvironments, they present challenging requirements: point operations requiring\nhigh throughput, working sets much larger than main memory, and natural skew in\nkey access patterns. Traditional KV stores, based on LSM- and B-Trees, have\nbeen widely used to handle such use cases, but they often suffer from\nsuboptimal use of modern hardware resources. The FASTER project, developed as a\nhigh-performance open-source KV storage library, has demonstrated remarkable\nsuccess in both in-memory and hybrid storage environments. However, when tasked\nwith serving large skewed workloads, it faced challenges, including high\nindexing and compactions overheads, and inefficient management of\nnon-overlapping read-hot and write-hot working sets.\n  In this paper, we introduce F2 (for FASTER v2), an evolution of FASTER\ndesigned to meet the requirements of large skewed workloads common in industry\napplications. F2 adopts a two-tier record-oriented design to handle\nlarger-than-memory skewed workloads, along with new concurrent latch-free\nmechanisms and components to maximize performance on modern hardware. To\nrealize this design, F2 tackles key challenges and introduces several\ninnovations, including new latch-free algorithms for multi-threaded log\ncompaction, a two-level hash index to reduce indexing overhead for cold\nrecords, and a read-cache for serving read-hot records. Our evaluation shows\nthat F2 achieves 2-11.9x better throughput compared to existing KV stores,\neffectively serving the target workload. F2 is open-source and available as\npart of the FASTER project.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern large-scale services such as search engines, messaging platforms, and\nserverless functions, rely on key-value (KV) stores to maintain high\nperformance at scale. When such services are deployed in constrained memory\nenvironments, they present challenging requirements: point operations requiring\nhigh throughput, working sets much larger than main memory, and natural skew in\nkey access patterns. Traditional KV stores, based on LSM- and B-Trees, have\nbeen widely used to handle such use cases, but they often suffer from\nsuboptimal use of modern hardware resources. The FASTER project, developed as a\nhigh-performance open-source KV storage library, has demonstrated remarkable\nsuccess in both in-memory and hybrid storage environments. However, when tasked\nwith serving large skewed workloads, it faced challenges, including high\nindexing and compactions overheads, and inefficient management of\nnon-overlapping read-hot and write-hot working sets.\n  In this paper, we introduce F2 (for FASTER v2), an evolution of FASTER\ndesigned to meet the requirements of large skewed workloads common in industry\napplications. F2 adopts a two-tier record-oriented design to handle\nlarger-than-memory skewed workloads, along with new concurrent latch-free\nmechanisms and components to maximize performance on modern hardware. To\nrealize this design, F2 tackles key challenges and introduces several\ninnovations, including new latch-free algorithms for multi-threaded log\ncompaction, a two-level hash index to reduce indexing overhead for cold\nrecords, and a read-cache for serving read-hot records. Our evaluation shows\nthat F2 achieves 2-11.9x better throughput compared to existing KV stores,\neffectively serving the target workload. F2 is open-source and available as\npart of the FASTER project."
                },
                "authors": [
                    {
                        "name": "Konstantinos Kanellis"
                    },
                    {
                        "name": "Badrish Chandramouli"
                    },
                    {
                        "name": "Ted Hart"
                    },
                    {
                        "name": "Shivaram Venkataraman"
                    }
                ],
                "author_detail": {
                    "name": "Shivaram Venkataraman"
                },
                "author": "Shivaram Venkataraman",
                "arxiv_comment": "Proceedings of the VLDB Endowment 18 (VLDB'25)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.01516v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.01516v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04462v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04462v1",
                "updated": "2025-08-06T14:02:10Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    14,
                    2,
                    10,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T14:02:10Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    14,
                    2,
                    10,
                    2,
                    218,
                    0
                ],
                "title": "CARD: Cache-Assisted Parallel Speculative Decoding for Efficient Large\n  Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CARD: Cache-Assisted Parallel Speculative Decoding for Efficient Large\n  Language Model Inference"
                },
                "summary": "Speculative decoding (SD), where an extra draft model first provides multiple\ndraft tokens and the original target model then verifies these tokens in\nparallel, has shown great power for LLM inference acceleration. However,\nexisting SD methods must adhere to the 'draft-then-verify' paradigm, which\nforces drafting and verification processes to execute sequentially during SD,\nresulting in inefficient inference performance and limiting the size of the\ndraft model. Furthermore, once a single token in the candidate sequence is\nrejected during the drafting process, all subsequent candidate tokens must be\ndiscarded, leading to inefficient drafting. To address these challenges, we\npropose a cache-based parallel speculative decoding framework employing a\n'query-and-correct' paradigm. Specifically, CARD decouples drafting and\nverification: the draft model generates candidate tokens to populate a shared\ncache, while the target model concurrently rectifies the draft model's\ngeneration direction. This effectively enables the target model to perform\ninference at speed approaching that of the draft model. Our approach achieves\nup to 4.83 speedup over vanilla decoding without requiring fine-tuning of\neither the draft or target models. Our code is available at\nhttps://github.com/hunzhizi/CARD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding (SD), where an extra draft model first provides multiple\ndraft tokens and the original target model then verifies these tokens in\nparallel, has shown great power for LLM inference acceleration. However,\nexisting SD methods must adhere to the 'draft-then-verify' paradigm, which\nforces drafting and verification processes to execute sequentially during SD,\nresulting in inefficient inference performance and limiting the size of the\ndraft model. Furthermore, once a single token in the candidate sequence is\nrejected during the drafting process, all subsequent candidate tokens must be\ndiscarded, leading to inefficient drafting. To address these challenges, we\npropose a cache-based parallel speculative decoding framework employing a\n'query-and-correct' paradigm. Specifically, CARD decouples drafting and\nverification: the draft model generates candidate tokens to populate a shared\ncache, while the target model concurrently rectifies the draft model's\ngeneration direction. This effectively enables the target model to perform\ninference at speed approaching that of the draft model. Our approach achieves\nup to 4.83 speedup over vanilla decoding without requiring fine-tuning of\neither the draft or target models. Our code is available at\nhttps://github.com/hunzhizi/CARD."
                },
                "authors": [
                    {
                        "name": "Enyu Zhou"
                    },
                    {
                        "name": "Kai Sheng"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Xin He"
                    }
                ],
                "author_detail": {
                    "name": "Xin He"
                },
                "author": "Xin He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04462v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04462v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24007v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24007v3",
                "updated": "2025-08-06T11:46:11Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    11,
                    46,
                    11,
                    2,
                    218,
                    0
                ],
                "published": "2025-03-31T12:32:23Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    12,
                    32,
                    23,
                    0,
                    90,
                    0
                ],
                "title": "CITRAS: Covariate-Informed Transformer for Time Series Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CITRAS: Covariate-Informed Transformer for Time Series Forecasting"
                },
                "summary": "In practical time series forecasting, covariates provide rich contextual\ninformation that can potentially enhance the forecast of target variables.\nAlthough some covariates extend into the future forecasting horizon (e.g.,\ncalendar events, discount schedules), most multivariate models fail to leverage\nthis pivotal insight due to the length discrepancy with target variables.\nAdditionally, capturing the dependency between target variables and covariates\nis non-trivial, as models must precisely reflect the local impact of covariates\nwhile also capturing global cross-variate dependencies. To overcome these\nchallenges, we propose CITRAS, a decoder-only Transformer that flexibly\nleverages multiple targets, past covariates, and future covariates. While\npreserving strong autoregressive capabilities, CITRAS introduces two novel\nmechanisms in patch-wise cross-variate attention: Key-Value (KV) Shift and\nAttention Score Smoothing. KV Shift seamlessly incorporates future covariates\ninto the forecasting of target variables based on their concurrent\ndependencies. Additionally, Attention Score Smoothing refines locally accurate\npatch-wise cross-variate dependencies into global variate-level dependencies by\nsmoothing the past series of attention scores. Experimentally, CITRAS\noutperforms state-of-the-art models on thirteen real-world benchmarks from both\ncovariate-informed and multivariate settings, demonstrating its versatile\nability to leverage cross-variate and cross-time dependencies for improved\nforecasting accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In practical time series forecasting, covariates provide rich contextual\ninformation that can potentially enhance the forecast of target variables.\nAlthough some covariates extend into the future forecasting horizon (e.g.,\ncalendar events, discount schedules), most multivariate models fail to leverage\nthis pivotal insight due to the length discrepancy with target variables.\nAdditionally, capturing the dependency between target variables and covariates\nis non-trivial, as models must precisely reflect the local impact of covariates\nwhile also capturing global cross-variate dependencies. To overcome these\nchallenges, we propose CITRAS, a decoder-only Transformer that flexibly\nleverages multiple targets, past covariates, and future covariates. While\npreserving strong autoregressive capabilities, CITRAS introduces two novel\nmechanisms in patch-wise cross-variate attention: Key-Value (KV) Shift and\nAttention Score Smoothing. KV Shift seamlessly incorporates future covariates\ninto the forecasting of target variables based on their concurrent\ndependencies. Additionally, Attention Score Smoothing refines locally accurate\npatch-wise cross-variate dependencies into global variate-level dependencies by\nsmoothing the past series of attention scores. Experimentally, CITRAS\noutperforms state-of-the-art models on thirteen real-world benchmarks from both\ncovariate-informed and multivariate settings, demonstrating its versatile\nability to leverage cross-variate and cross-time dependencies for improved\nforecasting accuracy."
                },
                "authors": [
                    {
                        "name": "Yosuke Yamaguchi"
                    },
                    {
                        "name": "Issei Suemitsu"
                    },
                    {
                        "name": "Wenpeng Wei"
                    }
                ],
                "author_detail": {
                    "name": "Wenpeng Wei"
                },
                "author": "Wenpeng Wei",
                "arxiv_comment": "Submission under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24007v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24007v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04257v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04257v1",
                "updated": "2025-08-06T09:40:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    9,
                    40,
                    9,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T09:40:09Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    9,
                    40,
                    9,
                    2,
                    218,
                    0
                ],
                "title": "KVSink: Understanding and Enhancing the Preservation of Attention Sinks\n  in KV Cache Quantization for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVSink: Understanding and Enhancing the Preservation of Attention Sinks\n  in KV Cache Quantization for LLMs"
                },
                "summary": "Key-Value (KV) cache quantization has become a widely adopted optimization\ntechnique for efficient large language models (LLMs) inference by reducing KV\ncache memory usage and mitigating memory-bound constraints. Recent studies have\nemphasized the importance of preserving the original precision of KVs for the\nfirst few tokens to ensure the protection of attention sinks. While this\napproach has proven effective in mitigating performance degradation, its\nunderlying principles remain insufficiently understood. Moreover, it fails to\naddress the recent discovery that attention sinks can emerge beyond the initial\ntoken positions. In this work, we elucidate the underlying mechanisms of\nattention sinks during inference by examining their role in the cross-layer\nevolution of extreme activation outliers. Additionally, we provide a\ncomprehensive analysis of the interplay between attention sinks and KV cache\nquantization. Based on our enhanced understanding, we introduce\n\\textit{\\textbf{KVSink}}, a plug-and-play method that effectively predicts sink\ntokens with negligible overhead, enabling more thorough preservation. Extensive\nexperiments demonstrate that KVSink outperforms the existing Preserve-First-N\n(PFN) strategy, offering more effective preservation of attention sinks during\nKV cache quantization. Moreover, when applied to the well-established KVQuant\nmethod, KVSink further improves perplexity (PPL) and reduces reliance on 16-bit\nnumerical outliers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value (KV) cache quantization has become a widely adopted optimization\ntechnique for efficient large language models (LLMs) inference by reducing KV\ncache memory usage and mitigating memory-bound constraints. Recent studies have\nemphasized the importance of preserving the original precision of KVs for the\nfirst few tokens to ensure the protection of attention sinks. While this\napproach has proven effective in mitigating performance degradation, its\nunderlying principles remain insufficiently understood. Moreover, it fails to\naddress the recent discovery that attention sinks can emerge beyond the initial\ntoken positions. In this work, we elucidate the underlying mechanisms of\nattention sinks during inference by examining their role in the cross-layer\nevolution of extreme activation outliers. Additionally, we provide a\ncomprehensive analysis of the interplay between attention sinks and KV cache\nquantization. Based on our enhanced understanding, we introduce\n\\textit{\\textbf{KVSink}}, a plug-and-play method that effectively predicts sink\ntokens with negligible overhead, enabling more thorough preservation. Extensive\nexperiments demonstrate that KVSink outperforms the existing Preserve-First-N\n(PFN) strategy, offering more effective preservation of attention sinks during\nKV cache quantization. Moreover, when applied to the well-established KVQuant\nmethod, KVSink further improves perplexity (PPL) and reduces reliance on 16-bit\nnumerical outliers."
                },
                "authors": [
                    {
                        "name": "Zunhai Su"
                    },
                    {
                        "name": "Kehong Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Kehong Yuan"
                },
                "author": "Kehong Yuan",
                "arxiv_comment": "Published as a conference paper at COLM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04257v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04257v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00370v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00370v2",
                "updated": "2025-08-06T08:32:53Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    8,
                    32,
                    53,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-01T07:03:16Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    7,
                    3,
                    16,
                    4,
                    213,
                    0
                ],
                "title": "EdgeInfinite-Instruct: Bridging SFT-Based Optimization and NPU-Level\n  Efficiency for Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EdgeInfinite-Instruct: Bridging SFT-Based Optimization and NPU-Level\n  Efficiency for Edge Devices"
                },
                "summary": "Deploying Transformer-based large language models (LLMs) on\nresource-constrained edge devices for long-sequence tasks remains challenging\ndue to the quadratic time complexity of self-attention and growing Key-Value\n(KV) cache demands. While existing KV cache optimizations improve memory\nefficiency, they often fail to reduce time to first token (TTFT) and may\ndegrade performance through token pruning. Alternative sequence modeling\narchitectures address some of these limitations, but typically require full\nretraining and lack infrastructure support. EdgeInfinite offers an efficient\nsolution by fine-tuning only a small subset of parameters, maintaining quality\nwhile reducing both computational and memory costs, including improved TTFT.\nHowever, its instruction-following ability is limited, and it lacks\nmobile-specific optimizations. To address these issues, we propose\nEdgeInfinite-Instruct, which introduces a Segmented Supervised Fine-Tuning\n(S-SFT) strategy tailored to long-sequence tasks such as summarization and\nquestion answering. We further optimized EdgeInfinite-Instruct for efficient\ndeployment on edge NPUs by employing fine-grained post-training quantization\n(PTQ) to reduce computational demands while maintaining accuracy, and by\nimplementing a fixed-shape computation graph that balances memory usage and\non-device efficiency through scenario-specific customization of input token and\ncache sizes. Experiments on long-context benchmarks and real-world mobile tasks\nshow that our approach improves domain-specific performance while maintaining\nefficiency on NPU-accelerated edge devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying Transformer-based large language models (LLMs) on\nresource-constrained edge devices for long-sequence tasks remains challenging\ndue to the quadratic time complexity of self-attention and growing Key-Value\n(KV) cache demands. While existing KV cache optimizations improve memory\nefficiency, they often fail to reduce time to first token (TTFT) and may\ndegrade performance through token pruning. Alternative sequence modeling\narchitectures address some of these limitations, but typically require full\nretraining and lack infrastructure support. EdgeInfinite offers an efficient\nsolution by fine-tuning only a small subset of parameters, maintaining quality\nwhile reducing both computational and memory costs, including improved TTFT.\nHowever, its instruction-following ability is limited, and it lacks\nmobile-specific optimizations. To address these issues, we propose\nEdgeInfinite-Instruct, which introduces a Segmented Supervised Fine-Tuning\n(S-SFT) strategy tailored to long-sequence tasks such as summarization and\nquestion answering. We further optimized EdgeInfinite-Instruct for efficient\ndeployment on edge NPUs by employing fine-grained post-training quantization\n(PTQ) to reduce computational demands while maintaining accuracy, and by\nimplementing a fixed-shape computation graph that balances memory usage and\non-device efficiency through scenario-specific customization of input token and\ncache sizes. Experiments on long-context benchmarks and real-world mobile tasks\nshow that our approach improves domain-specific performance while maintaining\nefficiency on NPU-accelerated edge devices."
                },
                "authors": [
                    {
                        "name": "Jiyu Chen"
                    },
                    {
                        "name": "Poh Seng Lim"
                    },
                    {
                        "name": "Shuang Peng"
                    },
                    {
                        "name": "Daxiong Luo"
                    },
                    {
                        "name": "JungHau Foo"
                    },
                    {
                        "name": "Yap Deep"
                    },
                    {
                        "name": "Timothy Lee Jun Jie"
                    },
                    {
                        "name": "Kelvin Teh Kae Wen"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Danyu Feng"
                    },
                    {
                        "name": "Hao-Yun Chen"
                    },
                    {
                        "name": "Peng-Wen Chen"
                    },
                    {
                        "name": "Fangyuan Li"
                    },
                    {
                        "name": "Xiaoxin Chen"
                    },
                    {
                        "name": "Wong Wai Mun"
                    }
                ],
                "author_detail": {
                    "name": "Wong Wai Mun"
                },
                "author": "Wong Wai Mun",
                "arxiv_comment": "The data and method in the paper need to be re-audited",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00370v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00370v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.03837v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.03837v1",
                "updated": "2025-08-05T18:34:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    5,
                    18,
                    34,
                    48,
                    1,
                    217,
                    0
                ],
                "published": "2025-08-05T18:34:48Z",
                "published_parsed": [
                    2025,
                    8,
                    5,
                    18,
                    34,
                    48,
                    1,
                    217,
                    0
                ],
                "title": "Rhea: a Framework for Fast Design and Validation of RTL Cache-Coherent\n  Memory Subsystems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rhea: a Framework for Fast Design and Validation of RTL Cache-Coherent\n  Memory Subsystems"
                },
                "summary": "Designing and validating efficient cache-coherent memory subsystems is a\ncritical yet complex task in the development of modern multi-core\nsystem-on-chip architectures. Rhea is a unified framework that streamlines the\ndesign and system-level validation of RTL cache-coherent memory subsystems. On\nthe design side, Rhea generates synthesizable, highly configurable RTL\nsupporting various architectural parameters. On the validation side, Rhea\nintegrates Verilator's cycle-accurate RTL simulation with gem5's full-system\nsimulation, allowing realistic workloads and operating systems to run alongside\nthe actual RTL under test. We apply Rhea to design MSI-based RTL memory\nsubsystems with one and two levels of private caches and scaling up to sixteen\ncores. Their evaluation with 22 applications from state-of-the-art benchmark\nsuites shows intermediate performance relative to gem5 Ruby's MI and MOESI\nmodels. The hybrid gem5-Verilator co-simulation flow incurs a moderate\nsimulation overhead, up to 2.7 times compared to gem5 MI, but achieves higher\nfidelity by simulating real RTL hardware. This overhead decreases with scale,\ndown to 1.6 times in sixteen-core scenarios. These results demonstrate Rhea's\neffectiveness and scalability in enabling fast development of RTL\ncache-coherent memory subsystem designs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Designing and validating efficient cache-coherent memory subsystems is a\ncritical yet complex task in the development of modern multi-core\nsystem-on-chip architectures. Rhea is a unified framework that streamlines the\ndesign and system-level validation of RTL cache-coherent memory subsystems. On\nthe design side, Rhea generates synthesizable, highly configurable RTL\nsupporting various architectural parameters. On the validation side, Rhea\nintegrates Verilator's cycle-accurate RTL simulation with gem5's full-system\nsimulation, allowing realistic workloads and operating systems to run alongside\nthe actual RTL under test. We apply Rhea to design MSI-based RTL memory\nsubsystems with one and two levels of private caches and scaling up to sixteen\ncores. Their evaluation with 22 applications from state-of-the-art benchmark\nsuites shows intermediate performance relative to gem5 Ruby's MI and MOESI\nmodels. The hybrid gem5-Verilator co-simulation flow incurs a moderate\nsimulation overhead, up to 2.7 times compared to gem5 MI, but achieves higher\nfidelity by simulating real RTL hardware. This overhead decreases with scale,\ndown to 1.6 times in sixteen-core scenarios. These results demonstrate Rhea's\neffectiveness and scalability in enabling fast development of RTL\ncache-coherent memory subsystem designs."
                },
                "authors": [
                    {
                        "name": "Davide Zoni"
                    },
                    {
                        "name": "Andrea Galimberti"
                    },
                    {
                        "name": "Adriano Guarisco"
                    }
                ],
                "author_detail": {
                    "name": "Adriano Guarisco"
                },
                "author": "Adriano Guarisco",
                "arxiv_comment": "9 pages, 13 figures, 1 table, accepted for presentation at 2025\n  International Conference on Computer-Aided Design (ICCAD), Munich, Germany,\n  October 26-30, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.03837v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.03837v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07120v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07120v2",
                "updated": "2025-08-05T16:17:01Z",
                "updated_parsed": [
                    2025,
                    8,
                    5,
                    16,
                    17,
                    1,
                    1,
                    217,
                    0
                ],
                "published": "2025-03-10T09:49:18Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    9,
                    49,
                    18,
                    0,
                    69,
                    0
                ],
                "title": "FEB-Cache: Frequency-Guided Exposure Bias Reduction for Enhancing\n  Diffusion Transformer Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FEB-Cache: Frequency-Guided Exposure Bias Reduction for Enhancing\n  Diffusion Transformer Caching"
                },
                "summary": "Diffusion Transformer (DiT) has exhibited impressive generation capabilities\nbut faces great challenges due to its high computational complexity. To address\nthis issue, various methods, notably feature caching, have been introduced.\nHowever, these approaches focus on aligning non-cache diffusion without\nanalyzing why caching damage the generation processes. In this paper, we first\nconfirm that the cache greatly amplifies the exposure bias, resulting in a\ndecline in the generation quality. However, directly applying noise scaling is\nchallenging for this issue due to the non-smoothness of exposure bias. We found\nthat this phenomenon stems from the mismatch between its frequency response\ncharacteristics and the simple cache of Attention and MLP. Since these two\ncomponents exhibit unique preferences for frequency signals, which provides us\nwith a caching strategy to separate Attention and MLP to achieve an enhanced\nfit of exposure bias and reduce it. Based on this, we introduced FEB-Cache, a\njoint caching strategy that aligns with the non-exposed bias diffusion process\n(which gives us a higher performance cap) of caching Attention and MLP based on\nthe frequency-guided cache table. Our approach combines a comprehensive\nunderstanding of the caching mechanism and offers a new perspective on\nleveraging caching to accelerate the diffusion process. Empirical results\nindicate that FEB-Cache optimizes model performance while concurrently\nfacilitating acceleration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformer (DiT) has exhibited impressive generation capabilities\nbut faces great challenges due to its high computational complexity. To address\nthis issue, various methods, notably feature caching, have been introduced.\nHowever, these approaches focus on aligning non-cache diffusion without\nanalyzing why caching damage the generation processes. In this paper, we first\nconfirm that the cache greatly amplifies the exposure bias, resulting in a\ndecline in the generation quality. However, directly applying noise scaling is\nchallenging for this issue due to the non-smoothness of exposure bias. We found\nthat this phenomenon stems from the mismatch between its frequency response\ncharacteristics and the simple cache of Attention and MLP. Since these two\ncomponents exhibit unique preferences for frequency signals, which provides us\nwith a caching strategy to separate Attention and MLP to achieve an enhanced\nfit of exposure bias and reduce it. Based on this, we introduced FEB-Cache, a\njoint caching strategy that aligns with the non-exposed bias diffusion process\n(which gives us a higher performance cap) of caching Attention and MLP based on\nthe frequency-guided cache table. Our approach combines a comprehensive\nunderstanding of the caching mechanism and offers a new perspective on\nleveraging caching to accelerate the diffusion process. Empirical results\nindicate that FEB-Cache optimizes model performance while concurrently\nfacilitating acceleration."
                },
                "authors": [
                    {
                        "name": "Zhen Zou"
                    },
                    {
                        "name": "Feng Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Feng Zhao"
                },
                "author": "Feng Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07120v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07120v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.03321v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.03321v1",
                "updated": "2025-08-05T11:00:41Z",
                "updated_parsed": [
                    2025,
                    8,
                    5,
                    11,
                    0,
                    41,
                    1,
                    217,
                    0
                ],
                "published": "2025-08-05T11:00:41Z",
                "published_parsed": [
                    2025,
                    8,
                    5,
                    11,
                    0,
                    41,
                    1,
                    217,
                    0
                ],
                "title": "Bidirectional TLS Handshake Caching for Constrained Industrial IoT\n  Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bidirectional TLS Handshake Caching for Constrained Industrial IoT\n  Scenarios"
                },
                "summary": "While TLS has become the de-facto standard for end-to-end security, its use\nto secure critical communication in evolving industrial IoT scenarios is\nseverely limited by prevalent resource constraints of devices and networks.\nMost notably, the TLS handshake to establish secure connections incurs\nsignificant bandwidth and processing overhead that often cannot be handled in\nconstrained environments. To alleviate this situation, we present BiTHaC which\nrealizes bidirectional TLS handshake caching by exploiting that significant\nparts of repeated TLS handshakes, especially certificates, are static. Thus,\nredundant information neither needs to be transmitted nor corresponding\ncomputations performed, saving valuable bandwidth and processing resources. By\nimplementing BiTHaC for wolfSSL, we show that we can reduce the bandwidth\nconsumption of TLS handshakes by up to 61.1% and the computational overhead by\nup to 8.5%, while incurring only well-manageable memory overhead and preserving\nthe strict security guarantees of TLS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While TLS has become the de-facto standard for end-to-end security, its use\nto secure critical communication in evolving industrial IoT scenarios is\nseverely limited by prevalent resource constraints of devices and networks.\nMost notably, the TLS handshake to establish secure connections incurs\nsignificant bandwidth and processing overhead that often cannot be handled in\nconstrained environments. To alleviate this situation, we present BiTHaC which\nrealizes bidirectional TLS handshake caching by exploiting that significant\nparts of repeated TLS handshakes, especially certificates, are static. Thus,\nredundant information neither needs to be transmitted nor corresponding\ncomputations performed, saving valuable bandwidth and processing resources. By\nimplementing BiTHaC for wolfSSL, we show that we can reduce the bandwidth\nconsumption of TLS handshakes by up to 61.1% and the computational overhead by\nup to 8.5%, while incurring only well-manageable memory overhead and preserving\nthe strict security guarantees of TLS."
                },
                "authors": [
                    {
                        "name": "Jrn Bodenhausen"
                    },
                    {
                        "name": "Simon Mangel"
                    },
                    {
                        "name": "Thomas Vogt"
                    },
                    {
                        "name": "Martin Henze"
                    }
                ],
                "author_detail": {
                    "name": "Martin Henze"
                },
                "author": "Martin Henze",
                "arxiv_comment": "Accepted for publication in Proceedings of the 2025 IEEE 50th\n  Conference on Local Computer Networks (LCN)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.03321v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.03321v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.03258v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.03258v1",
                "updated": "2025-08-05T09:35:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    5,
                    9,
                    35,
                    52,
                    1,
                    217,
                    0
                ],
                "published": "2025-08-05T09:35:52Z",
                "published_parsed": [
                    2025,
                    8,
                    5,
                    9,
                    35,
                    52,
                    1,
                    217,
                    0
                ],
                "title": "SmartLLMs Scheduler: A Framework for Cost-Effective LLMs Utilization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SmartLLMs Scheduler: A Framework for Cost-Effective LLMs Utilization"
                },
                "summary": "Large Language Models (LLMs) such as GPT-4 and Llama have shown remarkable\ncapabilities in a variety of software engineering tasks. Despite the\nadvancements, their practical deployment faces challenges, including high\nfinancial costs, long response time, and varying performance, especially when\nhandling a large number of queries (jobs). Existing optimization strategies for\ndeploying LLMs for diverse tasks focus on static scheduling, which requires\nextensive training data for performance prediction, increasing the\ncomputational costs and limiting the applicability and flexibility. In this\npaper, we propose the SmartLLMs Scheduler (SLS), a dynamic and cost-effective\nscheduling solution. The key idea is to learn LLMs' performance on diverse\ntasks and incorporate their real-time feedback to update strategies\nperiodically. Specifically, SLS incorporates three key components, including an\nAdaptive Cache Manager, a Performance-Cost Optimized Scheduler, and a Dynamic\nUpdate Manager. The Cache Manager stores the outputs of previously processed\nqueries and employs an adaptive strategy to reduce redundant computations and\nminimize response times. For queries not found in the cache, the Scheduler\ndynamically allocates them to the most suitable LLM based on the predicted\nperformance and cost from models that take both query-specific and LLM-specific\nfeatures as input. The Update Manager continuously refines the cache and\nscheduling strategies based on real-time feedback from the assigned queries to\nenhance decision-making and adapt to evolving task characteristics. To evaluate\nthe effectiveness of SLS, we conduct extensive experiments on two LLM-based\nsoftware engineering tasks, including log parsing and code generation. The\nresults show that SLS significantly outperforms the baseline methods, achieving\nan average performance improvement of 198.82% and an average processing time\nreduction of 63.28%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) such as GPT-4 and Llama have shown remarkable\ncapabilities in a variety of software engineering tasks. Despite the\nadvancements, their practical deployment faces challenges, including high\nfinancial costs, long response time, and varying performance, especially when\nhandling a large number of queries (jobs). Existing optimization strategies for\ndeploying LLMs for diverse tasks focus on static scheduling, which requires\nextensive training data for performance prediction, increasing the\ncomputational costs and limiting the applicability and flexibility. In this\npaper, we propose the SmartLLMs Scheduler (SLS), a dynamic and cost-effective\nscheduling solution. The key idea is to learn LLMs' performance on diverse\ntasks and incorporate their real-time feedback to update strategies\nperiodically. Specifically, SLS incorporates three key components, including an\nAdaptive Cache Manager, a Performance-Cost Optimized Scheduler, and a Dynamic\nUpdate Manager. The Cache Manager stores the outputs of previously processed\nqueries and employs an adaptive strategy to reduce redundant computations and\nminimize response times. For queries not found in the cache, the Scheduler\ndynamically allocates them to the most suitable LLM based on the predicted\nperformance and cost from models that take both query-specific and LLM-specific\nfeatures as input. The Update Manager continuously refines the cache and\nscheduling strategies based on real-time feedback from the assigned queries to\nenhance decision-making and adapt to evolving task characteristics. To evaluate\nthe effectiveness of SLS, we conduct extensive experiments on two LLM-based\nsoftware engineering tasks, including log parsing and code generation. The\nresults show that SLS significantly outperforms the baseline methods, achieving\nan average performance improvement of 198.82% and an average processing time\nreduction of 63.28%."
                },
                "authors": [
                    {
                        "name": "Yueyue Liu"
                    },
                    {
                        "name": "Hongyu Zhang"
                    },
                    {
                        "name": "Yuantian Miao"
                    }
                ],
                "author_detail": {
                    "name": "Yuantian Miao"
                },
                "author": "Yuantian Miao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.03258v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.03258v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02240v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02240v2",
                "updated": "2025-08-05T02:13:39Z",
                "updated_parsed": [
                    2025,
                    8,
                    5,
                    2,
                    13,
                    39,
                    1,
                    217,
                    0
                ],
                "published": "2025-08-04T09:39:31Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    9,
                    39,
                    31,
                    0,
                    216,
                    0
                ],
                "title": "Forecasting When to Forecast: Accelerating Diffusion Models with\n  Confidence-Gated Taylor",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forecasting When to Forecast: Accelerating Diffusion Models with\n  Confidence-Gated Taylor"
                },
                "summary": "Diffusion Transformers (DiTs) have demonstrated remarkable performance in\nvisual generation tasks. However, their low inference speed limits their\ndeployment in low-resource applications. Recent training-free approaches\nexploit the redundancy of features across timesteps by caching and reusing past\nrepresentations to accelerate inference. Building on this idea, TaylorSeer\ninstead uses cached features to predict future ones via Taylor expansion.\nHowever, its module-level prediction across all transformer blocks (e.g.,\nattention or feedforward modules) requires storing fine-grained intermediate\nfeatures, leading to notable memory and computation overhead. Moreover, it\nadopts a fixed caching schedule without considering the varying accuracy of\npredictions across timesteps, which can lead to degraded outputs when\nprediction fails. To address these limitations, we propose a novel approach to\nbetter leverage Taylor-based acceleration. First, we shift the Taylor\nprediction target from the module level to the last block level, significantly\nreducing the number of cached features. Furthermore, observing strong\nsequential dependencies among Transformer blocks, we propose to use the error\nbetween the Taylor-estimated and actual outputs of the first block as an\nindicator of prediction reliability. If the error is small, we trust the Taylor\nprediction for the last block; otherwise, we fall back to full computation,\nthereby enabling a dynamic caching mechanism. Empirical results show that our\nmethod achieves a better balance between speed and quality, achieving a 3.17x\nacceleration on FLUX, 2.36x on DiT, and 4.14x on Wan Video with negligible\nquality drop. The Project Page is\n\\href{https://cg-taylor-acce.github.io/CG-Taylor/}{here.}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) have demonstrated remarkable performance in\nvisual generation tasks. However, their low inference speed limits their\ndeployment in low-resource applications. Recent training-free approaches\nexploit the redundancy of features across timesteps by caching and reusing past\nrepresentations to accelerate inference. Building on this idea, TaylorSeer\ninstead uses cached features to predict future ones via Taylor expansion.\nHowever, its module-level prediction across all transformer blocks (e.g.,\nattention or feedforward modules) requires storing fine-grained intermediate\nfeatures, leading to notable memory and computation overhead. Moreover, it\nadopts a fixed caching schedule without considering the varying accuracy of\npredictions across timesteps, which can lead to degraded outputs when\nprediction fails. To address these limitations, we propose a novel approach to\nbetter leverage Taylor-based acceleration. First, we shift the Taylor\nprediction target from the module level to the last block level, significantly\nreducing the number of cached features. Furthermore, observing strong\nsequential dependencies among Transformer blocks, we propose to use the error\nbetween the Taylor-estimated and actual outputs of the first block as an\nindicator of prediction reliability. If the error is small, we trust the Taylor\nprediction for the last block; otherwise, we fall back to full computation,\nthereby enabling a dynamic caching mechanism. Empirical results show that our\nmethod achieves a better balance between speed and quality, achieving a 3.17x\nacceleration on FLUX, 2.36x on DiT, and 4.14x on Wan Video with negligible\nquality drop. The Project Page is\n\\href{https://cg-taylor-acce.github.io/CG-Taylor/}{here.}"
                },
                "authors": [
                    {
                        "name": "Xiaoliu Guan"
                    },
                    {
                        "name": "Lielin Jiang"
                    },
                    {
                        "name": "Hanqi Chen"
                    },
                    {
                        "name": "Xu Zhang"
                    },
                    {
                        "name": "Jiaxing Yan"
                    },
                    {
                        "name": "Guanzhong Wang"
                    },
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Zetao Zhang"
                    },
                    {
                        "name": "Yu Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wu"
                },
                "author": "Yu Wu",
                "arxiv_comment": "15 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02240v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02240v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14866v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14866v5",
                "updated": "2025-08-05T00:25:53Z",
                "updated_parsed": [
                    2025,
                    8,
                    5,
                    0,
                    25,
                    53,
                    1,
                    217,
                    0
                ],
                "published": "2025-04-21T05:27:33Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    5,
                    27,
                    33,
                    0,
                    111,
                    0
                ],
                "title": "GainSight: A Unified Framework for Data Lifetime Profiling and\n  Heterogeneous Memory Composition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GainSight: A Unified Framework for Data Lifetime Profiling and\n  Heterogeneous Memory Composition"
                },
                "summary": "As AI workloads drive increasing memory requirements, domain-specific\naccelerators need higher-density on-chip memory beyond what current SRAM\nscaling trends can provide. Simultaneously, the vast amounts of short-lived\ndata in these workloads make SRAM overprovisioned in retention capability. To\naddress this mismatch, we propose a wholesale shift from uniform SRAM arrays to\nheterogeneous on-chip memory, incorporating denser short-term RAM (StRAM)\ndevices whose limited retention times align with transient data lifetimes. To\nfacilitate this shift, we introduce GainSight, the first comprehensive,\nopen-source framework that aligns dynamic, fine-grained workload lifetime\nprofiles with memory device characteristics to enable generation of optimal\nStRAM memory compositions. GainSight combines retargetable profiling backends\nwith an architecture-agnostic analytical frontend. The various backends capture\ncycle-accurate data lifetimes, while the frontend correlates workload patterns\nwith StRAM retention properties to generate optimal memory compositions and\nproject performance. GainSight elevates data lifetime to a first-class design\nconsideration for next-generation AI accelerators, enabling systematic\nexploitation of data transience for improved on-chip memory density and\nefficiency. Applying GainSight to MLPerf Inference and PolyBench workloads\nreveals that 64.3% of first-level GPU cache accesses and 79.01% of systolic\narray scratchpad accesses exhibit sub-microsecond lifetimes suitable for\nhigh-density StRAM, with optimal heterogeneous on-chip memory compositions\nachieving up to 3x active energy and 4x area reductions compared to uniform\nSRAM hierarchies. To facilitate adoption and further research, GainSight is\nopen-sourced at https://gainsight.stanford.edu/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As AI workloads drive increasing memory requirements, domain-specific\naccelerators need higher-density on-chip memory beyond what current SRAM\nscaling trends can provide. Simultaneously, the vast amounts of short-lived\ndata in these workloads make SRAM overprovisioned in retention capability. To\naddress this mismatch, we propose a wholesale shift from uniform SRAM arrays to\nheterogeneous on-chip memory, incorporating denser short-term RAM (StRAM)\ndevices whose limited retention times align with transient data lifetimes. To\nfacilitate this shift, we introduce GainSight, the first comprehensive,\nopen-source framework that aligns dynamic, fine-grained workload lifetime\nprofiles with memory device characteristics to enable generation of optimal\nStRAM memory compositions. GainSight combines retargetable profiling backends\nwith an architecture-agnostic analytical frontend. The various backends capture\ncycle-accurate data lifetimes, while the frontend correlates workload patterns\nwith StRAM retention properties to generate optimal memory compositions and\nproject performance. GainSight elevates data lifetime to a first-class design\nconsideration for next-generation AI accelerators, enabling systematic\nexploitation of data transience for improved on-chip memory density and\nefficiency. Applying GainSight to MLPerf Inference and PolyBench workloads\nreveals that 64.3% of first-level GPU cache accesses and 79.01% of systolic\narray scratchpad accesses exhibit sub-microsecond lifetimes suitable for\nhigh-density StRAM, with optimal heterogeneous on-chip memory compositions\nachieving up to 3x active energy and 4x area reductions compared to uniform\nSRAM hierarchies. To facilitate adoption and further research, GainSight is\nopen-sourced at https://gainsight.stanford.edu/."
                },
                "authors": [
                    {
                        "name": "Peijing Li"
                    },
                    {
                        "name": "Matthew Hung"
                    },
                    {
                        "name": "Yiming Tan"
                    },
                    {
                        "name": "Konstantin Hofeld"
                    },
                    {
                        "name": "Jake Cheng Jiajun"
                    },
                    {
                        "name": "Shuhan Liu"
                    },
                    {
                        "name": "Lixian Yan"
                    },
                    {
                        "name": "Xinxin Wang"
                    },
                    {
                        "name": "Philip Levis"
                    },
                    {
                        "name": "H. -S. Philip Wong"
                    },
                    {
                        "name": "Thierry Tambe"
                    }
                ],
                "author_detail": {
                    "name": "Thierry Tambe"
                },
                "author": "Thierry Tambe",
                "arxiv_comment": "14 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14866v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14866v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.7.1; B.3.1; C.3; I.6; I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02558v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02558v1",
                "updated": "2025-08-04T16:14:03Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    16,
                    14,
                    3,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T16:14:03Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    16,
                    14,
                    3,
                    0,
                    216,
                    0
                ],
                "title": "Sparse-dLLM: Accelerating Diffusion LLMs with Dynamic Cache Eviction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse-dLLM: Accelerating Diffusion LLMs with Dynamic Cache Eviction"
                },
                "summary": "Diffusion Large Language Models (dLLMs) enable breakthroughs in reasoning and\nparallel decoding but suffer from prohibitive quadratic computational\ncomplexity and memory overhead during inference. Current caching techniques\naccelerate decoding by storing full-layer states, yet impose substantial memory\nusage that limit long-context applications. Our analysis of attention patterns\nin dLLMs reveals persistent cross-layer sparsity, with pivotal tokens remaining\nsalient across decoding steps and low-relevance tokens staying unimportant,\nmotivating selective cache eviction. We propose Sparse-dLLM, the first\ntraining-free framework integrating dynamic cache eviction with sparse\nattention via delayed bidirectional sparse caching. By leveraging the stability\nof token saliency over steps, it retains critical tokens and dynamically evicts\nunimportant prefix/suffix entries using an attention-guided strategy. Extensive\nexperiments on LLaDA and Dream series demonstrate Sparse-dLLM achieves up to\n10$\\times$ higher throughput than vanilla dLLMs, with comparable performance\nand similar peak memory costs, outperforming previous methods in efficiency and\neffectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Large Language Models (dLLMs) enable breakthroughs in reasoning and\nparallel decoding but suffer from prohibitive quadratic computational\ncomplexity and memory overhead during inference. Current caching techniques\naccelerate decoding by storing full-layer states, yet impose substantial memory\nusage that limit long-context applications. Our analysis of attention patterns\nin dLLMs reveals persistent cross-layer sparsity, with pivotal tokens remaining\nsalient across decoding steps and low-relevance tokens staying unimportant,\nmotivating selective cache eviction. We propose Sparse-dLLM, the first\ntraining-free framework integrating dynamic cache eviction with sparse\nattention via delayed bidirectional sparse caching. By leveraging the stability\nof token saliency over steps, it retains critical tokens and dynamically evicts\nunimportant prefix/suffix entries using an attention-guided strategy. Extensive\nexperiments on LLaDA and Dream series demonstrate Sparse-dLLM achieves up to\n10$\\times$ higher throughput than vanilla dLLMs, with comparable performance\nand similar peak memory costs, outperforming previous methods in efficiency and\neffectiveness."
                },
                "authors": [
                    {
                        "name": "Yuerong Song"
                    },
                    {
                        "name": "Xiaoran Liu"
                    },
                    {
                        "name": "Ruixiao Li"
                    },
                    {
                        "name": "Zhigeng Liu"
                    },
                    {
                        "name": "Zengfeng Huang"
                    },
                    {
                        "name": "Qipeng Guo"
                    },
                    {
                        "name": "Ziwei He"
                    },
                    {
                        "name": "Xipeng Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Xipeng Qiu"
                },
                "author": "Xipeng Qiu",
                "arxiv_comment": "11 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02558v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02558v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02401v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02401v1",
                "updated": "2025-08-04T13:26:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    13,
                    26,
                    16,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T13:26:16Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    13,
                    26,
                    16,
                    0,
                    216,
                    0
                ],
                "title": "CompressKV: Semantic Retrieval Heads Know What Tokens are Not Important\n  Before Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CompressKV: Semantic Retrieval Heads Know What Tokens are Not Important\n  Before Generation"
                },
                "summary": "Recent advances in large language models (LLMs) have significantly boosted\nlong-context processing. However, the increasing key-value (KV) cache size\nposes critical challenges to memory and execution efficiency. Most KV cache\ncompression methods rely on heuristic token eviction using all attention heads\nin Grouped Query Attention (GQA)-based LLMs. This method ignores the different\nfunctionalities of attention heads, leading to the eviction of critical tokens\nand thus degrades the performance of LLMs.\n  To address the issue above, instead of using all the attention heads in\nGQA-based LLMs to determine important tokens as in the previous work, we first\nidentify the attention heads in each layer that are not only capable of\nretrieving the initial and final tokens of a prompt, but also capable of\nretrieving important tokens within the text and attending to their surrounding\nsemantic context. Afterwards, we exploit such heads to determine the important\ntokens and retain their corresponding KV cache pairs. Furthermore, we analyze\nthe cache eviction error of each layer individually and introduce a\nlayer-adaptive KV cache allocation strategy. Experimental results demonstrate\nthe proposed CompressKV consistently outperforms state-of-the-art approaches\nunder various memory budgets on LongBench and Needle-in-a-Haystack benchmarks.\nOur code is publicly available at: https://github.com/TUDa-HWAI/CompressKV.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have significantly boosted\nlong-context processing. However, the increasing key-value (KV) cache size\nposes critical challenges to memory and execution efficiency. Most KV cache\ncompression methods rely on heuristic token eviction using all attention heads\nin Grouped Query Attention (GQA)-based LLMs. This method ignores the different\nfunctionalities of attention heads, leading to the eviction of critical tokens\nand thus degrades the performance of LLMs.\n  To address the issue above, instead of using all the attention heads in\nGQA-based LLMs to determine important tokens as in the previous work, we first\nidentify the attention heads in each layer that are not only capable of\nretrieving the initial and final tokens of a prompt, but also capable of\nretrieving important tokens within the text and attending to their surrounding\nsemantic context. Afterwards, we exploit such heads to determine the important\ntokens and retain their corresponding KV cache pairs. Furthermore, we analyze\nthe cache eviction error of each layer individually and introduce a\nlayer-adaptive KV cache allocation strategy. Experimental results demonstrate\nthe proposed CompressKV consistently outperforms state-of-the-art approaches\nunder various memory budgets on LongBench and Needle-in-a-Haystack benchmarks.\nOur code is publicly available at: https://github.com/TUDa-HWAI/CompressKV.git."
                },
                "authors": [
                    {
                        "name": "Xiaolin Lin"
                    },
                    {
                        "name": "Jingcun Wang"
                    },
                    {
                        "name": "Olga Kondrateva"
                    },
                    {
                        "name": "Yiyu Shi"
                    },
                    {
                        "name": "Bing Li"
                    },
                    {
                        "name": "Grace Li Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Grace Li Zhang"
                },
                "author": "Grace Li Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02401v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02401v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02280v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02280v1",
                "updated": "2025-08-04T10:51:20Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    10,
                    51,
                    20,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T10:51:20Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    10,
                    51,
                    20,
                    0,
                    216,
                    0
                ],
                "title": "OnPair: Short Strings Compression for Fast Random Access",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OnPair: Short Strings Compression for Fast Random Access"
                },
                "summary": "We present OnPair, a dictionary-based compression algorithm designed to meet\nthe needs of in-memory database systems that require both high compression and\nfast random access. Existing methods either achieve strong compression ratios\nat significant computational and memory cost (e.g., BPE) or prioritize speed at\nthe expense of compression quality (e.g., FSST). OnPair bridges this gap by\nemploying a cache-friendly dictionary construction technique that incrementally\nmerges frequent adjacent substrings in a single sequential pass over a data\nsample. This enables fast, memory-efficient training without tracking global\npair positions, as required by traditional BPE. We also introduce OnPair16, a\nvariant that limits dictionary entries to 16 bytes, enabling faster parsing via\noptimized longest prefix matching. Both variants compress strings\nindependently, supporting fine-grained random access without block-level\noverhead. Experiments on real-world datasets show that OnPair and OnPair16\nachieve compression ratios comparable to BPE while significantly improving\ncompression speed and memory usage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present OnPair, a dictionary-based compression algorithm designed to meet\nthe needs of in-memory database systems that require both high compression and\nfast random access. Existing methods either achieve strong compression ratios\nat significant computational and memory cost (e.g., BPE) or prioritize speed at\nthe expense of compression quality (e.g., FSST). OnPair bridges this gap by\nemploying a cache-friendly dictionary construction technique that incrementally\nmerges frequent adjacent substrings in a single sequential pass over a data\nsample. This enables fast, memory-efficient training without tracking global\npair positions, as required by traditional BPE. We also introduce OnPair16, a\nvariant that limits dictionary entries to 16 bytes, enabling faster parsing via\noptimized longest prefix matching. Both variants compress strings\nindependently, supporting fine-grained random access without block-level\noverhead. Experiments on real-world datasets show that OnPair and OnPair16\nachieve compression ratios comparable to BPE while significantly improving\ncompression speed and memory usage."
                },
                "authors": [
                    {
                        "name": "Francesco Gargiulo"
                    },
                    {
                        "name": "Rossano Venturini"
                    }
                ],
                "author_detail": {
                    "name": "Rossano Venturini"
                },
                "author": "Rossano Venturini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02280v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02280v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.2.4; E.4; H.3.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02215v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02215v1",
                "updated": "2025-08-04T09:08:43Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    9,
                    8,
                    43,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T09:08:43Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    9,
                    8,
                    43,
                    0,
                    216,
                    0
                ],
                "title": "LeanK: Learnable K Cache Channel Pruning for Efficient Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LeanK: Learnable K Cache Channel Pruning for Efficient Decoding"
                },
                "summary": "Large language models (LLMs) enable long-context tasks but face efficiency\nchallenges due to the growing key-value (KV) cache. We propose LeanK, a\nlearning-based method that prunes unimportant key (K) cache channels by\nleveraging static channel sparsity. With a novel two-stage training process,\nLeanK learns channel-wise static mask that could satisfy specific sparsity\nratio and hardware alignment requirement. LeanK reduces GPU memory and\naccelerates decoding without sacrificing accuracy. Experiments demonstrate up\nto 70% K cache and 16%-18% V cache memory reduction. Custom decoding kernel\nenables 1.3x speedup for attention computation. We also provide insights into\nmodel channels and attention heads during long-context inference by analyzing\nthe learned importance distribution. Our code is available at\nhttps://aka.ms/LeanK.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) enable long-context tasks but face efficiency\nchallenges due to the growing key-value (KV) cache. We propose LeanK, a\nlearning-based method that prunes unimportant key (K) cache channels by\nleveraging static channel sparsity. With a novel two-stage training process,\nLeanK learns channel-wise static mask that could satisfy specific sparsity\nratio and hardware alignment requirement. LeanK reduces GPU memory and\naccelerates decoding without sacrificing accuracy. Experiments demonstrate up\nto 70% K cache and 16%-18% V cache memory reduction. Custom decoding kernel\nenables 1.3x speedup for attention computation. We also provide insights into\nmodel channels and attention heads during long-context inference by analyzing\nthe learned importance distribution. Our code is available at\nhttps://aka.ms/LeanK."
                },
                "authors": [
                    {
                        "name": "Yike Zhang"
                    },
                    {
                        "name": "Zhiyuan He"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Jianyong Wang"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02215v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02215v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19906v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19906v2",
                "updated": "2025-08-04T08:19:26Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    8,
                    19,
                    26,
                    0,
                    216,
                    0
                ],
                "published": "2025-07-26T10:34:53Z",
                "published_parsed": [
                    2025,
                    7,
                    26,
                    10,
                    34,
                    53,
                    5,
                    207,
                    0
                ],
                "title": "CaliDrop: KV Cache Compression with Calibration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CaliDrop: KV Cache Compression with Calibration"
                },
                "summary": "Large Language Models (LLMs) require substantial computational resources\nduring generation. While the Key-Value (KV) cache significantly accelerates\nthis process by storing attention intermediates, its memory footprint grows\nlinearly with sequence length, batch size, and model size, creating a\nbottleneck in long-context scenarios. Various KV cache compression techniques,\nincluding token eviction, quantization, and low-rank projection, have been\nproposed to mitigate this bottleneck, often complementing each other. This\npaper focuses on enhancing token eviction strategies. Token eviction leverages\nthe observation that the attention patterns are often sparse, allowing for the\nremoval of less critical KV entries to save memory. However, this reduction\nusually comes at the cost of notable accuracy degradation, particularly under\nhigh compression ratios. To address this issue, we propose \\textbf{CaliDrop}, a\nnovel strategy that enhances token eviction through calibration. Our\npreliminary experiments show that queries at nearby positions exhibit high\nsimilarity. Building on this observation, CaliDrop performs speculative\ncalibration on the discarded tokens to mitigate the accuracy loss caused by\ntoken eviction. Extensive experiments demonstrate that CaliDrop significantly\nimproves the accuracy of existing token eviction methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) require substantial computational resources\nduring generation. While the Key-Value (KV) cache significantly accelerates\nthis process by storing attention intermediates, its memory footprint grows\nlinearly with sequence length, batch size, and model size, creating a\nbottleneck in long-context scenarios. Various KV cache compression techniques,\nincluding token eviction, quantization, and low-rank projection, have been\nproposed to mitigate this bottleneck, often complementing each other. This\npaper focuses on enhancing token eviction strategies. Token eviction leverages\nthe observation that the attention patterns are often sparse, allowing for the\nremoval of less critical KV entries to save memory. However, this reduction\nusually comes at the cost of notable accuracy degradation, particularly under\nhigh compression ratios. To address this issue, we propose \\textbf{CaliDrop}, a\nnovel strategy that enhances token eviction through calibration. Our\npreliminary experiments show that queries at nearby positions exhibit high\nsimilarity. Building on this observation, CaliDrop performs speculative\ncalibration on the discarded tokens to mitigate the accuracy loss caused by\ntoken eviction. Extensive experiments demonstrate that CaliDrop significantly\nimproves the accuracy of existing token eviction methods."
                },
                "authors": [
                    {
                        "name": "Yi Su"
                    },
                    {
                        "name": "Quantong Qiu"
                    },
                    {
                        "name": "Yuechi Zhou"
                    },
                    {
                        "name": "Juntao Li"
                    },
                    {
                        "name": "Qingrong Xia"
                    },
                    {
                        "name": "Ping Li"
                    },
                    {
                        "name": "Xinyu Duan"
                    },
                    {
                        "name": "Zhefeng Wang"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19906v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19906v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.21492v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.21492v2",
                "updated": "2025-08-04T04:48:41Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    4,
                    48,
                    41,
                    0,
                    216,
                    0
                ],
                "published": "2025-07-29T04:21:11Z",
                "published_parsed": [
                    2025,
                    7,
                    29,
                    4,
                    21,
                    11,
                    1,
                    210,
                    0
                ],
                "title": "Bridging Cache-Friendliness and Concurrency: A Locality-Optimized\n  In-Memory B-Skiplist",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging Cache-Friendliness and Concurrency: A Locality-Optimized\n  In-Memory B-Skiplist"
                },
                "summary": "Skiplists are widely used for in-memory indexing in many key-value stores,\nsuch as RocksDB and LevelDB, due to their ease of implementation and simple\nconcurrency control mechanisms. However, traditional skiplists suffer from poor\ncache locality, as they store only a single element per node, leaving\nperformance on the table. Minimizing last-level cache misses is key to\nmaximizing in-memory index performance, making high cache locality essential.\nIn this paper, we present a practical concurrent B-skiplist that enhances cache\nlocality and performance while preserving the simplicity of traditional\nskiplist structures and concurrency control schemes. Our key contributions\ninclude a top-down, single-pass insertion algorithm for B-skiplists and a\ncorresponding simple and efficient top-down concurrency control scheme. On 128\nthreads, the proposed concurrent B-skiplist achieves between 2x-9x higher\nthroughput compared to state-of-the-art concurrent skiplist implementations,\nincluding Facebook's concurrent skiplist from Folly and the Java\nConcurrentSkipListMap. Furthermore, we find that the B-skiplist achieves\ncompetitive (0.9x-1.7x) throughput on point workloads compared to\nstate-of-the-art cache-optimized tree-based indices (e.g., Masstree). For a\nmore complete picture of the performance, we also measure the latency of\nskiplist and tree-based indices and find that the B-skiplist achieves between\n3.5x-103x lower 99% latency compared to other concurrent skiplists and between\n0.85x-64x lower 99% latency compared to tree-based indices on point workloads\nwith inserts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skiplists are widely used for in-memory indexing in many key-value stores,\nsuch as RocksDB and LevelDB, due to their ease of implementation and simple\nconcurrency control mechanisms. However, traditional skiplists suffer from poor\ncache locality, as they store only a single element per node, leaving\nperformance on the table. Minimizing last-level cache misses is key to\nmaximizing in-memory index performance, making high cache locality essential.\nIn this paper, we present a practical concurrent B-skiplist that enhances cache\nlocality and performance while preserving the simplicity of traditional\nskiplist structures and concurrency control schemes. Our key contributions\ninclude a top-down, single-pass insertion algorithm for B-skiplists and a\ncorresponding simple and efficient top-down concurrency control scheme. On 128\nthreads, the proposed concurrent B-skiplist achieves between 2x-9x higher\nthroughput compared to state-of-the-art concurrent skiplist implementations,\nincluding Facebook's concurrent skiplist from Folly and the Java\nConcurrentSkipListMap. Furthermore, we find that the B-skiplist achieves\ncompetitive (0.9x-1.7x) throughput on point workloads compared to\nstate-of-the-art cache-optimized tree-based indices (e.g., Masstree). For a\nmore complete picture of the performance, we also measure the latency of\nskiplist and tree-based indices and find that the B-skiplist achieves between\n3.5x-103x lower 99% latency compared to other concurrent skiplists and between\n0.85x-64x lower 99% latency compared to tree-based indices on point workloads\nwith inserts."
                },
                "authors": [
                    {
                        "name": "Yicong Luo"
                    },
                    {
                        "name": "Senhe Hao"
                    },
                    {
                        "name": "Brian Wheatman"
                    },
                    {
                        "name": "Prashant Pandey"
                    },
                    {
                        "name": "Helen Xu"
                    }
                ],
                "author_detail": {
                    "name": "Helen Xu"
                },
                "author": "Helen Xu",
                "arxiv_doi": "10.1145/3754598.3754655",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3754598.3754655",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.21492v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.21492v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Original paper was accepted into ICPP 2025",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.02930v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.02930v4",
                "updated": "2025-08-04T02:47:35Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    2,
                    47,
                    35,
                    0,
                    216,
                    0
                ],
                "published": "2024-07-03T09:02:05Z",
                "published_parsed": [
                    2024,
                    7,
                    3,
                    9,
                    2,
                    5,
                    2,
                    185,
                    0
                ],
                "title": "Timely Requesting for Time-Critical Content Users in Decentralized\n  F-RANs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Timely Requesting for Time-Critical Content Users in Decentralized\n  F-RANs"
                },
                "summary": "With the rising demand for high-rate and timely communications, fog radio\naccess networks (F-RANs) offer a promising solution. This work investigates age\nof information (AoI) performance in F-RANs, consisting of multiple content\nusers (CUs), enhanced remote radio heads (eRRHs), and content providers (CPs).\nTime-critical CUs need rapid content updates from CPs but cannot communicate\ndirectly with them; instead, eRRHs act as intermediaries. CUs decide whether to\nrequest content from a CP and which eRRH to send the request to, while eRRHs\ndecide whether to command CPs to update content or use cached content. We study\ntwo general classes of policies: (i) oblivious policies, where decision-making\nis independent of historical information, and (ii) non-oblivious policies,\nwhere decisions are influenced by historical information. First, we obtain\nclosed-form expressions for the average AoI of eRRHs under both policy types.\nDue to the complexity of calculating closed-form expressions for CUs, we then\nderive general upper bounds for their average AoI. Next, we identify optimal\npolicies for both types. Under both optimal policies, each CU requests content\nfrom each CP at an equal rate, consolidating all requests to a single eRRH when\ndemand is low or resources are limited, and distributing requests evenly among\neRRHs when demand is high and resources are ample. eRRHs command content from\neach CP at an equal rate under an optimal oblivious policy, while prioritize\nthe CP with the highest age under an optimal non-oblivious policy. Our\nnumerical results validate these theoretical findings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rising demand for high-rate and timely communications, fog radio\naccess networks (F-RANs) offer a promising solution. This work investigates age\nof information (AoI) performance in F-RANs, consisting of multiple content\nusers (CUs), enhanced remote radio heads (eRRHs), and content providers (CPs).\nTime-critical CUs need rapid content updates from CPs but cannot communicate\ndirectly with them; instead, eRRHs act as intermediaries. CUs decide whether to\nrequest content from a CP and which eRRH to send the request to, while eRRHs\ndecide whether to command CPs to update content or use cached content. We study\ntwo general classes of policies: (i) oblivious policies, where decision-making\nis independent of historical information, and (ii) non-oblivious policies,\nwhere decisions are influenced by historical information. First, we obtain\nclosed-form expressions for the average AoI of eRRHs under both policy types.\nDue to the complexity of calculating closed-form expressions for CUs, we then\nderive general upper bounds for their average AoI. Next, we identify optimal\npolicies for both types. Under both optimal policies, each CU requests content\nfrom each CP at an equal rate, consolidating all requests to a single eRRH when\ndemand is low or resources are limited, and distributing requests evenly among\neRRHs when demand is high and resources are ample. eRRHs command content from\neach CP at an equal rate under an optimal oblivious policy, while prioritize\nthe CP with the highest age under an optimal non-oblivious policy. Our\nnumerical results validate these theoretical findings."
                },
                "authors": [
                    {
                        "name": "Xingran Chen"
                    },
                    {
                        "name": "Kai Li"
                    },
                    {
                        "name": "Kun Yang"
                    }
                ],
                "author_detail": {
                    "name": "Kun Yang"
                },
                "author": "Kun Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.02930v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.02930v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02252v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02252v2",
                "updated": "2025-08-04T02:17:56Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    2,
                    17,
                    56,
                    0,
                    216,
                    0
                ],
                "published": "2024-12-03T08:29:27Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    8,
                    29,
                    27,
                    1,
                    338,
                    0
                ],
                "title": "Compressing KV Cache for Long-Context LLM Inference with Inter-Layer\n  Attention Similarity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compressing KV Cache for Long-Context LLM Inference with Inter-Layer\n  Attention Similarity"
                },
                "summary": "The rapid expansion of context window sizes in Large Language Models~(LLMs)\nhas enabled them to tackle increasingly complex tasks involving lengthy\ndocuments. However, this progress comes at the cost of a substantial increase\nin memory usage during inference, primarily due to the linear growth of the\nkey-value~(KV) cache. Existing KV cache compression methods often discard less\nrelevant tokens, which can lead to significant performance degradation when\ncritical information is lost. In this paper, we propose \\textsc{PoD}~(Proximal\ntokens over Distant tokens), a novel KV cache compression framework that\nallocates memory according to token importance, retaining less important tokens\nin a more compact, shared form rather than discarding them entirely. Our\napproach is motivated by two key observations: (1) proximal tokens -- those at\nthe beginning and end of the context -- are significantly more important for\nnext-token prediction, and (2) attention scores for distant tokens are highly\nredundant across consecutive layers. Leveraging these insights, \\textsc{PoD}\npreserves the full KV cache for proximal tokens, while for distant tokens, it\nshares key states across layers. Since attention scores are determined by both\nqueries and keys, sharing key states enables multiple layers to reuse a single\nset of keys for distant tokens, substantially reducing KV cache memory without\ndiscarding essential context. We further introduce a lightweight post-training\nadaptation to enable the model to adjust to this new attention-sharing\nstructure. Extensive experiments on both synthetic~(Needle in a Haystack) and\nreal-world long-context benchmarks demonstrate that \\textsc{PoD} reduces KV\ncache memory usage by up to 35\\% without compromising performance. Our method\nis orthogonal to existing token-selection-based techniques and can be combined\nwith them for further KV cache compression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid expansion of context window sizes in Large Language Models~(LLMs)\nhas enabled them to tackle increasingly complex tasks involving lengthy\ndocuments. However, this progress comes at the cost of a substantial increase\nin memory usage during inference, primarily due to the linear growth of the\nkey-value~(KV) cache. Existing KV cache compression methods often discard less\nrelevant tokens, which can lead to significant performance degradation when\ncritical information is lost. In this paper, we propose \\textsc{PoD}~(Proximal\ntokens over Distant tokens), a novel KV cache compression framework that\nallocates memory according to token importance, retaining less important tokens\nin a more compact, shared form rather than discarding them entirely. Our\napproach is motivated by two key observations: (1) proximal tokens -- those at\nthe beginning and end of the context -- are significantly more important for\nnext-token prediction, and (2) attention scores for distant tokens are highly\nredundant across consecutive layers. Leveraging these insights, \\textsc{PoD}\npreserves the full KV cache for proximal tokens, while for distant tokens, it\nshares key states across layers. Since attention scores are determined by both\nqueries and keys, sharing key states enables multiple layers to reuse a single\nset of keys for distant tokens, substantially reducing KV cache memory without\ndiscarding essential context. We further introduce a lightweight post-training\nadaptation to enable the model to adjust to this new attention-sharing\nstructure. Extensive experiments on both synthetic~(Needle in a Haystack) and\nreal-world long-context benchmarks demonstrate that \\textsc{PoD} reduces KV\ncache memory usage by up to 35\\% without compromising performance. Our method\nis orthogonal to existing token-selection-based techniques and can be combined\nwith them for further KV cache compression."
                },
                "authors": [
                    {
                        "name": "Da Ma"
                    },
                    {
                        "name": "Lu Chen"
                    },
                    {
                        "name": "Situo Zhang"
                    },
                    {
                        "name": "Yuxun Miao"
                    },
                    {
                        "name": "Su Zhu"
                    },
                    {
                        "name": "Zhi Chen"
                    },
                    {
                        "name": "Hongshen Xu"
                    },
                    {
                        "name": "Hanqi Li"
                    },
                    {
                        "name": "Shuai Fan"
                    },
                    {
                        "name": "Lei Pan"
                    },
                    {
                        "name": "Kai Yu"
                    }
                ],
                "author_detail": {
                    "name": "Kai Yu"
                },
                "author": "Kai Yu",
                "arxiv_comment": "14 pages, 7 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02252v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02252v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01898v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01898v1",
                "updated": "2025-08-03T19:16:40Z",
                "updated_parsed": [
                    2025,
                    8,
                    3,
                    19,
                    16,
                    40,
                    6,
                    215,
                    0
                ],
                "published": "2025-08-03T19:16:40Z",
                "published_parsed": [
                    2025,
                    8,
                    3,
                    19,
                    16,
                    40,
                    6,
                    215,
                    0
                ],
                "title": "Revenue Optimization in Wireless Video Caching Networks: A\n  Privacy-Preserving Two-Stage Solution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revenue Optimization in Wireless Video Caching Networks: A\n  Privacy-Preserving Two-Stage Solution"
                },
                "summary": "Video caching can significantly improve delivery efficiency and enhance\nquality of video streaming, which constitutes the majority of wireless\ncommunication traffic. Due to limited cache size, caching strategies must be\ndesigned to adapt to and dynamic user demand in order to maximize system\nrevenue. The system revenue depends on the benefits of delivering the requested\nvideos and costs for (a) transporting the files to the users and (b) cache\nreplacement. Since the cache content at any point in time impacts the\nreplacement costs in the future, demand predictions over multiple cache\nplacement slots become an important prerequisite for efficient cache planning.\nMotivated by this, we introduce a novel two-stage privacy-preserving solution\nfor revenue optimization in wireless video caching networks. First, we train a\nTransformer using privacy-preserving federated learning (FL) to predict\nmulti-slot future demands. Given that prediction results are never entirely\naccurate, especially for longer horizons, we further combine global content\npopularity with per-user prediction results to estimate the content demand\ndistribution. Then, in the second stage, we leverage these estimation results\nto find caching strategies that maximize the long-term system revenue. This\nlatter problem takes on the form of a multi-stage knapsack problem, which we\nthen transform to a integer linear program. Our extensive simulation results\ndemonstrate that (i) our FL solution delivers nearly identical performance to\nthat of the ideal centralized solution and outperforms other existing caching\nmethods, and (ii) our novel revenue optimization approach provides deeper\nsystem performance insights than traditional cache hit ratio (CHR)-based\noptimization approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video caching can significantly improve delivery efficiency and enhance\nquality of video streaming, which constitutes the majority of wireless\ncommunication traffic. Due to limited cache size, caching strategies must be\ndesigned to adapt to and dynamic user demand in order to maximize system\nrevenue. The system revenue depends on the benefits of delivering the requested\nvideos and costs for (a) transporting the files to the users and (b) cache\nreplacement. Since the cache content at any point in time impacts the\nreplacement costs in the future, demand predictions over multiple cache\nplacement slots become an important prerequisite for efficient cache planning.\nMotivated by this, we introduce a novel two-stage privacy-preserving solution\nfor revenue optimization in wireless video caching networks. First, we train a\nTransformer using privacy-preserving federated learning (FL) to predict\nmulti-slot future demands. Given that prediction results are never entirely\naccurate, especially for longer horizons, we further combine global content\npopularity with per-user prediction results to estimate the content demand\ndistribution. Then, in the second stage, we leverage these estimation results\nto find caching strategies that maximize the long-term system revenue. This\nlatter problem takes on the form of a multi-stage knapsack problem, which we\nthen transform to a integer linear program. Our extensive simulation results\ndemonstrate that (i) our FL solution delivers nearly identical performance to\nthat of the ideal centralized solution and outperforms other existing caching\nmethods, and (ii) our novel revenue optimization approach provides deeper\nsystem performance insights than traditional cache hit ratio (CHR)-based\noptimization approaches."
                },
                "authors": [
                    {
                        "name": "Yijing Zhang"
                    },
                    {
                        "name": "Md-Ferdous Pervej"
                    },
                    {
                        "name": "Andreas F. Molisch"
                    }
                ],
                "author_detail": {
                    "name": "Andreas F. Molisch"
                },
                "author": "Andreas F. Molisch",
                "arxiv_comment": "Under review for possible publication in the IEEE Transactions on\n  Communications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01898v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01898v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01875v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01875v1",
                "updated": "2025-08-03T18:15:42Z",
                "updated_parsed": [
                    2025,
                    8,
                    3,
                    18,
                    15,
                    42,
                    6,
                    215,
                    0
                ],
                "published": "2025-08-03T18:15:42Z",
                "published_parsed": [
                    2025,
                    8,
                    3,
                    18,
                    15,
                    42,
                    6,
                    215,
                    0
                ],
                "title": "StreamAgent: Towards Anticipatory Agents for Streaming Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StreamAgent: Towards Anticipatory Agents for Streaming Video\n  Understanding"
                },
                "summary": "Real-time streaming video understanding in domains such as autonomous driving\nand intelligent surveillance poses challenges beyond conventional offline video\nprocessing, requiring continuous perception, proactive decision making, and\nresponsive interaction based on dynamically evolving visual content. However,\nexisting methods rely on alternating perception-reaction or asynchronous\ntriggers, lacking task-driven planning and future anticipation, which limits\ntheir real-time responsiveness and proactive decision making in evolving video\nstreams. To this end, we propose a StreamAgent that anticipates the temporal\nintervals and spatial regions expected to contain future task-relevant\ninformation to enable proactive and goal-driven responses. Specifically, we\nintegrate question semantics and historical observations through prompting the\nanticipatory agent to anticipate the temporal progression of key events, align\ncurrent observations with the expected future evidence, and subsequently adjust\nthe perception action (e.g., attending to task-relevant regions or continuously\ntracking in subsequent frames). To enable efficient inference, we design a\nstreaming KV-cache memory mechanism that constructs a hierarchical memory\nstructure for selective recall of relevant tokens, enabling efficient semantic\nretrieval while reducing the overhead of storing all tokens in the traditional\nKV-cache. Extensive experiments on streaming and long video understanding tasks\ndemonstrate that our method outperforms existing methods in response accuracy\nand real-time efficiency, highlighting its practical value for real-world\nstreaming scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time streaming video understanding in domains such as autonomous driving\nand intelligent surveillance poses challenges beyond conventional offline video\nprocessing, requiring continuous perception, proactive decision making, and\nresponsive interaction based on dynamically evolving visual content. However,\nexisting methods rely on alternating perception-reaction or asynchronous\ntriggers, lacking task-driven planning and future anticipation, which limits\ntheir real-time responsiveness and proactive decision making in evolving video\nstreams. To this end, we propose a StreamAgent that anticipates the temporal\nintervals and spatial regions expected to contain future task-relevant\ninformation to enable proactive and goal-driven responses. Specifically, we\nintegrate question semantics and historical observations through prompting the\nanticipatory agent to anticipate the temporal progression of key events, align\ncurrent observations with the expected future evidence, and subsequently adjust\nthe perception action (e.g., attending to task-relevant regions or continuously\ntracking in subsequent frames). To enable efficient inference, we design a\nstreaming KV-cache memory mechanism that constructs a hierarchical memory\nstructure for selective recall of relevant tokens, enabling efficient semantic\nretrieval while reducing the overhead of storing all tokens in the traditional\nKV-cache. Extensive experiments on streaming and long video understanding tasks\ndemonstrate that our method outperforms existing methods in response accuracy\nand real-time efficiency, highlighting its practical value for real-world\nstreaming scenarios."
                },
                "authors": [
                    {
                        "name": "Haolin Yang"
                    },
                    {
                        "name": "Feilong Tang"
                    },
                    {
                        "name": "Linxiao Zhao"
                    },
                    {
                        "name": "Xiang An"
                    },
                    {
                        "name": "Ming Hu"
                    },
                    {
                        "name": "Huifa Li"
                    },
                    {
                        "name": "Xinlin Zhuang"
                    },
                    {
                        "name": "Boqian Wang"
                    },
                    {
                        "name": "Yifan Lu"
                    },
                    {
                        "name": "Xiaofeng Zhang"
                    },
                    {
                        "name": "Abdalla Swikir"
                    },
                    {
                        "name": "Junjun He"
                    },
                    {
                        "name": "Zongyuan Ge"
                    },
                    {
                        "name": "Imran Razzak"
                    }
                ],
                "author_detail": {
                    "name": "Imran Razzak"
                },
                "author": "Imran Razzak",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01875v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01875v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16607v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16607v2",
                "updated": "2025-08-03T10:27:19Z",
                "updated_parsed": [
                    2025,
                    8,
                    3,
                    10,
                    27,
                    19,
                    6,
                    215,
                    0
                ],
                "published": "2025-01-28T00:52:23Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    0,
                    52,
                    23,
                    1,
                    28,
                    0
                ],
                "title": "MCTS-SQL: Light-Weight LLMs can Master the Text-to-SQL through Monte\n  Carlo Tree Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MCTS-SQL: Light-Weight LLMs can Master the Text-to-SQL through Monte\n  Carlo Tree Search"
                },
                "summary": "Text-to-SQL is a fundamental yet challenging task in the NLP area, aiming at\ntranslating natural language questions into SQL queries. While recent advances\nin large language models have greatly improved performance, most existing\napproaches depend on models with tens of billions of parameters or costly APIs,\nlimiting their applicability in resource-constrained environments. For real\nworld, especially on edge devices, it is crucial for Text-to-SQL to ensure\ncost-effectiveness. Therefore, enabling the light-weight models for Text-to-SQL\nis of great practical significance. However, smaller LLMs often struggle with\ncomplicated user instruction, redundant schema linking or syntax correctness.\nTo address these challenges, we propose MCTS-SQL, a novel framework that uses\nMonte Carlo Tree Search to guide SQL generation through multi-step refinement.\nSince the light-weight models' weak performance of single-shot prediction, we\ngenerate better results through several trials with feedback. However, directly\napplying MCTS-based methods inevitably leads to significant time and\ncomputational overhead. Driven by this issue, we propose a token-level\nprefix-cache mechanism that stores prior information during iterations,\neffectively improved the execution speed. Experiments results on the SPIDER and\nBIRD benchmarks demonstrate the effectiveness of our approach. Using a small\nopen-source Qwen2.5-Coder-1.5B, our method outperforms ChatGPT-3.5. When\nleveraging a more powerful model Gemini 2.5 to explore the performance upper\nbound, we achieved results competitive with the SOTA. Our findings demonstrate\nthat even small models can be effectively deployed in practical Text-to-SQL\nsystems with the right strategy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-SQL is a fundamental yet challenging task in the NLP area, aiming at\ntranslating natural language questions into SQL queries. While recent advances\nin large language models have greatly improved performance, most existing\napproaches depend on models with tens of billions of parameters or costly APIs,\nlimiting their applicability in resource-constrained environments. For real\nworld, especially on edge devices, it is crucial for Text-to-SQL to ensure\ncost-effectiveness. Therefore, enabling the light-weight models for Text-to-SQL\nis of great practical significance. However, smaller LLMs often struggle with\ncomplicated user instruction, redundant schema linking or syntax correctness.\nTo address these challenges, we propose MCTS-SQL, a novel framework that uses\nMonte Carlo Tree Search to guide SQL generation through multi-step refinement.\nSince the light-weight models' weak performance of single-shot prediction, we\ngenerate better results through several trials with feedback. However, directly\napplying MCTS-based methods inevitably leads to significant time and\ncomputational overhead. Driven by this issue, we propose a token-level\nprefix-cache mechanism that stores prior information during iterations,\neffectively improved the execution speed. Experiments results on the SPIDER and\nBIRD benchmarks demonstrate the effectiveness of our approach. Using a small\nopen-source Qwen2.5-Coder-1.5B, our method outperforms ChatGPT-3.5. When\nleveraging a more powerful model Gemini 2.5 to explore the performance upper\nbound, we achieved results competitive with the SOTA. Our findings demonstrate\nthat even small models can be effectively deployed in practical Text-to-SQL\nsystems with the right strategy."
                },
                "authors": [
                    {
                        "name": "Shuozhi Yuan"
                    },
                    {
                        "name": "Limin Chen"
                    },
                    {
                        "name": "Miaomiao Yuan"
                    },
                    {
                        "name": "Jin Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Jin Zhao"
                },
                "author": "Jin Zhao",
                "arxiv_comment": "15 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16607v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16607v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02751v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02751v1",
                "updated": "2025-08-03T09:15:36Z",
                "updated_parsed": [
                    2025,
                    8,
                    3,
                    9,
                    15,
                    36,
                    6,
                    215,
                    0
                ],
                "published": "2025-08-03T09:15:36Z",
                "published_parsed": [
                    2025,
                    8,
                    3,
                    9,
                    15,
                    36,
                    6,
                    215,
                    0
                ],
                "title": "SmallKV: Small Model Assisted Compensation of KV Cache Compression for\n  Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SmallKV: Small Model Assisted Compensation of KV Cache Compression for\n  Efficient LLM Inference"
                },
                "summary": "KV cache eviction has emerged as an effective solution to alleviate resource\nconstraints faced by LLMs in long-context scenarios. However, existing\ntoken-level eviction methods often overlook two critical aspects: (1) their\nirreversible eviction strategy fails to adapt to dynamic attention patterns\nduring decoding (the saliency shift problem), and (2) they treat both\nmarginally important tokens and truly unimportant tokens equally, despite the\ncollective significance of marginal tokens to model performance (the marginal\ninformation over-compression problem). To address these issues, we design two\ncompensation mechanisms based on the high similarity of attention matrices\nbetween LLMs of different scales. We propose SmallKV, a small model assisted\ncompensation method for KV cache compression. SmallKV can maintain attention\nmatching between different-scale LLMs to: 1) assist the larger model in\nperceiving globally important information of attention; and 2) use the smaller\nmodel's attention scores to approximate those of marginal tokens in the larger\nmodel. Extensive experiments on benchmarks including GSM8K, BBH, MT-Bench, and\nLongBench demonstrate the effectiveness of SmallKV. Moreover, efficiency\nevaluations show that SmallKV achieves 1.75 - 2.56 times higher throughput than\nbaseline methods, highlighting its potential for efficient and performant LLM\ninference in resource constrained environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache eviction has emerged as an effective solution to alleviate resource\nconstraints faced by LLMs in long-context scenarios. However, existing\ntoken-level eviction methods often overlook two critical aspects: (1) their\nirreversible eviction strategy fails to adapt to dynamic attention patterns\nduring decoding (the saliency shift problem), and (2) they treat both\nmarginally important tokens and truly unimportant tokens equally, despite the\ncollective significance of marginal tokens to model performance (the marginal\ninformation over-compression problem). To address these issues, we design two\ncompensation mechanisms based on the high similarity of attention matrices\nbetween LLMs of different scales. We propose SmallKV, a small model assisted\ncompensation method for KV cache compression. SmallKV can maintain attention\nmatching between different-scale LLMs to: 1) assist the larger model in\nperceiving globally important information of attention; and 2) use the smaller\nmodel's attention scores to approximate those of marginal tokens in the larger\nmodel. Extensive experiments on benchmarks including GSM8K, BBH, MT-Bench, and\nLongBench demonstrate the effectiveness of SmallKV. Moreover, efficiency\nevaluations show that SmallKV achieves 1.75 - 2.56 times higher throughput than\nbaseline methods, highlighting its potential for efficient and performant LLM\ninference in resource constrained environments."
                },
                "authors": [
                    {
                        "name": "Yi Zhao"
                    },
                    {
                        "name": "Yajuan Peng"
                    },
                    {
                        "name": "Cam-Tu Nguyen"
                    },
                    {
                        "name": "Zuchao Li"
                    },
                    {
                        "name": "Xiaoliang Wang"
                    },
                    {
                        "name": "Hai Zhao"
                    },
                    {
                        "name": "Xiaoming Fu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoming Fu"
                },
                "author": "Xiaoming Fu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02751v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02751v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19718v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19718v2",
                "updated": "2025-08-02T23:59:11Z",
                "updated_parsed": [
                    2025,
                    8,
                    2,
                    23,
                    59,
                    11,
                    5,
                    214,
                    0
                ],
                "published": "2025-07-25T23:55:54Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    23,
                    55,
                    54,
                    4,
                    206,
                    0
                ],
                "title": "GSCache: Real-Time Radiance Caching for Volume Path Tracing using 3D\n  Gaussian Splatting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GSCache: Real-Time Radiance Caching for Volume Path Tracing using 3D\n  Gaussian Splatting"
                },
                "summary": "Real-time path tracing is rapidly becoming the standard for rendering in\nentertainment and professional applications. In scientific visualization,\nvolume rendering plays a crucial role in helping researchers analyze and\ninterpret complex 3D data. Recently, photorealistic rendering techniques have\ngained popularity in scientific visualization, yet they face significant\nchallenges. One of the most prominent issues is slow rendering performance and\nhigh pixel variance caused by Monte Carlo integration. In this work, we\nintroduce a novel radiance caching approach for path-traced volume rendering.\nOur method leverages advances in volumetric scene representation and adapts 3D\nGaussian splatting to function as a multi-level, path-space radiance cache.\nThis cache is designed to be trainable on the fly, dynamically adapting to\nchanges in scene parameters such as lighting configurations and transfer\nfunctions. By incorporating our cache, we achieve less noisy, higher-quality\nimages without increasing rendering costs. To evaluate our approach, we compare\nit against a baseline path tracer that supports uniform sampling and next-event\nestimation and the state-of-the-art for neural radiance caching. Through both\nquantitative and qualitative analyses, we demonstrate that our path-space\nradiance cache is a robust solution that is easy to integrate and significantly\nenhances the rendering quality of volumetric visualization applications while\nmaintaining comparable computational efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time path tracing is rapidly becoming the standard for rendering in\nentertainment and professional applications. In scientific visualization,\nvolume rendering plays a crucial role in helping researchers analyze and\ninterpret complex 3D data. Recently, photorealistic rendering techniques have\ngained popularity in scientific visualization, yet they face significant\nchallenges. One of the most prominent issues is slow rendering performance and\nhigh pixel variance caused by Monte Carlo integration. In this work, we\nintroduce a novel radiance caching approach for path-traced volume rendering.\nOur method leverages advances in volumetric scene representation and adapts 3D\nGaussian splatting to function as a multi-level, path-space radiance cache.\nThis cache is designed to be trainable on the fly, dynamically adapting to\nchanges in scene parameters such as lighting configurations and transfer\nfunctions. By incorporating our cache, we achieve less noisy, higher-quality\nimages without increasing rendering costs. To evaluate our approach, we compare\nit against a baseline path tracer that supports uniform sampling and next-event\nestimation and the state-of-the-art for neural radiance caching. Through both\nquantitative and qualitative analyses, we demonstrate that our path-space\nradiance cache is a robust solution that is easy to integrate and significantly\nenhances the rendering quality of volumetric visualization applications while\nmaintaining comparable computational efficiency."
                },
                "authors": [
                    {
                        "name": "David Bauer"
                    },
                    {
                        "name": "Qi Wu"
                    },
                    {
                        "name": "Hamid Gadirov"
                    },
                    {
                        "name": "Kwan-Liu Ma"
                    }
                ],
                "author_detail": {
                    "name": "Kwan-Liu Ma"
                },
                "author": "Kwan-Liu Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19718v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19718v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01488v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01488v1",
                "updated": "2025-08-02T21:00:55Z",
                "updated_parsed": [
                    2025,
                    8,
                    2,
                    21,
                    0,
                    55,
                    5,
                    214,
                    0
                ],
                "published": "2025-08-02T21:00:55Z",
                "published_parsed": [
                    2025,
                    8,
                    2,
                    21,
                    0,
                    55,
                    5,
                    214,
                    0
                ],
                "title": "PESTO: Real-Time Pitch Estimation with Self-supervised\n  Transposition-equivariant Objective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PESTO: Real-Time Pitch Estimation with Self-supervised\n  Transposition-equivariant Objective"
                },
                "summary": "In this paper, we introduce PESTO, a self-supervised learning approach for\nsingle-pitch estimation using a Siamese architecture. Our model processes\nindividual frames of a Variable-$Q$ Transform (VQT) and predicts pitch\ndistributions. The neural network is designed to be equivariant to\ntranslations, notably thanks to a Toeplitz fully-connected layer. In addition,\nwe construct pitch-shifted pairs by translating and cropping the VQT frames and\ntrain our model with a novel class-based transposition-equivariant objective,\neliminating the need for annotated data. Thanks to this architecture and\ntraining objective, our model achieves remarkable performances while being very\nlightweight ($130$k parameters). Evaluations on music and speech datasets\n(MIR-1K, MDB-stem-synth, and PTDB) demonstrate that PESTO not only outperforms\nself-supervised baselines but also competes with supervised methods, exhibiting\nsuperior cross-dataset generalization. Finally, we enhance PESTO's practical\nutility by developing a streamable VQT implementation using cached\nconvolutions. Combined with our model's low latency (less than 10 ms) and\nminimal parameter count, this makes PESTO particularly suitable for real-time\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce PESTO, a self-supervised learning approach for\nsingle-pitch estimation using a Siamese architecture. Our model processes\nindividual frames of a Variable-$Q$ Transform (VQT) and predicts pitch\ndistributions. The neural network is designed to be equivariant to\ntranslations, notably thanks to a Toeplitz fully-connected layer. In addition,\nwe construct pitch-shifted pairs by translating and cropping the VQT frames and\ntrain our model with a novel class-based transposition-equivariant objective,\neliminating the need for annotated data. Thanks to this architecture and\ntraining objective, our model achieves remarkable performances while being very\nlightweight ($130$k parameters). Evaluations on music and speech datasets\n(MIR-1K, MDB-stem-synth, and PTDB) demonstrate that PESTO not only outperforms\nself-supervised baselines but also competes with supervised methods, exhibiting\nsuperior cross-dataset generalization. Finally, we enhance PESTO's practical\nutility by developing a streamable VQT implementation using cached\nconvolutions. Combined with our model's low latency (less than 10 ms) and\nminimal parameter count, this makes PESTO particularly suitable for real-time\napplications."
                },
                "authors": [
                    {
                        "name": "Alain Riou"
                    },
                    {
                        "name": "Bernardo Torres"
                    },
                    {
                        "name": "Ben Hayes"
                    },
                    {
                        "name": "Stefan Lattner"
                    },
                    {
                        "name": "Gatan Hadjeres"
                    },
                    {
                        "name": "Gal Richard"
                    },
                    {
                        "name": "Geoffroy Peeters"
                    }
                ],
                "author_detail": {
                    "name": "Geoffroy Peeters"
                },
                "author": "Geoffroy Peeters",
                "arxiv_comment": "Accepted to the Transactions of the International Society for Music\n  Information Retrieval",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01488v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01488v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01298v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01298v1",
                "updated": "2025-08-02T10:12:45Z",
                "updated_parsed": [
                    2025,
                    8,
                    2,
                    10,
                    12,
                    45,
                    5,
                    214,
                    0
                ],
                "published": "2025-08-02T10:12:45Z",
                "published_parsed": [
                    2025,
                    8,
                    2,
                    10,
                    12,
                    45,
                    5,
                    214,
                    0
                ],
                "title": "Improving performance of content-centric networks via decentralized\n  coded caching for multi-level popularity and access",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving performance of content-centric networks via decentralized\n  coded caching for multi-level popularity and access"
                },
                "summary": "Content-Centric Networking (CCN) offers a novel architectural paradigm that\nseeks to address the inherent limitations of the prevailing Internet Protocol\n(IP)-based networking model. In contrast to the host-centric communication\napproach of IP networks, CCN prioritizes content by enabling direct addressing\nand routing based on content identifiers. The potential performance\nimprovements of CCN can be further amplified through optimized management of\ncoded data storage and transmission strategies. Decentralized Coded Caching\n(DCC) emerges as a promising technique that harnesses the collective caching\npower of distributed network elements. By strategically pre-positioning\nfrequently accessed content closer to potential consumers during periods of low\nnetwork utilization, DCC has the potential to mitigate content transfer rates\nduring peak traffic periods. This paper proposes a series of fundamental\nmodifications to the CCN architecture by integrating DCC. The proposed\nframework incorporates differentiated coding strategies tailored to user access\nprivileges, thereby eliminating the overhead associated with queue-based\nsearching. Additionally, the framework facilitates recoding of uncoded data\nencountered along the content delivery path. These combined methodologies\ndemonstrably enhance network throughput, elevate cache hit ratios, and\nconsequently, reduce content delivery latency compared to conventional CCN\nimplementations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Content-Centric Networking (CCN) offers a novel architectural paradigm that\nseeks to address the inherent limitations of the prevailing Internet Protocol\n(IP)-based networking model. In contrast to the host-centric communication\napproach of IP networks, CCN prioritizes content by enabling direct addressing\nand routing based on content identifiers. The potential performance\nimprovements of CCN can be further amplified through optimized management of\ncoded data storage and transmission strategies. Decentralized Coded Caching\n(DCC) emerges as a promising technique that harnesses the collective caching\npower of distributed network elements. By strategically pre-positioning\nfrequently accessed content closer to potential consumers during periods of low\nnetwork utilization, DCC has the potential to mitigate content transfer rates\nduring peak traffic periods. This paper proposes a series of fundamental\nmodifications to the CCN architecture by integrating DCC. The proposed\nframework incorporates differentiated coding strategies tailored to user access\nprivileges, thereby eliminating the overhead associated with queue-based\nsearching. Additionally, the framework facilitates recoding of uncoded data\nencountered along the content delivery path. These combined methodologies\ndemonstrably enhance network throughput, elevate cache hit ratios, and\nconsequently, reduce content delivery latency compared to conventional CCN\nimplementations."
                },
                "authors": [
                    {
                        "name": "Azadeh Sadat Miraftab"
                    },
                    {
                        "name": "Ahmadreza Montazerolghaem"
                    },
                    {
                        "name": "Behrad Mahboobi"
                    }
                ],
                "author_detail": {
                    "name": "Behrad Mahboobi"
                },
                "author": "Behrad Mahboobi",
                "arxiv_doi": "10.1007/s10586-025-05256-6",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s10586-025-05256-6",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.01298v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01298v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01261v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01261v1",
                "updated": "2025-08-02T08:33:30Z",
                "updated_parsed": [
                    2025,
                    8,
                    2,
                    8,
                    33,
                    30,
                    5,
                    214,
                    0
                ],
                "published": "2025-08-02T08:33:30Z",
                "published_parsed": [
                    2025,
                    8,
                    2,
                    8,
                    33,
                    30,
                    5,
                    214,
                    0
                ],
                "title": "Unifying Mixture of Experts and Multi-Head Latent Attention for\n  Efficient Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unifying Mixture of Experts and Multi-Head Latent Attention for\n  Efficient Language Models"
                },
                "summary": "We present MoE-MLA-RoPE, a novel architecture combination that combines\nMixture of Experts (MoE) with Multi-head Latent Attention (MLA) and Rotary\nPosition Embeddings (RoPE) for efficient language modeling. Our approach\naddresses the fundamental trade-off between model capacity and computational\nefficiency through three key innovations: (1) fine-grained expert routing with\n64 micro-experts and top-$k$ selection, enabling flexible specialization\nthrough 3.6 * 10^7 possible expert combinations; (2) shared expert isolation\nthat dedicates 2 always active experts for common patterns while routing to 6\nof 62 specialized experts; and (3) gradient-conflict-free load balancing that\nmaintains expert utilization without interfering with primary loss\noptimization.\n  Extensive experiments on models ranging from 17M to 202M parameters\ndemonstrate that MoE-MLA-RoPE with compression ratio r=d/2 achieves 68% KV\ncache memory reduction and 3.2x inference speedup while maintaining competitive\nperplexity (0.8% degradation). Compared to the parameters with 53.9M\nparameters, MoE-MLA-RoPE improves the validation loss by 6.9% over the vanilla\ntransformers while using 42% fewer active parameters per forward pass.\nFLOP-matched experiments reveal even larger gains: 11.1% improvement with 3.2x\ninference acceleration. Automated evaluation using GPT-4 as a judge confirms\nquality improvements in generation, with higher scores on coherence (8.1/10),\ncreativity (7.9/10) and grammatical correctness (8.2/10). Our results establish\nthat architectural novelty, not parameter scaling, defines the efficiency\nfrontier for resource-constrained language model deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present MoE-MLA-RoPE, a novel architecture combination that combines\nMixture of Experts (MoE) with Multi-head Latent Attention (MLA) and Rotary\nPosition Embeddings (RoPE) for efficient language modeling. Our approach\naddresses the fundamental trade-off between model capacity and computational\nefficiency through three key innovations: (1) fine-grained expert routing with\n64 micro-experts and top-$k$ selection, enabling flexible specialization\nthrough 3.6 * 10^7 possible expert combinations; (2) shared expert isolation\nthat dedicates 2 always active experts for common patterns while routing to 6\nof 62 specialized experts; and (3) gradient-conflict-free load balancing that\nmaintains expert utilization without interfering with primary loss\noptimization.\n  Extensive experiments on models ranging from 17M to 202M parameters\ndemonstrate that MoE-MLA-RoPE with compression ratio r=d/2 achieves 68% KV\ncache memory reduction and 3.2x inference speedup while maintaining competitive\nperplexity (0.8% degradation). Compared to the parameters with 53.9M\nparameters, MoE-MLA-RoPE improves the validation loss by 6.9% over the vanilla\ntransformers while using 42% fewer active parameters per forward pass.\nFLOP-matched experiments reveal even larger gains: 11.1% improvement with 3.2x\ninference acceleration. Automated evaluation using GPT-4 as a judge confirms\nquality improvements in generation, with higher scores on coherence (8.1/10),\ncreativity (7.9/10) and grammatical correctness (8.2/10). Our results establish\nthat architectural novelty, not parameter scaling, defines the efficiency\nfrontier for resource-constrained language model deployment."
                },
                "authors": [
                    {
                        "name": "Sushant Mehta"
                    },
                    {
                        "name": "Raj Dandekar"
                    },
                    {
                        "name": "Rajat Dandekar"
                    },
                    {
                        "name": "Sreedath Panat"
                    }
                ],
                "author_detail": {
                    "name": "Sreedath Panat"
                },
                "author": "Sreedath Panat",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01261v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01261v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11972v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11972v2",
                "updated": "2025-08-02T06:50:59Z",
                "updated_parsed": [
                    2025,
                    8,
                    2,
                    6,
                    50,
                    59,
                    5,
                    214,
                    0
                ],
                "published": "2025-03-15T02:48:27Z",
                "published_parsed": [
                    2025,
                    3,
                    15,
                    2,
                    48,
                    27,
                    5,
                    74,
                    0
                ],
                "title": "MoDM: Efficient Serving for Image Generation via Mixture-of-Diffusion\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoDM: Efficient Serving for Image Generation via Mixture-of-Diffusion\n  Models"
                },
                "summary": "Diffusion-based text-to-image generation models trade latency for quality:\nsmall models are fast but generate lower-quality images, while large models\nproduce better images but are slow.\n  We present MoDM, a novel caching-based serving system for diffusion models\nthat dynamically balances latency and quality through a mixture of diffusion\nmodels. Unlike prior approaches that rely on model-specific internal features,\nMoDM caches final images, allowing seamless retrieval and reuse across multiple\ndiffusion model families.\n  This design enables adaptive serving by dynamically balancing latency and\nimage quality: using smaller models for cache-hit requests to reduce latency\nwhile reserving larger models for cache-miss requests to maintain quality.\nSmall model image quality is preserved using retrieved cached images.\n  We design a global monitor that optimally allocates GPU resources and\nbalances inference workload, ensuring high throughput while meeting\nservice-level objectives under varying request rates. Our evaluations show that\nMoDM significantly reduces average serving time by 2.5x while retaining image\nquality, making it a practical solution for scalable and resource-efficient\nmodel deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based text-to-image generation models trade latency for quality:\nsmall models are fast but generate lower-quality images, while large models\nproduce better images but are slow.\n  We present MoDM, a novel caching-based serving system for diffusion models\nthat dynamically balances latency and quality through a mixture of diffusion\nmodels. Unlike prior approaches that rely on model-specific internal features,\nMoDM caches final images, allowing seamless retrieval and reuse across multiple\ndiffusion model families.\n  This design enables adaptive serving by dynamically balancing latency and\nimage quality: using smaller models for cache-hit requests to reduce latency\nwhile reserving larger models for cache-miss requests to maintain quality.\nSmall model image quality is preserved using retrieved cached images.\n  We design a global monitor that optimally allocates GPU resources and\nbalances inference workload, ensuring high throughput while meeting\nservice-level objectives under varying request rates. Our evaluations show that\nMoDM significantly reduces average serving time by 2.5x while retaining image\nquality, making it a practical solution for scalable and resource-efficient\nmodel deployment."
                },
                "authors": [
                    {
                        "name": "Yuchen Xia"
                    },
                    {
                        "name": "Divyam Sharma"
                    },
                    {
                        "name": "Yichao Yuan"
                    },
                    {
                        "name": "Souvik Kundu"
                    },
                    {
                        "name": "Nishil Talati"
                    }
                ],
                "author_detail": {
                    "name": "Nishil Talati"
                },
                "author": "Nishil Talati",
                "arxiv_comment": "To appear in ASPLOS'26",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11972v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11972v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01225v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01225v1",
                "updated": "2025-08-02T06:43:43Z",
                "updated_parsed": [
                    2025,
                    8,
                    2,
                    6,
                    43,
                    43,
                    5,
                    214,
                    0
                ],
                "published": "2025-08-02T06:43:43Z",
                "published_parsed": [
                    2025,
                    8,
                    2,
                    6,
                    43,
                    43,
                    5,
                    214,
                    0
                ],
                "title": "Multi-Cache Enhanced Prototype Learning for Test-Time Generalization of\n  Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Cache Enhanced Prototype Learning for Test-Time Generalization of\n  Vision-Language Models"
                },
                "summary": "In zero-shot setting, test-time adaptation adjusts pre-trained models using\nunlabeled data from the test phase to enhance performance on unknown test\ndistributions. Existing cache-enhanced TTA methods rely on a low-entropy\ncriterion to select samples for prototype construction, assuming intra-class\ncompactness. However, low-entropy samples may be unreliable under distribution\nshifts, and the resulting prototypes may not ensure compact intra-class\ndistributions. This study identifies a positive correlation between\ncache-enhanced performance and intra-class compactness. Based on this\nobservation, we propose a Multi-Cache enhanced Prototype-based Test-Time\nAdaptation (MCP) featuring three caches: an entropy cache for initializing\nprototype representations with low-entropy samples, an align cache for\nintegrating visual and textual information to achieve compact intra-class\ndistributions, and a negative cache for prediction calibration using\nhigh-entropy samples. We further developed MCP++, a framework incorporating\ncross-modal prototype alignment and residual learning, introducing prototype\nresidual fine-tuning. Comparative and ablation experiments across 15 downstream\ntasks demonstrate that the proposed method and framework achieve\nstate-of-the-art generalization performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In zero-shot setting, test-time adaptation adjusts pre-trained models using\nunlabeled data from the test phase to enhance performance on unknown test\ndistributions. Existing cache-enhanced TTA methods rely on a low-entropy\ncriterion to select samples for prototype construction, assuming intra-class\ncompactness. However, low-entropy samples may be unreliable under distribution\nshifts, and the resulting prototypes may not ensure compact intra-class\ndistributions. This study identifies a positive correlation between\ncache-enhanced performance and intra-class compactness. Based on this\nobservation, we propose a Multi-Cache enhanced Prototype-based Test-Time\nAdaptation (MCP) featuring three caches: an entropy cache for initializing\nprototype representations with low-entropy samples, an align cache for\nintegrating visual and textual information to achieve compact intra-class\ndistributions, and a negative cache for prediction calibration using\nhigh-entropy samples. We further developed MCP++, a framework incorporating\ncross-modal prototype alignment and residual learning, introducing prototype\nresidual fine-tuning. Comparative and ablation experiments across 15 downstream\ntasks demonstrate that the proposed method and framework achieve\nstate-of-the-art generalization performance."
                },
                "authors": [
                    {
                        "name": "Xinyu Chen"
                    },
                    {
                        "name": "Haotian Zhai"
                    },
                    {
                        "name": "Can Zhang"
                    },
                    {
                        "name": "Xiupeng Shi"
                    },
                    {
                        "name": "Ruirui Li"
                    }
                ],
                "author_detail": {
                    "name": "Ruirui Li"
                },
                "author": "Ruirui Li",
                "arxiv_comment": "Accepted by ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01225v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01225v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06526v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06526v1",
                "updated": "2025-08-02T03:50:14Z",
                "updated_parsed": [
                    2025,
                    8,
                    2,
                    3,
                    50,
                    14,
                    5,
                    214,
                    0
                ],
                "published": "2025-08-02T03:50:14Z",
                "published_parsed": [
                    2025,
                    8,
                    2,
                    3,
                    50,
                    14,
                    5,
                    214,
                    0
                ],
                "title": "PiKV: KV Cache Management System for Mixture of Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PiKV: KV Cache Management System for Mixture of Experts"
                },
                "summary": "As large language models continue to scale up in both size and context\nlength, the memory and communication cost of key-value (KV) cache storage has\nbecome a major bottleneck in multi-GPU and multi-node inference. While\nMoE-based architectures sparsify computation across experts, the corresponding\nKV caches remain dense and globally synchronized, resulting in significant\noverhead.\n  We introduce \\textbf{PiKV}, a parallel and distributed KV cache serving\nframework tailored for MoE architecture. PiKV leverages \\textit{expert-sharded\nKV storage} to partition caches across GPUs, \\textit{PiKV routing} to reduce\ntoken-to-KV access, and a \\textit{PiKV Scheduling} to adaptively retain\nquery-relevant entries. To further reduce memory usage, PiKV integrates\n\\textit{PiKV Compression} modules the caching pipeline for acceleration.\n  PiKV is recently publicly available as an open-source software library:\n\\href{https://github.com/NoakLiu/PiKV}{https://github.com/NoakLiu/PiKV}.\nExperiments details is recorded at:\n\\href{https://github.com/NoakLiu/PiKV/blob/main/downstream_tasks/README.md}{https://github.com/NoakLiu/PiKV/Experimental\\_Results}.\nWe also have PiKV integrated with Nvidia kvpress for acceleration, details see\n\\href{https://github.com/NoakLiu/PiKVpress}{https://github.com/NoakLiu/PiKVpress}.\nPiKV is still a living project, aiming to become a comprehesive KV Cache\nmanagement system for MoE Architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models continue to scale up in both size and context\nlength, the memory and communication cost of key-value (KV) cache storage has\nbecome a major bottleneck in multi-GPU and multi-node inference. While\nMoE-based architectures sparsify computation across experts, the corresponding\nKV caches remain dense and globally synchronized, resulting in significant\noverhead.\n  We introduce \\textbf{PiKV}, a parallel and distributed KV cache serving\nframework tailored for MoE architecture. PiKV leverages \\textit{expert-sharded\nKV storage} to partition caches across GPUs, \\textit{PiKV routing} to reduce\ntoken-to-KV access, and a \\textit{PiKV Scheduling} to adaptively retain\nquery-relevant entries. To further reduce memory usage, PiKV integrates\n\\textit{PiKV Compression} modules the caching pipeline for acceleration.\n  PiKV is recently publicly available as an open-source software library:\n\\href{https://github.com/NoakLiu/PiKV}{https://github.com/NoakLiu/PiKV}.\nExperiments details is recorded at:\n\\href{https://github.com/NoakLiu/PiKV/blob/main/downstream_tasks/README.md}{https://github.com/NoakLiu/PiKV/Experimental\\_Results}.\nWe also have PiKV integrated with Nvidia kvpress for acceleration, details see\n\\href{https://github.com/NoakLiu/PiKVpress}{https://github.com/NoakLiu/PiKVpress}.\nPiKV is still a living project, aiming to become a comprehesive KV Cache\nmanagement system for MoE Architectures."
                },
                "authors": [
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Yanxuan Yu"
                    },
                    {
                        "name": "Ben Lengerich"
                    },
                    {
                        "name": "Ying Nian Wu"
                    },
                    {
                        "name": "Xuhong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xuhong Wang"
                },
                "author": "Xuhong Wang",
                "arxiv_comment": "Accepted to ICML ES-MoFo III WorkShop Paper Link:\n  https://openreview.net/pdf?id=hHoK1kBPd9 Github Link:\n  https://github.com/NoakLiu/PiKV",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06526v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06526v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19849v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19849v2",
                "updated": "2025-08-02T00:31:18Z",
                "updated_parsed": [
                    2025,
                    8,
                    2,
                    0,
                    31,
                    18,
                    5,
                    214,
                    0
                ],
                "published": "2025-05-26T11:35:04Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    11,
                    35,
                    4,
                    0,
                    146,
                    0
                ],
                "title": "HIT Model: A Hierarchical Interaction-Enhanced Two-Tower Model for\n  Pre-Ranking Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HIT Model: A Hierarchical Interaction-Enhanced Two-Tower Model for\n  Pre-Ranking Systems"
                },
                "summary": "Online display advertising platforms rely on pre-ranking systems to\nefficiently filter and prioritize candidate ads from large corpora, balancing\nrelevance to users with strict computational constraints. The prevailing\ntwo-tower architecture, though highly efficient due to its decoupled design and\npre-caching, suffers from cross-domain interaction and coarse similarity\nmetrics, undermining its capacity to model complex user-ad relationships. In\nthis study, we propose the Hierarchical Interaction-Enhanced Two-Tower (HIT)\nmodel, a new architecture that augments the two-tower paradigm with two key\ncomponents: $\\textit{generators}$ that pre-generate holistic vectors\nincorporating coarse-grained user-ad interactions through a dual-generator\nframework with a cosine-similarity-based generation loss as the training\nobjective, and $\\textit{multi-head representers}$ that project embeddings into\nmultiple latent subspaces to capture fine-grained, multi-faceted user interests\nand multi-dimensional ad attributes. This design enhances modeling\neffectiveness without compromising inference efficiency. Extensive experiments\non public datasets and large-scale online A/B testing on Tencent's advertising\nplatform demonstrate that HIT significantly outperforms several baselines in\nrelevance metrics, yielding a $1.66\\%$ increase in Gross Merchandise Volume and\na $1.55\\%$ improvement in Return on Investment, alongside similar serving\nlatency to the vanilla two-tower models. The HIT model has been successfully\ndeployed in Tencent's online display advertising system, serving billions of\nimpressions daily. The code is available at\nhttps://github.com/HarveyYang123/HIT_model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online display advertising platforms rely on pre-ranking systems to\nefficiently filter and prioritize candidate ads from large corpora, balancing\nrelevance to users with strict computational constraints. The prevailing\ntwo-tower architecture, though highly efficient due to its decoupled design and\npre-caching, suffers from cross-domain interaction and coarse similarity\nmetrics, undermining its capacity to model complex user-ad relationships. In\nthis study, we propose the Hierarchical Interaction-Enhanced Two-Tower (HIT)\nmodel, a new architecture that augments the two-tower paradigm with two key\ncomponents: $\\textit{generators}$ that pre-generate holistic vectors\nincorporating coarse-grained user-ad interactions through a dual-generator\nframework with a cosine-similarity-based generation loss as the training\nobjective, and $\\textit{multi-head representers}$ that project embeddings into\nmultiple latent subspaces to capture fine-grained, multi-faceted user interests\nand multi-dimensional ad attributes. This design enhances modeling\neffectiveness without compromising inference efficiency. Extensive experiments\non public datasets and large-scale online A/B testing on Tencent's advertising\nplatform demonstrate that HIT significantly outperforms several baselines in\nrelevance metrics, yielding a $1.66\\%$ increase in Gross Merchandise Volume and\na $1.55\\%$ improvement in Return on Investment, alongside similar serving\nlatency to the vanilla two-tower models. The HIT model has been successfully\ndeployed in Tencent's online display advertising system, serving billions of\nimpressions daily. The code is available at\nhttps://github.com/HarveyYang123/HIT_model."
                },
                "authors": [
                    {
                        "name": "Haoqiang Yang"
                    },
                    {
                        "name": "Congde Yuan"
                    },
                    {
                        "name": "Kun Bai"
                    },
                    {
                        "name": "Mengzhuo Guo"
                    },
                    {
                        "name": "Wei Yang"
                    },
                    {
                        "name": "Chao Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Chao Zhou"
                },
                "author": "Chao Zhou",
                "arxiv_comment": "7 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.19849v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19849v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01051v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01051v1",
                "updated": "2025-08-01T20:08:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    20,
                    8,
                    52,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T20:08:52Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    20,
                    8,
                    52,
                    4,
                    213,
                    0
                ],
                "title": "QPP-RNG: A Conceptual Quantum System for True Randomness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QPP-RNG: A Conceptual Quantum System for True Randomness"
                },
                "summary": "We propose and experimentally demonstrate the \\emph{Quasi-Superposition\nQuantum-inspired System (QSQS)} -- a conceptual quantum system for randomness\ngeneration built on measuring two conjugate observables of a permutation\nsorting process: the deterministic permutation count $n_p$ and the\nfundamentally non-deterministic sorting time $t$. By analogy with quantum\nsystems, these observables are linked by an uncertainty-like constraint:\nalgorithmic determinism ensures structural uniformity, while system-level\nfluctuations introduce irreducible unpredictability. We realize this framework\nconcretely as \\emph{QPP-RNG}, a system-embedded, software-based true random\nnumber generator (TRNG). In QPP-RNG, real-time measurements of sorting time $t$\n-- shaped by CPU pipeline jitter, cache latency, and OS scheduling --\ndynamically reseed the PRNG driving the permutation sequence. Crucially, QSQS\ntransforms initially right-skewed raw distributions of $n_p$ and $t$ into\nnearly uniform outputs after modulo reduction, thanks to internal degeneracies\nthat collapse many distinct states into the same output symbol. Empirical\nresults show that as the repetition factor $m$ increases, output entropy\nconverges toward theoretical maxima: Shannon and min-entropy values approach 8\nbits, chi-squared statistics stabilize near ideal uniformity, and bell curves\nvisually confirm the flattening from skewed to uniform distributions. Beyond\npractical implications, QSQS unifies deterministic algorithmic processes with\nnon-deterministic physical fluctuations, offering a physics-based perspective\nfor engineering true randomness in post-quantum cryptographic systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose and experimentally demonstrate the \\emph{Quasi-Superposition\nQuantum-inspired System (QSQS)} -- a conceptual quantum system for randomness\ngeneration built on measuring two conjugate observables of a permutation\nsorting process: the deterministic permutation count $n_p$ and the\nfundamentally non-deterministic sorting time $t$. By analogy with quantum\nsystems, these observables are linked by an uncertainty-like constraint:\nalgorithmic determinism ensures structural uniformity, while system-level\nfluctuations introduce irreducible unpredictability. We realize this framework\nconcretely as \\emph{QPP-RNG}, a system-embedded, software-based true random\nnumber generator (TRNG). In QPP-RNG, real-time measurements of sorting time $t$\n-- shaped by CPU pipeline jitter, cache latency, and OS scheduling --\ndynamically reseed the PRNG driving the permutation sequence. Crucially, QSQS\ntransforms initially right-skewed raw distributions of $n_p$ and $t$ into\nnearly uniform outputs after modulo reduction, thanks to internal degeneracies\nthat collapse many distinct states into the same output symbol. Empirical\nresults show that as the repetition factor $m$ increases, output entropy\nconverges toward theoretical maxima: Shannon and min-entropy values approach 8\nbits, chi-squared statistics stabilize near ideal uniformity, and bell curves\nvisually confirm the flattening from skewed to uniform distributions. Beyond\npractical implications, QSQS unifies deterministic algorithmic processes with\nnon-deterministic physical fluctuations, offering a physics-based perspective\nfor engineering true randomness in post-quantum cryptographic systems."
                },
                "authors": [
                    {
                        "name": "Randy Kuang"
                    }
                ],
                "author_detail": {
                    "name": "Randy Kuang"
                },
                "author": "Randy Kuang",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01051v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01051v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00647v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00647v1",
                "updated": "2025-08-01T14:05:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    14,
                    5,
                    44,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T14:05:44Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    14,
                    5,
                    44,
                    4,
                    213,
                    0
                ],
                "title": "Study of the HV power supply modules for the CUbesat Solar Polarimeter\n  (CUSP)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Study of the HV power supply modules for the CUbesat Solar Polarimeter\n  (CUSP)"
                },
                "summary": "The CUbesat Solar Polarimeter (CUSP) project is a CubeSat mission orbiting\nthe Earth aimed to measure the linear polarization of solar flares in the hard\nX-ray band by means of a Compton scattering polarimeter. CUSP will allow to\nstudy the magnetic reconnection and particle acceleration in the flaring\nmagnetic structures of our star. CUSP is a project in the framework of the\nAlcor Program of the Italian Space Agency aimed to develop new CubeSat\nmissions. CUSP undergoing the Phase B started in December 2024 that will last\nfor 12 month. The Compton polarimeter of the CUSP payload performs coincidence\nmeasurements between plastic scintilaltors and GaGG(Ce) crystals to derive the\npolarization of X-rays. These sensors are readout by Multi Anode\nPhotomultiplier Tubes (MAPMTs) and Avalanche Photodiodes (APDs) respectively.\nBoth sensors need an HV power supply up to -1~kV (for the MAPMT) and +500~V\n(for the APD). We tested precision regulated High Voltage DC/DC Converters by\nHVM Technology Inc. with Sub-Miniature Case Size\n($0.85''\\times0.85''\\times0.60''$) of the SMHV series. These modules are\ncompact and suited for CubeSat missions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The CUbesat Solar Polarimeter (CUSP) project is a CubeSat mission orbiting\nthe Earth aimed to measure the linear polarization of solar flares in the hard\nX-ray band by means of a Compton scattering polarimeter. CUSP will allow to\nstudy the magnetic reconnection and particle acceleration in the flaring\nmagnetic structures of our star. CUSP is a project in the framework of the\nAlcor Program of the Italian Space Agency aimed to develop new CubeSat\nmissions. CUSP undergoing the Phase B started in December 2024 that will last\nfor 12 month. The Compton polarimeter of the CUSP payload performs coincidence\nmeasurements between plastic scintilaltors and GaGG(Ce) crystals to derive the\npolarization of X-rays. These sensors are readout by Multi Anode\nPhotomultiplier Tubes (MAPMTs) and Avalanche Photodiodes (APDs) respectively.\nBoth sensors need an HV power supply up to -1~kV (for the MAPMT) and +500~V\n(for the APD). We tested precision regulated High Voltage DC/DC Converters by\nHVM Technology Inc. with Sub-Miniature Case Size\n($0.85''\\times0.85''\\times0.60''$) of the SMHV series. These modules are\ncompact and suited for CubeSat missions."
                },
                "authors": [
                    {
                        "name": "Alessandro Lacerenza"
                    },
                    {
                        "name": "Alda Rubini"
                    },
                    {
                        "name": "Andrea Alimenti"
                    },
                    {
                        "name": "Sergio Fabiani"
                    },
                    {
                        "name": "Ettore Del Monte"
                    },
                    {
                        "name": "Riccardo Campana"
                    },
                    {
                        "name": "Mauro Centrone"
                    },
                    {
                        "name": "Enrico Costa"
                    },
                    {
                        "name": "Nicolas De Angelis"
                    },
                    {
                        "name": "Giovanni De Cesare"
                    },
                    {
                        "name": "Sergio Di Cosimo"
                    },
                    {
                        "name": "Giuseppe Di Persio"
                    },
                    {
                        "name": "Abhay Kumar"
                    },
                    {
                        "name": "Pasqualino Loffredo"
                    },
                    {
                        "name": "Giovanni Lombardi"
                    },
                    {
                        "name": "Gabriele Minervini"
                    },
                    {
                        "name": "Fabio Muleri"
                    },
                    {
                        "name": "Paolo Romano"
                    },
                    {
                        "name": "Emanuele Scalise"
                    },
                    {
                        "name": "Enrico Silva"
                    },
                    {
                        "name": "Paolo Soffitta"
                    },
                    {
                        "name": "Davide Albanesi"
                    },
                    {
                        "name": "Ilaria Baffo"
                    },
                    {
                        "name": "Daniele Brienza"
                    },
                    {
                        "name": "Valerio Campamaggiore"
                    },
                    {
                        "name": "Giovanni Cucinella"
                    },
                    {
                        "name": "Andrea Curatolo"
                    },
                    {
                        "name": "Giulia de Iulis"
                    },
                    {
                        "name": "Andrea Del Re"
                    },
                    {
                        "name": "Vito Di Bari"
                    },
                    {
                        "name": "Simone Di Filippo"
                    },
                    {
                        "name": "Immacolata Donnarumma"
                    },
                    {
                        "name": "Pierluigi Fanelli"
                    },
                    {
                        "name": "Nicolas Gagliardi"
                    },
                    {
                        "name": "Paolo Leonetti"
                    },
                    {
                        "name": "Matteo Merge"
                    },
                    {
                        "name": "Dario Modenini"
                    },
                    {
                        "name": "Andrea Negri"
                    },
                    {
                        "name": "Daniele Pecorella"
                    },
                    {
                        "name": "Massimo Perelli"
                    },
                    {
                        "name": "Alice Ponti"
                    },
                    {
                        "name": "Francesca Sbop"
                    },
                    {
                        "name": "Paolo Tortora"
                    },
                    {
                        "name": "Alessandro Turchi"
                    },
                    {
                        "name": "Valerio Vagelli"
                    },
                    {
                        "name": "Emanuele Zaccagnino"
                    },
                    {
                        "name": "Alessandro Zambardi"
                    },
                    {
                        "name": "Costantino Zazza"
                    }
                ],
                "author_detail": {
                    "name": "Costantino Zazza"
                },
                "author": "Costantino Zazza",
                "arxiv_comment": "6 pages, 2 figures, SPIE Optics+Photonics 2025 proceeding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00647v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00647v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.space-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00629v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00629v1",
                "updated": "2025-08-01T13:40:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    13,
                    40,
                    52,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T13:40:52Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    13,
                    40,
                    52,
                    4,
                    213,
                    0
                ],
                "title": "Energy-Aware CPU Orchestration in O-RAN: A dApp-Driven Lightweight\n  Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Energy-Aware CPU Orchestration in O-RAN: A dApp-Driven Lightweight\n  Approach"
                },
                "summary": "The transition toward softwarized Radio Access Networks (RANs), driven by the\nOpen RAN (O-RAN) paradigm, enables flexible, vendor-neutral deployments through\ndisaggregation and virtualization of base station functions. However, this\nshift introduces new challenges in managing CPU resources efficiently under\nstrict real-time constraints. In particular, the interplay between\nlatency-sensitive RAN workloads and general-purpose Operating System (OS)\nschedulers often leads to sub-optimal performance and unnecessary energy\nconsumption. This work proposes a lightweight, programmable distributed\napplication (dApp) deployed at the Distributed Unit (DU) level to dynamically\norchestrate CPU usage. The dApp operates in closed loop with the OS, leveraging\nthread-level telemetry like context switches, Instructions Per Cycle (IPC), and\ncache metrics, to adapt CPU thread affinity, core isolation, and frequency\nscaling in real time. Unlike existing solutions, it requires no access to\nproprietary RAN software, hardware-specific features, or kernel modifications.\nFully compliant with the O-RAN architecture and agnostic to the underlying RAN\nstack, the proposed solution introduces negligible overhead while improving\nenergy efficiency and CPU utilization. Experimental results using a\ncommercial-grade srsRAN deployment demonstrate consistent power savings without\ncompromising real-time processing performance, highlighting the potential of\nlow-latency dApps for fine-grained resource control in next-generation networks",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The transition toward softwarized Radio Access Networks (RANs), driven by the\nOpen RAN (O-RAN) paradigm, enables flexible, vendor-neutral deployments through\ndisaggregation and virtualization of base station functions. However, this\nshift introduces new challenges in managing CPU resources efficiently under\nstrict real-time constraints. In particular, the interplay between\nlatency-sensitive RAN workloads and general-purpose Operating System (OS)\nschedulers often leads to sub-optimal performance and unnecessary energy\nconsumption. This work proposes a lightweight, programmable distributed\napplication (dApp) deployed at the Distributed Unit (DU) level to dynamically\norchestrate CPU usage. The dApp operates in closed loop with the OS, leveraging\nthread-level telemetry like context switches, Instructions Per Cycle (IPC), and\ncache metrics, to adapt CPU thread affinity, core isolation, and frequency\nscaling in real time. Unlike existing solutions, it requires no access to\nproprietary RAN software, hardware-specific features, or kernel modifications.\nFully compliant with the O-RAN architecture and agnostic to the underlying RAN\nstack, the proposed solution introduces negligible overhead while improving\nenergy efficiency and CPU utilization. Experimental results using a\ncommercial-grade srsRAN deployment demonstrate consistent power savings without\ncompromising real-time processing performance, highlighting the potential of\nlow-latency dApps for fine-grained resource control in next-generation networks"
                },
                "authors": [
                    {
                        "name": "Francisco Crespo"
                    },
                    {
                        "name": "Javier Villegas"
                    },
                    {
                        "name": "Carlos Baena"
                    },
                    {
                        "name": "Eduardo Baena"
                    },
                    {
                        "name": "Sergio Fortes"
                    },
                    {
                        "name": "Raquel Barco"
                    }
                ],
                "author_detail": {
                    "name": "Raquel Barco"
                },
                "author": "Raquel Barco",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00629v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00629v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00616v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00616v1",
                "updated": "2025-08-01T13:25:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    13,
                    25,
                    28,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T13:25:28Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    13,
                    25,
                    28,
                    4,
                    213,
                    0
                ],
                "title": "Joint Association and Phase Shifts Design for UAV-mounted Stacked\n  Intelligent Metasurfaces-assisted Communications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint Association and Phase Shifts Design for UAV-mounted Stacked\n  Intelligent Metasurfaces-assisted Communications"
                },
                "summary": "Stacked intelligent metasurfaces (SIMs) have emerged as a promising\ntechnology for realizing wave-domain signal processing, while the fixed SIMs\nwill limit the communication performance of the system compared to the mobile\nSIMs. In this work, we consider a UAV-mounted SIMs (UAV-SIMs) assisted\ncommunication system, where UAVs as base stations (BSs) can cache the data\nprocessed by SIMs, and also as mobile vehicles flexibly deploy SIMs to enhance\nthe communication performance. To this end, we formulate a UAV-SIM-based joint\noptimization problem (USBJOP) to comprehensively consider the association\nbetween UAV-SIMs and users, the locations of UAV-SIMs, and the phase shifts of\nUAV-SIMs, aiming to maximize the network capacity. Due to the non-convexity and\nNP-hardness of USBJOP, we decompose it into three sub-optimization problems,\nwhich are the association between UAV-SIMs and users optimization problem\n(AUUOP), the UAV location optimization problem (ULOP), and the UAV-SIM phase\nshifts optimization problem (USPSOP). Then, these three sub-optimization\nproblems are solved by an alternating optimization (AO) strategy. Specifically,\nAUUOP and ULOP are transformed to a convex form and then solved by the CVX\ntool, while we employ a layer-by-layer iterative optimization method for\nUSPSOP. Simulation results verify the effectiveness of the proposed strategy\nunder different simulation setups.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stacked intelligent metasurfaces (SIMs) have emerged as a promising\ntechnology for realizing wave-domain signal processing, while the fixed SIMs\nwill limit the communication performance of the system compared to the mobile\nSIMs. In this work, we consider a UAV-mounted SIMs (UAV-SIMs) assisted\ncommunication system, where UAVs as base stations (BSs) can cache the data\nprocessed by SIMs, and also as mobile vehicles flexibly deploy SIMs to enhance\nthe communication performance. To this end, we formulate a UAV-SIM-based joint\noptimization problem (USBJOP) to comprehensively consider the association\nbetween UAV-SIMs and users, the locations of UAV-SIMs, and the phase shifts of\nUAV-SIMs, aiming to maximize the network capacity. Due to the non-convexity and\nNP-hardness of USBJOP, we decompose it into three sub-optimization problems,\nwhich are the association between UAV-SIMs and users optimization problem\n(AUUOP), the UAV location optimization problem (ULOP), and the UAV-SIM phase\nshifts optimization problem (USPSOP). Then, these three sub-optimization\nproblems are solved by an alternating optimization (AO) strategy. Specifically,\nAUUOP and ULOP are transformed to a convex form and then solved by the CVX\ntool, while we employ a layer-by-layer iterative optimization method for\nUSPSOP. Simulation results verify the effectiveness of the proposed strategy\nunder different simulation setups."
                },
                "authors": [
                    {
                        "name": "Mingzhe Fan"
                    },
                    {
                        "name": "Geng Sun"
                    },
                    {
                        "name": "Hongyang Pan"
                    },
                    {
                        "name": "Jiacheng Wang"
                    },
                    {
                        "name": "Jiancheng An"
                    },
                    {
                        "name": "Hongyang Du"
                    },
                    {
                        "name": "Chau Yuen"
                    }
                ],
                "author_detail": {
                    "name": "Chau Yuen"
                },
                "author": "Chau Yuen",
                "arxiv_comment": "This papar has been submitted to the IEEE Global Communications\n  Conference. arXiv admin note: substantial text overlap with arXiv:2506.23488",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00616v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00616v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00412v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00412v1",
                "updated": "2025-08-01T08:10:54Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    8,
                    10,
                    54,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T08:10:54Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    8,
                    10,
                    54,
                    4,
                    213,
                    0
                ],
                "title": "Sortblock: Similarity-Aware Feature Reuse for Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sortblock: Similarity-Aware Feature Reuse for Diffusion Model"
                },
                "summary": "Diffusion Transformers (DiTs) have demonstrated remarkable generative\ncapabilities, particularly benefiting from Transformer architectures that\nenhance visual and artistic fidelity. However, their inherently sequential\ndenoising process results in high inference latency, limiting their deployment\nin real-time scenarios. Existing training-free acceleration approaches\ntypically reuse intermediate features at fixed timesteps or layers, overlooking\nthe evolving semantic focus across denoising stages and Transformer blocks.To\naddress this, we propose Sortblock, a training-free inference acceleration\nframework that dynamically caches block-wise features based on their similarity\nacross adjacent timesteps. By ranking the evolution of residuals, Sortblock\nadaptively determines a recomputation ratio, selectively skipping redundant\ncomputations while preserving generation quality. Furthermore, we incorporate a\nlightweight linear prediction mechanism to reduce accumulated errors in skipped\nblocks.Extensive experiments across various tasks and DiT architectures\ndemonstrate that Sortblock achieves over 2$\\times$ inference speedup with\nminimal degradation in output quality, offering an effective and generalizable\nsolution for accelerating diffusion-based generative models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) have demonstrated remarkable generative\ncapabilities, particularly benefiting from Transformer architectures that\nenhance visual and artistic fidelity. However, their inherently sequential\ndenoising process results in high inference latency, limiting their deployment\nin real-time scenarios. Existing training-free acceleration approaches\ntypically reuse intermediate features at fixed timesteps or layers, overlooking\nthe evolving semantic focus across denoising stages and Transformer blocks.To\naddress this, we propose Sortblock, a training-free inference acceleration\nframework that dynamically caches block-wise features based on their similarity\nacross adjacent timesteps. By ranking the evolution of residuals, Sortblock\nadaptively determines a recomputation ratio, selectively skipping redundant\ncomputations while preserving generation quality. Furthermore, we incorporate a\nlightweight linear prediction mechanism to reduce accumulated errors in skipped\nblocks.Extensive experiments across various tasks and DiT architectures\ndemonstrate that Sortblock achieves over 2$\\times$ inference speedup with\nminimal degradation in output quality, offering an effective and generalizable\nsolution for accelerating diffusion-based generative models."
                },
                "authors": [
                    {
                        "name": "Hanqi Chen"
                    },
                    {
                        "name": "Xu Zhang"
                    },
                    {
                        "name": "Xiaoliu Guan"
                    },
                    {
                        "name": "Lielin Jiang"
                    },
                    {
                        "name": "Guanzhong Wang"
                    },
                    {
                        "name": "Zeyu Chen"
                    },
                    {
                        "name": "Yi Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yi Liu"
                },
                "author": "Yi Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00412v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00412v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23387v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23387v2",
                "updated": "2025-08-01T03:43:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    3,
                    43,
                    24,
                    4,
                    213,
                    0
                ],
                "published": "2025-07-31T10:02:26Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    10,
                    2,
                    26,
                    3,
                    212,
                    0
                ],
                "title": "SGEMM-cube: Emulating FP32 GEMM on Ascend NPUs Using FP16 Cube Units\n  with Precision Recovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SGEMM-cube: Emulating FP32 GEMM on Ascend NPUs Using FP16 Cube Units\n  with Precision Recovery"
                },
                "summary": "Low-precision matrix engines, such as FP16 cube, offer high throughput but\nlack support for full-precision computation. In this work, we propose\nSGEMM-cube, a high-performance algorithm for emulating FP32 general\nmatrix-matrix multiplication (GEMM) using only FP16 computation units on a\nrepresentative AI accelerator. The method decomposes each FP32 operand into two\nFP16 values and compensates for numerical errors through a tunable scaling\nstrategy. A detailed analysis of numerical errors, including underflow\nconditions and precision loss, guides the selection of scaling parameters to\npreserve up to 22 bits of mantissa accuracy. We further investigate the effect\nof computation order on accuracy and demonstrate that a term-wise accumulation\nscheme improves numerical stability over conventional FP32 GEMM in low-exponent\nregimes. Finally, a cache-aware blocking strategy and double-buffered pipeline\nare introduced to overlap memory transfers with computation, enabling\nSGEMM-cube to achieve up to 77\\% of the theoretical FP32-equivalent peak\nperformance on Ascend 910A NPU lacking native FP32 support. Extensive numerical\nexperiments confirm that our method not only recovers the accuracy of native\nFP32 GEMM but also exhibits superior numerical stability under certain\nconditions, due to its structured and error-aware computation order.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-precision matrix engines, such as FP16 cube, offer high throughput but\nlack support for full-precision computation. In this work, we propose\nSGEMM-cube, a high-performance algorithm for emulating FP32 general\nmatrix-matrix multiplication (GEMM) using only FP16 computation units on a\nrepresentative AI accelerator. The method decomposes each FP32 operand into two\nFP16 values and compensates for numerical errors through a tunable scaling\nstrategy. A detailed analysis of numerical errors, including underflow\nconditions and precision loss, guides the selection of scaling parameters to\npreserve up to 22 bits of mantissa accuracy. We further investigate the effect\nof computation order on accuracy and demonstrate that a term-wise accumulation\nscheme improves numerical stability over conventional FP32 GEMM in low-exponent\nregimes. Finally, a cache-aware blocking strategy and double-buffered pipeline\nare introduced to overlap memory transfers with computation, enabling\nSGEMM-cube to achieve up to 77\\% of the theoretical FP32-equivalent peak\nperformance on Ascend 910A NPU lacking native FP32 support. Extensive numerical\nexperiments confirm that our method not only recovers the accuracy of native\nFP32 GEMM but also exhibits superior numerical stability under certain\nconditions, due to its structured and error-aware computation order."
                },
                "authors": [
                    {
                        "name": "Weicheng Xue"
                    },
                    {
                        "name": "Baisong Xu"
                    },
                    {
                        "name": "Kai Yang"
                    },
                    {
                        "name": "Yongxiang Liu"
                    },
                    {
                        "name": "Dengdeng Fan"
                    },
                    {
                        "name": "Pengxiang Xu"
                    },
                    {
                        "name": "Yonghong Tian"
                    }
                ],
                "author_detail": {
                    "name": "Yonghong Tian"
                },
                "author": "Yonghong Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23387v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23387v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22746v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22746v2",
                "updated": "2025-08-01T03:37:42Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    3,
                    37,
                    42,
                    4,
                    213,
                    0
                ],
                "published": "2025-07-30T15:03:36Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    15,
                    3,
                    36,
                    2,
                    211,
                    0
                ],
                "title": "Next Tokens Denoising for Speech Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Next Tokens Denoising for Speech Synthesis"
                },
                "summary": "While diffusion and autoregressive (AR) models have significantly advanced\ngenerative modeling, they each present distinct limitations. AR models, which\nrely on causal attention, cannot exploit future context and suffer from slow\ngeneration speeds. Conversely, diffusion models struggle with key-value (KV)\ncaching. To overcome these challenges, we introduce Dragon-FM, a novel\ntext-to-speech (TTS) design that unifies AR and flow-matching. This model\nprocesses 48 kHz audio codec tokens in chunks at a compact rate of 12.5 tokens\nper second. This design enables AR modeling across chunks, ensuring global\ncoherence, while parallel flow-matching within chunks facilitates fast\niterative denoising. Thus, the model leverages KV-cache across chunks and\nutilizes bidirectional context within each chunk. Furthermore, it bridges\ncontinuous and discrete feature modeling, demonstrating that continuous AR\nflow-matching can predict discrete tokens with finite scalar quantizers. This\nefficient codec and fast chunk-autoregressive architecture also make the model\nhighly effective for generating long-form content, such as podcasts.\nExperiments on podcast datasets demonstrate its capability to efficiently\ngenerate high-quality zero-shot podcasts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While diffusion and autoregressive (AR) models have significantly advanced\ngenerative modeling, they each present distinct limitations. AR models, which\nrely on causal attention, cannot exploit future context and suffer from slow\ngeneration speeds. Conversely, diffusion models struggle with key-value (KV)\ncaching. To overcome these challenges, we introduce Dragon-FM, a novel\ntext-to-speech (TTS) design that unifies AR and flow-matching. This model\nprocesses 48 kHz audio codec tokens in chunks at a compact rate of 12.5 tokens\nper second. This design enables AR modeling across chunks, ensuring global\ncoherence, while parallel flow-matching within chunks facilitates fast\niterative denoising. Thus, the model leverages KV-cache across chunks and\nutilizes bidirectional context within each chunk. Furthermore, it bridges\ncontinuous and discrete feature modeling, demonstrating that continuous AR\nflow-matching can predict discrete tokens with finite scalar quantizers. This\nefficient codec and fast chunk-autoregressive architecture also make the model\nhighly effective for generating long-form content, such as podcasts.\nExperiments on podcast datasets demonstrate its capability to efficiently\ngenerate high-quality zero-shot podcasts."
                },
                "authors": [
                    {
                        "name": "Yanqing Liu"
                    },
                    {
                        "name": "Ruiqing Xue"
                    },
                    {
                        "name": "Chong Zhang"
                    },
                    {
                        "name": "Yufei Liu"
                    },
                    {
                        "name": "Gang Wang"
                    },
                    {
                        "name": "Bohan Li"
                    },
                    {
                        "name": "Yao Qian"
                    },
                    {
                        "name": "Lei He"
                    },
                    {
                        "name": "Shujie Liu"
                    },
                    {
                        "name": "Sheng Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Sheng Zhao"
                },
                "author": "Sheng Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22746v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22746v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02659v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02659v2",
                "updated": "2025-07-31T21:00:28Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    21,
                    0,
                    28,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-03T14:20:41Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    14,
                    20,
                    41,
                    3,
                    184,
                    0
                ],
                "title": "OmniDraft: A Cross-vocabulary, Online Adaptive Drafter for On-device\n  Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OmniDraft: A Cross-vocabulary, Online Adaptive Drafter for On-device\n  Speculative Decoding"
                },
                "summary": "Speculative decoding generally dictates having a small, efficient draft model\nthat is either pretrained or distilled offline to a particular target model\nseries, for instance, Llama or Qwen models. However, within online deployment\nsettings, there are two major challenges: 1) usage of a target model that is\nincompatible with the draft model; 2) expectation of latency improvements over\nusage and time. In this work, we propose OmniDraft, a unified framework that\nenables a single draft model to operate with any target model and adapt\ndynamically to user data. We introduce an online n-gram cache with hybrid\ndistillation fine-tuning to address the cross-vocabulary mismatch across draft\nand target models; and further improve decoding speed by leveraging adaptive\ndrafting techniques. OmniDraft is particularly suitable for on-device LLM\napplications where model cost, efficiency and user customization are the major\npoints of contention. This further highlights the need to tackle the above\nchallenges and motivates the \\textit{``one drafter for all''} paradigm. We\nshowcase the proficiency of the OmniDraft framework by performing online\nlearning on math reasoning, coding and text generation tasks. Notably,\nOmniDraft enables a single Llama-68M model to pair with various target models\nincluding Vicuna-7B, Qwen2-7B and Llama3-8B models for speculative decoding;\nand additionally provides up to 1.5-2x speedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding generally dictates having a small, efficient draft model\nthat is either pretrained or distilled offline to a particular target model\nseries, for instance, Llama or Qwen models. However, within online deployment\nsettings, there are two major challenges: 1) usage of a target model that is\nincompatible with the draft model; 2) expectation of latency improvements over\nusage and time. In this work, we propose OmniDraft, a unified framework that\nenables a single draft model to operate with any target model and adapt\ndynamically to user data. We introduce an online n-gram cache with hybrid\ndistillation fine-tuning to address the cross-vocabulary mismatch across draft\nand target models; and further improve decoding speed by leveraging adaptive\ndrafting techniques. OmniDraft is particularly suitable for on-device LLM\napplications where model cost, efficiency and user customization are the major\npoints of contention. This further highlights the need to tackle the above\nchallenges and motivates the \\textit{``one drafter for all''} paradigm. We\nshowcase the proficiency of the OmniDraft framework by performing online\nlearning on math reasoning, coding and text generation tasks. Notably,\nOmniDraft enables a single Llama-68M model to pair with various target models\nincluding Vicuna-7B, Qwen2-7B and Llama3-8B models for speculative decoding;\nand additionally provides up to 1.5-2x speedup."
                },
                "authors": [
                    {
                        "name": "Ramchalam Kinattinkara Ramakrishnan"
                    },
                    {
                        "name": "Zhaocong Yuan"
                    },
                    {
                        "name": "Shaojie Zhuo"
                    },
                    {
                        "name": "Chen Feng"
                    },
                    {
                        "name": "Yicheng Lin"
                    },
                    {
                        "name": "Chenzheng Su"
                    },
                    {
                        "name": "Xiaopeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaopeng Zhang"
                },
                "author": "Xiaopeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02659v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02659v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22701v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22701v2",
                "updated": "2025-07-31T16:21:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    16,
                    21,
                    3,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-30T14:10:16Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    14,
                    10,
                    16,
                    2,
                    211,
                    0
                ],
                "title": "SAM: A Stability-Aware Cache Manager for Multi-Tenant Embedded Databases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SAM: A Stability-Aware Cache Manager for Multi-Tenant Embedded Databases"
                },
                "summary": "The co-location of multiple database instances on resource constrained edge\nnodes creates significant cache contention, where traditional schemes are\ninefficient and unstable under dynamic workloads. To address this, we present\nSAM(a Stability-Aware Manager), an autonomic cache manager that establishes\ndecision stability as a first-class design principle. It achieves this through\nits core control policy, AURA(Autonomic Utility-balancing Resource Allocator),\nwhich resolves the classic exploitation-exploration dilemma by synthesizing two\northogonal factors: the H-factor, representing proven historical efficiency\n(exploitation), and the V-factor, for estimated marginal gain (exploration).\nThrough this practical synthesis and adaptive control, SAM achieves sustained\nhigh performance with strategic stability and robustness in volatile\nconditions.\n  Extensive experiments against 14 diverse baselines demonstrate SAM's\nsuperiority. It achieves top-tier throughput while being uniquely resilient to\ncomplex workload shifts and adversarial workloads like cache pollution.\nFurthermore, its decision latency is highly scalable, remaining nearly constant\nas the system grows to 120 databases. Crucially, SAM achieves superior decision\nstability -- maintaining consistent optimization directions despite noise,\navoiding performance oscillations while ensuring predictable Quality of\nService. These results prove that a principled, stability-aware design is\nessential for sustained high performance in real-world, large-scale systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The co-location of multiple database instances on resource constrained edge\nnodes creates significant cache contention, where traditional schemes are\ninefficient and unstable under dynamic workloads. To address this, we present\nSAM(a Stability-Aware Manager), an autonomic cache manager that establishes\ndecision stability as a first-class design principle. It achieves this through\nits core control policy, AURA(Autonomic Utility-balancing Resource Allocator),\nwhich resolves the classic exploitation-exploration dilemma by synthesizing two\northogonal factors: the H-factor, representing proven historical efficiency\n(exploitation), and the V-factor, for estimated marginal gain (exploration).\nThrough this practical synthesis and adaptive control, SAM achieves sustained\nhigh performance with strategic stability and robustness in volatile\nconditions.\n  Extensive experiments against 14 diverse baselines demonstrate SAM's\nsuperiority. It achieves top-tier throughput while being uniquely resilient to\ncomplex workload shifts and adversarial workloads like cache pollution.\nFurthermore, its decision latency is highly scalable, remaining nearly constant\nas the system grows to 120 databases. Crucially, SAM achieves superior decision\nstability -- maintaining consistent optimization directions despite noise,\navoiding performance oscillations while ensuring predictable Quality of\nService. These results prove that a principled, stability-aware design is\nessential for sustained high performance in real-world, large-scale systems."
                },
                "authors": [
                    {
                        "name": "Haoran Zhang"
                    },
                    {
                        "name": "Decheng Zuo"
                    },
                    {
                        "name": "Yu Yan"
                    },
                    {
                        "name": "Zhiyu Liang"
                    },
                    {
                        "name": "Hongzhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hongzhi Wang"
                },
                "author": "Hongzhi Wang",
                "arxiv_comment": "17 pages, 10 figures. An extended version of a paper under review at\n  the VLDB 2026 conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22701v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22701v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.2.4; H.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2508.10899v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10899v1",
                "updated": "2025-08-14T17:59:37Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    17,
                    59,
                    37,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T17:59:37Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    17,
                    59,
                    37,
                    3,
                    226,
                    0
                ],
                "title": "A Dataset for Distilling Knowledge Priors from Literature for\n  Therapeutic Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Dataset for Distilling Knowledge Priors from Literature for\n  Therapeutic Design"
                },
                "summary": "AI-driven discovery can greatly reduce design time and enhance new\ntherapeutics' effectiveness. Models using simulators explore broad design\nspaces but risk violating implicit constraints due to a lack of experimental\npriors. For example, in a new analysis we performed on a diverse set of models\non the GuacaMol benchmark using supervised classifiers, over 60\\% of molecules\nproposed had high probability of being mutagenic. In this work, we introduce\n\\ourdataset, a dataset of priors for design problems extracted from literature\ndescribing compounds used in lab settings. It is constructed with LLM pipelines\nfor discovering therapeutic entities in relevant paragraphs and summarizing\ninformation in concise fair-use facts. \\ourdataset~ consists of 32.3 million\npairs of natural language facts, and appropriate entity representations (i.e.\nSMILES or refseq IDs). To demonstrate the potential of the data, we train LLM,\nCLIP, and LLava architectures to reason jointly about text and design targets\nand evaluate on tasks from the Therapeutic Data Commons (TDC). \\ourdataset~is\nhighly effective for creating models with strong priors: in supervised\nprediction problems that use our data as pretraining, our best models with 15M\nlearnable parameters outperform larger 2B TxGemma on both regression and\nclassification TDC tasks, and perform comparably to 9B models on average.\nModels built with \\ourdataset~can be used as constraints while optimizing for\nnovel molecules in GuacaMol, resulting in proposals that are safer and nearly\nas effective. We release our dataset at\n\\href{https://huggingface.co/datasets/medexanon/Medex}{huggingface.co/datasets/medexanon/Medex},\nand will provide expanded versions as available literature grows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI-driven discovery can greatly reduce design time and enhance new\ntherapeutics' effectiveness. Models using simulators explore broad design\nspaces but risk violating implicit constraints due to a lack of experimental\npriors. For example, in a new analysis we performed on a diverse set of models\non the GuacaMol benchmark using supervised classifiers, over 60\\% of molecules\nproposed had high probability of being mutagenic. In this work, we introduce\n\\ourdataset, a dataset of priors for design problems extracted from literature\ndescribing compounds used in lab settings. It is constructed with LLM pipelines\nfor discovering therapeutic entities in relevant paragraphs and summarizing\ninformation in concise fair-use facts. \\ourdataset~ consists of 32.3 million\npairs of natural language facts, and appropriate entity representations (i.e.\nSMILES or refseq IDs). To demonstrate the potential of the data, we train LLM,\nCLIP, and LLava architectures to reason jointly about text and design targets\nand evaluate on tasks from the Therapeutic Data Commons (TDC). \\ourdataset~is\nhighly effective for creating models with strong priors: in supervised\nprediction problems that use our data as pretraining, our best models with 15M\nlearnable parameters outperform larger 2B TxGemma on both regression and\nclassification TDC tasks, and perform comparably to 9B models on average.\nModels built with \\ourdataset~can be used as constraints while optimizing for\nnovel molecules in GuacaMol, resulting in proposals that are safer and nearly\nas effective. We release our dataset at\n\\href{https://huggingface.co/datasets/medexanon/Medex}{huggingface.co/datasets/medexanon/Medex},\nand will provide expanded versions as available literature grows."
                },
                "authors": [
                    {
                        "name": "Haydn Thomas Jones"
                    },
                    {
                        "name": "Natalie Maus"
                    },
                    {
                        "name": "Josh Magnus Ludan"
                    },
                    {
                        "name": "Maggie Ziyu Huan"
                    },
                    {
                        "name": "Jiaming Liang"
                    },
                    {
                        "name": "Marcelo Der Torossian Torres"
                    },
                    {
                        "name": "Jiatao Liang"
                    },
                    {
                        "name": "Zachary Ives"
                    },
                    {
                        "name": "Yoseph Barash"
                    },
                    {
                        "name": "Cesar de la Fuente-Nunez"
                    },
                    {
                        "name": "Jacob R. Gardner"
                    },
                    {
                        "name": "Mark Yatskar"
                    }
                ],
                "author_detail": {
                    "name": "Mark Yatskar"
                },
                "author": "Mark Yatskar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10899v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10899v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10898v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10898v1",
                "updated": "2025-08-14T17:59:31Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    17,
                    59,
                    31,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T17:59:31Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    17,
                    59,
                    31,
                    3,
                    226,
                    0
                ],
                "title": "Puppeteer: Rig and Animate Your 3D Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Puppeteer: Rig and Animate Your 3D Models"
                },
                "summary": "Modern interactive applications increasingly demand dynamic 3D content, yet\nthe transformation of static 3D models into animated assets constitutes a\nsignificant bottleneck in content creation pipelines. While recent advances in\ngenerative AI have revolutionized static 3D model creation, rigging and\nanimation continue to depend heavily on expert intervention. We present\nPuppeteer, a comprehensive framework that addresses both automatic rigging and\nanimation for diverse 3D objects. Our system first predicts plausible skeletal\nstructures via an auto-regressive transformer that introduces a joint-based\ntokenization strategy for compact representation and a hierarchical ordering\nmethodology with stochastic perturbation that enhances bidirectional learning\ncapabilities. It then infers skinning weights via an attention-based\narchitecture incorporating topology-aware joint attention that explicitly\nencodes inter-joint relationships based on skeletal graph distances. Finally,\nwe complement these rigging advances with a differentiable optimization-based\nanimation pipeline that generates stable, high-fidelity animations while being\ncomputationally more efficient than existing approaches. Extensive evaluations\nacross multiple benchmarks demonstrate that our method significantly\noutperforms state-of-the-art techniques in both skeletal prediction accuracy\nand skinning quality. The system robustly processes diverse 3D content, ranging\nfrom professionally designed game assets to AI-generated shapes, producing\ntemporally coherent animations that eliminate the jittering issues common in\nexisting methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern interactive applications increasingly demand dynamic 3D content, yet\nthe transformation of static 3D models into animated assets constitutes a\nsignificant bottleneck in content creation pipelines. While recent advances in\ngenerative AI have revolutionized static 3D model creation, rigging and\nanimation continue to depend heavily on expert intervention. We present\nPuppeteer, a comprehensive framework that addresses both automatic rigging and\nanimation for diverse 3D objects. Our system first predicts plausible skeletal\nstructures via an auto-regressive transformer that introduces a joint-based\ntokenization strategy for compact representation and a hierarchical ordering\nmethodology with stochastic perturbation that enhances bidirectional learning\ncapabilities. It then infers skinning weights via an attention-based\narchitecture incorporating topology-aware joint attention that explicitly\nencodes inter-joint relationships based on skeletal graph distances. Finally,\nwe complement these rigging advances with a differentiable optimization-based\nanimation pipeline that generates stable, high-fidelity animations while being\ncomputationally more efficient than existing approaches. Extensive evaluations\nacross multiple benchmarks demonstrate that our method significantly\noutperforms state-of-the-art techniques in both skeletal prediction accuracy\nand skinning quality. The system robustly processes diverse 3D content, ranging\nfrom professionally designed game assets to AI-generated shapes, producing\ntemporally coherent animations that eliminate the jittering issues common in\nexisting methods."
                },
                "authors": [
                    {
                        "name": "Chaoyue Song"
                    },
                    {
                        "name": "Xiu Li"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Zhongcong Xu"
                    },
                    {
                        "name": "Jiacheng Wei"
                    },
                    {
                        "name": "Fayao Liu"
                    },
                    {
                        "name": "Jiashi Feng"
                    },
                    {
                        "name": "Guosheng Lin"
                    },
                    {
                        "name": "Jianfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jianfeng Zhang"
                },
                "author": "Jianfeng Zhang",
                "arxiv_comment": "Project page: https://chaoyuesong.github.io/Puppeteer/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10898v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10898v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10535v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10535v2",
                "updated": "2025-08-14T17:58:50Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    17,
                    58,
                    50,
                    3,
                    226,
                    0
                ],
                "published": "2025-07-14T17:56:29Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    17,
                    56,
                    29,
                    0,
                    195,
                    0
                ],
                "title": "CodeJudgeBench: Benchmarking LLM-as-a-Judge for Coding Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeJudgeBench: Benchmarking LLM-as-a-Judge for Coding Tasks"
                },
                "summary": "Large Language Models (LLMs) have significantly advanced the state-of-the-art\nin various coding tasks. Beyond directly answering user queries, LLMs can also\nserve as judges, assessing and comparing the quality of responses generated by\nother models. Such an evaluation capability is crucial both for benchmarking\ndifferent LLMs and for improving response quality through response ranking.\nHowever, despite the growing adoption of the LLM-as-a-Judge paradigm, its\neffectiveness in coding scenarios remains underexplored due to the absence of\ndedicated benchmarks. To address this gap, we introduce CodeJudgeBench, a\nbenchmark explicitly designed to evaluate the performance of LLM-as-a-Judge\nmodels across three critical coding tasks: code generation, code repair, and\nunit test generation. Through comprehensive benchmarking of 26 LLM-as-a-Judge\nmodels, we find that recent thinking models significantly outperform\nnon-thinking models on our carefully designed code judging tasks. Notably, even\nrelatively small thinking models, such as Qwen3-8B, can outperform specially\ntrained LLM-as-a-Judge models up to 70B in size. Nevertheless, all models still\nexhibit significant randomness in their judgment of coding tasks. For pairwise\njudging tasks, simply changing the order in which responses are presented can\nsubstantially impact accuracy. In addition, when judging code and unit tests\nwritten by different LLMs, LLM-as-a-Judge models also show variance in\nperformance. This sensitivity raises concerns about the reliability and\nconsistency of LLM-as-a-Judge in coding scenarios. Lastly, we study optimal\nprompting strategies for LLM-as-a-Judge. We find that using pair-wise\ncomparison outperforms scalar point-wise judging. Furthermore, retaining\ncomments and reasoning in the full, unprocessed LLM response leads to improved\njudge performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have significantly advanced the state-of-the-art\nin various coding tasks. Beyond directly answering user queries, LLMs can also\nserve as judges, assessing and comparing the quality of responses generated by\nother models. Such an evaluation capability is crucial both for benchmarking\ndifferent LLMs and for improving response quality through response ranking.\nHowever, despite the growing adoption of the LLM-as-a-Judge paradigm, its\neffectiveness in coding scenarios remains underexplored due to the absence of\ndedicated benchmarks. To address this gap, we introduce CodeJudgeBench, a\nbenchmark explicitly designed to evaluate the performance of LLM-as-a-Judge\nmodels across three critical coding tasks: code generation, code repair, and\nunit test generation. Through comprehensive benchmarking of 26 LLM-as-a-Judge\nmodels, we find that recent thinking models significantly outperform\nnon-thinking models on our carefully designed code judging tasks. Notably, even\nrelatively small thinking models, such as Qwen3-8B, can outperform specially\ntrained LLM-as-a-Judge models up to 70B in size. Nevertheless, all models still\nexhibit significant randomness in their judgment of coding tasks. For pairwise\njudging tasks, simply changing the order in which responses are presented can\nsubstantially impact accuracy. In addition, when judging code and unit tests\nwritten by different LLMs, LLM-as-a-Judge models also show variance in\nperformance. This sensitivity raises concerns about the reliability and\nconsistency of LLM-as-a-Judge in coding scenarios. Lastly, we study optimal\nprompting strategies for LLM-as-a-Judge. We find that using pair-wise\ncomparison outperforms scalar point-wise judging. Furthermore, retaining\ncomments and reasoning in the full, unprocessed LLM response leads to improved\njudge performance."
                },
                "authors": [
                    {
                        "name": "Hongchao Jiang"
                    },
                    {
                        "name": "Yiming Chen"
                    },
                    {
                        "name": "Yushi Cao"
                    },
                    {
                        "name": "Hung-yi Lee"
                    },
                    {
                        "name": "Robby T. Tan"
                    }
                ],
                "author_detail": {
                    "name": "Robby T. Tan"
                },
                "author": "Robby T. Tan",
                "arxiv_comment": "Dataset is available at\n  https://huggingface.co/datasets/mattymchen/codejudgebench",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10535v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10535v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10893v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10893v1",
                "updated": "2025-08-14T17:58:05Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    17,
                    58,
                    5,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T17:58:05Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    17,
                    58,
                    5,
                    3,
                    226,
                    0
                ],
                "title": "STream3R: Scalable Sequential 3D Reconstruction with Causal Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STream3R: Scalable Sequential 3D Reconstruction with Causal Transformer"
                },
                "summary": "We present STream3R, a novel approach to 3D reconstruction that reformulates\npointmap prediction as a decoder-only Transformer problem. Existing\nstate-of-the-art methods for multi-view reconstruction either depend on\nexpensive global optimization or rely on simplistic memory mechanisms that\nscale poorly with sequence length. In contrast, STream3R introduces an\nstreaming framework that processes image sequences efficiently using causal\nattention, inspired by advances in modern language modeling. By learning\ngeometric priors from large-scale 3D datasets, STream3R generalizes well to\ndiverse and challenging scenarios, including dynamic scenes where traditional\nmethods often fail. Extensive experiments show that our method consistently\noutperforms prior work across both static and dynamic scene benchmarks.\nMoreover, STream3R is inherently compatible with LLM-style training\ninfrastructure, enabling efficient large-scale pretraining and fine-tuning for\nvarious downstream 3D tasks. Our results underscore the potential of causal\nTransformer models for online 3D perception, paving the way for real-time 3D\nunderstanding in streaming environments. More details can be found in our\nproject page: https://nirvanalan.github.io/projects/stream3r.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present STream3R, a novel approach to 3D reconstruction that reformulates\npointmap prediction as a decoder-only Transformer problem. Existing\nstate-of-the-art methods for multi-view reconstruction either depend on\nexpensive global optimization or rely on simplistic memory mechanisms that\nscale poorly with sequence length. In contrast, STream3R introduces an\nstreaming framework that processes image sequences efficiently using causal\nattention, inspired by advances in modern language modeling. By learning\ngeometric priors from large-scale 3D datasets, STream3R generalizes well to\ndiverse and challenging scenarios, including dynamic scenes where traditional\nmethods often fail. Extensive experiments show that our method consistently\noutperforms prior work across both static and dynamic scene benchmarks.\nMoreover, STream3R is inherently compatible with LLM-style training\ninfrastructure, enabling efficient large-scale pretraining and fine-tuning for\nvarious downstream 3D tasks. Our results underscore the potential of causal\nTransformer models for online 3D perception, paving the way for real-time 3D\nunderstanding in streaming environments. More details can be found in our\nproject page: https://nirvanalan.github.io/projects/stream3r."
                },
                "authors": [
                    {
                        "name": "Yushi Lan"
                    },
                    {
                        "name": "Yihang Luo"
                    },
                    {
                        "name": "Fangzhou Hong"
                    },
                    {
                        "name": "Shangchen Zhou"
                    },
                    {
                        "name": "Honghua Chen"
                    },
                    {
                        "name": "Zhaoyang Lyu"
                    },
                    {
                        "name": "Shuai Yang"
                    },
                    {
                        "name": "Bo Dai"
                    },
                    {
                        "name": "Chen Change Loy"
                    },
                    {
                        "name": "Xingang Pan"
                    }
                ],
                "author_detail": {
                    "name": "Xingang Pan"
                },
                "author": "Xingang Pan",
                "arxiv_comment": "TL;DR: Streaming 4D reconstruction using causal transformer. Project\n  page: https://nirvanalan.github.io/projects/stream3r",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10893v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10893v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08855v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08855v2",
                "updated": "2025-08-14T17:57:53Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    17,
                    57,
                    53,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-12T11:23:44Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    11,
                    23,
                    44,
                    1,
                    224,
                    0
                ],
                "title": "BiasGym: Fantastic LLM Biases and How to Find (and Remove) Them",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BiasGym: Fantastic LLM Biases and How to Find (and Remove) Them"
                },
                "summary": "Understanding biases and stereotypes encoded in the weights of Large Language\nModels (LLMs) is crucial for developing effective mitigation strategies. Biased\nbehaviour is often subtle and non-trivial to isolate, even when deliberately\nelicited, making systematic analysis and debiasing particularly challenging. To\naddress this, we introduce BiasGym, a simple, cost-effective, and generalizable\nframework for reliably injecting, analyzing, and mitigating conceptual\nassociations within LLMs. BiasGym consists of two components: BiasInject, which\ninjects specific biases into the model via token-based fine-tuning while\nkeeping the model frozen, and BiasScope, which leverages these injected signals\nto identify and steer the components responsible for biased behavior. Our\nmethod enables consistent bias elicitation for mechanistic analysis, supports\ntargeted debiasing without degrading performance on downstream tasks, and\ngeneralizes to biases unseen during token-based fine-tuning. We demonstrate the\neffectiveness of BiasGym in reducing real-world stereotypes (e.g., people from\nItaly being `reckless drivers') and in probing fictional associations (e.g.,\npeople from a fictional country having `blue skin'), showing its utility for\nboth safety interventions and interpretability research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding biases and stereotypes encoded in the weights of Large Language\nModels (LLMs) is crucial for developing effective mitigation strategies. Biased\nbehaviour is often subtle and non-trivial to isolate, even when deliberately\nelicited, making systematic analysis and debiasing particularly challenging. To\naddress this, we introduce BiasGym, a simple, cost-effective, and generalizable\nframework for reliably injecting, analyzing, and mitigating conceptual\nassociations within LLMs. BiasGym consists of two components: BiasInject, which\ninjects specific biases into the model via token-based fine-tuning while\nkeeping the model frozen, and BiasScope, which leverages these injected signals\nto identify and steer the components responsible for biased behavior. Our\nmethod enables consistent bias elicitation for mechanistic analysis, supports\ntargeted debiasing without degrading performance on downstream tasks, and\ngeneralizes to biases unseen during token-based fine-tuning. We demonstrate the\neffectiveness of BiasGym in reducing real-world stereotypes (e.g., people from\nItaly being `reckless drivers') and in probing fictional associations (e.g.,\npeople from a fictional country having `blue skin'), showing its utility for\nboth safety interventions and interpretability research."
                },
                "authors": [
                    {
                        "name": "Sekh Mainul Islam"
                    },
                    {
                        "name": "Nadav Borenstein"
                    },
                    {
                        "name": "Siddhesh Milind Pawar"
                    },
                    {
                        "name": "Haeun Yu"
                    },
                    {
                        "name": "Arnav Arora"
                    },
                    {
                        "name": "Isabelle Augenstein"
                    }
                ],
                "author_detail": {
                    "name": "Isabelle Augenstein"
                },
                "author": "Isabelle Augenstein",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08855v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08855v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10880v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10880v1",
                "updated": "2025-08-14T17:49:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    17,
                    49,
                    9,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T17:49:09Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    17,
                    49,
                    9,
                    3,
                    226,
                    0
                ],
                "title": "Searching for Privacy Risks in LLM Agents via Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Searching for Privacy Risks in LLM Agents via Simulation"
                },
                "summary": "The widespread deployment of LLM-based agents is likely to introduce a\ncritical privacy threat: malicious agents that proactively engage others in\nmulti-turn interactions to extract sensitive information. These dynamic\ndialogues enable adaptive attack strategies that can cause severe privacy\nviolations, yet their evolving nature makes it difficult to anticipate and\ndiscover sophisticated vulnerabilities manually. To tackle this problem, we\npresent a search-based framework that alternates between improving attacker and\ndefender instructions by simulating privacy-critical agent interactions. Each\nsimulation involves three roles: data subject, data sender, and data recipient.\nWhile the data subject's behavior is fixed, the attacker (data recipient)\nattempts to extract sensitive information from the defender (data sender)\nthrough persistent and interactive exchanges. To explore this interaction space\nefficiently, our search algorithm employs LLMs as optimizers, using parallel\nsearch with multiple threads and cross-thread propagation to analyze simulation\ntrajectories and iteratively propose new instructions. Through this process, we\nfind that attack strategies escalate from simple direct requests to\nsophisticated multi-turn tactics such as impersonation and consent forgery,\nwhile defenses advance from rule-based constraints to identity-verification\nstate machines. The discovered attacks and defenses transfer across diverse\nscenarios and backbone models, demonstrating strong practical utility for\nbuilding privacy-aware agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread deployment of LLM-based agents is likely to introduce a\ncritical privacy threat: malicious agents that proactively engage others in\nmulti-turn interactions to extract sensitive information. These dynamic\ndialogues enable adaptive attack strategies that can cause severe privacy\nviolations, yet their evolving nature makes it difficult to anticipate and\ndiscover sophisticated vulnerabilities manually. To tackle this problem, we\npresent a search-based framework that alternates between improving attacker and\ndefender instructions by simulating privacy-critical agent interactions. Each\nsimulation involves three roles: data subject, data sender, and data recipient.\nWhile the data subject's behavior is fixed, the attacker (data recipient)\nattempts to extract sensitive information from the defender (data sender)\nthrough persistent and interactive exchanges. To explore this interaction space\nefficiently, our search algorithm employs LLMs as optimizers, using parallel\nsearch with multiple threads and cross-thread propagation to analyze simulation\ntrajectories and iteratively propose new instructions. Through this process, we\nfind that attack strategies escalate from simple direct requests to\nsophisticated multi-turn tactics such as impersonation and consent forgery,\nwhile defenses advance from rule-based constraints to identity-verification\nstate machines. The discovered attacks and defenses transfer across diverse\nscenarios and backbone models, demonstrating strong practical utility for\nbuilding privacy-aware agents."
                },
                "authors": [
                    {
                        "name": "Yanzhe Zhang"
                    },
                    {
                        "name": "Diyi Yang"
                    }
                ],
                "author_detail": {
                    "name": "Diyi Yang"
                },
                "author": "Diyi Yang",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10880v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10880v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10876v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10876v1",
                "updated": "2025-08-14T17:47:41Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    17,
                    47,
                    41,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T17:47:41Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    17,
                    47,
                    41,
                    3,
                    226,
                    0
                ],
                "title": "Accelerating cosmological inference of interacting dark energy with\n  neural emulators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating cosmological inference of interacting dark energy with\n  neural emulators"
                },
                "summary": "The present thesis aims to tackle two critical aspects of present and future\ncosmological analysis of Large-Scale Structure (LSS): accurate modelling of the\nnonlinear matter power spectrum beyond $\\Lambda$CDM, and efficient\ncomputational techniques for Bayesian parameter estimation. Both are crucial\nfor testing alternative cosmologies and avoiding spurious results. We focus on\nthe Dark Scattering (DS) model, describing pure momentum transfer between dark\nmatter -- dark energy through the parameter $A_{\\rm ds}$. To capture DS\neffects, we adopt the halo model reaction framework within $\\tt{ReACT}$,\ncompute the nonlinear DS spectrum, and validate it against $N$-body\nsimulations. We further include baryonic feedback and massive neutrinos,\nfinding degeneracies between DS and baryonic effects but not with neutrinos. We\nthen constrain DS using cosmic shear from KiDS-1000, accelerated by neural\nemulators from $\\tt{CosmoPower}$, which speed up predictions by\n$\\mathcal{O}(10^4)$. Our DS emulator, trained on halo model reaction outputs,\npreserves percent-level accuracy and incorporates baryonic feedback. Analysing\nKiDS shear statistics, we obtain $\\vert A_{\\rm ds}\\vert \\lesssim 20$ b/GeV at\n$68 \\%$ C.L. Combining KiDS with Planck CMB and BAO data, we find $A_{\\rm\nds}=10.6^{+4.5}_{-7.3}$ b/GeV at $68 \\%$ C.L., suggesting the DS model as a\npromising resolution to the $S_8$ tension. Finally, we present weak lensing\nforecasts for Stage IV surveys using an automatically differentiable pipeline\nwith $\\tt{jax-cosmo}$ and gradient-based samplers in $\\tt{NumPyro}$, reducing\ncomputational cost from months on CPUs to days on GPUs. Model evidence is\nevaluated with $\\tt{harmonic}$ under multiple scale cuts. To put things into\nperspective, the modelling strategies and machine learning accelerations\ndeveloped here provide powerful tools for the next generation of LSS cosmology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The present thesis aims to tackle two critical aspects of present and future\ncosmological analysis of Large-Scale Structure (LSS): accurate modelling of the\nnonlinear matter power spectrum beyond $\\Lambda$CDM, and efficient\ncomputational techniques for Bayesian parameter estimation. Both are crucial\nfor testing alternative cosmologies and avoiding spurious results. We focus on\nthe Dark Scattering (DS) model, describing pure momentum transfer between dark\nmatter -- dark energy through the parameter $A_{\\rm ds}$. To capture DS\neffects, we adopt the halo model reaction framework within $\\tt{ReACT}$,\ncompute the nonlinear DS spectrum, and validate it against $N$-body\nsimulations. We further include baryonic feedback and massive neutrinos,\nfinding degeneracies between DS and baryonic effects but not with neutrinos. We\nthen constrain DS using cosmic shear from KiDS-1000, accelerated by neural\nemulators from $\\tt{CosmoPower}$, which speed up predictions by\n$\\mathcal{O}(10^4)$. Our DS emulator, trained on halo model reaction outputs,\npreserves percent-level accuracy and incorporates baryonic feedback. Analysing\nKiDS shear statistics, we obtain $\\vert A_{\\rm ds}\\vert \\lesssim 20$ b/GeV at\n$68 \\%$ C.L. Combining KiDS with Planck CMB and BAO data, we find $A_{\\rm\nds}=10.6^{+4.5}_{-7.3}$ b/GeV at $68 \\%$ C.L., suggesting the DS model as a\npromising resolution to the $S_8$ tension. Finally, we present weak lensing\nforecasts for Stage IV surveys using an automatically differentiable pipeline\nwith $\\tt{jax-cosmo}$ and gradient-based samplers in $\\tt{NumPyro}$, reducing\ncomputational cost from months on CPUs to days on GPUs. Model evidence is\nevaluated with $\\tt{harmonic}$ under multiple scale cuts. To put things into\nperspective, the modelling strategies and machine learning accelerations\ndeveloped here provide powerful tools for the next generation of LSS cosmology."
                },
                "authors": [
                    {
                        "name": "Karim Carrion"
                    }
                ],
                "author_detail": {
                    "name": "Karim Carrion"
                },
                "author": "Karim Carrion",
                "arxiv_comment": "Ph.D. thesis (defended July 2025). 138 pages + appendices, 38 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10876v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10876v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10875v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10875v1",
                "updated": "2025-08-14T17:47:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    17,
                    47,
                    22,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T17:47:22Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    17,
                    47,
                    22,
                    3,
                    226,
                    0
                ],
                "title": "A Survey on Diffusion Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Diffusion Language Models"
                },
                "summary": "Diffusion Language Models (DLMs) are rapidly emerging as a powerful and\npromising alternative to the dominant autoregressive (AR) paradigm. By\ngenerating tokens in parallel through an iterative denoising process, DLMs\npossess inherent advantages in reducing inference latency and capturing\nbidirectional context, thereby enabling fine-grained control over the\ngeneration process. While achieving a several-fold speed-up, recent\nadvancements have allowed DLMs to show performance comparable to their\nautoregressive counterparts, making them a compelling choice for various\nnatural language processing tasks. In this survey, we provide a holistic\noverview of the current DLM landscape. We trace its evolution and relationship\nwith other paradigms, such as autoregressive and masked language models, and\ncover both foundational principles and state-of-the-art models. Our work offers\nan up-to-date, comprehensive taxonomy and an in-depth analysis of current\ntechniques, from pre-training strategies to advanced post-training methods.\nAnother contribution of this survey is a thorough review of DLM inference\nstrategies and optimizations, including improvements in decoding parallelism,\ncaching mechanisms, and generation quality. We also highlight the latest\napproaches to multimodal extensions of DLMs and delineate their applications\nacross various practical scenarios. Furthermore, our discussion addresses the\nlimitations and challenges of DLMs, including efficiency, long-sequence\nhandling, and infrastructure requirements, while outlining future research\ndirections to sustain progress in this rapidly evolving field. Project GitHub\nis available at https://github.com/VILA-Lab/Awesome-DLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Language Models (DLMs) are rapidly emerging as a powerful and\npromising alternative to the dominant autoregressive (AR) paradigm. By\ngenerating tokens in parallel through an iterative denoising process, DLMs\npossess inherent advantages in reducing inference latency and capturing\nbidirectional context, thereby enabling fine-grained control over the\ngeneration process. While achieving a several-fold speed-up, recent\nadvancements have allowed DLMs to show performance comparable to their\nautoregressive counterparts, making them a compelling choice for various\nnatural language processing tasks. In this survey, we provide a holistic\noverview of the current DLM landscape. We trace its evolution and relationship\nwith other paradigms, such as autoregressive and masked language models, and\ncover both foundational principles and state-of-the-art models. Our work offers\nan up-to-date, comprehensive taxonomy and an in-depth analysis of current\ntechniques, from pre-training strategies to advanced post-training methods.\nAnother contribution of this survey is a thorough review of DLM inference\nstrategies and optimizations, including improvements in decoding parallelism,\ncaching mechanisms, and generation quality. We also highlight the latest\napproaches to multimodal extensions of DLMs and delineate their applications\nacross various practical scenarios. Furthermore, our discussion addresses the\nlimitations and challenges of DLMs, including efficiency, long-sequence\nhandling, and infrastructure requirements, while outlining future research\ndirections to sustain progress in this rapidly evolving field. Project GitHub\nis available at https://github.com/VILA-Lab/Awesome-DLMs."
                },
                "authors": [
                    {
                        "name": "Tianyi Li"
                    },
                    {
                        "name": "Mingda Chen"
                    },
                    {
                        "name": "Bowei Guo"
                    },
                    {
                        "name": "Zhiqiang Shen"
                    }
                ],
                "author_detail": {
                    "name": "Zhiqiang Shen"
                },
                "author": "Zhiqiang Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10875v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10875v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10874v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10874v1",
                "updated": "2025-08-14T17:46:01Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    17,
                    46,
                    1,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T17:46:01Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    17,
                    46,
                    1,
                    3,
                    226,
                    0
                ],
                "title": "SSRL: Self-Search Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SSRL: Self-Search Reinforcement Learning"
                },
                "summary": "We investigate the potential of large language models (LLMs) to serve as\nefficient simulators for agentic search tasks in reinforcement learning (RL),\nthereby reducing dependence on costly interactions with external search\nengines. To this end, we first quantify the intrinsic search capability of LLMs\nvia structured prompting and repeated sampling, which we term Self-Search. Our\nresults reveal that LLMs exhibit strong scaling behavior with respect to the\ninference budget, achieving high pass@k on question-answering benchmarks,\nincluding the challenging BrowseComp task. Building on these observations, we\nintroduce Self-Search RL (SSRL), which enhances LLMs' Self-Search capability\nthrough format-based and rule-based rewards. SSRL enables models to iteratively\nrefine their knowledge utilization internally, without requiring access to\nexternal tools. Empirical evaluations demonstrate that SSRL-trained policy\nmodels provide a cost-effective and stable environment for search-driven RL\ntraining, reducing reliance on external search engines and facilitating robust\nsim-to-real transfer. We draw the following conclusions: 1) LLMs possess world\nknowledge that can be effectively elicited to achieve high performance; 2) SSRL\ndemonstrates the potential of leveraging internal knowledge to reduce\nhallucination; 3) SSRL-trained models integrate seamlessly with external search\nengines without additional effort. Our findings highlight the potential of LLMs\nto support more scalable RL agent training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate the potential of large language models (LLMs) to serve as\nefficient simulators for agentic search tasks in reinforcement learning (RL),\nthereby reducing dependence on costly interactions with external search\nengines. To this end, we first quantify the intrinsic search capability of LLMs\nvia structured prompting and repeated sampling, which we term Self-Search. Our\nresults reveal that LLMs exhibit strong scaling behavior with respect to the\ninference budget, achieving high pass@k on question-answering benchmarks,\nincluding the challenging BrowseComp task. Building on these observations, we\nintroduce Self-Search RL (SSRL), which enhances LLMs' Self-Search capability\nthrough format-based and rule-based rewards. SSRL enables models to iteratively\nrefine their knowledge utilization internally, without requiring access to\nexternal tools. Empirical evaluations demonstrate that SSRL-trained policy\nmodels provide a cost-effective and stable environment for search-driven RL\ntraining, reducing reliance on external search engines and facilitating robust\nsim-to-real transfer. We draw the following conclusions: 1) LLMs possess world\nknowledge that can be effectively elicited to achieve high performance; 2) SSRL\ndemonstrates the potential of leveraging internal knowledge to reduce\nhallucination; 3) SSRL-trained models integrate seamlessly with external search\nengines without additional effort. Our findings highlight the potential of LLMs\nto support more scalable RL agent training."
                },
                "authors": [
                    {
                        "name": "Yuchen Fan"
                    },
                    {
                        "name": "Kaiyan Zhang"
                    },
                    {
                        "name": "Heng Zhou"
                    },
                    {
                        "name": "Yuxin Zuo"
                    },
                    {
                        "name": "Yanxu Chen"
                    },
                    {
                        "name": "Yu Fu"
                    },
                    {
                        "name": "Xinwei Long"
                    },
                    {
                        "name": "Xuekai Zhu"
                    },
                    {
                        "name": "Che Jiang"
                    },
                    {
                        "name": "Yuchen Zhang"
                    },
                    {
                        "name": "Li Kang"
                    },
                    {
                        "name": "Gang Chen"
                    },
                    {
                        "name": "Cheng Huang"
                    },
                    {
                        "name": "Zhizhou He"
                    },
                    {
                        "name": "Bingning Wang"
                    },
                    {
                        "name": "Lei Bai"
                    },
                    {
                        "name": "Ning Ding"
                    },
                    {
                        "name": "Bowen Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Bowen Zhou"
                },
                "author": "Bowen Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10874v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10874v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10865v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10865v1",
                "updated": "2025-08-14T17:35:31Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    17,
                    35,
                    31,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T17:35:31Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    17,
                    35,
                    31,
                    3,
                    226,
                    0
                ],
                "title": "Performance of GPT-5 in Brain Tumor MRI Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance of GPT-5 in Brain Tumor MRI Reasoning"
                },
                "summary": "Accurate differentiation of brain tumor types on magnetic resonance imaging\n(MRI) is critical for guiding treatment planning in neuro-oncology. Recent\nadvances in large language models (LLMs) have enabled visual question answering\n(VQA) approaches that integrate image interpretation with natural language\nreasoning. In this study, we evaluated GPT-4o, GPT-5-nano, GPT-5-mini, and\nGPT-5 on a curated brain tumor VQA benchmark derived from 3 Brain Tumor\nSegmentation (BraTS) datasets - glioblastoma (GLI), meningioma (MEN), and brain\nmetastases (MET). Each case included multi-sequence MRI triplanar mosaics and\nstructured clinical features transformed into standardized VQA items. Models\nwere assessed in a zero-shot chain-of-thought setting for accuracy on both\nvisual and reasoning tasks. Results showed that GPT-5-mini achieved the highest\nmacro-average accuracy (44.19%), followed by GPT-5 (43.71%), GPT-4o (41.49%),\nand GPT-5-nano (35.85%). Performance varied by tumor subtype, with no single\nmodel dominating across all cohorts. These findings suggest that GPT-5 family\nmodels can achieve moderate accuracy in structured neuro-oncological VQA tasks,\nbut not at a level acceptable for clinical use.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate differentiation of brain tumor types on magnetic resonance imaging\n(MRI) is critical for guiding treatment planning in neuro-oncology. Recent\nadvances in large language models (LLMs) have enabled visual question answering\n(VQA) approaches that integrate image interpretation with natural language\nreasoning. In this study, we evaluated GPT-4o, GPT-5-nano, GPT-5-mini, and\nGPT-5 on a curated brain tumor VQA benchmark derived from 3 Brain Tumor\nSegmentation (BraTS) datasets - glioblastoma (GLI), meningioma (MEN), and brain\nmetastases (MET). Each case included multi-sequence MRI triplanar mosaics and\nstructured clinical features transformed into standardized VQA items. Models\nwere assessed in a zero-shot chain-of-thought setting for accuracy on both\nvisual and reasoning tasks. Results showed that GPT-5-mini achieved the highest\nmacro-average accuracy (44.19%), followed by GPT-5 (43.71%), GPT-4o (41.49%),\nand GPT-5-nano (35.85%). Performance varied by tumor subtype, with no single\nmodel dominating across all cohorts. These findings suggest that GPT-5 family\nmodels can achieve moderate accuracy in structured neuro-oncological VQA tasks,\nbut not at a level acceptable for clinical use."
                },
                "authors": [
                    {
                        "name": "Mojtaba Safari"
                    },
                    {
                        "name": "Shansong Wang"
                    },
                    {
                        "name": "Mingzhe Hu"
                    },
                    {
                        "name": "Zach Eidex"
                    },
                    {
                        "name": "Qiang Li"
                    },
                    {
                        "name": "Xiaofeng Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaofeng Yang"
                },
                "author": "Xiaofeng Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10865v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10865v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08809v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08809v3",
                "updated": "2025-08-14T17:23:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    17,
                    23,
                    22,
                    3,
                    226,
                    0
                ],
                "published": "2024-12-11T22:50:05Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    22,
                    50,
                    5,
                    2,
                    346,
                    0
                ],
                "title": "DAmodel: Hierarchical Bayesian Modelling of DA White Dwarfs for\n  Spectrophotometric Calibration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DAmodel: Hierarchical Bayesian Modelling of DA White Dwarfs for\n  Spectrophotometric Calibration"
                },
                "summary": "We use hierarchical Bayesian modelling to calibrate a network of 32 all-sky\nfaint DA white dwarf (DA WD) spectrophotometric standards ($16.5 < V < 19.5$)\nalongside three CALSPEC standards, from 912 \\r{A} to 32 $\\mu$m. The framework\nis the first of its kind to jointly infer photometric zeropoints and WD\nparameters (surface gravity $\\log g$, effective temperature $T_{\\text{eff}}$,\nextinction $A_V$, dust relation parameter $R_V$) by simultaneously modelling\nboth photometric and spectroscopic data. We model panchromatic Hubble Space\nTelescope Wide Field Camera 3 (HST/WFC3) UVIS and IR photometry, HST/STIS UV\nspectroscopy and ground-based optical spectroscopy to sub-percent precision.\nPhotometric residuals for the sample are the lowest yet yielding $<0.004$ mag\nRMS on average from the UV to the NIR, achieved by jointly inferring\ntime-dependent changes in system sensitivity and WFC3/IR count-rate\nnonlinearity. Our GPU-accelerated implementation enables efficient sampling via\nHamiltonian Monte Carlo, critical for exploring the high-dimensional posterior\nspace. The hierarchical nature of the model enables population analysis of\nintrinsic WD and dust parameters. Inferred spectral energy distributions from\nthis model will be essential for calibrating the James Webb Space Telescope as\nwell as next-generation surveys, including Vera Rubin Observatory's Legacy\nSurvey of Space and Time and the Nancy Grace Roman Space Telescope.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We use hierarchical Bayesian modelling to calibrate a network of 32 all-sky\nfaint DA white dwarf (DA WD) spectrophotometric standards ($16.5 < V < 19.5$)\nalongside three CALSPEC standards, from 912 \\r{A} to 32 $\\mu$m. The framework\nis the first of its kind to jointly infer photometric zeropoints and WD\nparameters (surface gravity $\\log g$, effective temperature $T_{\\text{eff}}$,\nextinction $A_V$, dust relation parameter $R_V$) by simultaneously modelling\nboth photometric and spectroscopic data. We model panchromatic Hubble Space\nTelescope Wide Field Camera 3 (HST/WFC3) UVIS and IR photometry, HST/STIS UV\nspectroscopy and ground-based optical spectroscopy to sub-percent precision.\nPhotometric residuals for the sample are the lowest yet yielding $<0.004$ mag\nRMS on average from the UV to the NIR, achieved by jointly inferring\ntime-dependent changes in system sensitivity and WFC3/IR count-rate\nnonlinearity. Our GPU-accelerated implementation enables efficient sampling via\nHamiltonian Monte Carlo, critical for exploring the high-dimensional posterior\nspace. The hierarchical nature of the model enables population analysis of\nintrinsic WD and dust parameters. Inferred spectral energy distributions from\nthis model will be essential for calibrating the James Webb Space Telescope as\nwell as next-generation surveys, including Vera Rubin Observatory's Legacy\nSurvey of Space and Time and the Nancy Grace Roman Space Telescope."
                },
                "authors": [
                    {
                        "name": "Benjamin M. Boyd"
                    },
                    {
                        "name": "Gautham Narayan"
                    },
                    {
                        "name": "Kaisey S. Mandel"
                    },
                    {
                        "name": "Matthew Grayling"
                    },
                    {
                        "name": "Abhijit Saha"
                    },
                    {
                        "name": "Tim Axelrod"
                    },
                    {
                        "name": "Thomas Matheson"
                    },
                    {
                        "name": "Edward W. Olszewski"
                    },
                    {
                        "name": "Annalisa Calamida"
                    },
                    {
                        "name": "Aaron Do"
                    },
                    {
                        "name": "Ralph C. Bohlin"
                    },
                    {
                        "name": "Jay B. Holberg"
                    },
                    {
                        "name": "Ivan Hubeny"
                    },
                    {
                        "name": "Susana Deustua"
                    },
                    {
                        "name": "Armin Rest"
                    },
                    {
                        "name": "Christopher W. Stubbs"
                    },
                    {
                        "name": "Aidan Berres"
                    },
                    {
                        "name": "Mai Li"
                    },
                    {
                        "name": "John W. Mackenty"
                    },
                    {
                        "name": "Elena Sabbi"
                    }
                ],
                "author_detail": {
                    "name": "Elena Sabbi"
                },
                "author": "Elena Sabbi",
                "arxiv_doi": "10.1093/mnras/staf629",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1093/mnras/staf629",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.08809v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08809v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "34 pages, 26 figures, 6 tables. Accepted for publication in MNRAS.\n  Published SEDs can be found at: https://zenodo.org/records/14339960",
                "arxiv_journal_ref": "Monthly Notices of the Royal Astronomical Society, Volume 540,\n  Issue 1, June 2025, Pages 385-415",
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10848v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10848v1",
                "updated": "2025-08-14T17:18:35Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    17,
                    18,
                    35,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T17:18:35Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    17,
                    18,
                    35,
                    3,
                    226,
                    0
                ],
                "title": "Psyche-R1: Towards Reliable Psychological LLMs through Unified Empathy,\n  Expertise, and Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Psyche-R1: Towards Reliable Psychological LLMs through Unified Empathy,\n  Expertise, and Reasoning"
                },
                "summary": "Amidst a shortage of qualified mental health professionals, the integration\nof large language models (LLMs) into psychological applications offers a\npromising way to alleviate the growing burden of mental health disorders.\nRecent reasoning-augmented LLMs have achieved remarkable performance in\nmathematics and programming, while research in the psychological domain has\npredominantly emphasized emotional support and empathetic dialogue, with\nlimited attention to reasoning mechanisms that are beneficial to generating\nreliable responses. Therefore, in this paper, we propose Psyche-R1, the first\nChinese psychological LLM that jointly integrates empathy, psychological\nexpertise, and reasoning, built upon a novel data curation pipeline.\nSpecifically, we design a comprehensive data synthesis pipeline that produces\nover 75k high-quality psychological questions paired with detailed rationales,\ngenerated through chain-of-thought (CoT) reasoning and iterative\nprompt-rationale optimization, along with 73k empathetic dialogues.\nSubsequently, we employ a hybrid training strategy wherein challenging samples\nare identified through a multi-LLM cross-selection strategy for group relative\npolicy optimization (GRPO) to improve reasoning ability, while the remaining\ndata is used for supervised fine-tuning (SFT) to enhance empathetic response\ngeneration and psychological domain knowledge. Extensive experiment results\ndemonstrate the effectiveness of the Psyche-R1 across several psychological\nbenchmarks, where our 7B Psyche-R1 achieves comparable results to 671B\nDeepSeek-R1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Amidst a shortage of qualified mental health professionals, the integration\nof large language models (LLMs) into psychological applications offers a\npromising way to alleviate the growing burden of mental health disorders.\nRecent reasoning-augmented LLMs have achieved remarkable performance in\nmathematics and programming, while research in the psychological domain has\npredominantly emphasized emotional support and empathetic dialogue, with\nlimited attention to reasoning mechanisms that are beneficial to generating\nreliable responses. Therefore, in this paper, we propose Psyche-R1, the first\nChinese psychological LLM that jointly integrates empathy, psychological\nexpertise, and reasoning, built upon a novel data curation pipeline.\nSpecifically, we design a comprehensive data synthesis pipeline that produces\nover 75k high-quality psychological questions paired with detailed rationales,\ngenerated through chain-of-thought (CoT) reasoning and iterative\nprompt-rationale optimization, along with 73k empathetic dialogues.\nSubsequently, we employ a hybrid training strategy wherein challenging samples\nare identified through a multi-LLM cross-selection strategy for group relative\npolicy optimization (GRPO) to improve reasoning ability, while the remaining\ndata is used for supervised fine-tuning (SFT) to enhance empathetic response\ngeneration and psychological domain knowledge. Extensive experiment results\ndemonstrate the effectiveness of the Psyche-R1 across several psychological\nbenchmarks, where our 7B Psyche-R1 achieves comparable results to 671B\nDeepSeek-R1."
                },
                "authors": [
                    {
                        "name": "Chongyuan Dai"
                    },
                    {
                        "name": "Jinpeng Hu"
                    },
                    {
                        "name": "Hongchang Shi"
                    },
                    {
                        "name": "Zhuo Li"
                    },
                    {
                        "name": "Xun Yang"
                    },
                    {
                        "name": "Meng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Meng Wang"
                },
                "author": "Meng Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10848v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10848v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05571v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05571v2",
                "updated": "2025-08-14T17:13:26Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    17,
                    13,
                    26,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-07T17:02:23Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    17,
                    2,
                    23,
                    3,
                    219,
                    0
                ],
                "title": "iFairy: the First 2-bit Complex LLM with All Parameters in $\\{\\pm1, \\pm\n  i\\}$",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "iFairy: the First 2-bit Complex LLM with All Parameters in $\\{\\pm1, \\pm\n  i\\}$"
                },
                "summary": "Quantization-Aware Training (QAT) integrates quantization into the training\nloop, enabling LLMs to learn robust low-bit representations, and is widely\nrecognized as one of the most promising research directions. All current QAT\nresearch focuses on minimizing quantization error on full-precision models,\nwhere the full-precision accuracy acts as an upper bound (accuracy ceiling). No\nexisting method has even attempted to surpass this ceiling. To break this\nceiling, we propose a new paradigm: raising the ceiling (full-precision model),\nand then still quantizing it efficiently into 2 bits. We propose Fairy$\\pm i$,\nthe first 2-bit quantization framework for complex-valued LLMs. Specifically,\nour method leverages the representational advantages of the complex domain to\nboost full-precision accuracy. We map weights to the fourth roots of unity\n$\\{\\pm1, \\pm i\\}$, forming a perfectly symmetric and information-theoretically\noptimal 2-bit representation. Importantly, each quantized weight has either a\nzero real or imaginary part, enabling multiplication-free inference using only\nadditions and element swaps. Experimental results show that Fairy$\\pm i$\noutperforms the ceiling of existing 2-bit quantization approaches in terms of\nboth PPL and downstream tasks, while maintaining strict storage and compute\nefficiency. This work opens a new direction for building highly accurate and\npractical LLMs under extremely low-bit constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization-Aware Training (QAT) integrates quantization into the training\nloop, enabling LLMs to learn robust low-bit representations, and is widely\nrecognized as one of the most promising research directions. All current QAT\nresearch focuses on minimizing quantization error on full-precision models,\nwhere the full-precision accuracy acts as an upper bound (accuracy ceiling). No\nexisting method has even attempted to surpass this ceiling. To break this\nceiling, we propose a new paradigm: raising the ceiling (full-precision model),\nand then still quantizing it efficiently into 2 bits. We propose Fairy$\\pm i$,\nthe first 2-bit quantization framework for complex-valued LLMs. Specifically,\nour method leverages the representational advantages of the complex domain to\nboost full-precision accuracy. We map weights to the fourth roots of unity\n$\\{\\pm1, \\pm i\\}$, forming a perfectly symmetric and information-theoretically\noptimal 2-bit representation. Importantly, each quantized weight has either a\nzero real or imaginary part, enabling multiplication-free inference using only\nadditions and element swaps. Experimental results show that Fairy$\\pm i$\noutperforms the ceiling of existing 2-bit quantization approaches in terms of\nboth PPL and downstream tasks, while maintaining strict storage and compute\nefficiency. This work opens a new direction for building highly accurate and\npractical LLMs under extremely low-bit constraints."
                },
                "authors": [
                    {
                        "name": "Feiyu Wang"
                    },
                    {
                        "name": "Guoan Wang"
                    },
                    {
                        "name": "Yihao Zhang"
                    },
                    {
                        "name": "Shengfan Wang"
                    },
                    {
                        "name": "Weitao Li"
                    },
                    {
                        "name": "Bokai Huang"
                    },
                    {
                        "name": "Shimao Chen"
                    },
                    {
                        "name": "Zihan Jiang"
                    },
                    {
                        "name": "Rui Xu"
                    },
                    {
                        "name": "Tong Yang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Yang"
                },
                "author": "Tong Yang",
                "arxiv_comment": "15 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05571v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05571v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10839v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10839v1",
                "updated": "2025-08-14T17:05:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    17,
                    5,
                    44,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T17:05:44Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    17,
                    5,
                    44,
                    3,
                    226,
                    0
                ],
                "title": "Reinforced Language Models for Sequential Decision Making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforced Language Models for Sequential Decision Making"
                },
                "summary": "Large Language Models (LLMs) show potential as sequential decision-making\nagents, but their application is often limited due to a reliance on large,\ncomputationally expensive models. This creates a need to improve smaller\nmodels, yet existing post-training methods are designed for single-turn\ninteractions and cannot handle credit assignment in multi-step agentic tasks.\nTo address this, we introduce Multi-Step Group-Relative Policy Optimization\n(MS-GRPO), a new algorithm for post-training LLM agents, grounded in formal\nText-Mediated Stochastic Game (TSMG) and Language-Agent Policy (LAP)\nframeworks. For credit assignment, MS-GRPO attributes the entire cumulative\nepisode reward to each individual episode step. We supplement this algorithm\nwith a novel absolute-advantage-weighted episode sampling strategy that we show\nimproves training performance. We evaluate our approach by post-training a\n3-billion parameter model on Snake and Frozen Lake. Our experiments demonstrate\nthat the method is effective in improving decision-making performance: our\npost-trained 3B parameter model outperforms a 72B parameter baseline by 50% on\nthe Frozen Lake task. This work demonstrates that targeted post-training is a\npractical and efficient alternative to relying on model scale for creating\nsequential decision-making agents using LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) show potential as sequential decision-making\nagents, but their application is often limited due to a reliance on large,\ncomputationally expensive models. This creates a need to improve smaller\nmodels, yet existing post-training methods are designed for single-turn\ninteractions and cannot handle credit assignment in multi-step agentic tasks.\nTo address this, we introduce Multi-Step Group-Relative Policy Optimization\n(MS-GRPO), a new algorithm for post-training LLM agents, grounded in formal\nText-Mediated Stochastic Game (TSMG) and Language-Agent Policy (LAP)\nframeworks. For credit assignment, MS-GRPO attributes the entire cumulative\nepisode reward to each individual episode step. We supplement this algorithm\nwith a novel absolute-advantage-weighted episode sampling strategy that we show\nimproves training performance. We evaluate our approach by post-training a\n3-billion parameter model on Snake and Frozen Lake. Our experiments demonstrate\nthat the method is effective in improving decision-making performance: our\npost-trained 3B parameter model outperforms a 72B parameter baseline by 50% on\nthe Frozen Lake task. This work demonstrates that targeted post-training is a\npractical and efficient alternative to relying on model scale for creating\nsequential decision-making agents using LLMs."
                },
                "authors": [
                    {
                        "name": "Jim Dilkes"
                    },
                    {
                        "name": "Vahid Yazdanpanah"
                    },
                    {
                        "name": "Sebastian Stein"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Stein"
                },
                "author": "Sebastian Stein",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10839v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10839v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.8",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02409v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02409v3",
                "updated": "2025-08-14T17:04:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    17,
                    4,
                    28,
                    3,
                    226,
                    0
                ],
                "published": "2025-01-05T01:04:23Z",
                "published_parsed": [
                    2025,
                    1,
                    5,
                    1,
                    4,
                    23,
                    6,
                    5,
                    0
                ],
                "title": "Interpretable Neural ODEs for Gene Regulatory Network Discovery under\n  Perturbations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interpretable Neural ODEs for Gene Regulatory Network Discovery under\n  Perturbations"
                },
                "summary": "Modern high-throughput biological datasets with thousands of perturbations\nprovide the opportunity for large-scale discovery of causal graphs that\nrepresent the regulatory interactions between genes. Differentiable causal\ngraphical models have been proposed to infer a gene regulatory network (GRN)\nfrom large scale interventional datasets, capturing the causal gene regulatory\nrelationships from genetic perturbations. However, existing models are limited\nin their expressivity and scalability while failing to address the dynamic\nnature of biological processes such as cellular differentiation. We propose\nPerturbODE, a novel framework that incorporates biologically informative neural\nordinary differential equations (neural ODEs) to model cell state trajectories\nunder perturbations and derive the causal GRN from the neural ODE's parameters.\nWe demonstrate PerturbODE's efficacy in trajectory prediction and GRN inference\nacross simulated and real over-expression datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern high-throughput biological datasets with thousands of perturbations\nprovide the opportunity for large-scale discovery of causal graphs that\nrepresent the regulatory interactions between genes. Differentiable causal\ngraphical models have been proposed to infer a gene regulatory network (GRN)\nfrom large scale interventional datasets, capturing the causal gene regulatory\nrelationships from genetic perturbations. However, existing models are limited\nin their expressivity and scalability while failing to address the dynamic\nnature of biological processes such as cellular differentiation. We propose\nPerturbODE, a novel framework that incorporates biologically informative neural\nordinary differential equations (neural ODEs) to model cell state trajectories\nunder perturbations and derive the causal GRN from the neural ODE's parameters.\nWe demonstrate PerturbODE's efficacy in trajectory prediction and GRN inference\nacross simulated and real over-expression datasets."
                },
                "authors": [
                    {
                        "name": "Zaikang Lin"
                    },
                    {
                        "name": "Sei Chang"
                    },
                    {
                        "name": "Aaron Zweig"
                    },
                    {
                        "name": "Minseo Kang"
                    },
                    {
                        "name": "Elham Azizi"
                    },
                    {
                        "name": "David A. Knowles"
                    }
                ],
                "author_detail": {
                    "name": "David A. Knowles"
                },
                "author": "David A. Knowles",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02409v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02409v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.MN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10795v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10795v1",
                "updated": "2025-08-14T16:18:37Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    16,
                    18,
                    37,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T16:18:37Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    16,
                    18,
                    37,
                    3,
                    226,
                    0
                ],
                "title": "Beyond \"Not Novel Enough\": Enriching Scholarly Critique with\n  LLM-Assisted Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond \"Not Novel Enough\": Enriching Scholarly Critique with\n  LLM-Assisted Feedback"
                },
                "summary": "Novelty assessment is a central yet understudied aspect of peer review,\nparticularly in high volume fields like NLP where reviewer capacity is\nincreasingly strained. We present a structured approach for automated novelty\nevaluation that models expert reviewer behavior through three stages: content\nextraction from submissions, retrieval and synthesis of related work, and\nstructured comparison for evidence based assessment. Our method is informed by\na large scale analysis of human written novelty reviews and captures key\npatterns such as independent claim verification and contextual reasoning.\nEvaluated on 182 ICLR 2025 submissions with human annotated reviewer novelty\nassessments, the approach achieves 86.5% alignment with human reasoning and\n75.3% agreement on novelty conclusions - substantially outperforming existing\nLLM based baselines. The method produces detailed, literature aware analyses\nand improves consistency over ad hoc reviewer judgments. These results\nhighlight the potential for structured LLM assisted approaches to support more\nrigorous and transparent peer review without displacing human expertise. Data\nand code are made available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Novelty assessment is a central yet understudied aspect of peer review,\nparticularly in high volume fields like NLP where reviewer capacity is\nincreasingly strained. We present a structured approach for automated novelty\nevaluation that models expert reviewer behavior through three stages: content\nextraction from submissions, retrieval and synthesis of related work, and\nstructured comparison for evidence based assessment. Our method is informed by\na large scale analysis of human written novelty reviews and captures key\npatterns such as independent claim verification and contextual reasoning.\nEvaluated on 182 ICLR 2025 submissions with human annotated reviewer novelty\nassessments, the approach achieves 86.5% alignment with human reasoning and\n75.3% agreement on novelty conclusions - substantially outperforming existing\nLLM based baselines. The method produces detailed, literature aware analyses\nand improves consistency over ad hoc reviewer judgments. These results\nhighlight the potential for structured LLM assisted approaches to support more\nrigorous and transparent peer review without displacing human expertise. Data\nand code are made available."
                },
                "authors": [
                    {
                        "name": "Osama Mohammed Afzal"
                    },
                    {
                        "name": "Preslav Nakov"
                    },
                    {
                        "name": "Tom Hope"
                    },
                    {
                        "name": "Iryna Gurevych"
                    }
                ],
                "author_detail": {
                    "name": "Iryna Gurevych"
                },
                "author": "Iryna Gurevych",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10795v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10795v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13109v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13109v2",
                "updated": "2025-08-14T16:12:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    16,
                    12,
                    44,
                    3,
                    226,
                    0
                ],
                "published": "2025-05-19T13:36:45Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    13,
                    36,
                    45,
                    0,
                    139,
                    0
                ],
                "title": "FreeKV: Boosting KV Cache Retrieval for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FreeKV: Boosting KV Cache Retrieval for Efficient LLM Inference"
                },
                "summary": "Large language models (LLMs) have been widely deployed with rapidly expanding\ncontext windows to support increasingly demanding applications. However, long\ncontexts pose significant deployment challenges, primarily due to the KV cache\nwhose size grows proportionally with context length. While KV cache compression\nmethods are proposed to address this issue, KV dropping methods incur\nconsiderable accuracy loss, and KV retrieval methods suffer from significant\nefficiency bottlenecks. We propose FreeKV, an algorithm-system co-optimization\nframework to enhance KV retrieval efficiency while preserving accuracy. On the\nalgorithm side, FreeKV introduces speculative retrieval to shift the KV\nselection and recall processes out of the critical path, combined with\nfine-grained correction to ensure accuracy. On the system side, FreeKV employs\nhybrid KV layouts across CPU and GPU memory to eliminate fragmented data\ntransfers, and leverages double-buffered streamed recall to further improve\nefficiency. Experiments demonstrate that FreeKV achieves near-lossless accuracy\nacross various scenarios and models, delivering up to 13$\\times$ speedup\ncompared to SOTA KV retrieval methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been widely deployed with rapidly expanding\ncontext windows to support increasingly demanding applications. However, long\ncontexts pose significant deployment challenges, primarily due to the KV cache\nwhose size grows proportionally with context length. While KV cache compression\nmethods are proposed to address this issue, KV dropping methods incur\nconsiderable accuracy loss, and KV retrieval methods suffer from significant\nefficiency bottlenecks. We propose FreeKV, an algorithm-system co-optimization\nframework to enhance KV retrieval efficiency while preserving accuracy. On the\nalgorithm side, FreeKV introduces speculative retrieval to shift the KV\nselection and recall processes out of the critical path, combined with\nfine-grained correction to ensure accuracy. On the system side, FreeKV employs\nhybrid KV layouts across CPU and GPU memory to eliminate fragmented data\ntransfers, and leverages double-buffered streamed recall to further improve\nefficiency. Experiments demonstrate that FreeKV achieves near-lossless accuracy\nacross various scenarios and models, delivering up to 13$\\times$ speedup\ncompared to SOTA KV retrieval methods."
                },
                "authors": [
                    {
                        "name": "Guangda Liu"
                    },
                    {
                        "name": "Chengwei Li"
                    },
                    {
                        "name": "Zhenyu Ning"
                    },
                    {
                        "name": "Minyi Guo"
                    },
                    {
                        "name": "Jieru Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Jieru Zhao"
                },
                "author": "Jieru Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13109v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13109v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10777v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10777v1",
                "updated": "2025-08-14T16:01:10Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    16,
                    1,
                    10,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T16:01:10Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    16,
                    1,
                    10,
                    3,
                    226,
                    0
                ],
                "title": "The Knowledge-Reasoning Dissociation: Fundamental Limitations of LLMs in\n  Clinical Natural Language Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Knowledge-Reasoning Dissociation: Fundamental Limitations of LLMs in\n  Clinical Natural Language Inference"
                },
                "summary": "Large language models are often assumed to acquire increasingly structured,\ngeneralizable internal representations simply by scaling data and parameters.\nWe interrogate this assumption by introducing a Clinical Trial Natural Language\nInference benchmark comprising four reasoning families, Causal Attribution,\nCompositional Grounding, Epistemic Verification, and Risk State Abstraction.\nEach item is paired with a targeted Ground Knowledge and Meta-Level Reasoning\nVerification (GKMRV) probe, allowing us to dissociate failures of factual\naccess from failures of inference. We evaluate six contemporary LLMs under both\ndirect and chain of thought prompting.\n  Models achieve near-ceiling GKMRV accuracy (mean accuracy 0.918) yet perform\npoorly on the main reasoning tasks (mean accuracy 0.25). Despite low accuracy,\noutput inferences are highly consistent across samples (mean 0.87), indicating\na systematic application of underlying heuristics and shortcuts.\n  These results reveal fundamental structural and representational limitations:\ncurrent LLMs often possess the relevant clinical knowledge but lack the\nstructured, composable internal representations needed to deploy it reliably\n(e.g., integrating constraints, weighing evidence, or simulating\ncounterfactuals). Decoupling knowledge from reasoning with GKMRV makes this\ndissociation explicit and measurable, providing an effective framework for\nprobing the reliability of LLMs in high-stakes domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models are often assumed to acquire increasingly structured,\ngeneralizable internal representations simply by scaling data and parameters.\nWe interrogate this assumption by introducing a Clinical Trial Natural Language\nInference benchmark comprising four reasoning families, Causal Attribution,\nCompositional Grounding, Epistemic Verification, and Risk State Abstraction.\nEach item is paired with a targeted Ground Knowledge and Meta-Level Reasoning\nVerification (GKMRV) probe, allowing us to dissociate failures of factual\naccess from failures of inference. We evaluate six contemporary LLMs under both\ndirect and chain of thought prompting.\n  Models achieve near-ceiling GKMRV accuracy (mean accuracy 0.918) yet perform\npoorly on the main reasoning tasks (mean accuracy 0.25). Despite low accuracy,\noutput inferences are highly consistent across samples (mean 0.87), indicating\na systematic application of underlying heuristics and shortcuts.\n  These results reveal fundamental structural and representational limitations:\ncurrent LLMs often possess the relevant clinical knowledge but lack the\nstructured, composable internal representations needed to deploy it reliably\n(e.g., integrating constraints, weighing evidence, or simulating\ncounterfactuals). Decoupling knowledge from reasoning with GKMRV makes this\ndissociation explicit and measurable, providing an effective framework for\nprobing the reliability of LLMs in high-stakes domains."
                },
                "authors": [
                    {
                        "name": "Mal Jullien"
                    },
                    {
                        "name": "Marco Valentino"
                    },
                    {
                        "name": "Andr Freitas"
                    }
                ],
                "author_detail": {
                    "name": "Andr Freitas"
                },
                "author": "Andr Freitas",
                "arxiv_comment": "19 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10777v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10777v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10774v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10774v1",
                "updated": "2025-08-14T15:58:59Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    15,
                    58,
                    59,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T15:58:59Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    15,
                    58,
                    59,
                    3,
                    226,
                    0
                ],
                "title": "Video-BLADE: Block-Sparse Attention Meets Step Distillation for\n  Efficient Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video-BLADE: Block-Sparse Attention Meets Step Distillation for\n  Efficient Video Generation"
                },
                "summary": "Diffusion transformers currently lead the field in high-quality video\ngeneration, but their slow iterative denoising process and prohibitive\nquadratic attention costs for long sequences create significant inference\nbottlenecks. While both step distillation and sparse attention mechanisms have\nshown promise as independent acceleration strategies, effectively combining\nthese approaches presents critical challenges -- training-free integration\nyields suboptimal results, while separately training sparse attention after\nstep distillation requires prohibitively expensive high-quality video data. To\novercome these limitations, we propose BLADE, an innovative data-free joint\ntraining framework that introduces: (1) an Adaptive Block-Sparse Attention\n(ASA) mechanism for dynamically generating content-aware sparsity masks to\nfocus computation on salient spatiotemporal features, and (2) a sparsity-aware\nstep distillation paradigm built upon Trajectory Distribution Matching (TDM)\nthat directly incorporates sparsity into the distillation process rather than\ntreating it as a separate compression step, with fast convergence. We validate\nBLADE on text-to-video models like CogVideoX-5B and Wan2.1-1.3B. Our framework\ndemonstrates remarkable efficiency gains across different scales. On\nWan2.1-1.3B, BLADE achieves a 14.10x end-to-end inference acceleration over a\n50-step baseline. Moreover, on models such as CogVideoX-5B with short video\nsequence lengths, our framework delivers a robust 8.89x speedup. Crucially, the\nacceleration is accompanied by a consistent quality improvement. On the\nVBench-2.0 benchmark, BLADE boosts the score of CogVideoX-5B to 0.569 (from\n0.534) and Wan2.1-1.3B to 0.570 (from 0.563), results that are further\ncorroborated by superior ratings in human evaluations. Our code and model\nweights are publicly available at: http://ziplab.co/BLADE-Homepage/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion transformers currently lead the field in high-quality video\ngeneration, but their slow iterative denoising process and prohibitive\nquadratic attention costs for long sequences create significant inference\nbottlenecks. While both step distillation and sparse attention mechanisms have\nshown promise as independent acceleration strategies, effectively combining\nthese approaches presents critical challenges -- training-free integration\nyields suboptimal results, while separately training sparse attention after\nstep distillation requires prohibitively expensive high-quality video data. To\novercome these limitations, we propose BLADE, an innovative data-free joint\ntraining framework that introduces: (1) an Adaptive Block-Sparse Attention\n(ASA) mechanism for dynamically generating content-aware sparsity masks to\nfocus computation on salient spatiotemporal features, and (2) a sparsity-aware\nstep distillation paradigm built upon Trajectory Distribution Matching (TDM)\nthat directly incorporates sparsity into the distillation process rather than\ntreating it as a separate compression step, with fast convergence. We validate\nBLADE on text-to-video models like CogVideoX-5B and Wan2.1-1.3B. Our framework\ndemonstrates remarkable efficiency gains across different scales. On\nWan2.1-1.3B, BLADE achieves a 14.10x end-to-end inference acceleration over a\n50-step baseline. Moreover, on models such as CogVideoX-5B with short video\nsequence lengths, our framework delivers a robust 8.89x speedup. Crucially, the\nacceleration is accompanied by a consistent quality improvement. On the\nVBench-2.0 benchmark, BLADE boosts the score of CogVideoX-5B to 0.569 (from\n0.534) and Wan2.1-1.3B to 0.570 (from 0.563), results that are further\ncorroborated by superior ratings in human evaluations. Our code and model\nweights are publicly available at: http://ziplab.co/BLADE-Homepage/."
                },
                "authors": [
                    {
                        "name": "Youping Gu"
                    },
                    {
                        "name": "Xiaolong Li"
                    },
                    {
                        "name": "Yuhao Hu"
                    },
                    {
                        "name": "Bohan Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Bohan Zhuang"
                },
                "author": "Bohan Zhuang",
                "arxiv_comment": "Tech report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10774v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10774v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10769v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10769v1",
                "updated": "2025-08-14T15:55:19Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    15,
                    55,
                    19,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T15:55:19Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    15,
                    55,
                    19,
                    3,
                    226,
                    0
                ],
                "title": "Modeling Human Responses to Multimodal AI Content",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling Human Responses to Multimodal AI Content"
                },
                "summary": "As AI-generated content becomes widespread, so does the risk of\nmisinformation. While prior research has primarily focused on identifying\nwhether content is authentic, much less is known about how such content\ninfluences human perception and behavior. In domains like trading or the stock\nmarket, predicting how people react (e.g., whether a news post will go viral),\ncan be more critical than verifying its factual accuracy. To address this, we\ntake a human-centered approach and introduce the MhAIM Dataset, which contains\n154,552 online posts (111,153 of them AI-generated), enabling large-scale\nanalysis of how people respond to AI-generated content. Our human study reveals\nthat people are better at identifying AI content when posts include both text\nand visuals, particularly when inconsistencies exist between the two. We\npropose three new metrics: trustworthiness, impact, and openness, to quantify\nhow users judge and engage with online content. We present T-Lens, an LLM-based\nagent system designed to answer user queries by incorporating predicted human\nresponses to multimodal information. At its core is HR-MCP (Human Response\nModel Context Protocol), built on the standardized Model Context Protocol\n(MCP), enabling seamless integration with any LLM. This integration allows\nT-Lens to better align with human reactions, enhancing both interpretability\nand interaction capabilities. Our work provides empirical insights and\npractical tools to equip LLMs with human-awareness capabilities. By\nhighlighting the complex interplay among AI, human cognition, and information\nreception, our findings suggest actionable strategies for mitigating the risks\nof AI-driven misinformation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As AI-generated content becomes widespread, so does the risk of\nmisinformation. While prior research has primarily focused on identifying\nwhether content is authentic, much less is known about how such content\ninfluences human perception and behavior. In domains like trading or the stock\nmarket, predicting how people react (e.g., whether a news post will go viral),\ncan be more critical than verifying its factual accuracy. To address this, we\ntake a human-centered approach and introduce the MhAIM Dataset, which contains\n154,552 online posts (111,153 of them AI-generated), enabling large-scale\nanalysis of how people respond to AI-generated content. Our human study reveals\nthat people are better at identifying AI content when posts include both text\nand visuals, particularly when inconsistencies exist between the two. We\npropose three new metrics: trustworthiness, impact, and openness, to quantify\nhow users judge and engage with online content. We present T-Lens, an LLM-based\nagent system designed to answer user queries by incorporating predicted human\nresponses to multimodal information. At its core is HR-MCP (Human Response\nModel Context Protocol), built on the standardized Model Context Protocol\n(MCP), enabling seamless integration with any LLM. This integration allows\nT-Lens to better align with human reactions, enhancing both interpretability\nand interaction capabilities. Our work provides empirical insights and\npractical tools to equip LLMs with human-awareness capabilities. By\nhighlighting the complex interplay among AI, human cognition, and information\nreception, our findings suggest actionable strategies for mitigating the risks\nof AI-driven misinformation."
                },
                "authors": [
                    {
                        "name": "Zhiqi Shen"
                    },
                    {
                        "name": "Shaojing Fan"
                    },
                    {
                        "name": "Danni Xu"
                    },
                    {
                        "name": "Terence Sim"
                    },
                    {
                        "name": "Mohan Kankanhalli"
                    }
                ],
                "author_detail": {
                    "name": "Mohan Kankanhalli"
                },
                "author": "Mohan Kankanhalli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10769v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10769v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18773v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18773v2",
                "updated": "2025-08-14T15:37:43Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    15,
                    37,
                    43,
                    3,
                    226,
                    0
                ],
                "published": "2025-03-24T15:22:41Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    15,
                    22,
                    41,
                    0,
                    83,
                    0
                ],
                "title": "BitDecoding: Unlocking Tensor Cores for Long-Context LLMs with Low-Bit\n  KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BitDecoding: Unlocking Tensor Cores for Long-Context LLMs with Low-Bit\n  KV Cache"
                },
                "summary": "The rise of long-context Large Language Models (LLMs) amplifies memory and\nbandwidth demands during autoregressive decoding, as the Key-Value (KV) cache\ngrows with each generated token. Low-bit KV-cache quantization (e.g., 4-bit or\n2-bit) can reduce memory footprint while preserving accuracy, but existing\nsystems suffer from slow decoding due to their exclusive reliance on CUDA\ncores, neglecting Tensor Cores (the primary source of compute on modern GPUs).\nWe present BitDecoding, a new long-context LLM inference system with a low-bit\nKV cache. BitDecoding enables efficient low-bit KV-cache decoding by\ncooperatively leveraging CUDA cores and Tensor Cores. It introduces methods for\nautomatically inducing optimized layouts to exploit Tensor Cores, along with\nwarp-level parallelization strategies for dequantization. For unified system\nsupport, BitDecoding includes a query transformation module supporting diverse\nattention variants, a quantization kernel that supports both tensor-wise and\nchannel-wise scaling used in various quantization algorithms with high\nperformance, and a dequantization kernel with a software-defined pipeline to\ncoordinate CUDA and Tensor Cores execution for mixed-precision operations.\nEvaluated on RTX 4090, A100, and H100, BitDecoding accelerates decoding by up\nto 7.5x, 4.8x, and 8.9x, respectively, over FP16 FlashDecoding-v2, and\nsurpasses the state-of-the-art low-bit system QServe by up to 4.3x. On\nLLaMA-3.1-8B with a 128K context, BitDecoding reduces single-batch decoding\nlatency by 3x, showing substantial improvements for long-context generation.\nThe code is available at https://github.com/DD-DuDa/BitDecoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of long-context Large Language Models (LLMs) amplifies memory and\nbandwidth demands during autoregressive decoding, as the Key-Value (KV) cache\ngrows with each generated token. Low-bit KV-cache quantization (e.g., 4-bit or\n2-bit) can reduce memory footprint while preserving accuracy, but existing\nsystems suffer from slow decoding due to their exclusive reliance on CUDA\ncores, neglecting Tensor Cores (the primary source of compute on modern GPUs).\nWe present BitDecoding, a new long-context LLM inference system with a low-bit\nKV cache. BitDecoding enables efficient low-bit KV-cache decoding by\ncooperatively leveraging CUDA cores and Tensor Cores. It introduces methods for\nautomatically inducing optimized layouts to exploit Tensor Cores, along with\nwarp-level parallelization strategies for dequantization. For unified system\nsupport, BitDecoding includes a query transformation module supporting diverse\nattention variants, a quantization kernel that supports both tensor-wise and\nchannel-wise scaling used in various quantization algorithms with high\nperformance, and a dequantization kernel with a software-defined pipeline to\ncoordinate CUDA and Tensor Cores execution for mixed-precision operations.\nEvaluated on RTX 4090, A100, and H100, BitDecoding accelerates decoding by up\nto 7.5x, 4.8x, and 8.9x, respectively, over FP16 FlashDecoding-v2, and\nsurpasses the state-of-the-art low-bit system QServe by up to 4.3x. On\nLLaMA-3.1-8B with a 128K context, BitDecoding reduces single-batch decoding\nlatency by 3x, showing substantial improvements for long-context generation.\nThe code is available at https://github.com/DD-DuDa/BitDecoding."
                },
                "authors": [
                    {
                        "name": "Dayou Du"
                    },
                    {
                        "name": "Shijie Cao"
                    },
                    {
                        "name": "Jianyi Cheng"
                    },
                    {
                        "name": "Luo Mai"
                    },
                    {
                        "name": "Ting Cao"
                    },
                    {
                        "name": "Mao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Mao Yang"
                },
                "author": "Mao Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18773v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18773v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.06818v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.06818v3",
                "updated": "2025-08-14T15:36:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    15,
                    36,
                    22,
                    3,
                    226,
                    0
                ],
                "published": "2025-06-07T14:50:26Z",
                "published_parsed": [
                    2025,
                    6,
                    7,
                    14,
                    50,
                    26,
                    5,
                    158,
                    0
                ],
                "title": "Stepwise Decomposition and Dual-stream Focus: A Novel Approach for\n  Training-free Camouflaged Object Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stepwise Decomposition and Dual-stream Focus: A Novel Approach for\n  Training-free Camouflaged Object Segmentation"
                },
                "summary": "While promptable segmentation (\\textit{e.g.}, SAM) has shown promise for\nvarious segmentation tasks, it still requires manual visual prompts for each\nobject to be segmented. In contrast, task-generic promptable segmentation aims\nto reduce the need for such detailed prompts by employing only a task-generic\nprompt to guide segmentation across all test samples. However, when applied to\nCamouflaged Object Segmentation (COS), current methods still face two critical\nissues: 1) \\textit{\\textbf{semantic ambiguity in getting instance-specific text\nprompts}}, which arises from insufficient discriminative cues in holistic\ncaptions, leading to foreground-background confusion; 2)\n\\textit{\\textbf{semantic discrepancy combined with spatial separation in\ngetting instance-specific visual prompts}}, which results from global\nbackground sampling far from object boundaries with low feature correlation,\ncausing SAM to segment irrelevant regions. To address the issues above, we\npropose \\textbf{RDVP-MSD}, a novel training-free test-time adaptation framework\nthat synergizes \\textbf{R}egion-constrained \\textbf{D}ual-stream\n\\textbf{V}isual \\textbf{P}rompting (RDVP) via \\textbf{M}ultimodal\n\\textbf{S}tepwise \\textbf{D}ecomposition Chain of Thought (MSD-CoT). MSD-CoT\nprogressively disentangles image captions to eliminate semantic ambiguity,\nwhile RDVP injects spatial constraints into visual prompting and independently\nsamples visual prompts for foreground and background points, effectively\nmitigating semantic discrepancy and spatial separation. Without requiring any\ntraining or supervision, RDVP-MSD achieves a state-of-the-art segmentation\nresult on multiple COS benchmarks and delivers a faster inference speed than\nprevious methods, demonstrating significantly improved accuracy and efficiency.\nThe codes will be available at\n\\href{https://github.com/ycyinchao/RDVP-MSD}{https://github.com/ycyinchao/RDVP-MSD}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While promptable segmentation (\\textit{e.g.}, SAM) has shown promise for\nvarious segmentation tasks, it still requires manual visual prompts for each\nobject to be segmented. In contrast, task-generic promptable segmentation aims\nto reduce the need for such detailed prompts by employing only a task-generic\nprompt to guide segmentation across all test samples. However, when applied to\nCamouflaged Object Segmentation (COS), current methods still face two critical\nissues: 1) \\textit{\\textbf{semantic ambiguity in getting instance-specific text\nprompts}}, which arises from insufficient discriminative cues in holistic\ncaptions, leading to foreground-background confusion; 2)\n\\textit{\\textbf{semantic discrepancy combined with spatial separation in\ngetting instance-specific visual prompts}}, which results from global\nbackground sampling far from object boundaries with low feature correlation,\ncausing SAM to segment irrelevant regions. To address the issues above, we\npropose \\textbf{RDVP-MSD}, a novel training-free test-time adaptation framework\nthat synergizes \\textbf{R}egion-constrained \\textbf{D}ual-stream\n\\textbf{V}isual \\textbf{P}rompting (RDVP) via \\textbf{M}ultimodal\n\\textbf{S}tepwise \\textbf{D}ecomposition Chain of Thought (MSD-CoT). MSD-CoT\nprogressively disentangles image captions to eliminate semantic ambiguity,\nwhile RDVP injects spatial constraints into visual prompting and independently\nsamples visual prompts for foreground and background points, effectively\nmitigating semantic discrepancy and spatial separation. Without requiring any\ntraining or supervision, RDVP-MSD achieves a state-of-the-art segmentation\nresult on multiple COS benchmarks and delivers a faster inference speed than\nprevious methods, demonstrating significantly improved accuracy and efficiency.\nThe codes will be available at\n\\href{https://github.com/ycyinchao/RDVP-MSD}{https://github.com/ycyinchao/RDVP-MSD}"
                },
                "authors": [
                    {
                        "name": "Chao Yin"
                    },
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Kequan Yang"
                    },
                    {
                        "name": "Jide Li"
                    },
                    {
                        "name": "Pinpin Zhu"
                    },
                    {
                        "name": "Xiaoqiang Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoqiang Li"
                },
                "author": "Xiaoqiang Li",
                "arxiv_comment": "accepted by ACM MM2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.06818v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.06818v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10752v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10752v1",
                "updated": "2025-08-14T15:35:55Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    15,
                    35,
                    55,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T15:35:55Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    15,
                    35,
                    55,
                    3,
                    226,
                    0
                ],
                "title": "Signature of the explosion mechanism of Type-Ia SNe in substructures of\n  the remnant: the case of Tycho's SNR",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Signature of the explosion mechanism of Type-Ia SNe in substructures of\n  the remnant: the case of Tycho's SNR"
                },
                "summary": "Type-Ia supernovae (SNe), or runaway thermonuclear explosions of white dwarfs\n(WDs), play a critical role in the chemical evolution of galaxies, and are\nimportant cosmological distance indicators due to their 'standardizable'\nlightcurves. Growing evidence, however, suggests greater diversity in their\nobserved lightcurves (and spectra) than thought previously. This is usually\nattributed to a variety of WD explosion mechanisms and progenitor system\nproperties, but a direct link between the explosion mechanisms and Type-Ia SN\nobservables remains elusive. Here we present a novel approach to identify\nexplosion mechanisms of Type-Ia SNe, by analyzing the sizes of small-scale\nturbulent substructures of different elements in their extended ejecta, i.e.,\nin Supernova Remnants (SNRs). Our three-dimensional hydrodynamical models show\nthat substructures in an SNR dominated by iron-group elements may have a\ntypical size different from substructures dominated by intermediate mass\nelements (e.g., Si, S) in the same SNR. This size difference is governed by the\nexplosion mechanism. Applying this approach to Tycho's SNR, we find that its\nobserved structure is most consistent with the explosion of a sub-Chandrasekhar\nmass WD via the double-detonation mechanism. Extending this method to other\nwell-characterized SNRs can let us connect the inferred explosion mechanism to\nthe associated historical SNe, which often have spectra reconstructed through\nlight echo observations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Type-Ia supernovae (SNe), or runaway thermonuclear explosions of white dwarfs\n(WDs), play a critical role in the chemical evolution of galaxies, and are\nimportant cosmological distance indicators due to their 'standardizable'\nlightcurves. Growing evidence, however, suggests greater diversity in their\nobserved lightcurves (and spectra) than thought previously. This is usually\nattributed to a variety of WD explosion mechanisms and progenitor system\nproperties, but a direct link between the explosion mechanisms and Type-Ia SN\nobservables remains elusive. Here we present a novel approach to identify\nexplosion mechanisms of Type-Ia SNe, by analyzing the sizes of small-scale\nturbulent substructures of different elements in their extended ejecta, i.e.,\nin Supernova Remnants (SNRs). Our three-dimensional hydrodynamical models show\nthat substructures in an SNR dominated by iron-group elements may have a\ntypical size different from substructures dominated by intermediate mass\nelements (e.g., Si, S) in the same SNR. This size difference is governed by the\nexplosion mechanism. Applying this approach to Tycho's SNR, we find that its\nobserved structure is most consistent with the explosion of a sub-Chandrasekhar\nmass WD via the double-detonation mechanism. Extending this method to other\nwell-characterized SNRs can let us connect the inferred explosion mechanism to\nthe associated historical SNe, which often have spectra reconstructed through\nlight echo observations."
                },
                "authors": [
                    {
                        "name": "Soham Mandal"
                    },
                    {
                        "name": "Nria Torres-Alb"
                    },
                    {
                        "name": "Carles Badenes"
                    },
                    {
                        "name": "Shazrene Mohamed"
                    }
                ],
                "author_detail": {
                    "name": "Shazrene Mohamed"
                },
                "author": "Shazrene Mohamed",
                "arxiv_comment": "20 pages, 10 figures; submitted to ApJ. Comments are welcome!",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10752v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10752v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10751v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10751v1",
                "updated": "2025-08-14T15:34:47Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    15,
                    34,
                    47,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T15:34:47Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    15,
                    34,
                    47,
                    3,
                    226,
                    0
                ],
                "title": "Pass@k Training for Adaptively Balancing Exploration and Exploitation of\n  Large Reasoning Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pass@k Training for Adaptively Balancing Exploration and Exploitation of\n  Large Reasoning Models"
                },
                "summary": "Reinforcement learning with verifiable rewards (RLVR), which typically adopts\nPass@1 as the reward, has faced the issues in balancing exploration and\nexploitation, causing policies to prefer conservative actions, converging to a\nlocal optimum. Identifying an appropriate reward metric is therefore crucial.\nRegarding the prior work, although Pass@k has been used in evaluation, its\nconnection to LLM exploration ability in RLVR remains largely overlooked. To\ninvestigate this, we first use Pass@k as the reward to train the policy model\n(i.e., $\\textbf{Pass@k Training}$), and observe the improvement on its\nexploration ability. Next, we derive an analytical solution for the advantage\nof Pass@k Training, leading to an efficient and effective process. Building on\nthis, our analysis reveals that exploration and exploitation are not inherently\nconflicting objectives, while they can mutually enhance each other. Moreover,\nPass@k Training with analytical derivation essentially involves directly\ndesigning the advantage function. Inspired by this, we preliminarily explore\nthe advantage design for RLVR, showing promising results and highlighting a\npotential future direction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning with verifiable rewards (RLVR), which typically adopts\nPass@1 as the reward, has faced the issues in balancing exploration and\nexploitation, causing policies to prefer conservative actions, converging to a\nlocal optimum. Identifying an appropriate reward metric is therefore crucial.\nRegarding the prior work, although Pass@k has been used in evaluation, its\nconnection to LLM exploration ability in RLVR remains largely overlooked. To\ninvestigate this, we first use Pass@k as the reward to train the policy model\n(i.e., $\\textbf{Pass@k Training}$), and observe the improvement on its\nexploration ability. Next, we derive an analytical solution for the advantage\nof Pass@k Training, leading to an efficient and effective process. Building on\nthis, our analysis reveals that exploration and exploitation are not inherently\nconflicting objectives, while they can mutually enhance each other. Moreover,\nPass@k Training with analytical derivation essentially involves directly\ndesigning the advantage function. Inspired by this, we preliminarily explore\nthe advantage design for RLVR, showing promising results and highlighting a\npotential future direction."
                },
                "authors": [
                    {
                        "name": "Zhipeng Chen"
                    },
                    {
                        "name": "Xiaobo Qin"
                    },
                    {
                        "name": "Youbin Wu"
                    },
                    {
                        "name": "Yue Ling"
                    },
                    {
                        "name": "Qinghao Ye"
                    },
                    {
                        "name": "Wayne Xin Zhao"
                    },
                    {
                        "name": "Guang Shi"
                    }
                ],
                "author_detail": {
                    "name": "Guang Shi"
                },
                "author": "Guang Shi",
                "arxiv_comment": "Technical Report about RLVR: 32 pages, 18 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10751v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10751v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10743v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10743v1",
                "updated": "2025-08-14T15:28:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    15,
                    28,
                    9,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T15:28:09Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    15,
                    28,
                    9,
                    3,
                    226,
                    0
                ],
                "title": "An Efficient Model-Driven Groupwise Approach for Atlas Construction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Efficient Model-Driven Groupwise Approach for Atlas Construction"
                },
                "summary": "Atlas construction is fundamental to medical image analysis, offering a\nstandardized spatial reference for tasks such as population-level anatomical\nmodeling. While data-driven registration methods have recently shown promise in\npairwise settings, their reliance on large training datasets, limited\ngeneralizability, and lack of true inference phases in groupwise contexts\nhinder their practical use. In contrast, model-driven methods offer\ntraining-free, theoretically grounded, and data-efficient alternatives, though\nthey often face scalability and optimization challenges when applied to large\n3D datasets. In this work, we introduce DARC (Diffeomorphic Atlas Registration\nvia Coordinate descent), a novel model-driven groupwise registration framework\nfor atlas construction. DARC supports a broad range of image dissimilarity\nmetrics and efficiently handles arbitrary numbers of 3D images without\nincurring GPU memory issues. Through a coordinate descent strategy and a\ncentrality-enforcing activation function, DARC produces unbiased, diffeomorphic\natlases with high anatomical fidelity. Beyond atlas construction, we\ndemonstrate two key applications: (1) One-shot segmentation, where labels\nannotated only on the atlas are propagated to subjects via inverse\ndeformations, outperforming state-of-the-art few-shot methods; and (2) shape\nsynthesis, where new anatomical variants are generated by warping the atlas\nmesh using synthesized diffeomorphic deformation fields. Overall, DARC offers a\nflexible, generalizable, and resource-efficient framework for atlas\nconstruction and applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Atlas construction is fundamental to medical image analysis, offering a\nstandardized spatial reference for tasks such as population-level anatomical\nmodeling. While data-driven registration methods have recently shown promise in\npairwise settings, their reliance on large training datasets, limited\ngeneralizability, and lack of true inference phases in groupwise contexts\nhinder their practical use. In contrast, model-driven methods offer\ntraining-free, theoretically grounded, and data-efficient alternatives, though\nthey often face scalability and optimization challenges when applied to large\n3D datasets. In this work, we introduce DARC (Diffeomorphic Atlas Registration\nvia Coordinate descent), a novel model-driven groupwise registration framework\nfor atlas construction. DARC supports a broad range of image dissimilarity\nmetrics and efficiently handles arbitrary numbers of 3D images without\nincurring GPU memory issues. Through a coordinate descent strategy and a\ncentrality-enforcing activation function, DARC produces unbiased, diffeomorphic\natlases with high anatomical fidelity. Beyond atlas construction, we\ndemonstrate two key applications: (1) One-shot segmentation, where labels\nannotated only on the atlas are propagated to subjects via inverse\ndeformations, outperforming state-of-the-art few-shot methods; and (2) shape\nsynthesis, where new anatomical variants are generated by warping the atlas\nmesh using synthesized diffeomorphic deformation fields. Overall, DARC offers a\nflexible, generalizable, and resource-efficient framework for atlas\nconstruction and applications."
                },
                "authors": [
                    {
                        "name": "Ziwei Zou"
                    },
                    {
                        "name": "Bei Zou"
                    },
                    {
                        "name": "Xiaoyan Kui"
                    },
                    {
                        "name": "Wenqi Lu"
                    },
                    {
                        "name": "Haoran Dou"
                    },
                    {
                        "name": "Arezoo Zakeri"
                    },
                    {
                        "name": "Timothy Cootes"
                    },
                    {
                        "name": "Alejandro F Frangi"
                    },
                    {
                        "name": "Jinming Duan"
                    }
                ],
                "author_detail": {
                    "name": "Jinming Duan"
                },
                "author": "Jinming Duan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10743v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10743v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10736v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10736v1",
                "updated": "2025-08-14T15:16:25Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    15,
                    16,
                    25,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T15:16:25Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    15,
                    16,
                    25,
                    3,
                    226,
                    0
                ],
                "title": "Thinking Inside the Mask: In-Place Prompting in Diffusion LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Thinking Inside the Mask: In-Place Prompting in Diffusion LLMs"
                },
                "summary": "Despite large language models (LLMs) have achieved remarkable success, their\nprefix-only prompting paradigm and sequential generation process offer limited\nflexibility for bidirectional information. Diffusion large language models\n(dLLMs) present new opportunities through their bidirectional attention\nmechanisms and iterative refinement processes, enabling more flexible in-place\nprompting strategies. We introduce ICE (In-Place Chain-of-Thought Prompting\nwith Early Exit), a novel framework that transforms prefix-only prompting into\nin-place prompting specifically designed for dLLMs. ICE integrates in-place\nprompts directly within masked token positions during iterative refinement and\nemploys a confidence-aware early exit mechanism to significantly reduce\ncomputational overhead. Extensive experiments demonstrate ICE's effectiveness,\nachieving up to 17.29% accuracy improvement with 4.12$\\times$ speedup on GSM8K,\nand up to 276.67$\\times$ acceleration on MMLU while maintaining competitive\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite large language models (LLMs) have achieved remarkable success, their\nprefix-only prompting paradigm and sequential generation process offer limited\nflexibility for bidirectional information. Diffusion large language models\n(dLLMs) present new opportunities through their bidirectional attention\nmechanisms and iterative refinement processes, enabling more flexible in-place\nprompting strategies. We introduce ICE (In-Place Chain-of-Thought Prompting\nwith Early Exit), a novel framework that transforms prefix-only prompting into\nin-place prompting specifically designed for dLLMs. ICE integrates in-place\nprompts directly within masked token positions during iterative refinement and\nemploys a confidence-aware early exit mechanism to significantly reduce\ncomputational overhead. Extensive experiments demonstrate ICE's effectiveness,\nachieving up to 17.29% accuracy improvement with 4.12$\\times$ speedup on GSM8K,\nand up to 276.67$\\times$ acceleration on MMLU while maintaining competitive\nperformance."
                },
                "authors": [
                    {
                        "name": "Xiangqi Jin"
                    },
                    {
                        "name": "Yuxuan Wang"
                    },
                    {
                        "name": "Yifeng Gao"
                    },
                    {
                        "name": "Zichen Wen"
                    },
                    {
                        "name": "Biqing Qi"
                    },
                    {
                        "name": "Dongrui Liu"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10736v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10736v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.12215v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.12215v2",
                "updated": "2025-08-14T15:13:01Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    15,
                    13,
                    1,
                    3,
                    226,
                    0
                ],
                "published": "2025-06-13T20:33:20Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    20,
                    33,
                    20,
                    4,
                    164,
                    0
                ],
                "title": "Partial identification via conditional linear programs: estimation and\n  policy learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Partial identification via conditional linear programs: estimation and\n  policy learning"
                },
                "summary": "Many important quantities of interest are only partially identified from\nobservable data: the data can limit them to a set of plausible values, but not\nuniquely determine them. This paper develops a unified framework for\ncovariate-assisted estimation, inference, and decision making in partial\nidentification problems where the parameter of interest satisfies a series of\nlinear constraints, conditional on covariates. In such settings, bounds on the\nparameter can be written as expectations of solutions to conditional linear\nprograms that optimize a linear function subject to linear constraints, where\nboth the objective function and the constraints may depend on covariates and\nneed to be estimated from data. Examples include estimands involving the joint\ndistributions of potential outcomes, policy learning with inequality-aware\nvalue functions, and instrumental variable settings. We propose two de-biased\nestimators for bounds defined by conditional linear programs. The first\ndirectly solves the conditional linear programs with plugin estimates and uses\noutput from standard LP solvers to de-bias the plugin estimate, avoiding the\nneed for computationally demanding vertex enumeration of all possible solutions\nfor symbolic bounds. The second uses entropic regularization to create smooth\napproximations to the conditional linear programs, trading a small amount of\napproximation error for improved estimation and computational efficiency. We\nestablish conditions for asymptotic normality of both estimators, show that\nboth estimators are robust to first-order errors in estimating the conditional\nconstraints and objectives, and construct Wald-type confidence intervals for\nthe partially identified parameters. These results also extend to policy\nlearning problems where the value of a decision policy is only partially\nidentified. We apply our methods to a study on the effects of Medicaid\nenrollment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many important quantities of interest are only partially identified from\nobservable data: the data can limit them to a set of plausible values, but not\nuniquely determine them. This paper develops a unified framework for\ncovariate-assisted estimation, inference, and decision making in partial\nidentification problems where the parameter of interest satisfies a series of\nlinear constraints, conditional on covariates. In such settings, bounds on the\nparameter can be written as expectations of solutions to conditional linear\nprograms that optimize a linear function subject to linear constraints, where\nboth the objective function and the constraints may depend on covariates and\nneed to be estimated from data. Examples include estimands involving the joint\ndistributions of potential outcomes, policy learning with inequality-aware\nvalue functions, and instrumental variable settings. We propose two de-biased\nestimators for bounds defined by conditional linear programs. The first\ndirectly solves the conditional linear programs with plugin estimates and uses\noutput from standard LP solvers to de-bias the plugin estimate, avoiding the\nneed for computationally demanding vertex enumeration of all possible solutions\nfor symbolic bounds. The second uses entropic regularization to create smooth\napproximations to the conditional linear programs, trading a small amount of\napproximation error for improved estimation and computational efficiency. We\nestablish conditions for asymptotic normality of both estimators, show that\nboth estimators are robust to first-order errors in estimating the conditional\nconstraints and objectives, and construct Wald-type confidence intervals for\nthe partially identified parameters. These results also extend to policy\nlearning problems where the value of a decision policy is only partially\nidentified. We apply our methods to a study on the effects of Medicaid\nenrollment."
                },
                "authors": [
                    {
                        "name": "Eli Ben-Michael"
                    }
                ],
                "author_detail": {
                    "name": "Eli Ben-Michael"
                },
                "author": "Eli Ben-Michael",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.12215v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.12215v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06412v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06412v2",
                "updated": "2025-08-14T14:59:51Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    14,
                    59,
                    51,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-08T15:56:49Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    15,
                    56,
                    49,
                    4,
                    220,
                    0
                ],
                "title": "Sample-efficient LLM Optimization with Reset Replay",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sample-efficient LLM Optimization with Reset Replay"
                },
                "summary": "Recent advancements in post-training Large Language Models (LLMs),\nparticularly through Reinforcement Learning (RL) and preference optimization\nmethods, are key drivers for enhancing their reasoning capabilities. However,\nthese methods are often plagued by low sample efficiency and a susceptibility\nto primacy bias, where overfitting to initial experiences degrades policy\nquality and damages the learning process. To address these challenges, we\nintroduce LLM optimization with Reset Replay (LoRR), a general and powerful\nplugin designed to enhance sample efficiency in any preference-based\noptimization framework. LoRR core mechanism enables training at a high replay\nnumber, maximizing the utility of each collected data batch. To counteract the\nrisk of overfitting inherent in high-replay training, LoRR incorporates a\nperiodic reset strategy with reusing initial data, which preserves network\nplasticity. Furthermore, it leverages a hybrid optimization objective,\ncombining supervised fine-tuning (SFT) and preference-based losses to further\nbolster data exploitation. Our extensive experiments demonstrate that LoRR\nsignificantly boosts the performance of various preference optimization methods\non both mathematical and general reasoning benchmarks. Notably, an iterative\nDPO approach augmented with LoRR achieves comparable performance on challenging\nmath tasks, outperforming some complex and computationally intensive RL-based\nalgorithms. These findings highlight that LoRR offers a practical,\nsample-efficient, and highly effective paradigm for LLM finetuning, unlocking\ngreater performance from limited data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in post-training Large Language Models (LLMs),\nparticularly through Reinforcement Learning (RL) and preference optimization\nmethods, are key drivers for enhancing their reasoning capabilities. However,\nthese methods are often plagued by low sample efficiency and a susceptibility\nto primacy bias, where overfitting to initial experiences degrades policy\nquality and damages the learning process. To address these challenges, we\nintroduce LLM optimization with Reset Replay (LoRR), a general and powerful\nplugin designed to enhance sample efficiency in any preference-based\noptimization framework. LoRR core mechanism enables training at a high replay\nnumber, maximizing the utility of each collected data batch. To counteract the\nrisk of overfitting inherent in high-replay training, LoRR incorporates a\nperiodic reset strategy with reusing initial data, which preserves network\nplasticity. Furthermore, it leverages a hybrid optimization objective,\ncombining supervised fine-tuning (SFT) and preference-based losses to further\nbolster data exploitation. Our extensive experiments demonstrate that LoRR\nsignificantly boosts the performance of various preference optimization methods\non both mathematical and general reasoning benchmarks. Notably, an iterative\nDPO approach augmented with LoRR achieves comparable performance on challenging\nmath tasks, outperforming some complex and computationally intensive RL-based\nalgorithms. These findings highlight that LoRR offers a practical,\nsample-efficient, and highly effective paradigm for LLM finetuning, unlocking\ngreater performance from limited data."
                },
                "authors": [
                    {
                        "name": "Zichuan Liu"
                    },
                    {
                        "name": "Jinyu Wang"
                    },
                    {
                        "name": "Lei Song"
                    },
                    {
                        "name": "Jiang Bian"
                    }
                ],
                "author_detail": {
                    "name": "Jiang Bian"
                },
                "author": "Jiang Bian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06412v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06412v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10712v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10712v1",
                "updated": "2025-08-14T14:55:19Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    14,
                    55,
                    19,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T14:55:19Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    14,
                    55,
                    19,
                    3,
                    226,
                    0
                ],
                "title": "Lightweight CNNs for Embedded SAR Ship Target Detection and\n  Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lightweight CNNs for Embedded SAR Ship Target Detection and\n  Classification"
                },
                "summary": "Synthetic Aperture Radar (SAR) data enables large-scale surveillance of\nmaritime vessels. However, near-real-time monitoring is currently constrained\nby the need to downlink all raw data, perform image focusing, and subsequently\nanalyze it on the ground. On-board processing to generate higher-level products\ncould reduce the data volume that needs to be downlinked, alleviating bandwidth\nconstraints and minimizing latency. However, traditional image focusing and\nprocessing algorithms face challenges due to the satellite's limited memory,\nprocessing power, and computational resources. This work proposes and evaluates\nneural networks designed for real-time inference on unfocused SAR data acquired\nin Stripmap and Interferometric Wide (IW) modes captured with Sentinel-1. Our\nresults demonstrate the feasibility of using one of our models for on-board\nprocessing and deployment on an FPGA. Additionally, by investigating a binary\nclassification task between ships and windmills, we demonstrate that target\nclassification is possible.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synthetic Aperture Radar (SAR) data enables large-scale surveillance of\nmaritime vessels. However, near-real-time monitoring is currently constrained\nby the need to downlink all raw data, perform image focusing, and subsequently\nanalyze it on the ground. On-board processing to generate higher-level products\ncould reduce the data volume that needs to be downlinked, alleviating bandwidth\nconstraints and minimizing latency. However, traditional image focusing and\nprocessing algorithms face challenges due to the satellite's limited memory,\nprocessing power, and computational resources. This work proposes and evaluates\nneural networks designed for real-time inference on unfocused SAR data acquired\nin Stripmap and Interferometric Wide (IW) modes captured with Sentinel-1. Our\nresults demonstrate the feasibility of using one of our models for on-board\nprocessing and deployment on an FPGA. Additionally, by investigating a binary\nclassification task between ships and windmills, we demonstrate that target\nclassification is possible."
                },
                "authors": [
                    {
                        "name": "Fabian Kresse"
                    },
                    {
                        "name": "Georgios Pilikos"
                    },
                    {
                        "name": "Mario Azcueta"
                    },
                    {
                        "name": "Nicolas Floury"
                    }
                ],
                "author_detail": {
                    "name": "Nicolas Floury"
                },
                "author": "Nicolas Floury",
                "arxiv_comment": "Accepted at Big Data from Space 2025 (BiDS'25)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10712v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10712v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10710v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10710v1",
                "updated": "2025-08-14T14:53:53Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    14,
                    53,
                    53,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T14:53:53Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    14,
                    53,
                    53,
                    3,
                    226,
                    0
                ],
                "title": "CountCluster: Training-Free Object Quantity Guidance with\n  Cross-Attention Map Clustering for Text-to-Image Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CountCluster: Training-Free Object Quantity Guidance with\n  Cross-Attention Map Clustering for Text-to-Image Generation"
                },
                "summary": "Diffusion-based text-to-image generation models have demonstrated strong\nperformance in terms of image quality and diversity. However, they still\nstruggle to generate images that accurately reflect the number of objects\nspecified in the input prompt. Several approaches have been proposed that rely\non either external counting modules for iterative refinement or quantity\nrepresentations derived from learned tokens or latent features. However, they\nstill have limitations in accurately reflecting the specified number of objects\nand overlook an important structural characteristic--The number of object\ninstances in the generated image is largely determined in the early timesteps\nof the denoising process. To correctly reflect the object quantity for image\ngeneration, the highly activated regions in the object cross-attention map at\nthe early timesteps should match the input object quantity, while each region\nshould be clearly separated. To address this issue, we propose\n\\textit{CountCluster}, a method that guides the object cross-attention map to\nbe clustered according to the specified object count in the input, without\nrelying on any external tools or additional training. The proposed method\npartitions the object cross-attention map into $k$ clusters at inference time\nbased on attention scores, defines an ideal distribution in which each cluster\nis spatially well-separated, and optimizes the latent to align with this target\ndistribution. Our method achieves an average improvement of 18.5\\%p in object\ncount accuracy compared to existing methods, and demonstrates superior quantity\ncontrol performance across a variety of prompts. Code will be released at:\nhttps://github.com/JoohyeonL22/CountCluster .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based text-to-image generation models have demonstrated strong\nperformance in terms of image quality and diversity. However, they still\nstruggle to generate images that accurately reflect the number of objects\nspecified in the input prompt. Several approaches have been proposed that rely\non either external counting modules for iterative refinement or quantity\nrepresentations derived from learned tokens or latent features. However, they\nstill have limitations in accurately reflecting the specified number of objects\nand overlook an important structural characteristic--The number of object\ninstances in the generated image is largely determined in the early timesteps\nof the denoising process. To correctly reflect the object quantity for image\ngeneration, the highly activated regions in the object cross-attention map at\nthe early timesteps should match the input object quantity, while each region\nshould be clearly separated. To address this issue, we propose\n\\textit{CountCluster}, a method that guides the object cross-attention map to\nbe clustered according to the specified object count in the input, without\nrelying on any external tools or additional training. The proposed method\npartitions the object cross-attention map into $k$ clusters at inference time\nbased on attention scores, defines an ideal distribution in which each cluster\nis spatially well-separated, and optimizes the latent to align with this target\ndistribution. Our method achieves an average improvement of 18.5\\%p in object\ncount accuracy compared to existing methods, and demonstrates superior quantity\ncontrol performance across a variety of prompts. Code will be released at:\nhttps://github.com/JoohyeonL22/CountCluster ."
                },
                "authors": [
                    {
                        "name": "Joohyeon Lee"
                    },
                    {
                        "name": "Jin-Seop Lee"
                    },
                    {
                        "name": "Jee-Hyong Lee"
                    }
                ],
                "author_detail": {
                    "name": "Jee-Hyong Lee"
                },
                "author": "Jee-Hyong Lee",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10710v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10710v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10703v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10703v1",
                "updated": "2025-08-14T14:48:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    14,
                    48,
                    9,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T14:48:09Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    14,
                    48,
                    9,
                    3,
                    226,
                    0
                ],
                "title": "GenOM: Ontology Matching with Description Generation and Large Language\n  Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GenOM: Ontology Matching with Description Generation and Large Language\n  Model"
                },
                "summary": "Ontology matching (OM) plays an essential role in enabling semantic\ninteroperability and integration across heterogeneous knowledge sources,\nparticularly in the biomedical domain which contains numerous complex concepts\nrelated to diseases and pharmaceuticals. This paper introduces GenOM, a large\nlanguage model (LLM)-based ontology alignment framework, which enriches the\nsemantic representations of ontology concepts via generating textual\ndefinitions, retrieves alignment candidates with an embedding model, and\nincorporates exact matching-based tools to improve precision. Extensive\nexperiments conducted on the OAEI Bio-ML track demonstrate that GenOM can often\nachieve competitive performance, surpassing many baselines including\ntraditional OM systems and recent LLM-based methods. Further ablation studies\nconfirm the effectiveness of semantic enrichment and few-shot prompting,\nhighlighting the framework's robustness and adaptability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ontology matching (OM) plays an essential role in enabling semantic\ninteroperability and integration across heterogeneous knowledge sources,\nparticularly in the biomedical domain which contains numerous complex concepts\nrelated to diseases and pharmaceuticals. This paper introduces GenOM, a large\nlanguage model (LLM)-based ontology alignment framework, which enriches the\nsemantic representations of ontology concepts via generating textual\ndefinitions, retrieves alignment candidates with an embedding model, and\nincorporates exact matching-based tools to improve precision. Extensive\nexperiments conducted on the OAEI Bio-ML track demonstrate that GenOM can often\nachieve competitive performance, surpassing many baselines including\ntraditional OM systems and recent LLM-based methods. Further ablation studies\nconfirm the effectiveness of semantic enrichment and few-shot prompting,\nhighlighting the framework's robustness and adaptability."
                },
                "authors": [
                    {
                        "name": "Yiping Song"
                    },
                    {
                        "name": "Jiaoyan Chen"
                    },
                    {
                        "name": "Renate A. Schmidt"
                    }
                ],
                "author_detail": {
                    "name": "Renate A. Schmidt"
                },
                "author": "Renate A. Schmidt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10703v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10703v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10702v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10702v1",
                "updated": "2025-08-14T14:46:26Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    14,
                    46,
                    26,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T14:46:26Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    14,
                    46,
                    26,
                    3,
                    226,
                    0
                ],
                "title": "Encoding and inference on separable effects for sustained treatments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Encoding and inference on separable effects for sustained treatments"
                },
                "summary": "Sustained treatment strategies are common in many domains, particularly in\nmedicine, where many treatment are delivered repeatedly over time. The effects\nof adherence to a treatment strategy throughout follow-up are often more\nrelevant to decision-makers than effects of treatment assignment or initiation.\nHere we consider the separable effect of sustained use of a time-varying\ntreatment. Despite the potential usefulness of this estimand, the theory of\nseparable effects has yet to be extended to settings with sustained treatment\nstrategies. To derive our results, we use an unconventional encoding of\ntime-varying treatment strategies. This allows us to obtain concise\nformulations of identifying assumptions with better practical properties; for\nexample, they admit frugal graphical representations and formulations of\nidentifying functionals. These functionals are used to motivate doubly robust\nsemiparametrically efficient estimators. The results are applied to the\nSystolic Blood Pressure Intervention Trial (SPRINT), where we estimate a\nseparable effect of modified blood pressure treatments on the risk of acute\nkidney injury.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sustained treatment strategies are common in many domains, particularly in\nmedicine, where many treatment are delivered repeatedly over time. The effects\nof adherence to a treatment strategy throughout follow-up are often more\nrelevant to decision-makers than effects of treatment assignment or initiation.\nHere we consider the separable effect of sustained use of a time-varying\ntreatment. Despite the potential usefulness of this estimand, the theory of\nseparable effects has yet to be extended to settings with sustained treatment\nstrategies. To derive our results, we use an unconventional encoding of\ntime-varying treatment strategies. This allows us to obtain concise\nformulations of identifying assumptions with better practical properties; for\nexample, they admit frugal graphical representations and formulations of\nidentifying functionals. These functionals are used to motivate doubly robust\nsemiparametrically efficient estimators. The results are applied to the\nSystolic Blood Pressure Intervention Trial (SPRINT), where we estimate a\nseparable effect of modified blood pressure treatments on the risk of acute\nkidney injury."
                },
                "authors": [
                    {
                        "name": "Ignacio Gonzalez-Perez"
                    },
                    {
                        "name": "Kerollos Nashat Wanis"
                    },
                    {
                        "name": "Aaron Leor Sarvet"
                    },
                    {
                        "name": "Mats Julius Stensrud"
                    }
                ],
                "author_detail": {
                    "name": "Mats Julius Stensrud"
                },
                "author": "Mats Julius Stensrud",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10702v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10702v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10701v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10701v1",
                "updated": "2025-08-14T14:45:45Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    14,
                    45,
                    45,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T14:45:45Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    14,
                    45,
                    45,
                    3,
                    226,
                    0
                ],
                "title": "REFN: A Reinforcement-Learning-From-Network Framework against\n  1-day/n-day Exploitations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REFN: A Reinforcement-Learning-From-Network Framework against\n  1-day/n-day Exploitations"
                },
                "summary": "The exploitation of 1 day or n day vulnerabilities poses severe threats to\nnetworked devices due to massive deployment scales and delayed patching\n(average Mean Time To Patch exceeds 60 days). Existing defenses, including host\nbased patching and network based filtering, are inadequate due to limited\nscalability across diverse devices, compatibility issues especially with\nembedded or legacy systems, and error prone deployment process (manual patch\nvalidation). To address these issues, we introduce REFN (Reinforcement Learning\nFrom Network), a novel framework that trains Large Language Models (LLMs) to\nautonomously generate network filters to prevent 1 day or n day exploitations.\nREFN ensures scalability by uniquely employs Reinforcement Learning (RL) driven\nby online network rewards instead of traditional Human Feedback (RLHF). REFN\nguarantees compatibility via unified deployment on edge security gateways\n(Amazon Eero). REFN provides robustness via online validation using real\nnetwork traffic. Crucially, REFN addresses three core challenges in training\nLLMs for exploit prevention: 1) expanding current LLMs limited vulnerability\nfixing expertise via Agentic RAG based Knowledge Distillation, 2) bridging\ncurrent LLMs language to network gaps through an RL From VNF Pipeline that\ntranslates language context (vulnerability description) into network\nenforcement, 3) addressing the LLM hallucination and non determinism via the\nOnline Agentic Validation that penalizes erroneous outputs. Evaluated across 22\nfamilies of 1 day or n day exploits, REFN demonstrates effectiveness (21.1\npercent higher accuracy than alternatives), efficiency (Mean Time To Patch of\n3.65 hours) and scalability (easily scale to 10K devices). REFN serves as an\ninitial step toward training LLMs to rapidly prevent massive scale 1 day or n\nday exploitations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The exploitation of 1 day or n day vulnerabilities poses severe threats to\nnetworked devices due to massive deployment scales and delayed patching\n(average Mean Time To Patch exceeds 60 days). Existing defenses, including host\nbased patching and network based filtering, are inadequate due to limited\nscalability across diverse devices, compatibility issues especially with\nembedded or legacy systems, and error prone deployment process (manual patch\nvalidation). To address these issues, we introduce REFN (Reinforcement Learning\nFrom Network), a novel framework that trains Large Language Models (LLMs) to\nautonomously generate network filters to prevent 1 day or n day exploitations.\nREFN ensures scalability by uniquely employs Reinforcement Learning (RL) driven\nby online network rewards instead of traditional Human Feedback (RLHF). REFN\nguarantees compatibility via unified deployment on edge security gateways\n(Amazon Eero). REFN provides robustness via online validation using real\nnetwork traffic. Crucially, REFN addresses three core challenges in training\nLLMs for exploit prevention: 1) expanding current LLMs limited vulnerability\nfixing expertise via Agentic RAG based Knowledge Distillation, 2) bridging\ncurrent LLMs language to network gaps through an RL From VNF Pipeline that\ntranslates language context (vulnerability description) into network\nenforcement, 3) addressing the LLM hallucination and non determinism via the\nOnline Agentic Validation that penalizes erroneous outputs. Evaluated across 22\nfamilies of 1 day or n day exploits, REFN demonstrates effectiveness (21.1\npercent higher accuracy than alternatives), efficiency (Mean Time To Patch of\n3.65 hours) and scalability (easily scale to 10K devices). REFN serves as an\ninitial step toward training LLMs to rapidly prevent massive scale 1 day or n\nday exploitations."
                },
                "authors": [
                    {
                        "name": "Tianlong Yu"
                    },
                    {
                        "name": "Lihong Liu"
                    },
                    {
                        "name": "Ziyi Zhou"
                    },
                    {
                        "name": "Fudu Xing"
                    },
                    {
                        "name": "Kailong Wang"
                    },
                    {
                        "name": "Yang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yang Yang"
                },
                "author": "Yang Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10701v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10701v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10696v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10696v1",
                "updated": "2025-08-14T14:37:54Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    14,
                    37,
                    54,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T14:37:54Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    14,
                    37,
                    54,
                    3,
                    226,
                    0
                ],
                "title": "Chem3DLLM: 3D Multimodal Large Language Models for Chemistry",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chem3DLLM: 3D Multimodal Large Language Models for Chemistry"
                },
                "summary": "In the real world, a molecule is a 3D geometric structure. Compared to 1D\nSMILES sequences and 2D molecular graphs, 3D molecules represent the most\ninformative molecular modality. Despite the rapid progress of\nautoregressive-based language models, they cannot handle the generation of 3D\nmolecular conformation due to several challenges: 1) 3D molecular structures\nare incompatible with LLMs' discrete token space, 2) integrating heterogeneous\ninputs like proteins, ligands, and text remains difficult within a unified\nmodel, and 3) LLMs lack essential scientific priors, hindering the enforcement\nof physical and chemical constraints during generation. To tackle these issues,\nwe present Chem3DLLM, a unified protein-conditioned multimodal large language\nmodel. Our approach designs a novel reversible text encoding for 3D molecular\nstructures using run-length compression, achieving 3x size reduction while\npreserving complete structural information. This enables seamless integration\nof molecular geometry with protein pocket features in a single LLM\narchitecture. We employ reinforcement learning with stability-based rewards to\noptimize chemical validity and incorporate a lightweight protein embedding\nprojector for end-to-end training. Experimental results on structure-based drug\ndesign demonstrate state-of-the-art performance with a Vina score of -7.21,\nvalidating our unified multimodal approach for practical drug discovery\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the real world, a molecule is a 3D geometric structure. Compared to 1D\nSMILES sequences and 2D molecular graphs, 3D molecules represent the most\ninformative molecular modality. Despite the rapid progress of\nautoregressive-based language models, they cannot handle the generation of 3D\nmolecular conformation due to several challenges: 1) 3D molecular structures\nare incompatible with LLMs' discrete token space, 2) integrating heterogeneous\ninputs like proteins, ligands, and text remains difficult within a unified\nmodel, and 3) LLMs lack essential scientific priors, hindering the enforcement\nof physical and chemical constraints during generation. To tackle these issues,\nwe present Chem3DLLM, a unified protein-conditioned multimodal large language\nmodel. Our approach designs a novel reversible text encoding for 3D molecular\nstructures using run-length compression, achieving 3x size reduction while\npreserving complete structural information. This enables seamless integration\nof molecular geometry with protein pocket features in a single LLM\narchitecture. We employ reinforcement learning with stability-based rewards to\noptimize chemical validity and incorporate a lightweight protein embedding\nprojector for end-to-end training. Experimental results on structure-based drug\ndesign demonstrate state-of-the-art performance with a Vina score of -7.21,\nvalidating our unified multimodal approach for practical drug discovery\napplications."
                },
                "authors": [
                    {
                        "name": "Lei Jiang"
                    },
                    {
                        "name": "Shuzhou Sun"
                    },
                    {
                        "name": "Biqing Qi"
                    },
                    {
                        "name": "Yuchen Fu"
                    },
                    {
                        "name": "Xiaohua Xu"
                    },
                    {
                        "name": "Yuqiang Li"
                    },
                    {
                        "name": "Dongzhan Zhou"
                    },
                    {
                        "name": "Tianfan Fu"
                    }
                ],
                "author_detail": {
                    "name": "Tianfan Fu"
                },
                "author": "Tianfan Fu",
                "arxiv_comment": "15 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10696v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10696v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10695v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10695v1",
                "updated": "2025-08-14T14:36:53Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    14,
                    36,
                    53,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T14:36:53Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    14,
                    36,
                    53,
                    3,
                    226,
                    0
                ],
                "title": "Learning from Natural Language Feedback for Personalized Question\n  Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning from Natural Language Feedback for Personalized Question\n  Answering"
                },
                "summary": "Personalization is crucial for enhancing both the effectiveness and user\nsatisfaction of language technologies, particularly in information-seeking\ntasks like question answering. Current approaches for personalizing large\nlanguage models (LLMs) often rely on retrieval-augmented generation (RAG),\nfollowed by reinforcement learning with scalar reward signals to teach models\nhow to use retrieved personal context. We believe that these scalar rewards\nsometimes provide weak, non-instructive feedback, limiting learning efficiency\nand personalization quality. We introduce VAC, a novel framework for\npersonalized response generation that replaces scalar rewards with natural\nlanguage feedback (NLF) that are generated conditioned on the user profiles and\nthe question narratives. NLF serves as a rich and actionable supervision\nsignal, allowing the policy model to iteratively refine its outputs and\ninternalize effective personalization strategies. Training alternates between\noptimizing the feedback model and fine-tuning the policy model on the improved\nresponses, resulting in a policy model that no longer requires feedback at\ninference. Evaluation on the LaMP-QA benchmark that consists of three diverse\ndomains demonstrates consistent and significant improvements over the\nstate-of-the-art results. Human evaluations further confirm the superior\nquality of the generated responses. These results demonstrate that NLF provides\nmore effective signals for optimizing personalized question answering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalization is crucial for enhancing both the effectiveness and user\nsatisfaction of language technologies, particularly in information-seeking\ntasks like question answering. Current approaches for personalizing large\nlanguage models (LLMs) often rely on retrieval-augmented generation (RAG),\nfollowed by reinforcement learning with scalar reward signals to teach models\nhow to use retrieved personal context. We believe that these scalar rewards\nsometimes provide weak, non-instructive feedback, limiting learning efficiency\nand personalization quality. We introduce VAC, a novel framework for\npersonalized response generation that replaces scalar rewards with natural\nlanguage feedback (NLF) that are generated conditioned on the user profiles and\nthe question narratives. NLF serves as a rich and actionable supervision\nsignal, allowing the policy model to iteratively refine its outputs and\ninternalize effective personalization strategies. Training alternates between\noptimizing the feedback model and fine-tuning the policy model on the improved\nresponses, resulting in a policy model that no longer requires feedback at\ninference. Evaluation on the LaMP-QA benchmark that consists of three diverse\ndomains demonstrates consistent and significant improvements over the\nstate-of-the-art results. Human evaluations further confirm the superior\nquality of the generated responses. These results demonstrate that NLF provides\nmore effective signals for optimizing personalized question answering."
                },
                "authors": [
                    {
                        "name": "Alireza Salemi"
                    },
                    {
                        "name": "Hamed Zamani"
                    }
                ],
                "author_detail": {
                    "name": "Hamed Zamani"
                },
                "author": "Hamed Zamani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10695v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10695v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10691v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10691v1",
                "updated": "2025-08-14T14:35:54Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    14,
                    35,
                    54,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T14:35:54Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    14,
                    35,
                    54,
                    3,
                    226,
                    0
                ],
                "title": "THERMOS: Thermally-Aware Multi-Objective Scheduling of AI Workloads on\n  Heterogeneous Multi-Chiplet PIM Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "THERMOS: Thermally-Aware Multi-Objective Scheduling of AI Workloads on\n  Heterogeneous Multi-Chiplet PIM Architectures"
                },
                "summary": "Chiplet-based integration enables large-scale systems that combine diverse\ntechnologies, enabling higher yield, lower costs, and scalability, making them\nwell-suited to AI workloads. Processing-in-Memory (PIM) has emerged as a\npromising solution for AI inference, leveraging technologies such as ReRAM,\nSRAM, and FeFET, each offering unique advantages and trade-offs. A\nheterogeneous chiplet-based PIM architecture can harness the complementary\nstrengths of these technologies to enable higher performance and energy\nefficiency. However, scheduling AI workloads across such a heterogeneous system\nis challenging due to competing performance objectives, dynamic workload\ncharacteristics, and power and thermal constraints. To address this need, we\npropose THERMOS, a thermally-aware, multi-objective scheduling framework for AI\nworkloads on heterogeneous multi-chiplet PIM architectures. THERMOS trains a\nsingle multi-objective reinforcement learning (MORL) policy that is capable of\nachieving Pareto-optimal execution time, energy, or a balanced objective at\nruntime, depending on the target preferences. Comprehensive evaluations show\nthat THERMOS achieves up to 89% faster average execution time and 57% lower\naverage energy consumption than baseline AI workload scheduling algorithms with\nonly 0.14% runtime and 0.022% energy overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chiplet-based integration enables large-scale systems that combine diverse\ntechnologies, enabling higher yield, lower costs, and scalability, making them\nwell-suited to AI workloads. Processing-in-Memory (PIM) has emerged as a\npromising solution for AI inference, leveraging technologies such as ReRAM,\nSRAM, and FeFET, each offering unique advantages and trade-offs. A\nheterogeneous chiplet-based PIM architecture can harness the complementary\nstrengths of these technologies to enable higher performance and energy\nefficiency. However, scheduling AI workloads across such a heterogeneous system\nis challenging due to competing performance objectives, dynamic workload\ncharacteristics, and power and thermal constraints. To address this need, we\npropose THERMOS, a thermally-aware, multi-objective scheduling framework for AI\nworkloads on heterogeneous multi-chiplet PIM architectures. THERMOS trains a\nsingle multi-objective reinforcement learning (MORL) policy that is capable of\nachieving Pareto-optimal execution time, energy, or a balanced objective at\nruntime, depending on the target preferences. Comprehensive evaluations show\nthat THERMOS achieves up to 89% faster average execution time and 57% lower\naverage energy consumption than baseline AI workload scheduling algorithms with\nonly 0.14% runtime and 0.022% energy overhead."
                },
                "authors": [
                    {
                        "name": "Alish Kanani"
                    },
                    {
                        "name": "Lukas Pfromm"
                    },
                    {
                        "name": "Harsh Sharma"
                    },
                    {
                        "name": "Janardhan Rao Doppa"
                    },
                    {
                        "name": "Partha Pratim Pande"
                    },
                    {
                        "name": "Umit Y. Ogras"
                    }
                ],
                "author_detail": {
                    "name": "Umit Y. Ogras"
                },
                "author": "Umit Y. Ogras",
                "arxiv_comment": "Paper accepted at ESWEEK 2025 (CODES+ISSS) conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10691v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10691v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10688v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10688v1",
                "updated": "2025-08-14T14:32:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    14,
                    32,
                    52,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T14:32:52Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    14,
                    32,
                    52,
                    3,
                    226,
                    0
                ],
                "title": "Novel View Synthesis using DDIM Inversion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Novel View Synthesis using DDIM Inversion"
                },
                "summary": "Synthesizing novel views from a single input image is a challenging task. It\nrequires extrapolating the 3D structure of a scene while inferring details in\noccluded regions, and maintaining geometric consistency across viewpoints. Many\nexisting methods must fine-tune large diffusion backbones using multiple views\nor train a diffusion model from scratch, which is extremely expensive.\nAdditionally, they suffer from blurry reconstruction and poor generalization.\nThis gap presents the opportunity to explore an explicit lightweight view\ntranslation framework that can directly utilize the high-fidelity generative\ncapabilities of a pretrained diffusion model while reconstructing a scene from\na novel view. Given the DDIM-inverted latent of a single input image, we employ\na camera pose-conditioned translation U-Net, TUNet, to predict the inverted\nlatent corresponding to the desired target view. However, the image sampled\nusing the predicted latent may result in a blurry reconstruction. To this end,\nwe propose a novel fusion strategy that exploits the inherent noise correlation\nstructure observed in DDIM inversion. The proposed fusion strategy helps\npreserve the texture and fine-grained details. To synthesize the novel view, we\nuse the fused latent as the initial condition for DDIM sampling, leveraging the\ngenerative prior of the pretrained diffusion model. Extensive experiments on\nMVImgNet demonstrate that our method outperforms existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synthesizing novel views from a single input image is a challenging task. It\nrequires extrapolating the 3D structure of a scene while inferring details in\noccluded regions, and maintaining geometric consistency across viewpoints. Many\nexisting methods must fine-tune large diffusion backbones using multiple views\nor train a diffusion model from scratch, which is extremely expensive.\nAdditionally, they suffer from blurry reconstruction and poor generalization.\nThis gap presents the opportunity to explore an explicit lightweight view\ntranslation framework that can directly utilize the high-fidelity generative\ncapabilities of a pretrained diffusion model while reconstructing a scene from\na novel view. Given the DDIM-inverted latent of a single input image, we employ\na camera pose-conditioned translation U-Net, TUNet, to predict the inverted\nlatent corresponding to the desired target view. However, the image sampled\nusing the predicted latent may result in a blurry reconstruction. To this end,\nwe propose a novel fusion strategy that exploits the inherent noise correlation\nstructure observed in DDIM inversion. The proposed fusion strategy helps\npreserve the texture and fine-grained details. To synthesize the novel view, we\nuse the fused latent as the initial condition for DDIM sampling, leveraging the\ngenerative prior of the pretrained diffusion model. Extensive experiments on\nMVImgNet demonstrate that our method outperforms existing methods."
                },
                "authors": [
                    {
                        "name": "Sehajdeep SIngh"
                    },
                    {
                        "name": "A V Subramanyam"
                    }
                ],
                "author_detail": {
                    "name": "A V Subramanyam"
                },
                "author": "A V Subramanyam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10688v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10688v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10509v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10509v2",
                "updated": "2025-08-14T14:31:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    14,
                    31,
                    22,
                    3,
                    226,
                    0
                ],
                "published": "2025-03-13T16:10:14Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    16,
                    10,
                    14,
                    3,
                    72,
                    0
                ],
                "title": "From Actions to Words: Towards Abstractive-Textual Policy Summarization\n  in RL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Actions to Words: Towards Abstractive-Textual Policy Summarization\n  in RL"
                },
                "summary": "Policies generated by Reinforcement Learning (RL) algorithms are difficult to\nexplain to users, as they emerge from the interaction of complex reward\nstructures and neural network representations. Consequently, analyzing and\npredicting agent behavior can be challenging, undermining user trust in\nreal-world applications. To facilitate user understanding, current methods for\nglobal policy summarization typically rely on videos that demonstrate agent\nbehavior in a subset of world states. However, users can only watch a limited\nnumber of demonstrations, constraining their understanding. Moreover, these\nmethods place the burden of interpretation on users by presenting raw behaviors\nrather than synthesizing them into coherent patterns. To resolve these issues,\nwe introduce SySLLM (Synthesized Summary using Large Language Models),\nadvocating for a new paradigm of abstractive-textual policy explanations. By\nleveraging Large Language Models (LLMs)-which possess extensive world knowledge\nand pattern synthesis capabilities-SySLLM generates textual summaries that\nprovide structured and comprehensible explanations of agent policies. SySLLM\ndemonstrates that LLMs can interpret spatio-temporally structured descriptions\nof state-action trajectories from an RL agent and generate valuable policy\ninsights in a zero-shot setting, without any prior knowledge or fine-tuning.\nOur evaluation shows that SySLLM captures key insights, such as goal\npreferences and exploration strategies, that were also identified by human\nexperts. Furthermore, in a large-scale user study (with 200 participants),\nSySLLM summaries were preferred over demonstration-based summaries (HIGHLIGHTS)\nby a clear majority (75.5%) of participants.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Policies generated by Reinforcement Learning (RL) algorithms are difficult to\nexplain to users, as they emerge from the interaction of complex reward\nstructures and neural network representations. Consequently, analyzing and\npredicting agent behavior can be challenging, undermining user trust in\nreal-world applications. To facilitate user understanding, current methods for\nglobal policy summarization typically rely on videos that demonstrate agent\nbehavior in a subset of world states. However, users can only watch a limited\nnumber of demonstrations, constraining their understanding. Moreover, these\nmethods place the burden of interpretation on users by presenting raw behaviors\nrather than synthesizing them into coherent patterns. To resolve these issues,\nwe introduce SySLLM (Synthesized Summary using Large Language Models),\nadvocating for a new paradigm of abstractive-textual policy explanations. By\nleveraging Large Language Models (LLMs)-which possess extensive world knowledge\nand pattern synthesis capabilities-SySLLM generates textual summaries that\nprovide structured and comprehensible explanations of agent policies. SySLLM\ndemonstrates that LLMs can interpret spatio-temporally structured descriptions\nof state-action trajectories from an RL agent and generate valuable policy\ninsights in a zero-shot setting, without any prior knowledge or fine-tuning.\nOur evaluation shows that SySLLM captures key insights, such as goal\npreferences and exploration strategies, that were also identified by human\nexperts. Furthermore, in a large-scale user study (with 200 participants),\nSySLLM summaries were preferred over demonstration-based summaries (HIGHLIGHTS)\nby a clear majority (75.5%) of participants."
                },
                "authors": [
                    {
                        "name": "Sahar Admoni"
                    },
                    {
                        "name": "Assaf Hallak"
                    },
                    {
                        "name": "Yftah Ziser"
                    },
                    {
                        "name": "Omer Ben-Porat"
                    },
                    {
                        "name": "Ofra Amir"
                    }
                ],
                "author_detail": {
                    "name": "Ofra Amir"
                },
                "author": "Ofra Amir",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10509v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10509v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10677v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10677v1",
                "updated": "2025-08-14T14:20:34Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    14,
                    20,
                    34,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T14:20:34Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    14,
                    20,
                    34,
                    3,
                    226,
                    0
                ],
                "title": "Advancing Autonomous Incident Response: Leveraging LLMs and Cyber Threat\n  Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Autonomous Incident Response: Leveraging LLMs and Cyber Threat\n  Intelligence"
                },
                "summary": "Effective incident response (IR) is critical for mitigating cyber threats,\nyet security teams are overwhelmed by alert fatigue, high false-positive rates,\nand the vast volume of unstructured Cyber Threat Intelligence (CTI) documents.\nWhile CTI holds immense potential for enriching security operations, its\nextensive and fragmented nature makes manual analysis time-consuming and\nresource-intensive. To bridge this gap, we introduce a novel\nRetrieval-Augmented Generation (RAG)-based framework that leverages Large\nLanguage Models (LLMs) to automate and enhance IR by integrating dynamically\nretrieved CTI. Our approach introduces a hybrid retrieval mechanism that\ncombines NLP-based similarity searches within a CTI vector database with\nstandardized queries to external CTI platforms, facilitating context-aware\nenrichment of security alerts. The augmented intelligence is then leveraged by\nan LLM-powered response generation module, which formulates precise,\nactionable, and contextually relevant incident mitigation strategies. We\npropose a dual evaluation paradigm, wherein automated assessment using an\nauxiliary LLM is systematically cross-validated by cybersecurity experts.\nEmpirical validation on real-world and simulated alerts demonstrates that our\napproach enhances the accuracy, contextualization, and efficiency of IR,\nalleviating analyst workload and reducing response latency. This work\nunderscores the potential of LLM-driven CTI fusion in advancing autonomous\nsecurity operations and establishing a foundation for intelligent, adaptive\ncybersecurity frameworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective incident response (IR) is critical for mitigating cyber threats,\nyet security teams are overwhelmed by alert fatigue, high false-positive rates,\nand the vast volume of unstructured Cyber Threat Intelligence (CTI) documents.\nWhile CTI holds immense potential for enriching security operations, its\nextensive and fragmented nature makes manual analysis time-consuming and\nresource-intensive. To bridge this gap, we introduce a novel\nRetrieval-Augmented Generation (RAG)-based framework that leverages Large\nLanguage Models (LLMs) to automate and enhance IR by integrating dynamically\nretrieved CTI. Our approach introduces a hybrid retrieval mechanism that\ncombines NLP-based similarity searches within a CTI vector database with\nstandardized queries to external CTI platforms, facilitating context-aware\nenrichment of security alerts. The augmented intelligence is then leveraged by\nan LLM-powered response generation module, which formulates precise,\nactionable, and contextually relevant incident mitigation strategies. We\npropose a dual evaluation paradigm, wherein automated assessment using an\nauxiliary LLM is systematically cross-validated by cybersecurity experts.\nEmpirical validation on real-world and simulated alerts demonstrates that our\napproach enhances the accuracy, contextualization, and efficiency of IR,\nalleviating analyst workload and reducing response latency. This work\nunderscores the potential of LLM-driven CTI fusion in advancing autonomous\nsecurity operations and establishing a foundation for intelligent, adaptive\ncybersecurity frameworks."
                },
                "authors": [
                    {
                        "name": "Amine Tellache"
                    },
                    {
                        "name": "Abdelaziz Amara Korba"
                    },
                    {
                        "name": "Amdjed Mokhtari"
                    },
                    {
                        "name": "Horea Moldovan"
                    },
                    {
                        "name": "Yacine Ghamri-Doudane"
                    }
                ],
                "author_detail": {
                    "name": "Yacine Ghamri-Doudane"
                },
                "author": "Yacine Ghamri-Doudane",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10677v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10677v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.12830v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.12830v3",
                "updated": "2025-08-14T14:12:12Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    14,
                    12,
                    12,
                    3,
                    226,
                    0
                ],
                "published": "2024-07-03T11:16:54Z",
                "published_parsed": [
                    2024,
                    7,
                    3,
                    11,
                    16,
                    54,
                    2,
                    185,
                    0
                ],
                "title": "Knowledge-based Consistency Testing of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge-based Consistency Testing of Large Language Models"
                },
                "summary": "In this work, we systematically expose and measure the inconsistency and\nknowledge gaps of Large Language Models (LLMs). Specifically, we propose an\nautomated testing framework (called KonTest) which leverages a knowledge graph\nto construct test cases. KonTest probes and measures the inconsistencies in the\nLLM's knowledge of the world via a combination of semantically-equivalent\nqueries and test oracles (metamorphic or ontological oracle). KonTest further\nmitigates knowledge gaps via a weighted LLM model ensemble. Using four\nstate-of-the-art LLMs (Falcon, Gemini, GPT3.5, and Llama2), we show that\nKonTest generates 19.2% error inducing inputs (1917 errors from 9979 test\ninputs). It also reveals a 16.5% knowledge gap across all tested LLMs. A\nmitigation method informed by KonTest's test suite reduces LLM knowledge gap by\n32.48%. Our ablation study further shows that GPT3.5 is not suitable for\nknowledge-based consistency testing because it is only 60%-68% effective in\nknowledge construction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we systematically expose and measure the inconsistency and\nknowledge gaps of Large Language Models (LLMs). Specifically, we propose an\nautomated testing framework (called KonTest) which leverages a knowledge graph\nto construct test cases. KonTest probes and measures the inconsistencies in the\nLLM's knowledge of the world via a combination of semantically-equivalent\nqueries and test oracles (metamorphic or ontological oracle). KonTest further\nmitigates knowledge gaps via a weighted LLM model ensemble. Using four\nstate-of-the-art LLMs (Falcon, Gemini, GPT3.5, and Llama2), we show that\nKonTest generates 19.2% error inducing inputs (1917 errors from 9979 test\ninputs). It also reveals a 16.5% knowledge gap across all tested LLMs. A\nmitigation method informed by KonTest's test suite reduces LLM knowledge gap by\n32.48%. Our ablation study further shows that GPT3.5 is not suitable for\nknowledge-based consistency testing because it is only 60%-68% effective in\nknowledge construction."
                },
                "authors": [
                    {
                        "name": "Sai Sathiesh Rajan"
                    },
                    {
                        "name": "Ezekiel Soremekun"
                    },
                    {
                        "name": "Sudipta Chattopadhyay"
                    }
                ],
                "author_detail": {
                    "name": "Sudipta Chattopadhyay"
                },
                "author": "Sudipta Chattopadhyay",
                "arxiv_comment": "12 pages, 4 figures, 8 tables, Accepted at EMNLP 2024 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.12830v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.12830v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10671v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10671v1",
                "updated": "2025-08-14T14:11:26Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    14,
                    11,
                    26,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T14:11:26Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    14,
                    11,
                    26,
                    3,
                    226,
                    0
                ],
                "title": "AEGISS -- Atomic orbital and Entropy-based Guided Inference for Space\n  Selection -- A novel semi-automated active space selection workflow for\n  quantum chemistry and quantum computing applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AEGISS -- Atomic orbital and Entropy-based Guided Inference for Space\n  Selection -- A novel semi-automated active space selection workflow for\n  quantum chemistry and quantum computing applications"
                },
                "summary": "The selection of a balanced active space is a critical step in\nmulti-reference quantum chemistry calculations, particularly for systems with\nstrong electron correlation. Likewise, active space selection is a key to\nunlock the potential of contemporary quantum computing in quantum chemistry.\nAlbeit recent progress, there remains a lack of a unified, robust, and fully\nautomated framework for active space selection that performs reliably across a\nwide range of molecular systems.\n  In this work, we present a novel approach inspired by both the AVAS (Atomic\nValence Active Space) and AutoCAS methods. Our method unifies orbital entropy\nanalysis with atomic orbital projections to guide the construction of\nchemically and physically meaningful active spaces. This integrated scheme\nenables a more consistent and flexible selection of active orbitals while\nretaining automation and scalability. We validate our approach on a set of\nmolecular systems relevant to photodynamic therapy, in particular a set of\nRu(II)-complexes, selected to span increasing levels of electron correlation\nand structural complexity. These molecules serve as challenging test cases due\nto the presence of strong static correlation and the need for highly accurate\nelectronic structure descriptions. Our results demonstrate that the method can\nreliably identify compact, chemically intuitive active spaces that capture the\nessential physics, making it suitable for both classical and quantum\ncomputational frameworks.\n  Furthermore, we have developed this approach in a package that is intuitive\nto use for users and can be interfaced with both standard quantum chemistry and\nquantum computing applications, making it accessible to a broad research\ncommunity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The selection of a balanced active space is a critical step in\nmulti-reference quantum chemistry calculations, particularly for systems with\nstrong electron correlation. Likewise, active space selection is a key to\nunlock the potential of contemporary quantum computing in quantum chemistry.\nAlbeit recent progress, there remains a lack of a unified, robust, and fully\nautomated framework for active space selection that performs reliably across a\nwide range of molecular systems.\n  In this work, we present a novel approach inspired by both the AVAS (Atomic\nValence Active Space) and AutoCAS methods. Our method unifies orbital entropy\nanalysis with atomic orbital projections to guide the construction of\nchemically and physically meaningful active spaces. This integrated scheme\nenables a more consistent and flexible selection of active orbitals while\nretaining automation and scalability. We validate our approach on a set of\nmolecular systems relevant to photodynamic therapy, in particular a set of\nRu(II)-complexes, selected to span increasing levels of electron correlation\nand structural complexity. These molecules serve as challenging test cases due\nto the presence of strong static correlation and the need for highly accurate\nelectronic structure descriptions. Our results demonstrate that the method can\nreliably identify compact, chemically intuitive active spaces that capture the\nessential physics, making it suitable for both classical and quantum\ncomputational frameworks.\n  Furthermore, we have developed this approach in a package that is intuitive\nto use for users and can be interfaced with both standard quantum chemistry and\nquantum computing applications, making it accessible to a broad research\ncommunity."
                },
                "authors": [
                    {
                        "name": "Fabio Tarocco"
                    },
                    {
                        "name": "Pi A. B. Haase"
                    },
                    {
                        "name": "Fabijan Pavoevi"
                    },
                    {
                        "name": "Vijay Krishna"
                    },
                    {
                        "name": "Leonardo Guidoni"
                    },
                    {
                        "name": "Stefan Knecht"
                    },
                    {
                        "name": "Martina Stella"
                    }
                ],
                "author_detail": {
                    "name": "Martina Stella"
                },
                "author": "Martina Stella",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10671v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10671v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.chem-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.chem-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00579v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00579v2",
                "updated": "2025-08-14T14:11:07Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    14,
                    11,
                    7,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-01T12:22:53Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    12,
                    22,
                    53,
                    4,
                    213,
                    0
                ],
                "title": "MMRAG-DocQA: A Multi-Modal Retrieval-Augmented Generation Method for\n  Document Question-Answering with Hierarchical Index and Multi-Granularity\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MMRAG-DocQA: A Multi-Modal Retrieval-Augmented Generation Method for\n  Document Question-Answering with Hierarchical Index and Multi-Granularity\n  Retrieval"
                },
                "summary": "The multi-modal long-context document question-answering task aims to locate\nand integrate multi-modal evidences (such as texts, tables, charts, images, and\nlayouts) distributed across multiple pages, for question understanding and\nanswer generation. The existing methods can be categorized into Large\nVision-Language Model (LVLM)-based and Retrieval-Augmented Generation\n(RAG)-based methods. However, the former were susceptible to hallucinations,\nwhile the latter struggled for inter-modal disconnection and cross-page\nfragmentation. To address these challenges, a novel multi-modal RAG model,\nnamed MMRAG-DocQA, was proposed, leveraging both textual and visual information\nacross long-range pages to facilitate accurate question answering. A\nhierarchical indexing method with the integration of flattened in-page chunks\nand topological cross-page chunks was designed to jointly establish in-page\nmulti-modal associations and long-distance cross-page dependencies. By means of\njoint similarity evaluation and large language model (LLM)-based re-ranking, a\nmulti-granularity semantic retrieval method, including the page-level parent\npage retrieval and document-level summary retrieval, was proposed to foster\nmulti-modal evidence connection and long-distance evidence integration and\nreasoning. Experimental results performed on public datasets, MMLongBench-Doc\nand LongDocURL, demonstrated the superiority of our MMRAG-DocQA method in\nunderstanding and answering modality-rich and multi-page documents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The multi-modal long-context document question-answering task aims to locate\nand integrate multi-modal evidences (such as texts, tables, charts, images, and\nlayouts) distributed across multiple pages, for question understanding and\nanswer generation. The existing methods can be categorized into Large\nVision-Language Model (LVLM)-based and Retrieval-Augmented Generation\n(RAG)-based methods. However, the former were susceptible to hallucinations,\nwhile the latter struggled for inter-modal disconnection and cross-page\nfragmentation. To address these challenges, a novel multi-modal RAG model,\nnamed MMRAG-DocQA, was proposed, leveraging both textual and visual information\nacross long-range pages to facilitate accurate question answering. A\nhierarchical indexing method with the integration of flattened in-page chunks\nand topological cross-page chunks was designed to jointly establish in-page\nmulti-modal associations and long-distance cross-page dependencies. By means of\njoint similarity evaluation and large language model (LLM)-based re-ranking, a\nmulti-granularity semantic retrieval method, including the page-level parent\npage retrieval and document-level summary retrieval, was proposed to foster\nmulti-modal evidence connection and long-distance evidence integration and\nreasoning. Experimental results performed on public datasets, MMLongBench-Doc\nand LongDocURL, demonstrated the superiority of our MMRAG-DocQA method in\nunderstanding and answering modality-rich and multi-page documents."
                },
                "authors": [
                    {
                        "name": "Ziyu Gong"
                    },
                    {
                        "name": "Yihua Huang"
                    },
                    {
                        "name": "Chengcheng Mai"
                    }
                ],
                "author_detail": {
                    "name": "Chengcheng Mai"
                },
                "author": "Chengcheng Mai",
                "arxiv_comment": "Comments: Removed the footnote in page 1",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00579v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00579v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10655v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10655v1",
                "updated": "2025-08-14T13:54:04Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    13,
                    54,
                    4,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T13:54:04Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    13,
                    54,
                    4,
                    3,
                    226,
                    0
                ],
                "title": "Serial Over Parallel: Learning Continual Unification for Multi-Modal\n  Visual Object Tracking and Benchmarking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serial Over Parallel: Learning Continual Unification for Multi-Modal\n  Visual Object Tracking and Benchmarking"
                },
                "summary": "Unifying multiple multi-modal visual object tracking (MMVOT) tasks draws\nincreasing attention due to the complementary nature of different modalities in\nbuilding robust tracking systems. Existing practices mix all data sensor types\nin a single training procedure, structuring a parallel paradigm from the\ndata-centric perspective and aiming for a global optimum on the joint\ndistribution of the involved tasks. However, the absence of a unified benchmark\nwhere all types of data coexist forces evaluations on separated benchmarks,\ncausing \\textit{inconsistency} between training and testing, thus leading to\nperformance \\textit{degradation}. To address these issues, this work advances\nin two aspects: \\ding{182} A unified benchmark, coined as UniBench300, is\nintroduced to bridge the inconsistency by incorporating multiple task data,\nreducing inference passes from three to one and cutting time consumption by\n27\\%. \\ding{183} The unification process is reformulated in a serial format,\nprogressively integrating new tasks. In this way, the performance degradation\ncan be specified as knowledge forgetting of previous tasks, which naturally\naligns with the philosophy of continual learning (CL), motivating further\nexploration of injecting CL into the unification process. Extensive experiments\nconducted on two baselines and four benchmarks demonstrate the significance of\nUniBench300 and the superiority of CL in supporting a stable unification\nprocess. Moreover, while conducting dedicated analyses, the performance\ndegradation is found to be negatively correlated with network capacity.\nAdditionally, modality discrepancies contribute to varying degradation levels\nacross tasks (RGBT > RGBD > RGBE in MMVOT), offering valuable insights for\nfuture multi-modal vision research. Source codes and the proposed benchmark is\navailable at \\textit{https://github.com/Zhangyong-Tang/UniBench300}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unifying multiple multi-modal visual object tracking (MMVOT) tasks draws\nincreasing attention due to the complementary nature of different modalities in\nbuilding robust tracking systems. Existing practices mix all data sensor types\nin a single training procedure, structuring a parallel paradigm from the\ndata-centric perspective and aiming for a global optimum on the joint\ndistribution of the involved tasks. However, the absence of a unified benchmark\nwhere all types of data coexist forces evaluations on separated benchmarks,\ncausing \\textit{inconsistency} between training and testing, thus leading to\nperformance \\textit{degradation}. To address these issues, this work advances\nin two aspects: \\ding{182} A unified benchmark, coined as UniBench300, is\nintroduced to bridge the inconsistency by incorporating multiple task data,\nreducing inference passes from three to one and cutting time consumption by\n27\\%. \\ding{183} The unification process is reformulated in a serial format,\nprogressively integrating new tasks. In this way, the performance degradation\ncan be specified as knowledge forgetting of previous tasks, which naturally\naligns with the philosophy of continual learning (CL), motivating further\nexploration of injecting CL into the unification process. Extensive experiments\nconducted on two baselines and four benchmarks demonstrate the significance of\nUniBench300 and the superiority of CL in supporting a stable unification\nprocess. Moreover, while conducting dedicated analyses, the performance\ndegradation is found to be negatively correlated with network capacity.\nAdditionally, modality discrepancies contribute to varying degradation levels\nacross tasks (RGBT > RGBD > RGBE in MMVOT), offering valuable insights for\nfuture multi-modal vision research. Source codes and the proposed benchmark is\navailable at \\textit{https://github.com/Zhangyong-Tang/UniBench300}."
                },
                "authors": [
                    {
                        "name": "Zhangyong Tang"
                    },
                    {
                        "name": "Tianyang Xu"
                    },
                    {
                        "name": "Xuefeng Zhu"
                    },
                    {
                        "name": "Chunyang Cheng"
                    },
                    {
                        "name": "Tao Zhou"
                    },
                    {
                        "name": "Xiaojun Wu"
                    },
                    {
                        "name": "Josef Kittler"
                    }
                ],
                "author_detail": {
                    "name": "Josef Kittler"
                },
                "author": "Josef Kittler",
                "arxiv_comment": "ACMMM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10655v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10655v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10645v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10645v1",
                "updated": "2025-08-14T13:41:59Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    13,
                    41,
                    59,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T13:41:59Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    13,
                    41,
                    59,
                    3,
                    226,
                    0
                ],
                "title": "SemPT: Semantic Prompt Tuning for Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SemPT: Semantic Prompt Tuning for Vision-Language Models"
                },
                "summary": "Visual transfer learning for unseen categories presents an active research\ntopic yet a challenging task, due to the inherent conflict between preserving\ncategory-specific representations and acquiring transferable knowledge.\nVision-Language Models (VLMs) pre-trained on large amounts of image-text pairs\noffer a promising solution. However, existing prompt tuning methods rely on\nsparse category labels or disparate LLM-generated descriptions, which fragment\nknowledge representation and hinder transferability. To address this\nlimitation, we introduce Semantic Prompt Tuning (SemPT), a novel framework that\ntackles the generalization challenge by leveraging shared attribute-level\nknowledge across categories. Specifically, SemPT adopts a two-step prompting\nstrategy to guide LLM in extracting shared visual attributes and generating\nattribute-level descriptions, capturing transferable semantic cues beyond\nlabels while ensuring coherent structure. Then, visually guided weighting is\napplied to the embeddings of attribute-level descriptions to reduce noise from\nirrelevant attributes and enhance the text embeddings. Additionally, image\nembeddings are jointly aligned with both label and attribute-enhanced text\nembeddings, balancing discrimination for seen categories and transferability to\nunseen ones. Considering the availability of category exposure, our inference\ndynamically selects between standard label embeddings for seen categories and\nattribute-enhanced embeddings for unseen ones to ensure effective adaptation.\nExtensive experiments on 15 benchmark datasets demonstrate that SemPT achieves\nstate-of-the-art performance across various settings, including base-to-novel\ngeneralization, cross-dataset transfer, cross-domain transfer, and few-shot\nlearning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual transfer learning for unseen categories presents an active research\ntopic yet a challenging task, due to the inherent conflict between preserving\ncategory-specific representations and acquiring transferable knowledge.\nVision-Language Models (VLMs) pre-trained on large amounts of image-text pairs\noffer a promising solution. However, existing prompt tuning methods rely on\nsparse category labels or disparate LLM-generated descriptions, which fragment\nknowledge representation and hinder transferability. To address this\nlimitation, we introduce Semantic Prompt Tuning (SemPT), a novel framework that\ntackles the generalization challenge by leveraging shared attribute-level\nknowledge across categories. Specifically, SemPT adopts a two-step prompting\nstrategy to guide LLM in extracting shared visual attributes and generating\nattribute-level descriptions, capturing transferable semantic cues beyond\nlabels while ensuring coherent structure. Then, visually guided weighting is\napplied to the embeddings of attribute-level descriptions to reduce noise from\nirrelevant attributes and enhance the text embeddings. Additionally, image\nembeddings are jointly aligned with both label and attribute-enhanced text\nembeddings, balancing discrimination for seen categories and transferability to\nunseen ones. Considering the availability of category exposure, our inference\ndynamically selects between standard label embeddings for seen categories and\nattribute-enhanced embeddings for unseen ones to ensure effective adaptation.\nExtensive experiments on 15 benchmark datasets demonstrate that SemPT achieves\nstate-of-the-art performance across various settings, including base-to-novel\ngeneralization, cross-dataset transfer, cross-domain transfer, and few-shot\nlearning."
                },
                "authors": [
                    {
                        "name": "Xiao Shi"
                    },
                    {
                        "name": "Yangjun Ou"
                    },
                    {
                        "name": "Zhenzhong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhenzhong Chen"
                },
                "author": "Zhenzhong Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10645v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10645v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09932v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09932v2",
                "updated": "2025-08-14T13:25:18Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    13,
                    25,
                    18,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-13T16:33:02Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    16,
                    33,
                    2,
                    2,
                    225,
                    0
                ],
                "title": "Mathematical Computation and Reasoning Errors by Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mathematical Computation and Reasoning Errors by Large Language Models"
                },
                "summary": "Large Language Models (LLMs) are increasingly utilized in AI-driven\neducational instruction and assessment, particularly within mathematics\neducation. The capability of LLMs to generate accurate answers and detailed\nsolutions for math problem-solving tasks is foundational for ensuring reliable\nand precise feedback and assessment in math education practices. Our study\nfocuses on evaluating the accuracy of four LLMs (OpenAI GPT-4o and o1,\nDeepSeek-V3 and DeepSeek-R1) solving three categories of math tasks, including\narithmetic, algebra, and number theory, and identifies step-level reasoning\nerrors within their solutions. Instead of relying on standard benchmarks, we\nintentionally build math tasks (via item models) that are challenging for LLMs\nand prone to errors. The accuracy of final answers and the presence of errors\nin individual solution steps were systematically analyzed and coded. Both\nsingle-agent and dual-agent configurations were tested. It is observed that the\nreasoning-enhanced OpenAI o1 model consistently achieved higher or nearly\nperfect accuracy across all three math task categories. Analysis of errors\nrevealed that procedural slips were the most frequent and significantly\nimpacted overall performance, while conceptual misunderstandings were less\nfrequent. Deploying dual-agent configurations substantially improved overall\nperformance. These findings offer actionable insights into enhancing LLM\nperformance and underscore effective strategies for integrating LLMs into\nmathematics education, thereby advancing AI-driven instructional practices and\nassessment precision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly utilized in AI-driven\neducational instruction and assessment, particularly within mathematics\neducation. The capability of LLMs to generate accurate answers and detailed\nsolutions for math problem-solving tasks is foundational for ensuring reliable\nand precise feedback and assessment in math education practices. Our study\nfocuses on evaluating the accuracy of four LLMs (OpenAI GPT-4o and o1,\nDeepSeek-V3 and DeepSeek-R1) solving three categories of math tasks, including\narithmetic, algebra, and number theory, and identifies step-level reasoning\nerrors within their solutions. Instead of relying on standard benchmarks, we\nintentionally build math tasks (via item models) that are challenging for LLMs\nand prone to errors. The accuracy of final answers and the presence of errors\nin individual solution steps were systematically analyzed and coded. Both\nsingle-agent and dual-agent configurations were tested. It is observed that the\nreasoning-enhanced OpenAI o1 model consistently achieved higher or nearly\nperfect accuracy across all three math task categories. Analysis of errors\nrevealed that procedural slips were the most frequent and significantly\nimpacted overall performance, while conceptual misunderstandings were less\nfrequent. Deploying dual-agent configurations substantially improved overall\nperformance. These findings offer actionable insights into enhancing LLM\nperformance and underscore effective strategies for integrating LLMs into\nmathematics education, thereby advancing AI-driven instructional practices and\nassessment precision."
                },
                "authors": [
                    {
                        "name": "Liang Zhang"
                    },
                    {
                        "name": "Edith Aurora Graf"
                    }
                ],
                "author_detail": {
                    "name": "Edith Aurora Graf"
                },
                "author": "Edith Aurora Graf",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09932v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09932v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04756v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04756v2",
                "updated": "2025-08-14T13:21:47Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    13,
                    21,
                    47,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-06T14:14:13Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    14,
                    14,
                    13,
                    2,
                    218,
                    0
                ],
                "title": "Comment on \"Energy-speed relationship of quantum particles challenges\n  Bohmian mechanics\"",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comment on \"Energy-speed relationship of quantum particles challenges\n  Bohmian mechanics\""
                },
                "summary": "In their recent paper [Nature 643, 67 (2025)], Sharaglazova et al. report an\noptical microcavity experiment yielding an \"energy-speed relationship\" for\nquantum particles in evanescent states, which they infer from the observed\npopulation transfer between two coupled waveguides. The authors argue that\ntheir findings challenge the validity of Bohmian particle dynamics because,\naccording to the Bohmian guiding equation, the velocities in the classically\nforbidden region would be zero. In this note, we explain why this claim is\nfalse and the experimental findings are in perfect agreement with Bohmian\nmechanics. We also clarify why the operationally defined speeds reported in the\npaper are unrelated to particle velocities in the sense described by Bohmian\nmechanics. In contrast to other recent replies, our analysis relies solely on\nthe standard Bohmian guidance equation for single particles.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In their recent paper [Nature 643, 67 (2025)], Sharaglazova et al. report an\noptical microcavity experiment yielding an \"energy-speed relationship\" for\nquantum particles in evanescent states, which they infer from the observed\npopulation transfer between two coupled waveguides. The authors argue that\ntheir findings challenge the validity of Bohmian particle dynamics because,\naccording to the Bohmian guiding equation, the velocities in the classically\nforbidden region would be zero. In this note, we explain why this claim is\nfalse and the experimental findings are in perfect agreement with Bohmian\nmechanics. We also clarify why the operationally defined speeds reported in the\npaper are unrelated to particle velocities in the sense described by Bohmian\nmechanics. In contrast to other recent replies, our analysis relies solely on\nthe standard Bohmian guidance equation for single particles."
                },
                "authors": [
                    {
                        "name": "Aurlien Drezet"
                    },
                    {
                        "name": "Dustin Lazarovici"
                    },
                    {
                        "name": "Bernard Michael Nabet"
                    }
                ],
                "author_detail": {
                    "name": "Bernard Michael Nabet"
                },
                "author": "Bernard Michael Nabet",
                "arxiv_comment": "Comment on DOI: 10.1038/s41586-025-09099-4. 5 pages, 1 figure. V2:\n  Sign error corrected and minor edits",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04756v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04756v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10615v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10615v1",
                "updated": "2025-08-14T13:12:29Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    13,
                    12,
                    29,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T13:12:29Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    13,
                    12,
                    29,
                    3,
                    226,
                    0
                ],
                "title": "FuXi-: Towards a Lightweight and Fast Large-Scale Generative\n  Recommendation Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FuXi-: Towards a Lightweight and Fast Large-Scale Generative\n  Recommendation Model"
                },
                "summary": "Scaling laws for autoregressive generative recommenders reveal potential for\nlarger, more versatile systems but mean greater latency and training costs. To\naccelerate training and inference, we investigated the recent generative\nrecommendation models HSTU and FuXi-$\\alpha$, identifying two efficiency\nbottlenecks: the indexing operations in relative temporal attention bias and\nthe computation of the query-key attention map. Additionally, we observed that\nrelative attention bias in self-attention mechanisms can also serve as\nattention maps. Previous works like Synthesizer have shown that alternative\nforms of attention maps can achieve similar performance, naturally raising the\nquestion of whether some attention maps are redundant. Through empirical\nexperiments, we discovered that using the query-key attention map might degrade\nthe model's performance in recommendation tasks. To address these bottlenecks,\nwe propose a new framework applicable to Transformer-like recommendation\nmodels. On one hand, we introduce Functional Relative Attention Bias, which\navoids the time-consuming operations of the original relative attention bias,\nthereby accelerating the process. On the other hand, we remove the query-key\nattention map from the original self-attention layer and design a new\nAttention-Free Token Mixer module. Furthermore, by applying this framework to\nFuXi-$\\alpha$, we introduce a new model, FuXi-$\\beta$. Experiments across\nmultiple datasets demonstrate that FuXi-$\\beta$ outperforms previous\nstate-of-the-art models and achieves significant acceleration compared to\nFuXi-$\\alpha$, while also adhering to the scaling law. Notably, FuXi-$\\beta$\nshows an improvement of 27% to 47% in the NDCG@10 metric on large-scale\nindustrial datasets compared to FuXi-$\\alpha$. Our code is available in a\npublic repository: https://github.com/USTC-StarTeam/FuXi-beta",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling laws for autoregressive generative recommenders reveal potential for\nlarger, more versatile systems but mean greater latency and training costs. To\naccelerate training and inference, we investigated the recent generative\nrecommendation models HSTU and FuXi-$\\alpha$, identifying two efficiency\nbottlenecks: the indexing operations in relative temporal attention bias and\nthe computation of the query-key attention map. Additionally, we observed that\nrelative attention bias in self-attention mechanisms can also serve as\nattention maps. Previous works like Synthesizer have shown that alternative\nforms of attention maps can achieve similar performance, naturally raising the\nquestion of whether some attention maps are redundant. Through empirical\nexperiments, we discovered that using the query-key attention map might degrade\nthe model's performance in recommendation tasks. To address these bottlenecks,\nwe propose a new framework applicable to Transformer-like recommendation\nmodels. On one hand, we introduce Functional Relative Attention Bias, which\navoids the time-consuming operations of the original relative attention bias,\nthereby accelerating the process. On the other hand, we remove the query-key\nattention map from the original self-attention layer and design a new\nAttention-Free Token Mixer module. Furthermore, by applying this framework to\nFuXi-$\\alpha$, we introduce a new model, FuXi-$\\beta$. Experiments across\nmultiple datasets demonstrate that FuXi-$\\beta$ outperforms previous\nstate-of-the-art models and achieves significant acceleration compared to\nFuXi-$\\alpha$, while also adhering to the scaling law. Notably, FuXi-$\\beta$\nshows an improvement of 27% to 47% in the NDCG@10 metric on large-scale\nindustrial datasets compared to FuXi-$\\alpha$. Our code is available in a\npublic repository: https://github.com/USTC-StarTeam/FuXi-beta"
                },
                "authors": [
                    {
                        "name": "Yufei Ye"
                    },
                    {
                        "name": "Wei Guo"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Hong Zhu"
                    },
                    {
                        "name": "Yuyang Ye"
                    },
                    {
                        "name": "Yong Liu"
                    },
                    {
                        "name": "Huifeng Guo"
                    },
                    {
                        "name": "Ruiming Tang"
                    },
                    {
                        "name": "Defu Lian"
                    },
                    {
                        "name": "Enhong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Enhong Chen"
                },
                "author": "Enhong Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10615v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10615v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05929v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05929v2",
                "updated": "2025-08-14T13:02:35Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    13,
                    2,
                    35,
                    3,
                    226,
                    0
                ],
                "published": "2025-06-06T09:55:36Z",
                "published_parsed": [
                    2025,
                    6,
                    6,
                    9,
                    55,
                    36,
                    4,
                    157,
                    0
                ],
                "title": "Exploration of features in the black hole mass spectrum inspired by\n  non-parametric analyses of gravitational wave observations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploration of features in the black hole mass spectrum inspired by\n  non-parametric analyses of gravitational wave observations"
                },
                "summary": "Current gravitational-wave data reveal structures in the mass function of\nbinary compact objects. Properly modelling and deciphering such structures is\nthe ultimate goal of gravitational-wave population analysis: in this context,\nnon-parametric models are a powerful tool to infer the distribution of black\nholes from gravitational waves without committing to any specific functional\nform. Here, we aim to quantitatively corroborate the findings of non-parametric\nmethods with parametrised models incorporating the features found in such\nanalyses. We propose two modifications of the currently favoured PowerLaw+Peak\nmodel, inspired by non-parametric studies, and use them to analyse the third\nGravitational Wave Transient Catalogue. Our analysis marginally supports the\nexistence of two distinct, differently redshift-evolving subpopulations in the\nblack hole primary mass function, and suggests that, to date, we are still\nunable to robustly assess the shape of the mass ratio distribution for\nsymmetric ($q>0.7$) binaries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current gravitational-wave data reveal structures in the mass function of\nbinary compact objects. Properly modelling and deciphering such structures is\nthe ultimate goal of gravitational-wave population analysis: in this context,\nnon-parametric models are a powerful tool to infer the distribution of black\nholes from gravitational waves without committing to any specific functional\nform. Here, we aim to quantitatively corroborate the findings of non-parametric\nmethods with parametrised models incorporating the features found in such\nanalyses. We propose two modifications of the currently favoured PowerLaw+Peak\nmodel, inspired by non-parametric studies, and use them to analyse the third\nGravitational Wave Transient Catalogue. Our analysis marginally supports the\nexistence of two distinct, differently redshift-evolving subpopulations in the\nblack hole primary mass function, and suggests that, to date, we are still\nunable to robustly assess the shape of the mass ratio distribution for\nsymmetric ($q>0.7$) binaries."
                },
                "authors": [
                    {
                        "name": "Stefano Rinaldi"
                    },
                    {
                        "name": "Yajie Liang"
                    },
                    {
                        "name": "Gabriele Demasi"
                    },
                    {
                        "name": "Michela Mapelli"
                    },
                    {
                        "name": "Walter Del Pozzo"
                    }
                ],
                "author_detail": {
                    "name": "Walter Del Pozzo"
                },
                "author": "Walter Del Pozzo",
                "arxiv_comment": "8 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05929v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05929v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17493v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17493v3",
                "updated": "2025-08-14T13:00:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    13,
                    0,
                    9,
                    3,
                    226,
                    0
                ],
                "published": "2025-04-24T12:34:43Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    12,
                    34,
                    43,
                    3,
                    114,
                    0
                ],
                "title": "Goal-Oriented Time-Series Forecasting: Foundation Framework Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Goal-Oriented Time-Series Forecasting: Foundation Framework Design"
                },
                "summary": "Conventional time-series forecasting methods typically aim to minimize\noverall prediction error, without accounting for the varying importance of\ndifferent forecast ranges in downstream applications. We propose a training\nmethodology that enables forecasting models to adapt their focus to\napplication-specific regions of interest at inference time, without retraining.\nThe approach partitions the prediction space into fine-grained segments during\ntraining, which are dynamically reweighted and aggregated to emphasize the\ntarget range specified by the application. Unlike prior methods that predefine\nthese ranges, our framework supports flexible, on-demand adjustments.\nExperiments on standard benchmarks and a newly collected wireless communication\ndataset demonstrate that our method not only improves forecast accuracy within\nregions of interest but also yields measurable gains in downstream task\nperformance. These results highlight the potential for closer integration\nbetween predictive modeling and decision-making in real-world systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conventional time-series forecasting methods typically aim to minimize\noverall prediction error, without accounting for the varying importance of\ndifferent forecast ranges in downstream applications. We propose a training\nmethodology that enables forecasting models to adapt their focus to\napplication-specific regions of interest at inference time, without retraining.\nThe approach partitions the prediction space into fine-grained segments during\ntraining, which are dynamically reweighted and aggregated to emphasize the\ntarget range specified by the application. Unlike prior methods that predefine\nthese ranges, our framework supports flexible, on-demand adjustments.\nExperiments on standard benchmarks and a newly collected wireless communication\ndataset demonstrate that our method not only improves forecast accuracy within\nregions of interest but also yields measurable gains in downstream task\nperformance. These results highlight the potential for closer integration\nbetween predictive modeling and decision-making in real-world systems."
                },
                "authors": [
                    {
                        "name": "Luca-Andrei Fechete"
                    },
                    {
                        "name": "Mohamed Sana"
                    },
                    {
                        "name": "Fadhel Ayed"
                    },
                    {
                        "name": "Nicola Piovesan"
                    },
                    {
                        "name": "Wenjie Li"
                    },
                    {
                        "name": "Antonio De Domenico"
                    },
                    {
                        "name": "Tareq Si Salem"
                    }
                ],
                "author_detail": {
                    "name": "Tareq Si Salem"
                },
                "author": "Tareq Si Salem",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17493v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17493v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00372v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00372v3",
                "updated": "2025-08-14T12:58:31Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    12,
                    58,
                    31,
                    3,
                    226,
                    0
                ],
                "published": "2025-02-01T09:19:08Z",
                "published_parsed": [
                    2025,
                    2,
                    1,
                    9,
                    19,
                    8,
                    5,
                    32,
                    0
                ],
                "title": "NAVER: A Neuro-Symbolic Compositional Automaton for Visual Grounding\n  with Explicit Logic Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NAVER: A Neuro-Symbolic Compositional Automaton for Visual Grounding\n  with Explicit Logic Reasoning"
                },
                "summary": "Visual Grounding (VG) tasks, such as referring expression detection and\nsegmentation tasks are important for linking visual entities to context,\nespecially in complex reasoning tasks that require detailed query\ninterpretation. This paper explores VG beyond basic perception, highlighting\nchallenges for methods that require reasoning like human cognition. Recent\nadvances in large language methods (LLMs) and Vision-Language methods (VLMs)\nhave improved abilities for visual comprehension, contextual understanding, and\nreasoning. These methods are mainly split into end-to-end and compositional\nmethods, with the latter offering more flexibility. Compositional approaches\nthat integrate LLMs and foundation models show promising performance but still\nstruggle with complex reasoning with language-based logical representations. To\naddress these limitations, we propose NAVER, a compositional visual grounding\nmethod that integrates explicit probabilistic logic reasoning within a\nfinite-state automaton, equipped with a self-correcting mechanism. This design\nimproves robustness and interpretability in inference through explicit logic\nreasoning. Our results show that NAVER achieves SoTA performance comparing to\nrecent end-to-end and compositional baselines. The code is available at\nhttps://github.com/ControlNet/NAVER .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual Grounding (VG) tasks, such as referring expression detection and\nsegmentation tasks are important for linking visual entities to context,\nespecially in complex reasoning tasks that require detailed query\ninterpretation. This paper explores VG beyond basic perception, highlighting\nchallenges for methods that require reasoning like human cognition. Recent\nadvances in large language methods (LLMs) and Vision-Language methods (VLMs)\nhave improved abilities for visual comprehension, contextual understanding, and\nreasoning. These methods are mainly split into end-to-end and compositional\nmethods, with the latter offering more flexibility. Compositional approaches\nthat integrate LLMs and foundation models show promising performance but still\nstruggle with complex reasoning with language-based logical representations. To\naddress these limitations, we propose NAVER, a compositional visual grounding\nmethod that integrates explicit probabilistic logic reasoning within a\nfinite-state automaton, equipped with a self-correcting mechanism. This design\nimproves robustness and interpretability in inference through explicit logic\nreasoning. Our results show that NAVER achieves SoTA performance comparing to\nrecent end-to-end and compositional baselines. The code is available at\nhttps://github.com/ControlNet/NAVER ."
                },
                "authors": [
                    {
                        "name": "Zhixi Cai"
                    },
                    {
                        "name": "Fucai Ke"
                    },
                    {
                        "name": "Simindokht Jahangard"
                    },
                    {
                        "name": "Maria Garcia de la Banda"
                    },
                    {
                        "name": "Reza Haffari"
                    },
                    {
                        "name": "Peter J. Stuckey"
                    },
                    {
                        "name": "Hamid Rezatofighi"
                    }
                ],
                "author_detail": {
                    "name": "Hamid Rezatofighi"
                },
                "author": "Hamid Rezatofighi",
                "arxiv_comment": "ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00372v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00372v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05294v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05294v2",
                "updated": "2025-08-14T12:55:31Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    12,
                    55,
                    31,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-07T11:48:03Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    11,
                    48,
                    3,
                    3,
                    219,
                    0
                ],
                "title": "Towards Embodied Agentic AI: Review and Classification of LLM- and\n  VLM-Driven Robot Autonomy and Interaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Embodied Agentic AI: Review and Classification of LLM- and\n  VLM-Driven Robot Autonomy and Interaction"
                },
                "summary": "Foundation models, including large language models (LLMs) and vision-language\nmodels (VLMs), have recently enabled novel approaches to robot autonomy and\nhuman-robot interfaces. In parallel, vision-language-action models (VLAs) or\nlarge behavior models (LBMs) are increasing the dexterity and capabilities of\nrobotic systems. This survey paper focuses on those works advancing towards\nagentic applications and architectures. This includes initial efforts exploring\nGPT-style interfaces to tooling, as well as more complex system where AI agents\nare coordinators, planners, perception actors, or generalist interfaces. Such\nagentic architectures allow robots to reason over natural language\ninstructions, invoke APIs, plan task sequences, or assist in operations and\ndiagnostics. In addition to peer-reviewed research, due to the fast-evolving\nnature of the field, we highlight and include community-driven projects, ROS\npackages, and industrial frameworks that show emerging trends. We propose a\ntaxonomy for classifying model integration approaches and present a comparative\nanalysis of the role that agents play in different solutions in today's\nliterature.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foundation models, including large language models (LLMs) and vision-language\nmodels (VLMs), have recently enabled novel approaches to robot autonomy and\nhuman-robot interfaces. In parallel, vision-language-action models (VLAs) or\nlarge behavior models (LBMs) are increasing the dexterity and capabilities of\nrobotic systems. This survey paper focuses on those works advancing towards\nagentic applications and architectures. This includes initial efforts exploring\nGPT-style interfaces to tooling, as well as more complex system where AI agents\nare coordinators, planners, perception actors, or generalist interfaces. Such\nagentic architectures allow robots to reason over natural language\ninstructions, invoke APIs, plan task sequences, or assist in operations and\ndiagnostics. In addition to peer-reviewed research, due to the fast-evolving\nnature of the field, we highlight and include community-driven projects, ROS\npackages, and industrial frameworks that show emerging trends. We propose a\ntaxonomy for classifying model integration approaches and present a comparative\nanalysis of the role that agents play in different solutions in today's\nliterature."
                },
                "authors": [
                    {
                        "name": "Sahar Salimpour"
                    },
                    {
                        "name": "Lei Fu"
                    },
                    {
                        "name": "Farhad Keramat"
                    },
                    {
                        "name": "Leonardo Militano"
                    },
                    {
                        "name": "Giovanni Toffetti"
                    },
                    {
                        "name": "Harry Edelman"
                    },
                    {
                        "name": "Jorge Pea Queralta"
                    }
                ],
                "author_detail": {
                    "name": "Jorge Pea Queralta"
                },
                "author": "Jorge Pea Queralta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05294v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05294v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10603v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10603v2",
                "updated": "2025-08-15T16:07:03Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    16,
                    7,
                    3,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-14T12:44:39Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    12,
                    44,
                    39,
                    3,
                    226,
                    0
                ],
                "title": "Why Report Failed Interactions With Robots?! Towards Vignette-based\n  Interaction Quality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Why Report Failed Interactions With Robots?! Towards Vignette-based\n  Interaction Quality"
                },
                "summary": "Although the quality of human-robot interactions has improved with the advent\nof LLMs, there are still various factors that cause systems to be sub-optimal\nwhen compared to human-human interactions. The nature and criticality of\nfailures are often dependent on the context of the interaction and so cannot be\ngeneralized across the wide range of scenarios and experiments which have been\nimplemented in HRI research. In this work we propose the use of a technique\noverlooked in the field of HRI, ethnographic vignettes, to clearly highlight\nthese failures, particularly those that are rarely documented. We describe the\nmethodology behind the process of writing vignettes and create our own based on\nour personal experiences with failures in HRI systems. We emphasize the\nstrength of vignettes as the ability to communicate failures from a\nmulti-disciplinary perspective, promote transparency about the capabilities of\nrobots, and document unexpected behaviours which would otherwise be omitted\nfrom research reports. We encourage the use of vignettes to augment existing\ninteraction evaluation methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although the quality of human-robot interactions has improved with the advent\nof LLMs, there are still various factors that cause systems to be sub-optimal\nwhen compared to human-human interactions. The nature and criticality of\nfailures are often dependent on the context of the interaction and so cannot be\ngeneralized across the wide range of scenarios and experiments which have been\nimplemented in HRI research. In this work we propose the use of a technique\noverlooked in the field of HRI, ethnographic vignettes, to clearly highlight\nthese failures, particularly those that are rarely documented. We describe the\nmethodology behind the process of writing vignettes and create our own based on\nour personal experiences with failures in HRI systems. We emphasize the\nstrength of vignettes as the ability to communicate failures from a\nmulti-disciplinary perspective, promote transparency about the capabilities of\nrobots, and document unexpected behaviours which would otherwise be omitted\nfrom research reports. We encourage the use of vignettes to augment existing\ninteraction evaluation methods."
                },
                "authors": [
                    {
                        "name": "Agnes Axelsson"
                    },
                    {
                        "name": "Merle Reimann"
                    },
                    {
                        "name": "Ronald Cumbal"
                    },
                    {
                        "name": "Hannah Pelikan"
                    },
                    {
                        "name": "Divesh Lala"
                    }
                ],
                "author_detail": {
                    "name": "Divesh Lala"
                },
                "author": "Divesh Lala",
                "arxiv_comment": "Accepted at the workshop on Real-World HRI in Public and Private\n  Spaces: Successes, Failures, and Lessons Learned (PubRob-Fails), held at the\n  IEEE RO-MAN Conference, 2025. 6 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10603v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10603v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10599v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10599v1",
                "updated": "2025-08-14T12:40:19Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    12,
                    40,
                    19,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T12:40:19Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    12,
                    40,
                    19,
                    3,
                    226,
                    0
                ],
                "title": "MSRS: Adaptive Multi-Subspace Representation Steering for Attribute\n  Alignment in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MSRS: Adaptive Multi-Subspace Representation Steering for Attribute\n  Alignment in Large Language Models"
                },
                "summary": "Activation steering offers a promising approach to controlling the behavior\nof Large Language Models by directly manipulating their internal activations.\nHowever, most existing methods struggle to jointly steer multiple attributes,\noften resulting in interference and undesirable trade-offs. To address this\nchallenge, we propose Multi-Subspace Representation Steering (MSRS), a novel\nframework for effective multi-attribute steering via subspace representation\nfine-tuning. MSRS reduces inter-attribute interference by allocating orthogonal\nsubspaces to each attribute, isolating their influence within the model's\nrepresentation space. MSRS also incorporates a hybrid subspace composition\nstrategy: it combines attribute-specific subspaces for unique steering\ndirections with a shared subspace for common steering directions. A dynamic\nweighting function learns to efficiently integrate these components for precise\ncontrol. During inference, MSRS introduces a token-level steering mechanism\nthat dynamically identifies and intervenes on the most semantically relevant\ntokens, enabling fine-grained behavioral modulation. Experimental results show\nthat MSRS significantly reduces attribute conflicts, surpasses existing methods\nacross a range of attributes, and generalizes effectively to diverse downstream\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Activation steering offers a promising approach to controlling the behavior\nof Large Language Models by directly manipulating their internal activations.\nHowever, most existing methods struggle to jointly steer multiple attributes,\noften resulting in interference and undesirable trade-offs. To address this\nchallenge, we propose Multi-Subspace Representation Steering (MSRS), a novel\nframework for effective multi-attribute steering via subspace representation\nfine-tuning. MSRS reduces inter-attribute interference by allocating orthogonal\nsubspaces to each attribute, isolating their influence within the model's\nrepresentation space. MSRS also incorporates a hybrid subspace composition\nstrategy: it combines attribute-specific subspaces for unique steering\ndirections with a shared subspace for common steering directions. A dynamic\nweighting function learns to efficiently integrate these components for precise\ncontrol. During inference, MSRS introduces a token-level steering mechanism\nthat dynamically identifies and intervenes on the most semantically relevant\ntokens, enabling fine-grained behavioral modulation. Experimental results show\nthat MSRS significantly reduces attribute conflicts, surpasses existing methods\nacross a range of attributes, and generalizes effectively to diverse downstream\ntasks."
                },
                "authors": [
                    {
                        "name": "Xinyan Jiang"
                    },
                    {
                        "name": "Lin Zhang"
                    },
                    {
                        "name": "Jiayi Zhang"
                    },
                    {
                        "name": "Qingsong Yang"
                    },
                    {
                        "name": "Guimin Hu"
                    },
                    {
                        "name": "Di Wang"
                    },
                    {
                        "name": "Lijie Hu"
                    }
                ],
                "author_detail": {
                    "name": "Lijie Hu"
                },
                "author": "Lijie Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10599v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10599v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.05262v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.05262v3",
                "updated": "2025-08-14T12:30:25Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    12,
                    30,
                    25,
                    3,
                    226,
                    0
                ],
                "published": "2024-03-08T12:35:07Z",
                "published_parsed": [
                    2024,
                    3,
                    8,
                    12,
                    35,
                    7,
                    4,
                    68,
                    0
                ],
                "title": "Debiasing Multimodal Large Language Models via Penalization of Language\n  Priors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Debiasing Multimodal Large Language Models via Penalization of Language\n  Priors"
                },
                "summary": "In the realms of computer vision and natural language processing, Multimodal\nLarge Language Models (MLLMs) have become indispensable tools, proficient in\ngenerating textual responses based on visual inputs. Despite their\nadvancements, our investigation reveals a noteworthy bias: the generated\ncontent is often driven more by the inherent priors of the underlying Large\nLanguage Models (LLMs) than by the input image. Empirical experiments\nunderscore the persistence of this bias, as MLLMs often provide confident\nanswers even in the absence of relevant images or given incongruent visual\ninputs. To rectify these biases and redirect the model's focus toward visual\ninformation, we propose two simple, training-free strategies. First, for tasks\nsuch as classification or multi-choice question answering, we introduce a\n\"Post-Hoc Debias\" method using an affine calibration step to adjust the output\ndistribution. This approach ensures uniform answer scores when the image is\nabsent, acting as an effective regularization technique to alleviate the\ninfluence of LLM priors. For more intricate open-ended generation tasks, we\nextend this method to \"Visual Debias Decoding\", which mitigates bias by\ncontrasting token log-probabilities conditioned on a correct image versus a\nmeaningless one. Additionally, our investigation sheds light on the instability\nof MLLMs across various decoding configurations. Through systematic exploration\nof different settings, we achieve significant performance\nimprovements--surpassing previously reported results--and raise concerns about\nthe fairness of current evaluation practices. Comprehensive experiments\nsubstantiate the effectiveness of our proposed strategies in mitigating biases.\nThese strategies not only prove beneficial in minimizing hallucinations but\nalso contribute to the generation of more helpful and precise illustrations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the realms of computer vision and natural language processing, Multimodal\nLarge Language Models (MLLMs) have become indispensable tools, proficient in\ngenerating textual responses based on visual inputs. Despite their\nadvancements, our investigation reveals a noteworthy bias: the generated\ncontent is often driven more by the inherent priors of the underlying Large\nLanguage Models (LLMs) than by the input image. Empirical experiments\nunderscore the persistence of this bias, as MLLMs often provide confident\nanswers even in the absence of relevant images or given incongruent visual\ninputs. To rectify these biases and redirect the model's focus toward visual\ninformation, we propose two simple, training-free strategies. First, for tasks\nsuch as classification or multi-choice question answering, we introduce a\n\"Post-Hoc Debias\" method using an affine calibration step to adjust the output\ndistribution. This approach ensures uniform answer scores when the image is\nabsent, acting as an effective regularization technique to alleviate the\ninfluence of LLM priors. For more intricate open-ended generation tasks, we\nextend this method to \"Visual Debias Decoding\", which mitigates bias by\ncontrasting token log-probabilities conditioned on a correct image versus a\nmeaningless one. Additionally, our investigation sheds light on the instability\nof MLLMs across various decoding configurations. Through systematic exploration\nof different settings, we achieve significant performance\nimprovements--surpassing previously reported results--and raise concerns about\nthe fairness of current evaluation practices. Comprehensive experiments\nsubstantiate the effectiveness of our proposed strategies in mitigating biases.\nThese strategies not only prove beneficial in minimizing hallucinations but\nalso contribute to the generation of more helpful and precise illustrations."
                },
                "authors": [
                    {
                        "name": "YiFan Zhang"
                    },
                    {
                        "name": "Yang Shi"
                    },
                    {
                        "name": "Weichen Yu"
                    },
                    {
                        "name": "Qingsong Wen"
                    },
                    {
                        "name": "Xue Wang"
                    },
                    {
                        "name": "Wenjing Yang"
                    },
                    {
                        "name": "Zhang Zhang"
                    },
                    {
                        "name": "Liang Wang"
                    },
                    {
                        "name": "Rong Jin"
                    }
                ],
                "author_detail": {
                    "name": "Rong Jin"
                },
                "author": "Rong Jin",
                "arxiv_comment": "10 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.05262v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.05262v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10581v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10581v1",
                "updated": "2025-08-14T12:20:51Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    12,
                    20,
                    51,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T12:20:51Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    12,
                    20,
                    51,
                    3,
                    226,
                    0
                ],
                "title": "Technical Report: Facilitating the Adoption of Causal Inference Methods\n  Through LLM-Empowered Co-Pilot",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Technical Report: Facilitating the Adoption of Causal Inference Methods\n  Through LLM-Empowered Co-Pilot"
                },
                "summary": "Estimating treatment effects (TE) from observational data is a critical yet\ncomplex task in many fields, from healthcare and economics to public policy.\nWhile recent advances in machine learning and causal inference have produced\npowerful estimation techniques, their adoption remains limited due to the need\nfor deep expertise in causal assumptions, adjustment strategies, and model\nselection. In this paper, we introduce CATE-B, an open-source co-pilot system\nthat uses large language models (LLMs) within an agentic framework to guide\nusers through the end-to-end process of treatment effect estimation. CATE-B\nassists in (i) constructing a structural causal model via causal discovery and\nLLM-based edge orientation, (ii) identifying robust adjustment sets through a\nnovel Minimal Uncertainty Adjustment Set criterion, and (iii) selecting\nappropriate regression methods tailored to the causal structure and dataset\ncharacteristics. To encourage reproducibility and evaluation, we release a\nsuite of benchmark tasks spanning diverse domains and causal complexities. By\ncombining causal inference with intelligent, interactive assistance, CATE-B\nlowers the barrier to rigorous causal analysis and lays the foundation for a\nnew class of benchmarks in automated treatment effect estimation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimating treatment effects (TE) from observational data is a critical yet\ncomplex task in many fields, from healthcare and economics to public policy.\nWhile recent advances in machine learning and causal inference have produced\npowerful estimation techniques, their adoption remains limited due to the need\nfor deep expertise in causal assumptions, adjustment strategies, and model\nselection. In this paper, we introduce CATE-B, an open-source co-pilot system\nthat uses large language models (LLMs) within an agentic framework to guide\nusers through the end-to-end process of treatment effect estimation. CATE-B\nassists in (i) constructing a structural causal model via causal discovery and\nLLM-based edge orientation, (ii) identifying robust adjustment sets through a\nnovel Minimal Uncertainty Adjustment Set criterion, and (iii) selecting\nappropriate regression methods tailored to the causal structure and dataset\ncharacteristics. To encourage reproducibility and evaluation, we release a\nsuite of benchmark tasks spanning diverse domains and causal complexities. By\ncombining causal inference with intelligent, interactive assistance, CATE-B\nlowers the barrier to rigorous causal analysis and lays the foundation for a\nnew class of benchmarks in automated treatment effect estimation."
                },
                "authors": [
                    {
                        "name": "Jeroen Berrevoets"
                    },
                    {
                        "name": "Julianna Piskorz"
                    },
                    {
                        "name": "Robert Davis"
                    },
                    {
                        "name": "Harry Amad"
                    },
                    {
                        "name": "Jim Weatherall"
                    },
                    {
                        "name": "Mihaela van der Schaar"
                    }
                ],
                "author_detail": {
                    "name": "Mihaela van der Schaar"
                },
                "author": "Mihaela van der Schaar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10581v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10581v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14325v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14325v3",
                "updated": "2025-08-14T12:12:53Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    12,
                    12,
                    53,
                    3,
                    226,
                    0
                ],
                "published": "2025-04-19T15:29:04Z",
                "published_parsed": [
                    2025,
                    4,
                    19,
                    15,
                    29,
                    4,
                    5,
                    109,
                    0
                ],
                "title": "FAIRGAME: a Framework for AI Agents Bias Recognition using Game Theory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FAIRGAME: a Framework for AI Agents Bias Recognition using Game Theory"
                },
                "summary": "Letting AI agents interact in multi-agent applications adds a layer of\ncomplexity to the interpretability and prediction of AI outcomes, with profound\nimplications for their trustworthy adoption in research and society. Game\ntheory offers powerful models to capture and interpret strategic interaction\namong agents, but requires the support of reproducible, standardized and\nuser-friendly IT frameworks to enable comparison and interpretation of results.\nTo this end, we present FAIRGAME, a Framework for AI Agents Bias Recognition\nusing Game Theory. We describe its implementation and usage, and we employ it\nto uncover biased outcomes in popular games among AI agents, depending on the\nemployed Large Language Model (LLM) and used language, as well as on the\npersonality trait or strategic knowledge of the agents. Overall, FAIRGAME\nallows users to reliably and easily simulate their desired games and scenarios\nand compare the results across simulation campaigns and with game-theoretic\npredictions, enabling the systematic discovery of biases, the anticipation of\nemerging behavior out of strategic interplays, and empowering further research\ninto strategic decision-making using LLM agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Letting AI agents interact in multi-agent applications adds a layer of\ncomplexity to the interpretability and prediction of AI outcomes, with profound\nimplications for their trustworthy adoption in research and society. Game\ntheory offers powerful models to capture and interpret strategic interaction\namong agents, but requires the support of reproducible, standardized and\nuser-friendly IT frameworks to enable comparison and interpretation of results.\nTo this end, we present FAIRGAME, a Framework for AI Agents Bias Recognition\nusing Game Theory. We describe its implementation and usage, and we employ it\nto uncover biased outcomes in popular games among AI agents, depending on the\nemployed Large Language Model (LLM) and used language, as well as on the\npersonality trait or strategic knowledge of the agents. Overall, FAIRGAME\nallows users to reliably and easily simulate their desired games and scenarios\nand compare the results across simulation campaigns and with game-theoretic\npredictions, enabling the systematic discovery of biases, the anticipation of\nemerging behavior out of strategic interplays, and empowering further research\ninto strategic decision-making using LLM agents."
                },
                "authors": [
                    {
                        "name": "Alessio Buscemi"
                    },
                    {
                        "name": "Daniele Proverbio"
                    },
                    {
                        "name": "Alessandro Di Stefano"
                    },
                    {
                        "name": "The Anh Han"
                    },
                    {
                        "name": "German Castignani"
                    },
                    {
                        "name": "Pietro Li"
                    }
                ],
                "author_detail": {
                    "name": "Pietro Li"
                },
                "author": "Pietro Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14325v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14325v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10572v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10572v1",
                "updated": "2025-08-14T12:11:15Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    12,
                    11,
                    15,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T12:11:15Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    12,
                    11,
                    15,
                    3,
                    226,
                    0
                ],
                "title": "Towards Agentic AI for Multimodal-Guided Video Object Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Agentic AI for Multimodal-Guided Video Object Segmentation"
                },
                "summary": "Referring-based Video Object Segmentation is a multimodal problem that\nrequires producing fine-grained segmentation results guided by external cues.\nTraditional approaches to this task typically involve training specialized\nmodels, which come with high computational complexity and manual annotation\neffort. Recent advances in vision-language foundation models open a promising\ndirection toward training-free approaches. Several studies have explored\nleveraging these general-purpose models for fine-grained segmentation,\nachieving performance comparable to that of fully supervised, task-specific\nmodels. However, existing methods rely on fixed pipelines that lack the\nflexibility needed to adapt to the dynamic nature of the task. To address this\nlimitation, we propose Multi-Modal Agent, a novel agentic system designed to\nsolve this task in a more flexible and adaptive manner. Specifically, our\nmethod leverages the reasoning capabilities of large language models (LLMs) to\ngenerate dynamic workflows tailored to each input. This adaptive procedure\niteratively interacts with a set of specialized tools designed for low-level\ntasks across different modalities to identify the target object described by\nthe multimodal cues. Our agentic approach demonstrates clear improvements over\nprior methods on two multimodal-conditioned VOS tasks: RVOS and Ref-AVS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Referring-based Video Object Segmentation is a multimodal problem that\nrequires producing fine-grained segmentation results guided by external cues.\nTraditional approaches to this task typically involve training specialized\nmodels, which come with high computational complexity and manual annotation\neffort. Recent advances in vision-language foundation models open a promising\ndirection toward training-free approaches. Several studies have explored\nleveraging these general-purpose models for fine-grained segmentation,\nachieving performance comparable to that of fully supervised, task-specific\nmodels. However, existing methods rely on fixed pipelines that lack the\nflexibility needed to adapt to the dynamic nature of the task. To address this\nlimitation, we propose Multi-Modal Agent, a novel agentic system designed to\nsolve this task in a more flexible and adaptive manner. Specifically, our\nmethod leverages the reasoning capabilities of large language models (LLMs) to\ngenerate dynamic workflows tailored to each input. This adaptive procedure\niteratively interacts with a set of specialized tools designed for low-level\ntasks across different modalities to identify the target object described by\nthe multimodal cues. Our agentic approach demonstrates clear improvements over\nprior methods on two multimodal-conditioned VOS tasks: RVOS and Ref-AVS."
                },
                "authors": [
                    {
                        "name": "Tuyen Tran"
                    },
                    {
                        "name": "Thao Minh Le"
                    },
                    {
                        "name": "Truyen Tran"
                    }
                ],
                "author_detail": {
                    "name": "Truyen Tran"
                },
                "author": "Truyen Tran",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10572v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10572v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01164v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01164v2",
                "updated": "2025-08-14T11:53:31Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    11,
                    53,
                    31,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-02T03:02:47Z",
                "published_parsed": [
                    2025,
                    8,
                    2,
                    3,
                    2,
                    47,
                    5,
                    214,
                    0
                ],
                "title": "M-estimation for Gaussian processes with time-inhomogeneous drifts from\n  high-frequency data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "M-estimation for Gaussian processes with time-inhomogeneous drifts from\n  high-frequency data"
                },
                "summary": "We propose a contrast-based estimation method for Gaussian processes with\ntime-inhomogeneous drifts, observed under high-frequency sampling. The process\nis modeled as the sum of a deterministic drift function and a stationary\nGaussian component with a parametric kernel. Our method constructs a local\ncontrast function from adjacent increments, which avoids inversion of large\ncovariance matrices and allows for efficient computation. We prove consistency\nand asymptotic normality of the resulting estimators under general ergodicity\nconditions. A distinctive feature of our approach is that the drift estimator\nattains a nonstandard convergence rate, stemming from the direct Riemann\nintegrability of the drift density. This highlights a fundamental difference\nfrom standard estimation regimes. Furthermore, when the local contrast fails to\nidentify all parameters in the covariance kernel, moment-based corrections can\nbe incorporated to recover identifiability. The proposed framework is simple,\nflexible, and particularly well suited for high-frequency inference with\ntime-inhomogeneous structure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a contrast-based estimation method for Gaussian processes with\ntime-inhomogeneous drifts, observed under high-frequency sampling. The process\nis modeled as the sum of a deterministic drift function and a stationary\nGaussian component with a parametric kernel. Our method constructs a local\ncontrast function from adjacent increments, which avoids inversion of large\ncovariance matrices and allows for efficient computation. We prove consistency\nand asymptotic normality of the resulting estimators under general ergodicity\nconditions. A distinctive feature of our approach is that the drift estimator\nattains a nonstandard convergence rate, stemming from the direct Riemann\nintegrability of the drift density. This highlights a fundamental difference\nfrom standard estimation regimes. Furthermore, when the local contrast fails to\nidentify all parameters in the covariance kernel, moment-based corrections can\nbe incorporated to recover identifiability. The proposed framework is simple,\nflexible, and particularly well suited for high-frequency inference with\ntime-inhomogeneous structure."
                },
                "authors": [
                    {
                        "name": "Yasutaka Shimizu"
                    }
                ],
                "author_detail": {
                    "name": "Yasutaka Shimizu"
                },
                "author": "Yasutaka Shimizu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01164v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01164v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62M10, 62F12, 60G15",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22107v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22107v4",
                "updated": "2025-08-14T11:51:31Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    11,
                    51,
                    31,
                    3,
                    226,
                    0
                ],
                "published": "2025-05-28T08:34:46Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    8,
                    34,
                    46,
                    2,
                    148,
                    0
                ],
                "title": "Curse of High Dimensionality Issue in Transformer for Long-context\n  Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Curse of High Dimensionality Issue in Transformer for Long-context\n  Modeling"
                },
                "summary": "Transformer-based large language models (LLMs) excel in natural language\nprocessing tasks by capturing long-range dependencies through self-attention\nmechanisms. However, long-context modeling faces significant computational\ninefficiencies due to \\textit{redundant} attention computations: while\nattention weights are often \\textit{sparse}, all tokens consume \\textit{equal}\ncomputational resources. In this paper, we reformulate traditional\nprobabilistic sequence modeling as a \\textit{supervised learning task},\nenabling the separation of relevant and irrelevant tokens and providing a\nclearer understanding of redundancy. Based on this reformulation, we\ntheoretically analyze attention sparsity, revealing that only a few tokens\nsignificantly contribute to predictions. Building on this, we formulate\nattention optimization as a linear coding problem and propose a \\textit{group\ncoding strategy}, theoretically showing its ability to improve robustness\nagainst random noise and enhance learning efficiency. Motivated by this, we\npropose \\textit{Dynamic Group Attention} (DGA), which leverages the group\ncoding to explicitly reduce redundancy by aggregating less important tokens\nduring attention computation. Empirical results show that our DGA significantly\nreduces computational costs while maintaining competitive performance.Code is\navailable at https://github.com/bolixinyu/DynamicGroupAttention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) excel in natural language\nprocessing tasks by capturing long-range dependencies through self-attention\nmechanisms. However, long-context modeling faces significant computational\ninefficiencies due to \\textit{redundant} attention computations: while\nattention weights are often \\textit{sparse}, all tokens consume \\textit{equal}\ncomputational resources. In this paper, we reformulate traditional\nprobabilistic sequence modeling as a \\textit{supervised learning task},\nenabling the separation of relevant and irrelevant tokens and providing a\nclearer understanding of redundancy. Based on this reformulation, we\ntheoretically analyze attention sparsity, revealing that only a few tokens\nsignificantly contribute to predictions. Building on this, we formulate\nattention optimization as a linear coding problem and propose a \\textit{group\ncoding strategy}, theoretically showing its ability to improve robustness\nagainst random noise and enhance learning efficiency. Motivated by this, we\npropose \\textit{Dynamic Group Attention} (DGA), which leverages the group\ncoding to explicitly reduce redundancy by aggregating less important tokens\nduring attention computation. Empirical results show that our DGA significantly\nreduces computational costs while maintaining competitive performance.Code is\navailable at https://github.com/bolixinyu/DynamicGroupAttention."
                },
                "authors": [
                    {
                        "name": "Shuhai Zhang"
                    },
                    {
                        "name": "Zeng You"
                    },
                    {
                        "name": "Yaofo Chen"
                    },
                    {
                        "name": "Zhiquan Wen"
                    },
                    {
                        "name": "Qianyue Wang"
                    },
                    {
                        "name": "Zhijie Qiu"
                    },
                    {
                        "name": "Yuanqing Li"
                    },
                    {
                        "name": "Mingkui Tan"
                    }
                ],
                "author_detail": {
                    "name": "Mingkui Tan"
                },
                "author": "Mingkui Tan",
                "arxiv_comment": "Accepted at ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22107v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22107v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10553v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10553v1",
                "updated": "2025-08-14T11:45:34Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    11,
                    45,
                    34,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T11:45:34Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    11,
                    45,
                    34,
                    3,
                    226,
                    0
                ],
                "title": "eDIF: A European Deep Inference Fabric for Remote Interpretability of\n  LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "eDIF: A European Deep Inference Fabric for Remote Interpretability of\n  LLM"
                },
                "summary": "This paper presents a feasibility study on the deployment of a European Deep\nInference Fabric (eDIF), an NDIF-compatible infrastructure designed to support\nmechanistic interpretability research on large language models. The need for\nwidespread accessibility of LLM interpretability infrastructure in Europe\ndrives this initiative to democratize advanced model analysis capabilities for\nthe research community. The project introduces a GPU-based cluster hosted at\nAnsbach University of Applied Sciences and interconnected with partner\ninstitutions, enabling remote model inspection via the NNsight API. A\nstructured pilot study involving 16 researchers from across Europe evaluated\nthe platform's technical performance, usability, and scientific utility. Users\nconducted interventions such as activation patching, causal tracing, and\nrepresentation analysis on models including GPT-2 and DeepSeek-R1-70B. The\nstudy revealed a gradual increase in user engagement, stable platform\nperformance throughout, and a positive reception of the remote experimentation\ncapabilities. It also marked the starting point for building a user community\naround the platform. Identified limitations such as prolonged download\ndurations for activation data as well as intermittent execution interruptions\nare addressed in the roadmap for future development. This initiative marks a\nsignificant step towards widespread accessibility of LLM interpretability\ninfrastructure in Europe and lays the groundwork for broader deployment,\nexpanded tooling, and sustained community collaboration in mechanistic\ninterpretability research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a feasibility study on the deployment of a European Deep\nInference Fabric (eDIF), an NDIF-compatible infrastructure designed to support\nmechanistic interpretability research on large language models. The need for\nwidespread accessibility of LLM interpretability infrastructure in Europe\ndrives this initiative to democratize advanced model analysis capabilities for\nthe research community. The project introduces a GPU-based cluster hosted at\nAnsbach University of Applied Sciences and interconnected with partner\ninstitutions, enabling remote model inspection via the NNsight API. A\nstructured pilot study involving 16 researchers from across Europe evaluated\nthe platform's technical performance, usability, and scientific utility. Users\nconducted interventions such as activation patching, causal tracing, and\nrepresentation analysis on models including GPT-2 and DeepSeek-R1-70B. The\nstudy revealed a gradual increase in user engagement, stable platform\nperformance throughout, and a positive reception of the remote experimentation\ncapabilities. It also marked the starting point for building a user community\naround the platform. Identified limitations such as prolonged download\ndurations for activation data as well as intermittent execution interruptions\nare addressed in the roadmap for future development. This initiative marks a\nsignificant step towards widespread accessibility of LLM interpretability\ninfrastructure in Europe and lays the groundwork for broader deployment,\nexpanded tooling, and sustained community collaboration in mechanistic\ninterpretability research."
                },
                "authors": [
                    {
                        "name": "Irma Heithoff. Marc Guggenberger"
                    },
                    {
                        "name": "Sandra Kalogiannis"
                    },
                    {
                        "name": "Susanne Mayer"
                    },
                    {
                        "name": "Fabian Maag"
                    },
                    {
                        "name": "Sigurd Schacht"
                    },
                    {
                        "name": "Carsten Lanquillon"
                    }
                ],
                "author_detail": {
                    "name": "Carsten Lanquillon"
                },
                "author": "Carsten Lanquillon",
                "arxiv_comment": "9 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10553v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10553v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10552v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10552v1",
                "updated": "2025-08-14T11:44:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    11,
                    44,
                    52,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T11:44:52Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    11,
                    44,
                    52,
                    3,
                    226,
                    0
                ],
                "title": "When Language Overrules: Revealing Text Dominance in Multimodal Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Language Overrules: Revealing Text Dominance in Multimodal Large\n  Language Models"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable\ncapabilities across a diverse range of multimodal tasks. However, these models\nsuffer from a core problem known as text dominance: they depend heavily on text\nfor their inference, while underutilizing other modalities. While prior work\nhas acknowledged this phenomenon in vision-language tasks, often attributing it\nto data biases or model architectures. In this paper, we conduct the first\nsystematic investigation of text dominance across diverse data modalities,\nincluding images, videos, audio, time-series, and graphs. To measure this\nimbalance, we propose two evaluation metrics: the Modality Dominance Index\n(MDI) and the Attention Efficiency Index (AEI). Our comprehensive analysis\nreveals that text dominance is both significant and pervasive across all tested\nmodalities. Our in-depth analysis identifies three underlying causes: attention\ndilution from severe token redundancy in non-textual modalities, the influence\nof fusion architecture design, and task formulations that implicitly favor\ntextual inputs. Furthermore, we propose a simple token compression method that\neffectively rebalances model attention. Applying this method to LLaVA-7B, for\ninstance, drastically reduces its MDI from 10.23 to a well-balanced value of\n0.86. Our analysis and methodological framework offer a foundation for the\ndevelopment of more equitable and comprehensive multimodal language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable\ncapabilities across a diverse range of multimodal tasks. However, these models\nsuffer from a core problem known as text dominance: they depend heavily on text\nfor their inference, while underutilizing other modalities. While prior work\nhas acknowledged this phenomenon in vision-language tasks, often attributing it\nto data biases or model architectures. In this paper, we conduct the first\nsystematic investigation of text dominance across diverse data modalities,\nincluding images, videos, audio, time-series, and graphs. To measure this\nimbalance, we propose two evaluation metrics: the Modality Dominance Index\n(MDI) and the Attention Efficiency Index (AEI). Our comprehensive analysis\nreveals that text dominance is both significant and pervasive across all tested\nmodalities. Our in-depth analysis identifies three underlying causes: attention\ndilution from severe token redundancy in non-textual modalities, the influence\nof fusion architecture design, and task formulations that implicitly favor\ntextual inputs. Furthermore, we propose a simple token compression method that\neffectively rebalances model attention. Applying this method to LLaVA-7B, for\ninstance, drastically reduces its MDI from 10.23 to a well-balanced value of\n0.86. Our analysis and methodological framework offer a foundation for the\ndevelopment of more equitable and comprehensive multimodal language models."
                },
                "authors": [
                    {
                        "name": "Huyu Wu"
                    },
                    {
                        "name": "Meng Tang"
                    },
                    {
                        "name": "Xinhan Zheng"
                    },
                    {
                        "name": "Haiyun Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Haiyun Jiang"
                },
                "author": "Haiyun Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10552v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10552v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22267v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22267v2",
                "updated": "2025-08-14T11:40:12Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    11,
                    40,
                    12,
                    3,
                    226,
                    0
                ],
                "published": "2025-06-27T14:36:39Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    14,
                    36,
                    39,
                    4,
                    178,
                    0
                ],
                "title": "From Data Center IoT Telemetry to Data Analytics Chatbots -- Virtual\n  Knowledge Graph is All You Need",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Data Center IoT Telemetry to Data Analytics Chatbots -- Virtual\n  Knowledge Graph is All You Need"
                },
                "summary": "Industry 5.0 demands IoT systems that support seamless human-machine\ncollaboration, yet current IoT data analysis requires deep domain, deployment,\nand query expertise. We show that combining Large Language Models (LLMs) with\nKnowledge Graphs (KGs) enables natural language access to heterogeneous IoT\ndata. Focusing on data center IoT telemetry, we introduce a rule-based Virtual\nKnowledge Graph (VKG) construction process and an on-premise LLM inference\nservice to create an end-to-end Data Analytics (DA) chatbot. Our system\ndynamically generates VKGs per query and translates user input into SPARQL,\nachieving 92.5% accuracy (vs. 25% for LLM-to-NoSQL) while reducing latency by\n85% (20.36s to 3.03s) and keeping VKG sizes under 179 MiB. This work\ndemonstrates that VKG-powered LLM interfaces deliver accurate, low-latency, and\nrelationship-aware access to large-scale telemetry, bridging the gap between\nusers and complex IoT systems in Industry 5.0.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Industry 5.0 demands IoT systems that support seamless human-machine\ncollaboration, yet current IoT data analysis requires deep domain, deployment,\nand query expertise. We show that combining Large Language Models (LLMs) with\nKnowledge Graphs (KGs) enables natural language access to heterogeneous IoT\ndata. Focusing on data center IoT telemetry, we introduce a rule-based Virtual\nKnowledge Graph (VKG) construction process and an on-premise LLM inference\nservice to create an end-to-end Data Analytics (DA) chatbot. Our system\ndynamically generates VKGs per query and translates user input into SPARQL,\nachieving 92.5% accuracy (vs. 25% for LLM-to-NoSQL) while reducing latency by\n85% (20.36s to 3.03s) and keeping VKG sizes under 179 MiB. This work\ndemonstrates that VKG-powered LLM interfaces deliver accurate, low-latency, and\nrelationship-aware access to large-scale telemetry, bridging the gap between\nusers and complex IoT systems in Industry 5.0."
                },
                "authors": [
                    {
                        "name": "Junaid Ahmed Khan"
                    },
                    {
                        "name": "Hiari Pizzini Cavagna"
                    },
                    {
                        "name": "Andrea Proia"
                    },
                    {
                        "name": "Andrea Bartolini"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Bartolini"
                },
                "author": "Andrea Bartolini",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22267v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22267v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10543v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10543v1",
                "updated": "2025-08-14T11:32:25Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    11,
                    32,
                    25,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T11:32:25Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    11,
                    32,
                    25,
                    3,
                    226,
                    0
                ],
                "title": "Dark Star Clusters or Ultra-Faint Dwarf galaxies? Revisiting UMa3/U1",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dark Star Clusters or Ultra-Faint Dwarf galaxies? Revisiting UMa3/U1"
                },
                "summary": "Owing to sparse spectroscopic observations, the classification of faint\nsatellites as either dark matter-dominated dwarf galaxies or self-gravitating\nstar clusters remains unresolved. The recently discovered Ursa Major III/UNIONS\n1 (UMa3/U1) object, with its measured velocity dispersion, provides a rare\nobservational anchor in this regime. Despite its cluster-like compactness, its\ninferred dynamical mass-to-light ratio (M_dyn/L) suggests a dark\nmatter-dominated nature, prompting interpretations of UMa3/U1 as a microgalaxy,\nthough current measurements remain inconclusive. Thousand-level M_dyn/L values\nare not unique to galaxies; self-gravitating dark star clusters (DSCs) can\nreach comparable levels via energy injection driven by a centrally segregated\nblack hole subsystem (BHSub), which accelerates the evaporation of luminous\nstars and leads to a super-virial appearance with elevated velocity dispersion.\nTo assess whether UMa3/U1 is a DSC, we conducted direct N-body simulations and\nidentified a model that successfully reproduces both its compact structure and\nelevated M_dyn/L, supporting a self-gravitating cluster origin. We find the\ncluster entered the DSC phase around 4 Gyr ago, with its luminous stars\nexpected to be depleted within the next 1 Gyr, followed by the gradual\ndisruption of the central BHSub over the subsequent Gyr. We broaden our\nanalysis by mapping DSC evolutionary tracks in the size versus total luminosity\n(L) and M_dyn/L-L spaces, showing that DSCs occupy a region overlapping with\nfaint, ambiguous satellites. In the M_dyn/L-L diagram, DSCs trace a\ntransitional channel bridging globular clusters and dwarf galaxies as they rise\nfrom M_dyn/L ~ 2 to 10^4 M_sun/L_sun.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Owing to sparse spectroscopic observations, the classification of faint\nsatellites as either dark matter-dominated dwarf galaxies or self-gravitating\nstar clusters remains unresolved. The recently discovered Ursa Major III/UNIONS\n1 (UMa3/U1) object, with its measured velocity dispersion, provides a rare\nobservational anchor in this regime. Despite its cluster-like compactness, its\ninferred dynamical mass-to-light ratio (M_dyn/L) suggests a dark\nmatter-dominated nature, prompting interpretations of UMa3/U1 as a microgalaxy,\nthough current measurements remain inconclusive. Thousand-level M_dyn/L values\nare not unique to galaxies; self-gravitating dark star clusters (DSCs) can\nreach comparable levels via energy injection driven by a centrally segregated\nblack hole subsystem (BHSub), which accelerates the evaporation of luminous\nstars and leads to a super-virial appearance with elevated velocity dispersion.\nTo assess whether UMa3/U1 is a DSC, we conducted direct N-body simulations and\nidentified a model that successfully reproduces both its compact structure and\nelevated M_dyn/L, supporting a self-gravitating cluster origin. We find the\ncluster entered the DSC phase around 4 Gyr ago, with its luminous stars\nexpected to be depleted within the next 1 Gyr, followed by the gradual\ndisruption of the central BHSub over the subsequent Gyr. We broaden our\nanalysis by mapping DSC evolutionary tracks in the size versus total luminosity\n(L) and M_dyn/L-L spaces, showing that DSCs occupy a region overlapping with\nfaint, ambiguous satellites. In the M_dyn/L-L diagram, DSCs trace a\ntransitional channel bridging globular clusters and dwarf galaxies as they rise\nfrom M_dyn/L ~ 2 to 10^4 M_sun/L_sun."
                },
                "authors": [
                    {
                        "name": "Ali Rostami-Shirazi"
                    },
                    {
                        "name": "Hosein Haghi"
                    },
                    {
                        "name": "Akram Hasani Zonoozi"
                    },
                    {
                        "name": "Pavel Kroupa"
                    }
                ],
                "author_detail": {
                    "name": "Pavel Kroupa"
                },
                "author": "Pavel Kroupa",
                "arxiv_doi": "10.3847/2041-8213/adf320",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3847/2041-8213/adf320",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.10543v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10543v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "11 pages, 4 figures, 1 table. Accepted for publication in APJL",
                "arxiv_journal_ref": "The Astrophysical Journal Letters, Volume 989, year 2025, L14",
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10539v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10539v1",
                "updated": "2025-08-14T11:22:29Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    11,
                    22,
                    29,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T11:22:29Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    11,
                    22,
                    29,
                    3,
                    226,
                    0
                ],
                "title": "Improving Value-based Process Verifier via Low-Cost Variance Reduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Value-based Process Verifier via Low-Cost Variance Reduction"
                },
                "summary": "Large language models (LLMs) have achieved remarkable success in a wide range\nof tasks. However, their reasoning capabilities, particularly in complex\ndomains like mathematics, remain a significant challenge. Value-based process\nverifiers, which estimate the probability of a partial reasoning chain leading\nto a correct solution, are a promising approach for improving reasoning.\nNevertheless, their effectiveness is often hindered by estimation error in\ntheir training annotations, a consequence of the limited number of Monte Carlo\n(MC) samples feasible due to the high cost of LLM inference. In this paper, we\nidentify that the estimation error primarily arises from high variance rather\nthan bias, and the MC estimator is a Minimum Variance Unbiased Estimator\n(MVUE). To address the problem, we propose the \\textsc{Com}pound \\textsc{M}onte\n\\textsc{C}arlo \\textsc{S}ampling (ComMCS) method, which constructs an unbiased\nestimator by linearly combining the MC estimators from the current and\nsubsequent steps. Theoretically, we show that our method leads to a predictable\nreduction in variance, while maintaining an unbiased estimation without\nadditional LLM inference cost. We also perform empirical experiments on the\nMATH-500 and GSM8K benchmarks to demonstrate the effectiveness of our method.\nNotably, ComMCS outperforms regression-based optimization method by 2.8 points,\nthe non-variance-reduced baseline by 2.2 points on MATH-500 on Best-of-32\nsampling experiment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved remarkable success in a wide range\nof tasks. However, their reasoning capabilities, particularly in complex\ndomains like mathematics, remain a significant challenge. Value-based process\nverifiers, which estimate the probability of a partial reasoning chain leading\nto a correct solution, are a promising approach for improving reasoning.\nNevertheless, their effectiveness is often hindered by estimation error in\ntheir training annotations, a consequence of the limited number of Monte Carlo\n(MC) samples feasible due to the high cost of LLM inference. In this paper, we\nidentify that the estimation error primarily arises from high variance rather\nthan bias, and the MC estimator is a Minimum Variance Unbiased Estimator\n(MVUE). To address the problem, we propose the \\textsc{Com}pound \\textsc{M}onte\n\\textsc{C}arlo \\textsc{S}ampling (ComMCS) method, which constructs an unbiased\nestimator by linearly combining the MC estimators from the current and\nsubsequent steps. Theoretically, we show that our method leads to a predictable\nreduction in variance, while maintaining an unbiased estimation without\nadditional LLM inference cost. We also perform empirical experiments on the\nMATH-500 and GSM8K benchmarks to demonstrate the effectiveness of our method.\nNotably, ComMCS outperforms regression-based optimization method by 2.8 points,\nthe non-variance-reduced baseline by 2.2 points on MATH-500 on Best-of-32\nsampling experiment."
                },
                "authors": [
                    {
                        "name": "Zetian Sun"
                    },
                    {
                        "name": "Dongfang Li"
                    },
                    {
                        "name": "Baotian Hu"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10539v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10539v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10535v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10535v1",
                "updated": "2025-08-14T11:13:45Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    11,
                    13,
                    45,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T11:13:45Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    11,
                    13,
                    45,
                    3,
                    226,
                    0
                ],
                "title": "Active Automata Learning with Advice",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Active Automata Learning with Advice"
                },
                "summary": "We present an extended automata learning framework that combines active\nautomata learning with deductive inference. The learning algorithm asks\nmembership and equivalence queries as in the original framework, but it is also\ngiven advice, which is used to infer answers to queries when possible and\nreduce the burden on the teacher. We consider advice given via string rewriting\nsystems, which specify equivalence of words w.r.t. the target languages. The\nmain motivation for the proposed framework is to reduce the number of queries.\nWe show how to adapt Angluin-style learning algorithms to this framework with\nlow overhead. Finally, we present empirical evaluation of our approach and\nobserve substantial improvement in query complexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present an extended automata learning framework that combines active\nautomata learning with deductive inference. The learning algorithm asks\nmembership and equivalence queries as in the original framework, but it is also\ngiven advice, which is used to infer answers to queries when possible and\nreduce the burden on the teacher. We consider advice given via string rewriting\nsystems, which specify equivalence of words w.r.t. the target languages. The\nmain motivation for the proposed framework is to reduce the number of queries.\nWe show how to adapt Angluin-style learning algorithms to this framework with\nlow overhead. Finally, we present empirical evaluation of our approach and\nobserve substantial improvement in query complexity."
                },
                "authors": [
                    {
                        "name": "Micha Fica"
                    },
                    {
                        "name": "Jan Otop"
                    }
                ],
                "author_detail": {
                    "name": "Jan Otop"
                },
                "author": "Jan Otop",
                "arxiv_comment": "The full version of the paper accepted to ECAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10535v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10535v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.FL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.FL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.4.3; I.2.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10533v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10533v1",
                "updated": "2025-08-14T11:10:07Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    11,
                    10,
                    7,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T11:10:07Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    11,
                    10,
                    7,
                    3,
                    226,
                    0
                ],
                "title": "Mitigating Exponential Mixed Frequency Growth through Frequency\n  Selection and Dimensional Separation in Quantum Machine Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating Exponential Mixed Frequency Growth through Frequency\n  Selection and Dimensional Separation in Quantum Machine Learning"
                },
                "summary": "To leverage the potential computational speedup of quantum computing (QC),\nresearch in quantum machine learning (QML) has gained increasing prominence.\nAngle encoding techniques in QML models have been shown to generate truncated\nFourier series, offering asymptotically universal function approximation\ncapabilities. By selecting efficient feature maps (FMs) within quantum\ncircuits, one can leverage the exponential growth of Fourier frequencies for\nimproved approximation. In multi-dimensional settings, additional input\ndimensions induce further exponential scaling via mixed frequencies. In\npractice, however, quantum models frequently fail at regression tasks. Through\ntwo white-box experiments, we show that such failures can occur even when the\nrelevant frequencies are present, due to an insufficient number of trainable\nparameters.\n  In order to mitigate the double-exponential parameter growth resulting from\ndouble-exponentially growing frequencies, we propose frequency selection and\ndimensional separation as techniques to constrain the number of parameters,\nthereby improving trainability. By restricting the QML model to essential\nfrequencies and permitting mixed frequencies only among feature dimensions with\nknown interdependence, we expand the set of tractable problems on current\nhardware. We demonstrate the reduced parameter requirements by fitting two\nwhite-box functions with known frequency spectrum and dimensional\ninterdependencies that could not be fitted with the default methods. The\nreduced parameter requirements permit us to perform training on a noisy quantum\nsimulator and to demonstrate inference on real quantum hardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To leverage the potential computational speedup of quantum computing (QC),\nresearch in quantum machine learning (QML) has gained increasing prominence.\nAngle encoding techniques in QML models have been shown to generate truncated\nFourier series, offering asymptotically universal function approximation\ncapabilities. By selecting efficient feature maps (FMs) within quantum\ncircuits, one can leverage the exponential growth of Fourier frequencies for\nimproved approximation. In multi-dimensional settings, additional input\ndimensions induce further exponential scaling via mixed frequencies. In\npractice, however, quantum models frequently fail at regression tasks. Through\ntwo white-box experiments, we show that such failures can occur even when the\nrelevant frequencies are present, due to an insufficient number of trainable\nparameters.\n  In order to mitigate the double-exponential parameter growth resulting from\ndouble-exponentially growing frequencies, we propose frequency selection and\ndimensional separation as techniques to constrain the number of parameters,\nthereby improving trainability. By restricting the QML model to essential\nfrequencies and permitting mixed frequencies only among feature dimensions with\nknown interdependence, we expand the set of tractable problems on current\nhardware. We demonstrate the reduced parameter requirements by fitting two\nwhite-box functions with known frequency spectrum and dimensional\ninterdependencies that could not be fitted with the default methods. The\nreduced parameter requirements permit us to perform training on a noisy quantum\nsimulator and to demonstrate inference on real quantum hardware."
                },
                "authors": [
                    {
                        "name": "Michael Poppel"
                    },
                    {
                        "name": "David Bucher"
                    },
                    {
                        "name": "Maximilian Zorn"
                    },
                    {
                        "name": "Nico Kraus"
                    },
                    {
                        "name": "Jonas Stein"
                    },
                    {
                        "name": "Claudia Linnhoff-Popien"
                    }
                ],
                "author_detail": {
                    "name": "Claudia Linnhoff-Popien"
                },
                "author": "Claudia Linnhoff-Popien",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10533v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10533v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.13871v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.13871v2",
                "updated": "2025-08-14T10:59:03Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    10,
                    59,
                    3,
                    3,
                    226,
                    0
                ],
                "published": "2024-02-21T15:23:21Z",
                "published_parsed": [
                    2024,
                    2,
                    21,
                    15,
                    23,
                    21,
                    2,
                    52,
                    0
                ],
                "title": "An Explainable Transformer-based Model for Phishing Email Detection: A\n  Large Language Model Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Explainable Transformer-based Model for Phishing Email Detection: A\n  Large Language Model Approach"
                },
                "summary": "Phishing email is a serious cyber threat that tries to deceive users by\nsending false emails with the intention of stealing confidential information or\ncausing financial harm. Attackers, often posing as trustworthy entities,\nexploit technological advancements and sophistication to make detection and\nprevention of phishing more challenging. Despite extensive academic research,\nphishing detection remains an ongoing and formidable challenge in the\ncybersecurity landscape. Large Language Models (LLMs) and Masked Language\nModels (MLMs) possess immense potential to offer innovative solutions to\naddress long-standing challenges. In this research paper, we present an\noptimized, fine-tuned transformer-based DistilBERT model designed for the\ndetection of phishing emails. In the detection process, we work with a phishing\nemail dataset and utilize the preprocessing techniques to clean and solve the\nimbalance class issues. Through our experiments, we found that our model\neffectively achieves high accuracy, demonstrating its capability to perform\nwell. Finally, we demonstrate our fine-tuned model using Explainable-AI (XAI)\ntechniques such as Local Interpretable Model-Agnostic Explanations (LIME) and\nTransformer Interpret to explain how our model makes predictions in the context\nof text classification for phishing emails.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Phishing email is a serious cyber threat that tries to deceive users by\nsending false emails with the intention of stealing confidential information or\ncausing financial harm. Attackers, often posing as trustworthy entities,\nexploit technological advancements and sophistication to make detection and\nprevention of phishing more challenging. Despite extensive academic research,\nphishing detection remains an ongoing and formidable challenge in the\ncybersecurity landscape. Large Language Models (LLMs) and Masked Language\nModels (MLMs) possess immense potential to offer innovative solutions to\naddress long-standing challenges. In this research paper, we present an\noptimized, fine-tuned transformer-based DistilBERT model designed for the\ndetection of phishing emails. In the detection process, we work with a phishing\nemail dataset and utilize the preprocessing techniques to clean and solve the\nimbalance class issues. Through our experiments, we found that our model\neffectively achieves high accuracy, demonstrating its capability to perform\nwell. Finally, we demonstrate our fine-tuned model using Explainable-AI (XAI)\ntechniques such as Local Interpretable Model-Agnostic Explanations (LIME) and\nTransformer Interpret to explain how our model makes predictions in the context\nof text classification for phishing emails."
                },
                "authors": [
                    {
                        "name": "Mohammad Amaz Uddin"
                    },
                    {
                        "name": "Md Mahiuddin"
                    },
                    {
                        "name": "Iqbal H. Sarker"
                    }
                ],
                "author_detail": {
                    "name": "Iqbal H. Sarker"
                },
                "author": "Iqbal H. Sarker",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.13871v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.13871v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10517v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10517v1",
                "updated": "2025-08-14T10:42:26Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    10,
                    42,
                    26,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T10:42:26Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    10,
                    42,
                    26,
                    3,
                    226,
                    0
                ],
                "title": "Bridging Solidity Evolution Gaps: An LLM-Enhanced Approach for Smart\n  Contract Compilation Error Resolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging Solidity Evolution Gaps: An LLM-Enhanced Approach for Smart\n  Contract Compilation Error Resolution"
                },
                "summary": "Solidity, the dominant smart contract language for Ethereum, has rapidly\nevolved with frequent version updates to enhance security, functionality, and\ndeveloper experience. However, these continual changes introduce significant\nchallenges, particularly in compilation errors, code migration, and\nmaintenance. Therefore, we conduct an empirical study to investigate the\nchallenges in the Solidity version evolution and reveal that 81.68% of examined\ncontracts encounter errors when compiled across different versions, with 86.92%\nof compilation errors.\n  To mitigate these challenges, we conducted a systematic evaluation of large\nlanguage models (LLMs) for resolving Solidity compilation errors during version\nmigrations. Our empirical analysis across both open-source (LLaMA3, DeepSeek)\nand closed-source (GPT-4o, GPT-3.5-turbo) LLMs reveals that although these\nmodels exhibit error repair capabilities, their effectiveness diminishes\nsignificantly for semantic-level issues and shows strong dependency on prompt\nengineering strategies. This underscores the critical need for domain-specific\nadaptation in developing reliable LLM-based repair systems for smart contracts.\n  Building upon these insights, we introduce SMCFIXER, a novel framework that\nsystematically integrates expert knowledge retrieval with LLM-based repair\nmechanisms for Solidity compilation error resolution. The architecture\ncomprises three core phases: (1) context-aware code slicing that extracts\nrelevant error information; (2) expert knowledge retrieval from official\ndocumentation; and (3) iterative patch generation for Solidity migration.\nExperimental validation across Solidity version migrations demonstrates our\napproach's statistically significant 24.24% improvement over baseline GPT-4o on\nreal-world datasets, achieving near-perfect 96.97% accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Solidity, the dominant smart contract language for Ethereum, has rapidly\nevolved with frequent version updates to enhance security, functionality, and\ndeveloper experience. However, these continual changes introduce significant\nchallenges, particularly in compilation errors, code migration, and\nmaintenance. Therefore, we conduct an empirical study to investigate the\nchallenges in the Solidity version evolution and reveal that 81.68% of examined\ncontracts encounter errors when compiled across different versions, with 86.92%\nof compilation errors.\n  To mitigate these challenges, we conducted a systematic evaluation of large\nlanguage models (LLMs) for resolving Solidity compilation errors during version\nmigrations. Our empirical analysis across both open-source (LLaMA3, DeepSeek)\nand closed-source (GPT-4o, GPT-3.5-turbo) LLMs reveals that although these\nmodels exhibit error repair capabilities, their effectiveness diminishes\nsignificantly for semantic-level issues and shows strong dependency on prompt\nengineering strategies. This underscores the critical need for domain-specific\nadaptation in developing reliable LLM-based repair systems for smart contracts.\n  Building upon these insights, we introduce SMCFIXER, a novel framework that\nsystematically integrates expert knowledge retrieval with LLM-based repair\nmechanisms for Solidity compilation error resolution. The architecture\ncomprises three core phases: (1) context-aware code slicing that extracts\nrelevant error information; (2) expert knowledge retrieval from official\ndocumentation; and (3) iterative patch generation for Solidity migration.\nExperimental validation across Solidity version migrations demonstrates our\napproach's statistically significant 24.24% improvement over baseline GPT-4o on\nreal-world datasets, achieving near-perfect 96.97% accuracy."
                },
                "authors": [
                    {
                        "name": "Likai Ye"
                    },
                    {
                        "name": "Mengliang Li"
                    },
                    {
                        "name": "Dehai Zhao"
                    },
                    {
                        "name": "Jiamou Sun"
                    },
                    {
                        "name": "Xiaoxue Ren"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoxue Ren"
                },
                "author": "Xiaoxue Ren",
                "arxiv_comment": "International Conference on Software Maintenance and Evolution\n  (ICSME) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10517v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10517v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06259v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06259v2",
                "updated": "2025-08-14T10:34:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    10,
                    34,
                    22,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-08T12:26:20Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    12,
                    26,
                    20,
                    4,
                    220,
                    0
                ],
                "title": "SIFThinker: Spatially-Aware Image Focus for Visual Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SIFThinker: Spatially-Aware Image Focus for Visual Reasoning"
                },
                "summary": "Current multimodal large language models (MLLMs) still face significant\nchallenges in complex visual tasks (e.g., spatial understanding, fine-grained\nperception). Prior methods have tried to incorporate visual reasoning, however,\nthey fail to leverage attention correction with spatial cues to iteratively\nrefine their focus on prompt-relevant regions. In this paper, we introduce\nSIFThinker, a spatially-aware \"think-with-images\" framework that mimics human\nvisual perception. Specifically, SIFThinker enables attention correcting and\nimage region focusing by interleaving depth-enhanced bounding boxes and natural\nlanguage. Our contributions are twofold: First, we introduce a\nreverse-expansion-forward-inference strategy that facilitates the generation of\ninterleaved image-text chains of thought for process-level supervision, which\nin turn leads to the construction of the SIF-50K dataset. Besides, we propose\nGRPO-SIF, a reinforced training paradigm that integrates depth-informed visual\ngrounding into a unified reasoning pipeline, teaching the model to dynamically\ncorrect and focus on prompt-relevant regions. Extensive experiments demonstrate\nthat SIFThinker outperforms state-of-the-art methods in spatial understanding\nand fine-grained visual perception, while maintaining strong general\ncapabilities, highlighting the effectiveness of our method. Code:\nhttps://github.com/zhangquanchen/SIFThinker.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current multimodal large language models (MLLMs) still face significant\nchallenges in complex visual tasks (e.g., spatial understanding, fine-grained\nperception). Prior methods have tried to incorporate visual reasoning, however,\nthey fail to leverage attention correction with spatial cues to iteratively\nrefine their focus on prompt-relevant regions. In this paper, we introduce\nSIFThinker, a spatially-aware \"think-with-images\" framework that mimics human\nvisual perception. Specifically, SIFThinker enables attention correcting and\nimage region focusing by interleaving depth-enhanced bounding boxes and natural\nlanguage. Our contributions are twofold: First, we introduce a\nreverse-expansion-forward-inference strategy that facilitates the generation of\ninterleaved image-text chains of thought for process-level supervision, which\nin turn leads to the construction of the SIF-50K dataset. Besides, we propose\nGRPO-SIF, a reinforced training paradigm that integrates depth-informed visual\ngrounding into a unified reasoning pipeline, teaching the model to dynamically\ncorrect and focus on prompt-relevant regions. Extensive experiments demonstrate\nthat SIFThinker outperforms state-of-the-art methods in spatial understanding\nand fine-grained visual perception, while maintaining strong general\ncapabilities, highlighting the effectiveness of our method. Code:\nhttps://github.com/zhangquanchen/SIFThinker."
                },
                "authors": [
                    {
                        "name": "Zhangquan Chen"
                    },
                    {
                        "name": "Ruihui Zhao"
                    },
                    {
                        "name": "Chuwei Luo"
                    },
                    {
                        "name": "Mingze Sun"
                    },
                    {
                        "name": "Xinlei Yu"
                    },
                    {
                        "name": "Yangyang Kang"
                    },
                    {
                        "name": "Ruqi Huang"
                    }
                ],
                "author_detail": {
                    "name": "Ruqi Huang"
                },
                "author": "Ruqi Huang",
                "arxiv_comment": "15 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06259v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06259v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08601v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08601v3",
                "updated": "2025-08-14T10:26:51Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    10,
                    26,
                    51,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-12T03:34:21Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    3,
                    34,
                    21,
                    1,
                    224,
                    0
                ],
                "title": "Yan: Foundational Interactive Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Yan: Foundational Interactive Video Generation"
                },
                "summary": "We present Yan, a foundational framework for interactive video generation,\ncovering the entire pipeline from simulation and generation to editing.\nSpecifically, Yan comprises three core modules. AAA-level Simulation: We design\na highly-compressed, low-latency 3D-VAE coupled with a KV-cache-based\nshift-window denoising inference process, achieving real-time 1080P/60FPS\ninteractive simulation. Multi-Modal Generation: We introduce a hierarchical\nautoregressive caption method that injects game-specific knowledge into\nopen-domain multi-modal video diffusion models (VDMs), then transforming the\nVDM into a frame-wise, action-controllable, real-time infinite interactive\nvideo generator. Notably, when the textual and visual prompts are sourced from\ndifferent domains, the model demonstrates strong generalization, allowing it to\nblend and compose the style and mechanics across domains flexibly according to\nuser prompts. Multi-Granularity Editing: We propose a hybrid model that\nexplicitly disentangles interactive mechanics simulation from visual rendering,\nenabling multi-granularity video content editing during interaction through\ntext. Collectively, Yan offers an integration of these modules, pushing\ninteractive video generation beyond isolated capabilities toward a\ncomprehensive AI-driven interactive creation paradigm, paving the way for the\nnext generation of creative tools, media, and entertainment. The project page\nis: https://greatx3.github.io/Yan/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Yan, a foundational framework for interactive video generation,\ncovering the entire pipeline from simulation and generation to editing.\nSpecifically, Yan comprises three core modules. AAA-level Simulation: We design\na highly-compressed, low-latency 3D-VAE coupled with a KV-cache-based\nshift-window denoising inference process, achieving real-time 1080P/60FPS\ninteractive simulation. Multi-Modal Generation: We introduce a hierarchical\nautoregressive caption method that injects game-specific knowledge into\nopen-domain multi-modal video diffusion models (VDMs), then transforming the\nVDM into a frame-wise, action-controllable, real-time infinite interactive\nvideo generator. Notably, when the textual and visual prompts are sourced from\ndifferent domains, the model demonstrates strong generalization, allowing it to\nblend and compose the style and mechanics across domains flexibly according to\nuser prompts. Multi-Granularity Editing: We propose a hybrid model that\nexplicitly disentangles interactive mechanics simulation from visual rendering,\nenabling multi-granularity video content editing during interaction through\ntext. Collectively, Yan offers an integration of these modules, pushing\ninteractive video generation beyond isolated capabilities toward a\ncomprehensive AI-driven interactive creation paradigm, paving the way for the\nnext generation of creative tools, media, and entertainment. The project page\nis: https://greatx3.github.io/Yan/."
                },
                "authors": [
                    {
                        "name": "Deheng Ye"
                    },
                    {
                        "name": "Fangyun Zhou"
                    },
                    {
                        "name": "Jiacheng Lv"
                    },
                    {
                        "name": "Jianqi Ma"
                    },
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Junyan Lv"
                    },
                    {
                        "name": "Junyou Li"
                    },
                    {
                        "name": "Minwen Deng"
                    },
                    {
                        "name": "Mingyu Yang"
                    },
                    {
                        "name": "Qiang Fu"
                    },
                    {
                        "name": "Wei Yang"
                    },
                    {
                        "name": "Wenkai Lv"
                    },
                    {
                        "name": "Yangbin Yu"
                    },
                    {
                        "name": "Yewen Wang"
                    },
                    {
                        "name": "Yonghang Guan"
                    },
                    {
                        "name": "Zhihao Hu"
                    },
                    {
                        "name": "Zhongbin Fang"
                    },
                    {
                        "name": "Zhongqian Sun"
                    }
                ],
                "author_detail": {
                    "name": "Zhongqian Sun"
                },
                "author": "Zhongqian Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08601v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08601v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09600v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09600v2",
                "updated": "2025-08-14T10:23:59Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    10,
                    23,
                    59,
                    3,
                    226,
                    0
                ],
                "published": "2025-05-14T17:50:28Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    17,
                    50,
                    28,
                    2,
                    134,
                    0
                ],
                "title": "Accelerating the time-domain LISA response model with central finite\n  differences and hybridization techniques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating the time-domain LISA response model with central finite\n  differences and hybridization techniques"
                },
                "summary": "Accurate and efficient modeling of the Laser Interferometer Space Antenna\n(LISA) response is crucial for gravitational-wave (GW) data analysis. A key\ncomputational challenge lies in evaluating time-delay interferometry (TDI)\nvariables, which require projecting GW polarizations onto the LISA arms at\ndifferent retarded times. Without approximations, the full LISA response is\ncomputationally expensive, and traditional approaches, such as the\nlong-wavelength approximation, accelerate the response calculation at the cost\nof reducing accuracy at high frequencies. In this work, we introduce a novel\nhybrid time-domain response for LISA that balances computational efficiency and\naccuracy across the binary's evolution. Our method is applicable to massive\nblack hole binaries and implements a fast low-frequency approximation during\nthe early inspiral$\\unicode{x2013}$where most of these binaries spend most of\nthe time in the sensitive frequency band of LISA$\\unicode{x2013}$while\nreserving the computationally intensive full-response calculations for the late\ninspiral, merger, and ringdown phases. The low-frequency approximation (LFA) is\nbased on Taylor expanding the response quantities around a chosen evaluation\ntime such that time delays correspond to central finite differences. Our hybrid\napproach supports CPU and GPU implementations, TDI generations 1.5 and 2.0, and\nflexible time-delay complexity, and has the potential to accelerate parts of\nthe global fit and reduce energy consumption. We also test our LFA and hybrid\nresponses on eccentric binaries, and we perform parameter estimation for a\n\"golden\" binary. Additionally, we assess the efficacy of our low-frequency\nresponse for \"deep alerts\" by performing inspiral-only Bayesian inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate and efficient modeling of the Laser Interferometer Space Antenna\n(LISA) response is crucial for gravitational-wave (GW) data analysis. A key\ncomputational challenge lies in evaluating time-delay interferometry (TDI)\nvariables, which require projecting GW polarizations onto the LISA arms at\ndifferent retarded times. Without approximations, the full LISA response is\ncomputationally expensive, and traditional approaches, such as the\nlong-wavelength approximation, accelerate the response calculation at the cost\nof reducing accuracy at high frequencies. In this work, we introduce a novel\nhybrid time-domain response for LISA that balances computational efficiency and\naccuracy across the binary's evolution. Our method is applicable to massive\nblack hole binaries and implements a fast low-frequency approximation during\nthe early inspiral$\\unicode{x2013}$where most of these binaries spend most of\nthe time in the sensitive frequency band of LISA$\\unicode{x2013}$while\nreserving the computationally intensive full-response calculations for the late\ninspiral, merger, and ringdown phases. The low-frequency approximation (LFA) is\nbased on Taylor expanding the response quantities around a chosen evaluation\ntime such that time delays correspond to central finite differences. Our hybrid\napproach supports CPU and GPU implementations, TDI generations 1.5 and 2.0, and\nflexible time-delay complexity, and has the potential to accelerate parts of\nthe global fit and reduce energy consumption. We also test our LFA and hybrid\nresponses on eccentric binaries, and we perform parameter estimation for a\n\"golden\" binary. Additionally, we assess the efficacy of our low-frequency\nresponse for \"deep alerts\" by performing inspiral-only Bayesian inference."
                },
                "authors": [
                    {
                        "name": "Jorge Valencia"
                    },
                    {
                        "name": "Sascha Husa"
                    }
                ],
                "author_detail": {
                    "name": "Sascha Husa"
                },
                "author": "Sascha Husa",
                "arxiv_doi": "10.1103/lk4h-lz7y",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/lk4h-lz7y",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2505.09600v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09600v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "47 pages, 16 figures",
                "arxiv_journal_ref": "Phys. Rev. D 112, 044020 (2025)",
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10494v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10494v1",
                "updated": "2025-08-14T09:52:51Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    9,
                    52,
                    51,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T09:52:51Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    9,
                    52,
                    51,
                    3,
                    226,
                    0
                ],
                "title": "A Unified Multi-Agent Framework for Universal Multimodal Understanding\n  and Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Unified Multi-Agent Framework for Universal Multimodal Understanding\n  and Generation"
                },
                "summary": "Real-world multimodal applications often require any-to-any capabilities,\nenabling both understanding and generation across modalities including text,\nimage, audio, and video. However, integrating the strengths of autoregressive\nlanguage models (LLMs) for reasoning and diffusion models for high-fidelity\ngeneration remains challenging. Existing approaches rely on rigid pipelines or\ntightly coupled architectures, limiting flexibility and scalability. We propose\nMAGUS (Multi-Agent Guided Unified Multimodal System), a modular framework that\nunifies multimodal understanding and generation via two decoupled phases:\nCognition and Deliberation. MAGUS enables symbolic multi-agent collaboration\nwithin a shared textual workspace. In the Cognition phase, three\nrole-conditioned multimodal LLM agents - Perceiver, Planner, and Reflector -\nengage in collaborative dialogue to perform structured understanding and\nplanning. The Deliberation phase incorporates a Growth-Aware Search mechanism\nthat orchestrates LLM-based reasoning and diffusion-based generation in a\nmutually reinforcing manner. MAGUS supports plug-and-play extensibility,\nscalable any-to-any modality conversion, and semantic alignment - all without\nthe need for joint training. Experiments across multiple benchmarks, including\nimage, video, and audio generation, as well as cross-modal instruction\nfollowing, demonstrate that MAGUS outperforms strong baselines and\nstate-of-the-art systems. Notably, on the MME benchmark, MAGUS surpasses the\npowerful closed-source model GPT-4o.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-world multimodal applications often require any-to-any capabilities,\nenabling both understanding and generation across modalities including text,\nimage, audio, and video. However, integrating the strengths of autoregressive\nlanguage models (LLMs) for reasoning and diffusion models for high-fidelity\ngeneration remains challenging. Existing approaches rely on rigid pipelines or\ntightly coupled architectures, limiting flexibility and scalability. We propose\nMAGUS (Multi-Agent Guided Unified Multimodal System), a modular framework that\nunifies multimodal understanding and generation via two decoupled phases:\nCognition and Deliberation. MAGUS enables symbolic multi-agent collaboration\nwithin a shared textual workspace. In the Cognition phase, three\nrole-conditioned multimodal LLM agents - Perceiver, Planner, and Reflector -\nengage in collaborative dialogue to perform structured understanding and\nplanning. The Deliberation phase incorporates a Growth-Aware Search mechanism\nthat orchestrates LLM-based reasoning and diffusion-based generation in a\nmutually reinforcing manner. MAGUS supports plug-and-play extensibility,\nscalable any-to-any modality conversion, and semantic alignment - all without\nthe need for joint training. Experiments across multiple benchmarks, including\nimage, video, and audio generation, as well as cross-modal instruction\nfollowing, demonstrate that MAGUS outperforms strong baselines and\nstate-of-the-art systems. Notably, on the MME benchmark, MAGUS surpasses the\npowerful closed-source model GPT-4o."
                },
                "authors": [
                    {
                        "name": "Jiulin Li"
                    },
                    {
                        "name": "Ping Huang"
                    },
                    {
                        "name": "Yexin Li"
                    },
                    {
                        "name": "Shuo Chen"
                    },
                    {
                        "name": "Juewen Hu"
                    },
                    {
                        "name": "Ye Tian"
                    }
                ],
                "author_detail": {
                    "name": "Ye Tian"
                },
                "author": "Ye Tian",
                "arxiv_comment": "8 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10494v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10494v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10492v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10492v1",
                "updated": "2025-08-14T09:51:20Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    9,
                    51,
                    20,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T09:51:20Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    9,
                    51,
                    20,
                    3,
                    226,
                    0
                ],
                "title": "Reverse Physician-AI Relationship: Full-process Clinical Diagnosis\n  Driven by a Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reverse Physician-AI Relationship: Full-process Clinical Diagnosis\n  Driven by a Large Language Model"
                },
                "summary": "Full-process clinical diagnosis in the real world encompasses the entire\ndiagnostic workflow that begins with only an ambiguous chief complaint. While\nartificial intelligence (AI), particularly large language models (LLMs), is\ntransforming clinical diagnosis, its role remains largely as an assistant to\nphysicians. This AI-assisted working pattern makes AI can only answer specific\nmedical questions at certain parts within the diagnostic process, but lack the\nability to drive the entire diagnostic process starting from an ambiguous\ncomplaint, which still relies heavily on human physicians. This gap limits AI's\nability to fully reduce physicians' workload and enhance diagnostic efficiency.\nTo address this, we propose a paradigm shift that reverses the relationship\nbetween physicians and AI: repositioning AI as the primary director, with\nphysicians serving as its assistants. So we present DxDirector-7B, an LLM\nendowed with advanced deep thinking capabilities, enabling it to drive the\nfull-process diagnosis with minimal physician involvement. Furthermore,\nDxDirector-7B establishes a robust accountability framework for misdiagnoses,\ndelineating responsibility between AI and human physicians. In evaluations\nacross rare, complex, and real-world cases under full-process diagnosis\nsetting, DxDirector-7B not only achieves significant superior diagnostic\naccuracy but also substantially reduces physician workload than\nstate-of-the-art medical LLMs as well as general-purpose LLMs. Fine-grained\nanalyses across multiple clinical departments and tasks validate its efficacy,\nwith expert evaluations indicating its potential to serve as a viable\nsubstitute for medical specialists. These findings mark a new era where AI,\ntraditionally a physicians' assistant, now drives the entire diagnostic process\nto drastically reduce physicians' workload, indicating an efficient and\naccurate diagnostic solution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Full-process clinical diagnosis in the real world encompasses the entire\ndiagnostic workflow that begins with only an ambiguous chief complaint. While\nartificial intelligence (AI), particularly large language models (LLMs), is\ntransforming clinical diagnosis, its role remains largely as an assistant to\nphysicians. This AI-assisted working pattern makes AI can only answer specific\nmedical questions at certain parts within the diagnostic process, but lack the\nability to drive the entire diagnostic process starting from an ambiguous\ncomplaint, which still relies heavily on human physicians. This gap limits AI's\nability to fully reduce physicians' workload and enhance diagnostic efficiency.\nTo address this, we propose a paradigm shift that reverses the relationship\nbetween physicians and AI: repositioning AI as the primary director, with\nphysicians serving as its assistants. So we present DxDirector-7B, an LLM\nendowed with advanced deep thinking capabilities, enabling it to drive the\nfull-process diagnosis with minimal physician involvement. Furthermore,\nDxDirector-7B establishes a robust accountability framework for misdiagnoses,\ndelineating responsibility between AI and human physicians. In evaluations\nacross rare, complex, and real-world cases under full-process diagnosis\nsetting, DxDirector-7B not only achieves significant superior diagnostic\naccuracy but also substantially reduces physician workload than\nstate-of-the-art medical LLMs as well as general-purpose LLMs. Fine-grained\nanalyses across multiple clinical departments and tasks validate its efficacy,\nwith expert evaluations indicating its potential to serve as a viable\nsubstitute for medical specialists. These findings mark a new era where AI,\ntraditionally a physicians' assistant, now drives the entire diagnostic process\nto drastically reduce physicians' workload, indicating an efficient and\naccurate diagnostic solution."
                },
                "authors": [
                    {
                        "name": "Shicheng Xu"
                    },
                    {
                        "name": "Xin Huang"
                    },
                    {
                        "name": "Zihao Wei"
                    },
                    {
                        "name": "Liang Pang"
                    },
                    {
                        "name": "Huawei Shen"
                    },
                    {
                        "name": "Xueqi Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Xueqi Cheng"
                },
                "author": "Xueqi Cheng",
                "arxiv_comment": "39 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10492v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10492v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10486v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10486v1",
                "updated": "2025-08-14T09:41:55Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    9,
                    41,
                    55,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T09:41:55Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    9,
                    41,
                    55,
                    3,
                    226,
                    0
                ],
                "title": "SEQ-GPT: LLM-assisted Spatial Query via Example",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SEQ-GPT: LLM-assisted Spatial Query via Example"
                },
                "summary": "Contemporary spatial services such as online maps predominantly rely on user\nqueries for location searches. However, the user experience is limited when\nperforming complex tasks, such as searching for a group of locations\nsimultaneously. In this study, we examine the extended scenario known as\nSpatial Exemplar Query (SEQ), where multiple relevant locations are jointly\nsearched based on user-specified examples. We introduce SEQ-GPT, a spatial\nquery system powered by Large Language Models (LLMs) towards more versatile SEQ\nsearch using natural language. The language capabilities of LLMs enable unique\ninteractive operations in the SEQ process, including asking users to clarify\nquery details and dynamically adjusting the search based on user feedback. We\nalso propose a tailored LLM adaptation pipeline that aligns natural language\nwith structured spatial data and queries through dialogue synthesis and\nmulti-model cooperation. SEQ-GPT offers an end-to-end demonstration for\nbroadening spatial search with realistic data and application scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contemporary spatial services such as online maps predominantly rely on user\nqueries for location searches. However, the user experience is limited when\nperforming complex tasks, such as searching for a group of locations\nsimultaneously. In this study, we examine the extended scenario known as\nSpatial Exemplar Query (SEQ), where multiple relevant locations are jointly\nsearched based on user-specified examples. We introduce SEQ-GPT, a spatial\nquery system powered by Large Language Models (LLMs) towards more versatile SEQ\nsearch using natural language. The language capabilities of LLMs enable unique\ninteractive operations in the SEQ process, including asking users to clarify\nquery details and dynamically adjusting the search based on user feedback. We\nalso propose a tailored LLM adaptation pipeline that aligns natural language\nwith structured spatial data and queries through dialogue synthesis and\nmulti-model cooperation. SEQ-GPT offers an end-to-end demonstration for\nbroadening spatial search with realistic data and application scenarios."
                },
                "authors": [
                    {
                        "name": "Ivan Khai Ze Lim"
                    },
                    {
                        "name": "Ningyi Liao"
                    },
                    {
                        "name": "Yiming Yang"
                    },
                    {
                        "name": "Gerald Wei Yong Yip"
                    },
                    {
                        "name": "Siqiang Luo"
                    }
                ],
                "author_detail": {
                    "name": "Siqiang Luo"
                },
                "author": "Siqiang Luo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10486v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10486v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08307v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08307v3",
                "updated": "2025-08-14T09:36:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    9,
                    36,
                    33,
                    3,
                    226,
                    0
                ],
                "published": "2025-07-11T04:48:12Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    4,
                    48,
                    12,
                    4,
                    192,
                    0
                ],
                "title": "M2DAO-Talker: Harmonizing Multi-granular Motion Decoupling and\n  Alternating Optimization for Talking-head Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "M2DAO-Talker: Harmonizing Multi-granular Motion Decoupling and\n  Alternating Optimization for Talking-head Generation"
                },
                "summary": "Audio-driven talking head generation holds significant potential for film\nproduction. While existing 3D methods have advanced motion modeling and content\nsynthesis, they often produce rendering artifacts, such as motion blur,\ntemporal jitter, and local penetration, due to limitations in representing\nstable, fine-grained motion fields. Through systematic analysis, we reformulate\ntalking head generation into a unified framework comprising three steps: video\npreprocessing, motion representation, and rendering reconstruction. This\nframework underpins our proposed M2DAO-Talker, which addresses current\nlimitations via multi-granular motion decoupling and alternating optimization.\nSpecifically, we devise a novel 2D portrait preprocessing pipeline to extract\nframe-wise deformation control conditions (motion region segmentation masks,\nand camera parameters) to facilitate motion representation. To ameliorate\nmotion modeling, we elaborate a multi-granular motion decoupling strategy,\nwhich independently models non-rigid (oral and facial) and rigid (head) motions\nfor improved reconstruction accuracy. Meanwhile, a motion consistency\nconstraint is developed to ensure head-torso kinematic consistency, thereby\nmitigating penetration artifacts caused by motion aliasing. In addition, an\nalternating optimization strategy is designed to iteratively refine facial and\noral motion parameters, enabling more realistic video generation. Experiments\nacross multiple datasets show that M2DAO-Talker achieves state-of-the-art\nperformance, with the 2.43 dB PSNR improvement in generation quality and 0.64\ngain in user-evaluated video realness versus TalkingGaussian while with 150 FPS\ninference speed. Our project homepage is\nhttps://m2dao-talker.github.io/M2DAO-Talk.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Audio-driven talking head generation holds significant potential for film\nproduction. While existing 3D methods have advanced motion modeling and content\nsynthesis, they often produce rendering artifacts, such as motion blur,\ntemporal jitter, and local penetration, due to limitations in representing\nstable, fine-grained motion fields. Through systematic analysis, we reformulate\ntalking head generation into a unified framework comprising three steps: video\npreprocessing, motion representation, and rendering reconstruction. This\nframework underpins our proposed M2DAO-Talker, which addresses current\nlimitations via multi-granular motion decoupling and alternating optimization.\nSpecifically, we devise a novel 2D portrait preprocessing pipeline to extract\nframe-wise deformation control conditions (motion region segmentation masks,\nand camera parameters) to facilitate motion representation. To ameliorate\nmotion modeling, we elaborate a multi-granular motion decoupling strategy,\nwhich independently models non-rigid (oral and facial) and rigid (head) motions\nfor improved reconstruction accuracy. Meanwhile, a motion consistency\nconstraint is developed to ensure head-torso kinematic consistency, thereby\nmitigating penetration artifacts caused by motion aliasing. In addition, an\nalternating optimization strategy is designed to iteratively refine facial and\noral motion parameters, enabling more realistic video generation. Experiments\nacross multiple datasets show that M2DAO-Talker achieves state-of-the-art\nperformance, with the 2.43 dB PSNR improvement in generation quality and 0.64\ngain in user-evaluated video realness versus TalkingGaussian while with 150 FPS\ninference speed. Our project homepage is\nhttps://m2dao-talker.github.io/M2DAO-Talk.github.io."
                },
                "authors": [
                    {
                        "name": "Kui Jiang"
                    },
                    {
                        "name": "Shiyu Liu"
                    },
                    {
                        "name": "Junjun Jiang"
                    },
                    {
                        "name": "Hongxun Yao"
                    },
                    {
                        "name": "Xiaopeng Fan"
                    }
                ],
                "author_detail": {
                    "name": "Xiaopeng Fan"
                },
                "author": "Xiaopeng Fan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08307v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08307v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10480v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10480v1",
                "updated": "2025-08-14T09:32:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    9,
                    32,
                    9,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T09:32:09Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    9,
                    32,
                    9,
                    3,
                    226,
                    0
                ],
                "title": "Pinet: Optimizing hard-constrained neural networks with orthogonal\n  projection layers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pinet: Optimizing hard-constrained neural networks with orthogonal\n  projection layers"
                },
                "summary": "We introduce an output layer for neural networks that ensures satisfaction of\nconvex constraints. Our approach, $\\Pi$net, leverages operator splitting for\nrapid and reliable projections in the forward pass, and the implicit function\ntheorem for backpropagation. We deploy $\\Pi$net as a feasible-by-design\noptimization proxy for parametric constrained optimization problems and obtain\nmodest-accuracy solutions faster than traditional solvers when solving a single\nproblem, and significantly faster for a batch of problems. We surpass\nstate-of-the-art learning approaches in terms of training time, solution\nquality, and robustness to hyperparameter tuning, while maintaining similar\ninference times. Finally, we tackle multi-vehicle motion planning with\nnon-convex trajectory preferences and provide $\\Pi$net as a GPU-ready package\nimplemented in JAX with effective tuning heuristics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce an output layer for neural networks that ensures satisfaction of\nconvex constraints. Our approach, $\\Pi$net, leverages operator splitting for\nrapid and reliable projections in the forward pass, and the implicit function\ntheorem for backpropagation. We deploy $\\Pi$net as a feasible-by-design\noptimization proxy for parametric constrained optimization problems and obtain\nmodest-accuracy solutions faster than traditional solvers when solving a single\nproblem, and significantly faster for a batch of problems. We surpass\nstate-of-the-art learning approaches in terms of training time, solution\nquality, and robustness to hyperparameter tuning, while maintaining similar\ninference times. Finally, we tackle multi-vehicle motion planning with\nnon-convex trajectory preferences and provide $\\Pi$net as a GPU-ready package\nimplemented in JAX with effective tuning heuristics."
                },
                "authors": [
                    {
                        "name": "Panagiotis D. Grontas"
                    },
                    {
                        "name": "Antonio Terpin"
                    },
                    {
                        "name": "Efe C. Balta"
                    },
                    {
                        "name": "Raffaello D'Andrea"
                    },
                    {
                        "name": "John Lygeros"
                    }
                ],
                "author_detail": {
                    "name": "John Lygeros"
                },
                "author": "John Lygeros",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10480v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10480v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10478v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10478v1",
                "updated": "2025-08-14T09:28:49Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    9,
                    28,
                    49,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T09:28:49Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    9,
                    28,
                    49,
                    3,
                    226,
                    0
                ],
                "title": "Semantic IDs for Joint Generative Search and Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic IDs for Joint Generative Search and Recommendation"
                },
                "summary": "Generative models powered by Large Language Models (LLMs) are emerging as a\nunified solution for powering both recommendation and search tasks. A key\ndesign choice in these models is how to represent items, traditionally through\nunique identifiers (IDs) and more recently with Semantic IDs composed of\ndiscrete codes, obtained from embeddings. While task-specific embedding models\ncan improve performance for individual tasks, they may not generalize well in a\njoint setting. In this paper, we explore how to construct Semantic IDs that\nperform well both in search and recommendation when using a unified model. We\ncompare a range of strategies to construct Semantic IDs, looking into\ntask-specific and cross-tasks approaches, and also whether each task should\nhave its own semantic ID tokens in a joint search and recommendation generative\nmodel. Our results show that using a bi-encoder model fine-tuned on both search\nand recommendation tasks to obtain item embeddings, followed by the\nconstruction of a unified Semantic ID space provides an effective trade-off,\nenabling strong performance in both tasks. We hope these findings spark\nfollow-up work on generalisable, semantically grounded ID schemes and inform\nthe next wave of unified generative recommender architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative models powered by Large Language Models (LLMs) are emerging as a\nunified solution for powering both recommendation and search tasks. A key\ndesign choice in these models is how to represent items, traditionally through\nunique identifiers (IDs) and more recently with Semantic IDs composed of\ndiscrete codes, obtained from embeddings. While task-specific embedding models\ncan improve performance for individual tasks, they may not generalize well in a\njoint setting. In this paper, we explore how to construct Semantic IDs that\nperform well both in search and recommendation when using a unified model. We\ncompare a range of strategies to construct Semantic IDs, looking into\ntask-specific and cross-tasks approaches, and also whether each task should\nhave its own semantic ID tokens in a joint search and recommendation generative\nmodel. Our results show that using a bi-encoder model fine-tuned on both search\nand recommendation tasks to obtain item embeddings, followed by the\nconstruction of a unified Semantic ID space provides an effective trade-off,\nenabling strong performance in both tasks. We hope these findings spark\nfollow-up work on generalisable, semantically grounded ID schemes and inform\nthe next wave of unified generative recommender architectures."
                },
                "authors": [
                    {
                        "name": "Gustavo Penha"
                    },
                    {
                        "name": "Edoardo D'Amico"
                    },
                    {
                        "name": "Marco De Nadai"
                    },
                    {
                        "name": "Enrico Palumbo"
                    },
                    {
                        "name": "Alexandre Tamborrino"
                    },
                    {
                        "name": "Ali Vardasbi"
                    },
                    {
                        "name": "Max Lefarov"
                    },
                    {
                        "name": "Shawn Lin"
                    },
                    {
                        "name": "Timothy Heath"
                    },
                    {
                        "name": "Francesco Fabbri"
                    },
                    {
                        "name": "Hugues Bouchard"
                    }
                ],
                "author_detail": {
                    "name": "Hugues Bouchard"
                },
                "author": "Hugues Bouchard",
                "arxiv_comment": "Accepted for publication in the 19th ACM Conference on Recommender\n  Systems (RecSys 2025), Late-Breaking Results track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10478v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10478v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10467v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10467v1",
                "updated": "2025-08-14T09:08:50Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    9,
                    8,
                    50,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T09:08:50Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    9,
                    8,
                    50,
                    3,
                    226,
                    0
                ],
                "title": "FIRESPARQL: A LLM-based Framework for SPARQL Query Generation over\n  Scholarly Knowledge Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FIRESPARQL: A LLM-based Framework for SPARQL Query Generation over\n  Scholarly Knowledge Graphs"
                },
                "summary": "Question answering over Scholarly Knowledge Graphs (SKGs) remains a\nchallenging task due to the complexity of scholarly content and the intricate\nstructure of these graphs. Large Language Model (LLM) approaches could be used\nto translate natural language questions (NLQs) into SPARQL queries; however,\nthese LLM-based approaches struggle with SPARQL query generation due to limited\nexposure to SKG-specific content and the underlying schema. We identified two\nmain types of errors in the LLM-generated SPARQL queries: (i) structural\ninconsistencies, such as missing or redundant triples in the queries, and (ii)\nsemantic inaccuracies, where incorrect entities or properties are shown in the\nqueries despite a correct query structure. To address these issues, we propose\nFIRESPARQL, a modular framework that supports fine-tuned LLMs as a core\ncomponent, with optional context provided via retrieval-augmented generation\n(RAG) and a SPARQL query correction layer. We evaluate the framework on the\nSciQA Benchmark using various configurations (zero-shot, zero-shot with RAG,\none-shot, fine-tuning, and fine-tuning with RAG) and compare the performance\nwith baseline and state-of-the-art approaches. We measure query accuracy using\nBLEU and ROUGE metrics, and query result accuracy using relaxed exact\nmatch(RelaxedEM), with respect to the gold standards containing the NLQs,\nSPARQL queries, and the results of the queries. Experimental results\ndemonstrate that fine-tuning achieves the highest overall performance, reaching\n0.90 ROUGE-L for query accuracy and 0.85 RelaxedEM for result accuracy on the\ntest set.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Question answering over Scholarly Knowledge Graphs (SKGs) remains a\nchallenging task due to the complexity of scholarly content and the intricate\nstructure of these graphs. Large Language Model (LLM) approaches could be used\nto translate natural language questions (NLQs) into SPARQL queries; however,\nthese LLM-based approaches struggle with SPARQL query generation due to limited\nexposure to SKG-specific content and the underlying schema. We identified two\nmain types of errors in the LLM-generated SPARQL queries: (i) structural\ninconsistencies, such as missing or redundant triples in the queries, and (ii)\nsemantic inaccuracies, where incorrect entities or properties are shown in the\nqueries despite a correct query structure. To address these issues, we propose\nFIRESPARQL, a modular framework that supports fine-tuned LLMs as a core\ncomponent, with optional context provided via retrieval-augmented generation\n(RAG) and a SPARQL query correction layer. We evaluate the framework on the\nSciQA Benchmark using various configurations (zero-shot, zero-shot with RAG,\none-shot, fine-tuning, and fine-tuning with RAG) and compare the performance\nwith baseline and state-of-the-art approaches. We measure query accuracy using\nBLEU and ROUGE metrics, and query result accuracy using relaxed exact\nmatch(RelaxedEM), with respect to the gold standards containing the NLQs,\nSPARQL queries, and the results of the queries. Experimental results\ndemonstrate that fine-tuning achieves the highest overall performance, reaching\n0.90 ROUGE-L for query accuracy and 0.85 RelaxedEM for result accuracy on the\ntest set."
                },
                "authors": [
                    {
                        "name": "Xueli Pan"
                    },
                    {
                        "name": "Victor de Boer"
                    },
                    {
                        "name": "Jacco van Ossenbruggen"
                    }
                ],
                "author_detail": {
                    "name": "Jacco van Ossenbruggen"
                },
                "author": "Jacco van Ossenbruggen",
                "arxiv_comment": "Accepted at 17th International Joint Conference on Knowledge\n  Discovery, Knowledge Engineering and Knowledge Management (IC3K)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10467v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10467v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12188v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12188v3",
                "updated": "2025-08-14T09:07:37Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    9,
                    7,
                    37,
                    3,
                    226,
                    0
                ],
                "published": "2025-02-15T08:04:00Z",
                "published_parsed": [
                    2025,
                    2,
                    15,
                    8,
                    4,
                    0,
                    5,
                    46,
                    0
                ],
                "title": "Boosting Cross-problem Generalization in Diffusion-Based Neural\n  Combinatorial Solver via Inference Time Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Boosting Cross-problem Generalization in Diffusion-Based Neural\n  Combinatorial Solver via Inference Time Adaptation"
                },
                "summary": "Diffusion-based Neural Combinatorial Optimization (NCO) has demonstrated\neffectiveness in solving NP-complete (NPC) problems by learning discrete\ndiffusion models for solution generation, eliminating hand-crafted domain\nknowledge. Despite their success, existing NCO methods face significant\nchallenges in both cross-scale and cross-problem generalization, and high\ntraining costs compared to traditional solvers. While recent studies on\ndiffusion models have introduced training-free guidance approaches that\nleverage pre-defined guidance functions for conditional generation, such\nmethodologies have not been extensively explored in combinatorial optimization.\nTo bridge this gap, we propose a training-free inference time adaptation\nframework (DIFU-Ada) that enables both the zero-shot cross-problem transfer and\ncross-scale generalization capabilities of diffusion-based NCO solvers without\nrequiring additional training. We provide theoretical analysis that helps\nunderstanding the cross-problem transfer capability. Our experimental results\ndemonstrate that a diffusion solver, trained exclusively on the Traveling\nSalesman Problem (TSP), can achieve competitive zero-shot transfer performance\nacross different problem scales on TSP variants, such as Prize Collecting TSP\n(PCTSP) and the Orienteering Problem (OP), through inference time adaptation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based Neural Combinatorial Optimization (NCO) has demonstrated\neffectiveness in solving NP-complete (NPC) problems by learning discrete\ndiffusion models for solution generation, eliminating hand-crafted domain\nknowledge. Despite their success, existing NCO methods face significant\nchallenges in both cross-scale and cross-problem generalization, and high\ntraining costs compared to traditional solvers. While recent studies on\ndiffusion models have introduced training-free guidance approaches that\nleverage pre-defined guidance functions for conditional generation, such\nmethodologies have not been extensively explored in combinatorial optimization.\nTo bridge this gap, we propose a training-free inference time adaptation\nframework (DIFU-Ada) that enables both the zero-shot cross-problem transfer and\ncross-scale generalization capabilities of diffusion-based NCO solvers without\nrequiring additional training. We provide theoretical analysis that helps\nunderstanding the cross-problem transfer capability. Our experimental results\ndemonstrate that a diffusion solver, trained exclusively on the Traveling\nSalesman Problem (TSP), can achieve competitive zero-shot transfer performance\nacross different problem scales on TSP variants, such as Prize Collecting TSP\n(PCTSP) and the Orienteering Problem (OP), through inference time adaptation."
                },
                "authors": [
                    {
                        "name": "Haoyu Lei"
                    },
                    {
                        "name": "Kaiwen Zhou"
                    },
                    {
                        "name": "Yinchuan Li"
                    },
                    {
                        "name": "Zhitang Chen"
                    },
                    {
                        "name": "Farzan Farnia"
                    }
                ],
                "author_detail": {
                    "name": "Farzan Farnia"
                },
                "author": "Farzan Farnia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12188v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12188v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08895v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08895v2",
                "updated": "2025-08-14T09:04:56Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    9,
                    4,
                    56,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-12T12:35:55Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    12,
                    35,
                    55,
                    1,
                    224,
                    0
                ],
                "title": "ASPD: Unlocking Adaptive Serial-Parallel Decoding by Exploring Intrinsic\n  Parallelism in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASPD: Unlocking Adaptive Serial-Parallel Decoding by Exploring Intrinsic\n  Parallelism in LLMs"
                },
                "summary": "The increasing scale and complexity of large language models (LLMs) pose\nsignificant inference latency challenges, primarily due to their autoregressive\ndecoding paradigm characterized by the sequential nature of next-token\nprediction. By re-examining the outputs of autoregressive models, we observed\nthat some segments exhibit parallelizable structures, which we term intrinsic\nparallelism. Decoding each parallelizable branch simultaneously (i.e. parallel\ndecoding) can significantly improve the overall inference speed of LLMs. In\nthis paper, we propose an Adaptive Serial-Parallel Decoding (ASPD), which\naddresses two core challenges: automated construction of parallelizable data\nand efficient parallel decoding mechanism. More specifically, we introduce a\nnon-invasive pipeline that automatically extracts and validates parallelizable\nstructures from the responses of autoregressive models. To empower efficient\nadaptive serial-parallel decoding, we implement a Hybrid Decoding Engine which\nenables seamless transitions between serial and parallel decoding modes while\nmaintaining a reusable KV cache, maximizing computational efficiency. Extensive\nevaluations across General Tasks, Retrieval-Augmented Generation, Mathematical\nReasoning, demonstrate that ASPD achieves unprecedented performance in both\neffectiveness and efficiency. Notably, on Vicuna Bench, our method achieves up\nto 3.19x speedup (1.85x on average) while maintaining response quality within\n1% difference compared to autoregressive models, realizing significant\nacceleration without compromising generation quality. Our framework sets a\ngroundbreaking benchmark for efficient LLM parallel inference, paving the way\nfor its deployment in latency-sensitive applications such as AI-powered\ncustomer service bots and answer retrieval engines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing scale and complexity of large language models (LLMs) pose\nsignificant inference latency challenges, primarily due to their autoregressive\ndecoding paradigm characterized by the sequential nature of next-token\nprediction. By re-examining the outputs of autoregressive models, we observed\nthat some segments exhibit parallelizable structures, which we term intrinsic\nparallelism. Decoding each parallelizable branch simultaneously (i.e. parallel\ndecoding) can significantly improve the overall inference speed of LLMs. In\nthis paper, we propose an Adaptive Serial-Parallel Decoding (ASPD), which\naddresses two core challenges: automated construction of parallelizable data\nand efficient parallel decoding mechanism. More specifically, we introduce a\nnon-invasive pipeline that automatically extracts and validates parallelizable\nstructures from the responses of autoregressive models. To empower efficient\nadaptive serial-parallel decoding, we implement a Hybrid Decoding Engine which\nenables seamless transitions between serial and parallel decoding modes while\nmaintaining a reusable KV cache, maximizing computational efficiency. Extensive\nevaluations across General Tasks, Retrieval-Augmented Generation, Mathematical\nReasoning, demonstrate that ASPD achieves unprecedented performance in both\neffectiveness and efficiency. Notably, on Vicuna Bench, our method achieves up\nto 3.19x speedup (1.85x on average) while maintaining response quality within\n1% difference compared to autoregressive models, realizing significant\nacceleration without compromising generation quality. Our framework sets a\ngroundbreaking benchmark for efficient LLM parallel inference, paving the way\nfor its deployment in latency-sensitive applications such as AI-powered\ncustomer service bots and answer retrieval engines."
                },
                "authors": [
                    {
                        "name": "Keyu Chen"
                    },
                    {
                        "name": "Zhifeng Shen"
                    },
                    {
                        "name": "Daohai Yu"
                    },
                    {
                        "name": "Haoqian Wu"
                    },
                    {
                        "name": "Wei Wen"
                    },
                    {
                        "name": "Jianfeng He"
                    },
                    {
                        "name": "Ruizhi Qiao"
                    },
                    {
                        "name": "Xing Sun"
                    }
                ],
                "author_detail": {
                    "name": "Xing Sun"
                },
                "author": "Xing Sun",
                "arxiv_comment": "20 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08895v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08895v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10461v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10461v1",
                "updated": "2025-08-14T09:00:45Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    9,
                    0,
                    45,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T09:00:45Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    9,
                    0,
                    45,
                    3,
                    226,
                    0
                ],
                "title": "X-Node: Self-Explanation is All We Need",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "X-Node: Self-Explanation is All We Need"
                },
                "summary": "Graph neural networks (GNNs) have achieved state-of-the-art results in\ncomputer vision and medical image classification tasks by capturing structural\ndependencies across data instances. However, their decision-making remains\nlargely opaque, limiting their trustworthiness in high-stakes clinical\napplications where interpretability is essential. Existing explainability\ntechniques for GNNs are typically post-hoc and global, offering limited insight\ninto individual node decisions or local reasoning. We introduce X-Node, a\nself-explaining GNN framework in which each node generates its own explanation\nas part of the prediction process. For every node, we construct a structured\ncontext vector encoding interpretable cues such as degree, centrality,\nclustering, feature saliency, and label agreement within its local topology. A\nlightweight Reasoner module maps this context into a compact explanation\nvector, which serves three purposes: (1) reconstructing the node's latent\nembedding via a decoder to enforce faithfulness, (2) generating a natural\nlanguage explanation using a pre-trained LLM (e.g., Grok or Gemini), and (3)\nguiding the GNN itself via a \"text-injection\" mechanism that feeds explanations\nback into the message-passing pipeline. We evaluate X-Node on two graph\ndatasets derived from MedMNIST and MorphoMNIST, integrating it with GCN, GAT,\nand GIN backbones. Our results show that X-Node maintains competitive\nclassification accuracy while producing faithful, per-node explanations.\nRepository: https://github.com/basiralab/X-Node.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph neural networks (GNNs) have achieved state-of-the-art results in\ncomputer vision and medical image classification tasks by capturing structural\ndependencies across data instances. However, their decision-making remains\nlargely opaque, limiting their trustworthiness in high-stakes clinical\napplications where interpretability is essential. Existing explainability\ntechniques for GNNs are typically post-hoc and global, offering limited insight\ninto individual node decisions or local reasoning. We introduce X-Node, a\nself-explaining GNN framework in which each node generates its own explanation\nas part of the prediction process. For every node, we construct a structured\ncontext vector encoding interpretable cues such as degree, centrality,\nclustering, feature saliency, and label agreement within its local topology. A\nlightweight Reasoner module maps this context into a compact explanation\nvector, which serves three purposes: (1) reconstructing the node's latent\nembedding via a decoder to enforce faithfulness, (2) generating a natural\nlanguage explanation using a pre-trained LLM (e.g., Grok or Gemini), and (3)\nguiding the GNN itself via a \"text-injection\" mechanism that feeds explanations\nback into the message-passing pipeline. We evaluate X-Node on two graph\ndatasets derived from MedMNIST and MorphoMNIST, integrating it with GCN, GAT,\nand GIN backbones. Our results show that X-Node maintains competitive\nclassification accuracy while producing faithful, per-node explanations.\nRepository: https://github.com/basiralab/X-Node."
                },
                "authors": [
                    {
                        "name": "Prajit Sengupta"
                    },
                    {
                        "name": "Islem Rekik"
                    }
                ],
                "author_detail": {
                    "name": "Islem Rekik"
                },
                "author": "Islem Rekik",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10461v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10461v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07321v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07321v2",
                "updated": "2025-08-14T09:00:40Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    9,
                    0,
                    40,
                    3,
                    226,
                    0
                ],
                "published": "2024-08-14T06:43:06Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    6,
                    43,
                    6,
                    2,
                    227,
                    0
                ],
                "title": "VERCATION: Precise Vulnerable Open-source Software Version\n  Identification based on Static Analysis and LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VERCATION: Precise Vulnerable Open-source Software Version\n  Identification based on Static Analysis and LLM"
                },
                "summary": "Open-source software (OSS) has experienced a surge in popularity, attributed\nto its collaborative development model and cost-effective nature. However, the\nadoption of specific software versions in development projects may introduce\nsecurity risks when these versions bring along vulnerabilities. Current methods\nof identifying vulnerable versions typically analyze and extract the code\nfeatures involved in vulnerability patches using static analysis with\npre-defined rules. They then use code clone detection to identify the\nvulnerable versions. These methods are hindered by imprecision due to (1) the\nexclusion of vulnerability-irrelevant code in the analysis and (2) the\ninadequacy of code clone detection. This paper presents VERCATION, an approach\ndesigned to identify vulnerable versions of OSS written in C/C++. VERCATION\ncombines program slicing with a Large Language Model (LLM) to identify\nvulnerability-relevant code from vulnerability patches. It then backtracks\nhistorical commits to gather previous modifications of identified\nvulnerability-relevant code. We propose code clone detection based on expanded\nand normalized ASTs to compare the differences between pre-modification and\npost-modification code, thereby locating the vulnerability-introducing commit\n(vic) and enabling the identification of the vulnerable versions between the\nvulnerability-fixing commit and the vic. We curate a dataset linking 122 OSS\nvulnerabilities and 1,211 versions to evaluate VERCATION. On this dataset, our\napproach achieves an F1 score of 93.1%, outperforming current state-of-the-art\nmethods. More importantly, VERCATION detected 202 incorrect vulnerable OSS\nversions in NVD reports.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-source software (OSS) has experienced a surge in popularity, attributed\nto its collaborative development model and cost-effective nature. However, the\nadoption of specific software versions in development projects may introduce\nsecurity risks when these versions bring along vulnerabilities. Current methods\nof identifying vulnerable versions typically analyze and extract the code\nfeatures involved in vulnerability patches using static analysis with\npre-defined rules. They then use code clone detection to identify the\nvulnerable versions. These methods are hindered by imprecision due to (1) the\nexclusion of vulnerability-irrelevant code in the analysis and (2) the\ninadequacy of code clone detection. This paper presents VERCATION, an approach\ndesigned to identify vulnerable versions of OSS written in C/C++. VERCATION\ncombines program slicing with a Large Language Model (LLM) to identify\nvulnerability-relevant code from vulnerability patches. It then backtracks\nhistorical commits to gather previous modifications of identified\nvulnerability-relevant code. We propose code clone detection based on expanded\nand normalized ASTs to compare the differences between pre-modification and\npost-modification code, thereby locating the vulnerability-introducing commit\n(vic) and enabling the identification of the vulnerable versions between the\nvulnerability-fixing commit and the vic. We curate a dataset linking 122 OSS\nvulnerabilities and 1,211 versions to evaluate VERCATION. On this dataset, our\napproach achieves an F1 score of 93.1%, outperforming current state-of-the-art\nmethods. More importantly, VERCATION detected 202 incorrect vulnerable OSS\nversions in NVD reports."
                },
                "authors": [
                    {
                        "name": "Yiran Cheng"
                    },
                    {
                        "name": "Ting Zhang"
                    },
                    {
                        "name": "Lwin Khin Shar"
                    },
                    {
                        "name": "Shouguo Yang"
                    },
                    {
                        "name": "Chaopeng Dong"
                    },
                    {
                        "name": "David Lo"
                    },
                    {
                        "name": "Shichao Lv"
                    },
                    {
                        "name": "Zhiqiang Shi"
                    },
                    {
                        "name": "Limin Sun"
                    }
                ],
                "author_detail": {
                    "name": "Limin Sun"
                },
                "author": "Limin Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07321v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07321v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10460v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10460v1",
                "updated": "2025-08-14T09:00:37Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    9,
                    0,
                    37,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T09:00:37Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    9,
                    0,
                    37,
                    3,
                    226,
                    0
                ],
                "title": "Efficient Methods for Accurate Sparse Trajectory Recovery and Map\n  Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Methods for Accurate Sparse Trajectory Recovery and Map\n  Matching"
                },
                "summary": "Real-world trajectories are often sparse with low-sampling rates (i.e., long\nintervals between consecutive GPS points) and misaligned with road networks,\nyet many applications demand high-quality data for optimal performance. To\nimprove data quality with sparse trajectories as input, we systematically study\ntwo related research problems: trajectory recovery on road network, which aims\nto infer missing points to recover high-sampling trajectories, and map\nmatching, which aims to map GPS points to road segments to determine underlying\nroutes. In this paper, we present efficient methods TRMMA and MMA for accurate\ntrajectory recovery and map matching, respectively, where MMA serves as the\nfirst step of TRMMA. In MMA, we carefully formulate a classification task to\nmap a GPS point from sparse trajectories to a road segment over a small\ncandidate segment set, rather than the entire road network. We develop\ntechniques in MMA to generate effective embeddings that capture the patterns of\nGPS data, directional information, and road segments, to accurately align\nsparse trajectories to routes. For trajectory recovery, TRMMA focuses on the\nsegments in the route returned by MMA to infer missing points with position\nratios on road segments, producing high-sampling trajectories efficiently by\navoiding evaluation of all road segments. Specifically, in TRMMA, we design a\ndual-transformer encoding process to cohesively capture latent patterns in\ntrajectories and routes, and an effective decoding technique to sequentially\npredict the position ratios and road segments of missing points. We conduct\nextensive experiments to compare TRMMA and MMA with numerous existing methods\nfor trajectory recovery and map matching, respectively, on 4 large real-world\ndatasets. TRMMA and MMA consistently achieve the best result quality, often by\na significant margin.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-world trajectories are often sparse with low-sampling rates (i.e., long\nintervals between consecutive GPS points) and misaligned with road networks,\nyet many applications demand high-quality data for optimal performance. To\nimprove data quality with sparse trajectories as input, we systematically study\ntwo related research problems: trajectory recovery on road network, which aims\nto infer missing points to recover high-sampling trajectories, and map\nmatching, which aims to map GPS points to road segments to determine underlying\nroutes. In this paper, we present efficient methods TRMMA and MMA for accurate\ntrajectory recovery and map matching, respectively, where MMA serves as the\nfirst step of TRMMA. In MMA, we carefully formulate a classification task to\nmap a GPS point from sparse trajectories to a road segment over a small\ncandidate segment set, rather than the entire road network. We develop\ntechniques in MMA to generate effective embeddings that capture the patterns of\nGPS data, directional information, and road segments, to accurately align\nsparse trajectories to routes. For trajectory recovery, TRMMA focuses on the\nsegments in the route returned by MMA to infer missing points with position\nratios on road segments, producing high-sampling trajectories efficiently by\navoiding evaluation of all road segments. Specifically, in TRMMA, we design a\ndual-transformer encoding process to cohesively capture latent patterns in\ntrajectories and routes, and an effective decoding technique to sequentially\npredict the position ratios and road segments of missing points. We conduct\nextensive experiments to compare TRMMA and MMA with numerous existing methods\nfor trajectory recovery and map matching, respectively, on 4 large real-world\ndatasets. TRMMA and MMA consistently achieve the best result quality, often by\na significant margin."
                },
                "authors": [
                    {
                        "name": "Wei Tian"
                    },
                    {
                        "name": "Jieming Shi"
                    },
                    {
                        "name": "Man Lung Yiu"
                    }
                ],
                "author_detail": {
                    "name": "Man Lung Yiu"
                },
                "author": "Man Lung Yiu",
                "arxiv_doi": "10.1109/ICDE65448.2025.00034",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/ICDE65448.2025.00034",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.10460v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10460v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "13 pages, accepted by 2025 IEEE 41st International Conference on Data\n  Engineering (ICDE)",
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23714v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23714v2",
                "updated": "2025-08-14T08:59:56Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    8,
                    59,
                    56,
                    3,
                    226,
                    0
                ],
                "published": "2025-03-31T04:28:38Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    4,
                    28,
                    38,
                    0,
                    90,
                    0
                ],
                "title": "Building Instruction-Tuning Datasets from Human-Written Instructions\n  with Open-Weight Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building Instruction-Tuning Datasets from Human-Written Instructions\n  with Open-Weight Large Language Models"
                },
                "summary": "Instruction tuning is crucial for enabling Large Language Models (LLMs) to\nsolve real-world tasks. Prior work has shown the effectiveness of\ninstruction-tuning data synthesized solely from LLMs, raising a fundamental\nquestion: Do we still need human-originated signals for instruction tuning?\nThis work answers the question affirmatively: we build state-of-the-art\ninstruction-tuning datasets sourced from human-written instructions, by simply\npairing them with LLM-generated responses. LLMs fine-tuned on our datasets\nconsistently outperform those fine-tuned on existing ones. Our data\nconstruction approach can be easily adapted to other languages; we build\ndatasets for Japanese and confirm that LLMs tuned with our data reach\nstate-of-the-art performance. Analyses suggest that instruction-tuning in a new\nlanguage allows LLMs to follow instructions, while the tuned models exhibit a\nnotable lack of culture-specific knowledge in that language. The datasets and\nfine-tuned models will be publicly available. Our datasets, synthesized with\nopen-weight LLMs, are openly distributed under permissive licenses, allowing\nfor diverse use cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction tuning is crucial for enabling Large Language Models (LLMs) to\nsolve real-world tasks. Prior work has shown the effectiveness of\ninstruction-tuning data synthesized solely from LLMs, raising a fundamental\nquestion: Do we still need human-originated signals for instruction tuning?\nThis work answers the question affirmatively: we build state-of-the-art\ninstruction-tuning datasets sourced from human-written instructions, by simply\npairing them with LLM-generated responses. LLMs fine-tuned on our datasets\nconsistently outperform those fine-tuned on existing ones. Our data\nconstruction approach can be easily adapted to other languages; we build\ndatasets for Japanese and confirm that LLMs tuned with our data reach\nstate-of-the-art performance. Analyses suggest that instruction-tuning in a new\nlanguage allows LLMs to follow instructions, while the tuned models exhibit a\nnotable lack of culture-specific knowledge in that language. The datasets and\nfine-tuned models will be publicly available. Our datasets, synthesized with\nopen-weight LLMs, are openly distributed under permissive licenses, allowing\nfor diverse use cases."
                },
                "authors": [
                    {
                        "name": "Youmi Ma"
                    },
                    {
                        "name": "Sakae Mizuki"
                    },
                    {
                        "name": "Kazuki Fujii"
                    },
                    {
                        "name": "Taishi Nakamura"
                    },
                    {
                        "name": "Masanari Ohi"
                    },
                    {
                        "name": "Hinari Shimada"
                    },
                    {
                        "name": "Taihei Shiotani"
                    },
                    {
                        "name": "Koshiro Saito"
                    },
                    {
                        "name": "Koki Maeda"
                    },
                    {
                        "name": "Kakeru Hattori"
                    },
                    {
                        "name": "Takumi Okamoto"
                    },
                    {
                        "name": "Shigeki Ishida"
                    },
                    {
                        "name": "Rio Yokota"
                    },
                    {
                        "name": "Hiroya Takamura"
                    },
                    {
                        "name": "Naoaki Okazaki"
                    }
                ],
                "author_detail": {
                    "name": "Naoaki Okazaki"
                },
                "author": "Naoaki Okazaki",
                "arxiv_comment": "COLM 2025; Datasets are available at\n  https://huggingface.co/datasets/tokyotech-llm/lmsys-chat-1m-synth",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23714v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23714v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10457v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10457v1",
                "updated": "2025-08-14T08:56:58Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    8,
                    56,
                    58,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T08:56:58Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    8,
                    56,
                    58,
                    3,
                    226,
                    0
                ],
                "title": "Multi-Label Plant Species Prediction with Metadata-Enhanced Multi-Head\n  Vision Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Label Plant Species Prediction with Metadata-Enhanced Multi-Head\n  Vision Transformers"
                },
                "summary": "We present a multi-head vision transformer approach for multi-label plant\nspecies prediction in vegetation plot images, addressing the PlantCLEF 2025\nchallenge. The task involves training models on single-species plant images\nwhile testing on multi-species quadrat images, creating a drastic domain shift.\nOur methodology leverages a pre-trained DINOv2 Vision Transformer Base\n(ViT-B/14) backbone with multiple classification heads for species, genus, and\nfamily prediction, utilizing taxonomic hierarchies. Key contributions include\nmulti-scale tiling to capture plants at different scales, dynamic threshold\noptimization based on mean prediction length, and ensemble strategies through\nbagging and Hydra model architectures. The approach incorporates various\ninference techniques including image cropping to remove non-plant artifacts,\ntop-n filtering for prediction constraints, and logit thresholding strategies.\nExperiments were conducted on approximately 1.4 million training images\ncovering 7,806 plant species. Results demonstrate strong performance, making\nour submission 3rd best on the private leaderboard. Our code is available at\nhttps://github.com/geranium12/plant-clef-2025/tree/v1.0.0.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a multi-head vision transformer approach for multi-label plant\nspecies prediction in vegetation plot images, addressing the PlantCLEF 2025\nchallenge. The task involves training models on single-species plant images\nwhile testing on multi-species quadrat images, creating a drastic domain shift.\nOur methodology leverages a pre-trained DINOv2 Vision Transformer Base\n(ViT-B/14) backbone with multiple classification heads for species, genus, and\nfamily prediction, utilizing taxonomic hierarchies. Key contributions include\nmulti-scale tiling to capture plants at different scales, dynamic threshold\noptimization based on mean prediction length, and ensemble strategies through\nbagging and Hydra model architectures. The approach incorporates various\ninference techniques including image cropping to remove non-plant artifacts,\ntop-n filtering for prediction constraints, and logit thresholding strategies.\nExperiments were conducted on approximately 1.4 million training images\ncovering 7,806 plant species. Results demonstrate strong performance, making\nour submission 3rd best on the private leaderboard. Our code is available at\nhttps://github.com/geranium12/plant-clef-2025/tree/v1.0.0."
                },
                "authors": [
                    {
                        "name": "Hanna Herasimchyk"
                    },
                    {
                        "name": "Robin Labryga"
                    },
                    {
                        "name": "Tomislav Prusina"
                    }
                ],
                "author_detail": {
                    "name": "Tomislav Prusina"
                },
                "author": "Tomislav Prusina",
                "arxiv_comment": "Accepted for publication at: LifeCLEF Lab at CLEF 2025 Working Notes,\n  2025, Madrid, Spain",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10457v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10457v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11509v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11509v3",
                "updated": "2025-08-14T08:52:03Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    8,
                    52,
                    3,
                    3,
                    226,
                    0
                ],
                "published": "2025-03-14T15:29:58Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    15,
                    29,
                    58,
                    4,
                    73,
                    0
                ],
                "title": "TikZero: Zero-Shot Text-Guided Graphics Program Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TikZero: Zero-Shot Text-Guided Graphics Program Synthesis"
                },
                "summary": "Automatically synthesizing figures from text captions is a compelling\ncapability. However, achieving high geometric precision and editability\nrequires representing figures as graphics programs in languages like TikZ, and\naligned training data (i.e., graphics programs with captions) remains scarce.\nMeanwhile, large amounts of unaligned graphics programs and captioned raster\nimages are more readily available. We reconcile these disparate data sources by\npresenting TikZero, which decouples graphics program generation from text\nunderstanding by using image representations as an intermediary bridge. It\nenables independent training on graphics programs and captioned images and\nallows for zero-shot text-guided graphics program synthesis during inference.\nWe show that our method substantially outperforms baselines that can only\noperate with caption-aligned graphics programs. Furthermore, when leveraging\ncaption-aligned graphics programs as a complementary training signal, TikZero\nmatches or exceeds the performance of much larger models, including commercial\nsystems like GPT-4o. Our code, datasets, and select models are publicly\navailable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatically synthesizing figures from text captions is a compelling\ncapability. However, achieving high geometric precision and editability\nrequires representing figures as graphics programs in languages like TikZ, and\naligned training data (i.e., graphics programs with captions) remains scarce.\nMeanwhile, large amounts of unaligned graphics programs and captioned raster\nimages are more readily available. We reconcile these disparate data sources by\npresenting TikZero, which decouples graphics program generation from text\nunderstanding by using image representations as an intermediary bridge. It\nenables independent training on graphics programs and captioned images and\nallows for zero-shot text-guided graphics program synthesis during inference.\nWe show that our method substantially outperforms baselines that can only\noperate with caption-aligned graphics programs. Furthermore, when leveraging\ncaption-aligned graphics programs as a complementary training signal, TikZero\nmatches or exceeds the performance of much larger models, including commercial\nsystems like GPT-4o. Our code, datasets, and select models are publicly\navailable."
                },
                "authors": [
                    {
                        "name": "Jonas Belouadi"
                    },
                    {
                        "name": "Eddy Ilg"
                    },
                    {
                        "name": "Margret Keuper"
                    },
                    {
                        "name": "Hideki Tanaka"
                    },
                    {
                        "name": "Masao Utiyama"
                    },
                    {
                        "name": "Raj Dabre"
                    },
                    {
                        "name": "Steffen Eger"
                    },
                    {
                        "name": "Simone Paolo Ponzetto"
                    }
                ],
                "author_detail": {
                    "name": "Simone Paolo Ponzetto"
                },
                "author": "Simone Paolo Ponzetto",
                "arxiv_comment": "Accepted at ICCV 2025 (highlight); Project page:\n  https://github.com/potamides/DeTikZify",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11509v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11509v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10447v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10447v1",
                "updated": "2025-08-14T08:34:47Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    8,
                    34,
                    47,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T08:34:47Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    8,
                    34,
                    47,
                    3,
                    226,
                    0
                ],
                "title": "BKP: An R Package for Beta Kernel Process Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BKP: An R Package for Beta Kernel Process Modeling"
                },
                "summary": "We present BKP, a user-friendly and extensible R package that implements the\nBeta Kernel Process (BKP) -- a fully nonparametric and computationally\nefficient framework for modeling spatially varying binomial probabilities. The\nBKP model combines localized kernel-weighted likelihoods with conjugate beta\npriors, resulting in closed-form posterior inference without requiring latent\nvariable augmentation or intensive MCMC sampling. The package supports binary\nand aggregated binomial responses, allows flexible choices of kernel functions\nand prior specification, and provides loss-based kernel hyperparameter tuning\nprocedures. In addition, BKP extends naturally to the Dirichlet Kernel Process\n(DKP) for modeling spatially varying multinomial or compositional data. To our\nknowledge, this is the first publicly available R package for implementing\nBKP-based methods. We illustrate the use of BKP through several synthetic and\nreal-world datasets, highlighting its interpretability, accuracy, and\nscalability. The package aims to facilitate practical application and future\nmethodological development of kernel-based beta modeling in statistics and\nmachine learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present BKP, a user-friendly and extensible R package that implements the\nBeta Kernel Process (BKP) -- a fully nonparametric and computationally\nefficient framework for modeling spatially varying binomial probabilities. The\nBKP model combines localized kernel-weighted likelihoods with conjugate beta\npriors, resulting in closed-form posterior inference without requiring latent\nvariable augmentation or intensive MCMC sampling. The package supports binary\nand aggregated binomial responses, allows flexible choices of kernel functions\nand prior specification, and provides loss-based kernel hyperparameter tuning\nprocedures. In addition, BKP extends naturally to the Dirichlet Kernel Process\n(DKP) for modeling spatially varying multinomial or compositional data. To our\nknowledge, this is the first publicly available R package for implementing\nBKP-based methods. We illustrate the use of BKP through several synthetic and\nreal-world datasets, highlighting its interpretability, accuracy, and\nscalability. The package aims to facilitate practical application and future\nmethodological development of kernel-based beta modeling in statistics and\nmachine learning."
                },
                "authors": [
                    {
                        "name": "Jiangyan Zhao"
                    },
                    {
                        "name": "Kunhai Qing"
                    },
                    {
                        "name": "Jin Xu"
                    }
                ],
                "author_detail": {
                    "name": "Jin Xu"
                },
                "author": "Jin Xu",
                "arxiv_comment": "32 pages, 12 figures, and 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10447v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10447v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.14189v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.14189v2",
                "updated": "2025-08-14T08:14:12Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    8,
                    14,
                    12,
                    3,
                    226,
                    0
                ],
                "published": "2025-07-14T02:13:22Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    2,
                    13,
                    22,
                    0,
                    195,
                    0
                ],
                "title": "DeepWriter: A Fact-Grounded Multimodal Writing Assistant Based On\n  Offline Knowledge Base",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepWriter: A Fact-Grounded Multimodal Writing Assistant Based On\n  Offline Knowledge Base"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nvarious applications. However, their use as writing assistants in specialized\ndomains like finance, medicine, and law is often hampered by a lack of deep\ndomain-specific knowledge and a tendency to hallucinate. Existing solutions,\nsuch as Retrieval-Augmented Generation (RAG), can suffer from inconsistency\nacross multiple retrieval steps, while online search-based methods often\ndegrade quality due to unreliable web content. To address these challenges, we\nintroduce DeepWriter, a customizable, multimodal, long-form writing assistant\nthat operates on a curated, offline knowledge base. DeepWriter leverages a\nnovel pipeline that involves task decomposition, outline generation, multimodal\nretrieval, and section-by-section composition with reflection. By deeply mining\ninformation from a structured corpus and incorporating both textual and visual\nelements, DeepWriter generates coherent, factually grounded, and\nprofessional-grade documents. We also propose a hierarchical knowledge\nrepresentation to enhance retrieval efficiency and accuracy. Our experiments on\nfinancial report generation demonstrate that DeepWriter produces high-quality,\nverifiable articles that surpasses existing baselines in factual accuracy and\ngenerated content quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nvarious applications. However, their use as writing assistants in specialized\ndomains like finance, medicine, and law is often hampered by a lack of deep\ndomain-specific knowledge and a tendency to hallucinate. Existing solutions,\nsuch as Retrieval-Augmented Generation (RAG), can suffer from inconsistency\nacross multiple retrieval steps, while online search-based methods often\ndegrade quality due to unreliable web content. To address these challenges, we\nintroduce DeepWriter, a customizable, multimodal, long-form writing assistant\nthat operates on a curated, offline knowledge base. DeepWriter leverages a\nnovel pipeline that involves task decomposition, outline generation, multimodal\nretrieval, and section-by-section composition with reflection. By deeply mining\ninformation from a structured corpus and incorporating both textual and visual\nelements, DeepWriter generates coherent, factually grounded, and\nprofessional-grade documents. We also propose a hierarchical knowledge\nrepresentation to enhance retrieval efficiency and accuracy. Our experiments on\nfinancial report generation demonstrate that DeepWriter produces high-quality,\nverifiable articles that surpasses existing baselines in factual accuracy and\ngenerated content quality."
                },
                "authors": [
                    {
                        "name": "Song Mao"
                    },
                    {
                        "name": "Lejun Cheng"
                    },
                    {
                        "name": "Pinlong Cai"
                    },
                    {
                        "name": "Guohang Yan"
                    },
                    {
                        "name": "Ding Wang"
                    },
                    {
                        "name": "Botian Shi"
                    }
                ],
                "author_detail": {
                    "name": "Botian Shi"
                },
                "author": "Botian Shi",
                "arxiv_comment": "work in process",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.14189v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.14189v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15758v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15758v2",
                "updated": "2025-08-14T08:13:36Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    8,
                    13,
                    36,
                    3,
                    226,
                    0
                ],
                "published": "2025-07-21T16:14:41Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    16,
                    14,
                    41,
                    0,
                    202,
                    0
                ],
                "title": "LAPO: Internalizing Reasoning Efficiency via Length-Adaptive Policy\n  Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LAPO: Internalizing Reasoning Efficiency via Length-Adaptive Policy\n  Optimization"
                },
                "summary": "Large reasoning models have achieved remarkable performance through extended\nchain-of-thought sequences, yet this computational freedom leads to excessive\ntoken generation even for simple problems. We present Length-Adaptive Policy\nOptimization (LAPO), a novel framework that transforms reasoning length control\nfrom an external constraint into an intrinsic model capability. Unlike existing\napproaches that impose rigid limits or rely on post-hoc interventions, LAPO\nenables models to internalize an understanding of appropriate reasoning depth\nthrough a two-stage reinforcement learning process. In the first stage, models\nlearn natural reasoning patterns by discovering the statistical distribution of\nsuccessful solution lengths. The second stage leverages these patterns as\nmeta-cognitive guidance, embedding them directly within the model's reasoning\ncontext to ensure inference-time flexibility. Experiments on mathematical\nreasoning benchmarks demonstrate that LAPO reduces token usage by up to 40.9%\nwhile improving accuracy by 2.3%. Our analysis reveals that models trained with\nLAPO develop emergent abilities to allocate computational resources based on\nproblem complexity, achieving efficient reasoning without sacrificing quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large reasoning models have achieved remarkable performance through extended\nchain-of-thought sequences, yet this computational freedom leads to excessive\ntoken generation even for simple problems. We present Length-Adaptive Policy\nOptimization (LAPO), a novel framework that transforms reasoning length control\nfrom an external constraint into an intrinsic model capability. Unlike existing\napproaches that impose rigid limits or rely on post-hoc interventions, LAPO\nenables models to internalize an understanding of appropriate reasoning depth\nthrough a two-stage reinforcement learning process. In the first stage, models\nlearn natural reasoning patterns by discovering the statistical distribution of\nsuccessful solution lengths. The second stage leverages these patterns as\nmeta-cognitive guidance, embedding them directly within the model's reasoning\ncontext to ensure inference-time flexibility. Experiments on mathematical\nreasoning benchmarks demonstrate that LAPO reduces token usage by up to 40.9%\nwhile improving accuracy by 2.3%. Our analysis reveals that models trained with\nLAPO develop emergent abilities to allocate computational resources based on\nproblem complexity, achieving efficient reasoning without sacrificing quality."
                },
                "authors": [
                    {
                        "name": "Xingyu Wu"
                    },
                    {
                        "name": "Yuchen Yan"
                    },
                    {
                        "name": "Shangke Lyu"
                    },
                    {
                        "name": "Linjuan Wu"
                    },
                    {
                        "name": "Yiwen Qiu"
                    },
                    {
                        "name": "Yongliang Shen"
                    },
                    {
                        "name": "Weiming Lu"
                    },
                    {
                        "name": "Jian Shao"
                    },
                    {
                        "name": "Jun Xiao"
                    },
                    {
                        "name": "Yueting Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Yueting Zhuang"
                },
                "author": "Yueting Zhuang",
                "arxiv_comment": "GitHub:https://github.com/zju-real/lapoProject:https://zju-real.github.io/lapo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15758v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15758v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21534v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21534v2",
                "updated": "2025-08-14T08:12:50Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    8,
                    12,
                    50,
                    3,
                    226,
                    0
                ],
                "published": "2025-03-27T14:26:20Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    14,
                    26,
                    20,
                    3,
                    86,
                    0
                ],
                "title": "Inequality Restricted Minimum Density Power Divergence Estimation in\n  Panel Count Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inequality Restricted Minimum Density Power Divergence Estimation in\n  Panel Count Data"
                },
                "summary": "Analysis of panel count data has garnered a considerable amount of attention\nin the literature, leading to the development of multiple statistical\ntechniques. In inferential analysis, most of the works focus on leveraging\nestimating equations-based techniques or conventional maximum likelihood\nestimation. However, the robustness of these methods is largely questionable.\nIn this paper, we present the robust density power divergence estimation for\npanel count data arising from nonhomogeneous Poisson processes, correlated\nthrough a latent frailty variable. In order to cope with real-world incidents,\nit is often desired to impose certain inequality constraints on the parameter\nspace, giving rise to the restricted minimum density power divergence\nestimator. The significant contribution of this study lies in deriving its\nasymptotic properties. The proposed method ensures high efficiency in the model\nestimation while providing reliable inference despite data contamination.\nMoreover, the density power divergence measure is governed by a tuning\nparameter $\\gamma$, which controls the trade-off between robustness and\nefficiency. To effectively determine the optimal value of $\\gamma$, this study\nemploys a generalized score-matching technique, marking considerable progress\nin the data analysis. Simulation studies and real data examples are provided to\nillustrate the performance of the estimator and to substantiate the theory\ndeveloped.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analysis of panel count data has garnered a considerable amount of attention\nin the literature, leading to the development of multiple statistical\ntechniques. In inferential analysis, most of the works focus on leveraging\nestimating equations-based techniques or conventional maximum likelihood\nestimation. However, the robustness of these methods is largely questionable.\nIn this paper, we present the robust density power divergence estimation for\npanel count data arising from nonhomogeneous Poisson processes, correlated\nthrough a latent frailty variable. In order to cope with real-world incidents,\nit is often desired to impose certain inequality constraints on the parameter\nspace, giving rise to the restricted minimum density power divergence\nestimator. The significant contribution of this study lies in deriving its\nasymptotic properties. The proposed method ensures high efficiency in the model\nestimation while providing reliable inference despite data contamination.\nMoreover, the density power divergence measure is governed by a tuning\nparameter $\\gamma$, which controls the trade-off between robustness and\nefficiency. To effectively determine the optimal value of $\\gamma$, this study\nemploys a generalized score-matching technique, marking considerable progress\nin the data analysis. Simulation studies and real data examples are provided to\nillustrate the performance of the estimator and to substantiate the theory\ndeveloped."
                },
                "authors": [
                    {
                        "name": "Udita Goswami"
                    },
                    {
                        "name": "Shuvashree Mondal"
                    }
                ],
                "author_detail": {
                    "name": "Shuvashree Mondal"
                },
                "author": "Shuvashree Mondal",
                "arxiv_comment": "55 Pages, 12 Figures, 11 Tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21534v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21534v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10428v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10428v1",
                "updated": "2025-08-14T07:58:01Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    7,
                    58,
                    1,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T07:58:01Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    7,
                    58,
                    1,
                    3,
                    226,
                    0
                ],
                "title": "SC2Arena and StarEvolve: Benchmark and Self-Improvement Framework for\n  LLMs in Complex Decision-Making Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SC2Arena and StarEvolve: Benchmark and Self-Improvement Framework for\n  LLMs in Complex Decision-Making Tasks"
                },
                "summary": "Evaluating large language models (LLMs) in complex decision-making is\nessential for advancing AI's ability for strategic planning and real-time\nadaptation. However, existing benchmarks for tasks like StarCraft II fail to\ncapture the game's full complexity, such as its complete game context, diverse\naction spaces, and all playable races. To address this gap, we present\nSC2Arena, a benchmark that fully supports all playable races, low-level action\nspaces, and optimizes text-based observations to tackle spatial reasoning\nchallenges. Complementing this, we introduce StarEvolve, a hierarchical\nframework that integrates strategic planning with tactical execution, featuring\niterative self-correction and continuous improvement via fine-tuning on\nhigh-quality gameplay data. Its key components include a\nPlanner-Executor-Verifier structure to break down gameplay, and a scoring\nsystem for selecting high-quality training samples. Comprehensive analysis\nusing SC2Arena provides valuable insights into developing generalist agents\nthat were not possible with previous benchmarks. Experimental results also\ndemonstrate that our proposed StarEvolve achieves superior performance in\nstrategic planning. Our code, environment, and algorithms are publicly\navailable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating large language models (LLMs) in complex decision-making is\nessential for advancing AI's ability for strategic planning and real-time\nadaptation. However, existing benchmarks for tasks like StarCraft II fail to\ncapture the game's full complexity, such as its complete game context, diverse\naction spaces, and all playable races. To address this gap, we present\nSC2Arena, a benchmark that fully supports all playable races, low-level action\nspaces, and optimizes text-based observations to tackle spatial reasoning\nchallenges. Complementing this, we introduce StarEvolve, a hierarchical\nframework that integrates strategic planning with tactical execution, featuring\niterative self-correction and continuous improvement via fine-tuning on\nhigh-quality gameplay data. Its key components include a\nPlanner-Executor-Verifier structure to break down gameplay, and a scoring\nsystem for selecting high-quality training samples. Comprehensive analysis\nusing SC2Arena provides valuable insights into developing generalist agents\nthat were not possible with previous benchmarks. Experimental results also\ndemonstrate that our proposed StarEvolve achieves superior performance in\nstrategic planning. Our code, environment, and algorithms are publicly\navailable."
                },
                "authors": [
                    {
                        "name": "Pengbo Shen"
                    },
                    {
                        "name": "Yaqing Wang"
                    },
                    {
                        "name": "Ni Mu"
                    },
                    {
                        "name": "Yao Luan"
                    },
                    {
                        "name": "Runpeng Xie"
                    },
                    {
                        "name": "Senhao Yang"
                    },
                    {
                        "name": "Lexiang Wang"
                    },
                    {
                        "name": "Hao Hu"
                    },
                    {
                        "name": "Shuang Xu"
                    },
                    {
                        "name": "Yiqin Yang"
                    },
                    {
                        "name": "Bo Xu"
                    }
                ],
                "author_detail": {
                    "name": "Bo Xu"
                },
                "author": "Bo Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10428v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10428v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10426v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10426v1",
                "updated": "2025-08-14T07:55:45Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    7,
                    55,
                    45,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T07:55:45Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    7,
                    55,
                    45,
                    3,
                    226,
                    0
                ],
                "title": "Computational Economics in Large Language Models: Exploring Model\n  Behavior and Incentive Design under Resource Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computational Economics in Large Language Models: Exploring Model\n  Behavior and Incentive Design under Resource Constraints"
                },
                "summary": "Large language models (LLMs) are limited by substantial computational cost.\nWe introduce a \"computational economics\" framework that treats an LLM as an\ninternal economy of resource-constrained agents (attention heads and neuron\nblocks) that must allocate scarce computation to maximize task utility. First,\nwe show empirically that when computation is scarce, standard LLMs reallocate\nattention toward high-value tokens while preserving accuracy. Building on this\nobservation, we propose an incentive-driven training paradigm that augments the\ntask loss with a differentiable computation cost term, encouraging sparse and\nefficient activations. On GLUE (MNLI, STS-B, CoLA) and WikiText-103, the method\nyields a family of models that trace a Pareto frontier and consistently\ndominate post-hoc pruning; for a similar accuracy we obtain roughly a forty\npercent reduction in FLOPS and lower latency, together with more interpretable\nattention patterns. These results indicate that economic principles offer a\nprincipled route to designing efficient, adaptive, and more transparent LLMs\nunder strict resource constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are limited by substantial computational cost.\nWe introduce a \"computational economics\" framework that treats an LLM as an\ninternal economy of resource-constrained agents (attention heads and neuron\nblocks) that must allocate scarce computation to maximize task utility. First,\nwe show empirically that when computation is scarce, standard LLMs reallocate\nattention toward high-value tokens while preserving accuracy. Building on this\nobservation, we propose an incentive-driven training paradigm that augments the\ntask loss with a differentiable computation cost term, encouraging sparse and\nefficient activations. On GLUE (MNLI, STS-B, CoLA) and WikiText-103, the method\nyields a family of models that trace a Pareto frontier and consistently\ndominate post-hoc pruning; for a similar accuracy we obtain roughly a forty\npercent reduction in FLOPS and lower latency, together with more interpretable\nattention patterns. These results indicate that economic principles offer a\nprincipled route to designing efficient, adaptive, and more transparent LLMs\nunder strict resource constraints."
                },
                "authors": [
                    {
                        "name": "Sandeep Reddy"
                    },
                    {
                        "name": "Kabir Khan"
                    },
                    {
                        "name": "Rohit Patil"
                    },
                    {
                        "name": "Ananya Chakraborty"
                    },
                    {
                        "name": "Faizan A. Khan"
                    },
                    {
                        "name": "Swati Kulkarni"
                    },
                    {
                        "name": "Arjun Verma"
                    },
                    {
                        "name": "Neha Singh"
                    }
                ],
                "author_detail": {
                    "name": "Neha Singh"
                },
                "author": "Neha Singh",
                "arxiv_comment": "Preprint; 7 figures, 4 tables, 1 algorithm. Experiments on GLUE\n  (MNLI, STS-B, CoLA) and WikiText-103 with BERT-base; evaluation includes\n  FLOPS, latency, Gini and entropy metrics",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10426v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10426v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6; I.2.7; I.5.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10421v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10421v1",
                "updated": "2025-08-14T07:52:56Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    7,
                    52,
                    56,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T07:52:56Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    7,
                    52,
                    56,
                    3,
                    226,
                    0
                ],
                "title": "Evaluating LLMs on Chinese Idiom Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating LLMs on Chinese Idiom Translation"
                },
                "summary": "Idioms, whose figurative meanings usually differ from their literal\ninterpretations, are common in everyday language, especially in Chinese, where\nthey often contain historical references and follow specific structural\npatterns. Despite recent progress in machine translation with large language\nmodels, little is known about Chinese idiom translation. In this work, we\nintroduce IdiomEval, a framework with a comprehensive error taxonomy for\nChinese idiom translation. We annotate 900 translation pairs from nine modern\nsystems, including GPT-4o and Google Translate, across four domains: web, news,\nWikipedia, and social media. We find these systems fail at idiom translation,\nproducing incorrect, literal, partial, or even missing translations. The\nbest-performing system, GPT-4, makes errors in 28% of cases. We also find that\nexisting evaluation metrics measure idiom quality poorly with Pearson\ncorrelation below 0.48 with human ratings. We thus develop improved models that\nachieve F$_1$ scores of 0.68 for detecting idiom translation errors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Idioms, whose figurative meanings usually differ from their literal\ninterpretations, are common in everyday language, especially in Chinese, where\nthey often contain historical references and follow specific structural\npatterns. Despite recent progress in machine translation with large language\nmodels, little is known about Chinese idiom translation. In this work, we\nintroduce IdiomEval, a framework with a comprehensive error taxonomy for\nChinese idiom translation. We annotate 900 translation pairs from nine modern\nsystems, including GPT-4o and Google Translate, across four domains: web, news,\nWikipedia, and social media. We find these systems fail at idiom translation,\nproducing incorrect, literal, partial, or even missing translations. The\nbest-performing system, GPT-4, makes errors in 28% of cases. We also find that\nexisting evaluation metrics measure idiom quality poorly with Pearson\ncorrelation below 0.48 with human ratings. We thus develop improved models that\nachieve F$_1$ scores of 0.68 for detecting idiom translation errors."
                },
                "authors": [
                    {
                        "name": "Cai Yang"
                    },
                    {
                        "name": "Yao Dou"
                    },
                    {
                        "name": "David Heineman"
                    },
                    {
                        "name": "Xiaofeng Wu"
                    },
                    {
                        "name": "Wei Xu"
                    }
                ],
                "author_detail": {
                    "name": "Wei Xu"
                },
                "author": "Wei Xu",
                "arxiv_comment": "Accepted at COLM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10421v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10421v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10419v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10419v1",
                "updated": "2025-08-14T07:52:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    7,
                    52,
                    9,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T07:52:09Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    7,
                    52,
                    9,
                    3,
                    226,
                    0
                ],
                "title": "ComoRAG: A Cognitive-Inspired Memory-Organized RAG for Stateful Long\n  Narrative Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ComoRAG: A Cognitive-Inspired Memory-Organized RAG for Stateful Long\n  Narrative Reasoning"
                },
                "summary": "Narrative comprehension on long stories and novels has been a challenging\ndomain attributed to their intricate plotlines and entangled, often evolving\nrelations among characters and entities. Given the LLM's diminished reasoning\nover extended context and high computational cost, retrieval-based approaches\nremain a pivotal role in practice. However, traditional RAG methods can fall\nshort due to their stateless, single-step retrieval process, which often\noverlooks the dynamic nature of capturing interconnected relations within\nlong-range context. In this work, we propose ComoRAG, holding the principle\nthat narrative reasoning is not a one-shot process, but a dynamic, evolving\ninterplay between new evidence acquisition and past knowledge consolidation,\nanalogous to human cognition when reasoning with memory-related signals in the\nbrain. Specifically, when encountering a reasoning impasse, ComoRAG undergoes\niterative reasoning cycles while interacting with a dynamic memory workspace.\nIn each cycle, it generates probing queries to devise new exploratory paths,\nthen integrates the retrieved evidence of new aspects into a global memory\npool, thereby supporting the emergence of a coherent context for the query\nresolution. Across four challenging long-context narrative benchmarks (200K+\ntokens), ComoRAG outperforms strong RAG baselines with consistent relative\ngains up to 11% compared to the strongest baseline. Further analysis reveals\nthat ComoRAG is particularly advantageous for complex queries requiring global\ncomprehension, offering a principled, cognitively motivated paradigm for\nretrieval-based long context comprehension towards stateful reasoning. Our code\nis publicly released at https://github.com/EternityJune25/ComoRAG",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Narrative comprehension on long stories and novels has been a challenging\ndomain attributed to their intricate plotlines and entangled, often evolving\nrelations among characters and entities. Given the LLM's diminished reasoning\nover extended context and high computational cost, retrieval-based approaches\nremain a pivotal role in practice. However, traditional RAG methods can fall\nshort due to their stateless, single-step retrieval process, which often\noverlooks the dynamic nature of capturing interconnected relations within\nlong-range context. In this work, we propose ComoRAG, holding the principle\nthat narrative reasoning is not a one-shot process, but a dynamic, evolving\ninterplay between new evidence acquisition and past knowledge consolidation,\nanalogous to human cognition when reasoning with memory-related signals in the\nbrain. Specifically, when encountering a reasoning impasse, ComoRAG undergoes\niterative reasoning cycles while interacting with a dynamic memory workspace.\nIn each cycle, it generates probing queries to devise new exploratory paths,\nthen integrates the retrieved evidence of new aspects into a global memory\npool, thereby supporting the emergence of a coherent context for the query\nresolution. Across four challenging long-context narrative benchmarks (200K+\ntokens), ComoRAG outperforms strong RAG baselines with consistent relative\ngains up to 11% compared to the strongest baseline. Further analysis reveals\nthat ComoRAG is particularly advantageous for complex queries requiring global\ncomprehension, offering a principled, cognitively motivated paradigm for\nretrieval-based long context comprehension towards stateful reasoning. Our code\nis publicly released at https://github.com/EternityJune25/ComoRAG"
                },
                "authors": [
                    {
                        "name": "Juyuan Wang"
                    },
                    {
                        "name": "Rongchen Zhao"
                    },
                    {
                        "name": "Wei Wei"
                    },
                    {
                        "name": "Yufeng Wang"
                    },
                    {
                        "name": "Mo Yu"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Jin Xu"
                    },
                    {
                        "name": "Liyan Xu"
                    }
                ],
                "author_detail": {
                    "name": "Liyan Xu"
                },
                "author": "Liyan Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10419v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10419v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10414v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10414v1",
                "updated": "2025-08-14T07:38:01Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    7,
                    38,
                    1,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T07:38:01Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    7,
                    38,
                    1,
                    3,
                    226,
                    0
                ],
                "title": "MCP2OSC: Parametric Control by Natural Language",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MCP2OSC: Parametric Control by Natural Language"
                },
                "summary": "Text prompts enable intuitive content creation but may fall short in\nachieving high precision for intricate tasks; knob or slider controls offer\nprecise adjustments at the cost of increased complexity. To address the gap\nbetween knobs and prompts, a new MCP (Model Context Protocol) server and a\nunique set of prompt design criteria are presented to enable exploring\nparametric OSC (OpenSoundControl) control by natural language prompts.\nDemonstrated by 14 practical QA examples with best practices and the\ngeneralized prompt templates, this study finds Claude integrated with the\nMCP2OSC server effective in generating OSC messages by natural language,\ninterpreting, searching, and visualizing OSC messages, validating and debugging\nOSC messages, and managing OSC address patterns. MCP2OSC enhances human-machine\ncollaboration by leveraging LLM (Large Language Model) to handle intricate OSC\ndevelopment tasks, and by empowering human creativity with an intuitive\nlanguage interface featuring flexible precision controls: a prompt-based OSC\ntool. This study provides a novel perspective on the creative MCP application\nat the network protocol level by utilizing LLM's strength in directly\nprocessing and generating human-readable OSC messages. The results suggest its\npotential for a LLM-based universal control mechanism for multimedia devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text prompts enable intuitive content creation but may fall short in\nachieving high precision for intricate tasks; knob or slider controls offer\nprecise adjustments at the cost of increased complexity. To address the gap\nbetween knobs and prompts, a new MCP (Model Context Protocol) server and a\nunique set of prompt design criteria are presented to enable exploring\nparametric OSC (OpenSoundControl) control by natural language prompts.\nDemonstrated by 14 practical QA examples with best practices and the\ngeneralized prompt templates, this study finds Claude integrated with the\nMCP2OSC server effective in generating OSC messages by natural language,\ninterpreting, searching, and visualizing OSC messages, validating and debugging\nOSC messages, and managing OSC address patterns. MCP2OSC enhances human-machine\ncollaboration by leveraging LLM (Large Language Model) to handle intricate OSC\ndevelopment tasks, and by empowering human creativity with an intuitive\nlanguage interface featuring flexible precision controls: a prompt-based OSC\ntool. This study provides a novel perspective on the creative MCP application\nat the network protocol level by utilizing LLM's strength in directly\nprocessing and generating human-readable OSC messages. The results suggest its\npotential for a LLM-based universal control mechanism for multimedia devices."
                },
                "authors": [
                    {
                        "name": "Yuan-Yi Fan"
                    }
                ],
                "author_detail": {
                    "name": "Yuan-Yi Fan"
                },
                "author": "Yuan-Yi Fan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10414v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10414v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09549v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09549v2",
                "updated": "2025-08-14T07:26:38Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    7,
                    26,
                    38,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-13T07:13:45Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    7,
                    13,
                    45,
                    2,
                    225,
                    0
                ],
                "title": "CS-Agent: LLM-based Community Search via Dual-agent Collaboration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CS-Agent: LLM-based Community Search via Dual-agent Collaboration"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nnatural language processing tasks, yet their application to graph structure\nanalysis, particularly in community search, remains underexplored. Community\nsearch, a fundamental task in graph analysis, aims to identify groups of nodes\nwith dense interconnections, which is crucial for understanding the macroscopic\nstructure of graphs. In this paper, we propose GraphCS, a comprehensive\nbenchmark designed to evaluate the performance of LLMs in community search\ntasks. Our experiments reveal that while LLMs exhibit preliminary potential,\nthey frequently fail to return meaningful results and suffer from output bias.\nTo address these limitations, we introduce CS-Agent, a dual-agent collaborative\nframework to enhance LLM-based community search. CS-Agent leverages the\ncomplementary strengths of two LLMs acting as Solver and Validator. Through\niterative feedback and refinement, CS-Agent dynamically refines initial results\nwithout fine-tuning or additional training. After the multi-round dialogue,\nDecider module selects the optimal community. Extensive experiments demonstrate\nthat CS-Agent significantly improves the quality and stability of identified\ncommunities compared to baseline methods. To our knowledge, this is the first\nwork to apply LLMs to community search, bridging the gap between LLMs and graph\nanalysis while providing a robust and adaptive solution for real-world\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nnatural language processing tasks, yet their application to graph structure\nanalysis, particularly in community search, remains underexplored. Community\nsearch, a fundamental task in graph analysis, aims to identify groups of nodes\nwith dense interconnections, which is crucial for understanding the macroscopic\nstructure of graphs. In this paper, we propose GraphCS, a comprehensive\nbenchmark designed to evaluate the performance of LLMs in community search\ntasks. Our experiments reveal that while LLMs exhibit preliminary potential,\nthey frequently fail to return meaningful results and suffer from output bias.\nTo address these limitations, we introduce CS-Agent, a dual-agent collaborative\nframework to enhance LLM-based community search. CS-Agent leverages the\ncomplementary strengths of two LLMs acting as Solver and Validator. Through\niterative feedback and refinement, CS-Agent dynamically refines initial results\nwithout fine-tuning or additional training. After the multi-round dialogue,\nDecider module selects the optimal community. Extensive experiments demonstrate\nthat CS-Agent significantly improves the quality and stability of identified\ncommunities compared to baseline methods. To our knowledge, this is the first\nwork to apply LLMs to community search, bridging the gap between LLMs and graph\nanalysis while providing a robust and adaptive solution for real-world\napplications."
                },
                "authors": [
                    {
                        "name": "Jiahao Hua"
                    },
                    {
                        "name": "Long Yuan"
                    },
                    {
                        "name": "Qingshuai Feng"
                    },
                    {
                        "name": "Qiang Fan"
                    },
                    {
                        "name": "Shan Huang"
                    }
                ],
                "author_detail": {
                    "name": "Shan Huang"
                },
                "author": "Shan Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09549v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09549v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.07697v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.07697v2",
                "updated": "2025-08-14T07:25:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    7,
                    25,
                    44,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-11T07:19:21Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    7,
                    19,
                    21,
                    0,
                    223,
                    0
                ],
                "title": "Semantic-Enhanced Time-Series Forecasting via Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic-Enhanced Time-Series Forecasting via Large Language Models"
                },
                "summary": "Time series forecasting plays a significant role in finance, energy,\nmeteorology, and IoT applications. Recent studies have leveraged the\ngeneralization capabilities of large language models (LLMs) to adapt to time\nseries forecasting, achieving promising performance. However, existing studies\nfocus on token-level modal alignment, instead of bridging the intrinsic\nmodality gap between linguistic knowledge structures and time series data\npatterns, greatly limiting the semantic representation. To address this issue,\nwe propose a novel Semantic-Enhanced LLM (SE-LLM) that explores the inherent\nperiodicity and anomalous characteristics of time series to embed into the\nsemantic space to enhance the token embedding. This process enhances the\ninterpretability of tokens for LLMs, thereby activating the potential of LLMs\nfor temporal sequence analysis. Moreover, existing Transformer-based LLMs excel\nat capturing long-range dependencies but are weak at modeling short-term\nanomalies in time-series data. Hence, we propose a plugin module embedded\nwithin self-attention that models long-term and short-term dependencies to\neffectively adapt LLMs to time-series analysis. Our approach freezes the LLM\nand reduces the sequence dimensionality of tokens, greatly reducing\ncomputational consumption. Experiments demonstrate the superiority performance\nof our SE-LLM against the state-of-the-art (SOTA) methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time series forecasting plays a significant role in finance, energy,\nmeteorology, and IoT applications. Recent studies have leveraged the\ngeneralization capabilities of large language models (LLMs) to adapt to time\nseries forecasting, achieving promising performance. However, existing studies\nfocus on token-level modal alignment, instead of bridging the intrinsic\nmodality gap between linguistic knowledge structures and time series data\npatterns, greatly limiting the semantic representation. To address this issue,\nwe propose a novel Semantic-Enhanced LLM (SE-LLM) that explores the inherent\nperiodicity and anomalous characteristics of time series to embed into the\nsemantic space to enhance the token embedding. This process enhances the\ninterpretability of tokens for LLMs, thereby activating the potential of LLMs\nfor temporal sequence analysis. Moreover, existing Transformer-based LLMs excel\nat capturing long-range dependencies but are weak at modeling short-term\nanomalies in time-series data. Hence, we propose a plugin module embedded\nwithin self-attention that models long-term and short-term dependencies to\neffectively adapt LLMs to time-series analysis. Our approach freezes the LLM\nand reduces the sequence dimensionality of tokens, greatly reducing\ncomputational consumption. Experiments demonstrate the superiority performance\nof our SE-LLM against the state-of-the-art (SOTA) methods."
                },
                "authors": [
                    {
                        "name": "Hao Liu"
                    },
                    {
                        "name": "Chun Yang"
                    },
                    {
                        "name": "Zhang xiaoxing"
                    },
                    {
                        "name": "Xiaobin Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaobin Zhu"
                },
                "author": "Xiaobin Zhu",
                "arxiv_comment": "14 pages,9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.07697v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.07697v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01618v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01618v5",
                "updated": "2025-08-14T07:21:40Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    7,
                    21,
                    40,
                    3,
                    226,
                    0
                ],
                "published": "2025-02-03T18:50:50Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    18,
                    50,
                    50,
                    0,
                    34,
                    0
                ],
                "title": "Rollout Roulette: A Probabilistic Inference Approach to Inference-Time\n  Scaling of LLMs using Particle-Based Monte Carlo Methods",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rollout Roulette: A Probabilistic Inference Approach to Inference-Time\n  Scaling of LLMs using Particle-Based Monte Carlo Methods"
                },
                "summary": "Large language models (LLMs) have achieved significant performance gains via\nscaling up model sizes and/or data. However, recent evidence suggests\ndiminishing returns from such approaches, motivating scaling the computation\nspent at inference time. Existing inference-time scaling methods, usually with\nreward models, cast the task as a search problem, which tends to be vulnerable\nto reward hacking as a consequence of approximation errors in reward models. In\nthis paper, we instead cast inference-time scaling as a probabilistic inference\ntask and leverage sampling-based techniques to explore the typical set of the\nstate distribution of a state-space model with an approximate likelihood,\nrather than optimize for its mode directly. We propose a novel inference-time\nscaling approach by adapting particle-based Monte Carlo methods to this task.\nOur empirical evaluation demonstrates that our methods have a 4-16x better\nscaling rate over our deterministic search counterparts on various challenging\nmathematical reasoning tasks. Using our approach, we show that\nQwen2.5-Math-1.5B-Instruct can surpass GPT-4o accuracy in only 4 rollouts,\nwhile Qwen2.5-Math-7B-Instruct scales to o1 level accuracy in only 32 rollouts.\nOur work not only presents an effective method to inference-time scaling, but\nalso connects the rich literature in probabilistic inference with\ninference-time scaling of LLMs to develop more robust algorithms in future\nwork. Code, videos, and further information available at\nhttps://probabilistic-inference-scaling.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved significant performance gains via\nscaling up model sizes and/or data. However, recent evidence suggests\ndiminishing returns from such approaches, motivating scaling the computation\nspent at inference time. Existing inference-time scaling methods, usually with\nreward models, cast the task as a search problem, which tends to be vulnerable\nto reward hacking as a consequence of approximation errors in reward models. In\nthis paper, we instead cast inference-time scaling as a probabilistic inference\ntask and leverage sampling-based techniques to explore the typical set of the\nstate distribution of a state-space model with an approximate likelihood,\nrather than optimize for its mode directly. We propose a novel inference-time\nscaling approach by adapting particle-based Monte Carlo methods to this task.\nOur empirical evaluation demonstrates that our methods have a 4-16x better\nscaling rate over our deterministic search counterparts on various challenging\nmathematical reasoning tasks. Using our approach, we show that\nQwen2.5-Math-1.5B-Instruct can surpass GPT-4o accuracy in only 4 rollouts,\nwhile Qwen2.5-Math-7B-Instruct scales to o1 level accuracy in only 32 rollouts.\nOur work not only presents an effective method to inference-time scaling, but\nalso connects the rich literature in probabilistic inference with\ninference-time scaling of LLMs to develop more robust algorithms in future\nwork. Code, videos, and further information available at\nhttps://probabilistic-inference-scaling.github.io."
                },
                "authors": [
                    {
                        "name": "Isha Puri"
                    },
                    {
                        "name": "Shivchander Sudalairaj"
                    },
                    {
                        "name": "Guangxuan Xu"
                    },
                    {
                        "name": "Kai Xu"
                    },
                    {
                        "name": "Akash Srivastava"
                    }
                ],
                "author_detail": {
                    "name": "Akash Srivastava"
                },
                "author": "Akash Srivastava",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01618v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01618v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08512v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08512v3",
                "updated": "2025-08-14T07:15:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    7,
                    15,
                    48,
                    3,
                    226,
                    0
                ],
                "published": "2025-02-12T15:46:34Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    15,
                    46,
                    34,
                    2,
                    43,
                    0
                ],
                "title": "Measuring Diversity in Synthetic Datasets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring Diversity in Synthetic Datasets"
                },
                "summary": "Large language models (LLMs) are widely adopted to generate synthetic\ndatasets for various natural language processing (NLP) tasks, such as text\nclassification and summarization. However, accurately measuring the diversity\nof these synthetic datasets-an aspect crucial for robust model\nperformance-remains a significant challenge. In this paper, we introduce\nDCScore, a novel method for measuring synthetic dataset diversity from a\nclassification perspective. Specifically, DCScore formulates diversity\nevaluation as a sample classification task, leveraging mutual relationships\namong samples. We further provide theoretical verification of the\ndiversity-related axioms satisfied by DCScore, highlighting its role as a\nprincipled diversity evaluation method. Experimental results on synthetic\ndatasets reveal that DCScore enjoys a stronger correlation with multiple\ndiversity pseudo-truths of evaluated datasets, underscoring its effectiveness.\nMoreover, both empirical and theoretical evidence demonstrate that DCScore\nsubstantially reduces computational costs compared to existing methods. Code is\navailable at: https://github.com/bluewhalelab/dcscore.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are widely adopted to generate synthetic\ndatasets for various natural language processing (NLP) tasks, such as text\nclassification and summarization. However, accurately measuring the diversity\nof these synthetic datasets-an aspect crucial for robust model\nperformance-remains a significant challenge. In this paper, we introduce\nDCScore, a novel method for measuring synthetic dataset diversity from a\nclassification perspective. Specifically, DCScore formulates diversity\nevaluation as a sample classification task, leveraging mutual relationships\namong samples. We further provide theoretical verification of the\ndiversity-related axioms satisfied by DCScore, highlighting its role as a\nprincipled diversity evaluation method. Experimental results on synthetic\ndatasets reveal that DCScore enjoys a stronger correlation with multiple\ndiversity pseudo-truths of evaluated datasets, underscoring its effectiveness.\nMoreover, both empirical and theoretical evidence demonstrate that DCScore\nsubstantially reduces computational costs compared to existing methods. Code is\navailable at: https://github.com/bluewhalelab/dcscore."
                },
                "authors": [
                    {
                        "name": "Yuchang Zhu"
                    },
                    {
                        "name": "Huizhe Zhang"
                    },
                    {
                        "name": "Bingzhe Wu"
                    },
                    {
                        "name": "Jintang Li"
                    },
                    {
                        "name": "Zibin Zheng"
                    },
                    {
                        "name": "Peilin Zhao"
                    },
                    {
                        "name": "Liang Chen"
                    },
                    {
                        "name": "Yatao Bian"
                    }
                ],
                "author_detail": {
                    "name": "Yatao Bian"
                },
                "author": "Yatao Bian",
                "arxiv_comment": "Accepted by ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08512v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08512v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16770v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16770v2",
                "updated": "2025-08-14T07:15:21Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    7,
                    15,
                    21,
                    3,
                    226,
                    0
                ],
                "published": "2025-02-24T01:19:43Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    1,
                    19,
                    43,
                    0,
                    55,
                    0
                ],
                "title": "LED-Merging: Mitigating Safety-Utility Conflicts in Model Merging with\n  Location-Election-Disjoint",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LED-Merging: Mitigating Safety-Utility Conflicts in Model Merging with\n  Location-Election-Disjoint"
                },
                "summary": "Fine-tuning pre-trained Large Language Models (LLMs) for specialized tasks\nincurs substantial computational and data costs. While model merging offers a\ntraining-free solution to integrate multiple task-specific models, existing\nmethods suffer from safety-utility conflicts where enhanced general\ncapabilities degrade safety safeguards. We identify two root causes:\n$\\textbf{neuron misidentification}$ due to simplistic parameter magnitude-based\nselection, and $\\textbf{cross-task neuron interference}$ during merging. To\naddress these challenges, we propose $\\textbf{LED-Merging}$, a three-stage\nframework that $\\textbf{L}$ocates task-specific neurons via gradient-based\nattribution, dynamically $\\textbf{E}$lects critical neurons through multi-model\nimportance fusion, and $\\textbf{D}$isjoints conflicting updates through\nparameter isolation. Extensive experiments on Llama-3-8B, Mistral-7B, and\nLlama2-13B demonstrate that LED-Merging effectively reduces harmful response\nrates, showing a 31.4\\% decrease on Llama-3-8B-Instruct on HarmBench, while\nsimultaneously preserving 95\\% of utility performance, such as achieving\n52.39\\% accuracy on GSM8K. LED-Merging resolves safety-utility conflicts and\nprovides a lightweight, training-free paradigm for constructing reliable\nmulti-task LLMs. Code is available at\n$\\href{https://github.com/MqLeet/LED-Merging}{GitHub}$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning pre-trained Large Language Models (LLMs) for specialized tasks\nincurs substantial computational and data costs. While model merging offers a\ntraining-free solution to integrate multiple task-specific models, existing\nmethods suffer from safety-utility conflicts where enhanced general\ncapabilities degrade safety safeguards. We identify two root causes:\n$\\textbf{neuron misidentification}$ due to simplistic parameter magnitude-based\nselection, and $\\textbf{cross-task neuron interference}$ during merging. To\naddress these challenges, we propose $\\textbf{LED-Merging}$, a three-stage\nframework that $\\textbf{L}$ocates task-specific neurons via gradient-based\nattribution, dynamically $\\textbf{E}$lects critical neurons through multi-model\nimportance fusion, and $\\textbf{D}$isjoints conflicting updates through\nparameter isolation. Extensive experiments on Llama-3-8B, Mistral-7B, and\nLlama2-13B demonstrate that LED-Merging effectively reduces harmful response\nrates, showing a 31.4\\% decrease on Llama-3-8B-Instruct on HarmBench, while\nsimultaneously preserving 95\\% of utility performance, such as achieving\n52.39\\% accuracy on GSM8K. LED-Merging resolves safety-utility conflicts and\nprovides a lightweight, training-free paradigm for constructing reliable\nmulti-task LLMs. Code is available at\n$\\href{https://github.com/MqLeet/LED-Merging}{GitHub}$."
                },
                "authors": [
                    {
                        "name": "Qianli Ma"
                    },
                    {
                        "name": "Dongrui Liu"
                    },
                    {
                        "name": "Qian Chen"
                    },
                    {
                        "name": "Linfeng Zhang"
                    },
                    {
                        "name": "Jing Shao"
                    }
                ],
                "author_detail": {
                    "name": "Jing Shao"
                },
                "author": "Jing Shao",
                "arxiv_doi": "10.18653/v1/2025.acl-long.1055",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.18653/v1/2025.acl-long.1055",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.16770v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16770v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by ACL2025 main conference",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2508.10899v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10899v1",
                "updated": "2025-08-14T17:59:37Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    17,
                    59,
                    37,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T17:59:37Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    17,
                    59,
                    37,
                    3,
                    226,
                    0
                ],
                "title": "A Dataset for Distilling Knowledge Priors from Literature for\n  Therapeutic Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Dataset for Distilling Knowledge Priors from Literature for\n  Therapeutic Design"
                },
                "summary": "AI-driven discovery can greatly reduce design time and enhance new\ntherapeutics' effectiveness. Models using simulators explore broad design\nspaces but risk violating implicit constraints due to a lack of experimental\npriors. For example, in a new analysis we performed on a diverse set of models\non the GuacaMol benchmark using supervised classifiers, over 60\\% of molecules\nproposed had high probability of being mutagenic. In this work, we introduce\n\\ourdataset, a dataset of priors for design problems extracted from literature\ndescribing compounds used in lab settings. It is constructed with LLM pipelines\nfor discovering therapeutic entities in relevant paragraphs and summarizing\ninformation in concise fair-use facts. \\ourdataset~ consists of 32.3 million\npairs of natural language facts, and appropriate entity representations (i.e.\nSMILES or refseq IDs). To demonstrate the potential of the data, we train LLM,\nCLIP, and LLava architectures to reason jointly about text and design targets\nand evaluate on tasks from the Therapeutic Data Commons (TDC). \\ourdataset~is\nhighly effective for creating models with strong priors: in supervised\nprediction problems that use our data as pretraining, our best models with 15M\nlearnable parameters outperform larger 2B TxGemma on both regression and\nclassification TDC tasks, and perform comparably to 9B models on average.\nModels built with \\ourdataset~can be used as constraints while optimizing for\nnovel molecules in GuacaMol, resulting in proposals that are safer and nearly\nas effective. We release our dataset at\n\\href{https://huggingface.co/datasets/medexanon/Medex}{huggingface.co/datasets/medexanon/Medex},\nand will provide expanded versions as available literature grows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI-driven discovery can greatly reduce design time and enhance new\ntherapeutics' effectiveness. Models using simulators explore broad design\nspaces but risk violating implicit constraints due to a lack of experimental\npriors. For example, in a new analysis we performed on a diverse set of models\non the GuacaMol benchmark using supervised classifiers, over 60\\% of molecules\nproposed had high probability of being mutagenic. In this work, we introduce\n\\ourdataset, a dataset of priors for design problems extracted from literature\ndescribing compounds used in lab settings. It is constructed with LLM pipelines\nfor discovering therapeutic entities in relevant paragraphs and summarizing\ninformation in concise fair-use facts. \\ourdataset~ consists of 32.3 million\npairs of natural language facts, and appropriate entity representations (i.e.\nSMILES or refseq IDs). To demonstrate the potential of the data, we train LLM,\nCLIP, and LLava architectures to reason jointly about text and design targets\nand evaluate on tasks from the Therapeutic Data Commons (TDC). \\ourdataset~is\nhighly effective for creating models with strong priors: in supervised\nprediction problems that use our data as pretraining, our best models with 15M\nlearnable parameters outperform larger 2B TxGemma on both regression and\nclassification TDC tasks, and perform comparably to 9B models on average.\nModels built with \\ourdataset~can be used as constraints while optimizing for\nnovel molecules in GuacaMol, resulting in proposals that are safer and nearly\nas effective. We release our dataset at\n\\href{https://huggingface.co/datasets/medexanon/Medex}{huggingface.co/datasets/medexanon/Medex},\nand will provide expanded versions as available literature grows."
                },
                "authors": [
                    {
                        "name": "Haydn Thomas Jones"
                    },
                    {
                        "name": "Natalie Maus"
                    },
                    {
                        "name": "Josh Magnus Ludan"
                    },
                    {
                        "name": "Maggie Ziyu Huan"
                    },
                    {
                        "name": "Jiaming Liang"
                    },
                    {
                        "name": "Marcelo Der Torossian Torres"
                    },
                    {
                        "name": "Jiatao Liang"
                    },
                    {
                        "name": "Zachary Ives"
                    },
                    {
                        "name": "Yoseph Barash"
                    },
                    {
                        "name": "Cesar de la Fuente-Nunez"
                    },
                    {
                        "name": "Jacob R. Gardner"
                    },
                    {
                        "name": "Mark Yatskar"
                    }
                ],
                "author_detail": {
                    "name": "Mark Yatskar"
                },
                "author": "Mark Yatskar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10899v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10899v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10535v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10535v2",
                "updated": "2025-08-14T17:58:50Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    17,
                    58,
                    50,
                    3,
                    226,
                    0
                ],
                "published": "2025-07-14T17:56:29Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    17,
                    56,
                    29,
                    0,
                    195,
                    0
                ],
                "title": "CodeJudgeBench: Benchmarking LLM-as-a-Judge for Coding Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeJudgeBench: Benchmarking LLM-as-a-Judge for Coding Tasks"
                },
                "summary": "Large Language Models (LLMs) have significantly advanced the state-of-the-art\nin various coding tasks. Beyond directly answering user queries, LLMs can also\nserve as judges, assessing and comparing the quality of responses generated by\nother models. Such an evaluation capability is crucial both for benchmarking\ndifferent LLMs and for improving response quality through response ranking.\nHowever, despite the growing adoption of the LLM-as-a-Judge paradigm, its\neffectiveness in coding scenarios remains underexplored due to the absence of\ndedicated benchmarks. To address this gap, we introduce CodeJudgeBench, a\nbenchmark explicitly designed to evaluate the performance of LLM-as-a-Judge\nmodels across three critical coding tasks: code generation, code repair, and\nunit test generation. Through comprehensive benchmarking of 26 LLM-as-a-Judge\nmodels, we find that recent thinking models significantly outperform\nnon-thinking models on our carefully designed code judging tasks. Notably, even\nrelatively small thinking models, such as Qwen3-8B, can outperform specially\ntrained LLM-as-a-Judge models up to 70B in size. Nevertheless, all models still\nexhibit significant randomness in their judgment of coding tasks. For pairwise\njudging tasks, simply changing the order in which responses are presented can\nsubstantially impact accuracy. In addition, when judging code and unit tests\nwritten by different LLMs, LLM-as-a-Judge models also show variance in\nperformance. This sensitivity raises concerns about the reliability and\nconsistency of LLM-as-a-Judge in coding scenarios. Lastly, we study optimal\nprompting strategies for LLM-as-a-Judge. We find that using pair-wise\ncomparison outperforms scalar point-wise judging. Furthermore, retaining\ncomments and reasoning in the full, unprocessed LLM response leads to improved\njudge performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have significantly advanced the state-of-the-art\nin various coding tasks. Beyond directly answering user queries, LLMs can also\nserve as judges, assessing and comparing the quality of responses generated by\nother models. Such an evaluation capability is crucial both for benchmarking\ndifferent LLMs and for improving response quality through response ranking.\nHowever, despite the growing adoption of the LLM-as-a-Judge paradigm, its\neffectiveness in coding scenarios remains underexplored due to the absence of\ndedicated benchmarks. To address this gap, we introduce CodeJudgeBench, a\nbenchmark explicitly designed to evaluate the performance of LLM-as-a-Judge\nmodels across three critical coding tasks: code generation, code repair, and\nunit test generation. Through comprehensive benchmarking of 26 LLM-as-a-Judge\nmodels, we find that recent thinking models significantly outperform\nnon-thinking models on our carefully designed code judging tasks. Notably, even\nrelatively small thinking models, such as Qwen3-8B, can outperform specially\ntrained LLM-as-a-Judge models up to 70B in size. Nevertheless, all models still\nexhibit significant randomness in their judgment of coding tasks. For pairwise\njudging tasks, simply changing the order in which responses are presented can\nsubstantially impact accuracy. In addition, when judging code and unit tests\nwritten by different LLMs, LLM-as-a-Judge models also show variance in\nperformance. This sensitivity raises concerns about the reliability and\nconsistency of LLM-as-a-Judge in coding scenarios. Lastly, we study optimal\nprompting strategies for LLM-as-a-Judge. We find that using pair-wise\ncomparison outperforms scalar point-wise judging. Furthermore, retaining\ncomments and reasoning in the full, unprocessed LLM response leads to improved\njudge performance."
                },
                "authors": [
                    {
                        "name": "Hongchao Jiang"
                    },
                    {
                        "name": "Yiming Chen"
                    },
                    {
                        "name": "Yushi Cao"
                    },
                    {
                        "name": "Hung-yi Lee"
                    },
                    {
                        "name": "Robby T. Tan"
                    }
                ],
                "author_detail": {
                    "name": "Robby T. Tan"
                },
                "author": "Robby T. Tan",
                "arxiv_comment": "Dataset is available at\n  https://huggingface.co/datasets/mattymchen/codejudgebench",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10535v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10535v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10893v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10893v1",
                "updated": "2025-08-14T17:58:05Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    17,
                    58,
                    5,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T17:58:05Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    17,
                    58,
                    5,
                    3,
                    226,
                    0
                ],
                "title": "STream3R: Scalable Sequential 3D Reconstruction with Causal Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STream3R: Scalable Sequential 3D Reconstruction with Causal Transformer"
                },
                "summary": "We present STream3R, a novel approach to 3D reconstruction that reformulates\npointmap prediction as a decoder-only Transformer problem. Existing\nstate-of-the-art methods for multi-view reconstruction either depend on\nexpensive global optimization or rely on simplistic memory mechanisms that\nscale poorly with sequence length. In contrast, STream3R introduces an\nstreaming framework that processes image sequences efficiently using causal\nattention, inspired by advances in modern language modeling. By learning\ngeometric priors from large-scale 3D datasets, STream3R generalizes well to\ndiverse and challenging scenarios, including dynamic scenes where traditional\nmethods often fail. Extensive experiments show that our method consistently\noutperforms prior work across both static and dynamic scene benchmarks.\nMoreover, STream3R is inherently compatible with LLM-style training\ninfrastructure, enabling efficient large-scale pretraining and fine-tuning for\nvarious downstream 3D tasks. Our results underscore the potential of causal\nTransformer models for online 3D perception, paving the way for real-time 3D\nunderstanding in streaming environments. More details can be found in our\nproject page: https://nirvanalan.github.io/projects/stream3r.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present STream3R, a novel approach to 3D reconstruction that reformulates\npointmap prediction as a decoder-only Transformer problem. Existing\nstate-of-the-art methods for multi-view reconstruction either depend on\nexpensive global optimization or rely on simplistic memory mechanisms that\nscale poorly with sequence length. In contrast, STream3R introduces an\nstreaming framework that processes image sequences efficiently using causal\nattention, inspired by advances in modern language modeling. By learning\ngeometric priors from large-scale 3D datasets, STream3R generalizes well to\ndiverse and challenging scenarios, including dynamic scenes where traditional\nmethods often fail. Extensive experiments show that our method consistently\noutperforms prior work across both static and dynamic scene benchmarks.\nMoreover, STream3R is inherently compatible with LLM-style training\ninfrastructure, enabling efficient large-scale pretraining and fine-tuning for\nvarious downstream 3D tasks. Our results underscore the potential of causal\nTransformer models for online 3D perception, paving the way for real-time 3D\nunderstanding in streaming environments. More details can be found in our\nproject page: https://nirvanalan.github.io/projects/stream3r."
                },
                "authors": [
                    {
                        "name": "Yushi Lan"
                    },
                    {
                        "name": "Yihang Luo"
                    },
                    {
                        "name": "Fangzhou Hong"
                    },
                    {
                        "name": "Shangchen Zhou"
                    },
                    {
                        "name": "Honghua Chen"
                    },
                    {
                        "name": "Zhaoyang Lyu"
                    },
                    {
                        "name": "Shuai Yang"
                    },
                    {
                        "name": "Bo Dai"
                    },
                    {
                        "name": "Chen Change Loy"
                    },
                    {
                        "name": "Xingang Pan"
                    }
                ],
                "author_detail": {
                    "name": "Xingang Pan"
                },
                "author": "Xingang Pan",
                "arxiv_comment": "TL;DR: Streaming 4D reconstruction using causal transformer. Project\n  page: https://nirvanalan.github.io/projects/stream3r",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10893v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10893v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08855v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08855v2",
                "updated": "2025-08-14T17:57:53Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    17,
                    57,
                    53,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-12T11:23:44Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    11,
                    23,
                    44,
                    1,
                    224,
                    0
                ],
                "title": "BiasGym: Fantastic LLM Biases and How to Find (and Remove) Them",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BiasGym: Fantastic LLM Biases and How to Find (and Remove) Them"
                },
                "summary": "Understanding biases and stereotypes encoded in the weights of Large Language\nModels (LLMs) is crucial for developing effective mitigation strategies. Biased\nbehaviour is often subtle and non-trivial to isolate, even when deliberately\nelicited, making systematic analysis and debiasing particularly challenging. To\naddress this, we introduce BiasGym, a simple, cost-effective, and generalizable\nframework for reliably injecting, analyzing, and mitigating conceptual\nassociations within LLMs. BiasGym consists of two components: BiasInject, which\ninjects specific biases into the model via token-based fine-tuning while\nkeeping the model frozen, and BiasScope, which leverages these injected signals\nto identify and steer the components responsible for biased behavior. Our\nmethod enables consistent bias elicitation for mechanistic analysis, supports\ntargeted debiasing without degrading performance on downstream tasks, and\ngeneralizes to biases unseen during token-based fine-tuning. We demonstrate the\neffectiveness of BiasGym in reducing real-world stereotypes (e.g., people from\nItaly being `reckless drivers') and in probing fictional associations (e.g.,\npeople from a fictional country having `blue skin'), showing its utility for\nboth safety interventions and interpretability research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding biases and stereotypes encoded in the weights of Large Language\nModels (LLMs) is crucial for developing effective mitigation strategies. Biased\nbehaviour is often subtle and non-trivial to isolate, even when deliberately\nelicited, making systematic analysis and debiasing particularly challenging. To\naddress this, we introduce BiasGym, a simple, cost-effective, and generalizable\nframework for reliably injecting, analyzing, and mitigating conceptual\nassociations within LLMs. BiasGym consists of two components: BiasInject, which\ninjects specific biases into the model via token-based fine-tuning while\nkeeping the model frozen, and BiasScope, which leverages these injected signals\nto identify and steer the components responsible for biased behavior. Our\nmethod enables consistent bias elicitation for mechanistic analysis, supports\ntargeted debiasing without degrading performance on downstream tasks, and\ngeneralizes to biases unseen during token-based fine-tuning. We demonstrate the\neffectiveness of BiasGym in reducing real-world stereotypes (e.g., people from\nItaly being `reckless drivers') and in probing fictional associations (e.g.,\npeople from a fictional country having `blue skin'), showing its utility for\nboth safety interventions and interpretability research."
                },
                "authors": [
                    {
                        "name": "Sekh Mainul Islam"
                    },
                    {
                        "name": "Nadav Borenstein"
                    },
                    {
                        "name": "Siddhesh Milind Pawar"
                    },
                    {
                        "name": "Haeun Yu"
                    },
                    {
                        "name": "Arnav Arora"
                    },
                    {
                        "name": "Isabelle Augenstein"
                    }
                ],
                "author_detail": {
                    "name": "Isabelle Augenstein"
                },
                "author": "Isabelle Augenstein",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08855v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08855v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10880v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10880v1",
                "updated": "2025-08-14T17:49:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    17,
                    49,
                    9,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T17:49:09Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    17,
                    49,
                    9,
                    3,
                    226,
                    0
                ],
                "title": "Searching for Privacy Risks in LLM Agents via Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Searching for Privacy Risks in LLM Agents via Simulation"
                },
                "summary": "The widespread deployment of LLM-based agents is likely to introduce a\ncritical privacy threat: malicious agents that proactively engage others in\nmulti-turn interactions to extract sensitive information. These dynamic\ndialogues enable adaptive attack strategies that can cause severe privacy\nviolations, yet their evolving nature makes it difficult to anticipate and\ndiscover sophisticated vulnerabilities manually. To tackle this problem, we\npresent a search-based framework that alternates between improving attacker and\ndefender instructions by simulating privacy-critical agent interactions. Each\nsimulation involves three roles: data subject, data sender, and data recipient.\nWhile the data subject's behavior is fixed, the attacker (data recipient)\nattempts to extract sensitive information from the defender (data sender)\nthrough persistent and interactive exchanges. To explore this interaction space\nefficiently, our search algorithm employs LLMs as optimizers, using parallel\nsearch with multiple threads and cross-thread propagation to analyze simulation\ntrajectories and iteratively propose new instructions. Through this process, we\nfind that attack strategies escalate from simple direct requests to\nsophisticated multi-turn tactics such as impersonation and consent forgery,\nwhile defenses advance from rule-based constraints to identity-verification\nstate machines. The discovered attacks and defenses transfer across diverse\nscenarios and backbone models, demonstrating strong practical utility for\nbuilding privacy-aware agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread deployment of LLM-based agents is likely to introduce a\ncritical privacy threat: malicious agents that proactively engage others in\nmulti-turn interactions to extract sensitive information. These dynamic\ndialogues enable adaptive attack strategies that can cause severe privacy\nviolations, yet their evolving nature makes it difficult to anticipate and\ndiscover sophisticated vulnerabilities manually. To tackle this problem, we\npresent a search-based framework that alternates between improving attacker and\ndefender instructions by simulating privacy-critical agent interactions. Each\nsimulation involves three roles: data subject, data sender, and data recipient.\nWhile the data subject's behavior is fixed, the attacker (data recipient)\nattempts to extract sensitive information from the defender (data sender)\nthrough persistent and interactive exchanges. To explore this interaction space\nefficiently, our search algorithm employs LLMs as optimizers, using parallel\nsearch with multiple threads and cross-thread propagation to analyze simulation\ntrajectories and iteratively propose new instructions. Through this process, we\nfind that attack strategies escalate from simple direct requests to\nsophisticated multi-turn tactics such as impersonation and consent forgery,\nwhile defenses advance from rule-based constraints to identity-verification\nstate machines. The discovered attacks and defenses transfer across diverse\nscenarios and backbone models, demonstrating strong practical utility for\nbuilding privacy-aware agents."
                },
                "authors": [
                    {
                        "name": "Yanzhe Zhang"
                    },
                    {
                        "name": "Diyi Yang"
                    }
                ],
                "author_detail": {
                    "name": "Diyi Yang"
                },
                "author": "Diyi Yang",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10880v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10880v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10874v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10874v1",
                "updated": "2025-08-14T17:46:01Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    17,
                    46,
                    1,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T17:46:01Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    17,
                    46,
                    1,
                    3,
                    226,
                    0
                ],
                "title": "SSRL: Self-Search Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SSRL: Self-Search Reinforcement Learning"
                },
                "summary": "We investigate the potential of large language models (LLMs) to serve as\nefficient simulators for agentic search tasks in reinforcement learning (RL),\nthereby reducing dependence on costly interactions with external search\nengines. To this end, we first quantify the intrinsic search capability of LLMs\nvia structured prompting and repeated sampling, which we term Self-Search. Our\nresults reveal that LLMs exhibit strong scaling behavior with respect to the\ninference budget, achieving high pass@k on question-answering benchmarks,\nincluding the challenging BrowseComp task. Building on these observations, we\nintroduce Self-Search RL (SSRL), which enhances LLMs' Self-Search capability\nthrough format-based and rule-based rewards. SSRL enables models to iteratively\nrefine their knowledge utilization internally, without requiring access to\nexternal tools. Empirical evaluations demonstrate that SSRL-trained policy\nmodels provide a cost-effective and stable environment for search-driven RL\ntraining, reducing reliance on external search engines and facilitating robust\nsim-to-real transfer. We draw the following conclusions: 1) LLMs possess world\nknowledge that can be effectively elicited to achieve high performance; 2) SSRL\ndemonstrates the potential of leveraging internal knowledge to reduce\nhallucination; 3) SSRL-trained models integrate seamlessly with external search\nengines without additional effort. Our findings highlight the potential of LLMs\nto support more scalable RL agent training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate the potential of large language models (LLMs) to serve as\nefficient simulators for agentic search tasks in reinforcement learning (RL),\nthereby reducing dependence on costly interactions with external search\nengines. To this end, we first quantify the intrinsic search capability of LLMs\nvia structured prompting and repeated sampling, which we term Self-Search. Our\nresults reveal that LLMs exhibit strong scaling behavior with respect to the\ninference budget, achieving high pass@k on question-answering benchmarks,\nincluding the challenging BrowseComp task. Building on these observations, we\nintroduce Self-Search RL (SSRL), which enhances LLMs' Self-Search capability\nthrough format-based and rule-based rewards. SSRL enables models to iteratively\nrefine their knowledge utilization internally, without requiring access to\nexternal tools. Empirical evaluations demonstrate that SSRL-trained policy\nmodels provide a cost-effective and stable environment for search-driven RL\ntraining, reducing reliance on external search engines and facilitating robust\nsim-to-real transfer. We draw the following conclusions: 1) LLMs possess world\nknowledge that can be effectively elicited to achieve high performance; 2) SSRL\ndemonstrates the potential of leveraging internal knowledge to reduce\nhallucination; 3) SSRL-trained models integrate seamlessly with external search\nengines without additional effort. Our findings highlight the potential of LLMs\nto support more scalable RL agent training."
                },
                "authors": [
                    {
                        "name": "Yuchen Fan"
                    },
                    {
                        "name": "Kaiyan Zhang"
                    },
                    {
                        "name": "Heng Zhou"
                    },
                    {
                        "name": "Yuxin Zuo"
                    },
                    {
                        "name": "Yanxu Chen"
                    },
                    {
                        "name": "Yu Fu"
                    },
                    {
                        "name": "Xinwei Long"
                    },
                    {
                        "name": "Xuekai Zhu"
                    },
                    {
                        "name": "Che Jiang"
                    },
                    {
                        "name": "Yuchen Zhang"
                    },
                    {
                        "name": "Li Kang"
                    },
                    {
                        "name": "Gang Chen"
                    },
                    {
                        "name": "Cheng Huang"
                    },
                    {
                        "name": "Zhizhou He"
                    },
                    {
                        "name": "Bingning Wang"
                    },
                    {
                        "name": "Lei Bai"
                    },
                    {
                        "name": "Ning Ding"
                    },
                    {
                        "name": "Bowen Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Bowen Zhou"
                },
                "author": "Bowen Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10874v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10874v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10872v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10872v1",
                "updated": "2025-08-14T17:44:51Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    17,
                    44,
                    51,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T17:44:51Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    17,
                    44,
                    51,
                    3,
                    226,
                    0
                ],
                "title": "TLE-Based A2C Agent for Terrestrial Coverage Orbital Path Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TLE-Based A2C Agent for Terrestrial Coverage Orbital Path Planning"
                },
                "summary": "The increasing congestion of Low Earth Orbit (LEO) poses persistent\nchallenges to the efficient deployment and safe operation of Earth observation\nsatellites. Mission planners must now account not only for mission-specific\nrequirements but also for the increasing collision risk with active satellites\nand space debris. This work presents a reinforcement learning framework using\nthe Advantage Actor-Critic (A2C) algorithm to optimize satellite orbital\nparameters for precise terrestrial coverage within predefined surface radii. By\nformulating the problem as a Markov Decision Process (MDP) within a custom\nOpenAI Gymnasium environment, our method simulates orbital dynamics using\nclassical Keplerian elements. The agent progressively learns to adjust five of\nthe orbital parameters - semi-major axis, eccentricity, inclination, right\nascension of ascending node, and the argument of perigee-to achieve targeted\nterrestrial coverage. Comparative evaluation against Proximal Policy\nOptimization (PPO) demonstrates A2C's superior performance, achieving 5.8x\nhigher cumulative rewards (10.0 vs 9.263025) while converging in 31.5x fewer\ntimesteps (2,000 vs 63,000). The A2C agent consistently meets mission\nobjectives across diverse target coordinates while maintaining computational\nefficiency suitable for real-time mission planning applications. Key\ncontributions include: (1) a TLE-based orbital simulation environment\nincorporating physics constraints, (2) validation of actor-critic methods'\nsuperiority over trust region approaches in continuous orbital control, and (3)\ndemonstration of rapid convergence enabling adaptive satellite deployment. This\napproach establishes reinforcement learning as a computationally efficient\nalternative for scalable and intelligent LEO mission planning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing congestion of Low Earth Orbit (LEO) poses persistent\nchallenges to the efficient deployment and safe operation of Earth observation\nsatellites. Mission planners must now account not only for mission-specific\nrequirements but also for the increasing collision risk with active satellites\nand space debris. This work presents a reinforcement learning framework using\nthe Advantage Actor-Critic (A2C) algorithm to optimize satellite orbital\nparameters for precise terrestrial coverage within predefined surface radii. By\nformulating the problem as a Markov Decision Process (MDP) within a custom\nOpenAI Gymnasium environment, our method simulates orbital dynamics using\nclassical Keplerian elements. The agent progressively learns to adjust five of\nthe orbital parameters - semi-major axis, eccentricity, inclination, right\nascension of ascending node, and the argument of perigee-to achieve targeted\nterrestrial coverage. Comparative evaluation against Proximal Policy\nOptimization (PPO) demonstrates A2C's superior performance, achieving 5.8x\nhigher cumulative rewards (10.0 vs 9.263025) while converging in 31.5x fewer\ntimesteps (2,000 vs 63,000). The A2C agent consistently meets mission\nobjectives across diverse target coordinates while maintaining computational\nefficiency suitable for real-time mission planning applications. Key\ncontributions include: (1) a TLE-based orbital simulation environment\nincorporating physics constraints, (2) validation of actor-critic methods'\nsuperiority over trust region approaches in continuous orbital control, and (3)\ndemonstration of rapid convergence enabling adaptive satellite deployment. This\napproach establishes reinforcement learning as a computationally efficient\nalternative for scalable and intelligent LEO mission planning."
                },
                "authors": [
                    {
                        "name": "Anantha Narayanan"
                    },
                    {
                        "name": "Battu Bhanu Teja"
                    },
                    {
                        "name": "Pruthwik Mishra"
                    }
                ],
                "author_detail": {
                    "name": "Pruthwik Mishra"
                },
                "author": "Pruthwik Mishra",
                "arxiv_comment": "8 pages, 6 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10872v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10872v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10865v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10865v1",
                "updated": "2025-08-14T17:35:31Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    17,
                    35,
                    31,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T17:35:31Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    17,
                    35,
                    31,
                    3,
                    226,
                    0
                ],
                "title": "Performance of GPT-5 in Brain Tumor MRI Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance of GPT-5 in Brain Tumor MRI Reasoning"
                },
                "summary": "Accurate differentiation of brain tumor types on magnetic resonance imaging\n(MRI) is critical for guiding treatment planning in neuro-oncology. Recent\nadvances in large language models (LLMs) have enabled visual question answering\n(VQA) approaches that integrate image interpretation with natural language\nreasoning. In this study, we evaluated GPT-4o, GPT-5-nano, GPT-5-mini, and\nGPT-5 on a curated brain tumor VQA benchmark derived from 3 Brain Tumor\nSegmentation (BraTS) datasets - glioblastoma (GLI), meningioma (MEN), and brain\nmetastases (MET). Each case included multi-sequence MRI triplanar mosaics and\nstructured clinical features transformed into standardized VQA items. Models\nwere assessed in a zero-shot chain-of-thought setting for accuracy on both\nvisual and reasoning tasks. Results showed that GPT-5-mini achieved the highest\nmacro-average accuracy (44.19%), followed by GPT-5 (43.71%), GPT-4o (41.49%),\nand GPT-5-nano (35.85%). Performance varied by tumor subtype, with no single\nmodel dominating across all cohorts. These findings suggest that GPT-5 family\nmodels can achieve moderate accuracy in structured neuro-oncological VQA tasks,\nbut not at a level acceptable for clinical use.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate differentiation of brain tumor types on magnetic resonance imaging\n(MRI) is critical for guiding treatment planning in neuro-oncology. Recent\nadvances in large language models (LLMs) have enabled visual question answering\n(VQA) approaches that integrate image interpretation with natural language\nreasoning. In this study, we evaluated GPT-4o, GPT-5-nano, GPT-5-mini, and\nGPT-5 on a curated brain tumor VQA benchmark derived from 3 Brain Tumor\nSegmentation (BraTS) datasets - glioblastoma (GLI), meningioma (MEN), and brain\nmetastases (MET). Each case included multi-sequence MRI triplanar mosaics and\nstructured clinical features transformed into standardized VQA items. Models\nwere assessed in a zero-shot chain-of-thought setting for accuracy on both\nvisual and reasoning tasks. Results showed that GPT-5-mini achieved the highest\nmacro-average accuracy (44.19%), followed by GPT-5 (43.71%), GPT-4o (41.49%),\nand GPT-5-nano (35.85%). Performance varied by tumor subtype, with no single\nmodel dominating across all cohorts. These findings suggest that GPT-5 family\nmodels can achieve moderate accuracy in structured neuro-oncological VQA tasks,\nbut not at a level acceptable for clinical use."
                },
                "authors": [
                    {
                        "name": "Mojtaba Safari"
                    },
                    {
                        "name": "Shansong Wang"
                    },
                    {
                        "name": "Mingzhe Hu"
                    },
                    {
                        "name": "Zach Eidex"
                    },
                    {
                        "name": "Qiang Li"
                    },
                    {
                        "name": "Xiaofeng Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaofeng Yang"
                },
                "author": "Xiaofeng Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10865v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10865v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2302.11617v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2302.11617v3",
                "updated": "2025-08-14T17:31:02Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    17,
                    31,
                    2,
                    3,
                    226,
                    0
                ],
                "published": "2023-02-22T19:46:00Z",
                "published_parsed": [
                    2023,
                    2,
                    22,
                    19,
                    46,
                    0,
                    2,
                    53,
                    0
                ],
                "title": "A Reference Architecture for Governance of Cloud Native Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Reference Architecture for Governance of Cloud Native Applications"
                },
                "summary": "The evolution of cloud computing has given rise to Cloud Native Applications\n(CNAs), presenting new challenges in governance, particularly when faced with\nstrict compliance requirements. This work explores the unique characteristics\nof CNAs and their impact on governance. We introduce a comprehensive reference\narchitecture designed to streamline governance across CNAs, along with a sample\nimplementation, offering insights for both single and multi-cloud environments.\nOur architecture seamlessly integrates governance within the CNA framework,\nadhering to a ``battery-included'' philosophy. Tailored for both expansive and\ncompact CNA deployments across various industries, this design enables cloud\npractitioners to prioritize product development by alleviating the complexities\nassociated with governance. In addition, it provides a building block for\nacademic exploration of generic CNA frameworks, highlighting their relevance in\nthe evolving cloud computing landscape.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The evolution of cloud computing has given rise to Cloud Native Applications\n(CNAs), presenting new challenges in governance, particularly when faced with\nstrict compliance requirements. This work explores the unique characteristics\nof CNAs and their impact on governance. We introduce a comprehensive reference\narchitecture designed to streamline governance across CNAs, along with a sample\nimplementation, offering insights for both single and multi-cloud environments.\nOur architecture seamlessly integrates governance within the CNA framework,\nadhering to a ``battery-included'' philosophy. Tailored for both expansive and\ncompact CNA deployments across various industries, this design enables cloud\npractitioners to prioritize product development by alleviating the complexities\nassociated with governance. In addition, it provides a building block for\nacademic exploration of generic CNA frameworks, highlighting their relevance in\nthe evolving cloud computing landscape."
                },
                "authors": [
                    {
                        "name": "William Pourmajidi"
                    },
                    {
                        "name": "Lei Zhang"
                    },
                    {
                        "name": "John Steinbacher"
                    },
                    {
                        "name": "Tony Erwin"
                    },
                    {
                        "name": "Andriy Miranskyy"
                    }
                ],
                "author_detail": {
                    "name": "Andriy Miranskyy"
                },
                "author": "Andriy Miranskyy",
                "arxiv_doi": "10.1109/TCC.2025.3578557",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TCC.2025.3578557",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2302.11617v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2302.11617v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "In IEEE Transactions on Cloud Computing, 2025",
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10848v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10848v1",
                "updated": "2025-08-14T17:18:35Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    17,
                    18,
                    35,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T17:18:35Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    17,
                    18,
                    35,
                    3,
                    226,
                    0
                ],
                "title": "Psyche-R1: Towards Reliable Psychological LLMs through Unified Empathy,\n  Expertise, and Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Psyche-R1: Towards Reliable Psychological LLMs through Unified Empathy,\n  Expertise, and Reasoning"
                },
                "summary": "Amidst a shortage of qualified mental health professionals, the integration\nof large language models (LLMs) into psychological applications offers a\npromising way to alleviate the growing burden of mental health disorders.\nRecent reasoning-augmented LLMs have achieved remarkable performance in\nmathematics and programming, while research in the psychological domain has\npredominantly emphasized emotional support and empathetic dialogue, with\nlimited attention to reasoning mechanisms that are beneficial to generating\nreliable responses. Therefore, in this paper, we propose Psyche-R1, the first\nChinese psychological LLM that jointly integrates empathy, psychological\nexpertise, and reasoning, built upon a novel data curation pipeline.\nSpecifically, we design a comprehensive data synthesis pipeline that produces\nover 75k high-quality psychological questions paired with detailed rationales,\ngenerated through chain-of-thought (CoT) reasoning and iterative\nprompt-rationale optimization, along with 73k empathetic dialogues.\nSubsequently, we employ a hybrid training strategy wherein challenging samples\nare identified through a multi-LLM cross-selection strategy for group relative\npolicy optimization (GRPO) to improve reasoning ability, while the remaining\ndata is used for supervised fine-tuning (SFT) to enhance empathetic response\ngeneration and psychological domain knowledge. Extensive experiment results\ndemonstrate the effectiveness of the Psyche-R1 across several psychological\nbenchmarks, where our 7B Psyche-R1 achieves comparable results to 671B\nDeepSeek-R1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Amidst a shortage of qualified mental health professionals, the integration\nof large language models (LLMs) into psychological applications offers a\npromising way to alleviate the growing burden of mental health disorders.\nRecent reasoning-augmented LLMs have achieved remarkable performance in\nmathematics and programming, while research in the psychological domain has\npredominantly emphasized emotional support and empathetic dialogue, with\nlimited attention to reasoning mechanisms that are beneficial to generating\nreliable responses. Therefore, in this paper, we propose Psyche-R1, the first\nChinese psychological LLM that jointly integrates empathy, psychological\nexpertise, and reasoning, built upon a novel data curation pipeline.\nSpecifically, we design a comprehensive data synthesis pipeline that produces\nover 75k high-quality psychological questions paired with detailed rationales,\ngenerated through chain-of-thought (CoT) reasoning and iterative\nprompt-rationale optimization, along with 73k empathetic dialogues.\nSubsequently, we employ a hybrid training strategy wherein challenging samples\nare identified through a multi-LLM cross-selection strategy for group relative\npolicy optimization (GRPO) to improve reasoning ability, while the remaining\ndata is used for supervised fine-tuning (SFT) to enhance empathetic response\ngeneration and psychological domain knowledge. Extensive experiment results\ndemonstrate the effectiveness of the Psyche-R1 across several psychological\nbenchmarks, where our 7B Psyche-R1 achieves comparable results to 671B\nDeepSeek-R1."
                },
                "authors": [
                    {
                        "name": "Chongyuan Dai"
                    },
                    {
                        "name": "Jinpeng Hu"
                    },
                    {
                        "name": "Hongchang Shi"
                    },
                    {
                        "name": "Zhuo Li"
                    },
                    {
                        "name": "Xun Yang"
                    },
                    {
                        "name": "Meng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Meng Wang"
                },
                "author": "Meng Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10848v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10848v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05571v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05571v2",
                "updated": "2025-08-14T17:13:26Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    17,
                    13,
                    26,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-07T17:02:23Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    17,
                    2,
                    23,
                    3,
                    219,
                    0
                ],
                "title": "iFairy: the First 2-bit Complex LLM with All Parameters in $\\{\\pm1, \\pm\n  i\\}$",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "iFairy: the First 2-bit Complex LLM with All Parameters in $\\{\\pm1, \\pm\n  i\\}$"
                },
                "summary": "Quantization-Aware Training (QAT) integrates quantization into the training\nloop, enabling LLMs to learn robust low-bit representations, and is widely\nrecognized as one of the most promising research directions. All current QAT\nresearch focuses on minimizing quantization error on full-precision models,\nwhere the full-precision accuracy acts as an upper bound (accuracy ceiling). No\nexisting method has even attempted to surpass this ceiling. To break this\nceiling, we propose a new paradigm: raising the ceiling (full-precision model),\nand then still quantizing it efficiently into 2 bits. We propose Fairy$\\pm i$,\nthe first 2-bit quantization framework for complex-valued LLMs. Specifically,\nour method leverages the representational advantages of the complex domain to\nboost full-precision accuracy. We map weights to the fourth roots of unity\n$\\{\\pm1, \\pm i\\}$, forming a perfectly symmetric and information-theoretically\noptimal 2-bit representation. Importantly, each quantized weight has either a\nzero real or imaginary part, enabling multiplication-free inference using only\nadditions and element swaps. Experimental results show that Fairy$\\pm i$\noutperforms the ceiling of existing 2-bit quantization approaches in terms of\nboth PPL and downstream tasks, while maintaining strict storage and compute\nefficiency. This work opens a new direction for building highly accurate and\npractical LLMs under extremely low-bit constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization-Aware Training (QAT) integrates quantization into the training\nloop, enabling LLMs to learn robust low-bit representations, and is widely\nrecognized as one of the most promising research directions. All current QAT\nresearch focuses on minimizing quantization error on full-precision models,\nwhere the full-precision accuracy acts as an upper bound (accuracy ceiling). No\nexisting method has even attempted to surpass this ceiling. To break this\nceiling, we propose a new paradigm: raising the ceiling (full-precision model),\nand then still quantizing it efficiently into 2 bits. We propose Fairy$\\pm i$,\nthe first 2-bit quantization framework for complex-valued LLMs. Specifically,\nour method leverages the representational advantages of the complex domain to\nboost full-precision accuracy. We map weights to the fourth roots of unity\n$\\{\\pm1, \\pm i\\}$, forming a perfectly symmetric and information-theoretically\noptimal 2-bit representation. Importantly, each quantized weight has either a\nzero real or imaginary part, enabling multiplication-free inference using only\nadditions and element swaps. Experimental results show that Fairy$\\pm i$\noutperforms the ceiling of existing 2-bit quantization approaches in terms of\nboth PPL and downstream tasks, while maintaining strict storage and compute\nefficiency. This work opens a new direction for building highly accurate and\npractical LLMs under extremely low-bit constraints."
                },
                "authors": [
                    {
                        "name": "Feiyu Wang"
                    },
                    {
                        "name": "Guoan Wang"
                    },
                    {
                        "name": "Yihao Zhang"
                    },
                    {
                        "name": "Shengfan Wang"
                    },
                    {
                        "name": "Weitao Li"
                    },
                    {
                        "name": "Bokai Huang"
                    },
                    {
                        "name": "Shimao Chen"
                    },
                    {
                        "name": "Zihan Jiang"
                    },
                    {
                        "name": "Rui Xu"
                    },
                    {
                        "name": "Tong Yang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Yang"
                },
                "author": "Tong Yang",
                "arxiv_comment": "15 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05571v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05571v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20839v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20839v2",
                "updated": "2025-08-14T17:09:59Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    17,
                    9,
                    59,
                    3,
                    226,
                    0
                ],
                "published": "2025-03-26T12:49:26Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    12,
                    49,
                    26,
                    2,
                    85,
                    0
                ],
                "title": "TAR: Teacher-Aligned Representations via Contrastive Learning for\n  Quadrupedal Locomotion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TAR: Teacher-Aligned Representations via Contrastive Learning for\n  Quadrupedal Locomotion"
                },
                "summary": "Quadrupedal locomotion via Reinforcement Learning (RL) is commonly addressed\nusing the teacher-student paradigm, where a privileged teacher guides a\nproprioceptive student policy. However, key challenges such as representation\nmisalignment between privileged teacher and proprioceptive-only student,\ncovariate shift due to behavioral cloning, and lack of deployable adaptation;\nlead to poor generalization in real-world scenarios. We propose Teacher-Aligned\nRepresentations via Contrastive Learning (TAR), a framework that leverages\nprivileged information with self-supervised contrastive learning to bridge this\ngap. By aligning representations to a privileged teacher in simulation via\ncontrastive objectives, our student policy learns structured latent spaces and\nexhibits robust generalization to Out-of-Distribution (OOD) scenarios,\nsurpassing the fully privileged \"Teacher\". Results showed accelerated training\nby 2x compared to state-of-the-art baselines to achieve peak performance. OOD\nscenarios showed better generalization by 40% on average compared to existing\nmethods. Moreover, TAR transitions seamlessly into learning during deployment\nwithout requiring privileged states, setting a new benchmark in\nsample-efficient, adaptive locomotion and enabling continual fine-tuning in\nreal-world scenarios. Open-source code and videos are available at\nhttps://amrmousa.com/TARLoco/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quadrupedal locomotion via Reinforcement Learning (RL) is commonly addressed\nusing the teacher-student paradigm, where a privileged teacher guides a\nproprioceptive student policy. However, key challenges such as representation\nmisalignment between privileged teacher and proprioceptive-only student,\ncovariate shift due to behavioral cloning, and lack of deployable adaptation;\nlead to poor generalization in real-world scenarios. We propose Teacher-Aligned\nRepresentations via Contrastive Learning (TAR), a framework that leverages\nprivileged information with self-supervised contrastive learning to bridge this\ngap. By aligning representations to a privileged teacher in simulation via\ncontrastive objectives, our student policy learns structured latent spaces and\nexhibits robust generalization to Out-of-Distribution (OOD) scenarios,\nsurpassing the fully privileged \"Teacher\". Results showed accelerated training\nby 2x compared to state-of-the-art baselines to achieve peak performance. OOD\nscenarios showed better generalization by 40% on average compared to existing\nmethods. Moreover, TAR transitions seamlessly into learning during deployment\nwithout requiring privileged states, setting a new benchmark in\nsample-efficient, adaptive locomotion and enabling continual fine-tuning in\nreal-world scenarios. Open-source code and videos are available at\nhttps://amrmousa.com/TARLoco/."
                },
                "authors": [
                    {
                        "name": "Amr Mousa"
                    },
                    {
                        "name": "Neil Karavis"
                    },
                    {
                        "name": "Michele Caprio"
                    },
                    {
                        "name": "Wei Pan"
                    },
                    {
                        "name": "Richard Allmendinger"
                    }
                ],
                "author_detail": {
                    "name": "Richard Allmendinger"
                },
                "author": "Richard Allmendinger",
                "arxiv_comment": "This work has been accepted for publication at the IEEE/RSJ\n  International Conference on Intelligent Robots and Systems (IROS) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20839v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20839v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10840v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10840v1",
                "updated": "2025-08-14T17:06:50Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    17,
                    6,
                    50,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T17:06:50Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    17,
                    6,
                    50,
                    3,
                    226,
                    0
                ],
                "title": "Generalizable Federated Learning using Client Adaptive Focal Modulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generalizable Federated Learning using Client Adaptive Focal Modulation"
                },
                "summary": "Federated learning (FL) has proven essential for privacy-preserving,\ncollaborative training across distributed clients. Our prior work, TransFed,\nintroduced a robust transformer-based FL framework that leverages a\nlearn-to-adapt hypernetwork to generate personalized focal modulation layers\nper client, outperforming traditional methods in non-IID and cross-domain\nsettings. In this extended version, we propose AdaptFED, where we deepen the\ninvestigation of focal modulation in generalizable FL by incorporating: (1) a\nrefined adaptation strategy that integrates task-aware client embeddings to\npersonalize modulation dynamics further, (2) enhanced theoretical bounds on\nadaptation performance, and (3) broader empirical validation across additional\nmodalities, including time-series and multilingual data. We also introduce an\nefficient variant of TransFed that reduces server-client communication overhead\nvia low-rank hypernetwork conditioning, enabling scalable deployment in\nresource-constrained environments. Extensive experiments on eight diverse\ndatasets reaffirm the superiority of our method over state-of-the-art\nbaselines, particularly in source-free and cross-task federated setups. Our\nfindings not only extend the capabilities of focal modulation in FL but also\npave the way for more adaptive, scalable, and generalizable transformer-based\nfederated systems. The code is available at\nhttp://github.com/Tajamul21/TransFed",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated learning (FL) has proven essential for privacy-preserving,\ncollaborative training across distributed clients. Our prior work, TransFed,\nintroduced a robust transformer-based FL framework that leverages a\nlearn-to-adapt hypernetwork to generate personalized focal modulation layers\nper client, outperforming traditional methods in non-IID and cross-domain\nsettings. In this extended version, we propose AdaptFED, where we deepen the\ninvestigation of focal modulation in generalizable FL by incorporating: (1) a\nrefined adaptation strategy that integrates task-aware client embeddings to\npersonalize modulation dynamics further, (2) enhanced theoretical bounds on\nadaptation performance, and (3) broader empirical validation across additional\nmodalities, including time-series and multilingual data. We also introduce an\nefficient variant of TransFed that reduces server-client communication overhead\nvia low-rank hypernetwork conditioning, enabling scalable deployment in\nresource-constrained environments. Extensive experiments on eight diverse\ndatasets reaffirm the superiority of our method over state-of-the-art\nbaselines, particularly in source-free and cross-task federated setups. Our\nfindings not only extend the capabilities of focal modulation in FL but also\npave the way for more adaptive, scalable, and generalizable transformer-based\nfederated systems. The code is available at\nhttp://github.com/Tajamul21/TransFed"
                },
                "authors": [
                    {
                        "name": "Tajamul Ashraf"
                    },
                    {
                        "name": "Iqra Altaf Gillani"
                    }
                ],
                "author_detail": {
                    "name": "Iqra Altaf Gillani"
                },
                "author": "Iqra Altaf Gillani",
                "arxiv_comment": "WACV 2024 Extended Paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10840v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10840v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10839v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10839v1",
                "updated": "2025-08-14T17:05:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    17,
                    5,
                    44,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T17:05:44Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    17,
                    5,
                    44,
                    3,
                    226,
                    0
                ],
                "title": "Reinforced Language Models for Sequential Decision Making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforced Language Models for Sequential Decision Making"
                },
                "summary": "Large Language Models (LLMs) show potential as sequential decision-making\nagents, but their application is often limited due to a reliance on large,\ncomputationally expensive models. This creates a need to improve smaller\nmodels, yet existing post-training methods are designed for single-turn\ninteractions and cannot handle credit assignment in multi-step agentic tasks.\nTo address this, we introduce Multi-Step Group-Relative Policy Optimization\n(MS-GRPO), a new algorithm for post-training LLM agents, grounded in formal\nText-Mediated Stochastic Game (TSMG) and Language-Agent Policy (LAP)\nframeworks. For credit assignment, MS-GRPO attributes the entire cumulative\nepisode reward to each individual episode step. We supplement this algorithm\nwith a novel absolute-advantage-weighted episode sampling strategy that we show\nimproves training performance. We evaluate our approach by post-training a\n3-billion parameter model on Snake and Frozen Lake. Our experiments demonstrate\nthat the method is effective in improving decision-making performance: our\npost-trained 3B parameter model outperforms a 72B parameter baseline by 50% on\nthe Frozen Lake task. This work demonstrates that targeted post-training is a\npractical and efficient alternative to relying on model scale for creating\nsequential decision-making agents using LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) show potential as sequential decision-making\nagents, but their application is often limited due to a reliance on large,\ncomputationally expensive models. This creates a need to improve smaller\nmodels, yet existing post-training methods are designed for single-turn\ninteractions and cannot handle credit assignment in multi-step agentic tasks.\nTo address this, we introduce Multi-Step Group-Relative Policy Optimization\n(MS-GRPO), a new algorithm for post-training LLM agents, grounded in formal\nText-Mediated Stochastic Game (TSMG) and Language-Agent Policy (LAP)\nframeworks. For credit assignment, MS-GRPO attributes the entire cumulative\nepisode reward to each individual episode step. We supplement this algorithm\nwith a novel absolute-advantage-weighted episode sampling strategy that we show\nimproves training performance. We evaluate our approach by post-training a\n3-billion parameter model on Snake and Frozen Lake. Our experiments demonstrate\nthat the method is effective in improving decision-making performance: our\npost-trained 3B parameter model outperforms a 72B parameter baseline by 50% on\nthe Frozen Lake task. This work demonstrates that targeted post-training is a\npractical and efficient alternative to relying on model scale for creating\nsequential decision-making agents using LLMs."
                },
                "authors": [
                    {
                        "name": "Jim Dilkes"
                    },
                    {
                        "name": "Vahid Yazdanpanah"
                    },
                    {
                        "name": "Sebastian Stein"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Stein"
                },
                "author": "Sebastian Stein",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10839v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10839v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.8",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10817v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10817v1",
                "updated": "2025-08-14T16:43:27Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    16,
                    43,
                    27,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T16:43:27Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    16,
                    43,
                    27,
                    3,
                    226,
                    0
                ],
                "title": "Mobile-Friendly Deep Learning for Plant Disease Detection: A Lightweight\n  CNN Benchmark Across 101 Classes of 33 Crops",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile-Friendly Deep Learning for Plant Disease Detection: A Lightweight\n  CNN Benchmark Across 101 Classes of 33 Crops"
                },
                "summary": "Plant diseases are a major threat to food security globally. It is important\nto develop early detection systems which can accurately detect. The advancement\nin computer vision techniques has the potential to solve this challenge. We\nhave developed a mobile-friendly solution which can accurately classify 101\nplant diseases across 33 crops. We built a comprehensive dataset by combining\ndifferent datasets, Plant Doc, PlantVillage, and PlantWild, all of which are\nfor the same purpose. We evaluated performance across several lightweight\narchitectures - MobileNetV2, MobileNetV3, MobileNetV3-Large, and\nEfficientNet-B0, B1 - specifically chosen for their efficiency on\nresource-constrained devices. The results were promising, with EfficientNet-B1\ndelivering our best performance at 94.7% classification accuracy. This\narchitecture struck an optimal balance between accuracy and computational\nefficiency, making it well-suited for real-world deployment on mobile devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Plant diseases are a major threat to food security globally. It is important\nto develop early detection systems which can accurately detect. The advancement\nin computer vision techniques has the potential to solve this challenge. We\nhave developed a mobile-friendly solution which can accurately classify 101\nplant diseases across 33 crops. We built a comprehensive dataset by combining\ndifferent datasets, Plant Doc, PlantVillage, and PlantWild, all of which are\nfor the same purpose. We evaluated performance across several lightweight\narchitectures - MobileNetV2, MobileNetV3, MobileNetV3-Large, and\nEfficientNet-B0, B1 - specifically chosen for their efficiency on\nresource-constrained devices. The results were promising, with EfficientNet-B1\ndelivering our best performance at 94.7% classification accuracy. This\narchitecture struck an optimal balance between accuracy and computational\nefficiency, making it well-suited for real-world deployment on mobile devices."
                },
                "authors": [
                    {
                        "name": "Anand Kumar"
                    },
                    {
                        "name": "Harminder Pal Monga"
                    },
                    {
                        "name": "Tapasi Brahma"
                    },
                    {
                        "name": "Satyam Kalra"
                    },
                    {
                        "name": "Navas Sherif"
                    }
                ],
                "author_detail": {
                    "name": "Navas Sherif"
                },
                "author": "Navas Sherif",
                "arxiv_comment": "15 pages, 5 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10817v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10817v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10798v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10798v1",
                "updated": "2025-08-14T16:22:01Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    16,
                    22,
                    1,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T16:22:01Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    16,
                    22,
                    1,
                    3,
                    226,
                    0
                ],
                "title": "The SET Perceptual Factors Framework: Towards Assured Perception for\n  Autonomous Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The SET Perceptual Factors Framework: Towards Assured Perception for\n  Autonomous Systems"
                },
                "summary": "Future autonomous systems promise significant societal benefits, yet their\ndeployment raises concerns about safety and trustworthiness. A key concern is\nassuring the reliability of robot perception, as perception seeds safe\ndecision-making. Failures in perception are often due to complex yet common\nenvironmental factors and can lead to accidents that erode public trust. To\naddress this concern, we introduce the SET (Self, Environment, and Target)\nPerceptual Factors Framework. We designed the framework to systematically\nanalyze how factors such as weather, occlusion, or sensor limitations\nnegatively impact perception. To achieve this, the framework employs SET State\nTrees to categorize where such factors originate and SET Factor Trees to model\nhow these sources and factors impact perceptual tasks like object detection or\npose estimation. Next, we develop Perceptual Factor Models using both trees to\nquantify the uncertainty for a given task. Our framework aims to promote\nrigorous safety assurances and cultivate greater public understanding and trust\nin autonomous systems by offering a transparent and standardized method for\nidentifying, modeling, and communicating perceptual risks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Future autonomous systems promise significant societal benefits, yet their\ndeployment raises concerns about safety and trustworthiness. A key concern is\nassuring the reliability of robot perception, as perception seeds safe\ndecision-making. Failures in perception are often due to complex yet common\nenvironmental factors and can lead to accidents that erode public trust. To\naddress this concern, we introduce the SET (Self, Environment, and Target)\nPerceptual Factors Framework. We designed the framework to systematically\nanalyze how factors such as weather, occlusion, or sensor limitations\nnegatively impact perception. To achieve this, the framework employs SET State\nTrees to categorize where such factors originate and SET Factor Trees to model\nhow these sources and factors impact perceptual tasks like object detection or\npose estimation. Next, we develop Perceptual Factor Models using both trees to\nquantify the uncertainty for a given task. Our framework aims to promote\nrigorous safety assurances and cultivate greater public understanding and trust\nin autonomous systems by offering a transparent and standardized method for\nidentifying, modeling, and communicating perceptual risks."
                },
                "authors": [
                    {
                        "name": "Troi Williams"
                    }
                ],
                "author_detail": {
                    "name": "Troi Williams"
                },
                "author": "Troi Williams",
                "arxiv_comment": "4 pages, 4 figures, accepted to the Workshop on Public Trust in\n  Autonomous Systems at the 2025 IEEE International Conference on Robotics &\n  Automation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10798v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10798v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10795v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10795v1",
                "updated": "2025-08-14T16:18:37Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    16,
                    18,
                    37,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T16:18:37Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    16,
                    18,
                    37,
                    3,
                    226,
                    0
                ],
                "title": "Beyond \"Not Novel Enough\": Enriching Scholarly Critique with\n  LLM-Assisted Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond \"Not Novel Enough\": Enriching Scholarly Critique with\n  LLM-Assisted Feedback"
                },
                "summary": "Novelty assessment is a central yet understudied aspect of peer review,\nparticularly in high volume fields like NLP where reviewer capacity is\nincreasingly strained. We present a structured approach for automated novelty\nevaluation that models expert reviewer behavior through three stages: content\nextraction from submissions, retrieval and synthesis of related work, and\nstructured comparison for evidence based assessment. Our method is informed by\na large scale analysis of human written novelty reviews and captures key\npatterns such as independent claim verification and contextual reasoning.\nEvaluated on 182 ICLR 2025 submissions with human annotated reviewer novelty\nassessments, the approach achieves 86.5% alignment with human reasoning and\n75.3% agreement on novelty conclusions - substantially outperforming existing\nLLM based baselines. The method produces detailed, literature aware analyses\nand improves consistency over ad hoc reviewer judgments. These results\nhighlight the potential for structured LLM assisted approaches to support more\nrigorous and transparent peer review without displacing human expertise. Data\nand code are made available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Novelty assessment is a central yet understudied aspect of peer review,\nparticularly in high volume fields like NLP where reviewer capacity is\nincreasingly strained. We present a structured approach for automated novelty\nevaluation that models expert reviewer behavior through three stages: content\nextraction from submissions, retrieval and synthesis of related work, and\nstructured comparison for evidence based assessment. Our method is informed by\na large scale analysis of human written novelty reviews and captures key\npatterns such as independent claim verification and contextual reasoning.\nEvaluated on 182 ICLR 2025 submissions with human annotated reviewer novelty\nassessments, the approach achieves 86.5% alignment with human reasoning and\n75.3% agreement on novelty conclusions - substantially outperforming existing\nLLM based baselines. The method produces detailed, literature aware analyses\nand improves consistency over ad hoc reviewer judgments. These results\nhighlight the potential for structured LLM assisted approaches to support more\nrigorous and transparent peer review without displacing human expertise. Data\nand code are made available."
                },
                "authors": [
                    {
                        "name": "Osama Mohammed Afzal"
                    },
                    {
                        "name": "Preslav Nakov"
                    },
                    {
                        "name": "Tom Hope"
                    },
                    {
                        "name": "Iryna Gurevych"
                    }
                ],
                "author_detail": {
                    "name": "Iryna Gurevych"
                },
                "author": "Iryna Gurevych",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10795v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10795v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13109v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13109v2",
                "updated": "2025-08-14T16:12:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    16,
                    12,
                    44,
                    3,
                    226,
                    0
                ],
                "published": "2025-05-19T13:36:45Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    13,
                    36,
                    45,
                    0,
                    139,
                    0
                ],
                "title": "FreeKV: Boosting KV Cache Retrieval for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FreeKV: Boosting KV Cache Retrieval for Efficient LLM Inference"
                },
                "summary": "Large language models (LLMs) have been widely deployed with rapidly expanding\ncontext windows to support increasingly demanding applications. However, long\ncontexts pose significant deployment challenges, primarily due to the KV cache\nwhose size grows proportionally with context length. While KV cache compression\nmethods are proposed to address this issue, KV dropping methods incur\nconsiderable accuracy loss, and KV retrieval methods suffer from significant\nefficiency bottlenecks. We propose FreeKV, an algorithm-system co-optimization\nframework to enhance KV retrieval efficiency while preserving accuracy. On the\nalgorithm side, FreeKV introduces speculative retrieval to shift the KV\nselection and recall processes out of the critical path, combined with\nfine-grained correction to ensure accuracy. On the system side, FreeKV employs\nhybrid KV layouts across CPU and GPU memory to eliminate fragmented data\ntransfers, and leverages double-buffered streamed recall to further improve\nefficiency. Experiments demonstrate that FreeKV achieves near-lossless accuracy\nacross various scenarios and models, delivering up to 13$\\times$ speedup\ncompared to SOTA KV retrieval methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been widely deployed with rapidly expanding\ncontext windows to support increasingly demanding applications. However, long\ncontexts pose significant deployment challenges, primarily due to the KV cache\nwhose size grows proportionally with context length. While KV cache compression\nmethods are proposed to address this issue, KV dropping methods incur\nconsiderable accuracy loss, and KV retrieval methods suffer from significant\nefficiency bottlenecks. We propose FreeKV, an algorithm-system co-optimization\nframework to enhance KV retrieval efficiency while preserving accuracy. On the\nalgorithm side, FreeKV introduces speculative retrieval to shift the KV\nselection and recall processes out of the critical path, combined with\nfine-grained correction to ensure accuracy. On the system side, FreeKV employs\nhybrid KV layouts across CPU and GPU memory to eliminate fragmented data\ntransfers, and leverages double-buffered streamed recall to further improve\nefficiency. Experiments demonstrate that FreeKV achieves near-lossless accuracy\nacross various scenarios and models, delivering up to 13$\\times$ speedup\ncompared to SOTA KV retrieval methods."
                },
                "authors": [
                    {
                        "name": "Guangda Liu"
                    },
                    {
                        "name": "Chengwei Li"
                    },
                    {
                        "name": "Zhenyu Ning"
                    },
                    {
                        "name": "Minyi Guo"
                    },
                    {
                        "name": "Jieru Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Jieru Zhao"
                },
                "author": "Jieru Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13109v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13109v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10777v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10777v1",
                "updated": "2025-08-14T16:01:10Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    16,
                    1,
                    10,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T16:01:10Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    16,
                    1,
                    10,
                    3,
                    226,
                    0
                ],
                "title": "The Knowledge-Reasoning Dissociation: Fundamental Limitations of LLMs in\n  Clinical Natural Language Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Knowledge-Reasoning Dissociation: Fundamental Limitations of LLMs in\n  Clinical Natural Language Inference"
                },
                "summary": "Large language models are often assumed to acquire increasingly structured,\ngeneralizable internal representations simply by scaling data and parameters.\nWe interrogate this assumption by introducing a Clinical Trial Natural Language\nInference benchmark comprising four reasoning families, Causal Attribution,\nCompositional Grounding, Epistemic Verification, and Risk State Abstraction.\nEach item is paired with a targeted Ground Knowledge and Meta-Level Reasoning\nVerification (GKMRV) probe, allowing us to dissociate failures of factual\naccess from failures of inference. We evaluate six contemporary LLMs under both\ndirect and chain of thought prompting.\n  Models achieve near-ceiling GKMRV accuracy (mean accuracy 0.918) yet perform\npoorly on the main reasoning tasks (mean accuracy 0.25). Despite low accuracy,\noutput inferences are highly consistent across samples (mean 0.87), indicating\na systematic application of underlying heuristics and shortcuts.\n  These results reveal fundamental structural and representational limitations:\ncurrent LLMs often possess the relevant clinical knowledge but lack the\nstructured, composable internal representations needed to deploy it reliably\n(e.g., integrating constraints, weighing evidence, or simulating\ncounterfactuals). Decoupling knowledge from reasoning with GKMRV makes this\ndissociation explicit and measurable, providing an effective framework for\nprobing the reliability of LLMs in high-stakes domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models are often assumed to acquire increasingly structured,\ngeneralizable internal representations simply by scaling data and parameters.\nWe interrogate this assumption by introducing a Clinical Trial Natural Language\nInference benchmark comprising four reasoning families, Causal Attribution,\nCompositional Grounding, Epistemic Verification, and Risk State Abstraction.\nEach item is paired with a targeted Ground Knowledge and Meta-Level Reasoning\nVerification (GKMRV) probe, allowing us to dissociate failures of factual\naccess from failures of inference. We evaluate six contemporary LLMs under both\ndirect and chain of thought prompting.\n  Models achieve near-ceiling GKMRV accuracy (mean accuracy 0.918) yet perform\npoorly on the main reasoning tasks (mean accuracy 0.25). Despite low accuracy,\noutput inferences are highly consistent across samples (mean 0.87), indicating\na systematic application of underlying heuristics and shortcuts.\n  These results reveal fundamental structural and representational limitations:\ncurrent LLMs often possess the relevant clinical knowledge but lack the\nstructured, composable internal representations needed to deploy it reliably\n(e.g., integrating constraints, weighing evidence, or simulating\ncounterfactuals). Decoupling knowledge from reasoning with GKMRV makes this\ndissociation explicit and measurable, providing an effective framework for\nprobing the reliability of LLMs in high-stakes domains."
                },
                "authors": [
                    {
                        "name": "Mal Jullien"
                    },
                    {
                        "name": "Marco Valentino"
                    },
                    {
                        "name": "Andr Freitas"
                    }
                ],
                "author_detail": {
                    "name": "Andr Freitas"
                },
                "author": "Andr Freitas",
                "arxiv_comment": "19 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10777v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10777v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10769v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10769v1",
                "updated": "2025-08-14T15:55:19Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    15,
                    55,
                    19,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T15:55:19Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    15,
                    55,
                    19,
                    3,
                    226,
                    0
                ],
                "title": "Modeling Human Responses to Multimodal AI Content",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling Human Responses to Multimodal AI Content"
                },
                "summary": "As AI-generated content becomes widespread, so does the risk of\nmisinformation. While prior research has primarily focused on identifying\nwhether content is authentic, much less is known about how such content\ninfluences human perception and behavior. In domains like trading or the stock\nmarket, predicting how people react (e.g., whether a news post will go viral),\ncan be more critical than verifying its factual accuracy. To address this, we\ntake a human-centered approach and introduce the MhAIM Dataset, which contains\n154,552 online posts (111,153 of them AI-generated), enabling large-scale\nanalysis of how people respond to AI-generated content. Our human study reveals\nthat people are better at identifying AI content when posts include both text\nand visuals, particularly when inconsistencies exist between the two. We\npropose three new metrics: trustworthiness, impact, and openness, to quantify\nhow users judge and engage with online content. We present T-Lens, an LLM-based\nagent system designed to answer user queries by incorporating predicted human\nresponses to multimodal information. At its core is HR-MCP (Human Response\nModel Context Protocol), built on the standardized Model Context Protocol\n(MCP), enabling seamless integration with any LLM. This integration allows\nT-Lens to better align with human reactions, enhancing both interpretability\nand interaction capabilities. Our work provides empirical insights and\npractical tools to equip LLMs with human-awareness capabilities. By\nhighlighting the complex interplay among AI, human cognition, and information\nreception, our findings suggest actionable strategies for mitigating the risks\nof AI-driven misinformation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As AI-generated content becomes widespread, so does the risk of\nmisinformation. While prior research has primarily focused on identifying\nwhether content is authentic, much less is known about how such content\ninfluences human perception and behavior. In domains like trading or the stock\nmarket, predicting how people react (e.g., whether a news post will go viral),\ncan be more critical than verifying its factual accuracy. To address this, we\ntake a human-centered approach and introduce the MhAIM Dataset, which contains\n154,552 online posts (111,153 of them AI-generated), enabling large-scale\nanalysis of how people respond to AI-generated content. Our human study reveals\nthat people are better at identifying AI content when posts include both text\nand visuals, particularly when inconsistencies exist between the two. We\npropose three new metrics: trustworthiness, impact, and openness, to quantify\nhow users judge and engage with online content. We present T-Lens, an LLM-based\nagent system designed to answer user queries by incorporating predicted human\nresponses to multimodal information. At its core is HR-MCP (Human Response\nModel Context Protocol), built on the standardized Model Context Protocol\n(MCP), enabling seamless integration with any LLM. This integration allows\nT-Lens to better align with human reactions, enhancing both interpretability\nand interaction capabilities. Our work provides empirical insights and\npractical tools to equip LLMs with human-awareness capabilities. By\nhighlighting the complex interplay among AI, human cognition, and information\nreception, our findings suggest actionable strategies for mitigating the risks\nof AI-driven misinformation."
                },
                "authors": [
                    {
                        "name": "Zhiqi Shen"
                    },
                    {
                        "name": "Shaojing Fan"
                    },
                    {
                        "name": "Danni Xu"
                    },
                    {
                        "name": "Terence Sim"
                    },
                    {
                        "name": "Mohan Kankanhalli"
                    }
                ],
                "author_detail": {
                    "name": "Mohan Kankanhalli"
                },
                "author": "Mohan Kankanhalli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10769v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10769v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18773v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18773v2",
                "updated": "2025-08-14T15:37:43Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    15,
                    37,
                    43,
                    3,
                    226,
                    0
                ],
                "published": "2025-03-24T15:22:41Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    15,
                    22,
                    41,
                    0,
                    83,
                    0
                ],
                "title": "BitDecoding: Unlocking Tensor Cores for Long-Context LLMs with Low-Bit\n  KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BitDecoding: Unlocking Tensor Cores for Long-Context LLMs with Low-Bit\n  KV Cache"
                },
                "summary": "The rise of long-context Large Language Models (LLMs) amplifies memory and\nbandwidth demands during autoregressive decoding, as the Key-Value (KV) cache\ngrows with each generated token. Low-bit KV-cache quantization (e.g., 4-bit or\n2-bit) can reduce memory footprint while preserving accuracy, but existing\nsystems suffer from slow decoding due to their exclusive reliance on CUDA\ncores, neglecting Tensor Cores (the primary source of compute on modern GPUs).\nWe present BitDecoding, a new long-context LLM inference system with a low-bit\nKV cache. BitDecoding enables efficient low-bit KV-cache decoding by\ncooperatively leveraging CUDA cores and Tensor Cores. It introduces methods for\nautomatically inducing optimized layouts to exploit Tensor Cores, along with\nwarp-level parallelization strategies for dequantization. For unified system\nsupport, BitDecoding includes a query transformation module supporting diverse\nattention variants, a quantization kernel that supports both tensor-wise and\nchannel-wise scaling used in various quantization algorithms with high\nperformance, and a dequantization kernel with a software-defined pipeline to\ncoordinate CUDA and Tensor Cores execution for mixed-precision operations.\nEvaluated on RTX 4090, A100, and H100, BitDecoding accelerates decoding by up\nto 7.5x, 4.8x, and 8.9x, respectively, over FP16 FlashDecoding-v2, and\nsurpasses the state-of-the-art low-bit system QServe by up to 4.3x. On\nLLaMA-3.1-8B with a 128K context, BitDecoding reduces single-batch decoding\nlatency by 3x, showing substantial improvements for long-context generation.\nThe code is available at https://github.com/DD-DuDa/BitDecoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of long-context Large Language Models (LLMs) amplifies memory and\nbandwidth demands during autoregressive decoding, as the Key-Value (KV) cache\ngrows with each generated token. Low-bit KV-cache quantization (e.g., 4-bit or\n2-bit) can reduce memory footprint while preserving accuracy, but existing\nsystems suffer from slow decoding due to their exclusive reliance on CUDA\ncores, neglecting Tensor Cores (the primary source of compute on modern GPUs).\nWe present BitDecoding, a new long-context LLM inference system with a low-bit\nKV cache. BitDecoding enables efficient low-bit KV-cache decoding by\ncooperatively leveraging CUDA cores and Tensor Cores. It introduces methods for\nautomatically inducing optimized layouts to exploit Tensor Cores, along with\nwarp-level parallelization strategies for dequantization. For unified system\nsupport, BitDecoding includes a query transformation module supporting diverse\nattention variants, a quantization kernel that supports both tensor-wise and\nchannel-wise scaling used in various quantization algorithms with high\nperformance, and a dequantization kernel with a software-defined pipeline to\ncoordinate CUDA and Tensor Cores execution for mixed-precision operations.\nEvaluated on RTX 4090, A100, and H100, BitDecoding accelerates decoding by up\nto 7.5x, 4.8x, and 8.9x, respectively, over FP16 FlashDecoding-v2, and\nsurpasses the state-of-the-art low-bit system QServe by up to 4.3x. On\nLLaMA-3.1-8B with a 128K context, BitDecoding reduces single-batch decoding\nlatency by 3x, showing substantial improvements for long-context generation.\nThe code is available at https://github.com/DD-DuDa/BitDecoding."
                },
                "authors": [
                    {
                        "name": "Dayou Du"
                    },
                    {
                        "name": "Shijie Cao"
                    },
                    {
                        "name": "Jianyi Cheng"
                    },
                    {
                        "name": "Luo Mai"
                    },
                    {
                        "name": "Ting Cao"
                    },
                    {
                        "name": "Mao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Mao Yang"
                },
                "author": "Mao Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18773v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18773v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10751v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10751v1",
                "updated": "2025-08-14T15:34:47Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    15,
                    34,
                    47,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T15:34:47Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    15,
                    34,
                    47,
                    3,
                    226,
                    0
                ],
                "title": "Pass@k Training for Adaptively Balancing Exploration and Exploitation of\n  Large Reasoning Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pass@k Training for Adaptively Balancing Exploration and Exploitation of\n  Large Reasoning Models"
                },
                "summary": "Reinforcement learning with verifiable rewards (RLVR), which typically adopts\nPass@1 as the reward, has faced the issues in balancing exploration and\nexploitation, causing policies to prefer conservative actions, converging to a\nlocal optimum. Identifying an appropriate reward metric is therefore crucial.\nRegarding the prior work, although Pass@k has been used in evaluation, its\nconnection to LLM exploration ability in RLVR remains largely overlooked. To\ninvestigate this, we first use Pass@k as the reward to train the policy model\n(i.e., $\\textbf{Pass@k Training}$), and observe the improvement on its\nexploration ability. Next, we derive an analytical solution for the advantage\nof Pass@k Training, leading to an efficient and effective process. Building on\nthis, our analysis reveals that exploration and exploitation are not inherently\nconflicting objectives, while they can mutually enhance each other. Moreover,\nPass@k Training with analytical derivation essentially involves directly\ndesigning the advantage function. Inspired by this, we preliminarily explore\nthe advantage design for RLVR, showing promising results and highlighting a\npotential future direction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning with verifiable rewards (RLVR), which typically adopts\nPass@1 as the reward, has faced the issues in balancing exploration and\nexploitation, causing policies to prefer conservative actions, converging to a\nlocal optimum. Identifying an appropriate reward metric is therefore crucial.\nRegarding the prior work, although Pass@k has been used in evaluation, its\nconnection to LLM exploration ability in RLVR remains largely overlooked. To\ninvestigate this, we first use Pass@k as the reward to train the policy model\n(i.e., $\\textbf{Pass@k Training}$), and observe the improvement on its\nexploration ability. Next, we derive an analytical solution for the advantage\nof Pass@k Training, leading to an efficient and effective process. Building on\nthis, our analysis reveals that exploration and exploitation are not inherently\nconflicting objectives, while they can mutually enhance each other. Moreover,\nPass@k Training with analytical derivation essentially involves directly\ndesigning the advantage function. Inspired by this, we preliminarily explore\nthe advantage design for RLVR, showing promising results and highlighting a\npotential future direction."
                },
                "authors": [
                    {
                        "name": "Zhipeng Chen"
                    },
                    {
                        "name": "Xiaobo Qin"
                    },
                    {
                        "name": "Youbin Wu"
                    },
                    {
                        "name": "Yue Ling"
                    },
                    {
                        "name": "Qinghao Ye"
                    },
                    {
                        "name": "Wayne Xin Zhao"
                    },
                    {
                        "name": "Guang Shi"
                    }
                ],
                "author_detail": {
                    "name": "Guang Shi"
                },
                "author": "Guang Shi",
                "arxiv_comment": "Technical Report about RLVR: 32 pages, 18 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10751v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10751v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10736v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10736v1",
                "updated": "2025-08-14T15:16:25Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    15,
                    16,
                    25,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T15:16:25Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    15,
                    16,
                    25,
                    3,
                    226,
                    0
                ],
                "title": "Thinking Inside the Mask: In-Place Prompting in Diffusion LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Thinking Inside the Mask: In-Place Prompting in Diffusion LLMs"
                },
                "summary": "Despite large language models (LLMs) have achieved remarkable success, their\nprefix-only prompting paradigm and sequential generation process offer limited\nflexibility for bidirectional information. Diffusion large language models\n(dLLMs) present new opportunities through their bidirectional attention\nmechanisms and iterative refinement processes, enabling more flexible in-place\nprompting strategies. We introduce ICE (In-Place Chain-of-Thought Prompting\nwith Early Exit), a novel framework that transforms prefix-only prompting into\nin-place prompting specifically designed for dLLMs. ICE integrates in-place\nprompts directly within masked token positions during iterative refinement and\nemploys a confidence-aware early exit mechanism to significantly reduce\ncomputational overhead. Extensive experiments demonstrate ICE's effectiveness,\nachieving up to 17.29% accuracy improvement with 4.12$\\times$ speedup on GSM8K,\nand up to 276.67$\\times$ acceleration on MMLU while maintaining competitive\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite large language models (LLMs) have achieved remarkable success, their\nprefix-only prompting paradigm and sequential generation process offer limited\nflexibility for bidirectional information. Diffusion large language models\n(dLLMs) present new opportunities through their bidirectional attention\nmechanisms and iterative refinement processes, enabling more flexible in-place\nprompting strategies. We introduce ICE (In-Place Chain-of-Thought Prompting\nwith Early Exit), a novel framework that transforms prefix-only prompting into\nin-place prompting specifically designed for dLLMs. ICE integrates in-place\nprompts directly within masked token positions during iterative refinement and\nemploys a confidence-aware early exit mechanism to significantly reduce\ncomputational overhead. Extensive experiments demonstrate ICE's effectiveness,\nachieving up to 17.29% accuracy improvement with 4.12$\\times$ speedup on GSM8K,\nand up to 276.67$\\times$ acceleration on MMLU while maintaining competitive\nperformance."
                },
                "authors": [
                    {
                        "name": "Xiangqi Jin"
                    },
                    {
                        "name": "Yuxuan Wang"
                    },
                    {
                        "name": "Yifeng Gao"
                    },
                    {
                        "name": "Zichen Wen"
                    },
                    {
                        "name": "Biqing Qi"
                    },
                    {
                        "name": "Dongrui Liu"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10736v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10736v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10729v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10729v1",
                "updated": "2025-08-14T15:11:20Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    15,
                    11,
                    20,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T15:11:20Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    15,
                    11,
                    20,
                    3,
                    226,
                    0
                ],
                "title": "EgoCross: Benchmarking Multimodal Large Language Models for Cross-Domain\n  Egocentric Video Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EgoCross: Benchmarking Multimodal Large Language Models for Cross-Domain\n  Egocentric Video Question Answering"
                },
                "summary": "Recent advances in Multimodal Large Language Models (MLLMs) have\nsignificantly pushed the frontier of egocentric video question answering\n(EgocentricQA). However, existing benchmarks and studies are mainly limited to\ncommon daily activities such as cooking and cleaning. In contrast, real-world\ndeployment inevitably encounters domain shifts, where target domains differ\nsubstantially in both visual style and semantic content. To bridge this gap, we\nintroduce \\textbf{EgoCross}, a comprehensive benchmark designed to evaluate the\ncross-domain generalization of MLLMs in EgocentricQA. EgoCross covers four\ndiverse and challenging domains, including surgery, industry, extreme sports,\nand animal perspective, representing realistic and high-impact application\nscenarios. It comprises approximately 1,000 QA pairs across 798 video clips,\nspanning four key QA tasks: prediction, recognition, localization, and\ncounting. Each QA pair provides both OpenQA and CloseQA formats to support\nfine-grained evaluation. Extensive experiments show that most existing MLLMs,\nwhether general-purpose or egocentric-specialized, struggle to generalize to\ndomains beyond daily life, highlighting the limitations of current models.\nFurthermore, we conduct several pilot studies, \\eg, fine-tuning and\nreinforcement learning, to explore potential improvements. We hope EgoCross and\nour accompanying analysis will serve as a foundation for advancing\ndomain-adaptive, robust egocentric video understanding. Data and codes will be\nreleased at:\n\\href{https://github.com/MyUniverse0726/EgoCross}{https://github.com/MyUniverse0726/EgoCross.}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Multimodal Large Language Models (MLLMs) have\nsignificantly pushed the frontier of egocentric video question answering\n(EgocentricQA). However, existing benchmarks and studies are mainly limited to\ncommon daily activities such as cooking and cleaning. In contrast, real-world\ndeployment inevitably encounters domain shifts, where target domains differ\nsubstantially in both visual style and semantic content. To bridge this gap, we\nintroduce \\textbf{EgoCross}, a comprehensive benchmark designed to evaluate the\ncross-domain generalization of MLLMs in EgocentricQA. EgoCross covers four\ndiverse and challenging domains, including surgery, industry, extreme sports,\nand animal perspective, representing realistic and high-impact application\nscenarios. It comprises approximately 1,000 QA pairs across 798 video clips,\nspanning four key QA tasks: prediction, recognition, localization, and\ncounting. Each QA pair provides both OpenQA and CloseQA formats to support\nfine-grained evaluation. Extensive experiments show that most existing MLLMs,\nwhether general-purpose or egocentric-specialized, struggle to generalize to\ndomains beyond daily life, highlighting the limitations of current models.\nFurthermore, we conduct several pilot studies, \\eg, fine-tuning and\nreinforcement learning, to explore potential improvements. We hope EgoCross and\nour accompanying analysis will serve as a foundation for advancing\ndomain-adaptive, robust egocentric video understanding. Data and codes will be\nreleased at:\n\\href{https://github.com/MyUniverse0726/EgoCross}{https://github.com/MyUniverse0726/EgoCross.}"
                },
                "authors": [
                    {
                        "name": "Yanjun Li"
                    },
                    {
                        "name": "Yuqian Fu"
                    },
                    {
                        "name": "Tianwen Qian"
                    },
                    {
                        "name": "Qi'ao Xu"
                    },
                    {
                        "name": "Silong Dai"
                    },
                    {
                        "name": "Danda Pani Paudel"
                    },
                    {
                        "name": "Luc Van Gool"
                    },
                    {
                        "name": "Xiaoling Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoling Wang"
                },
                "author": "Xiaoling Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10729v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10729v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10720v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10720v1",
                "updated": "2025-08-14T15:00:51Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    15,
                    0,
                    51,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T15:00:51Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    15,
                    0,
                    51,
                    3,
                    226,
                    0
                ],
                "title": "Predictive Position Control for Movable Antenna Arrays in UAV\n  Communications: A Spatio-Temporal Transformer-LSTM Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predictive Position Control for Movable Antenna Arrays in UAV\n  Communications: A Spatio-Temporal Transformer-LSTM Framework"
                },
                "summary": "In complex urban environments, dynamic obstacles and multipath effects lead\nto significant link attenuation and pervasive coverage blind spots.\nConventional approaches based on large-scale fixed antenna arrays and UAV\ntrajectory optimization struggle to balance energy efficiency, real-time\nadaptation, and spatial flexibility. The movable antenna (MA) technology has\nemerged as a promising solution, offering enhanced spatial flexibility and\nreduced energy consumption to overcome the bottlenecks of urban low-altitude\ncommunications. However, MA deployment faces a critical velocity mismatch\nbetween UAV mobility and mechanical repositioning latency, undermining\nreal-time link optimization and security assurance. To overcome this, we\npropose a predictive MA-UAV collaborative control framework. First, optimal\nantenna positions are derived via secrecy rate maximization. Second, a\nTransformer-enhanced long short-term memory (LSTM) network predicts future MA\npositions by capturing spatio-temporal correlations in antenna trajectories.\nExtensive simulations demonstrate superior prediction accuracy (NMSE reduction\nexceeds 49\\%) and communication reliability versus current popular benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In complex urban environments, dynamic obstacles and multipath effects lead\nto significant link attenuation and pervasive coverage blind spots.\nConventional approaches based on large-scale fixed antenna arrays and UAV\ntrajectory optimization struggle to balance energy efficiency, real-time\nadaptation, and spatial flexibility. The movable antenna (MA) technology has\nemerged as a promising solution, offering enhanced spatial flexibility and\nreduced energy consumption to overcome the bottlenecks of urban low-altitude\ncommunications. However, MA deployment faces a critical velocity mismatch\nbetween UAV mobility and mechanical repositioning latency, undermining\nreal-time link optimization and security assurance. To overcome this, we\npropose a predictive MA-UAV collaborative control framework. First, optimal\nantenna positions are derived via secrecy rate maximization. Second, a\nTransformer-enhanced long short-term memory (LSTM) network predicts future MA\npositions by capturing spatio-temporal correlations in antenna trajectories.\nExtensive simulations demonstrate superior prediction accuracy (NMSE reduction\nexceeds 49\\%) and communication reliability versus current popular benchmarks."
                },
                "authors": [
                    {
                        "name": "Kan Yu"
                    },
                    {
                        "name": "Kaixuan Li"
                    },
                    {
                        "name": "Xiaowu Liu"
                    },
                    {
                        "name": "Qixun Zhang"
                    },
                    {
                        "name": "Zhiyong Feng"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyong Feng"
                },
                "author": "Zhiyong Feng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10720v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10720v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06412v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06412v2",
                "updated": "2025-08-14T14:59:51Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    14,
                    59,
                    51,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-08T15:56:49Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    15,
                    56,
                    49,
                    4,
                    220,
                    0
                ],
                "title": "Sample-efficient LLM Optimization with Reset Replay",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sample-efficient LLM Optimization with Reset Replay"
                },
                "summary": "Recent advancements in post-training Large Language Models (LLMs),\nparticularly through Reinforcement Learning (RL) and preference optimization\nmethods, are key drivers for enhancing their reasoning capabilities. However,\nthese methods are often plagued by low sample efficiency and a susceptibility\nto primacy bias, where overfitting to initial experiences degrades policy\nquality and damages the learning process. To address these challenges, we\nintroduce LLM optimization with Reset Replay (LoRR), a general and powerful\nplugin designed to enhance sample efficiency in any preference-based\noptimization framework. LoRR core mechanism enables training at a high replay\nnumber, maximizing the utility of each collected data batch. To counteract the\nrisk of overfitting inherent in high-replay training, LoRR incorporates a\nperiodic reset strategy with reusing initial data, which preserves network\nplasticity. Furthermore, it leverages a hybrid optimization objective,\ncombining supervised fine-tuning (SFT) and preference-based losses to further\nbolster data exploitation. Our extensive experiments demonstrate that LoRR\nsignificantly boosts the performance of various preference optimization methods\non both mathematical and general reasoning benchmarks. Notably, an iterative\nDPO approach augmented with LoRR achieves comparable performance on challenging\nmath tasks, outperforming some complex and computationally intensive RL-based\nalgorithms. These findings highlight that LoRR offers a practical,\nsample-efficient, and highly effective paradigm for LLM finetuning, unlocking\ngreater performance from limited data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in post-training Large Language Models (LLMs),\nparticularly through Reinforcement Learning (RL) and preference optimization\nmethods, are key drivers for enhancing their reasoning capabilities. However,\nthese methods are often plagued by low sample efficiency and a susceptibility\nto primacy bias, where overfitting to initial experiences degrades policy\nquality and damages the learning process. To address these challenges, we\nintroduce LLM optimization with Reset Replay (LoRR), a general and powerful\nplugin designed to enhance sample efficiency in any preference-based\noptimization framework. LoRR core mechanism enables training at a high replay\nnumber, maximizing the utility of each collected data batch. To counteract the\nrisk of overfitting inherent in high-replay training, LoRR incorporates a\nperiodic reset strategy with reusing initial data, which preserves network\nplasticity. Furthermore, it leverages a hybrid optimization objective,\ncombining supervised fine-tuning (SFT) and preference-based losses to further\nbolster data exploitation. Our extensive experiments demonstrate that LoRR\nsignificantly boosts the performance of various preference optimization methods\non both mathematical and general reasoning benchmarks. Notably, an iterative\nDPO approach augmented with LoRR achieves comparable performance on challenging\nmath tasks, outperforming some complex and computationally intensive RL-based\nalgorithms. These findings highlight that LoRR offers a practical,\nsample-efficient, and highly effective paradigm for LLM finetuning, unlocking\ngreater performance from limited data."
                },
                "authors": [
                    {
                        "name": "Zichuan Liu"
                    },
                    {
                        "name": "Jinyu Wang"
                    },
                    {
                        "name": "Lei Song"
                    },
                    {
                        "name": "Jiang Bian"
                    }
                ],
                "author_detail": {
                    "name": "Jiang Bian"
                },
                "author": "Jiang Bian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06412v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06412v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10712v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10712v1",
                "updated": "2025-08-14T14:55:19Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    14,
                    55,
                    19,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T14:55:19Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    14,
                    55,
                    19,
                    3,
                    226,
                    0
                ],
                "title": "Lightweight CNNs for Embedded SAR Ship Target Detection and\n  Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lightweight CNNs for Embedded SAR Ship Target Detection and\n  Classification"
                },
                "summary": "Synthetic Aperture Radar (SAR) data enables large-scale surveillance of\nmaritime vessels. However, near-real-time monitoring is currently constrained\nby the need to downlink all raw data, perform image focusing, and subsequently\nanalyze it on the ground. On-board processing to generate higher-level products\ncould reduce the data volume that needs to be downlinked, alleviating bandwidth\nconstraints and minimizing latency. However, traditional image focusing and\nprocessing algorithms face challenges due to the satellite's limited memory,\nprocessing power, and computational resources. This work proposes and evaluates\nneural networks designed for real-time inference on unfocused SAR data acquired\nin Stripmap and Interferometric Wide (IW) modes captured with Sentinel-1. Our\nresults demonstrate the feasibility of using one of our models for on-board\nprocessing and deployment on an FPGA. Additionally, by investigating a binary\nclassification task between ships and windmills, we demonstrate that target\nclassification is possible.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synthetic Aperture Radar (SAR) data enables large-scale surveillance of\nmaritime vessels. However, near-real-time monitoring is currently constrained\nby the need to downlink all raw data, perform image focusing, and subsequently\nanalyze it on the ground. On-board processing to generate higher-level products\ncould reduce the data volume that needs to be downlinked, alleviating bandwidth\nconstraints and minimizing latency. However, traditional image focusing and\nprocessing algorithms face challenges due to the satellite's limited memory,\nprocessing power, and computational resources. This work proposes and evaluates\nneural networks designed for real-time inference on unfocused SAR data acquired\nin Stripmap and Interferometric Wide (IW) modes captured with Sentinel-1. Our\nresults demonstrate the feasibility of using one of our models for on-board\nprocessing and deployment on an FPGA. Additionally, by investigating a binary\nclassification task between ships and windmills, we demonstrate that target\nclassification is possible."
                },
                "authors": [
                    {
                        "name": "Fabian Kresse"
                    },
                    {
                        "name": "Georgios Pilikos"
                    },
                    {
                        "name": "Mario Azcueta"
                    },
                    {
                        "name": "Nicolas Floury"
                    }
                ],
                "author_detail": {
                    "name": "Nicolas Floury"
                },
                "author": "Nicolas Floury",
                "arxiv_comment": "Accepted at Big Data from Space 2025 (BiDS'25)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10712v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10712v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10703v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10703v1",
                "updated": "2025-08-14T14:48:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    14,
                    48,
                    9,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T14:48:09Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    14,
                    48,
                    9,
                    3,
                    226,
                    0
                ],
                "title": "GenOM: Ontology Matching with Description Generation and Large Language\n  Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GenOM: Ontology Matching with Description Generation and Large Language\n  Model"
                },
                "summary": "Ontology matching (OM) plays an essential role in enabling semantic\ninteroperability and integration across heterogeneous knowledge sources,\nparticularly in the biomedical domain which contains numerous complex concepts\nrelated to diseases and pharmaceuticals. This paper introduces GenOM, a large\nlanguage model (LLM)-based ontology alignment framework, which enriches the\nsemantic representations of ontology concepts via generating textual\ndefinitions, retrieves alignment candidates with an embedding model, and\nincorporates exact matching-based tools to improve precision. Extensive\nexperiments conducted on the OAEI Bio-ML track demonstrate that GenOM can often\nachieve competitive performance, surpassing many baselines including\ntraditional OM systems and recent LLM-based methods. Further ablation studies\nconfirm the effectiveness of semantic enrichment and few-shot prompting,\nhighlighting the framework's robustness and adaptability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ontology matching (OM) plays an essential role in enabling semantic\ninteroperability and integration across heterogeneous knowledge sources,\nparticularly in the biomedical domain which contains numerous complex concepts\nrelated to diseases and pharmaceuticals. This paper introduces GenOM, a large\nlanguage model (LLM)-based ontology alignment framework, which enriches the\nsemantic representations of ontology concepts via generating textual\ndefinitions, retrieves alignment candidates with an embedding model, and\nincorporates exact matching-based tools to improve precision. Extensive\nexperiments conducted on the OAEI Bio-ML track demonstrate that GenOM can often\nachieve competitive performance, surpassing many baselines including\ntraditional OM systems and recent LLM-based methods. Further ablation studies\nconfirm the effectiveness of semantic enrichment and few-shot prompting,\nhighlighting the framework's robustness and adaptability."
                },
                "authors": [
                    {
                        "name": "Yiping Song"
                    },
                    {
                        "name": "Jiaoyan Chen"
                    },
                    {
                        "name": "Renate A. Schmidt"
                    }
                ],
                "author_detail": {
                    "name": "Renate A. Schmidt"
                },
                "author": "Renate A. Schmidt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10703v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10703v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10701v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10701v1",
                "updated": "2025-08-14T14:45:45Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    14,
                    45,
                    45,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T14:45:45Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    14,
                    45,
                    45,
                    3,
                    226,
                    0
                ],
                "title": "REFN: A Reinforcement-Learning-From-Network Framework against\n  1-day/n-day Exploitations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REFN: A Reinforcement-Learning-From-Network Framework against\n  1-day/n-day Exploitations"
                },
                "summary": "The exploitation of 1 day or n day vulnerabilities poses severe threats to\nnetworked devices due to massive deployment scales and delayed patching\n(average Mean Time To Patch exceeds 60 days). Existing defenses, including host\nbased patching and network based filtering, are inadequate due to limited\nscalability across diverse devices, compatibility issues especially with\nembedded or legacy systems, and error prone deployment process (manual patch\nvalidation). To address these issues, we introduce REFN (Reinforcement Learning\nFrom Network), a novel framework that trains Large Language Models (LLMs) to\nautonomously generate network filters to prevent 1 day or n day exploitations.\nREFN ensures scalability by uniquely employs Reinforcement Learning (RL) driven\nby online network rewards instead of traditional Human Feedback (RLHF). REFN\nguarantees compatibility via unified deployment on edge security gateways\n(Amazon Eero). REFN provides robustness via online validation using real\nnetwork traffic. Crucially, REFN addresses three core challenges in training\nLLMs for exploit prevention: 1) expanding current LLMs limited vulnerability\nfixing expertise via Agentic RAG based Knowledge Distillation, 2) bridging\ncurrent LLMs language to network gaps through an RL From VNF Pipeline that\ntranslates language context (vulnerability description) into network\nenforcement, 3) addressing the LLM hallucination and non determinism via the\nOnline Agentic Validation that penalizes erroneous outputs. Evaluated across 22\nfamilies of 1 day or n day exploits, REFN demonstrates effectiveness (21.1\npercent higher accuracy than alternatives), efficiency (Mean Time To Patch of\n3.65 hours) and scalability (easily scale to 10K devices). REFN serves as an\ninitial step toward training LLMs to rapidly prevent massive scale 1 day or n\nday exploitations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The exploitation of 1 day or n day vulnerabilities poses severe threats to\nnetworked devices due to massive deployment scales and delayed patching\n(average Mean Time To Patch exceeds 60 days). Existing defenses, including host\nbased patching and network based filtering, are inadequate due to limited\nscalability across diverse devices, compatibility issues especially with\nembedded or legacy systems, and error prone deployment process (manual patch\nvalidation). To address these issues, we introduce REFN (Reinforcement Learning\nFrom Network), a novel framework that trains Large Language Models (LLMs) to\nautonomously generate network filters to prevent 1 day or n day exploitations.\nREFN ensures scalability by uniquely employs Reinforcement Learning (RL) driven\nby online network rewards instead of traditional Human Feedback (RLHF). REFN\nguarantees compatibility via unified deployment on edge security gateways\n(Amazon Eero). REFN provides robustness via online validation using real\nnetwork traffic. Crucially, REFN addresses three core challenges in training\nLLMs for exploit prevention: 1) expanding current LLMs limited vulnerability\nfixing expertise via Agentic RAG based Knowledge Distillation, 2) bridging\ncurrent LLMs language to network gaps through an RL From VNF Pipeline that\ntranslates language context (vulnerability description) into network\nenforcement, 3) addressing the LLM hallucination and non determinism via the\nOnline Agentic Validation that penalizes erroneous outputs. Evaluated across 22\nfamilies of 1 day or n day exploits, REFN demonstrates effectiveness (21.1\npercent higher accuracy than alternatives), efficiency (Mean Time To Patch of\n3.65 hours) and scalability (easily scale to 10K devices). REFN serves as an\ninitial step toward training LLMs to rapidly prevent massive scale 1 day or n\nday exploitations."
                },
                "authors": [
                    {
                        "name": "Tianlong Yu"
                    },
                    {
                        "name": "Lihong Liu"
                    },
                    {
                        "name": "Ziyi Zhou"
                    },
                    {
                        "name": "Fudu Xing"
                    },
                    {
                        "name": "Kailong Wang"
                    },
                    {
                        "name": "Yang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yang Yang"
                },
                "author": "Yang Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10701v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10701v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10699v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10699v1",
                "updated": "2025-08-14T14:43:17Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    14,
                    43,
                    17,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T14:43:17Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    14,
                    43,
                    17,
                    3,
                    226,
                    0
                ],
                "title": "Towards Hybrid Lunar PNT: Error Models, Lower Bounds and Algorithms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Hybrid Lunar PNT: Error Models, Lower Bounds and Algorithms"
                },
                "summary": "Accurate positioning, navigation and timing (PNT) are crucial for upcoming\nlunar surface missions. Lunar satellite navigation systems are being developed,\nbut lack coverage during early deployment phases. Hybrid lunar PNT combining\ncooperative navigation, satellite systems, and an optional reference station\noffers improved accuracy and availability. This study develops realistic error\nmodels that incorporate temporal correlations often ignored in existing works.\nWe derive a cooperative navigation error model considering fading and\npseudorange bias from multipath propagation, and compare three error models for\nlunar satellite pseudorange and pseudorange rate signal-in-space error. These\ntemporal error correlation models integrate easily into Kalman filters and\nprovide realistic performance predictions essential for robust navigation\nengines. We perform case studies to demonstrate that hybrid navigation\nsignificantly improves accuracy, particularly with static users present. Most\nnotably, hybrid navigation enables optimal performance when using a lunar\nreference station, achieving sub-meter accuracy with only two visible\nsatellites.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate positioning, navigation and timing (PNT) are crucial for upcoming\nlunar surface missions. Lunar satellite navigation systems are being developed,\nbut lack coverage during early deployment phases. Hybrid lunar PNT combining\ncooperative navigation, satellite systems, and an optional reference station\noffers improved accuracy and availability. This study develops realistic error\nmodels that incorporate temporal correlations often ignored in existing works.\nWe derive a cooperative navigation error model considering fading and\npseudorange bias from multipath propagation, and compare three error models for\nlunar satellite pseudorange and pseudorange rate signal-in-space error. These\ntemporal error correlation models integrate easily into Kalman filters and\nprovide realistic performance predictions essential for robust navigation\nengines. We perform case studies to demonstrate that hybrid navigation\nsignificantly improves accuracy, particularly with static users present. Most\nnotably, hybrid navigation enables optimal performance when using a lunar\nreference station, achieving sub-meter accuracy with only two visible\nsatellites."
                },
                "authors": [
                    {
                        "name": "Robert Phlmann"
                    },
                    {
                        "name": "Emanuel Staudinger"
                    },
                    {
                        "name": "Gonzalo Seco-Granados"
                    }
                ],
                "author_detail": {
                    "name": "Gonzalo Seco-Granados"
                },
                "author": "Gonzalo Seco-Granados",
                "arxiv_comment": "Submitted to NAVIGATION, Journal of the Institute of Navigation. See\n  navi.ion.org",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10699v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10699v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10696v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10696v1",
                "updated": "2025-08-14T14:37:54Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    14,
                    37,
                    54,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T14:37:54Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    14,
                    37,
                    54,
                    3,
                    226,
                    0
                ],
                "title": "Chem3DLLM: 3D Multimodal Large Language Models for Chemistry",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chem3DLLM: 3D Multimodal Large Language Models for Chemistry"
                },
                "summary": "In the real world, a molecule is a 3D geometric structure. Compared to 1D\nSMILES sequences and 2D molecular graphs, 3D molecules represent the most\ninformative molecular modality. Despite the rapid progress of\nautoregressive-based language models, they cannot handle the generation of 3D\nmolecular conformation due to several challenges: 1) 3D molecular structures\nare incompatible with LLMs' discrete token space, 2) integrating heterogeneous\ninputs like proteins, ligands, and text remains difficult within a unified\nmodel, and 3) LLMs lack essential scientific priors, hindering the enforcement\nof physical and chemical constraints during generation. To tackle these issues,\nwe present Chem3DLLM, a unified protein-conditioned multimodal large language\nmodel. Our approach designs a novel reversible text encoding for 3D molecular\nstructures using run-length compression, achieving 3x size reduction while\npreserving complete structural information. This enables seamless integration\nof molecular geometry with protein pocket features in a single LLM\narchitecture. We employ reinforcement learning with stability-based rewards to\noptimize chemical validity and incorporate a lightweight protein embedding\nprojector for end-to-end training. Experimental results on structure-based drug\ndesign demonstrate state-of-the-art performance with a Vina score of -7.21,\nvalidating our unified multimodal approach for practical drug discovery\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the real world, a molecule is a 3D geometric structure. Compared to 1D\nSMILES sequences and 2D molecular graphs, 3D molecules represent the most\ninformative molecular modality. Despite the rapid progress of\nautoregressive-based language models, they cannot handle the generation of 3D\nmolecular conformation due to several challenges: 1) 3D molecular structures\nare incompatible with LLMs' discrete token space, 2) integrating heterogeneous\ninputs like proteins, ligands, and text remains difficult within a unified\nmodel, and 3) LLMs lack essential scientific priors, hindering the enforcement\nof physical and chemical constraints during generation. To tackle these issues,\nwe present Chem3DLLM, a unified protein-conditioned multimodal large language\nmodel. Our approach designs a novel reversible text encoding for 3D molecular\nstructures using run-length compression, achieving 3x size reduction while\npreserving complete structural information. This enables seamless integration\nof molecular geometry with protein pocket features in a single LLM\narchitecture. We employ reinforcement learning with stability-based rewards to\noptimize chemical validity and incorporate a lightweight protein embedding\nprojector for end-to-end training. Experimental results on structure-based drug\ndesign demonstrate state-of-the-art performance with a Vina score of -7.21,\nvalidating our unified multimodal approach for practical drug discovery\napplications."
                },
                "authors": [
                    {
                        "name": "Lei Jiang"
                    },
                    {
                        "name": "Shuzhou Sun"
                    },
                    {
                        "name": "Biqing Qi"
                    },
                    {
                        "name": "Yuchen Fu"
                    },
                    {
                        "name": "Xiaohua Xu"
                    },
                    {
                        "name": "Yuqiang Li"
                    },
                    {
                        "name": "Dongzhan Zhou"
                    },
                    {
                        "name": "Tianfan Fu"
                    }
                ],
                "author_detail": {
                    "name": "Tianfan Fu"
                },
                "author": "Tianfan Fu",
                "arxiv_comment": "15 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10696v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10696v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10695v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10695v1",
                "updated": "2025-08-14T14:36:53Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    14,
                    36,
                    53,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T14:36:53Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    14,
                    36,
                    53,
                    3,
                    226,
                    0
                ],
                "title": "Learning from Natural Language Feedback for Personalized Question\n  Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning from Natural Language Feedback for Personalized Question\n  Answering"
                },
                "summary": "Personalization is crucial for enhancing both the effectiveness and user\nsatisfaction of language technologies, particularly in information-seeking\ntasks like question answering. Current approaches for personalizing large\nlanguage models (LLMs) often rely on retrieval-augmented generation (RAG),\nfollowed by reinforcement learning with scalar reward signals to teach models\nhow to use retrieved personal context. We believe that these scalar rewards\nsometimes provide weak, non-instructive feedback, limiting learning efficiency\nand personalization quality. We introduce VAC, a novel framework for\npersonalized response generation that replaces scalar rewards with natural\nlanguage feedback (NLF) that are generated conditioned on the user profiles and\nthe question narratives. NLF serves as a rich and actionable supervision\nsignal, allowing the policy model to iteratively refine its outputs and\ninternalize effective personalization strategies. Training alternates between\noptimizing the feedback model and fine-tuning the policy model on the improved\nresponses, resulting in a policy model that no longer requires feedback at\ninference. Evaluation on the LaMP-QA benchmark that consists of three diverse\ndomains demonstrates consistent and significant improvements over the\nstate-of-the-art results. Human evaluations further confirm the superior\nquality of the generated responses. These results demonstrate that NLF provides\nmore effective signals for optimizing personalized question answering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalization is crucial for enhancing both the effectiveness and user\nsatisfaction of language technologies, particularly in information-seeking\ntasks like question answering. Current approaches for personalizing large\nlanguage models (LLMs) often rely on retrieval-augmented generation (RAG),\nfollowed by reinforcement learning with scalar reward signals to teach models\nhow to use retrieved personal context. We believe that these scalar rewards\nsometimes provide weak, non-instructive feedback, limiting learning efficiency\nand personalization quality. We introduce VAC, a novel framework for\npersonalized response generation that replaces scalar rewards with natural\nlanguage feedback (NLF) that are generated conditioned on the user profiles and\nthe question narratives. NLF serves as a rich and actionable supervision\nsignal, allowing the policy model to iteratively refine its outputs and\ninternalize effective personalization strategies. Training alternates between\noptimizing the feedback model and fine-tuning the policy model on the improved\nresponses, resulting in a policy model that no longer requires feedback at\ninference. Evaluation on the LaMP-QA benchmark that consists of three diverse\ndomains demonstrates consistent and significant improvements over the\nstate-of-the-art results. Human evaluations further confirm the superior\nquality of the generated responses. These results demonstrate that NLF provides\nmore effective signals for optimizing personalized question answering."
                },
                "authors": [
                    {
                        "name": "Alireza Salemi"
                    },
                    {
                        "name": "Hamed Zamani"
                    }
                ],
                "author_detail": {
                    "name": "Hamed Zamani"
                },
                "author": "Hamed Zamani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10695v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10695v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.07014v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.07014v4",
                "updated": "2025-08-14T14:34:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    14,
                    34,
                    52,
                    3,
                    226,
                    0
                ],
                "published": "2024-07-09T16:33:43Z",
                "published_parsed": [
                    2024,
                    7,
                    9,
                    16,
                    33,
                    43,
                    1,
                    191,
                    0
                ],
                "title": "An Attempt to Devise a Pairwise Ising-Type Maximum Entropy Model\n  Integrated Cost Function for Optimizing SNN Deployment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Attempt to Devise a Pairwise Ising-Type Maximum Entropy Model\n  Integrated Cost Function for Optimizing SNN Deployment"
                },
                "summary": "Spiking Neural Networks (SNNs) emulate the spiking behavior of biological\nneurons and are typically deployed on distributed-memory neuromorphic hardware.\nThe deployment of a SNN usually requires partitioning the network and mapping\nthese partitions onto the hardware's processing units.\n  However, finding optimal deployment configurations is an NP-hard problem,\noften addressed through optimization algorithms. While some objectives (e.g.,\nmemory utilization and chip count) are static, others (e.g., communication\nlatency and energy efficiency) depend on the network's dynamic behavior,\nnecessitating dynamic-aware optimization.\n  To address this, we model SNN dynamics using an Ising-type pairwise\ninteraction framework, bridging microscopic neuron interactions with\nmacroscopic network behavior. We optimize deployment by exploring the parameter\nand configuration spaces of the Ising model.\n  We evaluate our approach on two SNNs deployed on the sPyNNaker neuromorphic\nplatform. Initial results suggest that the method underperforms, potentially\ndue to the Ising model's equilibrium assumptions and the architectural\ncomplexity of real-world neuromorphic hardware, highlighting limitations in its\ncurrent formulation.\n  Update: The method proposed is with a equilibrium-dynamics SNN assumption,\nand the original paper does not mention this. The paper needs to be revisited\nand reuploaded after further experiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spiking Neural Networks (SNNs) emulate the spiking behavior of biological\nneurons and are typically deployed on distributed-memory neuromorphic hardware.\nThe deployment of a SNN usually requires partitioning the network and mapping\nthese partitions onto the hardware's processing units.\n  However, finding optimal deployment configurations is an NP-hard problem,\noften addressed through optimization algorithms. While some objectives (e.g.,\nmemory utilization and chip count) are static, others (e.g., communication\nlatency and energy efficiency) depend on the network's dynamic behavior,\nnecessitating dynamic-aware optimization.\n  To address this, we model SNN dynamics using an Ising-type pairwise\ninteraction framework, bridging microscopic neuron interactions with\nmacroscopic network behavior. We optimize deployment by exploring the parameter\nand configuration spaces of the Ising model.\n  We evaluate our approach on two SNNs deployed on the sPyNNaker neuromorphic\nplatform. Initial results suggest that the method underperforms, potentially\ndue to the Ising model's equilibrium assumptions and the architectural\ncomplexity of real-world neuromorphic hardware, highlighting limitations in its\ncurrent formulation.\n  Update: The method proposed is with a equilibrium-dynamics SNN assumption,\nand the original paper does not mention this. The paper needs to be revisited\nand reuploaded after further experiments."
                },
                "authors": [
                    {
                        "name": "Wanhong Huang"
                    }
                ],
                "author_detail": {
                    "name": "Wanhong Huang"
                },
                "author": "Wanhong Huang",
                "arxiv_comment": "Need further experiments for conclusive results",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.07014v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.07014v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08189v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08189v2",
                "updated": "2025-08-14T14:32:17Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    14,
                    32,
                    17,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-11T17:08:55Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    17,
                    8,
                    55,
                    0,
                    223,
                    0
                ],
                "title": "Reinforcement Learning in Vision: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning in Vision: A Survey"
                },
                "summary": "Recent advances at the intersection of reinforcement learning (RL) and visual\nintelligence have enabled agents that not only perceive complex visual scenes\nbut also reason, generate, and act within them. This survey offers a critical\nand up-to-date synthesis of the field. We first formalize visual RL problems\nand trace the evolution of policy-optimization strategies from RLHF to\nverifiable reward paradigms, and from Proximal Policy Optimization to Group\nRelative Policy Optimization. We then organize more than 200 representative\nworks into four thematic pillars: multi-modal large language models, visual\ngeneration, unified model frameworks, and vision-language-action models. For\neach pillar we examine algorithmic design, reward engineering, benchmark\nprogress, and we distill trends such as curriculum-driven training,\npreference-aligned diffusion, and unified reward modeling. Finally, we review\nevaluation protocols spanning set-level fidelity, sample-level preference, and\nstate-level stability, and we identify open challenges that include sample\nefficiency, generalization, and safe deployment. Our goal is to provide\nresearchers and practitioners with a coherent map of the rapidly expanding\nlandscape of visual RL and to highlight promising directions for future\ninquiry. Resources are available at:\nhttps://github.com/weijiawu/Awesome-Visual-Reinforcement-Learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances at the intersection of reinforcement learning (RL) and visual\nintelligence have enabled agents that not only perceive complex visual scenes\nbut also reason, generate, and act within them. This survey offers a critical\nand up-to-date synthesis of the field. We first formalize visual RL problems\nand trace the evolution of policy-optimization strategies from RLHF to\nverifiable reward paradigms, and from Proximal Policy Optimization to Group\nRelative Policy Optimization. We then organize more than 200 representative\nworks into four thematic pillars: multi-modal large language models, visual\ngeneration, unified model frameworks, and vision-language-action models. For\neach pillar we examine algorithmic design, reward engineering, benchmark\nprogress, and we distill trends such as curriculum-driven training,\npreference-aligned diffusion, and unified reward modeling. Finally, we review\nevaluation protocols spanning set-level fidelity, sample-level preference, and\nstate-level stability, and we identify open challenges that include sample\nefficiency, generalization, and safe deployment. Our goal is to provide\nresearchers and practitioners with a coherent map of the rapidly expanding\nlandscape of visual RL and to highlight promising directions for future\ninquiry. Resources are available at:\nhttps://github.com/weijiawu/Awesome-Visual-Reinforcement-Learning."
                },
                "authors": [
                    {
                        "name": "Weijia Wu"
                    },
                    {
                        "name": "Chen Gao"
                    },
                    {
                        "name": "Joya Chen"
                    },
                    {
                        "name": "Kevin Qinghong Lin"
                    },
                    {
                        "name": "Qingwei Meng"
                    },
                    {
                        "name": "Yiming Zhang"
                    },
                    {
                        "name": "Yuke Qiu"
                    },
                    {
                        "name": "Hong Zhou"
                    },
                    {
                        "name": "Mike Zheng Shou"
                    }
                ],
                "author_detail": {
                    "name": "Mike Zheng Shou"
                },
                "author": "Mike Zheng Shou",
                "arxiv_comment": "22 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08189v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08189v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10509v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10509v2",
                "updated": "2025-08-14T14:31:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    14,
                    31,
                    22,
                    3,
                    226,
                    0
                ],
                "published": "2025-03-13T16:10:14Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    16,
                    10,
                    14,
                    3,
                    72,
                    0
                ],
                "title": "From Actions to Words: Towards Abstractive-Textual Policy Summarization\n  in RL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Actions to Words: Towards Abstractive-Textual Policy Summarization\n  in RL"
                },
                "summary": "Policies generated by Reinforcement Learning (RL) algorithms are difficult to\nexplain to users, as they emerge from the interaction of complex reward\nstructures and neural network representations. Consequently, analyzing and\npredicting agent behavior can be challenging, undermining user trust in\nreal-world applications. To facilitate user understanding, current methods for\nglobal policy summarization typically rely on videos that demonstrate agent\nbehavior in a subset of world states. However, users can only watch a limited\nnumber of demonstrations, constraining their understanding. Moreover, these\nmethods place the burden of interpretation on users by presenting raw behaviors\nrather than synthesizing them into coherent patterns. To resolve these issues,\nwe introduce SySLLM (Synthesized Summary using Large Language Models),\nadvocating for a new paradigm of abstractive-textual policy explanations. By\nleveraging Large Language Models (LLMs)-which possess extensive world knowledge\nand pattern synthesis capabilities-SySLLM generates textual summaries that\nprovide structured and comprehensible explanations of agent policies. SySLLM\ndemonstrates that LLMs can interpret spatio-temporally structured descriptions\nof state-action trajectories from an RL agent and generate valuable policy\ninsights in a zero-shot setting, without any prior knowledge or fine-tuning.\nOur evaluation shows that SySLLM captures key insights, such as goal\npreferences and exploration strategies, that were also identified by human\nexperts. Furthermore, in a large-scale user study (with 200 participants),\nSySLLM summaries were preferred over demonstration-based summaries (HIGHLIGHTS)\nby a clear majority (75.5%) of participants.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Policies generated by Reinforcement Learning (RL) algorithms are difficult to\nexplain to users, as they emerge from the interaction of complex reward\nstructures and neural network representations. Consequently, analyzing and\npredicting agent behavior can be challenging, undermining user trust in\nreal-world applications. To facilitate user understanding, current methods for\nglobal policy summarization typically rely on videos that demonstrate agent\nbehavior in a subset of world states. However, users can only watch a limited\nnumber of demonstrations, constraining their understanding. Moreover, these\nmethods place the burden of interpretation on users by presenting raw behaviors\nrather than synthesizing them into coherent patterns. To resolve these issues,\nwe introduce SySLLM (Synthesized Summary using Large Language Models),\nadvocating for a new paradigm of abstractive-textual policy explanations. By\nleveraging Large Language Models (LLMs)-which possess extensive world knowledge\nand pattern synthesis capabilities-SySLLM generates textual summaries that\nprovide structured and comprehensible explanations of agent policies. SySLLM\ndemonstrates that LLMs can interpret spatio-temporally structured descriptions\nof state-action trajectories from an RL agent and generate valuable policy\ninsights in a zero-shot setting, without any prior knowledge or fine-tuning.\nOur evaluation shows that SySLLM captures key insights, such as goal\npreferences and exploration strategies, that were also identified by human\nexperts. Furthermore, in a large-scale user study (with 200 participants),\nSySLLM summaries were preferred over demonstration-based summaries (HIGHLIGHTS)\nby a clear majority (75.5%) of participants."
                },
                "authors": [
                    {
                        "name": "Sahar Admoni"
                    },
                    {
                        "name": "Assaf Hallak"
                    },
                    {
                        "name": "Yftah Ziser"
                    },
                    {
                        "name": "Omer Ben-Porat"
                    },
                    {
                        "name": "Ofra Amir"
                    }
                ],
                "author_detail": {
                    "name": "Ofra Amir"
                },
                "author": "Ofra Amir",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10509v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10509v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10677v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10677v1",
                "updated": "2025-08-14T14:20:34Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    14,
                    20,
                    34,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T14:20:34Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    14,
                    20,
                    34,
                    3,
                    226,
                    0
                ],
                "title": "Advancing Autonomous Incident Response: Leveraging LLMs and Cyber Threat\n  Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Autonomous Incident Response: Leveraging LLMs and Cyber Threat\n  Intelligence"
                },
                "summary": "Effective incident response (IR) is critical for mitigating cyber threats,\nyet security teams are overwhelmed by alert fatigue, high false-positive rates,\nand the vast volume of unstructured Cyber Threat Intelligence (CTI) documents.\nWhile CTI holds immense potential for enriching security operations, its\nextensive and fragmented nature makes manual analysis time-consuming and\nresource-intensive. To bridge this gap, we introduce a novel\nRetrieval-Augmented Generation (RAG)-based framework that leverages Large\nLanguage Models (LLMs) to automate and enhance IR by integrating dynamically\nretrieved CTI. Our approach introduces a hybrid retrieval mechanism that\ncombines NLP-based similarity searches within a CTI vector database with\nstandardized queries to external CTI platforms, facilitating context-aware\nenrichment of security alerts. The augmented intelligence is then leveraged by\nan LLM-powered response generation module, which formulates precise,\nactionable, and contextually relevant incident mitigation strategies. We\npropose a dual evaluation paradigm, wherein automated assessment using an\nauxiliary LLM is systematically cross-validated by cybersecurity experts.\nEmpirical validation on real-world and simulated alerts demonstrates that our\napproach enhances the accuracy, contextualization, and efficiency of IR,\nalleviating analyst workload and reducing response latency. This work\nunderscores the potential of LLM-driven CTI fusion in advancing autonomous\nsecurity operations and establishing a foundation for intelligent, adaptive\ncybersecurity frameworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective incident response (IR) is critical for mitigating cyber threats,\nyet security teams are overwhelmed by alert fatigue, high false-positive rates,\nand the vast volume of unstructured Cyber Threat Intelligence (CTI) documents.\nWhile CTI holds immense potential for enriching security operations, its\nextensive and fragmented nature makes manual analysis time-consuming and\nresource-intensive. To bridge this gap, we introduce a novel\nRetrieval-Augmented Generation (RAG)-based framework that leverages Large\nLanguage Models (LLMs) to automate and enhance IR by integrating dynamically\nretrieved CTI. Our approach introduces a hybrid retrieval mechanism that\ncombines NLP-based similarity searches within a CTI vector database with\nstandardized queries to external CTI platforms, facilitating context-aware\nenrichment of security alerts. The augmented intelligence is then leveraged by\nan LLM-powered response generation module, which formulates precise,\nactionable, and contextually relevant incident mitigation strategies. We\npropose a dual evaluation paradigm, wherein automated assessment using an\nauxiliary LLM is systematically cross-validated by cybersecurity experts.\nEmpirical validation on real-world and simulated alerts demonstrates that our\napproach enhances the accuracy, contextualization, and efficiency of IR,\nalleviating analyst workload and reducing response latency. This work\nunderscores the potential of LLM-driven CTI fusion in advancing autonomous\nsecurity operations and establishing a foundation for intelligent, adaptive\ncybersecurity frameworks."
                },
                "authors": [
                    {
                        "name": "Amine Tellache"
                    },
                    {
                        "name": "Abdelaziz Amara Korba"
                    },
                    {
                        "name": "Amdjed Mokhtari"
                    },
                    {
                        "name": "Horea Moldovan"
                    },
                    {
                        "name": "Yacine Ghamri-Doudane"
                    }
                ],
                "author_detail": {
                    "name": "Yacine Ghamri-Doudane"
                },
                "author": "Yacine Ghamri-Doudane",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10677v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10677v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.12830v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.12830v3",
                "updated": "2025-08-14T14:12:12Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    14,
                    12,
                    12,
                    3,
                    226,
                    0
                ],
                "published": "2024-07-03T11:16:54Z",
                "published_parsed": [
                    2024,
                    7,
                    3,
                    11,
                    16,
                    54,
                    2,
                    185,
                    0
                ],
                "title": "Knowledge-based Consistency Testing of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge-based Consistency Testing of Large Language Models"
                },
                "summary": "In this work, we systematically expose and measure the inconsistency and\nknowledge gaps of Large Language Models (LLMs). Specifically, we propose an\nautomated testing framework (called KonTest) which leverages a knowledge graph\nto construct test cases. KonTest probes and measures the inconsistencies in the\nLLM's knowledge of the world via a combination of semantically-equivalent\nqueries and test oracles (metamorphic or ontological oracle). KonTest further\nmitigates knowledge gaps via a weighted LLM model ensemble. Using four\nstate-of-the-art LLMs (Falcon, Gemini, GPT3.5, and Llama2), we show that\nKonTest generates 19.2% error inducing inputs (1917 errors from 9979 test\ninputs). It also reveals a 16.5% knowledge gap across all tested LLMs. A\nmitigation method informed by KonTest's test suite reduces LLM knowledge gap by\n32.48%. Our ablation study further shows that GPT3.5 is not suitable for\nknowledge-based consistency testing because it is only 60%-68% effective in\nknowledge construction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we systematically expose and measure the inconsistency and\nknowledge gaps of Large Language Models (LLMs). Specifically, we propose an\nautomated testing framework (called KonTest) which leverages a knowledge graph\nto construct test cases. KonTest probes and measures the inconsistencies in the\nLLM's knowledge of the world via a combination of semantically-equivalent\nqueries and test oracles (metamorphic or ontological oracle). KonTest further\nmitigates knowledge gaps via a weighted LLM model ensemble. Using four\nstate-of-the-art LLMs (Falcon, Gemini, GPT3.5, and Llama2), we show that\nKonTest generates 19.2% error inducing inputs (1917 errors from 9979 test\ninputs). It also reveals a 16.5% knowledge gap across all tested LLMs. A\nmitigation method informed by KonTest's test suite reduces LLM knowledge gap by\n32.48%. Our ablation study further shows that GPT3.5 is not suitable for\nknowledge-based consistency testing because it is only 60%-68% effective in\nknowledge construction."
                },
                "authors": [
                    {
                        "name": "Sai Sathiesh Rajan"
                    },
                    {
                        "name": "Ezekiel Soremekun"
                    },
                    {
                        "name": "Sudipta Chattopadhyay"
                    }
                ],
                "author_detail": {
                    "name": "Sudipta Chattopadhyay"
                },
                "author": "Sudipta Chattopadhyay",
                "arxiv_comment": "12 pages, 4 figures, 8 tables, Accepted at EMNLP 2024 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.12830v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.12830v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00579v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00579v2",
                "updated": "2025-08-14T14:11:07Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    14,
                    11,
                    7,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-01T12:22:53Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    12,
                    22,
                    53,
                    4,
                    213,
                    0
                ],
                "title": "MMRAG-DocQA: A Multi-Modal Retrieval-Augmented Generation Method for\n  Document Question-Answering with Hierarchical Index and Multi-Granularity\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MMRAG-DocQA: A Multi-Modal Retrieval-Augmented Generation Method for\n  Document Question-Answering with Hierarchical Index and Multi-Granularity\n  Retrieval"
                },
                "summary": "The multi-modal long-context document question-answering task aims to locate\nand integrate multi-modal evidences (such as texts, tables, charts, images, and\nlayouts) distributed across multiple pages, for question understanding and\nanswer generation. The existing methods can be categorized into Large\nVision-Language Model (LVLM)-based and Retrieval-Augmented Generation\n(RAG)-based methods. However, the former were susceptible to hallucinations,\nwhile the latter struggled for inter-modal disconnection and cross-page\nfragmentation. To address these challenges, a novel multi-modal RAG model,\nnamed MMRAG-DocQA, was proposed, leveraging both textual and visual information\nacross long-range pages to facilitate accurate question answering. A\nhierarchical indexing method with the integration of flattened in-page chunks\nand topological cross-page chunks was designed to jointly establish in-page\nmulti-modal associations and long-distance cross-page dependencies. By means of\njoint similarity evaluation and large language model (LLM)-based re-ranking, a\nmulti-granularity semantic retrieval method, including the page-level parent\npage retrieval and document-level summary retrieval, was proposed to foster\nmulti-modal evidence connection and long-distance evidence integration and\nreasoning. Experimental results performed on public datasets, MMLongBench-Doc\nand LongDocURL, demonstrated the superiority of our MMRAG-DocQA method in\nunderstanding and answering modality-rich and multi-page documents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The multi-modal long-context document question-answering task aims to locate\nand integrate multi-modal evidences (such as texts, tables, charts, images, and\nlayouts) distributed across multiple pages, for question understanding and\nanswer generation. The existing methods can be categorized into Large\nVision-Language Model (LVLM)-based and Retrieval-Augmented Generation\n(RAG)-based methods. However, the former were susceptible to hallucinations,\nwhile the latter struggled for inter-modal disconnection and cross-page\nfragmentation. To address these challenges, a novel multi-modal RAG model,\nnamed MMRAG-DocQA, was proposed, leveraging both textual and visual information\nacross long-range pages to facilitate accurate question answering. A\nhierarchical indexing method with the integration of flattened in-page chunks\nand topological cross-page chunks was designed to jointly establish in-page\nmulti-modal associations and long-distance cross-page dependencies. By means of\njoint similarity evaluation and large language model (LLM)-based re-ranking, a\nmulti-granularity semantic retrieval method, including the page-level parent\npage retrieval and document-level summary retrieval, was proposed to foster\nmulti-modal evidence connection and long-distance evidence integration and\nreasoning. Experimental results performed on public datasets, MMLongBench-Doc\nand LongDocURL, demonstrated the superiority of our MMRAG-DocQA method in\nunderstanding and answering modality-rich and multi-page documents."
                },
                "authors": [
                    {
                        "name": "Ziyu Gong"
                    },
                    {
                        "name": "Yihua Huang"
                    },
                    {
                        "name": "Chengcheng Mai"
                    }
                ],
                "author_detail": {
                    "name": "Chengcheng Mai"
                },
                "author": "Chengcheng Mai",
                "arxiv_comment": "Comments: Removed the footnote in page 1",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00579v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00579v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10645v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10645v1",
                "updated": "2025-08-14T13:41:59Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    13,
                    41,
                    59,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T13:41:59Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    13,
                    41,
                    59,
                    3,
                    226,
                    0
                ],
                "title": "SemPT: Semantic Prompt Tuning for Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SemPT: Semantic Prompt Tuning for Vision-Language Models"
                },
                "summary": "Visual transfer learning for unseen categories presents an active research\ntopic yet a challenging task, due to the inherent conflict between preserving\ncategory-specific representations and acquiring transferable knowledge.\nVision-Language Models (VLMs) pre-trained on large amounts of image-text pairs\noffer a promising solution. However, existing prompt tuning methods rely on\nsparse category labels or disparate LLM-generated descriptions, which fragment\nknowledge representation and hinder transferability. To address this\nlimitation, we introduce Semantic Prompt Tuning (SemPT), a novel framework that\ntackles the generalization challenge by leveraging shared attribute-level\nknowledge across categories. Specifically, SemPT adopts a two-step prompting\nstrategy to guide LLM in extracting shared visual attributes and generating\nattribute-level descriptions, capturing transferable semantic cues beyond\nlabels while ensuring coherent structure. Then, visually guided weighting is\napplied to the embeddings of attribute-level descriptions to reduce noise from\nirrelevant attributes and enhance the text embeddings. Additionally, image\nembeddings are jointly aligned with both label and attribute-enhanced text\nembeddings, balancing discrimination for seen categories and transferability to\nunseen ones. Considering the availability of category exposure, our inference\ndynamically selects between standard label embeddings for seen categories and\nattribute-enhanced embeddings for unseen ones to ensure effective adaptation.\nExtensive experiments on 15 benchmark datasets demonstrate that SemPT achieves\nstate-of-the-art performance across various settings, including base-to-novel\ngeneralization, cross-dataset transfer, cross-domain transfer, and few-shot\nlearning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual transfer learning for unseen categories presents an active research\ntopic yet a challenging task, due to the inherent conflict between preserving\ncategory-specific representations and acquiring transferable knowledge.\nVision-Language Models (VLMs) pre-trained on large amounts of image-text pairs\noffer a promising solution. However, existing prompt tuning methods rely on\nsparse category labels or disparate LLM-generated descriptions, which fragment\nknowledge representation and hinder transferability. To address this\nlimitation, we introduce Semantic Prompt Tuning (SemPT), a novel framework that\ntackles the generalization challenge by leveraging shared attribute-level\nknowledge across categories. Specifically, SemPT adopts a two-step prompting\nstrategy to guide LLM in extracting shared visual attributes and generating\nattribute-level descriptions, capturing transferable semantic cues beyond\nlabels while ensuring coherent structure. Then, visually guided weighting is\napplied to the embeddings of attribute-level descriptions to reduce noise from\nirrelevant attributes and enhance the text embeddings. Additionally, image\nembeddings are jointly aligned with both label and attribute-enhanced text\nembeddings, balancing discrimination for seen categories and transferability to\nunseen ones. Considering the availability of category exposure, our inference\ndynamically selects between standard label embeddings for seen categories and\nattribute-enhanced embeddings for unseen ones to ensure effective adaptation.\nExtensive experiments on 15 benchmark datasets demonstrate that SemPT achieves\nstate-of-the-art performance across various settings, including base-to-novel\ngeneralization, cross-dataset transfer, cross-domain transfer, and few-shot\nlearning."
                },
                "authors": [
                    {
                        "name": "Xiao Shi"
                    },
                    {
                        "name": "Yangjun Ou"
                    },
                    {
                        "name": "Zhenzhong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhenzhong Chen"
                },
                "author": "Zhenzhong Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10645v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10645v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10634v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10634v1",
                "updated": "2025-08-14T13:33:13Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    13,
                    33,
                    13,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T13:33:13Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    13,
                    33,
                    13,
                    3,
                    226,
                    0
                ],
                "title": "Synthesis of Deep Neural Networks with Safe Robust Adaptive Control for\n  Reliable Operation of Wheeled Mobile Robots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synthesis of Deep Neural Networks with Safe Robust Adaptive Control for\n  Reliable Operation of Wheeled Mobile Robots"
                },
                "summary": "Deep neural networks (DNNs) can enable precise control while maintaining low\ncomputational costs by circumventing the need for dynamic modeling. However,\nthe deployment of such black-box approaches remains challenging for heavy-duty\nwheeled mobile robots (WMRs), which are subject to strict international\nstandards and prone to faults and disturbances. We designed a hierarchical\ncontrol policy for heavy-duty WMRs, monitored by two safety layers with\ndiffering levels of authority. To this end, a DNN policy was trained and\ndeployed as the primary control strategy, providing high-precision performance\nunder nominal operating conditions. When external disturbances arise and reach\na level of intensity such that the system performance falls below a predefined\nthreshold, a low-level safety layer intervenes by deactivating the primary\ncontrol policy and activating a model-free robust adaptive control (RAC)\npolicy. This transition enables the system to continue operating while ensuring\nstability by effectively managing the inherent trade-off between system\nrobustness and responsiveness. Regardless of the control policy in use, a\nhigh-level safety layer continuously monitors system performance during\noperation. It initiates a shutdown only when disturbances become sufficiently\nsevere such that compensation is no longer viable and continued operation would\njeopardize the system or its environment. The proposed synthesis of DNN and RAC\npolicy guarantees uniform exponential stability of the entire WMR system while\nadhering to safety standards to some extent. The effectiveness of the proposed\napproach was further validated through real-time experiments using a 6,000 kg\nWMR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep neural networks (DNNs) can enable precise control while maintaining low\ncomputational costs by circumventing the need for dynamic modeling. However,\nthe deployment of such black-box approaches remains challenging for heavy-duty\nwheeled mobile robots (WMRs), which are subject to strict international\nstandards and prone to faults and disturbances. We designed a hierarchical\ncontrol policy for heavy-duty WMRs, monitored by two safety layers with\ndiffering levels of authority. To this end, a DNN policy was trained and\ndeployed as the primary control strategy, providing high-precision performance\nunder nominal operating conditions. When external disturbances arise and reach\na level of intensity such that the system performance falls below a predefined\nthreshold, a low-level safety layer intervenes by deactivating the primary\ncontrol policy and activating a model-free robust adaptive control (RAC)\npolicy. This transition enables the system to continue operating while ensuring\nstability by effectively managing the inherent trade-off between system\nrobustness and responsiveness. Regardless of the control policy in use, a\nhigh-level safety layer continuously monitors system performance during\noperation. It initiates a shutdown only when disturbances become sufficiently\nsevere such that compensation is no longer viable and continued operation would\njeopardize the system or its environment. The proposed synthesis of DNN and RAC\npolicy guarantees uniform exponential stability of the entire WMR system while\nadhering to safety standards to some extent. The effectiveness of the proposed\napproach was further validated through real-time experiments using a 6,000 kg\nWMR."
                },
                "authors": [
                    {
                        "name": "Mehdi Heydari Shahna"
                    },
                    {
                        "name": "Jouni Mattila"
                    }
                ],
                "author_detail": {
                    "name": "Jouni Mattila"
                },
                "author": "Jouni Mattila",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10634v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10634v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09932v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09932v2",
                "updated": "2025-08-14T13:25:18Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    13,
                    25,
                    18,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-13T16:33:02Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    16,
                    33,
                    2,
                    2,
                    225,
                    0
                ],
                "title": "Mathematical Computation and Reasoning Errors by Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mathematical Computation and Reasoning Errors by Large Language Models"
                },
                "summary": "Large Language Models (LLMs) are increasingly utilized in AI-driven\neducational instruction and assessment, particularly within mathematics\neducation. The capability of LLMs to generate accurate answers and detailed\nsolutions for math problem-solving tasks is foundational for ensuring reliable\nand precise feedback and assessment in math education practices. Our study\nfocuses on evaluating the accuracy of four LLMs (OpenAI GPT-4o and o1,\nDeepSeek-V3 and DeepSeek-R1) solving three categories of math tasks, including\narithmetic, algebra, and number theory, and identifies step-level reasoning\nerrors within their solutions. Instead of relying on standard benchmarks, we\nintentionally build math tasks (via item models) that are challenging for LLMs\nand prone to errors. The accuracy of final answers and the presence of errors\nin individual solution steps were systematically analyzed and coded. Both\nsingle-agent and dual-agent configurations were tested. It is observed that the\nreasoning-enhanced OpenAI o1 model consistently achieved higher or nearly\nperfect accuracy across all three math task categories. Analysis of errors\nrevealed that procedural slips were the most frequent and significantly\nimpacted overall performance, while conceptual misunderstandings were less\nfrequent. Deploying dual-agent configurations substantially improved overall\nperformance. These findings offer actionable insights into enhancing LLM\nperformance and underscore effective strategies for integrating LLMs into\nmathematics education, thereby advancing AI-driven instructional practices and\nassessment precision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly utilized in AI-driven\neducational instruction and assessment, particularly within mathematics\neducation. The capability of LLMs to generate accurate answers and detailed\nsolutions for math problem-solving tasks is foundational for ensuring reliable\nand precise feedback and assessment in math education practices. Our study\nfocuses on evaluating the accuracy of four LLMs (OpenAI GPT-4o and o1,\nDeepSeek-V3 and DeepSeek-R1) solving three categories of math tasks, including\narithmetic, algebra, and number theory, and identifies step-level reasoning\nerrors within their solutions. Instead of relying on standard benchmarks, we\nintentionally build math tasks (via item models) that are challenging for LLMs\nand prone to errors. The accuracy of final answers and the presence of errors\nin individual solution steps were systematically analyzed and coded. Both\nsingle-agent and dual-agent configurations were tested. It is observed that the\nreasoning-enhanced OpenAI o1 model consistently achieved higher or nearly\nperfect accuracy across all three math task categories. Analysis of errors\nrevealed that procedural slips were the most frequent and significantly\nimpacted overall performance, while conceptual misunderstandings were less\nfrequent. Deploying dual-agent configurations substantially improved overall\nperformance. These findings offer actionable insights into enhancing LLM\nperformance and underscore effective strategies for integrating LLMs into\nmathematics education, thereby advancing AI-driven instructional practices and\nassessment precision."
                },
                "authors": [
                    {
                        "name": "Liang Zhang"
                    },
                    {
                        "name": "Edith Aurora Graf"
                    }
                ],
                "author_detail": {
                    "name": "Edith Aurora Graf"
                },
                "author": "Edith Aurora Graf",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09932v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09932v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00372v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00372v3",
                "updated": "2025-08-14T12:58:31Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    12,
                    58,
                    31,
                    3,
                    226,
                    0
                ],
                "published": "2025-02-01T09:19:08Z",
                "published_parsed": [
                    2025,
                    2,
                    1,
                    9,
                    19,
                    8,
                    5,
                    32,
                    0
                ],
                "title": "NAVER: A Neuro-Symbolic Compositional Automaton for Visual Grounding\n  with Explicit Logic Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NAVER: A Neuro-Symbolic Compositional Automaton for Visual Grounding\n  with Explicit Logic Reasoning"
                },
                "summary": "Visual Grounding (VG) tasks, such as referring expression detection and\nsegmentation tasks are important for linking visual entities to context,\nespecially in complex reasoning tasks that require detailed query\ninterpretation. This paper explores VG beyond basic perception, highlighting\nchallenges for methods that require reasoning like human cognition. Recent\nadvances in large language methods (LLMs) and Vision-Language methods (VLMs)\nhave improved abilities for visual comprehension, contextual understanding, and\nreasoning. These methods are mainly split into end-to-end and compositional\nmethods, with the latter offering more flexibility. Compositional approaches\nthat integrate LLMs and foundation models show promising performance but still\nstruggle with complex reasoning with language-based logical representations. To\naddress these limitations, we propose NAVER, a compositional visual grounding\nmethod that integrates explicit probabilistic logic reasoning within a\nfinite-state automaton, equipped with a self-correcting mechanism. This design\nimproves robustness and interpretability in inference through explicit logic\nreasoning. Our results show that NAVER achieves SoTA performance comparing to\nrecent end-to-end and compositional baselines. The code is available at\nhttps://github.com/ControlNet/NAVER .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual Grounding (VG) tasks, such as referring expression detection and\nsegmentation tasks are important for linking visual entities to context,\nespecially in complex reasoning tasks that require detailed query\ninterpretation. This paper explores VG beyond basic perception, highlighting\nchallenges for methods that require reasoning like human cognition. Recent\nadvances in large language methods (LLMs) and Vision-Language methods (VLMs)\nhave improved abilities for visual comprehension, contextual understanding, and\nreasoning. These methods are mainly split into end-to-end and compositional\nmethods, with the latter offering more flexibility. Compositional approaches\nthat integrate LLMs and foundation models show promising performance but still\nstruggle with complex reasoning with language-based logical representations. To\naddress these limitations, we propose NAVER, a compositional visual grounding\nmethod that integrates explicit probabilistic logic reasoning within a\nfinite-state automaton, equipped with a self-correcting mechanism. This design\nimproves robustness and interpretability in inference through explicit logic\nreasoning. Our results show that NAVER achieves SoTA performance comparing to\nrecent end-to-end and compositional baselines. The code is available at\nhttps://github.com/ControlNet/NAVER ."
                },
                "authors": [
                    {
                        "name": "Zhixi Cai"
                    },
                    {
                        "name": "Fucai Ke"
                    },
                    {
                        "name": "Simindokht Jahangard"
                    },
                    {
                        "name": "Maria Garcia de la Banda"
                    },
                    {
                        "name": "Reza Haffari"
                    },
                    {
                        "name": "Peter J. Stuckey"
                    },
                    {
                        "name": "Hamid Rezatofighi"
                    }
                ],
                "author_detail": {
                    "name": "Hamid Rezatofighi"
                },
                "author": "Hamid Rezatofighi",
                "arxiv_comment": "ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00372v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00372v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05294v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05294v2",
                "updated": "2025-08-14T12:55:31Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    12,
                    55,
                    31,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-07T11:48:03Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    11,
                    48,
                    3,
                    3,
                    219,
                    0
                ],
                "title": "Towards Embodied Agentic AI: Review and Classification of LLM- and\n  VLM-Driven Robot Autonomy and Interaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Embodied Agentic AI: Review and Classification of LLM- and\n  VLM-Driven Robot Autonomy and Interaction"
                },
                "summary": "Foundation models, including large language models (LLMs) and vision-language\nmodels (VLMs), have recently enabled novel approaches to robot autonomy and\nhuman-robot interfaces. In parallel, vision-language-action models (VLAs) or\nlarge behavior models (LBMs) are increasing the dexterity and capabilities of\nrobotic systems. This survey paper focuses on those works advancing towards\nagentic applications and architectures. This includes initial efforts exploring\nGPT-style interfaces to tooling, as well as more complex system where AI agents\nare coordinators, planners, perception actors, or generalist interfaces. Such\nagentic architectures allow robots to reason over natural language\ninstructions, invoke APIs, plan task sequences, or assist in operations and\ndiagnostics. In addition to peer-reviewed research, due to the fast-evolving\nnature of the field, we highlight and include community-driven projects, ROS\npackages, and industrial frameworks that show emerging trends. We propose a\ntaxonomy for classifying model integration approaches and present a comparative\nanalysis of the role that agents play in different solutions in today's\nliterature.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foundation models, including large language models (LLMs) and vision-language\nmodels (VLMs), have recently enabled novel approaches to robot autonomy and\nhuman-robot interfaces. In parallel, vision-language-action models (VLAs) or\nlarge behavior models (LBMs) are increasing the dexterity and capabilities of\nrobotic systems. This survey paper focuses on those works advancing towards\nagentic applications and architectures. This includes initial efforts exploring\nGPT-style interfaces to tooling, as well as more complex system where AI agents\nare coordinators, planners, perception actors, or generalist interfaces. Such\nagentic architectures allow robots to reason over natural language\ninstructions, invoke APIs, plan task sequences, or assist in operations and\ndiagnostics. In addition to peer-reviewed research, due to the fast-evolving\nnature of the field, we highlight and include community-driven projects, ROS\npackages, and industrial frameworks that show emerging trends. We propose a\ntaxonomy for classifying model integration approaches and present a comparative\nanalysis of the role that agents play in different solutions in today's\nliterature."
                },
                "authors": [
                    {
                        "name": "Sahar Salimpour"
                    },
                    {
                        "name": "Lei Fu"
                    },
                    {
                        "name": "Farhad Keramat"
                    },
                    {
                        "name": "Leonardo Militano"
                    },
                    {
                        "name": "Giovanni Toffetti"
                    },
                    {
                        "name": "Harry Edelman"
                    },
                    {
                        "name": "Jorge Pea Queralta"
                    }
                ],
                "author_detail": {
                    "name": "Jorge Pea Queralta"
                },
                "author": "Jorge Pea Queralta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05294v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05294v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10603v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10603v2",
                "updated": "2025-08-15T16:07:03Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    16,
                    7,
                    3,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-14T12:44:39Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    12,
                    44,
                    39,
                    3,
                    226,
                    0
                ],
                "title": "Why Report Failed Interactions With Robots?! Towards Vignette-based\n  Interaction Quality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Why Report Failed Interactions With Robots?! Towards Vignette-based\n  Interaction Quality"
                },
                "summary": "Although the quality of human-robot interactions has improved with the advent\nof LLMs, there are still various factors that cause systems to be sub-optimal\nwhen compared to human-human interactions. The nature and criticality of\nfailures are often dependent on the context of the interaction and so cannot be\ngeneralized across the wide range of scenarios and experiments which have been\nimplemented in HRI research. In this work we propose the use of a technique\noverlooked in the field of HRI, ethnographic vignettes, to clearly highlight\nthese failures, particularly those that are rarely documented. We describe the\nmethodology behind the process of writing vignettes and create our own based on\nour personal experiences with failures in HRI systems. We emphasize the\nstrength of vignettes as the ability to communicate failures from a\nmulti-disciplinary perspective, promote transparency about the capabilities of\nrobots, and document unexpected behaviours which would otherwise be omitted\nfrom research reports. We encourage the use of vignettes to augment existing\ninteraction evaluation methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although the quality of human-robot interactions has improved with the advent\nof LLMs, there are still various factors that cause systems to be sub-optimal\nwhen compared to human-human interactions. The nature and criticality of\nfailures are often dependent on the context of the interaction and so cannot be\ngeneralized across the wide range of scenarios and experiments which have been\nimplemented in HRI research. In this work we propose the use of a technique\noverlooked in the field of HRI, ethnographic vignettes, to clearly highlight\nthese failures, particularly those that are rarely documented. We describe the\nmethodology behind the process of writing vignettes and create our own based on\nour personal experiences with failures in HRI systems. We emphasize the\nstrength of vignettes as the ability to communicate failures from a\nmulti-disciplinary perspective, promote transparency about the capabilities of\nrobots, and document unexpected behaviours which would otherwise be omitted\nfrom research reports. We encourage the use of vignettes to augment existing\ninteraction evaluation methods."
                },
                "authors": [
                    {
                        "name": "Agnes Axelsson"
                    },
                    {
                        "name": "Merle Reimann"
                    },
                    {
                        "name": "Ronald Cumbal"
                    },
                    {
                        "name": "Hannah Pelikan"
                    },
                    {
                        "name": "Divesh Lala"
                    }
                ],
                "author_detail": {
                    "name": "Divesh Lala"
                },
                "author": "Divesh Lala",
                "arxiv_comment": "Accepted at the workshop on Real-World HRI in Public and Private\n  Spaces: Successes, Failures, and Lessons Learned (PubRob-Fails), held at the\n  IEEE RO-MAN Conference, 2025. 6 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10603v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10603v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10600v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10600v1",
                "updated": "2025-08-14T12:41:32Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    12,
                    41,
                    32,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T12:41:32Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    12,
                    41,
                    32,
                    3,
                    226,
                    0
                ],
                "title": "Towards Powerful and Practical Patch Attacks for 2D Object Detection in\n  Autonomous Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Powerful and Practical Patch Attacks for 2D Object Detection in\n  Autonomous Driving"
                },
                "summary": "Learning-based autonomous driving systems remain critically vulnerable to\nadversarial patches, posing serious safety and security risks in their\nreal-world deployment. Black-box attacks, notable for their high attack success\nrate without model knowledge, are especially concerning, with their\ntransferability extensively studied to reduce computational costs compared to\nquery-based attacks. Previous transferability-based black-box attacks typically\nadopt mean Average Precision (mAP) as the evaluation metric and design training\nloss accordingly. However, due to the presence of multiple detected bounding\nboxes and the relatively lenient Intersection over Union (IoU) thresholds, the\nattack effectiveness of these approaches is often overestimated, resulting in\nreduced success rates in practical attacking scenarios. Furthermore, patches\ntrained on low-resolution data often fail to maintain effectiveness on\nhigh-resolution images, limiting their transferability to autonomous driving\ndatasets. To fill this gap, we propose P$^3$A, a Powerful and Practical Patch\nAttack framework for 2D object detection in autonomous driving, specifically\noptimized for high-resolution datasets. First, we introduce a novel metric,\nPractical Attack Success Rate (PASR), to more accurately quantify attack\neffectiveness with greater relevance for pedestrian safety. Second, we present\na tailored Localization-Confidence Suppression Loss (LCSL) to improve attack\ntransferability under PASR. Finally, to maintain the transferability for\nhigh-resolution datasets, we further incorporate the Probabilistic\nScale-Preserving Padding (PSPP) into the patch attack pipeline as a data\npreprocessing step. Extensive experiments show that P$^3$A outperforms\nstate-of-the-art attacks on unseen models and unseen high-resolution datasets,\nboth under the proposed practical IoU-based evaluation metric and the previous\nmAP-based metrics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning-based autonomous driving systems remain critically vulnerable to\nadversarial patches, posing serious safety and security risks in their\nreal-world deployment. Black-box attacks, notable for their high attack success\nrate without model knowledge, are especially concerning, with their\ntransferability extensively studied to reduce computational costs compared to\nquery-based attacks. Previous transferability-based black-box attacks typically\nadopt mean Average Precision (mAP) as the evaluation metric and design training\nloss accordingly. However, due to the presence of multiple detected bounding\nboxes and the relatively lenient Intersection over Union (IoU) thresholds, the\nattack effectiveness of these approaches is often overestimated, resulting in\nreduced success rates in practical attacking scenarios. Furthermore, patches\ntrained on low-resolution data often fail to maintain effectiveness on\nhigh-resolution images, limiting their transferability to autonomous driving\ndatasets. To fill this gap, we propose P$^3$A, a Powerful and Practical Patch\nAttack framework for 2D object detection in autonomous driving, specifically\noptimized for high-resolution datasets. First, we introduce a novel metric,\nPractical Attack Success Rate (PASR), to more accurately quantify attack\neffectiveness with greater relevance for pedestrian safety. Second, we present\na tailored Localization-Confidence Suppression Loss (LCSL) to improve attack\ntransferability under PASR. Finally, to maintain the transferability for\nhigh-resolution datasets, we further incorporate the Probabilistic\nScale-Preserving Padding (PSPP) into the patch attack pipeline as a data\npreprocessing step. Extensive experiments show that P$^3$A outperforms\nstate-of-the-art attacks on unseen models and unseen high-resolution datasets,\nboth under the proposed practical IoU-based evaluation metric and the previous\nmAP-based metrics."
                },
                "authors": [
                    {
                        "name": "Yuxin Cao"
                    },
                    {
                        "name": "Yedi Zhang"
                    },
                    {
                        "name": "Wentao He"
                    },
                    {
                        "name": "Yifan Liao"
                    },
                    {
                        "name": "Yan Xiao"
                    },
                    {
                        "name": "Chang Li"
                    },
                    {
                        "name": "Zhiyong Huang"
                    },
                    {
                        "name": "Jin Song Dong"
                    }
                ],
                "author_detail": {
                    "name": "Jin Song Dong"
                },
                "author": "Jin Song Dong",
                "arxiv_comment": "13 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10600v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10600v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10594v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10594v1",
                "updated": "2025-08-14T12:37:20Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    12,
                    37,
                    20,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T12:37:20Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    12,
                    37,
                    20,
                    3,
                    226,
                    0
                ],
                "title": "FreeGAD: A Training-Free yet Effective Approach for Graph Anomaly\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FreeGAD: A Training-Free yet Effective Approach for Graph Anomaly\n  Detection"
                },
                "summary": "Graph Anomaly Detection (GAD) aims to identify nodes that deviate from the\nmajority within a graph, playing a crucial role in applications such as social\nnetworks and e-commerce. Despite the current advancements in deep\nlearning-based GAD, existing approaches often suffer from high deployment costs\nand poor scalability due to their complex and resource-intensive training\nprocesses. Surprisingly, our empirical findings suggest that the training phase\nof deep GAD methods, commonly perceived as crucial, may actually contribute\nless to anomaly detection performance than expected. Inspired by this, we\npropose FreeGAD, a novel training-free yet effective GAD method. Specifically,\nit leverages an affinity-gated residual encoder to generate anomaly-aware\nrepresentations. Meanwhile, FreeGAD identifies anchor nodes as pseudo-normal\nand anomalous guides, followed by calculating anomaly scores through\nanchor-guided statistical deviations. Extensive experiments demonstrate that\nFreeGAD achieves superior anomaly detection performance, efficiency, and\nscalability on multiple benchmark datasets from diverse domains, without any\ntraining or iterative optimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Anomaly Detection (GAD) aims to identify nodes that deviate from the\nmajority within a graph, playing a crucial role in applications such as social\nnetworks and e-commerce. Despite the current advancements in deep\nlearning-based GAD, existing approaches often suffer from high deployment costs\nand poor scalability due to their complex and resource-intensive training\nprocesses. Surprisingly, our empirical findings suggest that the training phase\nof deep GAD methods, commonly perceived as crucial, may actually contribute\nless to anomaly detection performance than expected. Inspired by this, we\npropose FreeGAD, a novel training-free yet effective GAD method. Specifically,\nit leverages an affinity-gated residual encoder to generate anomaly-aware\nrepresentations. Meanwhile, FreeGAD identifies anchor nodes as pseudo-normal\nand anomalous guides, followed by calculating anomaly scores through\nanchor-guided statistical deviations. Extensive experiments demonstrate that\nFreeGAD achieves superior anomaly detection performance, efficiency, and\nscalability on multiple benchmark datasets from diverse domains, without any\ntraining or iterative optimization."
                },
                "authors": [
                    {
                        "name": "Yunfeng Zhao"
                    },
                    {
                        "name": "Yixin Liu"
                    },
                    {
                        "name": "Shiyuan Li"
                    },
                    {
                        "name": "Qingfeng Chen"
                    },
                    {
                        "name": "Yu Zheng"
                    },
                    {
                        "name": "Shirui Pan"
                    }
                ],
                "author_detail": {
                    "name": "Shirui Pan"
                },
                "author": "Shirui Pan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10594v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10594v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.03555v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.03555v6",
                "updated": "2025-08-14T12:30:30Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    12,
                    30,
                    30,
                    3,
                    226,
                    0
                ],
                "published": "2024-05-06T15:25:48Z",
                "published_parsed": [
                    2024,
                    5,
                    6,
                    15,
                    25,
                    48,
                    0,
                    127,
                    0
                ],
                "title": "A Comprehensive Tutorial and Survey of O-RAN: Exploring Slicing-aware\n  Architecture, Deployment Options, Use Cases, and Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comprehensive Tutorial and Survey of O-RAN: Exploring Slicing-aware\n  Architecture, Deployment Options, Use Cases, and Challenges"
                },
                "summary": "Open-radio access network (O-RAN) seeks to establish the principles of\nopenness, programmability, automation, intelligence, and hardware-software\ndisaggregation with interoperable and standard-compliant interfaces. It\nadvocates for multi-vendorism and multi-stakeholderism within a cloudified and\nvirtualized wireless infrastructure, aimed at enhancing the deployment,\noperation, and management of RAN architecture. These enhancements promise\nincreased flexibility, performance optimization, service innovation, energy\nefficiency, and cost effectiveness across fifth-generation (5G),\nsixth-generation (6G), and beyond networks. A silent feature of O-RAN\narchitecture is its support for network slicing, which entails interaction with\nother domains of the cellular network, notably the transport network (TN) and\nthe core network (CN), to realize end-to-end (E2E) network slicing. The study\nof this feature requires exploring the stances and contributions of diverse\nstandards development organizations (SDOs). In this context, we note that\ndespite the ongoing industrial deployments and standardization efforts, the\nresearch and standardization communities have yet to comprehensively address\nnetwork slicing in O-RAN. To address this gap, this paper provides a\ncomprehensive exploration of network slicing in O-RAN through an in-depth\nreview of specification documents from O-RAN Alliance and research papers from\nleading industry and academic institutions. The paper commences with an\noverview of the relevant standardization and open source contributions,\nsubsequently delving into the latest O-RAN architecture with an emphasis on its\nslicing aspects. Furthermore, the paper explores O-RAN deployment scenarios,\nexamining options for the deployment and orchestration of RAN and TN slice\nsubnets. It also discusses the slicing of the underlying infrastructure and\nprovides an overview of various use cases related...",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-radio access network (O-RAN) seeks to establish the principles of\nopenness, programmability, automation, intelligence, and hardware-software\ndisaggregation with interoperable and standard-compliant interfaces. It\nadvocates for multi-vendorism and multi-stakeholderism within a cloudified and\nvirtualized wireless infrastructure, aimed at enhancing the deployment,\noperation, and management of RAN architecture. These enhancements promise\nincreased flexibility, performance optimization, service innovation, energy\nefficiency, and cost effectiveness across fifth-generation (5G),\nsixth-generation (6G), and beyond networks. A silent feature of O-RAN\narchitecture is its support for network slicing, which entails interaction with\nother domains of the cellular network, notably the transport network (TN) and\nthe core network (CN), to realize end-to-end (E2E) network slicing. The study\nof this feature requires exploring the stances and contributions of diverse\nstandards development organizations (SDOs). In this context, we note that\ndespite the ongoing industrial deployments and standardization efforts, the\nresearch and standardization communities have yet to comprehensively address\nnetwork slicing in O-RAN. To address this gap, this paper provides a\ncomprehensive exploration of network slicing in O-RAN through an in-depth\nreview of specification documents from O-RAN Alliance and research papers from\nleading industry and academic institutions. The paper commences with an\noverview of the relevant standardization and open source contributions,\nsubsequently delving into the latest O-RAN architecture with an emphasis on its\nslicing aspects. Furthermore, the paper explores O-RAN deployment scenarios,\nexamining options for the deployment and orchestration of RAN and TN slice\nsubnets. It also discusses the slicing of the underlying infrastructure and\nprovides an overview of various use cases related..."
                },
                "authors": [
                    {
                        "name": "Khurshid Alam"
                    },
                    {
                        "name": "Mohammad Asif Habibi"
                    },
                    {
                        "name": "Matthias Tammen"
                    },
                    {
                        "name": "Dennis Krummacker"
                    },
                    {
                        "name": "Walid Saad"
                    },
                    {
                        "name": "Marco Di Renzo"
                    },
                    {
                        "name": "Tommaso Melodia"
                    },
                    {
                        "name": "Xavier Costa-Prez"
                    },
                    {
                        "name": "Mrouane Debbah"
                    },
                    {
                        "name": "Ashutosh Dutta"
                    },
                    {
                        "name": "Hans D. Schotten"
                    }
                ],
                "author_detail": {
                    "name": "Hans D. Schotten"
                },
                "author": "Hans D. Schotten",
                "arxiv_doi": "10.1109/COMST.2025.3598406",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/COMST.2025.3598406",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.03555v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.03555v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "This article has been accepted for publication in IEEE Communications\n  Surveys & Tutorials (2025)",
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.05262v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.05262v3",
                "updated": "2025-08-14T12:30:25Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    12,
                    30,
                    25,
                    3,
                    226,
                    0
                ],
                "published": "2024-03-08T12:35:07Z",
                "published_parsed": [
                    2024,
                    3,
                    8,
                    12,
                    35,
                    7,
                    4,
                    68,
                    0
                ],
                "title": "Debiasing Multimodal Large Language Models via Penalization of Language\n  Priors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Debiasing Multimodal Large Language Models via Penalization of Language\n  Priors"
                },
                "summary": "In the realms of computer vision and natural language processing, Multimodal\nLarge Language Models (MLLMs) have become indispensable tools, proficient in\ngenerating textual responses based on visual inputs. Despite their\nadvancements, our investigation reveals a noteworthy bias: the generated\ncontent is often driven more by the inherent priors of the underlying Large\nLanguage Models (LLMs) than by the input image. Empirical experiments\nunderscore the persistence of this bias, as MLLMs often provide confident\nanswers even in the absence of relevant images or given incongruent visual\ninputs. To rectify these biases and redirect the model's focus toward visual\ninformation, we propose two simple, training-free strategies. First, for tasks\nsuch as classification or multi-choice question answering, we introduce a\n\"Post-Hoc Debias\" method using an affine calibration step to adjust the output\ndistribution. This approach ensures uniform answer scores when the image is\nabsent, acting as an effective regularization technique to alleviate the\ninfluence of LLM priors. For more intricate open-ended generation tasks, we\nextend this method to \"Visual Debias Decoding\", which mitigates bias by\ncontrasting token log-probabilities conditioned on a correct image versus a\nmeaningless one. Additionally, our investigation sheds light on the instability\nof MLLMs across various decoding configurations. Through systematic exploration\nof different settings, we achieve significant performance\nimprovements--surpassing previously reported results--and raise concerns about\nthe fairness of current evaluation practices. Comprehensive experiments\nsubstantiate the effectiveness of our proposed strategies in mitigating biases.\nThese strategies not only prove beneficial in minimizing hallucinations but\nalso contribute to the generation of more helpful and precise illustrations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the realms of computer vision and natural language processing, Multimodal\nLarge Language Models (MLLMs) have become indispensable tools, proficient in\ngenerating textual responses based on visual inputs. Despite their\nadvancements, our investigation reveals a noteworthy bias: the generated\ncontent is often driven more by the inherent priors of the underlying Large\nLanguage Models (LLMs) than by the input image. Empirical experiments\nunderscore the persistence of this bias, as MLLMs often provide confident\nanswers even in the absence of relevant images or given incongruent visual\ninputs. To rectify these biases and redirect the model's focus toward visual\ninformation, we propose two simple, training-free strategies. First, for tasks\nsuch as classification or multi-choice question answering, we introduce a\n\"Post-Hoc Debias\" method using an affine calibration step to adjust the output\ndistribution. This approach ensures uniform answer scores when the image is\nabsent, acting as an effective regularization technique to alleviate the\ninfluence of LLM priors. For more intricate open-ended generation tasks, we\nextend this method to \"Visual Debias Decoding\", which mitigates bias by\ncontrasting token log-probabilities conditioned on a correct image versus a\nmeaningless one. Additionally, our investigation sheds light on the instability\nof MLLMs across various decoding configurations. Through systematic exploration\nof different settings, we achieve significant performance\nimprovements--surpassing previously reported results--and raise concerns about\nthe fairness of current evaluation practices. Comprehensive experiments\nsubstantiate the effectiveness of our proposed strategies in mitigating biases.\nThese strategies not only prove beneficial in minimizing hallucinations but\nalso contribute to the generation of more helpful and precise illustrations."
                },
                "authors": [
                    {
                        "name": "YiFan Zhang"
                    },
                    {
                        "name": "Yang Shi"
                    },
                    {
                        "name": "Weichen Yu"
                    },
                    {
                        "name": "Qingsong Wen"
                    },
                    {
                        "name": "Xue Wang"
                    },
                    {
                        "name": "Wenjing Yang"
                    },
                    {
                        "name": "Zhang Zhang"
                    },
                    {
                        "name": "Liang Wang"
                    },
                    {
                        "name": "Rong Jin"
                    }
                ],
                "author_detail": {
                    "name": "Rong Jin"
                },
                "author": "Rong Jin",
                "arxiv_comment": "10 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.05262v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.05262v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10581v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10581v1",
                "updated": "2025-08-14T12:20:51Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    12,
                    20,
                    51,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T12:20:51Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    12,
                    20,
                    51,
                    3,
                    226,
                    0
                ],
                "title": "Technical Report: Facilitating the Adoption of Causal Inference Methods\n  Through LLM-Empowered Co-Pilot",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Technical Report: Facilitating the Adoption of Causal Inference Methods\n  Through LLM-Empowered Co-Pilot"
                },
                "summary": "Estimating treatment effects (TE) from observational data is a critical yet\ncomplex task in many fields, from healthcare and economics to public policy.\nWhile recent advances in machine learning and causal inference have produced\npowerful estimation techniques, their adoption remains limited due to the need\nfor deep expertise in causal assumptions, adjustment strategies, and model\nselection. In this paper, we introduce CATE-B, an open-source co-pilot system\nthat uses large language models (LLMs) within an agentic framework to guide\nusers through the end-to-end process of treatment effect estimation. CATE-B\nassists in (i) constructing a structural causal model via causal discovery and\nLLM-based edge orientation, (ii) identifying robust adjustment sets through a\nnovel Minimal Uncertainty Adjustment Set criterion, and (iii) selecting\nappropriate regression methods tailored to the causal structure and dataset\ncharacteristics. To encourage reproducibility and evaluation, we release a\nsuite of benchmark tasks spanning diverse domains and causal complexities. By\ncombining causal inference with intelligent, interactive assistance, CATE-B\nlowers the barrier to rigorous causal analysis and lays the foundation for a\nnew class of benchmarks in automated treatment effect estimation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimating treatment effects (TE) from observational data is a critical yet\ncomplex task in many fields, from healthcare and economics to public policy.\nWhile recent advances in machine learning and causal inference have produced\npowerful estimation techniques, their adoption remains limited due to the need\nfor deep expertise in causal assumptions, adjustment strategies, and model\nselection. In this paper, we introduce CATE-B, an open-source co-pilot system\nthat uses large language models (LLMs) within an agentic framework to guide\nusers through the end-to-end process of treatment effect estimation. CATE-B\nassists in (i) constructing a structural causal model via causal discovery and\nLLM-based edge orientation, (ii) identifying robust adjustment sets through a\nnovel Minimal Uncertainty Adjustment Set criterion, and (iii) selecting\nappropriate regression methods tailored to the causal structure and dataset\ncharacteristics. To encourage reproducibility and evaluation, we release a\nsuite of benchmark tasks spanning diverse domains and causal complexities. By\ncombining causal inference with intelligent, interactive assistance, CATE-B\nlowers the barrier to rigorous causal analysis and lays the foundation for a\nnew class of benchmarks in automated treatment effect estimation."
                },
                "authors": [
                    {
                        "name": "Jeroen Berrevoets"
                    },
                    {
                        "name": "Julianna Piskorz"
                    },
                    {
                        "name": "Robert Davis"
                    },
                    {
                        "name": "Harry Amad"
                    },
                    {
                        "name": "Jim Weatherall"
                    },
                    {
                        "name": "Mihaela van der Schaar"
                    }
                ],
                "author_detail": {
                    "name": "Mihaela van der Schaar"
                },
                "author": "Mihaela van der Schaar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10581v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10581v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14325v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14325v3",
                "updated": "2025-08-14T12:12:53Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    12,
                    12,
                    53,
                    3,
                    226,
                    0
                ],
                "published": "2025-04-19T15:29:04Z",
                "published_parsed": [
                    2025,
                    4,
                    19,
                    15,
                    29,
                    4,
                    5,
                    109,
                    0
                ],
                "title": "FAIRGAME: a Framework for AI Agents Bias Recognition using Game Theory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FAIRGAME: a Framework for AI Agents Bias Recognition using Game Theory"
                },
                "summary": "Letting AI agents interact in multi-agent applications adds a layer of\ncomplexity to the interpretability and prediction of AI outcomes, with profound\nimplications for their trustworthy adoption in research and society. Game\ntheory offers powerful models to capture and interpret strategic interaction\namong agents, but requires the support of reproducible, standardized and\nuser-friendly IT frameworks to enable comparison and interpretation of results.\nTo this end, we present FAIRGAME, a Framework for AI Agents Bias Recognition\nusing Game Theory. We describe its implementation and usage, and we employ it\nto uncover biased outcomes in popular games among AI agents, depending on the\nemployed Large Language Model (LLM) and used language, as well as on the\npersonality trait or strategic knowledge of the agents. Overall, FAIRGAME\nallows users to reliably and easily simulate their desired games and scenarios\nand compare the results across simulation campaigns and with game-theoretic\npredictions, enabling the systematic discovery of biases, the anticipation of\nemerging behavior out of strategic interplays, and empowering further research\ninto strategic decision-making using LLM agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Letting AI agents interact in multi-agent applications adds a layer of\ncomplexity to the interpretability and prediction of AI outcomes, with profound\nimplications for their trustworthy adoption in research and society. Game\ntheory offers powerful models to capture and interpret strategic interaction\namong agents, but requires the support of reproducible, standardized and\nuser-friendly IT frameworks to enable comparison and interpretation of results.\nTo this end, we present FAIRGAME, a Framework for AI Agents Bias Recognition\nusing Game Theory. We describe its implementation and usage, and we employ it\nto uncover biased outcomes in popular games among AI agents, depending on the\nemployed Large Language Model (LLM) and used language, as well as on the\npersonality trait or strategic knowledge of the agents. Overall, FAIRGAME\nallows users to reliably and easily simulate their desired games and scenarios\nand compare the results across simulation campaigns and with game-theoretic\npredictions, enabling the systematic discovery of biases, the anticipation of\nemerging behavior out of strategic interplays, and empowering further research\ninto strategic decision-making using LLM agents."
                },
                "authors": [
                    {
                        "name": "Alessio Buscemi"
                    },
                    {
                        "name": "Daniele Proverbio"
                    },
                    {
                        "name": "Alessandro Di Stefano"
                    },
                    {
                        "name": "The Anh Han"
                    },
                    {
                        "name": "German Castignani"
                    },
                    {
                        "name": "Pietro Li"
                    }
                ],
                "author_detail": {
                    "name": "Pietro Li"
                },
                "author": "Pietro Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14325v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14325v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10572v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10572v1",
                "updated": "2025-08-14T12:11:15Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    12,
                    11,
                    15,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T12:11:15Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    12,
                    11,
                    15,
                    3,
                    226,
                    0
                ],
                "title": "Towards Agentic AI for Multimodal-Guided Video Object Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Agentic AI for Multimodal-Guided Video Object Segmentation"
                },
                "summary": "Referring-based Video Object Segmentation is a multimodal problem that\nrequires producing fine-grained segmentation results guided by external cues.\nTraditional approaches to this task typically involve training specialized\nmodels, which come with high computational complexity and manual annotation\neffort. Recent advances in vision-language foundation models open a promising\ndirection toward training-free approaches. Several studies have explored\nleveraging these general-purpose models for fine-grained segmentation,\nachieving performance comparable to that of fully supervised, task-specific\nmodels. However, existing methods rely on fixed pipelines that lack the\nflexibility needed to adapt to the dynamic nature of the task. To address this\nlimitation, we propose Multi-Modal Agent, a novel agentic system designed to\nsolve this task in a more flexible and adaptive manner. Specifically, our\nmethod leverages the reasoning capabilities of large language models (LLMs) to\ngenerate dynamic workflows tailored to each input. This adaptive procedure\niteratively interacts with a set of specialized tools designed for low-level\ntasks across different modalities to identify the target object described by\nthe multimodal cues. Our agentic approach demonstrates clear improvements over\nprior methods on two multimodal-conditioned VOS tasks: RVOS and Ref-AVS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Referring-based Video Object Segmentation is a multimodal problem that\nrequires producing fine-grained segmentation results guided by external cues.\nTraditional approaches to this task typically involve training specialized\nmodels, which come with high computational complexity and manual annotation\neffort. Recent advances in vision-language foundation models open a promising\ndirection toward training-free approaches. Several studies have explored\nleveraging these general-purpose models for fine-grained segmentation,\nachieving performance comparable to that of fully supervised, task-specific\nmodels. However, existing methods rely on fixed pipelines that lack the\nflexibility needed to adapt to the dynamic nature of the task. To address this\nlimitation, we propose Multi-Modal Agent, a novel agentic system designed to\nsolve this task in a more flexible and adaptive manner. Specifically, our\nmethod leverages the reasoning capabilities of large language models (LLMs) to\ngenerate dynamic workflows tailored to each input. This adaptive procedure\niteratively interacts with a set of specialized tools designed for low-level\ntasks across different modalities to identify the target object described by\nthe multimodal cues. Our agentic approach demonstrates clear improvements over\nprior methods on two multimodal-conditioned VOS tasks: RVOS and Ref-AVS."
                },
                "authors": [
                    {
                        "name": "Tuyen Tran"
                    },
                    {
                        "name": "Thao Minh Le"
                    },
                    {
                        "name": "Truyen Tran"
                    }
                ],
                "author_detail": {
                    "name": "Truyen Tran"
                },
                "author": "Truyen Tran",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10572v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10572v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10557v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10557v2",
                "updated": "2025-08-15T06:20:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    6,
                    20,
                    9,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-14T11:55:21Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    11,
                    55,
                    21,
                    3,
                    226,
                    0
                ],
                "title": "PTQAT: A Hybrid Parameter-Efficient Quantization Algorithm for 3D\n  Perception Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PTQAT: A Hybrid Parameter-Efficient Quantization Algorithm for 3D\n  Perception Tasks"
                },
                "summary": "Post-Training Quantization (PTQ) and Quantization-Aware Training (QAT)\nrepresent two mainstream model quantization approaches. However, PTQ often\nleads to unacceptable performance degradation in quantized models, while QAT\nimposes substantial GPU memory requirements and extended training time due to\nweight fine-tuning. In this paper, we propose PTQAT, a novel general hybrid\nquantization algorithm for the efficient deployment of 3D perception networks.\nTo address the speed accuracy trade-off between PTQ and QAT, our method selects\ncritical layers for QAT fine-tuning and performs PTQ on the remaining layers.\nContrary to intuition, fine-tuning the layers with smaller output discrepancies\nbefore and after quantization, rather than those with larger discrepancies,\nactually leads to greater improvements in the model's quantization accuracy.\nThis means we better compensate for quantization errors during their\npropagation, rather than addressing them at the point where they occur. The\nproposed PTQAT achieves similar performance to QAT with more efficiency by\nfreezing nearly 50% of quantifiable layers. Additionally, PTQAT is a universal\nquantization method that supports various quantization bit widths (4 bits) as\nwell as different model architectures, including CNNs and Transformers. The\nexperimental results on nuScenes across diverse 3D perception tasks, including\nobject detection, semantic segmentation, and occupancy prediction, show that\nour method consistently outperforms QAT-only baselines. Notably, it achieves\n0.2%-0.9% NDS and 0.3%-1.0% mAP gains in object detection, 0.3%-2.0% mIoU gains\nin semantic segmentation and occupancy prediction while fine-tuning fewer\nweights.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-Training Quantization (PTQ) and Quantization-Aware Training (QAT)\nrepresent two mainstream model quantization approaches. However, PTQ often\nleads to unacceptable performance degradation in quantized models, while QAT\nimposes substantial GPU memory requirements and extended training time due to\nweight fine-tuning. In this paper, we propose PTQAT, a novel general hybrid\nquantization algorithm for the efficient deployment of 3D perception networks.\nTo address the speed accuracy trade-off between PTQ and QAT, our method selects\ncritical layers for QAT fine-tuning and performs PTQ on the remaining layers.\nContrary to intuition, fine-tuning the layers with smaller output discrepancies\nbefore and after quantization, rather than those with larger discrepancies,\nactually leads to greater improvements in the model's quantization accuracy.\nThis means we better compensate for quantization errors during their\npropagation, rather than addressing them at the point where they occur. The\nproposed PTQAT achieves similar performance to QAT with more efficiency by\nfreezing nearly 50% of quantifiable layers. Additionally, PTQAT is a universal\nquantization method that supports various quantization bit widths (4 bits) as\nwell as different model architectures, including CNNs and Transformers. The\nexperimental results on nuScenes across diverse 3D perception tasks, including\nobject detection, semantic segmentation, and occupancy prediction, show that\nour method consistently outperforms QAT-only baselines. Notably, it achieves\n0.2%-0.9% NDS and 0.3%-1.0% mAP gains in object detection, 0.3%-2.0% mIoU gains\nin semantic segmentation and occupancy prediction while fine-tuning fewer\nweights."
                },
                "authors": [
                    {
                        "name": "Xinhao Wang"
                    },
                    {
                        "name": "Zhiwei Lin"
                    },
                    {
                        "name": "Zhongyu Xia"
                    },
                    {
                        "name": "Yongtao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yongtao Wang"
                },
                "author": "Yongtao Wang",
                "arxiv_comment": "8 pages, Accepted by ICCVW 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10557v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10557v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06867v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06867v3",
                "updated": "2025-08-14T11:54:11Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    11,
                    54,
                    11,
                    3,
                    226,
                    0
                ],
                "published": "2025-04-09T13:16:48Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    13,
                    16,
                    48,
                    2,
                    99,
                    0
                ],
                "title": "xApp Conflict Mitigation with Scheduler",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "xApp Conflict Mitigation with Scheduler"
                },
                "summary": "Open RAN (O-RAN) fosters multi-vendor interoperability and data-driven\ncontrol but simultaneously introduces the challenge of coordinating pre-trained\nxApps that may produce conflicting actions. Although O-RAN specifications\nmandate offline training and validation to prevent the deployment of untrained\nor inadequately tested models, operational conflicts can still arise under\ndynamic and context-dependent conditions.This work proposes a scheduler-based\nconflict mitigation framework to address these challenges without requiring\ntraining xApps together or further xApp re-training. By examining an indirect\nconflict involving power and resource block allocation xApps and employing an\nAdvantage Actor-Critic (A2C) approach to train both xApps and the scheduler, we\nillustrate that a straightforward A2C-based scheduler improves performance\nrelative to independently deployed xApps and conflicting cases. Notably, among\nall tested deployment scenarios (including individual xApp deployment, multiple\nconflicting xApps, and limited scheduler configurations), augmenting the system\nwith baseline xApps and enabling the scheduler to select from a broader pool\nachieves the highest total transmission rate, thereby underscoring the\nimportance of adaptive scheduling mechanisms. These findings highlight the\ncontext-dependent nature of conflicts in automated network management, as two\nxApps may conflict under certain conditions but coexist under others.\nConsequently, the ability to dynamically update and adapt the scheduler to\naccommodate diverse operational intents is vital for future network\ndeployments. By offering dynamic scheduling without re-training xApps, this\nframework advances practical conflict resolution solutions while supporting\nreal-world scalability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open RAN (O-RAN) fosters multi-vendor interoperability and data-driven\ncontrol but simultaneously introduces the challenge of coordinating pre-trained\nxApps that may produce conflicting actions. Although O-RAN specifications\nmandate offline training and validation to prevent the deployment of untrained\nor inadequately tested models, operational conflicts can still arise under\ndynamic and context-dependent conditions.This work proposes a scheduler-based\nconflict mitigation framework to address these challenges without requiring\ntraining xApps together or further xApp re-training. By examining an indirect\nconflict involving power and resource block allocation xApps and employing an\nAdvantage Actor-Critic (A2C) approach to train both xApps and the scheduler, we\nillustrate that a straightforward A2C-based scheduler improves performance\nrelative to independently deployed xApps and conflicting cases. Notably, among\nall tested deployment scenarios (including individual xApp deployment, multiple\nconflicting xApps, and limited scheduler configurations), augmenting the system\nwith baseline xApps and enabling the scheduler to select from a broader pool\nachieves the highest total transmission rate, thereby underscoring the\nimportance of adaptive scheduling mechanisms. These findings highlight the\ncontext-dependent nature of conflicts in automated network management, as two\nxApps may conflict under certain conditions but coexist under others.\nConsequently, the ability to dynamically update and adapt the scheduler to\naccommodate diverse operational intents is vital for future network\ndeployments. By offering dynamic scheduling without re-training xApps, this\nframework advances practical conflict resolution solutions while supporting\nreal-world scalability."
                },
                "authors": [
                    {
                        "name": "Idris Cinemre"
                    },
                    {
                        "name": "Toktam Mahmoodi"
                    },
                    {
                        "name": "Amirmohammad Farzaneh"
                    }
                ],
                "author_detail": {
                    "name": "Amirmohammad Farzaneh"
                },
                "author": "Amirmohammad Farzaneh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06867v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06867v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10556v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10556v1",
                "updated": "2025-08-14T11:52:43Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    11,
                    52,
                    43,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T11:52:43Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    11,
                    52,
                    43,
                    3,
                    226,
                    0
                ],
                "title": "Retrieval-Augmented Prompt for OOD Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Prompt for OOD Detection"
                },
                "summary": "Out-of-Distribution (OOD) detection is crucial for the reliable deployment of\nmachine learning models in-the-wild, enabling accurate identification of test\nsamples that differ from the training data distribution. Existing methods rely\non auxiliary outlier samples or in-distribution (ID) data to generate outlier\ninformation for training, but due to limited outliers and their mismatch with\nreal test OOD samples, they often fail to provide sufficient semantic\nsupervision, leading to suboptimal performance. To address this, we propose a\nnovel OOD detection method called Retrieval-Augmented Prompt (RAP). RAP\naugments a pre-trained vision-language model's prompts by retrieving external\nknowledge, offering enhanced semantic supervision for OOD detection. During\ntraining, RAP retrieves descriptive words for outliers based on joint\nsimilarity with external textual knowledge and uses them to augment the model's\nOOD prompts. During testing, RAP dynamically updates OOD prompts in real-time\nbased on the encountered OOD samples, enabling the model to rapidly adapt to\nthe test environment. Our extensive experiments demonstrate that RAP achieves\nstate-of-the-art performance on large-scale OOD detection benchmarks. For\nexample, in 1-shot OOD detection on the ImageNet-1k dataset, RAP reduces the\naverage FPR95 by 7.05% and improves the AUROC by 1.71% compared to previous\nmethods. Additionally, comprehensive ablation studies validate the\neffectiveness of each module and the underlying motivations of our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Out-of-Distribution (OOD) detection is crucial for the reliable deployment of\nmachine learning models in-the-wild, enabling accurate identification of test\nsamples that differ from the training data distribution. Existing methods rely\non auxiliary outlier samples or in-distribution (ID) data to generate outlier\ninformation for training, but due to limited outliers and their mismatch with\nreal test OOD samples, they often fail to provide sufficient semantic\nsupervision, leading to suboptimal performance. To address this, we propose a\nnovel OOD detection method called Retrieval-Augmented Prompt (RAP). RAP\naugments a pre-trained vision-language model's prompts by retrieving external\nknowledge, offering enhanced semantic supervision for OOD detection. During\ntraining, RAP retrieves descriptive words for outliers based on joint\nsimilarity with external textual knowledge and uses them to augment the model's\nOOD prompts. During testing, RAP dynamically updates OOD prompts in real-time\nbased on the encountered OOD samples, enabling the model to rapidly adapt to\nthe test environment. Our extensive experiments demonstrate that RAP achieves\nstate-of-the-art performance on large-scale OOD detection benchmarks. For\nexample, in 1-shot OOD detection on the ImageNet-1k dataset, RAP reduces the\naverage FPR95 by 7.05% and improves the AUROC by 1.71% compared to previous\nmethods. Additionally, comprehensive ablation studies validate the\neffectiveness of each module and the underlying motivations of our approach."
                },
                "authors": [
                    {
                        "name": "Ruisong Han"
                    },
                    {
                        "name": "Zongbo Han"
                    },
                    {
                        "name": "Jiahao Zhang"
                    },
                    {
                        "name": "Mingyue Cheng"
                    },
                    {
                        "name": "Changqing Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Changqing Zhang"
                },
                "author": "Changqing Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10556v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10556v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22107v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22107v4",
                "updated": "2025-08-14T11:51:31Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    11,
                    51,
                    31,
                    3,
                    226,
                    0
                ],
                "published": "2025-05-28T08:34:46Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    8,
                    34,
                    46,
                    2,
                    148,
                    0
                ],
                "title": "Curse of High Dimensionality Issue in Transformer for Long-context\n  Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Curse of High Dimensionality Issue in Transformer for Long-context\n  Modeling"
                },
                "summary": "Transformer-based large language models (LLMs) excel in natural language\nprocessing tasks by capturing long-range dependencies through self-attention\nmechanisms. However, long-context modeling faces significant computational\ninefficiencies due to \\textit{redundant} attention computations: while\nattention weights are often \\textit{sparse}, all tokens consume \\textit{equal}\ncomputational resources. In this paper, we reformulate traditional\nprobabilistic sequence modeling as a \\textit{supervised learning task},\nenabling the separation of relevant and irrelevant tokens and providing a\nclearer understanding of redundancy. Based on this reformulation, we\ntheoretically analyze attention sparsity, revealing that only a few tokens\nsignificantly contribute to predictions. Building on this, we formulate\nattention optimization as a linear coding problem and propose a \\textit{group\ncoding strategy}, theoretically showing its ability to improve robustness\nagainst random noise and enhance learning efficiency. Motivated by this, we\npropose \\textit{Dynamic Group Attention} (DGA), which leverages the group\ncoding to explicitly reduce redundancy by aggregating less important tokens\nduring attention computation. Empirical results show that our DGA significantly\nreduces computational costs while maintaining competitive performance.Code is\navailable at https://github.com/bolixinyu/DynamicGroupAttention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) excel in natural language\nprocessing tasks by capturing long-range dependencies through self-attention\nmechanisms. However, long-context modeling faces significant computational\ninefficiencies due to \\textit{redundant} attention computations: while\nattention weights are often \\textit{sparse}, all tokens consume \\textit{equal}\ncomputational resources. In this paper, we reformulate traditional\nprobabilistic sequence modeling as a \\textit{supervised learning task},\nenabling the separation of relevant and irrelevant tokens and providing a\nclearer understanding of redundancy. Based on this reformulation, we\ntheoretically analyze attention sparsity, revealing that only a few tokens\nsignificantly contribute to predictions. Building on this, we formulate\nattention optimization as a linear coding problem and propose a \\textit{group\ncoding strategy}, theoretically showing its ability to improve robustness\nagainst random noise and enhance learning efficiency. Motivated by this, we\npropose \\textit{Dynamic Group Attention} (DGA), which leverages the group\ncoding to explicitly reduce redundancy by aggregating less important tokens\nduring attention computation. Empirical results show that our DGA significantly\nreduces computational costs while maintaining competitive performance.Code is\navailable at https://github.com/bolixinyu/DynamicGroupAttention."
                },
                "authors": [
                    {
                        "name": "Shuhai Zhang"
                    },
                    {
                        "name": "Zeng You"
                    },
                    {
                        "name": "Yaofo Chen"
                    },
                    {
                        "name": "Zhiquan Wen"
                    },
                    {
                        "name": "Qianyue Wang"
                    },
                    {
                        "name": "Zhijie Qiu"
                    },
                    {
                        "name": "Yuanqing Li"
                    },
                    {
                        "name": "Mingkui Tan"
                    }
                ],
                "author_detail": {
                    "name": "Mingkui Tan"
                },
                "author": "Mingkui Tan",
                "arxiv_comment": "Accepted at ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22107v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22107v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10553v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10553v1",
                "updated": "2025-08-14T11:45:34Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    11,
                    45,
                    34,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T11:45:34Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    11,
                    45,
                    34,
                    3,
                    226,
                    0
                ],
                "title": "eDIF: A European Deep Inference Fabric for Remote Interpretability of\n  LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "eDIF: A European Deep Inference Fabric for Remote Interpretability of\n  LLM"
                },
                "summary": "This paper presents a feasibility study on the deployment of a European Deep\nInference Fabric (eDIF), an NDIF-compatible infrastructure designed to support\nmechanistic interpretability research on large language models. The need for\nwidespread accessibility of LLM interpretability infrastructure in Europe\ndrives this initiative to democratize advanced model analysis capabilities for\nthe research community. The project introduces a GPU-based cluster hosted at\nAnsbach University of Applied Sciences and interconnected with partner\ninstitutions, enabling remote model inspection via the NNsight API. A\nstructured pilot study involving 16 researchers from across Europe evaluated\nthe platform's technical performance, usability, and scientific utility. Users\nconducted interventions such as activation patching, causal tracing, and\nrepresentation analysis on models including GPT-2 and DeepSeek-R1-70B. The\nstudy revealed a gradual increase in user engagement, stable platform\nperformance throughout, and a positive reception of the remote experimentation\ncapabilities. It also marked the starting point for building a user community\naround the platform. Identified limitations such as prolonged download\ndurations for activation data as well as intermittent execution interruptions\nare addressed in the roadmap for future development. This initiative marks a\nsignificant step towards widespread accessibility of LLM interpretability\ninfrastructure in Europe and lays the groundwork for broader deployment,\nexpanded tooling, and sustained community collaboration in mechanistic\ninterpretability research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a feasibility study on the deployment of a European Deep\nInference Fabric (eDIF), an NDIF-compatible infrastructure designed to support\nmechanistic interpretability research on large language models. The need for\nwidespread accessibility of LLM interpretability infrastructure in Europe\ndrives this initiative to democratize advanced model analysis capabilities for\nthe research community. The project introduces a GPU-based cluster hosted at\nAnsbach University of Applied Sciences and interconnected with partner\ninstitutions, enabling remote model inspection via the NNsight API. A\nstructured pilot study involving 16 researchers from across Europe evaluated\nthe platform's technical performance, usability, and scientific utility. Users\nconducted interventions such as activation patching, causal tracing, and\nrepresentation analysis on models including GPT-2 and DeepSeek-R1-70B. The\nstudy revealed a gradual increase in user engagement, stable platform\nperformance throughout, and a positive reception of the remote experimentation\ncapabilities. It also marked the starting point for building a user community\naround the platform. Identified limitations such as prolonged download\ndurations for activation data as well as intermittent execution interruptions\nare addressed in the roadmap for future development. This initiative marks a\nsignificant step towards widespread accessibility of LLM interpretability\ninfrastructure in Europe and lays the groundwork for broader deployment,\nexpanded tooling, and sustained community collaboration in mechanistic\ninterpretability research."
                },
                "authors": [
                    {
                        "name": "Irma Heithoff. Marc Guggenberger"
                    },
                    {
                        "name": "Sandra Kalogiannis"
                    },
                    {
                        "name": "Susanne Mayer"
                    },
                    {
                        "name": "Fabian Maag"
                    },
                    {
                        "name": "Sigurd Schacht"
                    },
                    {
                        "name": "Carsten Lanquillon"
                    }
                ],
                "author_detail": {
                    "name": "Carsten Lanquillon"
                },
                "author": "Carsten Lanquillon",
                "arxiv_comment": "9 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10553v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10553v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22267v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22267v2",
                "updated": "2025-08-14T11:40:12Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    11,
                    40,
                    12,
                    3,
                    226,
                    0
                ],
                "published": "2025-06-27T14:36:39Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    14,
                    36,
                    39,
                    4,
                    178,
                    0
                ],
                "title": "From Data Center IoT Telemetry to Data Analytics Chatbots -- Virtual\n  Knowledge Graph is All You Need",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Data Center IoT Telemetry to Data Analytics Chatbots -- Virtual\n  Knowledge Graph is All You Need"
                },
                "summary": "Industry 5.0 demands IoT systems that support seamless human-machine\ncollaboration, yet current IoT data analysis requires deep domain, deployment,\nand query expertise. We show that combining Large Language Models (LLMs) with\nKnowledge Graphs (KGs) enables natural language access to heterogeneous IoT\ndata. Focusing on data center IoT telemetry, we introduce a rule-based Virtual\nKnowledge Graph (VKG) construction process and an on-premise LLM inference\nservice to create an end-to-end Data Analytics (DA) chatbot. Our system\ndynamically generates VKGs per query and translates user input into SPARQL,\nachieving 92.5% accuracy (vs. 25% for LLM-to-NoSQL) while reducing latency by\n85% (20.36s to 3.03s) and keeping VKG sizes under 179 MiB. This work\ndemonstrates that VKG-powered LLM interfaces deliver accurate, low-latency, and\nrelationship-aware access to large-scale telemetry, bridging the gap between\nusers and complex IoT systems in Industry 5.0.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Industry 5.0 demands IoT systems that support seamless human-machine\ncollaboration, yet current IoT data analysis requires deep domain, deployment,\nand query expertise. We show that combining Large Language Models (LLMs) with\nKnowledge Graphs (KGs) enables natural language access to heterogeneous IoT\ndata. Focusing on data center IoT telemetry, we introduce a rule-based Virtual\nKnowledge Graph (VKG) construction process and an on-premise LLM inference\nservice to create an end-to-end Data Analytics (DA) chatbot. Our system\ndynamically generates VKGs per query and translates user input into SPARQL,\nachieving 92.5% accuracy (vs. 25% for LLM-to-NoSQL) while reducing latency by\n85% (20.36s to 3.03s) and keeping VKG sizes under 179 MiB. This work\ndemonstrates that VKG-powered LLM interfaces deliver accurate, low-latency, and\nrelationship-aware access to large-scale telemetry, bridging the gap between\nusers and complex IoT systems in Industry 5.0."
                },
                "authors": [
                    {
                        "name": "Junaid Ahmed Khan"
                    },
                    {
                        "name": "Hiari Pizzini Cavagna"
                    },
                    {
                        "name": "Andrea Proia"
                    },
                    {
                        "name": "Andrea Bartolini"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Bartolini"
                },
                "author": "Andrea Bartolini",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22267v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22267v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10539v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10539v1",
                "updated": "2025-08-14T11:22:29Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    11,
                    22,
                    29,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T11:22:29Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    11,
                    22,
                    29,
                    3,
                    226,
                    0
                ],
                "title": "Improving Value-based Process Verifier via Low-Cost Variance Reduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Value-based Process Verifier via Low-Cost Variance Reduction"
                },
                "summary": "Large language models (LLMs) have achieved remarkable success in a wide range\nof tasks. However, their reasoning capabilities, particularly in complex\ndomains like mathematics, remain a significant challenge. Value-based process\nverifiers, which estimate the probability of a partial reasoning chain leading\nto a correct solution, are a promising approach for improving reasoning.\nNevertheless, their effectiveness is often hindered by estimation error in\ntheir training annotations, a consequence of the limited number of Monte Carlo\n(MC) samples feasible due to the high cost of LLM inference. In this paper, we\nidentify that the estimation error primarily arises from high variance rather\nthan bias, and the MC estimator is a Minimum Variance Unbiased Estimator\n(MVUE). To address the problem, we propose the \\textsc{Com}pound \\textsc{M}onte\n\\textsc{C}arlo \\textsc{S}ampling (ComMCS) method, which constructs an unbiased\nestimator by linearly combining the MC estimators from the current and\nsubsequent steps. Theoretically, we show that our method leads to a predictable\nreduction in variance, while maintaining an unbiased estimation without\nadditional LLM inference cost. We also perform empirical experiments on the\nMATH-500 and GSM8K benchmarks to demonstrate the effectiveness of our method.\nNotably, ComMCS outperforms regression-based optimization method by 2.8 points,\nthe non-variance-reduced baseline by 2.2 points on MATH-500 on Best-of-32\nsampling experiment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved remarkable success in a wide range\nof tasks. However, their reasoning capabilities, particularly in complex\ndomains like mathematics, remain a significant challenge. Value-based process\nverifiers, which estimate the probability of a partial reasoning chain leading\nto a correct solution, are a promising approach for improving reasoning.\nNevertheless, their effectiveness is often hindered by estimation error in\ntheir training annotations, a consequence of the limited number of Monte Carlo\n(MC) samples feasible due to the high cost of LLM inference. In this paper, we\nidentify that the estimation error primarily arises from high variance rather\nthan bias, and the MC estimator is a Minimum Variance Unbiased Estimator\n(MVUE). To address the problem, we propose the \\textsc{Com}pound \\textsc{M}onte\n\\textsc{C}arlo \\textsc{S}ampling (ComMCS) method, which constructs an unbiased\nestimator by linearly combining the MC estimators from the current and\nsubsequent steps. Theoretically, we show that our method leads to a predictable\nreduction in variance, while maintaining an unbiased estimation without\nadditional LLM inference cost. We also perform empirical experiments on the\nMATH-500 and GSM8K benchmarks to demonstrate the effectiveness of our method.\nNotably, ComMCS outperforms regression-based optimization method by 2.8 points,\nthe non-variance-reduced baseline by 2.2 points on MATH-500 on Best-of-32\nsampling experiment."
                },
                "authors": [
                    {
                        "name": "Zetian Sun"
                    },
                    {
                        "name": "Dongfang Li"
                    },
                    {
                        "name": "Baotian Hu"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10539v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10539v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10538v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10538v1",
                "updated": "2025-08-14T11:18:32Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    11,
                    18,
                    32,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T11:18:32Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    11,
                    18,
                    32,
                    3,
                    226,
                    0
                ],
                "title": "MLM: Learning Multi-task Loco-Manipulation Whole-Body Control for\n  Quadruped Robot with Arm",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MLM: Learning Multi-task Loco-Manipulation Whole-Body Control for\n  Quadruped Robot with Arm"
                },
                "summary": "Whole-body loco-manipulation for quadruped robots with arm remains a\nchallenging problem, particularly in achieving multi-task control. To address\nthis, we propose MLM, a reinforcement learning framework driven by both\nreal-world and simulation data. It enables a six-DoF robotic arm--equipped\nquadruped robot to perform whole-body loco-manipulation for multiple tasks\nautonomously or under human teleoperation. To address the problem of balancing\nmultiple tasks during the learning of loco-manipulation, we introduce a\ntrajectory library with an adaptive, curriculum-based sampling mechanism. This\napproach allows the policy to efficiently leverage real-world collected\ntrajectories for learning multi-task loco-manipulation. To address deployment\nscenarios with only historical observations and to enhance the performance of\npolicy execution across tasks with different spatial ranges, we propose a\nTrajectory-Velocity Prediction policy network. It predicts unobservable future\ntrajectories and velocities. By leveraging extensive simulation data and\ncurriculum-based rewards, our controller achieves whole-body behaviors in\nsimulation and zero-shot transfer to real-world deployment. Ablation studies in\nsimulation verify the necessity and effectiveness of our approach, while\nreal-world experiments on the Go2 robot with an Airbot robotic arm demonstrate\nthe policy's good performance in multi-task execution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Whole-body loco-manipulation for quadruped robots with arm remains a\nchallenging problem, particularly in achieving multi-task control. To address\nthis, we propose MLM, a reinforcement learning framework driven by both\nreal-world and simulation data. It enables a six-DoF robotic arm--equipped\nquadruped robot to perform whole-body loco-manipulation for multiple tasks\nautonomously or under human teleoperation. To address the problem of balancing\nmultiple tasks during the learning of loco-manipulation, we introduce a\ntrajectory library with an adaptive, curriculum-based sampling mechanism. This\napproach allows the policy to efficiently leverage real-world collected\ntrajectories for learning multi-task loco-manipulation. To address deployment\nscenarios with only historical observations and to enhance the performance of\npolicy execution across tasks with different spatial ranges, we propose a\nTrajectory-Velocity Prediction policy network. It predicts unobservable future\ntrajectories and velocities. By leveraging extensive simulation data and\ncurriculum-based rewards, our controller achieves whole-body behaviors in\nsimulation and zero-shot transfer to real-world deployment. Ablation studies in\nsimulation verify the necessity and effectiveness of our approach, while\nreal-world experiments on the Go2 robot with an Airbot robotic arm demonstrate\nthe policy's good performance in multi-task execution."
                },
                "authors": [
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Bida Ma"
                    },
                    {
                        "name": "Chenkun Qi"
                    },
                    {
                        "name": "Yan Ding"
                    },
                    {
                        "name": "Zhaxizhuoma"
                    },
                    {
                        "name": "Guorong Zhang"
                    },
                    {
                        "name": "Pengan Chen"
                    },
                    {
                        "name": "Kehui Liu"
                    },
                    {
                        "name": "Zhongjie Jia"
                    },
                    {
                        "name": "Chuyue Guan"
                    },
                    {
                        "name": "Yule Mo"
                    },
                    {
                        "name": "Jiaqi Liu"
                    },
                    {
                        "name": "Feng Gao"
                    },
                    {
                        "name": "Jiangwei Zhong"
                    },
                    {
                        "name": "Bin Zhao"
                    },
                    {
                        "name": "Xuelong Li"
                    }
                ],
                "author_detail": {
                    "name": "Xuelong Li"
                },
                "author": "Xuelong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10538v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10538v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01027v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01027v3",
                "updated": "2025-08-14T11:00:13Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    11,
                    0,
                    13,
                    3,
                    226,
                    0
                ],
                "published": "2025-02-03T03:44:35Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    3,
                    44,
                    35,
                    0,
                    34,
                    0
                ],
                "title": "Adversarial Robustness in Two-Stage Learning-to-Defer: Algorithms and\n  Guarantees",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adversarial Robustness in Two-Stage Learning-to-Defer: Algorithms and\n  Guarantees"
                },
                "summary": "Two-stage Learning-to-Defer (L2D) enables optimal task delegation by\nassigning each input to either a fixed main model or one of several offline\nexperts, supporting reliable decision-making in complex, multi-agent\nenvironments. However, existing L2D frameworks assume clean inputs and are\nvulnerable to adversarial perturbations that can manipulate query\nallocation--causing costly misrouting or expert overload. We present the first\ncomprehensive study of adversarial robustness in two-stage L2D systems. We\nintroduce two novel attack strategie--untargeted and targeted--which\nrespectively disrupt optimal allocations or force queries to specific agents.\nTo defend against such threats, we propose SARD, a convex learning algorithm\nbuilt on a family of surrogate losses that are provably Bayes-consistent and\n$(\\mathcal{R}, \\mathcal{G})$-consistent. These guarantees hold across\nclassification, regression, and multi-task settings. Empirical results\ndemonstrate that SARD significantly improves robustness under adversarial\nattacks while maintaining strong clean performance, marking a critical step\ntoward secure and trustworthy L2D deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Two-stage Learning-to-Defer (L2D) enables optimal task delegation by\nassigning each input to either a fixed main model or one of several offline\nexperts, supporting reliable decision-making in complex, multi-agent\nenvironments. However, existing L2D frameworks assume clean inputs and are\nvulnerable to adversarial perturbations that can manipulate query\nallocation--causing costly misrouting or expert overload. We present the first\ncomprehensive study of adversarial robustness in two-stage L2D systems. We\nintroduce two novel attack strategie--untargeted and targeted--which\nrespectively disrupt optimal allocations or force queries to specific agents.\nTo defend against such threats, we propose SARD, a convex learning algorithm\nbuilt on a family of surrogate losses that are provably Bayes-consistent and\n$(\\mathcal{R}, \\mathcal{G})$-consistent. These guarantees hold across\nclassification, regression, and multi-task settings. Empirical results\ndemonstrate that SARD significantly improves robustness under adversarial\nattacks while maintaining strong clean performance, marking a critical step\ntoward secure and trustworthy L2D deployment."
                },
                "authors": [
                    {
                        "name": "Yannis Montreuil"
                    },
                    {
                        "name": "Axel Carlier"
                    },
                    {
                        "name": "Lai Xing Ng"
                    },
                    {
                        "name": "Wei Tsang Ooi"
                    }
                ],
                "author_detail": {
                    "name": "Wei Tsang Ooi"
                },
                "author": "Wei Tsang Ooi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01027v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01027v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.13871v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.13871v2",
                "updated": "2025-08-14T10:59:03Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    10,
                    59,
                    3,
                    3,
                    226,
                    0
                ],
                "published": "2024-02-21T15:23:21Z",
                "published_parsed": [
                    2024,
                    2,
                    21,
                    15,
                    23,
                    21,
                    2,
                    52,
                    0
                ],
                "title": "An Explainable Transformer-based Model for Phishing Email Detection: A\n  Large Language Model Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Explainable Transformer-based Model for Phishing Email Detection: A\n  Large Language Model Approach"
                },
                "summary": "Phishing email is a serious cyber threat that tries to deceive users by\nsending false emails with the intention of stealing confidential information or\ncausing financial harm. Attackers, often posing as trustworthy entities,\nexploit technological advancements and sophistication to make detection and\nprevention of phishing more challenging. Despite extensive academic research,\nphishing detection remains an ongoing and formidable challenge in the\ncybersecurity landscape. Large Language Models (LLMs) and Masked Language\nModels (MLMs) possess immense potential to offer innovative solutions to\naddress long-standing challenges. In this research paper, we present an\noptimized, fine-tuned transformer-based DistilBERT model designed for the\ndetection of phishing emails. In the detection process, we work with a phishing\nemail dataset and utilize the preprocessing techniques to clean and solve the\nimbalance class issues. Through our experiments, we found that our model\neffectively achieves high accuracy, demonstrating its capability to perform\nwell. Finally, we demonstrate our fine-tuned model using Explainable-AI (XAI)\ntechniques such as Local Interpretable Model-Agnostic Explanations (LIME) and\nTransformer Interpret to explain how our model makes predictions in the context\nof text classification for phishing emails.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Phishing email is a serious cyber threat that tries to deceive users by\nsending false emails with the intention of stealing confidential information or\ncausing financial harm. Attackers, often posing as trustworthy entities,\nexploit technological advancements and sophistication to make detection and\nprevention of phishing more challenging. Despite extensive academic research,\nphishing detection remains an ongoing and formidable challenge in the\ncybersecurity landscape. Large Language Models (LLMs) and Masked Language\nModels (MLMs) possess immense potential to offer innovative solutions to\naddress long-standing challenges. In this research paper, we present an\noptimized, fine-tuned transformer-based DistilBERT model designed for the\ndetection of phishing emails. In the detection process, we work with a phishing\nemail dataset and utilize the preprocessing techniques to clean and solve the\nimbalance class issues. Through our experiments, we found that our model\neffectively achieves high accuracy, demonstrating its capability to perform\nwell. Finally, we demonstrate our fine-tuned model using Explainable-AI (XAI)\ntechniques such as Local Interpretable Model-Agnostic Explanations (LIME) and\nTransformer Interpret to explain how our model makes predictions in the context\nof text classification for phishing emails."
                },
                "authors": [
                    {
                        "name": "Mohammad Amaz Uddin"
                    },
                    {
                        "name": "Md Mahiuddin"
                    },
                    {
                        "name": "Iqbal H. Sarker"
                    }
                ],
                "author_detail": {
                    "name": "Iqbal H. Sarker"
                },
                "author": "Iqbal H. Sarker",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.13871v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.13871v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10517v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10517v1",
                "updated": "2025-08-14T10:42:26Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    10,
                    42,
                    26,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T10:42:26Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    10,
                    42,
                    26,
                    3,
                    226,
                    0
                ],
                "title": "Bridging Solidity Evolution Gaps: An LLM-Enhanced Approach for Smart\n  Contract Compilation Error Resolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging Solidity Evolution Gaps: An LLM-Enhanced Approach for Smart\n  Contract Compilation Error Resolution"
                },
                "summary": "Solidity, the dominant smart contract language for Ethereum, has rapidly\nevolved with frequent version updates to enhance security, functionality, and\ndeveloper experience. However, these continual changes introduce significant\nchallenges, particularly in compilation errors, code migration, and\nmaintenance. Therefore, we conduct an empirical study to investigate the\nchallenges in the Solidity version evolution and reveal that 81.68% of examined\ncontracts encounter errors when compiled across different versions, with 86.92%\nof compilation errors.\n  To mitigate these challenges, we conducted a systematic evaluation of large\nlanguage models (LLMs) for resolving Solidity compilation errors during version\nmigrations. Our empirical analysis across both open-source (LLaMA3, DeepSeek)\nand closed-source (GPT-4o, GPT-3.5-turbo) LLMs reveals that although these\nmodels exhibit error repair capabilities, their effectiveness diminishes\nsignificantly for semantic-level issues and shows strong dependency on prompt\nengineering strategies. This underscores the critical need for domain-specific\nadaptation in developing reliable LLM-based repair systems for smart contracts.\n  Building upon these insights, we introduce SMCFIXER, a novel framework that\nsystematically integrates expert knowledge retrieval with LLM-based repair\nmechanisms for Solidity compilation error resolution. The architecture\ncomprises three core phases: (1) context-aware code slicing that extracts\nrelevant error information; (2) expert knowledge retrieval from official\ndocumentation; and (3) iterative patch generation for Solidity migration.\nExperimental validation across Solidity version migrations demonstrates our\napproach's statistically significant 24.24% improvement over baseline GPT-4o on\nreal-world datasets, achieving near-perfect 96.97% accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Solidity, the dominant smart contract language for Ethereum, has rapidly\nevolved with frequent version updates to enhance security, functionality, and\ndeveloper experience. However, these continual changes introduce significant\nchallenges, particularly in compilation errors, code migration, and\nmaintenance. Therefore, we conduct an empirical study to investigate the\nchallenges in the Solidity version evolution and reveal that 81.68% of examined\ncontracts encounter errors when compiled across different versions, with 86.92%\nof compilation errors.\n  To mitigate these challenges, we conducted a systematic evaluation of large\nlanguage models (LLMs) for resolving Solidity compilation errors during version\nmigrations. Our empirical analysis across both open-source (LLaMA3, DeepSeek)\nand closed-source (GPT-4o, GPT-3.5-turbo) LLMs reveals that although these\nmodels exhibit error repair capabilities, their effectiveness diminishes\nsignificantly for semantic-level issues and shows strong dependency on prompt\nengineering strategies. This underscores the critical need for domain-specific\nadaptation in developing reliable LLM-based repair systems for smart contracts.\n  Building upon these insights, we introduce SMCFIXER, a novel framework that\nsystematically integrates expert knowledge retrieval with LLM-based repair\nmechanisms for Solidity compilation error resolution. The architecture\ncomprises three core phases: (1) context-aware code slicing that extracts\nrelevant error information; (2) expert knowledge retrieval from official\ndocumentation; and (3) iterative patch generation for Solidity migration.\nExperimental validation across Solidity version migrations demonstrates our\napproach's statistically significant 24.24% improvement over baseline GPT-4o on\nreal-world datasets, achieving near-perfect 96.97% accuracy."
                },
                "authors": [
                    {
                        "name": "Likai Ye"
                    },
                    {
                        "name": "Mengliang Li"
                    },
                    {
                        "name": "Dehai Zhao"
                    },
                    {
                        "name": "Jiamou Sun"
                    },
                    {
                        "name": "Xiaoxue Ren"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoxue Ren"
                },
                "author": "Xiaoxue Ren",
                "arxiv_comment": "International Conference on Software Maintenance and Evolution\n  (ICSME) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10517v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10517v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01454v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01454v2",
                "updated": "2025-08-14T10:23:18Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    10,
                    23,
                    18,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-02T18:06:38Z",
                "published_parsed": [
                    2025,
                    8,
                    2,
                    18,
                    6,
                    38,
                    5,
                    214,
                    0
                ],
                "title": "Error dependencies in the space-based CNEOS fireball database",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Error dependencies in the space-based CNEOS fireball database"
                },
                "summary": "We evaluate the reliability of CNEOS-derived ephemerides of fireball events\ngiven the absence of the underlying data. We analyzed 18 events that have both\n(i) sufficient satellite information to derive orbits and (ii) ground-based\nobservational counterparts. We quantify the uncertainties on these calibrated\nevents using the orbital similarity criterion $D_D$. We also examine the\nvelocity components imbalance and identify discriminants that can indicate the\naccuracy of an event. We identify two groups in the CNEOS database. CNEOS data\nproduces ephemeris determinations with $D_D$<0.1 for fireballs reported either\n(i) after late 2017 or (ii) with impact energies above 0.45 kt with 74-78% of\nevents having $D_D$=0.03$\\pm$0.02, while ~11% show $D_D$<0.008. Our statistical\ntest confirms these two parameters as the only reliable discriminants that,\nwhen combined, explain the two accuracy groups. Daylight, z-velocity component,\nlow altitude, long duration, and latitude might also indicate errors, although\nthe limited dataset may obscure correlations. No clear discriminants are\nidentified for more restrictive $D_D$ cut-offs. We provide estimates of orbital\nuncertainties for calibrated events. The hyperbolic fireball subset in the\nCNEOS database appears as an outlier in the velocity imbalance test. Our\nresults confirm that the fidelity of CNEOS fireball data improved significantly\nfrom 2018, likely due to the deployment of next-generation space sensors, and\nshow a growing number of high-velocity events. Hyperbolic candidates should be\ninterpreted with caution, as their velocities and inclinations likely reflect\nmeasurement errors. Accuracy constraints remain limited by the dataset size, as\nevidenced by the lack of statistically significant dependence on duration,\npreventing strong conclusions from being drawn.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We evaluate the reliability of CNEOS-derived ephemerides of fireball events\ngiven the absence of the underlying data. We analyzed 18 events that have both\n(i) sufficient satellite information to derive orbits and (ii) ground-based\nobservational counterparts. We quantify the uncertainties on these calibrated\nevents using the orbital similarity criterion $D_D$. We also examine the\nvelocity components imbalance and identify discriminants that can indicate the\naccuracy of an event. We identify two groups in the CNEOS database. CNEOS data\nproduces ephemeris determinations with $D_D$<0.1 for fireballs reported either\n(i) after late 2017 or (ii) with impact energies above 0.45 kt with 74-78% of\nevents having $D_D$=0.03$\\pm$0.02, while ~11% show $D_D$<0.008. Our statistical\ntest confirms these two parameters as the only reliable discriminants that,\nwhen combined, explain the two accuracy groups. Daylight, z-velocity component,\nlow altitude, long duration, and latitude might also indicate errors, although\nthe limited dataset may obscure correlations. No clear discriminants are\nidentified for more restrictive $D_D$ cut-offs. We provide estimates of orbital\nuncertainties for calibrated events. The hyperbolic fireball subset in the\nCNEOS database appears as an outlier in the velocity imbalance test. Our\nresults confirm that the fidelity of CNEOS fireball data improved significantly\nfrom 2018, likely due to the deployment of next-generation space sensors, and\nshow a growing number of high-velocity events. Hyperbolic candidates should be\ninterpreted with caution, as their velocities and inclinations likely reflect\nmeasurement errors. Accuracy constraints remain limited by the dataset size, as\nevidenced by the lack of statistically significant dependence on duration,\npreventing strong conclusions from being drawn."
                },
                "authors": [
                    {
                        "name": "Eloy Pea-Asensio"
                    },
                    {
                        "name": "Hector Socas-Navarro"
                    },
                    {
                        "name": "Darryl Z. Seligman"
                    }
                ],
                "author_detail": {
                    "name": "Darryl Z. Seligman"
                },
                "author": "Darryl Z. Seligman",
                "arxiv_doi": "10.1051/0004-6361/202554224",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1051/0004-6361/202554224",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.01454v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01454v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted in Astronomy & Astrophysics",
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10494v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10494v1",
                "updated": "2025-08-14T09:52:51Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    9,
                    52,
                    51,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T09:52:51Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    9,
                    52,
                    51,
                    3,
                    226,
                    0
                ],
                "title": "A Unified Multi-Agent Framework for Universal Multimodal Understanding\n  and Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Unified Multi-Agent Framework for Universal Multimodal Understanding\n  and Generation"
                },
                "summary": "Real-world multimodal applications often require any-to-any capabilities,\nenabling both understanding and generation across modalities including text,\nimage, audio, and video. However, integrating the strengths of autoregressive\nlanguage models (LLMs) for reasoning and diffusion models for high-fidelity\ngeneration remains challenging. Existing approaches rely on rigid pipelines or\ntightly coupled architectures, limiting flexibility and scalability. We propose\nMAGUS (Multi-Agent Guided Unified Multimodal System), a modular framework that\nunifies multimodal understanding and generation via two decoupled phases:\nCognition and Deliberation. MAGUS enables symbolic multi-agent collaboration\nwithin a shared textual workspace. In the Cognition phase, three\nrole-conditioned multimodal LLM agents - Perceiver, Planner, and Reflector -\nengage in collaborative dialogue to perform structured understanding and\nplanning. The Deliberation phase incorporates a Growth-Aware Search mechanism\nthat orchestrates LLM-based reasoning and diffusion-based generation in a\nmutually reinforcing manner. MAGUS supports plug-and-play extensibility,\nscalable any-to-any modality conversion, and semantic alignment - all without\nthe need for joint training. Experiments across multiple benchmarks, including\nimage, video, and audio generation, as well as cross-modal instruction\nfollowing, demonstrate that MAGUS outperforms strong baselines and\nstate-of-the-art systems. Notably, on the MME benchmark, MAGUS surpasses the\npowerful closed-source model GPT-4o.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-world multimodal applications often require any-to-any capabilities,\nenabling both understanding and generation across modalities including text,\nimage, audio, and video. However, integrating the strengths of autoregressive\nlanguage models (LLMs) for reasoning and diffusion models for high-fidelity\ngeneration remains challenging. Existing approaches rely on rigid pipelines or\ntightly coupled architectures, limiting flexibility and scalability. We propose\nMAGUS (Multi-Agent Guided Unified Multimodal System), a modular framework that\nunifies multimodal understanding and generation via two decoupled phases:\nCognition and Deliberation. MAGUS enables symbolic multi-agent collaboration\nwithin a shared textual workspace. In the Cognition phase, three\nrole-conditioned multimodal LLM agents - Perceiver, Planner, and Reflector -\nengage in collaborative dialogue to perform structured understanding and\nplanning. The Deliberation phase incorporates a Growth-Aware Search mechanism\nthat orchestrates LLM-based reasoning and diffusion-based generation in a\nmutually reinforcing manner. MAGUS supports plug-and-play extensibility,\nscalable any-to-any modality conversion, and semantic alignment - all without\nthe need for joint training. Experiments across multiple benchmarks, including\nimage, video, and audio generation, as well as cross-modal instruction\nfollowing, demonstrate that MAGUS outperforms strong baselines and\nstate-of-the-art systems. Notably, on the MME benchmark, MAGUS surpasses the\npowerful closed-source model GPT-4o."
                },
                "authors": [
                    {
                        "name": "Jiulin Li"
                    },
                    {
                        "name": "Ping Huang"
                    },
                    {
                        "name": "Yexin Li"
                    },
                    {
                        "name": "Shuo Chen"
                    },
                    {
                        "name": "Juewen Hu"
                    },
                    {
                        "name": "Ye Tian"
                    }
                ],
                "author_detail": {
                    "name": "Ye Tian"
                },
                "author": "Ye Tian",
                "arxiv_comment": "8 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10494v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10494v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10493v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10493v1",
                "updated": "2025-08-14T09:52:15Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    9,
                    52,
                    15,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T09:52:15Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    9,
                    52,
                    15,
                    3,
                    226,
                    0
                ],
                "title": "AlDBaran: Towards Blazingly Fast State Commitments for Blockchains",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AlDBaran: Towards Blazingly Fast State Commitments for Blockchains"
                },
                "summary": "The fundamental basis for maintaining integrity within contemporary\nblockchain systems is provided by authenticated databases. Our analysis\nindicates that a significant portion of the approaches applied in this domain\nfail to sufficiently meet the stringent requirements of systems processing\ntransactions at rates of multi-million TPS. AlDBaran signifies a substantial\nadvancement in authenticated databases. By eliminating disk I/O operations from\nthe critical path, implementing prefetching strategies, and refining the update\nmechanism of the Merkle tree, we have engineered an authenticated data\nstructure capable of handling state updates efficiently at a network throughput\nof 50 Gbps. This throughput capacity significantly surpasses any empirically\ndocumented blockchain throughput, guaranteeing the ability of even the most\nhigh-throughput blockchains to generate state commitments effectively.\n  AlDBaran provides support for historical state proofs, which facilitates a\nwide array of novel applications. For instance, the deployment of AlDBaran\ncould enable blockchains that do not currently support state commitments to\noffer functionalities for light clients and/or implement rollups.\n  When benchmarked against alternative authenticated data structure projects,\nAlDBaran exhibits superior performance and simplicity. In particular, AlDBaran\nachieves speeds of approximately 48 million updates per second using an\nidentical machine configuration. This characteristic renders AlDBaran an\nattractive solution for resource-limited environments, as its historical data\ncapabilities can be modularly isolated (and deactivated), which further\nenhances performance. On consumer-level portable hardware, it achieves\napproximately 8 million updates/s in an in-memory setting and 5 million\nupdates/s with snapshots at sub-second intervals, illustrating compelling and\ncost-effective scalability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The fundamental basis for maintaining integrity within contemporary\nblockchain systems is provided by authenticated databases. Our analysis\nindicates that a significant portion of the approaches applied in this domain\nfail to sufficiently meet the stringent requirements of systems processing\ntransactions at rates of multi-million TPS. AlDBaran signifies a substantial\nadvancement in authenticated databases. By eliminating disk I/O operations from\nthe critical path, implementing prefetching strategies, and refining the update\nmechanism of the Merkle tree, we have engineered an authenticated data\nstructure capable of handling state updates efficiently at a network throughput\nof 50 Gbps. This throughput capacity significantly surpasses any empirically\ndocumented blockchain throughput, guaranteeing the ability of even the most\nhigh-throughput blockchains to generate state commitments effectively.\n  AlDBaran provides support for historical state proofs, which facilitates a\nwide array of novel applications. For instance, the deployment of AlDBaran\ncould enable blockchains that do not currently support state commitments to\noffer functionalities for light clients and/or implement rollups.\n  When benchmarked against alternative authenticated data structure projects,\nAlDBaran exhibits superior performance and simplicity. In particular, AlDBaran\nachieves speeds of approximately 48 million updates per second using an\nidentical machine configuration. This characteristic renders AlDBaran an\nattractive solution for resource-limited environments, as its historical data\ncapabilities can be modularly isolated (and deactivated), which further\nenhances performance. On consumer-level portable hardware, it achieves\napproximately 8 million updates/s in an in-memory setting and 5 million\nupdates/s with snapshots at sub-second intervals, illustrating compelling and\ncost-effective scalability."
                },
                "authors": [
                    {
                        "name": "Bernhard Kauer"
                    },
                    {
                        "name": "Aleksandr Petrosyan"
                    },
                    {
                        "name": "Benjamin Livshits"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin Livshits"
                },
                "author": "Benjamin Livshits",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10493v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10493v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10492v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10492v1",
                "updated": "2025-08-14T09:51:20Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    9,
                    51,
                    20,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T09:51:20Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    9,
                    51,
                    20,
                    3,
                    226,
                    0
                ],
                "title": "Reverse Physician-AI Relationship: Full-process Clinical Diagnosis\n  Driven by a Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reverse Physician-AI Relationship: Full-process Clinical Diagnosis\n  Driven by a Large Language Model"
                },
                "summary": "Full-process clinical diagnosis in the real world encompasses the entire\ndiagnostic workflow that begins with only an ambiguous chief complaint. While\nartificial intelligence (AI), particularly large language models (LLMs), is\ntransforming clinical diagnosis, its role remains largely as an assistant to\nphysicians. This AI-assisted working pattern makes AI can only answer specific\nmedical questions at certain parts within the diagnostic process, but lack the\nability to drive the entire diagnostic process starting from an ambiguous\ncomplaint, which still relies heavily on human physicians. This gap limits AI's\nability to fully reduce physicians' workload and enhance diagnostic efficiency.\nTo address this, we propose a paradigm shift that reverses the relationship\nbetween physicians and AI: repositioning AI as the primary director, with\nphysicians serving as its assistants. So we present DxDirector-7B, an LLM\nendowed with advanced deep thinking capabilities, enabling it to drive the\nfull-process diagnosis with minimal physician involvement. Furthermore,\nDxDirector-7B establishes a robust accountability framework for misdiagnoses,\ndelineating responsibility between AI and human physicians. In evaluations\nacross rare, complex, and real-world cases under full-process diagnosis\nsetting, DxDirector-7B not only achieves significant superior diagnostic\naccuracy but also substantially reduces physician workload than\nstate-of-the-art medical LLMs as well as general-purpose LLMs. Fine-grained\nanalyses across multiple clinical departments and tasks validate its efficacy,\nwith expert evaluations indicating its potential to serve as a viable\nsubstitute for medical specialists. These findings mark a new era where AI,\ntraditionally a physicians' assistant, now drives the entire diagnostic process\nto drastically reduce physicians' workload, indicating an efficient and\naccurate diagnostic solution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Full-process clinical diagnosis in the real world encompasses the entire\ndiagnostic workflow that begins with only an ambiguous chief complaint. While\nartificial intelligence (AI), particularly large language models (LLMs), is\ntransforming clinical diagnosis, its role remains largely as an assistant to\nphysicians. This AI-assisted working pattern makes AI can only answer specific\nmedical questions at certain parts within the diagnostic process, but lack the\nability to drive the entire diagnostic process starting from an ambiguous\ncomplaint, which still relies heavily on human physicians. This gap limits AI's\nability to fully reduce physicians' workload and enhance diagnostic efficiency.\nTo address this, we propose a paradigm shift that reverses the relationship\nbetween physicians and AI: repositioning AI as the primary director, with\nphysicians serving as its assistants. So we present DxDirector-7B, an LLM\nendowed with advanced deep thinking capabilities, enabling it to drive the\nfull-process diagnosis with minimal physician involvement. Furthermore,\nDxDirector-7B establishes a robust accountability framework for misdiagnoses,\ndelineating responsibility between AI and human physicians. In evaluations\nacross rare, complex, and real-world cases under full-process diagnosis\nsetting, DxDirector-7B not only achieves significant superior diagnostic\naccuracy but also substantially reduces physician workload than\nstate-of-the-art medical LLMs as well as general-purpose LLMs. Fine-grained\nanalyses across multiple clinical departments and tasks validate its efficacy,\nwith expert evaluations indicating its potential to serve as a viable\nsubstitute for medical specialists. These findings mark a new era where AI,\ntraditionally a physicians' assistant, now drives the entire diagnostic process\nto drastically reduce physicians' workload, indicating an efficient and\naccurate diagnostic solution."
                },
                "authors": [
                    {
                        "name": "Shicheng Xu"
                    },
                    {
                        "name": "Xin Huang"
                    },
                    {
                        "name": "Zihao Wei"
                    },
                    {
                        "name": "Liang Pang"
                    },
                    {
                        "name": "Huawei Shen"
                    },
                    {
                        "name": "Xueqi Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Xueqi Cheng"
                },
                "author": "Xueqi Cheng",
                "arxiv_comment": "39 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10492v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10492v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18236v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18236v3",
                "updated": "2025-08-14T09:48:31Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    9,
                    48,
                    31,
                    3,
                    226,
                    0
                ],
                "published": "2025-04-25T10:34:00Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    10,
                    34,
                    0,
                    4,
                    115,
                    0
                ],
                "title": "Two Means to an End Goal: Connecting Explainability and Contestability\n  in the Regulation of Public Sector AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Two Means to an End Goal: Connecting Explainability and Contestability\n  in the Regulation of Public Sector AI"
                },
                "summary": "Explainability and its emerging counterpart contestability have become\nimportant normative and design principles for trustworthy AI as they enable\nusers and subjects to understand and challenge AI decisions. However, realizing\nthese principles is difficult, as they assume different meanings in technical,\nlegal, and organizational dimensions of AI regulation. To resolve this\nconceptual polysemy, in this paper, we present the findings of an interview\nstudy with 14 experts to examine the intersection and implementation of\nexplainability and contestability, and their understanding in different\nresearch communities. We outline differentiations between descriptive and\nnormative explainability, judicial and non-judicial channels of contestation,\nand individual and collective contestation action. We further describe the main\npoints of friction in the realization of both principles, including the\nalignment between top-down and bottom-up regulation, the assignment of\nresponsibility, and the need for interdisciplinary collaboration. Lastly, we\nformulate three recommendations for AI policy to implement both principles\nthrough a Regulation by Design perspective. We believe our contributions can\ninform policy-making and regulation of these core principles and enable more\neffective and equitable design, development, and deployment of trustworthy\npublic AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explainability and its emerging counterpart contestability have become\nimportant normative and design principles for trustworthy AI as they enable\nusers and subjects to understand and challenge AI decisions. However, realizing\nthese principles is difficult, as they assume different meanings in technical,\nlegal, and organizational dimensions of AI regulation. To resolve this\nconceptual polysemy, in this paper, we present the findings of an interview\nstudy with 14 experts to examine the intersection and implementation of\nexplainability and contestability, and their understanding in different\nresearch communities. We outline differentiations between descriptive and\nnormative explainability, judicial and non-judicial channels of contestation,\nand individual and collective contestation action. We further describe the main\npoints of friction in the realization of both principles, including the\nalignment between top-down and bottom-up regulation, the assignment of\nresponsibility, and the need for interdisciplinary collaboration. Lastly, we\nformulate three recommendations for AI policy to implement both principles\nthrough a Regulation by Design perspective. We believe our contributions can\ninform policy-making and regulation of these core principles and enable more\neffective and equitable design, development, and deployment of trustworthy\npublic AI systems."
                },
                "authors": [
                    {
                        "name": "Timothe Schmude"
                    },
                    {
                        "name": "Mireia Yurrita"
                    },
                    {
                        "name": "Kars Alfrink"
                    },
                    {
                        "name": "Thomas Le Goff"
                    },
                    {
                        "name": "Sebastian Tschiatschek"
                    },
                    {
                        "name": "Tiphaine Viard"
                    }
                ],
                "author_detail": {
                    "name": "Tiphaine Viard"
                },
                "author": "Tiphaine Viard",
                "arxiv_comment": "19 pages main text, 4 figures. Supplementary material is provided",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18236v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18236v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10486v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10486v1",
                "updated": "2025-08-14T09:41:55Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    9,
                    41,
                    55,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T09:41:55Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    9,
                    41,
                    55,
                    3,
                    226,
                    0
                ],
                "title": "SEQ-GPT: LLM-assisted Spatial Query via Example",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SEQ-GPT: LLM-assisted Spatial Query via Example"
                },
                "summary": "Contemporary spatial services such as online maps predominantly rely on user\nqueries for location searches. However, the user experience is limited when\nperforming complex tasks, such as searching for a group of locations\nsimultaneously. In this study, we examine the extended scenario known as\nSpatial Exemplar Query (SEQ), where multiple relevant locations are jointly\nsearched based on user-specified examples. We introduce SEQ-GPT, a spatial\nquery system powered by Large Language Models (LLMs) towards more versatile SEQ\nsearch using natural language. The language capabilities of LLMs enable unique\ninteractive operations in the SEQ process, including asking users to clarify\nquery details and dynamically adjusting the search based on user feedback. We\nalso propose a tailored LLM adaptation pipeline that aligns natural language\nwith structured spatial data and queries through dialogue synthesis and\nmulti-model cooperation. SEQ-GPT offers an end-to-end demonstration for\nbroadening spatial search with realistic data and application scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contemporary spatial services such as online maps predominantly rely on user\nqueries for location searches. However, the user experience is limited when\nperforming complex tasks, such as searching for a group of locations\nsimultaneously. In this study, we examine the extended scenario known as\nSpatial Exemplar Query (SEQ), where multiple relevant locations are jointly\nsearched based on user-specified examples. We introduce SEQ-GPT, a spatial\nquery system powered by Large Language Models (LLMs) towards more versatile SEQ\nsearch using natural language. The language capabilities of LLMs enable unique\ninteractive operations in the SEQ process, including asking users to clarify\nquery details and dynamically adjusting the search based on user feedback. We\nalso propose a tailored LLM adaptation pipeline that aligns natural language\nwith structured spatial data and queries through dialogue synthesis and\nmulti-model cooperation. SEQ-GPT offers an end-to-end demonstration for\nbroadening spatial search with realistic data and application scenarios."
                },
                "authors": [
                    {
                        "name": "Ivan Khai Ze Lim"
                    },
                    {
                        "name": "Ningyi Liao"
                    },
                    {
                        "name": "Yiming Yang"
                    },
                    {
                        "name": "Gerald Wei Yong Yip"
                    },
                    {
                        "name": "Siqiang Luo"
                    }
                ],
                "author_detail": {
                    "name": "Siqiang Luo"
                },
                "author": "Siqiang Luo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10486v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10486v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10478v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10478v1",
                "updated": "2025-08-14T09:28:49Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    9,
                    28,
                    49,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T09:28:49Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    9,
                    28,
                    49,
                    3,
                    226,
                    0
                ],
                "title": "Semantic IDs for Joint Generative Search and Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic IDs for Joint Generative Search and Recommendation"
                },
                "summary": "Generative models powered by Large Language Models (LLMs) are emerging as a\nunified solution for powering both recommendation and search tasks. A key\ndesign choice in these models is how to represent items, traditionally through\nunique identifiers (IDs) and more recently with Semantic IDs composed of\ndiscrete codes, obtained from embeddings. While task-specific embedding models\ncan improve performance for individual tasks, they may not generalize well in a\njoint setting. In this paper, we explore how to construct Semantic IDs that\nperform well both in search and recommendation when using a unified model. We\ncompare a range of strategies to construct Semantic IDs, looking into\ntask-specific and cross-tasks approaches, and also whether each task should\nhave its own semantic ID tokens in a joint search and recommendation generative\nmodel. Our results show that using a bi-encoder model fine-tuned on both search\nand recommendation tasks to obtain item embeddings, followed by the\nconstruction of a unified Semantic ID space provides an effective trade-off,\nenabling strong performance in both tasks. We hope these findings spark\nfollow-up work on generalisable, semantically grounded ID schemes and inform\nthe next wave of unified generative recommender architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative models powered by Large Language Models (LLMs) are emerging as a\nunified solution for powering both recommendation and search tasks. A key\ndesign choice in these models is how to represent items, traditionally through\nunique identifiers (IDs) and more recently with Semantic IDs composed of\ndiscrete codes, obtained from embeddings. While task-specific embedding models\ncan improve performance for individual tasks, they may not generalize well in a\njoint setting. In this paper, we explore how to construct Semantic IDs that\nperform well both in search and recommendation when using a unified model. We\ncompare a range of strategies to construct Semantic IDs, looking into\ntask-specific and cross-tasks approaches, and also whether each task should\nhave its own semantic ID tokens in a joint search and recommendation generative\nmodel. Our results show that using a bi-encoder model fine-tuned on both search\nand recommendation tasks to obtain item embeddings, followed by the\nconstruction of a unified Semantic ID space provides an effective trade-off,\nenabling strong performance in both tasks. We hope these findings spark\nfollow-up work on generalisable, semantically grounded ID schemes and inform\nthe next wave of unified generative recommender architectures."
                },
                "authors": [
                    {
                        "name": "Gustavo Penha"
                    },
                    {
                        "name": "Edoardo D'Amico"
                    },
                    {
                        "name": "Marco De Nadai"
                    },
                    {
                        "name": "Enrico Palumbo"
                    },
                    {
                        "name": "Alexandre Tamborrino"
                    },
                    {
                        "name": "Ali Vardasbi"
                    },
                    {
                        "name": "Max Lefarov"
                    },
                    {
                        "name": "Shawn Lin"
                    },
                    {
                        "name": "Timothy Heath"
                    },
                    {
                        "name": "Francesco Fabbri"
                    },
                    {
                        "name": "Hugues Bouchard"
                    }
                ],
                "author_detail": {
                    "name": "Hugues Bouchard"
                },
                "author": "Hugues Bouchard",
                "arxiv_comment": "Accepted for publication in the 19th ACM Conference on Recommender\n  Systems (RecSys 2025), Late-Breaking Results track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10478v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10478v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10474v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10474v1",
                "updated": "2025-08-14T09:23:25Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    9,
                    23,
                    25,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T09:23:25Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    9,
                    23,
                    25,
                    3,
                    226,
                    0
                ],
                "title": "EDAPT: Towards Calibration-Free BCIs with Continual Online Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EDAPT: Towards Calibration-Free BCIs with Continual Online Adaptation"
                },
                "summary": "Brain-computer interfaces (BCIs) suffer from accuracy degradation as neural\nsignals drift over time and vary across users, requiring frequent recalibration\nthat limits practical deployment. We introduce EDAPT, a task- and\nmodel-agnostic framework that eliminates calibration through continual model\nadaptation. EDAPT first trains a baseline decoder using data from multiple\nusers, then continually personalizes this model via supervised finetuning as\nthe neural patterns evolve during use. We tested EDAPT across nine datasets\ncovering three BCI tasks, and found that it consistently improved accuracy over\nconventional, static methods. These improvements primarily stem from combining\npopulation-level pretraining and online continual finetuning, with unsupervised\ndomain adaptation providing further gains on some datasets. EDAPT runs\nefficiently, updating models within 200 milliseconds on consumer-grade\nhardware. Finally, decoding accuracy scales with total data budget rather than\nits allocation between subjects and trials. EDAPT provides a practical pathway\ntoward calibration-free BCIs, reducing a major barrier to BCI deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Brain-computer interfaces (BCIs) suffer from accuracy degradation as neural\nsignals drift over time and vary across users, requiring frequent recalibration\nthat limits practical deployment. We introduce EDAPT, a task- and\nmodel-agnostic framework that eliminates calibration through continual model\nadaptation. EDAPT first trains a baseline decoder using data from multiple\nusers, then continually personalizes this model via supervised finetuning as\nthe neural patterns evolve during use. We tested EDAPT across nine datasets\ncovering three BCI tasks, and found that it consistently improved accuracy over\nconventional, static methods. These improvements primarily stem from combining\npopulation-level pretraining and online continual finetuning, with unsupervised\ndomain adaptation providing further gains on some datasets. EDAPT runs\nefficiently, updating models within 200 milliseconds on consumer-grade\nhardware. Finally, decoding accuracy scales with total data budget rather than\nits allocation between subjects and trials. EDAPT provides a practical pathway\ntoward calibration-free BCIs, reducing a major barrier to BCI deployment."
                },
                "authors": [
                    {
                        "name": "Lisa Haxel"
                    },
                    {
                        "name": "Jaivardhan Kapoor"
                    },
                    {
                        "name": "Ulf Ziemann"
                    },
                    {
                        "name": "Jakob H. Macke"
                    }
                ],
                "author_detail": {
                    "name": "Jakob H. Macke"
                },
                "author": "Jakob H. Macke",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10474v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10474v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.09959v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.09959v5",
                "updated": "2025-08-14T09:16:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    9,
                    16,
                    33,
                    3,
                    226,
                    0
                ],
                "published": "2023-08-19T09:27:46Z",
                "published_parsed": [
                    2023,
                    8,
                    19,
                    9,
                    27,
                    46,
                    5,
                    231,
                    0
                ],
                "title": "Hummingbird: Fast, Flexible, and Fair Inter-Domain Bandwidth\n  Reservations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hummingbird: Fast, Flexible, and Fair Inter-Domain Bandwidth\n  Reservations"
                },
                "summary": "To realize the long-standing vision of providing quality-of-service (QoS)\nguarantees on a public Internet, this paper introduces Hummingbird: a\nlightweight QoS-system that provides fine-grained inter-domain reservations for\nend hosts.\n  Hummingbird enables flexible and composable reservations with end-to-end\nguarantees, and addresses an often overlooked, but crucial, aspect of\nbandwidth-reservation systems: incentivization of network providers.\nHummingbird represents bandwidth reservations as tradable assets, allowing\nmarkets to emerge. These markets then ensure fair and efficient resource\nallocation and encourage deployment by remunerating providers. This\nincentivization is facilitated by decoupling reservations from network\nidentities, which enables novel control-plane mechanisms and allows the design\nof a control plane based on smart contracts.\n  Hummingbird also provides an efficient reservation data plane, which\nstreamlines the processing on routers and thus simplifies the implementation,\ndeployment, and traffic policing, while maintaining robust security properties.\nOur prototype implementation demonstrates the efficiency and scalability of\nHummingbird's asset-based control plane, and our high-speed software\nimplementation can fill a 160 Gbps link with Hummingbird packets on commodity\nhardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To realize the long-standing vision of providing quality-of-service (QoS)\nguarantees on a public Internet, this paper introduces Hummingbird: a\nlightweight QoS-system that provides fine-grained inter-domain reservations for\nend hosts.\n  Hummingbird enables flexible and composable reservations with end-to-end\nguarantees, and addresses an often overlooked, but crucial, aspect of\nbandwidth-reservation systems: incentivization of network providers.\nHummingbird represents bandwidth reservations as tradable assets, allowing\nmarkets to emerge. These markets then ensure fair and efficient resource\nallocation and encourage deployment by remunerating providers. This\nincentivization is facilitated by decoupling reservations from network\nidentities, which enables novel control-plane mechanisms and allows the design\nof a control plane based on smart contracts.\n  Hummingbird also provides an efficient reservation data plane, which\nstreamlines the processing on routers and thus simplifies the implementation,\ndeployment, and traffic policing, while maintaining robust security properties.\nOur prototype implementation demonstrates the efficiency and scalability of\nHummingbird's asset-based control plane, and our high-speed software\nimplementation can fill a 160 Gbps link with Hummingbird packets on commodity\nhardware."
                },
                "authors": [
                    {
                        "name": "Karl Wst"
                    },
                    {
                        "name": "Giacomo Giuliari"
                    },
                    {
                        "name": "Markus Legner"
                    },
                    {
                        "name": "Jean-Pierre Smith"
                    },
                    {
                        "name": "Marc Wyss"
                    },
                    {
                        "name": "Jules Bachmann"
                    },
                    {
                        "name": "Juan A. Garcia-Pardo"
                    },
                    {
                        "name": "Adrian Perrig"
                    }
                ],
                "author_detail": {
                    "name": "Adrian Perrig"
                },
                "author": "Adrian Perrig",
                "arxiv_doi": "10.1145/3718958.3750495",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3718958.3750495",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2308.09959v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.09959v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "20 pages, 15 figures",
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.2.1; C.2.2; C.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10467v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10467v1",
                "updated": "2025-08-14T09:08:50Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    9,
                    8,
                    50,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T09:08:50Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    9,
                    8,
                    50,
                    3,
                    226,
                    0
                ],
                "title": "FIRESPARQL: A LLM-based Framework for SPARQL Query Generation over\n  Scholarly Knowledge Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FIRESPARQL: A LLM-based Framework for SPARQL Query Generation over\n  Scholarly Knowledge Graphs"
                },
                "summary": "Question answering over Scholarly Knowledge Graphs (SKGs) remains a\nchallenging task due to the complexity of scholarly content and the intricate\nstructure of these graphs. Large Language Model (LLM) approaches could be used\nto translate natural language questions (NLQs) into SPARQL queries; however,\nthese LLM-based approaches struggle with SPARQL query generation due to limited\nexposure to SKG-specific content and the underlying schema. We identified two\nmain types of errors in the LLM-generated SPARQL queries: (i) structural\ninconsistencies, such as missing or redundant triples in the queries, and (ii)\nsemantic inaccuracies, where incorrect entities or properties are shown in the\nqueries despite a correct query structure. To address these issues, we propose\nFIRESPARQL, a modular framework that supports fine-tuned LLMs as a core\ncomponent, with optional context provided via retrieval-augmented generation\n(RAG) and a SPARQL query correction layer. We evaluate the framework on the\nSciQA Benchmark using various configurations (zero-shot, zero-shot with RAG,\none-shot, fine-tuning, and fine-tuning with RAG) and compare the performance\nwith baseline and state-of-the-art approaches. We measure query accuracy using\nBLEU and ROUGE metrics, and query result accuracy using relaxed exact\nmatch(RelaxedEM), with respect to the gold standards containing the NLQs,\nSPARQL queries, and the results of the queries. Experimental results\ndemonstrate that fine-tuning achieves the highest overall performance, reaching\n0.90 ROUGE-L for query accuracy and 0.85 RelaxedEM for result accuracy on the\ntest set.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Question answering over Scholarly Knowledge Graphs (SKGs) remains a\nchallenging task due to the complexity of scholarly content and the intricate\nstructure of these graphs. Large Language Model (LLM) approaches could be used\nto translate natural language questions (NLQs) into SPARQL queries; however,\nthese LLM-based approaches struggle with SPARQL query generation due to limited\nexposure to SKG-specific content and the underlying schema. We identified two\nmain types of errors in the LLM-generated SPARQL queries: (i) structural\ninconsistencies, such as missing or redundant triples in the queries, and (ii)\nsemantic inaccuracies, where incorrect entities or properties are shown in the\nqueries despite a correct query structure. To address these issues, we propose\nFIRESPARQL, a modular framework that supports fine-tuned LLMs as a core\ncomponent, with optional context provided via retrieval-augmented generation\n(RAG) and a SPARQL query correction layer. We evaluate the framework on the\nSciQA Benchmark using various configurations (zero-shot, zero-shot with RAG,\none-shot, fine-tuning, and fine-tuning with RAG) and compare the performance\nwith baseline and state-of-the-art approaches. We measure query accuracy using\nBLEU and ROUGE metrics, and query result accuracy using relaxed exact\nmatch(RelaxedEM), with respect to the gold standards containing the NLQs,\nSPARQL queries, and the results of the queries. Experimental results\ndemonstrate that fine-tuning achieves the highest overall performance, reaching\n0.90 ROUGE-L for query accuracy and 0.85 RelaxedEM for result accuracy on the\ntest set."
                },
                "authors": [
                    {
                        "name": "Xueli Pan"
                    },
                    {
                        "name": "Victor de Boer"
                    },
                    {
                        "name": "Jacco van Ossenbruggen"
                    }
                ],
                "author_detail": {
                    "name": "Jacco van Ossenbruggen"
                },
                "author": "Jacco van Ossenbruggen",
                "arxiv_comment": "Accepted at 17th International Joint Conference on Knowledge\n  Discovery, Knowledge Engineering and Knowledge Management (IC3K)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10467v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10467v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08895v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08895v2",
                "updated": "2025-08-14T09:04:56Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    9,
                    4,
                    56,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-12T12:35:55Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    12,
                    35,
                    55,
                    1,
                    224,
                    0
                ],
                "title": "ASPD: Unlocking Adaptive Serial-Parallel Decoding by Exploring Intrinsic\n  Parallelism in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASPD: Unlocking Adaptive Serial-Parallel Decoding by Exploring Intrinsic\n  Parallelism in LLMs"
                },
                "summary": "The increasing scale and complexity of large language models (LLMs) pose\nsignificant inference latency challenges, primarily due to their autoregressive\ndecoding paradigm characterized by the sequential nature of next-token\nprediction. By re-examining the outputs of autoregressive models, we observed\nthat some segments exhibit parallelizable structures, which we term intrinsic\nparallelism. Decoding each parallelizable branch simultaneously (i.e. parallel\ndecoding) can significantly improve the overall inference speed of LLMs. In\nthis paper, we propose an Adaptive Serial-Parallel Decoding (ASPD), which\naddresses two core challenges: automated construction of parallelizable data\nand efficient parallel decoding mechanism. More specifically, we introduce a\nnon-invasive pipeline that automatically extracts and validates parallelizable\nstructures from the responses of autoregressive models. To empower efficient\nadaptive serial-parallel decoding, we implement a Hybrid Decoding Engine which\nenables seamless transitions between serial and parallel decoding modes while\nmaintaining a reusable KV cache, maximizing computational efficiency. Extensive\nevaluations across General Tasks, Retrieval-Augmented Generation, Mathematical\nReasoning, demonstrate that ASPD achieves unprecedented performance in both\neffectiveness and efficiency. Notably, on Vicuna Bench, our method achieves up\nto 3.19x speedup (1.85x on average) while maintaining response quality within\n1% difference compared to autoregressive models, realizing significant\nacceleration without compromising generation quality. Our framework sets a\ngroundbreaking benchmark for efficient LLM parallel inference, paving the way\nfor its deployment in latency-sensitive applications such as AI-powered\ncustomer service bots and answer retrieval engines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing scale and complexity of large language models (LLMs) pose\nsignificant inference latency challenges, primarily due to their autoregressive\ndecoding paradigm characterized by the sequential nature of next-token\nprediction. By re-examining the outputs of autoregressive models, we observed\nthat some segments exhibit parallelizable structures, which we term intrinsic\nparallelism. Decoding each parallelizable branch simultaneously (i.e. parallel\ndecoding) can significantly improve the overall inference speed of LLMs. In\nthis paper, we propose an Adaptive Serial-Parallel Decoding (ASPD), which\naddresses two core challenges: automated construction of parallelizable data\nand efficient parallel decoding mechanism. More specifically, we introduce a\nnon-invasive pipeline that automatically extracts and validates parallelizable\nstructures from the responses of autoregressive models. To empower efficient\nadaptive serial-parallel decoding, we implement a Hybrid Decoding Engine which\nenables seamless transitions between serial and parallel decoding modes while\nmaintaining a reusable KV cache, maximizing computational efficiency. Extensive\nevaluations across General Tasks, Retrieval-Augmented Generation, Mathematical\nReasoning, demonstrate that ASPD achieves unprecedented performance in both\neffectiveness and efficiency. Notably, on Vicuna Bench, our method achieves up\nto 3.19x speedup (1.85x on average) while maintaining response quality within\n1% difference compared to autoregressive models, realizing significant\nacceleration without compromising generation quality. Our framework sets a\ngroundbreaking benchmark for efficient LLM parallel inference, paving the way\nfor its deployment in latency-sensitive applications such as AI-powered\ncustomer service bots and answer retrieval engines."
                },
                "authors": [
                    {
                        "name": "Keyu Chen"
                    },
                    {
                        "name": "Zhifeng Shen"
                    },
                    {
                        "name": "Daohai Yu"
                    },
                    {
                        "name": "Haoqian Wu"
                    },
                    {
                        "name": "Wei Wen"
                    },
                    {
                        "name": "Jianfeng He"
                    },
                    {
                        "name": "Ruizhi Qiao"
                    },
                    {
                        "name": "Xing Sun"
                    }
                ],
                "author_detail": {
                    "name": "Xing Sun"
                },
                "author": "Xing Sun",
                "arxiv_comment": "20 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08895v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08895v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10461v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10461v1",
                "updated": "2025-08-14T09:00:45Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    9,
                    0,
                    45,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T09:00:45Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    9,
                    0,
                    45,
                    3,
                    226,
                    0
                ],
                "title": "X-Node: Self-Explanation is All We Need",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "X-Node: Self-Explanation is All We Need"
                },
                "summary": "Graph neural networks (GNNs) have achieved state-of-the-art results in\ncomputer vision and medical image classification tasks by capturing structural\ndependencies across data instances. However, their decision-making remains\nlargely opaque, limiting their trustworthiness in high-stakes clinical\napplications where interpretability is essential. Existing explainability\ntechniques for GNNs are typically post-hoc and global, offering limited insight\ninto individual node decisions or local reasoning. We introduce X-Node, a\nself-explaining GNN framework in which each node generates its own explanation\nas part of the prediction process. For every node, we construct a structured\ncontext vector encoding interpretable cues such as degree, centrality,\nclustering, feature saliency, and label agreement within its local topology. A\nlightweight Reasoner module maps this context into a compact explanation\nvector, which serves three purposes: (1) reconstructing the node's latent\nembedding via a decoder to enforce faithfulness, (2) generating a natural\nlanguage explanation using a pre-trained LLM (e.g., Grok or Gemini), and (3)\nguiding the GNN itself via a \"text-injection\" mechanism that feeds explanations\nback into the message-passing pipeline. We evaluate X-Node on two graph\ndatasets derived from MedMNIST and MorphoMNIST, integrating it with GCN, GAT,\nand GIN backbones. Our results show that X-Node maintains competitive\nclassification accuracy while producing faithful, per-node explanations.\nRepository: https://github.com/basiralab/X-Node.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph neural networks (GNNs) have achieved state-of-the-art results in\ncomputer vision and medical image classification tasks by capturing structural\ndependencies across data instances. However, their decision-making remains\nlargely opaque, limiting their trustworthiness in high-stakes clinical\napplications where interpretability is essential. Existing explainability\ntechniques for GNNs are typically post-hoc and global, offering limited insight\ninto individual node decisions or local reasoning. We introduce X-Node, a\nself-explaining GNN framework in which each node generates its own explanation\nas part of the prediction process. For every node, we construct a structured\ncontext vector encoding interpretable cues such as degree, centrality,\nclustering, feature saliency, and label agreement within its local topology. A\nlightweight Reasoner module maps this context into a compact explanation\nvector, which serves three purposes: (1) reconstructing the node's latent\nembedding via a decoder to enforce faithfulness, (2) generating a natural\nlanguage explanation using a pre-trained LLM (e.g., Grok or Gemini), and (3)\nguiding the GNN itself via a \"text-injection\" mechanism that feeds explanations\nback into the message-passing pipeline. We evaluate X-Node on two graph\ndatasets derived from MedMNIST and MorphoMNIST, integrating it with GCN, GAT,\nand GIN backbones. Our results show that X-Node maintains competitive\nclassification accuracy while producing faithful, per-node explanations.\nRepository: https://github.com/basiralab/X-Node."
                },
                "authors": [
                    {
                        "name": "Prajit Sengupta"
                    },
                    {
                        "name": "Islem Rekik"
                    }
                ],
                "author_detail": {
                    "name": "Islem Rekik"
                },
                "author": "Islem Rekik",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10461v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10461v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07321v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07321v2",
                "updated": "2025-08-14T09:00:40Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    9,
                    0,
                    40,
                    3,
                    226,
                    0
                ],
                "published": "2024-08-14T06:43:06Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    6,
                    43,
                    6,
                    2,
                    227,
                    0
                ],
                "title": "VERCATION: Precise Vulnerable Open-source Software Version\n  Identification based on Static Analysis and LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VERCATION: Precise Vulnerable Open-source Software Version\n  Identification based on Static Analysis and LLM"
                },
                "summary": "Open-source software (OSS) has experienced a surge in popularity, attributed\nto its collaborative development model and cost-effective nature. However, the\nadoption of specific software versions in development projects may introduce\nsecurity risks when these versions bring along vulnerabilities. Current methods\nof identifying vulnerable versions typically analyze and extract the code\nfeatures involved in vulnerability patches using static analysis with\npre-defined rules. They then use code clone detection to identify the\nvulnerable versions. These methods are hindered by imprecision due to (1) the\nexclusion of vulnerability-irrelevant code in the analysis and (2) the\ninadequacy of code clone detection. This paper presents VERCATION, an approach\ndesigned to identify vulnerable versions of OSS written in C/C++. VERCATION\ncombines program slicing with a Large Language Model (LLM) to identify\nvulnerability-relevant code from vulnerability patches. It then backtracks\nhistorical commits to gather previous modifications of identified\nvulnerability-relevant code. We propose code clone detection based on expanded\nand normalized ASTs to compare the differences between pre-modification and\npost-modification code, thereby locating the vulnerability-introducing commit\n(vic) and enabling the identification of the vulnerable versions between the\nvulnerability-fixing commit and the vic. We curate a dataset linking 122 OSS\nvulnerabilities and 1,211 versions to evaluate VERCATION. On this dataset, our\napproach achieves an F1 score of 93.1%, outperforming current state-of-the-art\nmethods. More importantly, VERCATION detected 202 incorrect vulnerable OSS\nversions in NVD reports.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-source software (OSS) has experienced a surge in popularity, attributed\nto its collaborative development model and cost-effective nature. However, the\nadoption of specific software versions in development projects may introduce\nsecurity risks when these versions bring along vulnerabilities. Current methods\nof identifying vulnerable versions typically analyze and extract the code\nfeatures involved in vulnerability patches using static analysis with\npre-defined rules. They then use code clone detection to identify the\nvulnerable versions. These methods are hindered by imprecision due to (1) the\nexclusion of vulnerability-irrelevant code in the analysis and (2) the\ninadequacy of code clone detection. This paper presents VERCATION, an approach\ndesigned to identify vulnerable versions of OSS written in C/C++. VERCATION\ncombines program slicing with a Large Language Model (LLM) to identify\nvulnerability-relevant code from vulnerability patches. It then backtracks\nhistorical commits to gather previous modifications of identified\nvulnerability-relevant code. We propose code clone detection based on expanded\nand normalized ASTs to compare the differences between pre-modification and\npost-modification code, thereby locating the vulnerability-introducing commit\n(vic) and enabling the identification of the vulnerable versions between the\nvulnerability-fixing commit and the vic. We curate a dataset linking 122 OSS\nvulnerabilities and 1,211 versions to evaluate VERCATION. On this dataset, our\napproach achieves an F1 score of 93.1%, outperforming current state-of-the-art\nmethods. More importantly, VERCATION detected 202 incorrect vulnerable OSS\nversions in NVD reports."
                },
                "authors": [
                    {
                        "name": "Yiran Cheng"
                    },
                    {
                        "name": "Ting Zhang"
                    },
                    {
                        "name": "Lwin Khin Shar"
                    },
                    {
                        "name": "Shouguo Yang"
                    },
                    {
                        "name": "Chaopeng Dong"
                    },
                    {
                        "name": "David Lo"
                    },
                    {
                        "name": "Shichao Lv"
                    },
                    {
                        "name": "Zhiqiang Shi"
                    },
                    {
                        "name": "Limin Sun"
                    }
                ],
                "author_detail": {
                    "name": "Limin Sun"
                },
                "author": "Limin Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07321v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07321v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23714v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23714v2",
                "updated": "2025-08-14T08:59:56Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    8,
                    59,
                    56,
                    3,
                    226,
                    0
                ],
                "published": "2025-03-31T04:28:38Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    4,
                    28,
                    38,
                    0,
                    90,
                    0
                ],
                "title": "Building Instruction-Tuning Datasets from Human-Written Instructions\n  with Open-Weight Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building Instruction-Tuning Datasets from Human-Written Instructions\n  with Open-Weight Large Language Models"
                },
                "summary": "Instruction tuning is crucial for enabling Large Language Models (LLMs) to\nsolve real-world tasks. Prior work has shown the effectiveness of\ninstruction-tuning data synthesized solely from LLMs, raising a fundamental\nquestion: Do we still need human-originated signals for instruction tuning?\nThis work answers the question affirmatively: we build state-of-the-art\ninstruction-tuning datasets sourced from human-written instructions, by simply\npairing them with LLM-generated responses. LLMs fine-tuned on our datasets\nconsistently outperform those fine-tuned on existing ones. Our data\nconstruction approach can be easily adapted to other languages; we build\ndatasets for Japanese and confirm that LLMs tuned with our data reach\nstate-of-the-art performance. Analyses suggest that instruction-tuning in a new\nlanguage allows LLMs to follow instructions, while the tuned models exhibit a\nnotable lack of culture-specific knowledge in that language. The datasets and\nfine-tuned models will be publicly available. Our datasets, synthesized with\nopen-weight LLMs, are openly distributed under permissive licenses, allowing\nfor diverse use cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction tuning is crucial for enabling Large Language Models (LLMs) to\nsolve real-world tasks. Prior work has shown the effectiveness of\ninstruction-tuning data synthesized solely from LLMs, raising a fundamental\nquestion: Do we still need human-originated signals for instruction tuning?\nThis work answers the question affirmatively: we build state-of-the-art\ninstruction-tuning datasets sourced from human-written instructions, by simply\npairing them with LLM-generated responses. LLMs fine-tuned on our datasets\nconsistently outperform those fine-tuned on existing ones. Our data\nconstruction approach can be easily adapted to other languages; we build\ndatasets for Japanese and confirm that LLMs tuned with our data reach\nstate-of-the-art performance. Analyses suggest that instruction-tuning in a new\nlanguage allows LLMs to follow instructions, while the tuned models exhibit a\nnotable lack of culture-specific knowledge in that language. The datasets and\nfine-tuned models will be publicly available. Our datasets, synthesized with\nopen-weight LLMs, are openly distributed under permissive licenses, allowing\nfor diverse use cases."
                },
                "authors": [
                    {
                        "name": "Youmi Ma"
                    },
                    {
                        "name": "Sakae Mizuki"
                    },
                    {
                        "name": "Kazuki Fujii"
                    },
                    {
                        "name": "Taishi Nakamura"
                    },
                    {
                        "name": "Masanari Ohi"
                    },
                    {
                        "name": "Hinari Shimada"
                    },
                    {
                        "name": "Taihei Shiotani"
                    },
                    {
                        "name": "Koshiro Saito"
                    },
                    {
                        "name": "Koki Maeda"
                    },
                    {
                        "name": "Kakeru Hattori"
                    },
                    {
                        "name": "Takumi Okamoto"
                    },
                    {
                        "name": "Shigeki Ishida"
                    },
                    {
                        "name": "Rio Yokota"
                    },
                    {
                        "name": "Hiroya Takamura"
                    },
                    {
                        "name": "Naoaki Okazaki"
                    }
                ],
                "author_detail": {
                    "name": "Naoaki Okazaki"
                },
                "author": "Naoaki Okazaki",
                "arxiv_comment": "COLM 2025; Datasets are available at\n  https://huggingface.co/datasets/tokyotech-llm/lmsys-chat-1m-synth",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23714v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23714v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10458v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10458v1",
                "updated": "2025-08-14T08:57:59Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    8,
                    57,
                    59,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T08:57:59Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    8,
                    57,
                    59,
                    3,
                    226,
                    0
                ],
                "title": "System design and realisation towards optimising secure key bits in free\n  space QKD",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "System design and realisation towards optimising secure key bits in free\n  space QKD"
                },
                "summary": "Quantum Key Distribution (QKD) is rapidly transitioning from cutting-edge\nlaboratory research to real-world deployment in established communication\nnetworks. Although QKD promises future-proof security, practical challenges\nstil exist due to imperfections in physical devices. Many protocols offer\nstrong security guarantees, but their implementation can be complex and\ndifficult. To bridge this gap, we present a practical and systematic framework\nfor implementing QKD, focused on the BB84 protocol but designed with broader\napplicability in mind. The article includes key concepts for device\ncalibration, synchronisation,optical alignment, and key post-processing. We\noutline a simple algorithm for key sifting that is easily implementable in\nhardware. Our results highlight the importance of selecting the temporal window\nto optimise both the key rate and the quantum bit error rate (QBER). In\naddition, we show that random sampling of the sifted key bits for error\nestimation yields more reliable results than sequential sampling. We also\nintegrate the Entrapped Pulse Coincidence Detection (EPCD) protocol to boost\nkey generation rates, further enhancing performance. Although our work focuses\non BB84, the techniques and practices outlined are general enough to support a\nwide range of QKD protocols. This makes our framework a valuable tool for both\nresearch and real-world deployment of secure quantum communication systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum Key Distribution (QKD) is rapidly transitioning from cutting-edge\nlaboratory research to real-world deployment in established communication\nnetworks. Although QKD promises future-proof security, practical challenges\nstil exist due to imperfections in physical devices. Many protocols offer\nstrong security guarantees, but their implementation can be complex and\ndifficult. To bridge this gap, we present a practical and systematic framework\nfor implementing QKD, focused on the BB84 protocol but designed with broader\napplicability in mind. The article includes key concepts for device\ncalibration, synchronisation,optical alignment, and key post-processing. We\noutline a simple algorithm for key sifting that is easily implementable in\nhardware. Our results highlight the importance of selecting the temporal window\nto optimise both the key rate and the quantum bit error rate (QBER). In\naddition, we show that random sampling of the sifted key bits for error\nestimation yields more reliable results than sequential sampling. We also\nintegrate the Entrapped Pulse Coincidence Detection (EPCD) protocol to boost\nkey generation rates, further enhancing performance. Although our work focuses\non BB84, the techniques and practices outlined are general enough to support a\nwide range of QKD protocols. This makes our framework a valuable tool for both\nresearch and real-world deployment of secure quantum communication systems."
                },
                "authors": [
                    {
                        "name": "Pooja Chandravanshi"
                    },
                    {
                        "name": "Jayanth Ramakrishnan"
                    },
                    {
                        "name": "Tanya Sharma"
                    },
                    {
                        "name": "Ayan Biswas"
                    },
                    {
                        "name": "Ravindra P. Singh"
                    }
                ],
                "author_detail": {
                    "name": "Ravindra P. Singh"
                },
                "author": "Ravindra P. Singh",
                "arxiv_comment": "Comments are welcome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10458v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10458v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19160v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19160v3",
                "updated": "2025-08-14T08:27:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    8,
                    27,
                    24,
                    3,
                    226,
                    0
                ],
                "published": "2024-12-26T10:40:15Z",
                "published_parsed": [
                    2024,
                    12,
                    26,
                    10,
                    40,
                    15,
                    3,
                    361,
                    0
                ],
                "title": "A Lightweight Transformer with Phase-Only Cross-Attention for\n  Illumination-Invariant Biometric Authentication",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Lightweight Transformer with Phase-Only Cross-Attention for\n  Illumination-Invariant Biometric Authentication"
                },
                "summary": "Traditional biometric systems have encountered significant setbacks due to\nvarious unavoidable factors, for example, wearing of face masks in face\nrecognition-based biometrics and hygiene concerns in fingerprint-based\nbiometrics. This paper proposes a novel lightweight vision transformer with\nphase-only cross-attention (POC-ViT) using dual biometric traits of forehead\nand periocular portions of the face, capable of performing well even with face\nmasks and without any physical touch, offering a promising alternative to\ntraditional methods. The POC-ViT framework is designed to handle two biometric\ntraits and to capture inter-dependencies in terms of relative structural\npatterns. Each channel consists of a Cross-Attention using phase-only\ncorrelation (POC) that captures both their individual and correlated structural\npatterns. The computation of cross-attention using POC extracts the phase\ncorrelation in the spatial features. Therefore, it is robust against variations\nin resolution and intensity, as well as illumination changes in the input\nimages. The lightweight model is suitable for edge device deployment. The\nperformance of the proposed framework was successfully demonstrated using the\nForehead Subcutaneous Vein Pattern and Periocular Biometric Pattern (FSVP-PBP)\ndatabase, having 350 subjects. The POC-ViT framework outperformed\nstate-of-the-art methods with an outstanding classification accuracy of\n$98.8\\%$ with the dual biometric traits.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional biometric systems have encountered significant setbacks due to\nvarious unavoidable factors, for example, wearing of face masks in face\nrecognition-based biometrics and hygiene concerns in fingerprint-based\nbiometrics. This paper proposes a novel lightweight vision transformer with\nphase-only cross-attention (POC-ViT) using dual biometric traits of forehead\nand periocular portions of the face, capable of performing well even with face\nmasks and without any physical touch, offering a promising alternative to\ntraditional methods. The POC-ViT framework is designed to handle two biometric\ntraits and to capture inter-dependencies in terms of relative structural\npatterns. Each channel consists of a Cross-Attention using phase-only\ncorrelation (POC) that captures both their individual and correlated structural\npatterns. The computation of cross-attention using POC extracts the phase\ncorrelation in the spatial features. Therefore, it is robust against variations\nin resolution and intensity, as well as illumination changes in the input\nimages. The lightweight model is suitable for edge device deployment. The\nperformance of the proposed framework was successfully demonstrated using the\nForehead Subcutaneous Vein Pattern and Periocular Biometric Pattern (FSVP-PBP)\ndatabase, having 350 subjects. The POC-ViT framework outperformed\nstate-of-the-art methods with an outstanding classification accuracy of\n$98.8\\%$ with the dual biometric traits."
                },
                "authors": [
                    {
                        "name": "Arun K. Sharma"
                    },
                    {
                        "name": "Shubhobrata Bhattacharya"
                    },
                    {
                        "name": "Motahar Reza"
                    },
                    {
                        "name": "Bishakh Bhattacharya"
                    }
                ],
                "author_detail": {
                    "name": "Bishakh Bhattacharya"
                },
                "author": "Bishakh Bhattacharya",
                "arxiv_comment": "Submitted to IEEE",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19160v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19160v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.14189v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.14189v2",
                "updated": "2025-08-14T08:14:12Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    8,
                    14,
                    12,
                    3,
                    226,
                    0
                ],
                "published": "2025-07-14T02:13:22Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    2,
                    13,
                    22,
                    0,
                    195,
                    0
                ],
                "title": "DeepWriter: A Fact-Grounded Multimodal Writing Assistant Based On\n  Offline Knowledge Base",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepWriter: A Fact-Grounded Multimodal Writing Assistant Based On\n  Offline Knowledge Base"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nvarious applications. However, their use as writing assistants in specialized\ndomains like finance, medicine, and law is often hampered by a lack of deep\ndomain-specific knowledge and a tendency to hallucinate. Existing solutions,\nsuch as Retrieval-Augmented Generation (RAG), can suffer from inconsistency\nacross multiple retrieval steps, while online search-based methods often\ndegrade quality due to unreliable web content. To address these challenges, we\nintroduce DeepWriter, a customizable, multimodal, long-form writing assistant\nthat operates on a curated, offline knowledge base. DeepWriter leverages a\nnovel pipeline that involves task decomposition, outline generation, multimodal\nretrieval, and section-by-section composition with reflection. By deeply mining\ninformation from a structured corpus and incorporating both textual and visual\nelements, DeepWriter generates coherent, factually grounded, and\nprofessional-grade documents. We also propose a hierarchical knowledge\nrepresentation to enhance retrieval efficiency and accuracy. Our experiments on\nfinancial report generation demonstrate that DeepWriter produces high-quality,\nverifiable articles that surpasses existing baselines in factual accuracy and\ngenerated content quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nvarious applications. However, their use as writing assistants in specialized\ndomains like finance, medicine, and law is often hampered by a lack of deep\ndomain-specific knowledge and a tendency to hallucinate. Existing solutions,\nsuch as Retrieval-Augmented Generation (RAG), can suffer from inconsistency\nacross multiple retrieval steps, while online search-based methods often\ndegrade quality due to unreliable web content. To address these challenges, we\nintroduce DeepWriter, a customizable, multimodal, long-form writing assistant\nthat operates on a curated, offline knowledge base. DeepWriter leverages a\nnovel pipeline that involves task decomposition, outline generation, multimodal\nretrieval, and section-by-section composition with reflection. By deeply mining\ninformation from a structured corpus and incorporating both textual and visual\nelements, DeepWriter generates coherent, factually grounded, and\nprofessional-grade documents. We also propose a hierarchical knowledge\nrepresentation to enhance retrieval efficiency and accuracy. Our experiments on\nfinancial report generation demonstrate that DeepWriter produces high-quality,\nverifiable articles that surpasses existing baselines in factual accuracy and\ngenerated content quality."
                },
                "authors": [
                    {
                        "name": "Song Mao"
                    },
                    {
                        "name": "Lejun Cheng"
                    },
                    {
                        "name": "Pinlong Cai"
                    },
                    {
                        "name": "Guohang Yan"
                    },
                    {
                        "name": "Ding Wang"
                    },
                    {
                        "name": "Botian Shi"
                    }
                ],
                "author_detail": {
                    "name": "Botian Shi"
                },
                "author": "Botian Shi",
                "arxiv_comment": "work in process",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.14189v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.14189v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10428v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10428v1",
                "updated": "2025-08-14T07:58:01Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    7,
                    58,
                    1,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T07:58:01Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    7,
                    58,
                    1,
                    3,
                    226,
                    0
                ],
                "title": "SC2Arena and StarEvolve: Benchmark and Self-Improvement Framework for\n  LLMs in Complex Decision-Making Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SC2Arena and StarEvolve: Benchmark and Self-Improvement Framework for\n  LLMs in Complex Decision-Making Tasks"
                },
                "summary": "Evaluating large language models (LLMs) in complex decision-making is\nessential for advancing AI's ability for strategic planning and real-time\nadaptation. However, existing benchmarks for tasks like StarCraft II fail to\ncapture the game's full complexity, such as its complete game context, diverse\naction spaces, and all playable races. To address this gap, we present\nSC2Arena, a benchmark that fully supports all playable races, low-level action\nspaces, and optimizes text-based observations to tackle spatial reasoning\nchallenges. Complementing this, we introduce StarEvolve, a hierarchical\nframework that integrates strategic planning with tactical execution, featuring\niterative self-correction and continuous improvement via fine-tuning on\nhigh-quality gameplay data. Its key components include a\nPlanner-Executor-Verifier structure to break down gameplay, and a scoring\nsystem for selecting high-quality training samples. Comprehensive analysis\nusing SC2Arena provides valuable insights into developing generalist agents\nthat were not possible with previous benchmarks. Experimental results also\ndemonstrate that our proposed StarEvolve achieves superior performance in\nstrategic planning. Our code, environment, and algorithms are publicly\navailable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating large language models (LLMs) in complex decision-making is\nessential for advancing AI's ability for strategic planning and real-time\nadaptation. However, existing benchmarks for tasks like StarCraft II fail to\ncapture the game's full complexity, such as its complete game context, diverse\naction spaces, and all playable races. To address this gap, we present\nSC2Arena, a benchmark that fully supports all playable races, low-level action\nspaces, and optimizes text-based observations to tackle spatial reasoning\nchallenges. Complementing this, we introduce StarEvolve, a hierarchical\nframework that integrates strategic planning with tactical execution, featuring\niterative self-correction and continuous improvement via fine-tuning on\nhigh-quality gameplay data. Its key components include a\nPlanner-Executor-Verifier structure to break down gameplay, and a scoring\nsystem for selecting high-quality training samples. Comprehensive analysis\nusing SC2Arena provides valuable insights into developing generalist agents\nthat were not possible with previous benchmarks. Experimental results also\ndemonstrate that our proposed StarEvolve achieves superior performance in\nstrategic planning. Our code, environment, and algorithms are publicly\navailable."
                },
                "authors": [
                    {
                        "name": "Pengbo Shen"
                    },
                    {
                        "name": "Yaqing Wang"
                    },
                    {
                        "name": "Ni Mu"
                    },
                    {
                        "name": "Yao Luan"
                    },
                    {
                        "name": "Runpeng Xie"
                    },
                    {
                        "name": "Senhao Yang"
                    },
                    {
                        "name": "Lexiang Wang"
                    },
                    {
                        "name": "Hao Hu"
                    },
                    {
                        "name": "Shuang Xu"
                    },
                    {
                        "name": "Yiqin Yang"
                    },
                    {
                        "name": "Bo Xu"
                    }
                ],
                "author_detail": {
                    "name": "Bo Xu"
                },
                "author": "Bo Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10428v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10428v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10426v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10426v1",
                "updated": "2025-08-14T07:55:45Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    7,
                    55,
                    45,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T07:55:45Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    7,
                    55,
                    45,
                    3,
                    226,
                    0
                ],
                "title": "Computational Economics in Large Language Models: Exploring Model\n  Behavior and Incentive Design under Resource Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computational Economics in Large Language Models: Exploring Model\n  Behavior and Incentive Design under Resource Constraints"
                },
                "summary": "Large language models (LLMs) are limited by substantial computational cost.\nWe introduce a \"computational economics\" framework that treats an LLM as an\ninternal economy of resource-constrained agents (attention heads and neuron\nblocks) that must allocate scarce computation to maximize task utility. First,\nwe show empirically that when computation is scarce, standard LLMs reallocate\nattention toward high-value tokens while preserving accuracy. Building on this\nobservation, we propose an incentive-driven training paradigm that augments the\ntask loss with a differentiable computation cost term, encouraging sparse and\nefficient activations. On GLUE (MNLI, STS-B, CoLA) and WikiText-103, the method\nyields a family of models that trace a Pareto frontier and consistently\ndominate post-hoc pruning; for a similar accuracy we obtain roughly a forty\npercent reduction in FLOPS and lower latency, together with more interpretable\nattention patterns. These results indicate that economic principles offer a\nprincipled route to designing efficient, adaptive, and more transparent LLMs\nunder strict resource constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are limited by substantial computational cost.\nWe introduce a \"computational economics\" framework that treats an LLM as an\ninternal economy of resource-constrained agents (attention heads and neuron\nblocks) that must allocate scarce computation to maximize task utility. First,\nwe show empirically that when computation is scarce, standard LLMs reallocate\nattention toward high-value tokens while preserving accuracy. Building on this\nobservation, we propose an incentive-driven training paradigm that augments the\ntask loss with a differentiable computation cost term, encouraging sparse and\nefficient activations. On GLUE (MNLI, STS-B, CoLA) and WikiText-103, the method\nyields a family of models that trace a Pareto frontier and consistently\ndominate post-hoc pruning; for a similar accuracy we obtain roughly a forty\npercent reduction in FLOPS and lower latency, together with more interpretable\nattention patterns. These results indicate that economic principles offer a\nprincipled route to designing efficient, adaptive, and more transparent LLMs\nunder strict resource constraints."
                },
                "authors": [
                    {
                        "name": "Sandeep Reddy"
                    },
                    {
                        "name": "Kabir Khan"
                    },
                    {
                        "name": "Rohit Patil"
                    },
                    {
                        "name": "Ananya Chakraborty"
                    },
                    {
                        "name": "Faizan A. Khan"
                    },
                    {
                        "name": "Swati Kulkarni"
                    },
                    {
                        "name": "Arjun Verma"
                    },
                    {
                        "name": "Neha Singh"
                    }
                ],
                "author_detail": {
                    "name": "Neha Singh"
                },
                "author": "Neha Singh",
                "arxiv_comment": "Preprint; 7 figures, 4 tables, 1 algorithm. Experiments on GLUE\n  (MNLI, STS-B, CoLA) and WikiText-103 with BERT-base; evaluation includes\n  FLOPS, latency, Gini and entropy metrics",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10426v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10426v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6; I.2.7; I.5.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10421v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10421v1",
                "updated": "2025-08-14T07:52:56Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    7,
                    52,
                    56,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T07:52:56Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    7,
                    52,
                    56,
                    3,
                    226,
                    0
                ],
                "title": "Evaluating LLMs on Chinese Idiom Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating LLMs on Chinese Idiom Translation"
                },
                "summary": "Idioms, whose figurative meanings usually differ from their literal\ninterpretations, are common in everyday language, especially in Chinese, where\nthey often contain historical references and follow specific structural\npatterns. Despite recent progress in machine translation with large language\nmodels, little is known about Chinese idiom translation. In this work, we\nintroduce IdiomEval, a framework with a comprehensive error taxonomy for\nChinese idiom translation. We annotate 900 translation pairs from nine modern\nsystems, including GPT-4o and Google Translate, across four domains: web, news,\nWikipedia, and social media. We find these systems fail at idiom translation,\nproducing incorrect, literal, partial, or even missing translations. The\nbest-performing system, GPT-4, makes errors in 28% of cases. We also find that\nexisting evaluation metrics measure idiom quality poorly with Pearson\ncorrelation below 0.48 with human ratings. We thus develop improved models that\nachieve F$_1$ scores of 0.68 for detecting idiom translation errors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Idioms, whose figurative meanings usually differ from their literal\ninterpretations, are common in everyday language, especially in Chinese, where\nthey often contain historical references and follow specific structural\npatterns. Despite recent progress in machine translation with large language\nmodels, little is known about Chinese idiom translation. In this work, we\nintroduce IdiomEval, a framework with a comprehensive error taxonomy for\nChinese idiom translation. We annotate 900 translation pairs from nine modern\nsystems, including GPT-4o and Google Translate, across four domains: web, news,\nWikipedia, and social media. We find these systems fail at idiom translation,\nproducing incorrect, literal, partial, or even missing translations. The\nbest-performing system, GPT-4, makes errors in 28% of cases. We also find that\nexisting evaluation metrics measure idiom quality poorly with Pearson\ncorrelation below 0.48 with human ratings. We thus develop improved models that\nachieve F$_1$ scores of 0.68 for detecting idiom translation errors."
                },
                "authors": [
                    {
                        "name": "Cai Yang"
                    },
                    {
                        "name": "Yao Dou"
                    },
                    {
                        "name": "David Heineman"
                    },
                    {
                        "name": "Xiaofeng Wu"
                    },
                    {
                        "name": "Wei Xu"
                    }
                ],
                "author_detail": {
                    "name": "Wei Xu"
                },
                "author": "Wei Xu",
                "arxiv_comment": "Accepted at COLM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10421v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10421v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10419v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10419v1",
                "updated": "2025-08-14T07:52:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    7,
                    52,
                    9,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T07:52:09Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    7,
                    52,
                    9,
                    3,
                    226,
                    0
                ],
                "title": "ComoRAG: A Cognitive-Inspired Memory-Organized RAG for Stateful Long\n  Narrative Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ComoRAG: A Cognitive-Inspired Memory-Organized RAG for Stateful Long\n  Narrative Reasoning"
                },
                "summary": "Narrative comprehension on long stories and novels has been a challenging\ndomain attributed to their intricate plotlines and entangled, often evolving\nrelations among characters and entities. Given the LLM's diminished reasoning\nover extended context and high computational cost, retrieval-based approaches\nremain a pivotal role in practice. However, traditional RAG methods can fall\nshort due to their stateless, single-step retrieval process, which often\noverlooks the dynamic nature of capturing interconnected relations within\nlong-range context. In this work, we propose ComoRAG, holding the principle\nthat narrative reasoning is not a one-shot process, but a dynamic, evolving\ninterplay between new evidence acquisition and past knowledge consolidation,\nanalogous to human cognition when reasoning with memory-related signals in the\nbrain. Specifically, when encountering a reasoning impasse, ComoRAG undergoes\niterative reasoning cycles while interacting with a dynamic memory workspace.\nIn each cycle, it generates probing queries to devise new exploratory paths,\nthen integrates the retrieved evidence of new aspects into a global memory\npool, thereby supporting the emergence of a coherent context for the query\nresolution. Across four challenging long-context narrative benchmarks (200K+\ntokens), ComoRAG outperforms strong RAG baselines with consistent relative\ngains up to 11% compared to the strongest baseline. Further analysis reveals\nthat ComoRAG is particularly advantageous for complex queries requiring global\ncomprehension, offering a principled, cognitively motivated paradigm for\nretrieval-based long context comprehension towards stateful reasoning. Our code\nis publicly released at https://github.com/EternityJune25/ComoRAG",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Narrative comprehension on long stories and novels has been a challenging\ndomain attributed to their intricate plotlines and entangled, often evolving\nrelations among characters and entities. Given the LLM's diminished reasoning\nover extended context and high computational cost, retrieval-based approaches\nremain a pivotal role in practice. However, traditional RAG methods can fall\nshort due to their stateless, single-step retrieval process, which often\noverlooks the dynamic nature of capturing interconnected relations within\nlong-range context. In this work, we propose ComoRAG, holding the principle\nthat narrative reasoning is not a one-shot process, but a dynamic, evolving\ninterplay between new evidence acquisition and past knowledge consolidation,\nanalogous to human cognition when reasoning with memory-related signals in the\nbrain. Specifically, when encountering a reasoning impasse, ComoRAG undergoes\niterative reasoning cycles while interacting with a dynamic memory workspace.\nIn each cycle, it generates probing queries to devise new exploratory paths,\nthen integrates the retrieved evidence of new aspects into a global memory\npool, thereby supporting the emergence of a coherent context for the query\nresolution. Across four challenging long-context narrative benchmarks (200K+\ntokens), ComoRAG outperforms strong RAG baselines with consistent relative\ngains up to 11% compared to the strongest baseline. Further analysis reveals\nthat ComoRAG is particularly advantageous for complex queries requiring global\ncomprehension, offering a principled, cognitively motivated paradigm for\nretrieval-based long context comprehension towards stateful reasoning. Our code\nis publicly released at https://github.com/EternityJune25/ComoRAG"
                },
                "authors": [
                    {
                        "name": "Juyuan Wang"
                    },
                    {
                        "name": "Rongchen Zhao"
                    },
                    {
                        "name": "Wei Wei"
                    },
                    {
                        "name": "Yufeng Wang"
                    },
                    {
                        "name": "Mo Yu"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Jin Xu"
                    },
                    {
                        "name": "Liyan Xu"
                    }
                ],
                "author_detail": {
                    "name": "Liyan Xu"
                },
                "author": "Liyan Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10419v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10419v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10414v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10414v1",
                "updated": "2025-08-14T07:38:01Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    7,
                    38,
                    1,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T07:38:01Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    7,
                    38,
                    1,
                    3,
                    226,
                    0
                ],
                "title": "MCP2OSC: Parametric Control by Natural Language",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MCP2OSC: Parametric Control by Natural Language"
                },
                "summary": "Text prompts enable intuitive content creation but may fall short in\nachieving high precision for intricate tasks; knob or slider controls offer\nprecise adjustments at the cost of increased complexity. To address the gap\nbetween knobs and prompts, a new MCP (Model Context Protocol) server and a\nunique set of prompt design criteria are presented to enable exploring\nparametric OSC (OpenSoundControl) control by natural language prompts.\nDemonstrated by 14 practical QA examples with best practices and the\ngeneralized prompt templates, this study finds Claude integrated with the\nMCP2OSC server effective in generating OSC messages by natural language,\ninterpreting, searching, and visualizing OSC messages, validating and debugging\nOSC messages, and managing OSC address patterns. MCP2OSC enhances human-machine\ncollaboration by leveraging LLM (Large Language Model) to handle intricate OSC\ndevelopment tasks, and by empowering human creativity with an intuitive\nlanguage interface featuring flexible precision controls: a prompt-based OSC\ntool. This study provides a novel perspective on the creative MCP application\nat the network protocol level by utilizing LLM's strength in directly\nprocessing and generating human-readable OSC messages. The results suggest its\npotential for a LLM-based universal control mechanism for multimedia devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text prompts enable intuitive content creation but may fall short in\nachieving high precision for intricate tasks; knob or slider controls offer\nprecise adjustments at the cost of increased complexity. To address the gap\nbetween knobs and prompts, a new MCP (Model Context Protocol) server and a\nunique set of prompt design criteria are presented to enable exploring\nparametric OSC (OpenSoundControl) control by natural language prompts.\nDemonstrated by 14 practical QA examples with best practices and the\ngeneralized prompt templates, this study finds Claude integrated with the\nMCP2OSC server effective in generating OSC messages by natural language,\ninterpreting, searching, and visualizing OSC messages, validating and debugging\nOSC messages, and managing OSC address patterns. MCP2OSC enhances human-machine\ncollaboration by leveraging LLM (Large Language Model) to handle intricate OSC\ndevelopment tasks, and by empowering human creativity with an intuitive\nlanguage interface featuring flexible precision controls: a prompt-based OSC\ntool. This study provides a novel perspective on the creative MCP application\nat the network protocol level by utilizing LLM's strength in directly\nprocessing and generating human-readable OSC messages. The results suggest its\npotential for a LLM-based universal control mechanism for multimedia devices."
                },
                "authors": [
                    {
                        "name": "Yuan-Yi Fan"
                    }
                ],
                "author_detail": {
                    "name": "Yuan-Yi Fan"
                },
                "author": "Yuan-Yi Fan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10414v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10414v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09549v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09549v2",
                "updated": "2025-08-14T07:26:38Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    7,
                    26,
                    38,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-13T07:13:45Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    7,
                    13,
                    45,
                    2,
                    225,
                    0
                ],
                "title": "CS-Agent: LLM-based Community Search via Dual-agent Collaboration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CS-Agent: LLM-based Community Search via Dual-agent Collaboration"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nnatural language processing tasks, yet their application to graph structure\nanalysis, particularly in community search, remains underexplored. Community\nsearch, a fundamental task in graph analysis, aims to identify groups of nodes\nwith dense interconnections, which is crucial for understanding the macroscopic\nstructure of graphs. In this paper, we propose GraphCS, a comprehensive\nbenchmark designed to evaluate the performance of LLMs in community search\ntasks. Our experiments reveal that while LLMs exhibit preliminary potential,\nthey frequently fail to return meaningful results and suffer from output bias.\nTo address these limitations, we introduce CS-Agent, a dual-agent collaborative\nframework to enhance LLM-based community search. CS-Agent leverages the\ncomplementary strengths of two LLMs acting as Solver and Validator. Through\niterative feedback and refinement, CS-Agent dynamically refines initial results\nwithout fine-tuning or additional training. After the multi-round dialogue,\nDecider module selects the optimal community. Extensive experiments demonstrate\nthat CS-Agent significantly improves the quality and stability of identified\ncommunities compared to baseline methods. To our knowledge, this is the first\nwork to apply LLMs to community search, bridging the gap between LLMs and graph\nanalysis while providing a robust and adaptive solution for real-world\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nnatural language processing tasks, yet their application to graph structure\nanalysis, particularly in community search, remains underexplored. Community\nsearch, a fundamental task in graph analysis, aims to identify groups of nodes\nwith dense interconnections, which is crucial for understanding the macroscopic\nstructure of graphs. In this paper, we propose GraphCS, a comprehensive\nbenchmark designed to evaluate the performance of LLMs in community search\ntasks. Our experiments reveal that while LLMs exhibit preliminary potential,\nthey frequently fail to return meaningful results and suffer from output bias.\nTo address these limitations, we introduce CS-Agent, a dual-agent collaborative\nframework to enhance LLM-based community search. CS-Agent leverages the\ncomplementary strengths of two LLMs acting as Solver and Validator. Through\niterative feedback and refinement, CS-Agent dynamically refines initial results\nwithout fine-tuning or additional training. After the multi-round dialogue,\nDecider module selects the optimal community. Extensive experiments demonstrate\nthat CS-Agent significantly improves the quality and stability of identified\ncommunities compared to baseline methods. To our knowledge, this is the first\nwork to apply LLMs to community search, bridging the gap between LLMs and graph\nanalysis while providing a robust and adaptive solution for real-world\napplications."
                },
                "authors": [
                    {
                        "name": "Jiahao Hua"
                    },
                    {
                        "name": "Long Yuan"
                    },
                    {
                        "name": "Qingshuai Feng"
                    },
                    {
                        "name": "Qiang Fan"
                    },
                    {
                        "name": "Shan Huang"
                    }
                ],
                "author_detail": {
                    "name": "Shan Huang"
                },
                "author": "Shan Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09549v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09549v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.07697v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.07697v2",
                "updated": "2025-08-14T07:25:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    7,
                    25,
                    44,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-11T07:19:21Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    7,
                    19,
                    21,
                    0,
                    223,
                    0
                ],
                "title": "Semantic-Enhanced Time-Series Forecasting via Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic-Enhanced Time-Series Forecasting via Large Language Models"
                },
                "summary": "Time series forecasting plays a significant role in finance, energy,\nmeteorology, and IoT applications. Recent studies have leveraged the\ngeneralization capabilities of large language models (LLMs) to adapt to time\nseries forecasting, achieving promising performance. However, existing studies\nfocus on token-level modal alignment, instead of bridging the intrinsic\nmodality gap between linguistic knowledge structures and time series data\npatterns, greatly limiting the semantic representation. To address this issue,\nwe propose a novel Semantic-Enhanced LLM (SE-LLM) that explores the inherent\nperiodicity and anomalous characteristics of time series to embed into the\nsemantic space to enhance the token embedding. This process enhances the\ninterpretability of tokens for LLMs, thereby activating the potential of LLMs\nfor temporal sequence analysis. Moreover, existing Transformer-based LLMs excel\nat capturing long-range dependencies but are weak at modeling short-term\nanomalies in time-series data. Hence, we propose a plugin module embedded\nwithin self-attention that models long-term and short-term dependencies to\neffectively adapt LLMs to time-series analysis. Our approach freezes the LLM\nand reduces the sequence dimensionality of tokens, greatly reducing\ncomputational consumption. Experiments demonstrate the superiority performance\nof our SE-LLM against the state-of-the-art (SOTA) methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time series forecasting plays a significant role in finance, energy,\nmeteorology, and IoT applications. Recent studies have leveraged the\ngeneralization capabilities of large language models (LLMs) to adapt to time\nseries forecasting, achieving promising performance. However, existing studies\nfocus on token-level modal alignment, instead of bridging the intrinsic\nmodality gap between linguistic knowledge structures and time series data\npatterns, greatly limiting the semantic representation. To address this issue,\nwe propose a novel Semantic-Enhanced LLM (SE-LLM) that explores the inherent\nperiodicity and anomalous characteristics of time series to embed into the\nsemantic space to enhance the token embedding. This process enhances the\ninterpretability of tokens for LLMs, thereby activating the potential of LLMs\nfor temporal sequence analysis. Moreover, existing Transformer-based LLMs excel\nat capturing long-range dependencies but are weak at modeling short-term\nanomalies in time-series data. Hence, we propose a plugin module embedded\nwithin self-attention that models long-term and short-term dependencies to\neffectively adapt LLMs to time-series analysis. Our approach freezes the LLM\nand reduces the sequence dimensionality of tokens, greatly reducing\ncomputational consumption. Experiments demonstrate the superiority performance\nof our SE-LLM against the state-of-the-art (SOTA) methods."
                },
                "authors": [
                    {
                        "name": "Hao Liu"
                    },
                    {
                        "name": "Chun Yang"
                    },
                    {
                        "name": "Zhang xiaoxing"
                    },
                    {
                        "name": "Xiaobin Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaobin Zhu"
                },
                "author": "Xiaobin Zhu",
                "arxiv_comment": "14 pages,9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.07697v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.07697v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01618v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01618v5",
                "updated": "2025-08-14T07:21:40Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    7,
                    21,
                    40,
                    3,
                    226,
                    0
                ],
                "published": "2025-02-03T18:50:50Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    18,
                    50,
                    50,
                    0,
                    34,
                    0
                ],
                "title": "Rollout Roulette: A Probabilistic Inference Approach to Inference-Time\n  Scaling of LLMs using Particle-Based Monte Carlo Methods",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rollout Roulette: A Probabilistic Inference Approach to Inference-Time\n  Scaling of LLMs using Particle-Based Monte Carlo Methods"
                },
                "summary": "Large language models (LLMs) have achieved significant performance gains via\nscaling up model sizes and/or data. However, recent evidence suggests\ndiminishing returns from such approaches, motivating scaling the computation\nspent at inference time. Existing inference-time scaling methods, usually with\nreward models, cast the task as a search problem, which tends to be vulnerable\nto reward hacking as a consequence of approximation errors in reward models. In\nthis paper, we instead cast inference-time scaling as a probabilistic inference\ntask and leverage sampling-based techniques to explore the typical set of the\nstate distribution of a state-space model with an approximate likelihood,\nrather than optimize for its mode directly. We propose a novel inference-time\nscaling approach by adapting particle-based Monte Carlo methods to this task.\nOur empirical evaluation demonstrates that our methods have a 4-16x better\nscaling rate over our deterministic search counterparts on various challenging\nmathematical reasoning tasks. Using our approach, we show that\nQwen2.5-Math-1.5B-Instruct can surpass GPT-4o accuracy in only 4 rollouts,\nwhile Qwen2.5-Math-7B-Instruct scales to o1 level accuracy in only 32 rollouts.\nOur work not only presents an effective method to inference-time scaling, but\nalso connects the rich literature in probabilistic inference with\ninference-time scaling of LLMs to develop more robust algorithms in future\nwork. Code, videos, and further information available at\nhttps://probabilistic-inference-scaling.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved significant performance gains via\nscaling up model sizes and/or data. However, recent evidence suggests\ndiminishing returns from such approaches, motivating scaling the computation\nspent at inference time. Existing inference-time scaling methods, usually with\nreward models, cast the task as a search problem, which tends to be vulnerable\nto reward hacking as a consequence of approximation errors in reward models. In\nthis paper, we instead cast inference-time scaling as a probabilistic inference\ntask and leverage sampling-based techniques to explore the typical set of the\nstate distribution of a state-space model with an approximate likelihood,\nrather than optimize for its mode directly. We propose a novel inference-time\nscaling approach by adapting particle-based Monte Carlo methods to this task.\nOur empirical evaluation demonstrates that our methods have a 4-16x better\nscaling rate over our deterministic search counterparts on various challenging\nmathematical reasoning tasks. Using our approach, we show that\nQwen2.5-Math-1.5B-Instruct can surpass GPT-4o accuracy in only 4 rollouts,\nwhile Qwen2.5-Math-7B-Instruct scales to o1 level accuracy in only 32 rollouts.\nOur work not only presents an effective method to inference-time scaling, but\nalso connects the rich literature in probabilistic inference with\ninference-time scaling of LLMs to develop more robust algorithms in future\nwork. Code, videos, and further information available at\nhttps://probabilistic-inference-scaling.github.io."
                },
                "authors": [
                    {
                        "name": "Isha Puri"
                    },
                    {
                        "name": "Shivchander Sudalairaj"
                    },
                    {
                        "name": "Guangxuan Xu"
                    },
                    {
                        "name": "Kai Xu"
                    },
                    {
                        "name": "Akash Srivastava"
                    }
                ],
                "author_detail": {
                    "name": "Akash Srivastava"
                },
                "author": "Akash Srivastava",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01618v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01618v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09299v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09299v2",
                "updated": "2025-08-14T07:18:06Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    7,
                    18,
                    6,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-12T19:25:34Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    19,
                    25,
                    34,
                    1,
                    224,
                    0
                ],
                "title": "Decentralized Weather Forecasting via Distributed Machine Learning and\n  Blockchain-Based Model Validation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decentralized Weather Forecasting via Distributed Machine Learning and\n  Blockchain-Based Model Validation"
                },
                "summary": "Weather forecasting plays a vital role in disaster preparedness, agriculture,\nand resource management, yet current centralized forecasting systems are\nincreasingly strained by security vulnerabilities, limited scalability, and\nsusceptibility to single points of failure. To address these challenges, we\npropose a decentralized weather forecasting framework that integrates Federated\nLearning (FL) with blockchain technology. FL enables collaborative model\ntraining without exposing sensitive local data; this approach enhances privacy\nand reduces data transfer overhead. Meanwhile, the Ethereum blockchain ensures\ntransparent and dependable verification of model updates. To further enhance\nthe system's security, we introduce a reputation-based voting mechanism that\nassesses the trustworthiness of submitted models while utilizing the\nInterplanetary File System (IPFS) for efficient off-chain storage. Experimental\nresults demonstrate that our approach not only improves forecasting accuracy\nbut also enhances system resilience and scalability, making it a viable\ncandidate for deployment in real-world, security-critical environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Weather forecasting plays a vital role in disaster preparedness, agriculture,\nand resource management, yet current centralized forecasting systems are\nincreasingly strained by security vulnerabilities, limited scalability, and\nsusceptibility to single points of failure. To address these challenges, we\npropose a decentralized weather forecasting framework that integrates Federated\nLearning (FL) with blockchain technology. FL enables collaborative model\ntraining without exposing sensitive local data; this approach enhances privacy\nand reduces data transfer overhead. Meanwhile, the Ethereum blockchain ensures\ntransparent and dependable verification of model updates. To further enhance\nthe system's security, we introduce a reputation-based voting mechanism that\nassesses the trustworthiness of submitted models while utilizing the\nInterplanetary File System (IPFS) for efficient off-chain storage. Experimental\nresults demonstrate that our approach not only improves forecasting accuracy\nbut also enhances system resilience and scalability, making it a viable\ncandidate for deployment in real-world, security-critical environments."
                },
                "authors": [
                    {
                        "name": "Rilwan Umar"
                    },
                    {
                        "name": "Aydin Abadi"
                    },
                    {
                        "name": "Basil Aldali"
                    },
                    {
                        "name": "Benito Vincent"
                    },
                    {
                        "name": "Elliot A. J. Hurley"
                    },
                    {
                        "name": "Hotoon Aljazaeri"
                    },
                    {
                        "name": "Jamie Hedley-Cook"
                    },
                    {
                        "name": "Jamie-Lee Bell"
                    },
                    {
                        "name": "Lambert Uwuigbusun"
                    },
                    {
                        "name": "Mujeeb Ahmed"
                    },
                    {
                        "name": "Shishir Nagaraja"
                    },
                    {
                        "name": "Suleiman Sabo"
                    },
                    {
                        "name": "Weaam Alrbeiqi"
                    }
                ],
                "author_detail": {
                    "name": "Weaam Alrbeiqi"
                },
                "author": "Weaam Alrbeiqi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09299v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09299v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08512v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08512v3",
                "updated": "2025-08-14T07:15:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    7,
                    15,
                    48,
                    3,
                    226,
                    0
                ],
                "published": "2025-02-12T15:46:34Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    15,
                    46,
                    34,
                    2,
                    43,
                    0
                ],
                "title": "Measuring Diversity in Synthetic Datasets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring Diversity in Synthetic Datasets"
                },
                "summary": "Large language models (LLMs) are widely adopted to generate synthetic\ndatasets for various natural language processing (NLP) tasks, such as text\nclassification and summarization. However, accurately measuring the diversity\nof these synthetic datasets-an aspect crucial for robust model\nperformance-remains a significant challenge. In this paper, we introduce\nDCScore, a novel method for measuring synthetic dataset diversity from a\nclassification perspective. Specifically, DCScore formulates diversity\nevaluation as a sample classification task, leveraging mutual relationships\namong samples. We further provide theoretical verification of the\ndiversity-related axioms satisfied by DCScore, highlighting its role as a\nprincipled diversity evaluation method. Experimental results on synthetic\ndatasets reveal that DCScore enjoys a stronger correlation with multiple\ndiversity pseudo-truths of evaluated datasets, underscoring its effectiveness.\nMoreover, both empirical and theoretical evidence demonstrate that DCScore\nsubstantially reduces computational costs compared to existing methods. Code is\navailable at: https://github.com/bluewhalelab/dcscore.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are widely adopted to generate synthetic\ndatasets for various natural language processing (NLP) tasks, such as text\nclassification and summarization. However, accurately measuring the diversity\nof these synthetic datasets-an aspect crucial for robust model\nperformance-remains a significant challenge. In this paper, we introduce\nDCScore, a novel method for measuring synthetic dataset diversity from a\nclassification perspective. Specifically, DCScore formulates diversity\nevaluation as a sample classification task, leveraging mutual relationships\namong samples. We further provide theoretical verification of the\ndiversity-related axioms satisfied by DCScore, highlighting its role as a\nprincipled diversity evaluation method. Experimental results on synthetic\ndatasets reveal that DCScore enjoys a stronger correlation with multiple\ndiversity pseudo-truths of evaluated datasets, underscoring its effectiveness.\nMoreover, both empirical and theoretical evidence demonstrate that DCScore\nsubstantially reduces computational costs compared to existing methods. Code is\navailable at: https://github.com/bluewhalelab/dcscore."
                },
                "authors": [
                    {
                        "name": "Yuchang Zhu"
                    },
                    {
                        "name": "Huizhe Zhang"
                    },
                    {
                        "name": "Bingzhe Wu"
                    },
                    {
                        "name": "Jintang Li"
                    },
                    {
                        "name": "Zibin Zheng"
                    },
                    {
                        "name": "Peilin Zhao"
                    },
                    {
                        "name": "Liang Chen"
                    },
                    {
                        "name": "Yatao Bian"
                    }
                ],
                "author_detail": {
                    "name": "Yatao Bian"
                },
                "author": "Yatao Bian",
                "arxiv_comment": "Accepted by ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08512v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08512v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16770v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16770v2",
                "updated": "2025-08-14T07:15:21Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    7,
                    15,
                    21,
                    3,
                    226,
                    0
                ],
                "published": "2025-02-24T01:19:43Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    1,
                    19,
                    43,
                    0,
                    55,
                    0
                ],
                "title": "LED-Merging: Mitigating Safety-Utility Conflicts in Model Merging with\n  Location-Election-Disjoint",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LED-Merging: Mitigating Safety-Utility Conflicts in Model Merging with\n  Location-Election-Disjoint"
                },
                "summary": "Fine-tuning pre-trained Large Language Models (LLMs) for specialized tasks\nincurs substantial computational and data costs. While model merging offers a\ntraining-free solution to integrate multiple task-specific models, existing\nmethods suffer from safety-utility conflicts where enhanced general\ncapabilities degrade safety safeguards. We identify two root causes:\n$\\textbf{neuron misidentification}$ due to simplistic parameter magnitude-based\nselection, and $\\textbf{cross-task neuron interference}$ during merging. To\naddress these challenges, we propose $\\textbf{LED-Merging}$, a three-stage\nframework that $\\textbf{L}$ocates task-specific neurons via gradient-based\nattribution, dynamically $\\textbf{E}$lects critical neurons through multi-model\nimportance fusion, and $\\textbf{D}$isjoints conflicting updates through\nparameter isolation. Extensive experiments on Llama-3-8B, Mistral-7B, and\nLlama2-13B demonstrate that LED-Merging effectively reduces harmful response\nrates, showing a 31.4\\% decrease on Llama-3-8B-Instruct on HarmBench, while\nsimultaneously preserving 95\\% of utility performance, such as achieving\n52.39\\% accuracy on GSM8K. LED-Merging resolves safety-utility conflicts and\nprovides a lightweight, training-free paradigm for constructing reliable\nmulti-task LLMs. Code is available at\n$\\href{https://github.com/MqLeet/LED-Merging}{GitHub}$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning pre-trained Large Language Models (LLMs) for specialized tasks\nincurs substantial computational and data costs. While model merging offers a\ntraining-free solution to integrate multiple task-specific models, existing\nmethods suffer from safety-utility conflicts where enhanced general\ncapabilities degrade safety safeguards. We identify two root causes:\n$\\textbf{neuron misidentification}$ due to simplistic parameter magnitude-based\nselection, and $\\textbf{cross-task neuron interference}$ during merging. To\naddress these challenges, we propose $\\textbf{LED-Merging}$, a three-stage\nframework that $\\textbf{L}$ocates task-specific neurons via gradient-based\nattribution, dynamically $\\textbf{E}$lects critical neurons through multi-model\nimportance fusion, and $\\textbf{D}$isjoints conflicting updates through\nparameter isolation. Extensive experiments on Llama-3-8B, Mistral-7B, and\nLlama2-13B demonstrate that LED-Merging effectively reduces harmful response\nrates, showing a 31.4\\% decrease on Llama-3-8B-Instruct on HarmBench, while\nsimultaneously preserving 95\\% of utility performance, such as achieving\n52.39\\% accuracy on GSM8K. LED-Merging resolves safety-utility conflicts and\nprovides a lightweight, training-free paradigm for constructing reliable\nmulti-task LLMs. Code is available at\n$\\href{https://github.com/MqLeet/LED-Merging}{GitHub}$."
                },
                "authors": [
                    {
                        "name": "Qianli Ma"
                    },
                    {
                        "name": "Dongrui Liu"
                    },
                    {
                        "name": "Qian Chen"
                    },
                    {
                        "name": "Linfeng Zhang"
                    },
                    {
                        "name": "Jing Shao"
                    }
                ],
                "author_detail": {
                    "name": "Jing Shao"
                },
                "author": "Jing Shao",
                "arxiv_doi": "10.18653/v1/2025.acl-long.1055",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.18653/v1/2025.acl-long.1055",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.16770v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16770v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by ACL2025 main conference",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10404v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10404v1",
                "updated": "2025-08-14T07:12:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    7,
                    12,
                    44,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T07:12:44Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    7,
                    12,
                    44,
                    3,
                    226,
                    0
                ],
                "title": "Layer-Wise Perturbations via Sparse Autoencoders for Adversarial Text\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Layer-Wise Perturbations via Sparse Autoencoders for Adversarial Text\n  Generation"
                },
                "summary": "With the rapid proliferation of Natural Language Processing (NLP), especially\nLarge Language Models (LLMs), generating adversarial examples to jailbreak LLMs\nremains a key challenge for understanding model vulnerabilities and improving\nrobustness. In this context, we propose a new black-box attack method that\nleverages the interpretability of large models. We introduce the Sparse Feature\nPerturbation Framework (SFPF), a novel approach for adversarial text generation\nthat utilizes sparse autoencoders to identify and manipulate critical features\nin text. After using the SAE model to reconstruct hidden layer representations,\nwe perform feature clustering on the successfully attacked texts to identify\nfeatures with higher activations. These highly activated features are then\nperturbed to generate new adversarial texts. This selective perturbation\npreserves the malicious intent while amplifying safety signals, thereby\nincreasing their potential to evade existing defenses. Our method enables a new\nred-teaming strategy that balances adversarial effectiveness with safety\nalignment. Experimental results demonstrate that adversarial texts generated by\nSFPF can bypass state-of-the-art defense mechanisms, revealing persistent\nvulnerabilities in current NLP systems.However, the method's effectiveness\nvaries across prompts and layers, and its generalizability to other\narchitectures and larger models remains to be validated.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid proliferation of Natural Language Processing (NLP), especially\nLarge Language Models (LLMs), generating adversarial examples to jailbreak LLMs\nremains a key challenge for understanding model vulnerabilities and improving\nrobustness. In this context, we propose a new black-box attack method that\nleverages the interpretability of large models. We introduce the Sparse Feature\nPerturbation Framework (SFPF), a novel approach for adversarial text generation\nthat utilizes sparse autoencoders to identify and manipulate critical features\nin text. After using the SAE model to reconstruct hidden layer representations,\nwe perform feature clustering on the successfully attacked texts to identify\nfeatures with higher activations. These highly activated features are then\nperturbed to generate new adversarial texts. This selective perturbation\npreserves the malicious intent while amplifying safety signals, thereby\nincreasing their potential to evade existing defenses. Our method enables a new\nred-teaming strategy that balances adversarial effectiveness with safety\nalignment. Experimental results demonstrate that adversarial texts generated by\nSFPF can bypass state-of-the-art defense mechanisms, revealing persistent\nvulnerabilities in current NLP systems.However, the method's effectiveness\nvaries across prompts and layers, and its generalizability to other\narchitectures and larger models remains to be validated."
                },
                "authors": [
                    {
                        "name": "Huizhen Shu"
                    },
                    {
                        "name": "Xuying Li"
                    },
                    {
                        "name": "Qirui Wang"
                    },
                    {
                        "name": "Yuji Kosuga"
                    },
                    {
                        "name": "Mengqiu Tian"
                    },
                    {
                        "name": "Zhuo Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhuo Li"
                },
                "author": "Zhuo Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10404v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10404v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08909v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08909v2",
                "updated": "2025-08-14T07:12:38Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    7,
                    12,
                    38,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-12T12:58:12Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    12,
                    58,
                    12,
                    1,
                    224,
                    0
                ],
                "title": "Compass-Thinker-7B Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compass-Thinker-7B Technical Report"
                },
                "summary": "Recent R1-Zero-like research further demonstrates that reasoning extension\nhas given large language models (LLMs) unprecedented reasoning capabilities,\nand Reinforcement Learning is the core technology to elicit its complex\nreasoning. However, conducting RL experiments directly on hyperscale models\ninvolves high computational costs and resource demands, posing significant\nrisks. We propose the Compass-Thinker-7B model, which aims to explore the\npotential of Reinforcement Learning with less computational resources and\ncosts, and provides insights for further research into RL recipes for larger\nmodels. Compass-Thinker-7B is trained from an open source model through a\nspecially designed Reinforcement Learning Pipeline. We curate a dataset of 30k\nverifiable mathematics problems for the Reinforcement Learning Pipeline. By\nconfiguring data and training settings with different difficulty distributions\nfor different stages, the potential of the model is gradually released and the\ntraining efficiency is improved. Extensive evaluations show that\nCompass-Thinker-7B possesses exceptional reasoning potential, and achieves\nsuperior performance on mathematics compared to the same-sized RL model.\nEspecially in the challenging AIME2024 evaluation, Compass-Thinker-7B achieves\n40% accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent R1-Zero-like research further demonstrates that reasoning extension\nhas given large language models (LLMs) unprecedented reasoning capabilities,\nand Reinforcement Learning is the core technology to elicit its complex\nreasoning. However, conducting RL experiments directly on hyperscale models\ninvolves high computational costs and resource demands, posing significant\nrisks. We propose the Compass-Thinker-7B model, which aims to explore the\npotential of Reinforcement Learning with less computational resources and\ncosts, and provides insights for further research into RL recipes for larger\nmodels. Compass-Thinker-7B is trained from an open source model through a\nspecially designed Reinforcement Learning Pipeline. We curate a dataset of 30k\nverifiable mathematics problems for the Reinforcement Learning Pipeline. By\nconfiguring data and training settings with different difficulty distributions\nfor different stages, the potential of the model is gradually released and the\ntraining efficiency is improved. Extensive evaluations show that\nCompass-Thinker-7B possesses exceptional reasoning potential, and achieves\nsuperior performance on mathematics compared to the same-sized RL model.\nEspecially in the challenging AIME2024 evaluation, Compass-Thinker-7B achieves\n40% accuracy."
                },
                "authors": [
                    {
                        "name": "Anxiang Zeng"
                    },
                    {
                        "name": "Haibo Zhang"
                    },
                    {
                        "name": "Kaixiang Mo"
                    },
                    {
                        "name": "Long Zhang"
                    },
                    {
                        "name": "Shuman Liu"
                    },
                    {
                        "name": "Yanhui Huang"
                    },
                    {
                        "name": "Yawen Liu"
                    },
                    {
                        "name": "Yuepeng Sheng"
                    },
                    {
                        "name": "Yuwei Huang"
                    }
                ],
                "author_detail": {
                    "name": "Yuwei Huang"
                },
                "author": "Yuwei Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08909v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08909v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03810v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03810v2",
                "updated": "2025-08-14T07:02:58Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    7,
                    2,
                    58,
                    3,
                    226,
                    0
                ],
                "published": "2025-05-02T11:51:29Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    11,
                    51,
                    29,
                    4,
                    122,
                    0
                ],
                "title": "Grouped Sequency-arranged Rotation: Optimizing Rotation Transformation\n  for Quantization for Free",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Grouped Sequency-arranged Rotation: Optimizing Rotation Transformation\n  for Quantization for Free"
                },
                "summary": "Large Language Models (LLMs) face deployment challenges due to high\ncomputational costs, and while Post-Training Quantization (PTQ) offers a\nsolution, existing rotation-based methods struggle at very low bit-widths like\n2-bit. We introduce a novel, training-free approach to construct an improved\nrotation matrix, addressing the limitations of current methods. The key\ncontributions include leveraging the Walsh-Hadamard transform with sequency\nordering, which clusters similar frequency components to reduce quantization\nerror compared to standard Hadamard matrices, significantly improving\nperformance. Furthermore, we propose a Grouped Sequency-arranged Rotation (GSR)\nusing block-diagonal matrices with smaller Walsh blocks, effectively isolating\noutlier impacts and achieving performance comparable to optimization-based\nmethods without requiring any training. Our method demonstrates robust\nperformance on reasoning tasks and Perplexity (PPL) score on WikiText-2. Our\nmethod also enhances results even when applied over existing learned rotation\ntechniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) face deployment challenges due to high\ncomputational costs, and while Post-Training Quantization (PTQ) offers a\nsolution, existing rotation-based methods struggle at very low bit-widths like\n2-bit. We introduce a novel, training-free approach to construct an improved\nrotation matrix, addressing the limitations of current methods. The key\ncontributions include leveraging the Walsh-Hadamard transform with sequency\nordering, which clusters similar frequency components to reduce quantization\nerror compared to standard Hadamard matrices, significantly improving\nperformance. Furthermore, we propose a Grouped Sequency-arranged Rotation (GSR)\nusing block-diagonal matrices with smaller Walsh blocks, effectively isolating\noutlier impacts and achieving performance comparable to optimization-based\nmethods without requiring any training. Our method demonstrates robust\nperformance on reasoning tasks and Perplexity (PPL) score on WikiText-2. Our\nmethod also enhances results even when applied over existing learned rotation\ntechniques."
                },
                "authors": [
                    {
                        "name": "Euntae Choi"
                    },
                    {
                        "name": "Sumin Song"
                    },
                    {
                        "name": "Woosang Lim"
                    },
                    {
                        "name": "Sungjoo Yoo"
                    }
                ],
                "author_detail": {
                    "name": "Sungjoo Yoo"
                },
                "author": "Sungjoo Yoo",
                "arxiv_comment": "7 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03810v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03810v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05977v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05977v2",
                "updated": "2025-08-14T07:01:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    7,
                    1,
                    16,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-08T03:23:56Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    3,
                    23,
                    56,
                    4,
                    220,
                    0
                ],
                "title": "LinguaFluid: Language Guided Fluid Control via Semantic Rewards in\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LinguaFluid: Language Guided Fluid Control via Semantic Rewards in\n  Reinforcement Learning"
                },
                "summary": "In the domain of scientific machine learning, designing effective reward\nfunctions remains a challenge in reinforcement learning (RL), particularly in\nenvironments where task goals are difficult to specify numerically. Reward\nfunctions in existing work are predominantly based on heuristics, manual\nengineering, or task-specific tuning. In this work, we introduce a semantically\naligned reinforcement learning method where rewards are computed by aligning\nthe current state with a target semantic instruction using a\nSentence-Bidirectional Encoder Representations from Transformers (SBERT).\nInstead of relying on manually defined reward functions, the policy receives\nfeedback based on the reward, which is a cosine similarity between the goal\ntextual description and the statement description in the episode. We evaluated\nour approach in several environments and showed that semantic reward can guide\nlearning to achieve competitive control behavior, even in the absence of\nhand-crafted reward functions. Our study demonstrates a correlation between the\nlanguage embedding space and the conventional Euclidean space. This framework\nopens new horizons for aligning agent behavior with natural language goals and\nlays the groundwork for a more seamless integration of larger language models\n(LLMs) and fluid control applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the domain of scientific machine learning, designing effective reward\nfunctions remains a challenge in reinforcement learning (RL), particularly in\nenvironments where task goals are difficult to specify numerically. Reward\nfunctions in existing work are predominantly based on heuristics, manual\nengineering, or task-specific tuning. In this work, we introduce a semantically\naligned reinforcement learning method where rewards are computed by aligning\nthe current state with a target semantic instruction using a\nSentence-Bidirectional Encoder Representations from Transformers (SBERT).\nInstead of relying on manually defined reward functions, the policy receives\nfeedback based on the reward, which is a cosine similarity between the goal\ntextual description and the statement description in the episode. We evaluated\nour approach in several environments and showed that semantic reward can guide\nlearning to achieve competitive control behavior, even in the absence of\nhand-crafted reward functions. Our study demonstrates a correlation between the\nlanguage embedding space and the conventional Euclidean space. This framework\nopens new horizons for aligning agent behavior with natural language goals and\nlays the groundwork for a more seamless integration of larger language models\n(LLMs) and fluid control applications."
                },
                "authors": [
                    {
                        "name": "Aoming Liang"
                    },
                    {
                        "name": "Chi Cheng"
                    },
                    {
                        "name": "Dashuai Chen"
                    },
                    {
                        "name": "Boai Sun"
                    },
                    {
                        "name": "Dixia Fan"
                    }
                ],
                "author_detail": {
                    "name": "Dixia Fan"
                },
                "author": "Dixia Fan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05977v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05977v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10397v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10397v1",
                "updated": "2025-08-14T06:54:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    6,
                    54,
                    28,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T06:54:28Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    6,
                    54,
                    28,
                    3,
                    226,
                    0
                ],
                "title": "PQ-DAF: Pose-driven Quality-controlled Data Augmentation for Data-scarce\n  Driver Distraction Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PQ-DAF: Pose-driven Quality-controlled Data Augmentation for Data-scarce\n  Driver Distraction Detection"
                },
                "summary": "Driver distraction detection is essential for improving traffic safety and\nreducing road accidents. However, existing models often suffer from degraded\ngeneralization when deployed in real-world scenarios. This limitation primarily\narises from the few-shot learning challenge caused by the high cost of data\nannotation in practical environments, as well as the substantial domain shift\nbetween training datasets and target deployment conditions. To address these\nissues, we propose a Pose-driven Quality-controlled Data Augmentation Framework\n(PQ-DAF) that leverages a vision-language model for sample filtering to\ncost-effectively expand training data and enhance cross-domain robustness.\nSpecifically, we employ a Progressive Conditional Diffusion Model (PCDMs) to\naccurately capture key driver pose features and synthesize diverse training\nexamples. A sample quality assessment module, built upon the CogVLM\nvision-language model, is then introduced to filter out low-quality synthetic\nsamples based on a confidence threshold, ensuring the reliability of the\naugmented dataset. Extensive experiments demonstrate that PQ-DAF substantially\nimproves performance in few-shot driver distraction detection, achieving\nsignificant gains in model generalization under data-scarce conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Driver distraction detection is essential for improving traffic safety and\nreducing road accidents. However, existing models often suffer from degraded\ngeneralization when deployed in real-world scenarios. This limitation primarily\narises from the few-shot learning challenge caused by the high cost of data\nannotation in practical environments, as well as the substantial domain shift\nbetween training datasets and target deployment conditions. To address these\nissues, we propose a Pose-driven Quality-controlled Data Augmentation Framework\n(PQ-DAF) that leverages a vision-language model for sample filtering to\ncost-effectively expand training data and enhance cross-domain robustness.\nSpecifically, we employ a Progressive Conditional Diffusion Model (PCDMs) to\naccurately capture key driver pose features and synthesize diverse training\nexamples. A sample quality assessment module, built upon the CogVLM\nvision-language model, is then introduced to filter out low-quality synthetic\nsamples based on a confidence threshold, ensuring the reliability of the\naugmented dataset. Extensive experiments demonstrate that PQ-DAF substantially\nimproves performance in few-shot driver distraction detection, achieving\nsignificant gains in model generalization under data-scarce conditions."
                },
                "authors": [
                    {
                        "name": "Haibin Sun"
                    },
                    {
                        "name": "Xinghui Song"
                    }
                ],
                "author_detail": {
                    "name": "Xinghui Song"
                },
                "author": "Xinghui Song",
                "arxiv_comment": "11 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10397v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10397v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10395v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10395v1",
                "updated": "2025-08-14T06:52:38Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    6,
                    52,
                    38,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T06:52:38Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    6,
                    52,
                    38,
                    3,
                    226,
                    0
                ],
                "title": "XQuant: Breaking the Memory Wall for LLM Inference with KV Cache\n  Rematerialization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XQuant: Breaking the Memory Wall for LLM Inference with KV Cache\n  Rematerialization"
                },
                "summary": "Although LLM inference has emerged as a critical workload for many downstream\napplications, efficiently inferring LLMs is challenging due to the substantial\nmemory footprint and bandwidth requirements. In parallel, compute capabilities\nhave steadily outpaced both memory capacity and bandwidth over the last few\ndecades, a trend that remains evident in modern GPU hardware and exacerbates\nthe challenge of LLM inference. As such, new algorithms are emerging that trade\nincreased computation for reduced memory operations. To that end, we present\nXQuant, which takes advantage of this trend, enabling an order-of-magnitude\nreduction in memory consumption through low-bit quantization with substantial\naccuracy benefits relative to state-of-the-art KV cache quantization methods.\nWe accomplish this by quantizing and caching the layer input activations X,\ninstead of using standard KV caching, and then rematerializing the Keys and\nValues on-the-fly during inference. This results in an immediate 2$\\times$\nmemory savings compared to KV caching. By applying XQuant, we achieve up to\n$\\sim 7.7\\times$ memory savings with $<0.1$ perplexity degradation compared to\nthe FP16 baseline. Furthermore, our approach leverages the fact that X values\nare similar across layers. Building on this observation, we introduce\nXQuant-CL, which exploits the cross-layer similarity in the X embeddings for\nextreme compression. Across different models, XQuant-CL attains up to\n10$\\times$ memory savings relative to the FP16 baseline with only 0.01\nperplexity degradation, and 12.5$\\times$ memory savings with only $0.1$\nperplexity degradation. XQuant exploits the rapidly increasing compute\ncapabilities of hardware platforms to eliminate the memory bottleneck, while\nsurpassing state-of-the-art KV cache quantization methods and achieving\nnear-FP16 accuracy across a wide range of models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although LLM inference has emerged as a critical workload for many downstream\napplications, efficiently inferring LLMs is challenging due to the substantial\nmemory footprint and bandwidth requirements. In parallel, compute capabilities\nhave steadily outpaced both memory capacity and bandwidth over the last few\ndecades, a trend that remains evident in modern GPU hardware and exacerbates\nthe challenge of LLM inference. As such, new algorithms are emerging that trade\nincreased computation for reduced memory operations. To that end, we present\nXQuant, which takes advantage of this trend, enabling an order-of-magnitude\nreduction in memory consumption through low-bit quantization with substantial\naccuracy benefits relative to state-of-the-art KV cache quantization methods.\nWe accomplish this by quantizing and caching the layer input activations X,\ninstead of using standard KV caching, and then rematerializing the Keys and\nValues on-the-fly during inference. This results in an immediate 2$\\times$\nmemory savings compared to KV caching. By applying XQuant, we achieve up to\n$\\sim 7.7\\times$ memory savings with $<0.1$ perplexity degradation compared to\nthe FP16 baseline. Furthermore, our approach leverages the fact that X values\nare similar across layers. Building on this observation, we introduce\nXQuant-CL, which exploits the cross-layer similarity in the X embeddings for\nextreme compression. Across different models, XQuant-CL attains up to\n10$\\times$ memory savings relative to the FP16 baseline with only 0.01\nperplexity degradation, and 12.5$\\times$ memory savings with only $0.1$\nperplexity degradation. XQuant exploits the rapidly increasing compute\ncapabilities of hardware platforms to eliminate the memory bottleneck, while\nsurpassing state-of-the-art KV cache quantization methods and achieving\nnear-FP16 accuracy across a wide range of models."
                },
                "authors": [
                    {
                        "name": "Aditya Tomar"
                    },
                    {
                        "name": "Coleman Hooper"
                    },
                    {
                        "name": "Minjae Lee"
                    },
                    {
                        "name": "Haocheng Xi"
                    },
                    {
                        "name": "Rishabh Tiwari"
                    },
                    {
                        "name": "Wonjun Kang"
                    },
                    {
                        "name": "Luca Manolache"
                    },
                    {
                        "name": "Michael W. Mahoney"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Amir Gholami"
                    }
                ],
                "author_detail": {
                    "name": "Amir Gholami"
                },
                "author": "Amir Gholami",
                "arxiv_comment": "24 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10395v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10395v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10390v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10390v1",
                "updated": "2025-08-14T06:46:56Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    6,
                    46,
                    56,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T06:46:56Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    6,
                    46,
                    56,
                    3,
                    226,
                    0
                ],
                "title": "Jailbreaking Commercial Black-Box LLMs with Explicitly Harmful Prompts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jailbreaking Commercial Black-Box LLMs with Explicitly Harmful Prompts"
                },
                "summary": "Evaluating jailbreak attacks is challenging when prompts are not overtly\nharmful or fail to induce harmful outputs. Unfortunately, many existing\nred-teaming datasets contain such unsuitable prompts. To evaluate attacks\naccurately, these datasets need to be assessed and cleaned for maliciousness.\nHowever, existing malicious content detection methods rely on either manual\nannotation, which is labor-intensive, or large language models (LLMs), which\nhave inconsistent accuracy in harmful types. To balance accuracy and\nefficiency, we propose a hybrid evaluation framework named MDH (Malicious\ncontent Detection based on LLMs with Human assistance) that combines LLM-based\nannotation with minimal human oversight, and apply it to dataset cleaning and\ndetection of jailbroken responses. Furthermore, we find that well-crafted\ndeveloper messages can significantly boost jailbreak success, leading us to\npropose two new strategies: D-Attack, which leverages context simulation, and\nDH-CoT, which incorporates hijacked chains of thought. The Codes, datasets,\njudgements, and detection results will be released in github repository:\nhttps://github.com/AlienZhang1996/DH-CoT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating jailbreak attacks is challenging when prompts are not overtly\nharmful or fail to induce harmful outputs. Unfortunately, many existing\nred-teaming datasets contain such unsuitable prompts. To evaluate attacks\naccurately, these datasets need to be assessed and cleaned for maliciousness.\nHowever, existing malicious content detection methods rely on either manual\nannotation, which is labor-intensive, or large language models (LLMs), which\nhave inconsistent accuracy in harmful types. To balance accuracy and\nefficiency, we propose a hybrid evaluation framework named MDH (Malicious\ncontent Detection based on LLMs with Human assistance) that combines LLM-based\nannotation with minimal human oversight, and apply it to dataset cleaning and\ndetection of jailbroken responses. Furthermore, we find that well-crafted\ndeveloper messages can significantly boost jailbreak success, leading us to\npropose two new strategies: D-Attack, which leverages context simulation, and\nDH-CoT, which incorporates hijacked chains of thought. The Codes, datasets,\njudgements, and detection results will be released in github repository:\nhttps://github.com/AlienZhang1996/DH-CoT."
                },
                "authors": [
                    {
                        "name": "Chiyu Zhang"
                    },
                    {
                        "name": "Lu Zhou"
                    },
                    {
                        "name": "Xiaogang Xu"
                    },
                    {
                        "name": "Jiafei Wu"
                    },
                    {
                        "name": "Liming Fang"
                    },
                    {
                        "name": "Zhe Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zhe Liu"
                },
                "author": "Zhe Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10390v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10390v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.21817v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.21817v3",
                "updated": "2025-08-14T06:16:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    6,
                    16,
                    52,
                    3,
                    226,
                    0
                ],
                "published": "2025-07-29T13:51:46Z",
                "published_parsed": [
                    2025,
                    7,
                    29,
                    13,
                    51,
                    46,
                    1,
                    210,
                    0
                ],
                "title": "Out of Distribution, Out of Luck: How Well Can LLMs Trained on\n  Vulnerability Datasets Detect Top 25 CWE Weaknesses?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Out of Distribution, Out of Luck: How Well Can LLMs Trained on\n  Vulnerability Datasets Detect Top 25 CWE Weaknesses?"
                },
                "summary": "Automated vulnerability detection research has made substantial progress, yet\nits real-world impact remains limited. Current vulnerability datasets suffer\nfrom issues including label inaccuracy rates of 20-71%, extensive duplication,\nand poor coverage of critical CWE types. These issues create a significant\n\"generalization gap\" where models achieve misleading self-testing performance\n(measured on held-out data from the same dataset for training) by exploiting\nspurious correlations rather than learning true vulnerability patterns. Our\nanalysis reveals that many models experience substantial performance drops of\nup to 33% when evaluated on independent data, with some performing close to\nrandom guessing. To address these limitations, we present a three-part\nsolution. First, we introduce a manually curated test dataset, BenchVul,\ncovering the MITRE Top 25 Most Dangerous CWEs. Second, we construct a\nhigh-quality training dataset, TitanVul, comprising 38,863 functions by\naggregating seven public sources and applying deduplication and validation\nusing a novel multi-agent LLM framework. Third, we propose a Realistic\nVulnerability Generation (RVG) framework, which synthesizes context-aware\nvulnerability examples for underrepresented but critical CWE types through\nsimulated development workflows. Our evaluation shows the strengths of each\ncomponent in closing the generalization gap. First, BenchVul shows the\nlimitations of self-testing: models trained on existing datasets, such as\nBigVul and CVEfixes, experience performance drops on BenchVul (from 0.776 to\n0.519 and from 0.713 to 0.607). Second, training models on TitanVul\ndemonstrates improved generalization, with model performance increasing from\n0.584 when evaluated on the same dataset to 0.767 when tested on BenchVul.\nThird, supplementing TitanVul with RVG-generated data yields further gains,\nincreasing model performance by 14.0% to 0.874.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated vulnerability detection research has made substantial progress, yet\nits real-world impact remains limited. Current vulnerability datasets suffer\nfrom issues including label inaccuracy rates of 20-71%, extensive duplication,\nand poor coverage of critical CWE types. These issues create a significant\n\"generalization gap\" where models achieve misleading self-testing performance\n(measured on held-out data from the same dataset for training) by exploiting\nspurious correlations rather than learning true vulnerability patterns. Our\nanalysis reveals that many models experience substantial performance drops of\nup to 33% when evaluated on independent data, with some performing close to\nrandom guessing. To address these limitations, we present a three-part\nsolution. First, we introduce a manually curated test dataset, BenchVul,\ncovering the MITRE Top 25 Most Dangerous CWEs. Second, we construct a\nhigh-quality training dataset, TitanVul, comprising 38,863 functions by\naggregating seven public sources and applying deduplication and validation\nusing a novel multi-agent LLM framework. Third, we propose a Realistic\nVulnerability Generation (RVG) framework, which synthesizes context-aware\nvulnerability examples for underrepresented but critical CWE types through\nsimulated development workflows. Our evaluation shows the strengths of each\ncomponent in closing the generalization gap. First, BenchVul shows the\nlimitations of self-testing: models trained on existing datasets, such as\nBigVul and CVEfixes, experience performance drops on BenchVul (from 0.776 to\n0.519 and from 0.713 to 0.607). Second, training models on TitanVul\ndemonstrates improved generalization, with model performance increasing from\n0.584 when evaluated on the same dataset to 0.767 when tested on BenchVul.\nThird, supplementing TitanVul with RVG-generated data yields further gains,\nincreasing model performance by 14.0% to 0.874."
                },
                "authors": [
                    {
                        "name": "Yikun Li"
                    },
                    {
                        "name": "Ngoc Tan Bui"
                    },
                    {
                        "name": "Ting Zhang"
                    },
                    {
                        "name": "Martin Weyssow"
                    },
                    {
                        "name": "Chengran Yang"
                    },
                    {
                        "name": "Xin Zhou"
                    },
                    {
                        "name": "Jinfeng Jiang"
                    },
                    {
                        "name": "Junkai Chen"
                    },
                    {
                        "name": "Huihui Huang"
                    },
                    {
                        "name": "Huu Hung Nguyen"
                    },
                    {
                        "name": "Chiok Yew Ho"
                    },
                    {
                        "name": "Jie Tan"
                    },
                    {
                        "name": "Ruiyin Li"
                    },
                    {
                        "name": "Yide Yin"
                    },
                    {
                        "name": "Han Wei Ang"
                    },
                    {
                        "name": "Frank Liauw"
                    },
                    {
                        "name": "Eng Lieh Ouh"
                    },
                    {
                        "name": "Lwin Khin Shar"
                    },
                    {
                        "name": "David Lo"
                    }
                ],
                "author_detail": {
                    "name": "David Lo"
                },
                "author": "David Lo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.21817v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.21817v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.18405v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.18405v3",
                "updated": "2025-08-14T06:13:17Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    6,
                    13,
                    17,
                    3,
                    226,
                    0
                ],
                "published": "2024-03-27T09:46:56Z",
                "published_parsed": [
                    2024,
                    3,
                    27,
                    9,
                    46,
                    56,
                    2,
                    87,
                    0
                ],
                "title": "Leveraging Large Language Models for Relevance Judgments in Legal Case\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Large Language Models for Relevance Judgments in Legal Case\n  Retrieval"
                },
                "summary": "Determining which legal cases are relevant to a given query involves\nnavigating lengthy texts and applying nuanced legal reasoning. Traditionally,\nthis task has demanded significant time and domain expertise to identify key\nLegal Facts and reach sound juridical conclusions. In addition, existing data\nwith legal case similarities often lack interpretability, making it difficult\nto understand the rationale behind relevance judgments. With the growing\ncapabilities of large language models (LLMs), researchers have begun\ninvestigating their potential in this domain. Nonetheless, the method of\nemploying a general large language model for reliable relevance judgments in\nlegal case retrieval remains largely unexplored. To address this gap in\nresearch, we propose a novel few-shot approach where LLMs assist in generating\nexpert-aligned interpretable relevance judgments. The proposed approach\ndecomposes the judgment process into several stages, mimicking the workflow of\nhuman annotators and allowing for the flexible incorporation of expert\nreasoning to improve the accuracy of relevance judgments. Importantly, it also\nensures interpretable data labeling, providing transparency and clarity in the\nrelevance assessment process. Through a comparison of relevance judgments made\nby LLMs and human experts, we empirically demonstrate that the proposed\napproach can yield reliable and valid relevance assessments. Furthermore, we\ndemonstrate that with minimal expert supervision, our approach enables a large\nlanguage model to acquire case analysis expertise and subsequently transfers\nthis ability to a smaller model via annotation-based knowledge distillation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Determining which legal cases are relevant to a given query involves\nnavigating lengthy texts and applying nuanced legal reasoning. Traditionally,\nthis task has demanded significant time and domain expertise to identify key\nLegal Facts and reach sound juridical conclusions. In addition, existing data\nwith legal case similarities often lack interpretability, making it difficult\nto understand the rationale behind relevance judgments. With the growing\ncapabilities of large language models (LLMs), researchers have begun\ninvestigating their potential in this domain. Nonetheless, the method of\nemploying a general large language model for reliable relevance judgments in\nlegal case retrieval remains largely unexplored. To address this gap in\nresearch, we propose a novel few-shot approach where LLMs assist in generating\nexpert-aligned interpretable relevance judgments. The proposed approach\ndecomposes the judgment process into several stages, mimicking the workflow of\nhuman annotators and allowing for the flexible incorporation of expert\nreasoning to improve the accuracy of relevance judgments. Importantly, it also\nensures interpretable data labeling, providing transparency and clarity in the\nrelevance assessment process. Through a comparison of relevance judgments made\nby LLMs and human experts, we empirically demonstrate that the proposed\napproach can yield reliable and valid relevance assessments. Furthermore, we\ndemonstrate that with minimal expert supervision, our approach enables a large\nlanguage model to acquire case analysis expertise and subsequently transfers\nthis ability to a smaller model via annotation-based knowledge distillation."
                },
                "authors": [
                    {
                        "name": "Shengjie Ma"
                    },
                    {
                        "name": "Qi Chu"
                    },
                    {
                        "name": "Jiaxin Mao"
                    },
                    {
                        "name": "Xuhui Jiang"
                    },
                    {
                        "name": "Haozhe Duan"
                    },
                    {
                        "name": "Chong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Chong Chen"
                },
                "author": "Chong Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.18405v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.18405v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10369v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10369v1",
                "updated": "2025-08-14T06:07:53Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    6,
                    7,
                    53,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T06:07:53Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    6,
                    7,
                    53,
                    3,
                    226,
                    0
                ],
                "title": "Improving Generative Cross-lingual Aspect-Based Sentiment Analysis with\n  Constrained Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Generative Cross-lingual Aspect-Based Sentiment Analysis with\n  Constrained Decoding"
                },
                "summary": "While aspect-based sentiment analysis (ABSA) has made substantial progress,\nchallenges remain for low-resource languages, which are often overlooked in\nfavour of English. Current cross-lingual ABSA approaches focus on limited, less\ncomplex tasks and often rely on external translation tools. This paper\nintroduces a novel approach using constrained decoding with\nsequence-to-sequence models, eliminating the need for unreliable translation\ntools and improving cross-lingual performance by 5\\% on average for the most\ncomplex task. The proposed method also supports multi-tasking, which enables\nsolving multiple ABSA tasks with a single model, with constrained decoding\nboosting results by more than 10\\%.\n  We evaluate our approach across seven languages and six ABSA tasks,\nsurpassing state-of-the-art methods and setting new benchmarks for previously\nunexplored tasks. Additionally, we assess large language models (LLMs) in\nzero-shot, few-shot, and fine-tuning scenarios. While LLMs perform poorly in\nzero-shot and few-shot settings, fine-tuning achieves competitive results\ncompared to smaller multilingual models, albeit at the cost of longer training\nand inference times.\n  We provide practical recommendations for real-world applications, enhancing\nthe understanding of cross-lingual ABSA methodologies. This study offers\nvaluable insights into the strengths and limitations of cross-lingual ABSA\napproaches, advancing the state-of-the-art in this challenging research domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While aspect-based sentiment analysis (ABSA) has made substantial progress,\nchallenges remain for low-resource languages, which are often overlooked in\nfavour of English. Current cross-lingual ABSA approaches focus on limited, less\ncomplex tasks and often rely on external translation tools. This paper\nintroduces a novel approach using constrained decoding with\nsequence-to-sequence models, eliminating the need for unreliable translation\ntools and improving cross-lingual performance by 5\\% on average for the most\ncomplex task. The proposed method also supports multi-tasking, which enables\nsolving multiple ABSA tasks with a single model, with constrained decoding\nboosting results by more than 10\\%.\n  We evaluate our approach across seven languages and six ABSA tasks,\nsurpassing state-of-the-art methods and setting new benchmarks for previously\nunexplored tasks. Additionally, we assess large language models (LLMs) in\nzero-shot, few-shot, and fine-tuning scenarios. While LLMs perform poorly in\nzero-shot and few-shot settings, fine-tuning achieves competitive results\ncompared to smaller multilingual models, albeit at the cost of longer training\nand inference times.\n  We provide practical recommendations for real-world applications, enhancing\nthe understanding of cross-lingual ABSA methodologies. This study offers\nvaluable insights into the strengths and limitations of cross-lingual ABSA\napproaches, advancing the state-of-the-art in this challenging research domain."
                },
                "authors": [
                    {
                        "name": "Jakub md"
                    },
                    {
                        "name": "Pavel Pib"
                    },
                    {
                        "name": "Pavel Krl"
                    }
                ],
                "author_detail": {
                    "name": "Pavel Krl"
                },
                "author": "Pavel Krl",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10369v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10369v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]