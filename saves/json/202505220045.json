[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2504.15364v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15364v3",
                "updated": "2025-05-20T17:50:11Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    17,
                    50,
                    11,
                    1,
                    140,
                    0
                ],
                "published": "2025-04-21T18:12:46Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    18,
                    12,
                    46,
                    0,
                    111,
                    0
                ],
                "title": "KeyDiff: Key Similarity-Based KV Cache Eviction for Long-Context LLM\n  Inference in Resource-Constrained Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KeyDiff: Key Similarity-Based KV Cache Eviction for Long-Context LLM\n  Inference in Resource-Constrained Environments"
                },
                "summary": "We demonstrate that geometrically distinctive keys during LLM inference tend\nto have high attention scores. Based on the phenomenon we propose KeyDiff, a\ntraining-free KV cache eviction method based solely on key similarity. Unlike\nother KV cache eviction methods, KeyDiff can process arbitrarily long prompts\nwithin strict resource constraints and efficiently generate responses. We\nprovide a theoretical basis for KeyDiff by relating key diversity with\nattention scores. These results imply KeyDiff can efficiently identify the most\nimportant tokens to retain. Notably KeyDiff does not rely on attention scores,\nallowing the use of optimized attention mechanisms like FlashAttention. Under a\nstrict memory allowance, we demonstrate the effectiveness of KeyDiff for the\nLlama and Qwen model families by observing a performance gap of less than 0.04%\nwith 8K cache budget ($\\sim$23% KV cache reduction) from the non-evicting\nbaseline on LongBench for Llama 3.1-8B and Llama 3.2-3B. We also observe near\nbaseline performance for Deepseek-R1-Distill-Llama-8B on the Math500 reasoning\nbenchmark and decrease end-to-end inference latency by up to 30% compared to\nthe other token-eviction methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We demonstrate that geometrically distinctive keys during LLM inference tend\nto have high attention scores. Based on the phenomenon we propose KeyDiff, a\ntraining-free KV cache eviction method based solely on key similarity. Unlike\nother KV cache eviction methods, KeyDiff can process arbitrarily long prompts\nwithin strict resource constraints and efficiently generate responses. We\nprovide a theoretical basis for KeyDiff by relating key diversity with\nattention scores. These results imply KeyDiff can efficiently identify the most\nimportant tokens to retain. Notably KeyDiff does not rely on attention scores,\nallowing the use of optimized attention mechanisms like FlashAttention. Under a\nstrict memory allowance, we demonstrate the effectiveness of KeyDiff for the\nLlama and Qwen model families by observing a performance gap of less than 0.04%\nwith 8K cache budget ($\\sim$23% KV cache reduction) from the non-evicting\nbaseline on LongBench for Llama 3.1-8B and Llama 3.2-3B. We also observe near\nbaseline performance for Deepseek-R1-Distill-Llama-8B on the Math500 reasoning\nbenchmark and decrease end-to-end inference latency by up to 30% compared to\nthe other token-eviction methods."
                },
                "authors": [
                    {
                        "name": "Junyoung Park"
                    },
                    {
                        "name": "Dalton Jones"
                    },
                    {
                        "name": "Matthew J Morse"
                    },
                    {
                        "name": "Raghavv Goel"
                    },
                    {
                        "name": "Mingu Lee"
                    },
                    {
                        "name": "Chris Lott"
                    }
                ],
                "author_detail": {
                    "name": "Chris Lott"
                },
                "author": "Chris Lott",
                "arxiv_comment": "9 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15364v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15364v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07115v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07115v4",
                "updated": "2025-05-20T16:29:52Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    16,
                    29,
                    52,
                    1,
                    140,
                    0
                ],
                "published": "2025-02-10T23:11:44Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    23,
                    11,
                    44,
                    0,
                    41,
                    0
                ],
                "title": "Online Scheduling for LLM Inference with KV Cache Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Scheduling for LLM Inference with KV Cache Constraints"
                },
                "summary": "Large Language Model (LLM) inference, where a trained model generates text\none word at a time in response to user prompts, is a computationally intensive\nprocess requiring efficient scheduling to optimize latency and resource\nutilization. A key challenge in LLM inference is the management of the\nKey-Value (KV) cache, which reduces redundant computations but introduces\nmemory constraints. In this work, we model LLM inference with KV cache\nconstraints theoretically and propose a novel batching and scheduling algorithm\nthat minimizes inference latency while effectively managing the KV cache's\nmemory.\n  More specifically, we make the following contributions. First, to evaluate\nthe performance of online algorithms for scheduling in LLM inference, we\nintroduce a hindsight optimal benchmark, formulated as an integer program that\ncomputes the minimum total inference latency under full future information.\nSecond, we prove that no deterministic online algorithm can achieve a constant\ncompetitive ratio when the arrival process is arbitrary. Third, motivated by\nthe computational intractability of solving the integer program at scale, we\npropose a polynomial-time online scheduling algorithm and show that under\ncertain conditions it can achieve a constant competitive ratio. We also\ndemonstrate our algorithm's strong empirical performance by comparing it to the\nhindsight optimal in a synthetic dataset. Finally, we conduct empirical\nevaluations on a real-world public LLM inference dataset, simulating the\nLlama2-70B model on A100 GPUs, and show that our algorithm significantly\noutperforms the benchmark algorithms. Overall, our results offer a path toward\nmore sustainable and cost-effective LLM deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference, where a trained model generates text\none word at a time in response to user prompts, is a computationally intensive\nprocess requiring efficient scheduling to optimize latency and resource\nutilization. A key challenge in LLM inference is the management of the\nKey-Value (KV) cache, which reduces redundant computations but introduces\nmemory constraints. In this work, we model LLM inference with KV cache\nconstraints theoretically and propose a novel batching and scheduling algorithm\nthat minimizes inference latency while effectively managing the KV cache's\nmemory.\n  More specifically, we make the following contributions. First, to evaluate\nthe performance of online algorithms for scheduling in LLM inference, we\nintroduce a hindsight optimal benchmark, formulated as an integer program that\ncomputes the minimum total inference latency under full future information.\nSecond, we prove that no deterministic online algorithm can achieve a constant\ncompetitive ratio when the arrival process is arbitrary. Third, motivated by\nthe computational intractability of solving the integer program at scale, we\npropose a polynomial-time online scheduling algorithm and show that under\ncertain conditions it can achieve a constant competitive ratio. We also\ndemonstrate our algorithm's strong empirical performance by comparing it to the\nhindsight optimal in a synthetic dataset. Finally, we conduct empirical\nevaluations on a real-world public LLM inference dataset, simulating the\nLlama2-70B model on A100 GPUs, and show that our algorithm significantly\noutperforms the benchmark algorithms. Overall, our results offer a path toward\nmore sustainable and cost-effective LLM deployment."
                },
                "authors": [
                    {
                        "name": "Patrick Jaillet"
                    },
                    {
                        "name": "Jiashuo Jiang"
                    },
                    {
                        "name": "Konstantina Mellou"
                    },
                    {
                        "name": "Marco Molinaro"
                    },
                    {
                        "name": "Chara Podimata"
                    },
                    {
                        "name": "Zijie Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Zijie Zhou"
                },
                "author": "Zijie Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07115v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07115v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14427v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14427v1",
                "updated": "2025-05-20T14:38:34Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    14,
                    38,
                    34,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T14:38:34Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    14,
                    38,
                    34,
                    1,
                    140,
                    0
                ],
                "title": "SkyMemory: A LEO Edge Cache for Transformer Inference Optimization and\n  Scale Out",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SkyMemory: A LEO Edge Cache for Transformer Inference Optimization and\n  Scale Out"
                },
                "summary": "We expand the scope of cache memory to include LEO constellations, which are\nhighly distributed systems with thousands of satellites connected with\nfree-space optics inter-satellite links (ISL) always only one hop from any\npoint on earth. We show how to increase the number of cache hits and improve\nthe speed of inference for the important use case of LLMs. These benefits apply\nnot only to LLMs, both terrestrially hosted and on satellites, but also\ngeneralize to any cache distributed over multiple locations that needs to be\naccessed in a timely manner. We show the benefit of our key value cache (KVC)\nprotocol in simulations and present a proof-of-concept implementation of the\nprotocol for KVCs on a testbed comprising 5 Intel NUC Linux mini PCs hosting a\n19x5 constellation, with an NVIDIA Jetson Nano 8GB GPU hosting the LLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We expand the scope of cache memory to include LEO constellations, which are\nhighly distributed systems with thousands of satellites connected with\nfree-space optics inter-satellite links (ISL) always only one hop from any\npoint on earth. We show how to increase the number of cache hits and improve\nthe speed of inference for the important use case of LLMs. These benefits apply\nnot only to LLMs, both terrestrially hosted and on satellites, but also\ngeneralize to any cache distributed over multiple locations that needs to be\naccessed in a timely manner. We show the benefit of our key value cache (KVC)\nprotocol in simulations and present a proof-of-concept implementation of the\nprotocol for KVCs on a testbed comprising 5 Intel NUC Linux mini PCs hosting a\n19x5 constellation, with an NVIDIA Jetson Nano 8GB GPU hosting the LLM."
                },
                "authors": [
                    {
                        "name": "Thomas Sandholm"
                    },
                    {
                        "name": "Sayandev Mukherjee"
                    },
                    {
                        "name": "Lin Cheng"
                    },
                    {
                        "name": "Bernardo A. Huberman"
                    }
                ],
                "author_detail": {
                    "name": "Bernardo A. Huberman"
                },
                "author": "Bernardo A. Huberman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14427v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14427v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14398v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14398v1",
                "updated": "2025-05-20T14:14:38Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    14,
                    14,
                    38,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T14:14:38Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    14,
                    14,
                    38,
                    1,
                    140,
                    0
                ],
                "title": "Log-Augmented Generation: Scaling Test-Time Reasoning with Reusable\n  Computation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Log-Augmented Generation: Scaling Test-Time Reasoning with Reusable\n  Computation"
                },
                "summary": "While humans naturally learn and adapt from past experiences, large language\nmodels (LLMs) and their agentic counterparts struggle to retain reasoning from\nprevious tasks and apply them in future contexts. To address this limitation,\nwe propose a novel framework, log-augmented generation (LAG) that directly\nreuses prior computation and reasoning from past logs at test time to enhance\nmodel's ability to learn from previous tasks and perform better on new, unseen\nchallenges, all while keeping the system efficient and scalable. Specifically,\nour system represents task logs using key-value (KV) caches, encoding the full\nreasoning context of prior tasks while storing KV caches for only a selected\nsubset of tokens. When a new task arises, LAG retrieves the KV values from\nrelevant logs to augment generation. Our approach differs from reflection-based\nmemory mechanisms by directly reusing prior reasoning and computations without\nrequiring additional steps for knowledge extraction or distillation. Our method\nalso goes beyond existing KV caching techniques, which primarily target\nefficiency gains rather than improving accuracy. Experiments on knowledge- and\nreasoning-intensive datasets demonstrate that our method significantly\noutperforms standard agentic systems that do not utilize logs, as well as\nexisting solutions based on reflection and KV cache techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While humans naturally learn and adapt from past experiences, large language\nmodels (LLMs) and their agentic counterparts struggle to retain reasoning from\nprevious tasks and apply them in future contexts. To address this limitation,\nwe propose a novel framework, log-augmented generation (LAG) that directly\nreuses prior computation and reasoning from past logs at test time to enhance\nmodel's ability to learn from previous tasks and perform better on new, unseen\nchallenges, all while keeping the system efficient and scalable. Specifically,\nour system represents task logs using key-value (KV) caches, encoding the full\nreasoning context of prior tasks while storing KV caches for only a selected\nsubset of tokens. When a new task arises, LAG retrieves the KV values from\nrelevant logs to augment generation. Our approach differs from reflection-based\nmemory mechanisms by directly reusing prior reasoning and computations without\nrequiring additional steps for knowledge extraction or distillation. Our method\nalso goes beyond existing KV caching techniques, which primarily target\nefficiency gains rather than improving accuracy. Experiments on knowledge- and\nreasoning-intensive datasets demonstrate that our method significantly\noutperforms standard agentic systems that do not utilize logs, as well as\nexisting solutions based on reflection and KV cache techniques."
                },
                "authors": [
                    {
                        "name": "Peter Baile Chen"
                    },
                    {
                        "name": "Yi Zhang"
                    },
                    {
                        "name": "Dan Roth"
                    },
                    {
                        "name": "Samuel Madden"
                    },
                    {
                        "name": "Jacob Andreas"
                    },
                    {
                        "name": "Michael Cafarella"
                    }
                ],
                "author_detail": {
                    "name": "Michael Cafarella"
                },
                "author": "Michael Cafarella",
                "arxiv_comment": "Data and code are available at https://peterbaile.github.io/lag/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14398v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14398v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14085v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14085v1",
                "updated": "2025-05-20T08:46:23Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    8,
                    46,
                    23,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T08:46:23Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    8,
                    46,
                    23,
                    1,
                    140,
                    0
                ],
                "title": "CE-LSLM: Efficient Large-Small Language Model Inference and\n  Communication via Cloud-Edge Collaboration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CE-LSLM: Efficient Large-Small Language Model Inference and\n  Communication via Cloud-Edge Collaboration"
                },
                "summary": "Emerging intelligent service scenarios in 6G communication impose stringent\nrequirements for low latency, high reliability, and privacy preservation.\nGenerative large language models (LLMs) are gradually becoming key enablers for\nthe integration of semantic communication and computation. However, due to the\nlimited computational resources of edge devices and the increasing complexity\nof heterogeneous terminal access, existing centralized inference approaches\nfail to meet the dual demands of response efficiency and data privacy in\nedge-side inference tasks. To address these challenges, this paper proposes a\nnovel collaborative inference architecture that integrates cloud-based LLMs\nwith edge-deployed small language models (SLMs), enabling dynamic scheduling\nand sharing of semantic-level intermediate states, and establishing a unified\ncomputation-communication paradigm tailored for 6G networks. Specifically, a\nkey-value (KV) cache reuse mechanism is introduced to enhance the semantic\nunderstanding of edge models through contextual guidance from the cloud, while\nsignificantly reducing edge-side computational and storage overhead.\nFurthermore, a cross-node parallel scheduling mechanism is proposed to achieve\nasynchronous coordination between model state loading and decoding computation,\nthereby improving edge responsiveness. In addition, we investigate layer\nalignment and representation compression strategies between heterogeneous\nmodels to alleviate the communication burden on the edge. Experimental results\ndemonstrate that the proposed architecture exhibits superior adaptability and\nscalability in terms of inference latency, system stability, and concurrent\nprocessing capacity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emerging intelligent service scenarios in 6G communication impose stringent\nrequirements for low latency, high reliability, and privacy preservation.\nGenerative large language models (LLMs) are gradually becoming key enablers for\nthe integration of semantic communication and computation. However, due to the\nlimited computational resources of edge devices and the increasing complexity\nof heterogeneous terminal access, existing centralized inference approaches\nfail to meet the dual demands of response efficiency and data privacy in\nedge-side inference tasks. To address these challenges, this paper proposes a\nnovel collaborative inference architecture that integrates cloud-based LLMs\nwith edge-deployed small language models (SLMs), enabling dynamic scheduling\nand sharing of semantic-level intermediate states, and establishing a unified\ncomputation-communication paradigm tailored for 6G networks. Specifically, a\nkey-value (KV) cache reuse mechanism is introduced to enhance the semantic\nunderstanding of edge models through contextual guidance from the cloud, while\nsignificantly reducing edge-side computational and storage overhead.\nFurthermore, a cross-node parallel scheduling mechanism is proposed to achieve\nasynchronous coordination between model state loading and decoding computation,\nthereby improving edge responsiveness. In addition, we investigate layer\nalignment and representation compression strategies between heterogeneous\nmodels to alleviate the communication burden on the edge. Experimental results\ndemonstrate that the proposed architecture exhibits superior adaptability and\nscalability in terms of inference latency, system stability, and concurrent\nprocessing capacity."
                },
                "authors": [
                    {
                        "name": "Pengyan Zhu"
                    },
                    {
                        "name": "Tingting Yang"
                    }
                ],
                "author_detail": {
                    "name": "Tingting Yang"
                },
                "author": "Tingting Yang",
                "arxiv_comment": "14 pages, 7 figures including subplots",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14085v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14085v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13002v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13002v2",
                "updated": "2025-05-20T07:34:45Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    7,
                    34,
                    45,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-19T11:41:21Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    11,
                    41,
                    21,
                    0,
                    139,
                    0
                ],
                "title": "PIM-malloc: A Fast and Scalable Dynamic Memory Allocator for\n  Processing-In-Memory (PIM) Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PIM-malloc: A Fast and Scalable Dynamic Memory Allocator for\n  Processing-In-Memory (PIM) Architectures"
                },
                "summary": "Dynamic memory allocation is essential in modern programming but remains\nunder-supported in current PIM devices. In this work, we first conduct a design\nspace exploration of PIM memory allocators, examining optimal metadata\nplacement and management strategies. Building on these insights, we propose\nPIM-malloc, a fast and scalable allocator for real PIM hardware, improving\nallocation performance by $66\\times$. We further enhance this design with a\nlightweight, per-PIM core hardware cache for dynamic allocation, achieving an\nadditional $31\\%$ performance gain. Finally, we demonstrate the effectiveness\nof PIM-malloc using a dynamic graph update workload, achieving a $28\\times$\nthroughput increase.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic memory allocation is essential in modern programming but remains\nunder-supported in current PIM devices. In this work, we first conduct a design\nspace exploration of PIM memory allocators, examining optimal metadata\nplacement and management strategies. Building on these insights, we propose\nPIM-malloc, a fast and scalable allocator for real PIM hardware, improving\nallocation performance by $66\\times$. We further enhance this design with a\nlightweight, per-PIM core hardware cache for dynamic allocation, achieving an\nadditional $31\\%$ performance gain. Finally, we demonstrate the effectiveness\nof PIM-malloc using a dynamic graph update workload, achieving a $28\\times$\nthroughput increase."
                },
                "authors": [
                    {
                        "name": "Dongjae Lee"
                    },
                    {
                        "name": "Bongjoon Hyun"
                    },
                    {
                        "name": "Youngjin Kwon"
                    },
                    {
                        "name": "Minsoo Rhu"
                    }
                ],
                "author_detail": {
                    "name": "Minsoo Rhu"
                },
                "author": "Minsoo Rhu",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13002v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13002v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14010v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14010v1",
                "updated": "2025-05-20T07:04:34Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    7,
                    4,
                    34,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T07:04:34Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    7,
                    4,
                    34,
                    1,
                    140,
                    0
                ],
                "title": "UHD Image Dehazing via anDehazeFormer with Atmospheric-aware KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UHD Image Dehazing via anDehazeFormer with Atmospheric-aware KV Cache"
                },
                "summary": "In this paper, we propose an efficient visual transformer framework for\nultra-high-definition (UHD) image dehazing that addresses the key challenges of\nslow training speed and high memory consumption for existing methods. Our\napproach introduces two key innovations: 1) an \\textbf{a}daptive\n\\textbf{n}ormalization mechanism inspired by the nGPT architecture that enables\nultra-fast and stable training with a network with a restricted range of\nparameter expressions; and 2) we devise an atmospheric scattering-aware KV\ncaching mechanism that dynamically optimizes feature preservation based on the\nphysical haze formation model. The proposed architecture improves the training\nconvergence speed by \\textbf{5 $\\times$} while reducing memory overhead,\nenabling real-time processing of 50 high-resolution images per second on an\nRTX4090 GPU. Experimental results show that our approach maintains\nstate-of-the-art dehazing quality while significantly improving computational\nefficiency for 4K/8K image restoration tasks. Furthermore, we provide a new\ndehazing image interpretable method with the help of an integrated gradient\nattribution map. Our code can be found here:\nhttps://anonymous.4open.science/r/anDehazeFormer-632E/README.md.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose an efficient visual transformer framework for\nultra-high-definition (UHD) image dehazing that addresses the key challenges of\nslow training speed and high memory consumption for existing methods. Our\napproach introduces two key innovations: 1) an \\textbf{a}daptive\n\\textbf{n}ormalization mechanism inspired by the nGPT architecture that enables\nultra-fast and stable training with a network with a restricted range of\nparameter expressions; and 2) we devise an atmospheric scattering-aware KV\ncaching mechanism that dynamically optimizes feature preservation based on the\nphysical haze formation model. The proposed architecture improves the training\nconvergence speed by \\textbf{5 $\\times$} while reducing memory overhead,\nenabling real-time processing of 50 high-resolution images per second on an\nRTX4090 GPU. Experimental results show that our approach maintains\nstate-of-the-art dehazing quality while significantly improving computational\nefficiency for 4K/8K image restoration tasks. Furthermore, we provide a new\ndehazing image interpretable method with the help of an integrated gradient\nattribution map. Our code can be found here:\nhttps://anonymous.4open.science/r/anDehazeFormer-632E/README.md."
                },
                "authors": [
                    {
                        "name": "Pu Wang"
                    },
                    {
                        "name": "Pengwen Dai"
                    },
                    {
                        "name": "Chen Wu"
                    },
                    {
                        "name": "Yeying Jin"
                    },
                    {
                        "name": "Dianjie Lu"
                    },
                    {
                        "name": "Guijuan Zhang"
                    },
                    {
                        "name": "Youshan Zhang"
                    },
                    {
                        "name": "Zhuoran Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zhuoran Zheng"
                },
                "author": "Zhuoran Zheng",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14010v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14010v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01281v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01281v3",
                "updated": "2025-05-20T04:52:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    4,
                    52,
                    21,
                    1,
                    140,
                    0
                ],
                "published": "2025-04-02T01:16:10Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    1,
                    16,
                    10,
                    2,
                    92,
                    0
                ],
                "title": "Scaling Test-Time Inference with Policy-Optimized, Dynamic\n  Retrieval-Augmented Generation via KV Caching and Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Test-Time Inference with Policy-Optimized, Dynamic\n  Retrieval-Augmented Generation via KV Caching and Decoding"
                },
                "summary": "We present a comprehensive framework for enhancing Retrieval-Augmented\nGeneration (RAG) systems through dynamic retrieval strategies and reinforcement\nfine-tuning. This approach significantly improves large language models on\nknowledge-intensive tasks, including opendomain question answering and complex\nreasoning. Our framework integrates two complementary techniques:\nPolicy-Optimized RetrievalAugmented Generation (PORAG), which optimizes the use\nof retrieved information, and Adaptive Token-Layer Attention Scoring (ATLAS),\nwhich dynamically determines retrieval timing and content based on contextual\nneeds. Together, these techniques enhance both the utilization and relevance of\nretrieved content, improving factual accuracy and response quality. Designed as\na lightweight solution compatible with any Transformer-based LLM without\nrequiring additional training, our framework excels in knowledge-intensive\ntasks, boosting output accuracy in RAG settings. We further propose CRITIC, a\nnovel method to selectively compress key-value caches by token importance,\nmitigating memory bottlenecks in long-context applications. The framework also\nincorporates test-time scaling techniques to dynamically balance reasoning\ndepth and computational resources, alongside optimized decoding strategies for\nfaster inference. Experiments on benchmark datasets show that our framework\nreduces hallucinations, strengthens domain-specific reasoning, and achieves\nsignificant efficiency and scalability gains over traditional RAG systems. This\nintegrated approach advances the development of robust, efficient, and scalable\nRAG systems across diverse applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a comprehensive framework for enhancing Retrieval-Augmented\nGeneration (RAG) systems through dynamic retrieval strategies and reinforcement\nfine-tuning. This approach significantly improves large language models on\nknowledge-intensive tasks, including opendomain question answering and complex\nreasoning. Our framework integrates two complementary techniques:\nPolicy-Optimized RetrievalAugmented Generation (PORAG), which optimizes the use\nof retrieved information, and Adaptive Token-Layer Attention Scoring (ATLAS),\nwhich dynamically determines retrieval timing and content based on contextual\nneeds. Together, these techniques enhance both the utilization and relevance of\nretrieved content, improving factual accuracy and response quality. Designed as\na lightweight solution compatible with any Transformer-based LLM without\nrequiring additional training, our framework excels in knowledge-intensive\ntasks, boosting output accuracy in RAG settings. We further propose CRITIC, a\nnovel method to selectively compress key-value caches by token importance,\nmitigating memory bottlenecks in long-context applications. The framework also\nincorporates test-time scaling techniques to dynamically balance reasoning\ndepth and computational resources, alongside optimized decoding strategies for\nfaster inference. Experiments on benchmark datasets show that our framework\nreduces hallucinations, strengthens domain-specific reasoning, and achieves\nsignificant efficiency and scalability gains over traditional RAG systems. This\nintegrated approach advances the development of robust, efficient, and scalable\nRAG systems across diverse applications."
                },
                "authors": [
                    {
                        "name": "Sakhinana Sagar Srinivas"
                    },
                    {
                        "name": "Akash Das"
                    },
                    {
                        "name": "Shivam Gupta"
                    },
                    {
                        "name": "Venkataramana Runkana"
                    }
                ],
                "author_detail": {
                    "name": "Venkataramana Runkana"
                },
                "author": "Venkataramana Runkana",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01281v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01281v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13866v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13866v1",
                "updated": "2025-05-20T03:21:52Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    3,
                    21,
                    52,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T03:21:52Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    3,
                    21,
                    52,
                    1,
                    140,
                    0
                ],
                "title": "Reasoning Path Compression: Compressing Generation Trajectories for\n  Efficient LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning Path Compression: Compressing Generation Trajectories for\n  Efficient LLM Reasoning"
                },
                "summary": "Recent reasoning-focused language models achieve high accuracy by generating\nlengthy intermediate reasoning paths before producing final answers. While this\napproach is effective in solving problems that require logical thinking, long\nreasoning paths significantly increase memory usage and throughput of token\ngeneration, limiting the practical deployment of such models. We propose\nReasoning Path Compression (RPC), a training-free method that accelerates\ninference by leveraging the semantic sparsity of reasoning paths. RPC\nperiodically compresses the KV cache by retaining KV cache that receive high\nimportance score, which are computed using a selector window composed of\nrecently generated queries. Experiments show that RPC improves generation\nthroughput of QwQ-32B by up to 1.60$\\times$ compared to the inference with full\nKV cache, with an accuracy drop of 1.2% on the AIME 2024 benchmark. Our\nfindings demonstrate that semantic sparsity in reasoning traces can be\neffectively exploited for compression, offering a practical path toward\nefficient deployment of reasoning LLMs. Our code is available at\nhttps://github.com/jiwonsong-dev/ReasoningPathCompression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent reasoning-focused language models achieve high accuracy by generating\nlengthy intermediate reasoning paths before producing final answers. While this\napproach is effective in solving problems that require logical thinking, long\nreasoning paths significantly increase memory usage and throughput of token\ngeneration, limiting the practical deployment of such models. We propose\nReasoning Path Compression (RPC), a training-free method that accelerates\ninference by leveraging the semantic sparsity of reasoning paths. RPC\nperiodically compresses the KV cache by retaining KV cache that receive high\nimportance score, which are computed using a selector window composed of\nrecently generated queries. Experiments show that RPC improves generation\nthroughput of QwQ-32B by up to 1.60$\\times$ compared to the inference with full\nKV cache, with an accuracy drop of 1.2% on the AIME 2024 benchmark. Our\nfindings demonstrate that semantic sparsity in reasoning traces can be\neffectively exploited for compression, offering a practical path toward\nefficient deployment of reasoning LLMs. Our code is available at\nhttps://github.com/jiwonsong-dev/ReasoningPathCompression."
                },
                "authors": [
                    {
                        "name": "Jiwon Song"
                    },
                    {
                        "name": "Dongwon Jo"
                    },
                    {
                        "name": "Yulhwa Kim"
                    },
                    {
                        "name": "Jae-Joon Kim"
                    }
                ],
                "author_detail": {
                    "name": "Jae-Joon Kim"
                },
                "author": "Jae-Joon Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13866v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13866v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09561v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09561v2",
                "updated": "2025-05-19T20:37:41Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    20,
                    37,
                    41,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-14T17:00:47Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    17,
                    0,
                    47,
                    2,
                    134,
                    0
                ],
                "title": "Learning Long-Context Diffusion Policies via Past-Token Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Long-Context Diffusion Policies via Past-Token Prediction"
                },
                "summary": "Reasoning over long sequences of observations and actions is essential for\nmany robotic tasks. Yet, learning effective long-context policies from\ndemonstrations remains challenging. As context length increases, training\nbecomes increasingly expensive due to rising memory demands, and policy\nperformance often degrades as a result of spurious correlations. Recent methods\ntypically sidestep these issues by truncating context length, discarding\nhistorical information that may be critical for subsequent decisions. In this\npaper, we propose an alternative approach that explicitly regularizes the\nretention of past information. We first revisit the copycat problem in\nimitation learning and identify an opposite challenge in recent diffusion\npolicies: rather than over-relying on prior actions, they often fail to capture\nessential dependencies between past and future actions. To address this, we\nintroduce Past-Token Prediction (PTP), an auxiliary task in which the policy\nlearns to predict past action tokens alongside future ones. This regularization\nsignificantly improves temporal modeling in the policy head, with minimal\nreliance on visual representations. Building on this observation, we further\nintroduce a multistage training strategy: pre-train the visual encoder with\nshort contexts, and fine-tune the policy head using cached long-context\nembeddings. This strategy preserves the benefits of PTP while greatly reducing\nmemory and computational overhead. Finally, we extend PTP into a\nself-verification mechanism at test time, enabling the policy to score and\nselect candidates consistent with past actions during inference. Experiments\nacross four real-world and six simulated tasks demonstrate that our proposed\nmethod improves the performance of long-context diffusion policies by 3x and\naccelerates policy training by more than 10x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning over long sequences of observations and actions is essential for\nmany robotic tasks. Yet, learning effective long-context policies from\ndemonstrations remains challenging. As context length increases, training\nbecomes increasingly expensive due to rising memory demands, and policy\nperformance often degrades as a result of spurious correlations. Recent methods\ntypically sidestep these issues by truncating context length, discarding\nhistorical information that may be critical for subsequent decisions. In this\npaper, we propose an alternative approach that explicitly regularizes the\nretention of past information. We first revisit the copycat problem in\nimitation learning and identify an opposite challenge in recent diffusion\npolicies: rather than over-relying on prior actions, they often fail to capture\nessential dependencies between past and future actions. To address this, we\nintroduce Past-Token Prediction (PTP), an auxiliary task in which the policy\nlearns to predict past action tokens alongside future ones. This regularization\nsignificantly improves temporal modeling in the policy head, with minimal\nreliance on visual representations. Building on this observation, we further\nintroduce a multistage training strategy: pre-train the visual encoder with\nshort contexts, and fine-tune the policy head using cached long-context\nembeddings. This strategy preserves the benefits of PTP while greatly reducing\nmemory and computational overhead. Finally, we extend PTP into a\nself-verification mechanism at test time, enabling the policy to score and\nselect candidates consistent with past actions during inference. Experiments\nacross four real-world and six simulated tasks demonstrate that our proposed\nmethod improves the performance of long-context diffusion policies by 3x and\naccelerates policy training by more than 10x."
                },
                "authors": [
                    {
                        "name": "Marcel Torne"
                    },
                    {
                        "name": "Andy Tang"
                    },
                    {
                        "name": "Yuejiang Liu"
                    },
                    {
                        "name": "Chelsea Finn"
                    }
                ],
                "author_detail": {
                    "name": "Chelsea Finn"
                },
                "author": "Chelsea Finn",
                "arxiv_comment": "Videos are available at https://long-context-dp.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09561v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09561v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.12463v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.12463v2",
                "updated": "2025-05-19T19:09:45Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    19,
                    9,
                    45,
                    0,
                    139,
                    0
                ],
                "published": "2024-05-21T02:39:45Z",
                "published_parsed": [
                    2024,
                    5,
                    21,
                    2,
                    39,
                    45,
                    1,
                    142,
                    0
                ],
                "title": "Stochastic Learning of Computational Resource Usage as Graph Structured\n  Multimarginal Schrödinger Bridge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stochastic Learning of Computational Resource Usage as Graph Structured\n  Multimarginal Schrödinger Bridge"
                },
                "summary": "We propose to learn the time-varying stochastic computational resource usage\nof software as a graph structured Schr\\\"odinger bridge problem. In general,\nlearning the computational resource usage from data is challenging because\nresources such as the number of CPU instructions and the number of last level\ncache requests are both time-varying and statistically correlated. Our proposed\nmethod enables learning the joint time-varying stochasticity in computational\nresource usage from the measured profile snapshots in a nonparametric manner.\nThe method can be used to predict the most-likely time-varying distribution of\ncomputational resource availability at a desired time. We provide detailed\nalgorithms for stochastic learning in both single and multi-core cases, discuss\nthe convergence guarantees, computational complexities, and demonstrate their\npractical use in two case studies: a single-core nonlinear model predictive\ncontroller, and a synthetic multi-core software.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose to learn the time-varying stochastic computational resource usage\nof software as a graph structured Schr\\\"odinger bridge problem. In general,\nlearning the computational resource usage from data is challenging because\nresources such as the number of CPU instructions and the number of last level\ncache requests are both time-varying and statistically correlated. Our proposed\nmethod enables learning the joint time-varying stochasticity in computational\nresource usage from the measured profile snapshots in a nonparametric manner.\nThe method can be used to predict the most-likely time-varying distribution of\ncomputational resource availability at a desired time. We provide detailed\nalgorithms for stochastic learning in both single and multi-core cases, discuss\nthe convergence guarantees, computational complexities, and demonstrate their\npractical use in two case studies: a single-core nonlinear model predictive\ncontroller, and a synthetic multi-core software."
                },
                "authors": [
                    {
                        "name": "Georgiy A. Bondar"
                    },
                    {
                        "name": "Robert Gifford"
                    },
                    {
                        "name": "Linh Thi Xuan Phan"
                    },
                    {
                        "name": "Abhishek Halder"
                    }
                ],
                "author_detail": {
                    "name": "Abhishek Halder"
                },
                "author": "Abhishek Halder",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.12463v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.12463v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10951v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10951v2",
                "updated": "2025-05-19T17:51:26Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    17,
                    51,
                    26,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-16T07:39:41Z",
                "published_parsed": [
                    2025,
                    5,
                    16,
                    7,
                    39,
                    41,
                    4,
                    136,
                    0
                ],
                "title": "SubGCache: Accelerating Graph-based RAG with Subgraph-level KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SubGCache: Accelerating Graph-based RAG with Subgraph-level KV Cache"
                },
                "summary": "Graph-based retrieval-augmented generation (RAG) enables large language\nmodels (LLMs) to incorporate structured knowledge via graph retrieval as\ncontextual input, enhancing more accurate and context-aware reasoning. We\nobserve that for different queries, it could retrieve similar subgraphs as\nprompts, and thus we propose SubGCache, which aims to reduce inference latency\nby reusing computation across queries with similar structural prompts (i.e.,\nsubgraphs). Specifically, SubGCache clusters queries based on subgraph\nembeddings, constructs a representative subgraph for each cluster, and\npre-computes the key-value (KV) cache of the representative subgraph. For each\nquery with its retrieved subgraph within a cluster, it reuses the pre-computed\nKV cache of the representative subgraph of the cluster without computing the KV\ntensors again for saving computation. Experiments on two new datasets across\nmultiple LLM backbones and graph-based RAG frameworks demonstrate that\nSubGCache consistently reduces inference latency with comparable and even\nimproved generation quality, achieving up to 6.68$\\times$ reduction in\ntime-to-first-token (TTFT).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph-based retrieval-augmented generation (RAG) enables large language\nmodels (LLMs) to incorporate structured knowledge via graph retrieval as\ncontextual input, enhancing more accurate and context-aware reasoning. We\nobserve that for different queries, it could retrieve similar subgraphs as\nprompts, and thus we propose SubGCache, which aims to reduce inference latency\nby reusing computation across queries with similar structural prompts (i.e.,\nsubgraphs). Specifically, SubGCache clusters queries based on subgraph\nembeddings, constructs a representative subgraph for each cluster, and\npre-computes the key-value (KV) cache of the representative subgraph. For each\nquery with its retrieved subgraph within a cluster, it reuses the pre-computed\nKV cache of the representative subgraph of the cluster without computing the KV\ntensors again for saving computation. Experiments on two new datasets across\nmultiple LLM backbones and graph-based RAG frameworks demonstrate that\nSubGCache consistently reduces inference latency with comparable and even\nimproved generation quality, achieving up to 6.68$\\times$ reduction in\ntime-to-first-token (TTFT)."
                },
                "authors": [
                    {
                        "name": "Qiuyu Zhu"
                    },
                    {
                        "name": "Liang Zhang"
                    },
                    {
                        "name": "Qianxiong Xu"
                    },
                    {
                        "name": "Cheng Long"
                    },
                    {
                        "name": "Jie Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Zhang"
                },
                "author": "Jie Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10951v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10951v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13140v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13140v1",
                "updated": "2025-05-19T14:09:45Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    14,
                    9,
                    45,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T14:09:45Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    14,
                    9,
                    45,
                    0,
                    139,
                    0
                ],
                "title": "CacheFlow: Fast Human Motion Prediction by Cached Normalizing Flow",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CacheFlow: Fast Human Motion Prediction by Cached Normalizing Flow"
                },
                "summary": "Many density estimation techniques for 3D human motion prediction require a\nsignificant amount of inference time, often exceeding the duration of the\npredicted time horizon. To address the need for faster density estimation for\n3D human motion prediction, we introduce a novel flow-based method for human\nmotion prediction called CacheFlow. Unlike previous conditional generative\nmodels that suffer from time efficiency, CacheFlow takes advantage of an\nunconditional flow-based generative model that transforms a Gaussian mixture\ninto the density of future motions. The results of the computation of the\nflow-based generative model can be precomputed and cached. Then, for\nconditional prediction, we seek a mapping from historical trajectories to\nsamples in the Gaussian mixture. This mapping can be done by a much more\nlightweight model, thus saving significant computation overhead compared to a\ntypical conditional flow model. In such a two-stage fashion and by caching\nresults from the slow flow model computation, we build our CacheFlow without\nloss of prediction accuracy and model expressiveness. This inference process is\ncompleted in approximately one millisecond, making it 4 times faster than\nprevious VAE methods and 30 times faster than previous diffusion-based methods\non standard benchmarks such as Human3.6M and AMASS datasets. Furthermore, our\nmethod demonstrates improved density estimation accuracy and comparable\nprediction accuracy to a SOTA method on Human3.6M. Our code and models will be\npublicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many density estimation techniques for 3D human motion prediction require a\nsignificant amount of inference time, often exceeding the duration of the\npredicted time horizon. To address the need for faster density estimation for\n3D human motion prediction, we introduce a novel flow-based method for human\nmotion prediction called CacheFlow. Unlike previous conditional generative\nmodels that suffer from time efficiency, CacheFlow takes advantage of an\nunconditional flow-based generative model that transforms a Gaussian mixture\ninto the density of future motions. The results of the computation of the\nflow-based generative model can be precomputed and cached. Then, for\nconditional prediction, we seek a mapping from historical trajectories to\nsamples in the Gaussian mixture. This mapping can be done by a much more\nlightweight model, thus saving significant computation overhead compared to a\ntypical conditional flow model. In such a two-stage fashion and by caching\nresults from the slow flow model computation, we build our CacheFlow without\nloss of prediction accuracy and model expressiveness. This inference process is\ncompleted in approximately one millisecond, making it 4 times faster than\nprevious VAE methods and 30 times faster than previous diffusion-based methods\non standard benchmarks such as Human3.6M and AMASS datasets. Furthermore, our\nmethod demonstrates improved density estimation accuracy and comparable\nprediction accuracy to a SOTA method on Human3.6M. Our code and models will be\npublicly available."
                },
                "authors": [
                    {
                        "name": "Takahiro Maeda"
                    },
                    {
                        "name": "Jinkun Cao"
                    },
                    {
                        "name": "Norimichi Ukita"
                    },
                    {
                        "name": "Kris Kitani"
                    }
                ],
                "author_detail": {
                    "name": "Kris Kitani"
                },
                "author": "Kris Kitani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13140v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13140v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13109v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13109v1",
                "updated": "2025-05-19T13:36:45Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    13,
                    36,
                    45,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T13:36:45Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    13,
                    36,
                    45,
                    0,
                    139,
                    0
                ],
                "title": "FreeKV: Boosting KV Cache Retrieval for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FreeKV: Boosting KV Cache Retrieval for Efficient LLM Inference"
                },
                "summary": "Large language models (LLMs) have been widely deployed with rapidly expanding\ncontext windows to support increasingly demanding applications. However, long\ncontexts pose significant deployment challenges, primarily due to the KV cache\nwhose size grows proportionally with context length. While KV cache compression\nmethods are proposed to address this issue, KV dropping methods incur\nconsiderable accuracy loss, and KV retrieval methods suffer from significant\nefficiency bottlenecks. We propose FreeKV, an algorithm-system co-optimization\nframework to enhance KV retrieval efficiency while preserving accuracy. On the\nalgorithm side, FreeKV introduces speculative retrieval to shift the KV\nselection and recall processes out of the critical path, combined with\nfine-grained correction to ensure accuracy. On the system side, FreeKV employs\nhybrid KV layouts across CPU and GPU memory to eliminate fragmented data\ntransfers, and leverages double-buffered streamed recall to further improve\nefficiency. Experiments demonstrate that FreeKV achieves near-lossless accuracy\nacross various scenarios and models, delivering up to 13$\\times$ speedup\ncompared to SOTA KV retrieval methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been widely deployed with rapidly expanding\ncontext windows to support increasingly demanding applications. However, long\ncontexts pose significant deployment challenges, primarily due to the KV cache\nwhose size grows proportionally with context length. While KV cache compression\nmethods are proposed to address this issue, KV dropping methods incur\nconsiderable accuracy loss, and KV retrieval methods suffer from significant\nefficiency bottlenecks. We propose FreeKV, an algorithm-system co-optimization\nframework to enhance KV retrieval efficiency while preserving accuracy. On the\nalgorithm side, FreeKV introduces speculative retrieval to shift the KV\nselection and recall processes out of the critical path, combined with\nfine-grained correction to ensure accuracy. On the system side, FreeKV employs\nhybrid KV layouts across CPU and GPU memory to eliminate fragmented data\ntransfers, and leverages double-buffered streamed recall to further improve\nefficiency. Experiments demonstrate that FreeKV achieves near-lossless accuracy\nacross various scenarios and models, delivering up to 13$\\times$ speedup\ncompared to SOTA KV retrieval methods."
                },
                "authors": [
                    {
                        "name": "Guangda Liu"
                    },
                    {
                        "name": "Chengwei Li"
                    },
                    {
                        "name": "Zhenyu Ning"
                    },
                    {
                        "name": "Jing Lin"
                    },
                    {
                        "name": "Yiwu Yao"
                    },
                    {
                        "name": "Danning Ke"
                    },
                    {
                        "name": "Minyi Guo"
                    },
                    {
                        "name": "Jieru Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Jieru Zhao"
                },
                "author": "Jieru Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13109v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13109v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13094v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13094v1",
                "updated": "2025-05-19T13:25:51Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    13,
                    25,
                    51,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T13:25:51Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    13,
                    25,
                    51,
                    0,
                    139,
                    0
                ],
                "title": "Time-Frequency-Based Attention Cache Memory Model for Real-Time Speech\n  Separation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time-Frequency-Based Attention Cache Memory Model for Real-Time Speech\n  Separation"
                },
                "summary": "Existing causal speech separation models often underperform compared to\nnon-causal models due to difficulties in retaining historical information. To\naddress this, we propose the Time-Frequency Attention Cache Memory (TFACM)\nmodel, which effectively captures spatio-temporal relationships through an\nattention mechanism and cache memory (CM) for historical information storage.\nIn TFACM, an LSTM layer captures frequency-relative positions, while causal\nmodeling is applied to the time dimension using local and global\nrepresentations. The CM module stores past information, and the causal\nattention refinement (CAR) module further enhances time-based feature\nrepresentations for finer granularity. Experimental results showed that TFACM\nachieveed comparable performance to the SOTA TF-GridNet-Causal model, with\nsignificantly lower complexity and fewer trainable parameters. For more\ndetails, visit the project page: https://cslikai.cn/TFACM/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing causal speech separation models often underperform compared to\nnon-causal models due to difficulties in retaining historical information. To\naddress this, we propose the Time-Frequency Attention Cache Memory (TFACM)\nmodel, which effectively captures spatio-temporal relationships through an\nattention mechanism and cache memory (CM) for historical information storage.\nIn TFACM, an LSTM layer captures frequency-relative positions, while causal\nmodeling is applied to the time dimension using local and global\nrepresentations. The CM module stores past information, and the causal\nattention refinement (CAR) module further enhances time-based feature\nrepresentations for finer granularity. Experimental results showed that TFACM\nachieveed comparable performance to the SOTA TF-GridNet-Causal model, with\nsignificantly lower complexity and fewer trainable parameters. For more\ndetails, visit the project page: https://cslikai.cn/TFACM/."
                },
                "authors": [
                    {
                        "name": "Guo Chen"
                    },
                    {
                        "name": "Kai Li"
                    },
                    {
                        "name": "Runxuan Yang"
                    },
                    {
                        "name": "Xiaolin Hu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaolin Hu"
                },
                "author": "Xiaolin Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13094v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13094v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.12946v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.12946v1",
                "updated": "2025-05-19T10:34:54Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    10,
                    34,
                    54,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T10:34:54Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    10,
                    34,
                    54,
                    0,
                    139,
                    0
                ],
                "title": "6G-Enabled Smart Railways",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "6G-Enabled Smart Railways"
                },
                "summary": "Smart railways integrate advanced information technologies into railway\noperating systems to improve efficiency and reliability. Although the\ndevelopment of 5G has enhanced railway services, future smart railways require\nultra-high speeds, ultra-low latency, ultra-high security, full coverage, and\nultra-high positioning accuracy, which 5G cannot fully meet. Therefore, 6G is\nenvisioned to provide green and efficient all-day operations, strong\ninformation security, fully automatic driving, and low-cost intelligent\nmaintenance. To achieve these requirements, we propose an integrated network\narchitecture leveraging communications, computing, edge intelligence, and\ncaching in railway systems. We have conducted in-depth investigations on key\nenabling technologies for reliable transmissions and wireless coverage. For\nhigh-speed mobile scenarios, we propose an AI-enabled cross-domain channel\nmodeling and orthogonal time-frequency space-time spread multiple access\nmechanism to alleviate the conflict between limited spectrum availability and\nmassive user access. The roles of blockchain, edge intelligence, and privacy\ntechnologies in endogenously secure rail communications are also evaluated. We\nfurther explore the application of emerging paradigms such as integrated\nsensing and communications, AI-assisted Internet of Things, semantic\ncommunications, and digital twin networks for railway maintenance, monitoring,\nprediction, and accident warning. Finally, possible future research and\ndevelopment directions are discussed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Smart railways integrate advanced information technologies into railway\noperating systems to improve efficiency and reliability. Although the\ndevelopment of 5G has enhanced railway services, future smart railways require\nultra-high speeds, ultra-low latency, ultra-high security, full coverage, and\nultra-high positioning accuracy, which 5G cannot fully meet. Therefore, 6G is\nenvisioned to provide green and efficient all-day operations, strong\ninformation security, fully automatic driving, and low-cost intelligent\nmaintenance. To achieve these requirements, we propose an integrated network\narchitecture leveraging communications, computing, edge intelligence, and\ncaching in railway systems. We have conducted in-depth investigations on key\nenabling technologies for reliable transmissions and wireless coverage. For\nhigh-speed mobile scenarios, we propose an AI-enabled cross-domain channel\nmodeling and orthogonal time-frequency space-time spread multiple access\nmechanism to alleviate the conflict between limited spectrum availability and\nmassive user access. The roles of blockchain, edge intelligence, and privacy\ntechnologies in endogenously secure rail communications are also evaluated. We\nfurther explore the application of emerging paradigms such as integrated\nsensing and communications, AI-assisted Internet of Things, semantic\ncommunications, and digital twin networks for railway maintenance, monitoring,\nprediction, and accident warning. Finally, possible future research and\ndevelopment directions are discussed."
                },
                "authors": [
                    {
                        "name": "Bo Ai"
                    },
                    {
                        "name": "Yunlong Lu"
                    },
                    {
                        "name": "Yuguang Fang"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Ruisi He"
                    },
                    {
                        "name": "Wei Chen"
                    },
                    {
                        "name": "Jiayi Zhang"
                    },
                    {
                        "name": "Guoyu Ma"
                    },
                    {
                        "name": "Yong Niu"
                    },
                    {
                        "name": "Zhangdui Zhong"
                    }
                ],
                "author_detail": {
                    "name": "Zhangdui Zhong"
                },
                "author": "Zhangdui Zhong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.12946v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.12946v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.12942v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.12942v1",
                "updated": "2025-05-19T10:29:32Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    10,
                    29,
                    32,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T10:29:32Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    10,
                    29,
                    32,
                    0,
                    139,
                    0
                ],
                "title": "A3 : an Analytical Low-Rank Approximation Framework for Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A3 : an Analytical Low-Rank Approximation Framework for Attention"
                },
                "summary": "Large language models have demonstrated remarkable performance; however,\ntheir massive parameter counts make deployment highly expensive. Low-rank\napproximation offers a promising compression solution, yet existing approaches\nhave two main limitations: (1) They focus on minimizing the output error of\nindividual linear layers, without considering the architectural characteristics\nof Transformers, and (2) they decompose a large weight matrix into two small\nlow-rank matrices. Consequently, these methods often fall short compared to\nother compression techniques like pruning and quantization, and introduce\nruntime overhead such as the extra GEMM kernel launches for decomposed small\nmatrices. To address these limitations, we propose $\\tt A^\\tt 3$, a\npost-training low-rank approximation framework. $\\tt A^\\tt 3$ splits a\nTransformer layer into three functional components, namely $\\tt QK$, $\\tt OV$,\nand $\\tt MLP$. For each component, $\\tt A^\\tt 3$ provides an analytical\nsolution that reduces the hidden dimension size inside each component while\nminimizing the component's functional loss ($\\it i.e.$, error in attention\nscores, attention outputs, and MLP outputs). This approach directly reduces\nmodel sizes, KV cache sizes, and FLOPs without introducing any runtime\noverheads. In addition, it provides a new narrative in advancing the\noptimization problem from singular linear layer loss optimization toward\nimproved end-to-end performance. Through extensive experiments, we show that\n$\\tt A^\\tt 3$ maintains superior performance compared to SoTAs. For example,\nunder the same reduction budget in computation and memory, our low-rank\napproximated LLaMA 3.1-70B achieves a perplexity of 4.69 on WikiText-2,\noutperforming the previous SoTA's 7.87 by 3.18. We also demonstrate the\nversatility of $\\tt A^\\tt 3$, including KV cache compression, quantization, and\nmixed-rank assignments for enhanced performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have demonstrated remarkable performance; however,\ntheir massive parameter counts make deployment highly expensive. Low-rank\napproximation offers a promising compression solution, yet existing approaches\nhave two main limitations: (1) They focus on minimizing the output error of\nindividual linear layers, without considering the architectural characteristics\nof Transformers, and (2) they decompose a large weight matrix into two small\nlow-rank matrices. Consequently, these methods often fall short compared to\nother compression techniques like pruning and quantization, and introduce\nruntime overhead such as the extra GEMM kernel launches for decomposed small\nmatrices. To address these limitations, we propose $\\tt A^\\tt 3$, a\npost-training low-rank approximation framework. $\\tt A^\\tt 3$ splits a\nTransformer layer into three functional components, namely $\\tt QK$, $\\tt OV$,\nand $\\tt MLP$. For each component, $\\tt A^\\tt 3$ provides an analytical\nsolution that reduces the hidden dimension size inside each component while\nminimizing the component's functional loss ($\\it i.e.$, error in attention\nscores, attention outputs, and MLP outputs). This approach directly reduces\nmodel sizes, KV cache sizes, and FLOPs without introducing any runtime\noverheads. In addition, it provides a new narrative in advancing the\noptimization problem from singular linear layer loss optimization toward\nimproved end-to-end performance. Through extensive experiments, we show that\n$\\tt A^\\tt 3$ maintains superior performance compared to SoTAs. For example,\nunder the same reduction budget in computation and memory, our low-rank\napproximated LLaMA 3.1-70B achieves a perplexity of 4.69 on WikiText-2,\noutperforming the previous SoTA's 7.87 by 3.18. We also demonstrate the\nversatility of $\\tt A^\\tt 3$, including KV cache compression, quantization, and\nmixed-rank assignments for enhanced performance."
                },
                "authors": [
                    {
                        "name": "Jeffrey T. H. Wong"
                    },
                    {
                        "name": "Cheng Zhang"
                    },
                    {
                        "name": "Xinye Cao"
                    },
                    {
                        "name": "Pedro Gimenes"
                    },
                    {
                        "name": "George A. Constantinides"
                    },
                    {
                        "name": "Wayne Luk"
                    },
                    {
                        "name": "Yiren Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Yiren Zhao"
                },
                "author": "Yiren Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.12942v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.12942v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09928v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09928v2",
                "updated": "2025-05-19T10:13:31Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    10,
                    13,
                    31,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-15T03:25:41Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    3,
                    25,
                    41,
                    3,
                    135,
                    0
                ],
                "title": "DeFeed: Secure Decentralized Cross-Contract Data Feed in Web 3.0 for\n  Connected Autonomous Vehicles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeFeed: Secure Decentralized Cross-Contract Data Feed in Web 3.0 for\n  Connected Autonomous Vehicles"
                },
                "summary": "Smart contracts have been a topic of interest in blockchain research and are\na key enabling technology for Connected Autonomous Vehicles (CAVs) in the era\nof Web 3.0. These contracts enable trustless interactions without the need for\nintermediaries, as they operate based on predefined rules encoded on the\nblockchain. However, smart contacts face significant challenges in\ncross-contract communication and information sharing, making it difficult to\nestablish seamless connectivity and collaboration among CAVs with Web 3.0. In\nthis paper, we propose DeFeed, a novel secure protocol that incorporates\nvarious gas-saving functions for CAVs, originated from in-depth research into\nthe interaction among smart contracts for decentralized cross-contract data\nfeed in Web 3.0. DeFeed allows smart contracts to obtain information from other\ncontracts efficiently in a single click, without complicated operations. We\njudiciously design and complete various functions with DeFeed, including a pool\nfunction and a cache function for gas optimization, a subscribe function for\nfacilitating data access, and an update function for the future iteration of\nour protocol. Tailored for CAVs with Web 3.0 use cases, DeFeed enables\nefficient data feed between smart contracts underpinning decentralized\napplications and vehicle coordination. Implemented and tested on the Ethereum\nofficial test network, DeFeed demonstrates significant improvements in contract\ninteraction efficiency, reducing computational complexity and gas costs. Our\nsolution represents a critical step towards seamless, decentralized\ncommunication in Web 3.0 ecosystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Smart contracts have been a topic of interest in blockchain research and are\na key enabling technology for Connected Autonomous Vehicles (CAVs) in the era\nof Web 3.0. These contracts enable trustless interactions without the need for\nintermediaries, as they operate based on predefined rules encoded on the\nblockchain. However, smart contacts face significant challenges in\ncross-contract communication and information sharing, making it difficult to\nestablish seamless connectivity and collaboration among CAVs with Web 3.0. In\nthis paper, we propose DeFeed, a novel secure protocol that incorporates\nvarious gas-saving functions for CAVs, originated from in-depth research into\nthe interaction among smart contracts for decentralized cross-contract data\nfeed in Web 3.0. DeFeed allows smart contracts to obtain information from other\ncontracts efficiently in a single click, without complicated operations. We\njudiciously design and complete various functions with DeFeed, including a pool\nfunction and a cache function for gas optimization, a subscribe function for\nfacilitating data access, and an update function for the future iteration of\nour protocol. Tailored for CAVs with Web 3.0 use cases, DeFeed enables\nefficient data feed between smart contracts underpinning decentralized\napplications and vehicle coordination. Implemented and tested on the Ethereum\nofficial test network, DeFeed demonstrates significant improvements in contract\ninteraction efficiency, reducing computational complexity and gas costs. Our\nsolution represents a critical step towards seamless, decentralized\ncommunication in Web 3.0 ecosystems."
                },
                "authors": [
                    {
                        "name": "Xingchen Sun"
                    },
                    {
                        "name": "Runhua Xu"
                    },
                    {
                        "name": "Wei Ni"
                    },
                    {
                        "name": "Li Duan"
                    },
                    {
                        "name": "Chao Li"
                    }
                ],
                "author_detail": {
                    "name": "Chao Li"
                },
                "author": "Chao Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09928v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09928v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.12742v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.12742v1",
                "updated": "2025-05-19T05:56:44Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    5,
                    56,
                    44,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T05:56:44Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    5,
                    56,
                    44,
                    0,
                    139,
                    0
                ],
                "title": "MVAR: Visual Autoregressive Modeling with Scale and Spatial Markovian\n  Conditioning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MVAR: Visual Autoregressive Modeling with Scale and Spatial Markovian\n  Conditioning"
                },
                "summary": "Essential to visual generation is efficient modeling of visual data priors.\nConventional next-token prediction methods define the process as learning the\nconditional probability distribution of successive tokens. Recently, next-scale\nprediction methods redefine the process to learn the distribution over\nmulti-scale representations, significantly reducing generation latency.\nHowever, these methods condition each scale on all previous scales and require\neach token to consider all preceding tokens, exhibiting scale and spatial\nredundancy. To better model the distribution by mitigating redundancy, we\npropose Markovian Visual AutoRegressive modeling (MVAR), a novel autoregressive\nframework that introduces scale and spatial Markov assumptions to reduce the\ncomplexity of conditional probability modeling. Specifically, we introduce a\nscale-Markov trajectory that only takes as input the features of adjacent\npreceding scale for next-scale prediction, enabling the adoption of a parallel\ntraining strategy that significantly reduces GPU memory consumption.\nFurthermore, we propose spatial-Markov attention, which restricts the attention\nof each token to a localized neighborhood of size k at corresponding positions\non adjacent scales, rather than attending to every token across these scales,\nfor the pursuit of reduced modeling complexity. Building on these improvements,\nwe reduce the computational complexity of attention calculation from O(N^2) to\nO(Nk), enabling training with just eight NVIDIA RTX 4090 GPUs and eliminating\nthe need for KV cache during inference. Extensive experiments on ImageNet\ndemonstrate that MVAR achieves comparable or superior performance with both\nsmall model trained from scratch and large fine-tuned models, while reducing\nthe average GPU memory footprint by 3.0x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Essential to visual generation is efficient modeling of visual data priors.\nConventional next-token prediction methods define the process as learning the\nconditional probability distribution of successive tokens. Recently, next-scale\nprediction methods redefine the process to learn the distribution over\nmulti-scale representations, significantly reducing generation latency.\nHowever, these methods condition each scale on all previous scales and require\neach token to consider all preceding tokens, exhibiting scale and spatial\nredundancy. To better model the distribution by mitigating redundancy, we\npropose Markovian Visual AutoRegressive modeling (MVAR), a novel autoregressive\nframework that introduces scale and spatial Markov assumptions to reduce the\ncomplexity of conditional probability modeling. Specifically, we introduce a\nscale-Markov trajectory that only takes as input the features of adjacent\npreceding scale for next-scale prediction, enabling the adoption of a parallel\ntraining strategy that significantly reduces GPU memory consumption.\nFurthermore, we propose spatial-Markov attention, which restricts the attention\nof each token to a localized neighborhood of size k at corresponding positions\non adjacent scales, rather than attending to every token across these scales,\nfor the pursuit of reduced modeling complexity. Building on these improvements,\nwe reduce the computational complexity of attention calculation from O(N^2) to\nO(Nk), enabling training with just eight NVIDIA RTX 4090 GPUs and eliminating\nthe need for KV cache during inference. Extensive experiments on ImageNet\ndemonstrate that MVAR achieves comparable or superior performance with both\nsmall model trained from scratch and large fine-tuned models, while reducing\nthe average GPU memory footprint by 3.0x."
                },
                "authors": [
                    {
                        "name": "Jinhua Zhang"
                    },
                    {
                        "name": "Wei Long"
                    },
                    {
                        "name": "Minghao Han"
                    },
                    {
                        "name": "Weiyi You"
                    },
                    {
                        "name": "Shuhang Gu"
                    }
                ],
                "author_detail": {
                    "name": "Shuhang Gu"
                },
                "author": "Shuhang Gu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.12742v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.12742v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.12731v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.12731v1",
                "updated": "2025-05-19T05:39:38Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    5,
                    39,
                    38,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T05:39:38Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    5,
                    39,
                    38,
                    0,
                    139,
                    0
                ],
                "title": "Accelerating Adaptive Retrieval Augmented Generation via\n  Instruction-Driven Representation Reduction of Retrieval Overlaps",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Adaptive Retrieval Augmented Generation via\n  Instruction-Driven Representation Reduction of Retrieval Overlaps"
                },
                "summary": "Retrieval-augmented generation (RAG) has emerged as a pivotal method for\nexpanding the knowledge of large language models. To handle complex queries\nmore effectively, researchers developed Adaptive-RAG (A-RAG) to enhance the\ngenerated quality through multiple interactions with external knowledge bases.\nDespite its effectiveness, A-RAG exacerbates the pre-existing efficiency\nchallenges inherent in RAG, which are attributable to its reliance on multiple\niterations of generation. Existing A-RAG approaches process all retrieved\ncontents from scratch. However, they ignore the situation where there is a\nsignificant overlap in the content of the retrieval results across rounds. The\noverlapping content is redundantly represented, which leads to a large\nproportion of repeated computations, thus affecting the overall efficiency. To\naddress this issue, this paper introduces a model-agnostic approach that can be\ngenerally applied to A-RAG methods, which is dedicated to reducing the\nredundant representation process caused by the overlapping of retrieval\nresults. Specifically, we use cache access and parallel generation to speed up\nthe prefilling and decoding stages respectively. Additionally, we also propose\nan instruction-driven module to further guide the model to more effectively\nattend to each part of the content in a more suitable way for LLMs. Experiments\nshow that our approach achieves 2.79 and 2.33 times significant acceleration on\naverage for prefilling and decoding respectively while maintaining equal\ngeneration quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) has emerged as a pivotal method for\nexpanding the knowledge of large language models. To handle complex queries\nmore effectively, researchers developed Adaptive-RAG (A-RAG) to enhance the\ngenerated quality through multiple interactions with external knowledge bases.\nDespite its effectiveness, A-RAG exacerbates the pre-existing efficiency\nchallenges inherent in RAG, which are attributable to its reliance on multiple\niterations of generation. Existing A-RAG approaches process all retrieved\ncontents from scratch. However, they ignore the situation where there is a\nsignificant overlap in the content of the retrieval results across rounds. The\noverlapping content is redundantly represented, which leads to a large\nproportion of repeated computations, thus affecting the overall efficiency. To\naddress this issue, this paper introduces a model-agnostic approach that can be\ngenerally applied to A-RAG methods, which is dedicated to reducing the\nredundant representation process caused by the overlapping of retrieval\nresults. Specifically, we use cache access and parallel generation to speed up\nthe prefilling and decoding stages respectively. Additionally, we also propose\nan instruction-driven module to further guide the model to more effectively\nattend to each part of the content in a more suitable way for LLMs. Experiments\nshow that our approach achieves 2.79 and 2.33 times significant acceleration on\naverage for prefilling and decoding respectively while maintaining equal\ngeneration quality."
                },
                "authors": [
                    {
                        "name": "Jie Ou"
                    },
                    {
                        "name": "Jinyu Guo"
                    },
                    {
                        "name": "Shuaihong Jiang"
                    },
                    {
                        "name": "Zhaokun Wang"
                    },
                    {
                        "name": "Libo Qin"
                    },
                    {
                        "name": "Shunyu Yao"
                    },
                    {
                        "name": "Wenhong Tian"
                    }
                ],
                "author_detail": {
                    "name": "Wenhong Tian"
                },
                "author": "Wenhong Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.12731v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.12731v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00570v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00570v2",
                "updated": "2025-05-19T02:21:16Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    2,
                    21,
                    16,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-01T14:53:12Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    14,
                    53,
                    12,
                    3,
                    121,
                    0
                ],
                "title": "FreqKV: Frequency Domain Key-Value Compression for Efficient Context\n  Window Extension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FreqKV: Frequency Domain Key-Value Compression for Efficient Context\n  Window Extension"
                },
                "summary": "Frequency-domain compression has proven effective in reducing redundancies\nfor spatial signals. In this work, we propose FreqKV, a novel frequency domain\nkey-value (KV) compression technique that enables efficient context window\nextension for decoder-only large language models (LLMs). Our approach is\nmotivated by a key observation that, in the frequency domain, the energy\ndistribution of the KV cache is predominantly concentrated in low-frequency\ncomponents. By discarding high-frequency components, we achieve efficient\ncompression of the KV cache with minimal information loss. FreqKV iteratively\ncompresses the increasing KV cache to a fixed size in the frequency domain,\nallowing models to process lengthy contexts efficiently. Introducing no\nadditional parameters or architectural modifications, FreqKV is applicable to\nboth fine-tuning and inference. With minimal fine-tuning, LLMs can learn to\nleverage the limited cache that is compressed in the frequency domain and\nextend the context window. Experiments on a range of long context language\nmodeling and understanding tasks demonstrate the efficiency and effectiveness\nof the proposed method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Frequency-domain compression has proven effective in reducing redundancies\nfor spatial signals. In this work, we propose FreqKV, a novel frequency domain\nkey-value (KV) compression technique that enables efficient context window\nextension for decoder-only large language models (LLMs). Our approach is\nmotivated by a key observation that, in the frequency domain, the energy\ndistribution of the KV cache is predominantly concentrated in low-frequency\ncomponents. By discarding high-frequency components, we achieve efficient\ncompression of the KV cache with minimal information loss. FreqKV iteratively\ncompresses the increasing KV cache to a fixed size in the frequency domain,\nallowing models to process lengthy contexts efficiently. Introducing no\nadditional parameters or architectural modifications, FreqKV is applicable to\nboth fine-tuning and inference. With minimal fine-tuning, LLMs can learn to\nleverage the limited cache that is compressed in the frequency domain and\nextend the context window. Experiments on a range of long context language\nmodeling and understanding tasks demonstrate the efficiency and effectiveness\nof the proposed method."
                },
                "authors": [
                    {
                        "name": "Jushi Kai"
                    },
                    {
                        "name": "Boyi Zeng"
                    },
                    {
                        "name": "Yixuan Wang"
                    },
                    {
                        "name": "Haoli Bai"
                    },
                    {
                        "name": "Ziwei He"
                    },
                    {
                        "name": "Bo Jiang"
                    },
                    {
                        "name": "Zhouhan Lin"
                    }
                ],
                "author_detail": {
                    "name": "Zhouhan Lin"
                },
                "author": "Zhouhan Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00570v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00570v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13544v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13544v2",
                "updated": "2025-05-21T01:34:19Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    1,
                    34,
                    19,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-19T02:09:41Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    2,
                    9,
                    41,
                    0,
                    139,
                    0
                ],
                "title": "Multi-head Temporal Latent Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-head Temporal Latent Attention"
                },
                "summary": "While Transformer self-attention offers strong parallelism, the Key-Value\n(KV) cache grows linearly with sequence length and becomes a bottleneck for\ninference efficiency. Multi-head latent attention was recently developed to\ncompress the KV cache into a low-rank latent space. This paper proposes\nMulti-head Temporal Latent Attention (MTLA), which further reduces the KV cache\nsize along the temporal dimension, greatly lowering the memory footprint of\nself-attention inference. MTLA employs a hyper-network to dynamically merge\ntemporally adjacent KV cache vectors. To address the mismatch between the\ncompressed KV cache and processed sequence lengths, a stride-aware causal mask\nis proposed to ensure efficient parallel training and consistency with\ninference behaviour. Experiments across tasks, including speech translation,\nspeech recognition, speech understanding and text summarisation, demonstrate\nthat MTLA achieves competitive performance compared to standard Multi-Head\nAttention (MHA), while greatly improving inference speed and GPU memory usage.\nFor example, on a English-German speech translation task, MTLA achieves a 5.3x\nspeedup and a reduction in GPU memory usage by a factor of 8.3 compared to MHA,\nwhile maintaining translation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Transformer self-attention offers strong parallelism, the Key-Value\n(KV) cache grows linearly with sequence length and becomes a bottleneck for\ninference efficiency. Multi-head latent attention was recently developed to\ncompress the KV cache into a low-rank latent space. This paper proposes\nMulti-head Temporal Latent Attention (MTLA), which further reduces the KV cache\nsize along the temporal dimension, greatly lowering the memory footprint of\nself-attention inference. MTLA employs a hyper-network to dynamically merge\ntemporally adjacent KV cache vectors. To address the mismatch between the\ncompressed KV cache and processed sequence lengths, a stride-aware causal mask\nis proposed to ensure efficient parallel training and consistency with\ninference behaviour. Experiments across tasks, including speech translation,\nspeech recognition, speech understanding and text summarisation, demonstrate\nthat MTLA achieves competitive performance compared to standard Multi-Head\nAttention (MHA), while greatly improving inference speed and GPU memory usage.\nFor example, on a English-German speech translation task, MTLA achieves a 5.3x\nspeedup and a reduction in GPU memory usage by a factor of 8.3 compared to MHA,\nwhile maintaining translation quality."
                },
                "authors": [
                    {
                        "name": "Keqi Deng"
                    },
                    {
                        "name": "Philip C. Woodland"
                    }
                ],
                "author_detail": {
                    "name": "Philip C. Woodland"
                },
                "author": "Philip C. Woodland",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13544v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13544v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.12594v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.12594v1",
                "updated": "2025-05-19T01:14:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    1,
                    14,
                    57,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T01:14:57Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    1,
                    14,
                    57,
                    0,
                    139,
                    0
                ],
                "title": "AD-AGENT: A Multi-agent Framework for End-to-end Anomaly Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AD-AGENT: A Multi-agent Framework for End-to-end Anomaly Detection"
                },
                "summary": "Anomaly detection (AD) is essential in areas such as fraud detection, network\nmonitoring, and scientific research. However, the diversity of data modalities\nand the increasing number of specialized AD libraries pose challenges for\nnon-expert users who lack in-depth library-specific knowledge and advanced\nprogramming skills. To tackle this, we present AD-AGENT, an LLM-driven\nmulti-agent framework that turns natural-language instructions into fully\nexecutable AD pipelines. AD-AGENT coordinates specialized agents for intent\nparsing, data preparation, library and model selection, documentation mining,\nand iterative code generation and debugging. Using a shared short-term\nworkspace and a long-term cache, the agents integrate popular AD libraries like\nPyOD, PyGOD, and TSLib into a unified workflow. Experiments demonstrate that\nAD-AGENT produces reliable scripts and recommends competitive models across\nlibraries. The system is open-sourced to support further research and practical\napplications in AD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Anomaly detection (AD) is essential in areas such as fraud detection, network\nmonitoring, and scientific research. However, the diversity of data modalities\nand the increasing number of specialized AD libraries pose challenges for\nnon-expert users who lack in-depth library-specific knowledge and advanced\nprogramming skills. To tackle this, we present AD-AGENT, an LLM-driven\nmulti-agent framework that turns natural-language instructions into fully\nexecutable AD pipelines. AD-AGENT coordinates specialized agents for intent\nparsing, data preparation, library and model selection, documentation mining,\nand iterative code generation and debugging. Using a shared short-term\nworkspace and a long-term cache, the agents integrate popular AD libraries like\nPyOD, PyGOD, and TSLib into a unified workflow. Experiments demonstrate that\nAD-AGENT produces reliable scripts and recommends competitive models across\nlibraries. The system is open-sourced to support further research and practical\napplications in AD."
                },
                "authors": [
                    {
                        "name": "Tiankai Yang"
                    },
                    {
                        "name": "Junjun Liu"
                    },
                    {
                        "name": "Wingchun Siu"
                    },
                    {
                        "name": "Jiahang Wang"
                    },
                    {
                        "name": "Zhuangzhuang Qian"
                    },
                    {
                        "name": "Chanjuan Song"
                    },
                    {
                        "name": "Cheng Cheng"
                    },
                    {
                        "name": "Xiyang Hu"
                    },
                    {
                        "name": "Yue Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Yue Zhao"
                },
                "author": "Yue Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.12594v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.12594v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.12392v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.12392v1",
                "updated": "2025-05-18T12:37:56Z",
                "updated_parsed": [
                    2025,
                    5,
                    18,
                    12,
                    37,
                    56,
                    6,
                    138,
                    0
                ],
                "published": "2025-05-18T12:37:56Z",
                "published_parsed": [
                    2025,
                    5,
                    18,
                    12,
                    37,
                    56,
                    6,
                    138,
                    0
                ],
                "title": "SLOT: Sample-specific Language Model Optimization at Test-time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SLOT: Sample-specific Language Model Optimization at Test-time"
                },
                "summary": "We propose SLOT (Sample-specific Language Model Optimization at Test-time), a\nnovel and parameter-efficient test-time inference approach that enhances a\nlanguage model's ability to more accurately respond to individual prompts.\nExisting Large Language Models (LLMs) often struggle with complex instructions,\nleading to poor performances on those not well represented among general\nsamples. To address this, SLOT conducts few optimization steps at test-time to\nupdate a light-weight sample-specific parameter vector. It is added to the\nfinal hidden layer before the output head, and enables efficient adaptation by\ncaching the last layer features during per-sample optimization. By minimizing\nthe cross-entropy loss on the input prompt only, SLOT helps the model better\naligned with and follow each given instruction. In experiments, we demonstrate\nthat our method outperforms the compared models across multiple benchmarks and\nLLMs. For example, Qwen2.5-7B with SLOT achieves an accuracy gain of 8.6% on\nGSM8K from 57.54% to 66.19%, while DeepSeek-R1-Distill-Llama-70B with SLOT\nachieves a SOTA accuracy of 68.69% on GPQA among 70B-level models. Our code is\navailable at https://github.com/maple-research-lab/SLOT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose SLOT (Sample-specific Language Model Optimization at Test-time), a\nnovel and parameter-efficient test-time inference approach that enhances a\nlanguage model's ability to more accurately respond to individual prompts.\nExisting Large Language Models (LLMs) often struggle with complex instructions,\nleading to poor performances on those not well represented among general\nsamples. To address this, SLOT conducts few optimization steps at test-time to\nupdate a light-weight sample-specific parameter vector. It is added to the\nfinal hidden layer before the output head, and enables efficient adaptation by\ncaching the last layer features during per-sample optimization. By minimizing\nthe cross-entropy loss on the input prompt only, SLOT helps the model better\naligned with and follow each given instruction. In experiments, we demonstrate\nthat our method outperforms the compared models across multiple benchmarks and\nLLMs. For example, Qwen2.5-7B with SLOT achieves an accuracy gain of 8.6% on\nGSM8K from 57.54% to 66.19%, while DeepSeek-R1-Distill-Llama-70B with SLOT\nachieves a SOTA accuracy of 68.69% on GPQA among 70B-level models. Our code is\navailable at https://github.com/maple-research-lab/SLOT."
                },
                "authors": [
                    {
                        "name": "Yang Hu"
                    },
                    {
                        "name": "Xingyu Zhang"
                    },
                    {
                        "name": "Xueji Fang"
                    },
                    {
                        "name": "Zhiyang Chen"
                    },
                    {
                        "name": "Xiao Wang"
                    },
                    {
                        "name": "Huatian Zhang"
                    },
                    {
                        "name": "Guojun Qi"
                    }
                ],
                "author_detail": {
                    "name": "Guojun Qi"
                },
                "author": "Guojun Qi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.12392v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.12392v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18394v7",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18394v7",
                "updated": "2025-05-18T03:12:25Z",
                "updated_parsed": [
                    2025,
                    5,
                    18,
                    3,
                    12,
                    25,
                    6,
                    138,
                    0
                ],
                "published": "2025-02-25T17:43:43Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    17,
                    43,
                    43,
                    1,
                    56,
                    0
                ],
                "title": "SPECTRE: An FFT-Based Efficient Drop-In Replacement to Self-Attention\n  for Long Contexts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPECTRE: An FFT-Based Efficient Drop-In Replacement to Self-Attention\n  for Long Contexts"
                },
                "summary": "Long-context transformers face significant efficiency challenges due to the\nquadratic cost of self-attention. However, many modern applications-from\nmulti-turn dialogue to high-resolution vision-require contexts spanning tens of\nthousands of tokens. We introduce SPECTRE, a method that replaces each\nattention head with a fast real FFT, a content-adaptive spectral gate, and an\ninverse FFT, reducing per-layer complexity from $\\mathcal{O}(L^{2})$ to\n$O(L\\log L)$ while preserving the surrounding architecture. We extend this\nefficiency to autoregressive generation through our Prefix-FFT cache and\nenhance local feature representation with an optional wavelet module that adds\nnegligible computational overhead. Our experiments demonstrate that SPECTRE\noperates up to 7$\\times$ faster than FlashAttention-2 on 128k-token contexts\nwhile matching or exceeding baseline performance on PG-19 language modeling and\nImageNet-1k classification tasks. SPECTRE achieves these improvements by adding\nfewer than 6\\% parameters to the base model, making hundred-kilotoken context\nprocessing feasible on commodity GPUs without specialized hardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context transformers face significant efficiency challenges due to the\nquadratic cost of self-attention. However, many modern applications-from\nmulti-turn dialogue to high-resolution vision-require contexts spanning tens of\nthousands of tokens. We introduce SPECTRE, a method that replaces each\nattention head with a fast real FFT, a content-adaptive spectral gate, and an\ninverse FFT, reducing per-layer complexity from $\\mathcal{O}(L^{2})$ to\n$O(L\\log L)$ while preserving the surrounding architecture. We extend this\nefficiency to autoregressive generation through our Prefix-FFT cache and\nenhance local feature representation with an optional wavelet module that adds\nnegligible computational overhead. Our experiments demonstrate that SPECTRE\noperates up to 7$\\times$ faster than FlashAttention-2 on 128k-token contexts\nwhile matching or exceeding baseline performance on PG-19 language modeling and\nImageNet-1k classification tasks. SPECTRE achieves these improvements by adding\nfewer than 6\\% parameters to the base model, making hundred-kilotoken context\nprocessing feasible on commodity GPUs without specialized hardware."
                },
                "authors": [
                    {
                        "name": "Jacob Fein-Ashley"
                    },
                    {
                        "name": "Neelesh Gupta"
                    },
                    {
                        "name": "Rajgopal Kannan"
                    },
                    {
                        "name": "Viktor Prasanna"
                    }
                ],
                "author_detail": {
                    "name": "Viktor Prasanna"
                },
                "author": "Viktor Prasanna",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18394v7",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18394v7",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.02930v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.02930v3",
                "updated": "2025-05-17T23:26:08Z",
                "updated_parsed": [
                    2025,
                    5,
                    17,
                    23,
                    26,
                    8,
                    5,
                    137,
                    0
                ],
                "published": "2024-07-03T09:02:05Z",
                "published_parsed": [
                    2024,
                    7,
                    3,
                    9,
                    2,
                    5,
                    2,
                    185,
                    0
                ],
                "title": "Timely Requesting for Time-Critical Content Users in Decentralized\n  F-RANs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Timely Requesting for Time-Critical Content Users in Decentralized\n  F-RANs"
                },
                "summary": "With the rising demand for high-rate and timely communications, fog radio\naccess networks (F-RANs) offer a promising solution. This work investigates age\nof information (AoI) performance in F-RANs, consisting of multiple content\nusers (CUs), enhanced remote radio heads (eRRHs), and content providers (CPs).\nTime-critical CUs need rapid content updates from CPs but cannot communicate\ndirectly with them; instead, eRRHs act as intermediaries. CUs decide whether to\nrequest content from a CP and which eRRH to send the request to, while eRRHs\ndecide whether to command CPs to update content or use cached content. We study\ntwo general classes of policies: (i) oblivious policies, where decision-making\nis independent of historical information, and (ii) non-oblivious policies,\nwhere decisions are influenced by historical information. First, we obtain\nclosed-form expressions for the average AoI of eRRHs under both policy types.\nDue to the complexity of calculating closed-form expressions for CUs, we then\nderive general upper bounds for their average AoI. Next, we identify optimal\npolicies for both types. Under both optimal policies, each CU requests content\nfrom each CP at an equal rate, consolidating all requests to a single eRRH when\ndemand is low or resources are limited, and distributing requests evenly among\neRRHs when demand is high and resources are ample. eRRHs command content from\neach CP at an equal rate under an optimal oblivious policy, while prioritize\nthe CP with the highest age under an optimal non-oblivious policy. Our\nnumerical results validate these theoretical findings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rising demand for high-rate and timely communications, fog radio\naccess networks (F-RANs) offer a promising solution. This work investigates age\nof information (AoI) performance in F-RANs, consisting of multiple content\nusers (CUs), enhanced remote radio heads (eRRHs), and content providers (CPs).\nTime-critical CUs need rapid content updates from CPs but cannot communicate\ndirectly with them; instead, eRRHs act as intermediaries. CUs decide whether to\nrequest content from a CP and which eRRH to send the request to, while eRRHs\ndecide whether to command CPs to update content or use cached content. We study\ntwo general classes of policies: (i) oblivious policies, where decision-making\nis independent of historical information, and (ii) non-oblivious policies,\nwhere decisions are influenced by historical information. First, we obtain\nclosed-form expressions for the average AoI of eRRHs under both policy types.\nDue to the complexity of calculating closed-form expressions for CUs, we then\nderive general upper bounds for their average AoI. Next, we identify optimal\npolicies for both types. Under both optimal policies, each CU requests content\nfrom each CP at an equal rate, consolidating all requests to a single eRRH when\ndemand is low or resources are limited, and distributing requests evenly among\neRRHs when demand is high and resources are ample. eRRHs command content from\neach CP at an equal rate under an optimal oblivious policy, while prioritize\nthe CP with the highest age under an optimal non-oblivious policy. Our\nnumerical results validate these theoretical findings."
                },
                "authors": [
                    {
                        "name": "Xingran Chen"
                    },
                    {
                        "name": "Kai Li"
                    },
                    {
                        "name": "Kun Yang"
                    }
                ],
                "author_detail": {
                    "name": "Kun Yang"
                },
                "author": "Kun Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.02930v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.02930v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09573v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09573v3",
                "updated": "2025-05-17T21:15:02Z",
                "updated_parsed": [
                    2025,
                    5,
                    17,
                    21,
                    15,
                    2,
                    5,
                    137,
                    0
                ],
                "published": "2025-03-12T17:43:40Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    17,
                    43,
                    40,
                    2,
                    71,
                    0
                ],
                "title": "Block Diffusion: Interpolating Between Autoregressive and Diffusion\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Block Diffusion: Interpolating Between Autoregressive and Diffusion\n  Language Models"
                },
                "summary": "Diffusion language models offer unique benefits over autoregressive models\ndue to their potential for parallelized generation and controllability, yet\nthey lag in likelihood modeling and are limited to fixed-length generation. In\nthis work, we introduce a class of block diffusion language models that\ninterpolate between discrete denoising diffusion and autoregressive models.\nBlock diffusion overcomes key limitations of both approaches by supporting\nflexible-length generation and improving inference efficiency with KV caching\nand parallel token sampling. We propose a recipe for building effective block\ndiffusion models that includes an efficient training algorithm, estimators of\ngradient variance, and data-driven noise schedules to minimize the variance.\nBlock diffusion sets a new state-of-the-art performance among diffusion models\non language modeling benchmarks and enables generation of arbitrary-length\nsequences. We provide the code, along with the model weights and blog post on\nthe project page: https://m-arriola.com/bd3lms",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion language models offer unique benefits over autoregressive models\ndue to their potential for parallelized generation and controllability, yet\nthey lag in likelihood modeling and are limited to fixed-length generation. In\nthis work, we introduce a class of block diffusion language models that\ninterpolate between discrete denoising diffusion and autoregressive models.\nBlock diffusion overcomes key limitations of both approaches by supporting\nflexible-length generation and improving inference efficiency with KV caching\nand parallel token sampling. We propose a recipe for building effective block\ndiffusion models that includes an efficient training algorithm, estimators of\ngradient variance, and data-driven noise schedules to minimize the variance.\nBlock diffusion sets a new state-of-the-art performance among diffusion models\non language modeling benchmarks and enables generation of arbitrary-length\nsequences. We provide the code, along with the model weights and blog post on\nthe project page: https://m-arriola.com/bd3lms"
                },
                "authors": [
                    {
                        "name": "Marianne Arriola"
                    },
                    {
                        "name": "Aaron Gokaslan"
                    },
                    {
                        "name": "Justin T. Chiu"
                    },
                    {
                        "name": "Zhihan Yang"
                    },
                    {
                        "name": "Zhixuan Qi"
                    },
                    {
                        "name": "Jiaqi Han"
                    },
                    {
                        "name": "Subham Sekhar Sahoo"
                    },
                    {
                        "name": "Volodymyr Kuleshov"
                    }
                ],
                "author_detail": {
                    "name": "Volodymyr Kuleshov"
                },
                "author": "Volodymyr Kuleshov",
                "arxiv_comment": "ICLR 2025 Oral. We provide the code at\n  https://github.com/kuleshov-group/bd3lms",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09573v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09573v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15804v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15804v2",
                "updated": "2025-05-17T12:22:59Z",
                "updated_parsed": [
                    2025,
                    5,
                    17,
                    12,
                    22,
                    59,
                    5,
                    137,
                    0
                ],
                "published": "2025-02-19T06:14:27Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    6,
                    14,
                    27,
                    2,
                    50,
                    0
                ],
                "title": "FairKV: Balancing Per-Head KV Cache for Fast Multi-GPU Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FairKV: Balancing Per-Head KV Cache for Fast Multi-GPU Inference"
                },
                "summary": "KV cache techniques in Transformer models aim to reduce redundant\ncomputations at the expense of substantially increased memory usage, making KV\ncache compression an important and popular research topic. Recently,\nstate-of-the-art KV cache compression methods implement imbalanced, per-head\nallocation algorithms that dynamically adjust the KV cache budget for each\nattention head, achieving excellent performance in single-GPU scenarios.\nHowever, we observe that such imbalanced compression leads to significant load\nimbalance when deploying multi-GPU inference, as some GPUs become overburdened\nwhile others remain underutilized. In this paper, we propose FairKV, a method\ndesigned to ensure fair memory usage among attention heads in systems employing\nimbalanced KV cache compression. The core technique of FairKV is Fair-Copying,\nwhich replicates a small subset of memory-intensive attention heads across GPUs\nusing data parallelism to mitigate load imbalance. Our experiments on popular\nmodels, including LLaMA 70b and Mistral 24b model, demonstrate that FairKV\nincreases throughput by 1.66x compared to standard tensor parallelism\ninference. Our code will be released as open source upon acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache techniques in Transformer models aim to reduce redundant\ncomputations at the expense of substantially increased memory usage, making KV\ncache compression an important and popular research topic. Recently,\nstate-of-the-art KV cache compression methods implement imbalanced, per-head\nallocation algorithms that dynamically adjust the KV cache budget for each\nattention head, achieving excellent performance in single-GPU scenarios.\nHowever, we observe that such imbalanced compression leads to significant load\nimbalance when deploying multi-GPU inference, as some GPUs become overburdened\nwhile others remain underutilized. In this paper, we propose FairKV, a method\ndesigned to ensure fair memory usage among attention heads in systems employing\nimbalanced KV cache compression. The core technique of FairKV is Fair-Copying,\nwhich replicates a small subset of memory-intensive attention heads across GPUs\nusing data parallelism to mitigate load imbalance. Our experiments on popular\nmodels, including LLaMA 70b and Mistral 24b model, demonstrate that FairKV\nincreases throughput by 1.66x compared to standard tensor parallelism\ninference. Our code will be released as open source upon acceptance."
                },
                "authors": [
                    {
                        "name": "Bingzhe Zhao"
                    },
                    {
                        "name": "Ke Cheng"
                    },
                    {
                        "name": "Aomufei Yuan"
                    },
                    {
                        "name": "Yuxuan Tian"
                    },
                    {
                        "name": "Ruiguang Zhong"
                    },
                    {
                        "name": "Chengchen Hu"
                    },
                    {
                        "name": "Tong Yang"
                    },
                    {
                        "name": "Lian Yu"
                    }
                ],
                "author_detail": {
                    "name": "Lian Yu"
                },
                "author": "Lian Yu",
                "arxiv_comment": "11 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15804v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15804v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.11820v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.11820v1",
                "updated": "2025-05-17T04:06:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    17,
                    4,
                    6,
                    12,
                    5,
                    137,
                    0
                ],
                "published": "2025-05-17T04:06:12Z",
                "published_parsed": [
                    2025,
                    5,
                    17,
                    4,
                    6,
                    12,
                    5,
                    137,
                    0
                ],
                "title": "Chain-of-Model Learning for Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Model Learning for Language Model"
                },
                "summary": "In this paper, we propose a novel learning paradigm, termed Chain-of-Model\n(CoM), which incorporates the causal relationship into the hidden states of\neach layer as a chain style, thereby introducing great scaling efficiency in\nmodel training and inference flexibility in deployment. We introduce the\nconcept of Chain-of-Representation (CoR), which formulates the hidden states at\neach layer as a combination of multiple sub-representations (i.e., chains) at\nthe hidden dimension level. In each layer, each chain from the output\nrepresentations can only view all of its preceding chains in the input\nrepresentations. Consequently, the model built upon CoM framework can\nprogressively scale up the model size by increasing the chains based on the\nprevious models (i.e., chains), and offer multiple sub-models at varying sizes\nfor elastic inference by using different chain numbers. Based on this\nprinciple, we devise Chain-of-Language-Model (CoLM), which incorporates the\nidea of CoM into each layer of Transformer architecture. Based on CoLM, we\nfurther introduce CoLM-Air by introducing a KV sharing mechanism, that computes\nall keys and values within the first chain and then shares across all chains.\nThis design demonstrates additional extensibility, such as enabling seamless LM\nswitching, prefilling acceleration and so on. Experimental results demonstrate\nour CoLM family can achieve comparable performance to the standard Transformer,\nwhile simultaneously enabling greater flexiblity, such as progressive scaling\nto improve training efficiency and offer multiple varying model sizes for\nelastic inference, paving a a new way toward building language models. Our code\nwill be released in the future at: https://github.com/microsoft/CoLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose a novel learning paradigm, termed Chain-of-Model\n(CoM), which incorporates the causal relationship into the hidden states of\neach layer as a chain style, thereby introducing great scaling efficiency in\nmodel training and inference flexibility in deployment. We introduce the\nconcept of Chain-of-Representation (CoR), which formulates the hidden states at\neach layer as a combination of multiple sub-representations (i.e., chains) at\nthe hidden dimension level. In each layer, each chain from the output\nrepresentations can only view all of its preceding chains in the input\nrepresentations. Consequently, the model built upon CoM framework can\nprogressively scale up the model size by increasing the chains based on the\nprevious models (i.e., chains), and offer multiple sub-models at varying sizes\nfor elastic inference by using different chain numbers. Based on this\nprinciple, we devise Chain-of-Language-Model (CoLM), which incorporates the\nidea of CoM into each layer of Transformer architecture. Based on CoLM, we\nfurther introduce CoLM-Air by introducing a KV sharing mechanism, that computes\nall keys and values within the first chain and then shares across all chains.\nThis design demonstrates additional extensibility, such as enabling seamless LM\nswitching, prefilling acceleration and so on. Experimental results demonstrate\nour CoLM family can achieve comparable performance to the standard Transformer,\nwhile simultaneously enabling greater flexiblity, such as progressive scaling\nto improve training efficiency and offer multiple varying model sizes for\nelastic inference, paving a a new way toward building language models. Our code\nwill be released in the future at: https://github.com/microsoft/CoLM."
                },
                "authors": [
                    {
                        "name": "Kaitao Song"
                    },
                    {
                        "name": "Xiaohua Wang"
                    },
                    {
                        "name": "Xu Tan"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Yongliang Shen"
                    },
                    {
                        "name": "Cen LU"
                    },
                    {
                        "name": "Zihao Li"
                    },
                    {
                        "name": "Zifan Song"
                    },
                    {
                        "name": "Caihua Shan"
                    },
                    {
                        "name": "Yansen Wang"
                    },
                    {
                        "name": "Kan Ren"
                    },
                    {
                        "name": "Xiaoqing Zheng"
                    },
                    {
                        "name": "Tao Qin"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Dongsheng Li"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.11820v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.11820v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.11783v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.11783v1",
                "updated": "2025-05-17T01:31:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    17,
                    1,
                    31,
                    21,
                    5,
                    137,
                    0
                ],
                "published": "2025-05-17T01:31:21Z",
                "published_parsed": [
                    2025,
                    5,
                    17,
                    1,
                    31,
                    21,
                    5,
                    137,
                    0
                ],
                "title": "Efficient Vector Search on Disaggregated Memory with d-HNSW",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Vector Search on Disaggregated Memory with d-HNSW"
                },
                "summary": "Efficient vector query processing is critical to enable AI applications at\nscale. Recent solutions struggle with growing vector datasets that exceed\nsingle-machine memory capacity, forcing unnecessary data movement and resource\nunderutilization in monolithic architectures. We present d-HNSW, the first\ndisaggregated vector similarity search engine for RDMA-based remote memory\nsystems that achieves high performance while supporting fast data indexing with\nlow network communication overhead. The core of d-HNSW is a novel\ndisaggregation of the graph-based vector indexing data structure HNSW. It\nexploits the characteristics of greedy searching in HNSW to efficiently\ncoordinate data transfers from the memory pool to the compute pool while\nserving data requests. Specifically, it leverages three ideas: (i)\nRepresentative index caching, a lightweight index constructed from a sampled\nsubset of data, is cached in the compute pool to reduce frequent access to\ncritical components of the hierarchical graph-based index, (ii) RDMA-friendly\ndata layout design to reduce the networking round trips incurred by vector\nquery and insertion and (iii) batched query-aware data loading to reduce\nbandwidth usage on data transfer between pools, addressing the limited cache\ncapacity in compute nodes. We evaluate d-HNSW with extensive benchmarking\ndatasets. The experimental results show that d-HNSW outperforms Naive d-HNSW\nimplementation by up to 117x in latency while maintaining recall as 0.87 in\ndataset SIFT1M@1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient vector query processing is critical to enable AI applications at\nscale. Recent solutions struggle with growing vector datasets that exceed\nsingle-machine memory capacity, forcing unnecessary data movement and resource\nunderutilization in monolithic architectures. We present d-HNSW, the first\ndisaggregated vector similarity search engine for RDMA-based remote memory\nsystems that achieves high performance while supporting fast data indexing with\nlow network communication overhead. The core of d-HNSW is a novel\ndisaggregation of the graph-based vector indexing data structure HNSW. It\nexploits the characteristics of greedy searching in HNSW to efficiently\ncoordinate data transfers from the memory pool to the compute pool while\nserving data requests. Specifically, it leverages three ideas: (i)\nRepresentative index caching, a lightweight index constructed from a sampled\nsubset of data, is cached in the compute pool to reduce frequent access to\ncritical components of the hierarchical graph-based index, (ii) RDMA-friendly\ndata layout design to reduce the networking round trips incurred by vector\nquery and insertion and (iii) batched query-aware data loading to reduce\nbandwidth usage on data transfer between pools, addressing the limited cache\ncapacity in compute nodes. We evaluate d-HNSW with extensive benchmarking\ndatasets. The experimental results show that d-HNSW outperforms Naive d-HNSW\nimplementation by up to 117x in latency while maintaining recall as 0.87 in\ndataset SIFT1M@1."
                },
                "authors": [
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Fei Fang"
                    },
                    {
                        "name": "Chen Qian"
                    }
                ],
                "author_detail": {
                    "name": "Chen Qian"
                },
                "author": "Chen Qian",
                "arxiv_comment": "To appear in HotStorage 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.11783v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.11783v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.11695v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.11695v1",
                "updated": "2025-05-16T21:04:25Z",
                "updated_parsed": [
                    2025,
                    5,
                    16,
                    21,
                    4,
                    25,
                    4,
                    136,
                    0
                ],
                "published": "2025-05-16T21:04:25Z",
                "published_parsed": [
                    2025,
                    5,
                    16,
                    21,
                    4,
                    25,
                    4,
                    136,
                    0
                ],
                "title": "Qronos: Correcting the Past by Shaping the Future... in Post-Training\n  Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Qronos: Correcting the Past by Shaping the Future... in Post-Training\n  Quantization"
                },
                "summary": "We introduce Qronos -- a new state-of-the-art post-training quantization\nalgorithm that sequentially rounds and updates neural network weights. Qronos\nnot only explicitly corrects errors due to both weight and activation\nquantization, but also errors resulting from quantizing previous layers. Our\niterative algorithm is based on an interpretable and disciplined optimization\nframework that subsumes and surpasses existing data-driven approaches. At each\nstep, Qronos alternates between error correction and diffusion via optimal\nupdate rules. Importantly, we prove that Qronos admits an efficient\nimplementation that uses the Cholesky decomposition for solving least-squares\nproblems. We also demonstrate that Qronos is compatible with existing\ntransformation techniques such as Hadamard-based incoherence processing and\nweight-activation scaling equalization, among others. We evaluate Qronos using\nrecent autoregressive language generation models in the Llama3 family; Qronos\nconsistently outperforms previous state-of-the-art adaptive rounding methods\nwhen quantizing the weights, activations, and/or KV caches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Qronos -- a new state-of-the-art post-training quantization\nalgorithm that sequentially rounds and updates neural network weights. Qronos\nnot only explicitly corrects errors due to both weight and activation\nquantization, but also errors resulting from quantizing previous layers. Our\niterative algorithm is based on an interpretable and disciplined optimization\nframework that subsumes and surpasses existing data-driven approaches. At each\nstep, Qronos alternates between error correction and diffusion via optimal\nupdate rules. Importantly, we prove that Qronos admits an efficient\nimplementation that uses the Cholesky decomposition for solving least-squares\nproblems. We also demonstrate that Qronos is compatible with existing\ntransformation techniques such as Hadamard-based incoherence processing and\nweight-activation scaling equalization, among others. We evaluate Qronos using\nrecent autoregressive language generation models in the Llama3 family; Qronos\nconsistently outperforms previous state-of-the-art adaptive rounding methods\nwhen quantizing the weights, activations, and/or KV caches."
                },
                "authors": [
                    {
                        "name": "Shihao Zhang"
                    },
                    {
                        "name": "Haoyu Zhang"
                    },
                    {
                        "name": "Ian Colbert"
                    },
                    {
                        "name": "Rayan Saab"
                    }
                ],
                "author_detail": {
                    "name": "Rayan Saab"
                },
                "author": "Rayan Saab",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.11695v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.11695v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.11625v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.11625v1",
                "updated": "2025-05-16T18:41:33Z",
                "updated_parsed": [
                    2025,
                    5,
                    16,
                    18,
                    41,
                    33,
                    4,
                    136,
                    0
                ],
                "published": "2025-05-16T18:41:33Z",
                "published_parsed": [
                    2025,
                    5,
                    16,
                    18,
                    41,
                    33,
                    4,
                    136,
                    0
                ],
                "title": "Nearest Neighbor Multivariate Time Series Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nearest Neighbor Multivariate Time Series Forecasting"
                },
                "summary": "Multivariate time series (MTS) forecasting has a wide range of applications\nin both industry and academia. Recently, spatial-temporal graph neural networks\n(STGNNs) have gained popularity as MTS forecasting methods. However, current\nSTGNNs can only use the finite length of MTS input data due to the\ncomputational complexity. Moreover, they lack the ability to identify similar\npatterns throughout the entire dataset and struggle with data that exhibit\nsparsely and discontinuously distributed correlations among variables over an\nextensive historical period, resulting in only marginal improvements. In this\narticle, we introduce a simple yet effective k-nearest neighbor MTS forecasting\n( kNN-MTS) framework, which forecasts with a nearest neighbor retrieval\nmechanism over a large datastore of cached series, using representations from\nthe MTS model for similarity search. This approach requires no additional\ntraining and scales to give the MTS model direct access to the whole dataset at\ntest time, resulting in a highly expressive model that consistently improves\nperformance, and has the ability to extract sparse distributed but similar\npatterns spanning over multivariables from the entire dataset. Furthermore, a\nhybrid spatial-temporal encoder (HSTEncoder) is designed for kNN-MTS which can\ncapture both long-term temporal and short-term spatial-temporal dependencies\nand is shown to provide accurate representation for kNN-MTSfor better\nforecasting. Experimental results on several real-world datasets show a\nsignificant improvement in the forecasting performance of kNN-MTS. The\nquantitative analysis also illustrates the interpretability and efficiency of\nkNN-MTS, showing better application prospects and opening up a new path for\nefficiently using the large dataset in MTS models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multivariate time series (MTS) forecasting has a wide range of applications\nin both industry and academia. Recently, spatial-temporal graph neural networks\n(STGNNs) have gained popularity as MTS forecasting methods. However, current\nSTGNNs can only use the finite length of MTS input data due to the\ncomputational complexity. Moreover, they lack the ability to identify similar\npatterns throughout the entire dataset and struggle with data that exhibit\nsparsely and discontinuously distributed correlations among variables over an\nextensive historical period, resulting in only marginal improvements. In this\narticle, we introduce a simple yet effective k-nearest neighbor MTS forecasting\n( kNN-MTS) framework, which forecasts with a nearest neighbor retrieval\nmechanism over a large datastore of cached series, using representations from\nthe MTS model for similarity search. This approach requires no additional\ntraining and scales to give the MTS model direct access to the whole dataset at\ntest time, resulting in a highly expressive model that consistently improves\nperformance, and has the ability to extract sparse distributed but similar\npatterns spanning over multivariables from the entire dataset. Furthermore, a\nhybrid spatial-temporal encoder (HSTEncoder) is designed for kNN-MTS which can\ncapture both long-term temporal and short-term spatial-temporal dependencies\nand is shown to provide accurate representation for kNN-MTSfor better\nforecasting. Experimental results on several real-world datasets show a\nsignificant improvement in the forecasting performance of kNN-MTS. The\nquantitative analysis also illustrates the interpretability and efficiency of\nkNN-MTS, showing better application prospects and opening up a new path for\nefficiently using the large dataset in MTS models."
                },
                "authors": [
                    {
                        "name": "Huiliang Zhang"
                    },
                    {
                        "name": "Ping Nie"
                    },
                    {
                        "name": "Lijun Sun"
                    },
                    {
                        "name": "Benoit Boulet"
                    }
                ],
                "author_detail": {
                    "name": "Benoit Boulet"
                },
                "author": "Benoit Boulet",
                "arxiv_doi": "10.1109/TNNLS.2024.3490603",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TNNLS.2024.3490603",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2505.11625v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.11625v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "IEEE Trans. Neural Netw. Learn. Syst., early access, 14 Nov. 2024",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.11302v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.11302v1",
                "updated": "2025-05-16T14:30:46Z",
                "updated_parsed": [
                    2025,
                    5,
                    16,
                    14,
                    30,
                    46,
                    4,
                    136,
                    0
                ],
                "published": "2025-05-16T14:30:46Z",
                "published_parsed": [
                    2025,
                    5,
                    16,
                    14,
                    30,
                    46,
                    4,
                    136,
                    0
                ],
                "title": "Depth first representations of $k^2$-trees",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Depth first representations of $k^2$-trees"
                },
                "summary": "The $k^2$-tree is a compact data structure designed to efficiently store\nsparse binary matrices by leveraging both sparsity and clustering of nonzero\nelements. This representation supports efficiently navigational operations and\ncomplex binary operations, such as matrix-matrix multiplication, while\nmaintaining space efficiency. The standard $k^2$-tree follows a level-by-level\nrepresentation, which, while effective, prevents further compression of\nidentical subtrees and it si not cache friendly when accessing individual\nsubtrees. In this work, we introduce some novel depth-first representations of\nthe $k^2$-tree and propose an efficient linear-time algorithm to identify and\ncompress identical subtrees within these structures. Our experimental results\nshow that the use of a depth-first representations is a strategy worth\npursuing: for the adjacency matrix of web graphs exploiting the presence of\nidentical subtrees does improve the compression ratio, and for some matrices\ndepth-first representations turns out to be faster than the standard $k^2$-tree\nin computing the matrix-matrix multiplication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The $k^2$-tree is a compact data structure designed to efficiently store\nsparse binary matrices by leveraging both sparsity and clustering of nonzero\nelements. This representation supports efficiently navigational operations and\ncomplex binary operations, such as matrix-matrix multiplication, while\nmaintaining space efficiency. The standard $k^2$-tree follows a level-by-level\nrepresentation, which, while effective, prevents further compression of\nidentical subtrees and it si not cache friendly when accessing individual\nsubtrees. In this work, we introduce some novel depth-first representations of\nthe $k^2$-tree and propose an efficient linear-time algorithm to identify and\ncompress identical subtrees within these structures. Our experimental results\nshow that the use of a depth-first representations is a strategy worth\npursuing: for the adjacency matrix of web graphs exploiting the presence of\nidentical subtrees does improve the compression ratio, and for some matrices\ndepth-first representations turns out to be faster than the standard $k^2$-tree\nin computing the matrix-matrix multiplication."
                },
                "authors": [
                    {
                        "name": "Gabriel Carmona"
                    },
                    {
                        "name": "Giovanni Manzini"
                    }
                ],
                "author_detail": {
                    "name": "Giovanni Manzini"
                },
                "author": "Giovanni Manzini",
                "arxiv_comment": "extended submission for SPIRE 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.11302v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.11302v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.11271v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.11271v1",
                "updated": "2025-05-16T14:04:31Z",
                "updated_parsed": [
                    2025,
                    5,
                    16,
                    14,
                    4,
                    31,
                    4,
                    136,
                    0
                ],
                "published": "2025-05-16T14:04:31Z",
                "published_parsed": [
                    2025,
                    5,
                    16,
                    14,
                    4,
                    31,
                    4,
                    136,
                    0
                ],
                "title": "Semantic Caching of Contextual Summaries for Efficient\n  Question-Answering with Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Caching of Contextual Summaries for Efficient\n  Question-Answering with Language Models"
                },
                "summary": "Large Language Models (LLMs) are increasingly deployed across edge and cloud\nplatforms for real-time question-answering and retrieval-augmented generation.\nHowever, processing lengthy contexts in distributed systems incurs high\ncomputational overhead, memory usage, and network bandwidth. This paper\nintroduces a novel semantic caching approach for storing and reusing\nintermediate contextual summaries, enabling efficient information reuse across\nsimilar queries in LLM-based QA workflows. Our method reduces redundant\ncomputations by up to 50-60% while maintaining answer accuracy comparable to\nfull document processing, as demonstrated on NaturalQuestions, TriviaQA, and a\nsynthetic ArXiv dataset. This approach balances computational cost and response\nquality, critical for real-time AI assistants.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly deployed across edge and cloud\nplatforms for real-time question-answering and retrieval-augmented generation.\nHowever, processing lengthy contexts in distributed systems incurs high\ncomputational overhead, memory usage, and network bandwidth. This paper\nintroduces a novel semantic caching approach for storing and reusing\nintermediate contextual summaries, enabling efficient information reuse across\nsimilar queries in LLM-based QA workflows. Our method reduces redundant\ncomputations by up to 50-60% while maintaining answer accuracy comparable to\nfull document processing, as demonstrated on NaturalQuestions, TriviaQA, and a\nsynthetic ArXiv dataset. This approach balances computational cost and response\nquality, critical for real-time AI assistants."
                },
                "authors": [
                    {
                        "name": "Camille Couturier"
                    },
                    {
                        "name": "Spyros Mastorakis"
                    },
                    {
                        "name": "Haiying Shen"
                    },
                    {
                        "name": "Saravan Rajmohan"
                    },
                    {
                        "name": "Victor Rühle"
                    }
                ],
                "author_detail": {
                    "name": "Victor Rühle"
                },
                "author": "Victor Rühle",
                "arxiv_comment": "Preprint. Paper accepted at ICCCN 2025, the final version will appear\n  in the proceedings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.11271v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.11271v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2209.10272v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2209.10272v2",
                "updated": "2025-05-16T13:56:07Z",
                "updated_parsed": [
                    2025,
                    5,
                    16,
                    13,
                    56,
                    7,
                    4,
                    136,
                    0
                ],
                "published": "2022-09-21T11:24:10Z",
                "published_parsed": [
                    2022,
                    9,
                    21,
                    11,
                    24,
                    10,
                    2,
                    264,
                    0
                ],
                "title": "Evaluating Continuous Basic Graph Patterns over Dynamic Link Data Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Continuous Basic Graph Patterns over Dynamic Link Data Graphs"
                },
                "summary": "In this paper, we investigate the problem of evaluating Basic Graph Patterns\n(BGP, for short, a subclass of SPARQL queries) over dynamic Linked Data graphs;\ni.e., Linked Data graphs that are continuously updated. We consider a setting\nwhere the updates are continuously received through a stream of messages and\nsupport both insertions and deletions of triples (updates are straightforwardly\nhandled as a combination of deletions and insertions). In this context, we\npropose a set of in-memory algorithms minimizing the cached data to efficiently\nand continuously answer BGP queries. The queries are typically submitted into a\nsystem and continuously result in the delta answers while the update messages\nare processed.\n  To efficiently and continuously evaluate the submitted query over the\nstreaming data, as well as to minimize the amount of cached data, we propose an\napproach where the submitted query is decomposed into simpler subqueries and\nthe query evaluation is achieved by combining the intermediate answers of the\nsubqueries. Using this approach, the proposed algorithms compute the delta\nanswers of a BGP query in polynomial time and space. Note that for certain\nsubclasses of BGP queries, we show that the evaluation can be achieved in\nconstant or linear time and space. Consolidating all the historical delta\nanswers, the algorithms ensure that the answer to each query is constructed at\nany given time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we investigate the problem of evaluating Basic Graph Patterns\n(BGP, for short, a subclass of SPARQL queries) over dynamic Linked Data graphs;\ni.e., Linked Data graphs that are continuously updated. We consider a setting\nwhere the updates are continuously received through a stream of messages and\nsupport both insertions and deletions of triples (updates are straightforwardly\nhandled as a combination of deletions and insertions). In this context, we\npropose a set of in-memory algorithms minimizing the cached data to efficiently\nand continuously answer BGP queries. The queries are typically submitted into a\nsystem and continuously result in the delta answers while the update messages\nare processed.\n  To efficiently and continuously evaluate the submitted query over the\nstreaming data, as well as to minimize the amount of cached data, we propose an\napproach where the submitted query is decomposed into simpler subqueries and\nthe query evaluation is achieved by combining the intermediate answers of the\nsubqueries. Using this approach, the proposed algorithms compute the delta\nanswers of a BGP query in polynomial time and space. Note that for certain\nsubclasses of BGP queries, we show that the evaluation can be achieved in\nconstant or linear time and space. Consolidating all the historical delta\nanswers, the algorithms ensure that the answer to each query is constructed at\nany given time."
                },
                "authors": [
                    {
                        "name": "Manolis Gergatsoulis"
                    },
                    {
                        "name": "Matthew Damigos"
                    }
                ],
                "author_detail": {
                    "name": "Matthew Damigos"
                },
                "author": "Matthew Damigos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2209.10272v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2209.10272v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16525v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16525v2",
                "updated": "2025-05-16T12:42:48Z",
                "updated_parsed": [
                    2025,
                    5,
                    16,
                    12,
                    42,
                    48,
                    4,
                    136,
                    0
                ],
                "published": "2025-03-17T16:43:35Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    16,
                    43,
                    35,
                    0,
                    76,
                    0
                ],
                "title": "KVShare: An LLM Service System with Efficient and Effective Multi-Tenant\n  KV Cache Reuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVShare: An LLM Service System with Efficient and Effective Multi-Tenant\n  KV Cache Reuse"
                },
                "summary": "Recent advances in long-text understanding have pushed the context length of\nlarge language models (LLMs) up to one million tokens. It boosts LLMs's\naccuracy and reasoning capacity but causes exorbitant computational costs and\nunsatisfactory Time to First Token (TTFT). KV cache reuse, which reuses the\nexact same KV cache of prefixes and templates or shares similar ones but with\nextra selective recomputation, offers a promising way to tackle this issue.\nHowever, prior studies overlook the cross-request KV reuse and the attention\ndeviations introduced by new tokens during the decoding stage. In this paper,\nwe present a KV cache management module that shares the KV cache across\nrequests under multi-tenant scenarios without sacrificing model accuracy. Our\nsystem, KVShare, enables accurate and efficient LLM serving by 1) a Dual-Stage\nHigh Deviation algorithm (DHD) that conditionally selects a small portion of KV\ncache to be recomputed during both prefill and decode phases, and 2) a\ncache-aware scheduler that prioritizes requests based on their KV cache hit\nrates and orchestrates continuous batching to achieve enhanced system\nefficiency and faster TTFT. Multi-task experiments conducted on models such as\nQwen2.5-7B,Llama3.1-8B and Yi1.5-9B demonstrate that KVShare reduces TTFT by up\nto 9.39x and increases 1.2x of the throughput compared to the full KV\nrecompute. Moreover, KVShare achieves 20.38% boost in terms of accuracy\ncompared to SOTA methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in long-text understanding have pushed the context length of\nlarge language models (LLMs) up to one million tokens. It boosts LLMs's\naccuracy and reasoning capacity but causes exorbitant computational costs and\nunsatisfactory Time to First Token (TTFT). KV cache reuse, which reuses the\nexact same KV cache of prefixes and templates or shares similar ones but with\nextra selective recomputation, offers a promising way to tackle this issue.\nHowever, prior studies overlook the cross-request KV reuse and the attention\ndeviations introduced by new tokens during the decoding stage. In this paper,\nwe present a KV cache management module that shares the KV cache across\nrequests under multi-tenant scenarios without sacrificing model accuracy. Our\nsystem, KVShare, enables accurate and efficient LLM serving by 1) a Dual-Stage\nHigh Deviation algorithm (DHD) that conditionally selects a small portion of KV\ncache to be recomputed during both prefill and decode phases, and 2) a\ncache-aware scheduler that prioritizes requests based on their KV cache hit\nrates and orchestrates continuous batching to achieve enhanced system\nefficiency and faster TTFT. Multi-task experiments conducted on models such as\nQwen2.5-7B,Llama3.1-8B and Yi1.5-9B demonstrate that KVShare reduces TTFT by up\nto 9.39x and increases 1.2x of the throughput compared to the full KV\nrecompute. Moreover, KVShare achieves 20.38% boost in terms of accuracy\ncompared to SOTA methods."
                },
                "authors": [
                    {
                        "name": "Huan Yang"
                    },
                    {
                        "name": "Renji Zhang"
                    },
                    {
                        "name": "Mingzhe Huang"
                    },
                    {
                        "name": "Weijun Wang"
                    },
                    {
                        "name": "Yin Tang"
                    },
                    {
                        "name": "Yuanchun Li"
                    },
                    {
                        "name": "Yunxin Liu"
                    },
                    {
                        "name": "Deyu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Deyu Zhang"
                },
                "author": "Deyu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16525v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16525v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04987v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04987v3",
                "updated": "2025-05-16T12:32:36Z",
                "updated_parsed": [
                    2025,
                    5,
                    16,
                    12,
                    32,
                    36,
                    4,
                    136,
                    0
                ],
                "published": "2025-01-09T06:00:27Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    6,
                    0,
                    27,
                    3,
                    9,
                    0
                ],
                "title": "TreeKV: Smooth Key-Value Cache Compression with Tree Structures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TreeKV: Smooth Key-Value Cache Compression with Tree Structures"
                },
                "summary": "Efficient key-value (KV) cache compression is critical for scaling\ntransformer-based Large Language Models (LLMs) in long sequences and\nresource-limited settings. Existing methods evict tokens based on their\npositions or importance scores, but position-based strategies can miss crucial\ninformation outside predefined regions, while those relying on global\nimportance scores resulting in strong regional biases, limiting the KV cache's\noverall context retention and potentially impairing the performance of LLMs on\ncomplex tasks. Our wavelet analysis reveals that as tokens approach the end of\nsequence, their contributions to generation gradually increase and tends to\ndiverge more from neighboring tokens, indicating a smooth transition with\nincreasing complexity and variability from distant to nearby context. Motivated\nby this observation, we propose TreeKV, an intuitive, training-free method that\nemploys a tree structure for smooth cache compression. TreeKV maintains a fixed\ncache size, allowing LLMs to deliver high-quality output even in long text\nscenarios. Unlike most compression methods, TreeKV is applicable to both the\ngeneration and prefilling stages. TreeKV consistently surpasses all baseline\nmodels in language modeling tasks on PG19 and OpenWebText2, allowing LLMs\ntrained with short context window to generalize to longer window with a 16x\ncache reduction. On the Longbench benchmark, TreeKV achieves the best\nperformance with only 6\\% of the budget at optimal efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient key-value (KV) cache compression is critical for scaling\ntransformer-based Large Language Models (LLMs) in long sequences and\nresource-limited settings. Existing methods evict tokens based on their\npositions or importance scores, but position-based strategies can miss crucial\ninformation outside predefined regions, while those relying on global\nimportance scores resulting in strong regional biases, limiting the KV cache's\noverall context retention and potentially impairing the performance of LLMs on\ncomplex tasks. Our wavelet analysis reveals that as tokens approach the end of\nsequence, their contributions to generation gradually increase and tends to\ndiverge more from neighboring tokens, indicating a smooth transition with\nincreasing complexity and variability from distant to nearby context. Motivated\nby this observation, we propose TreeKV, an intuitive, training-free method that\nemploys a tree structure for smooth cache compression. TreeKV maintains a fixed\ncache size, allowing LLMs to deliver high-quality output even in long text\nscenarios. Unlike most compression methods, TreeKV is applicable to both the\ngeneration and prefilling stages. TreeKV consistently surpasses all baseline\nmodels in language modeling tasks on PG19 and OpenWebText2, allowing LLMs\ntrained with short context window to generalize to longer window with a 16x\ncache reduction. On the Longbench benchmark, TreeKV achieves the best\nperformance with only 6\\% of the budget at optimal efficiency."
                },
                "authors": [
                    {
                        "name": "Ziwei He"
                    },
                    {
                        "name": "Jian Yuan"
                    },
                    {
                        "name": "Haoli Bai"
                    },
                    {
                        "name": "Jingwen Leng"
                    },
                    {
                        "name": "Bo Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Bo Jiang"
                },
                "author": "Bo Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04987v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04987v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14731v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14731v2",
                "updated": "2025-05-16T09:40:01Z",
                "updated_parsed": [
                    2025,
                    5,
                    16,
                    9,
                    40,
                    1,
                    4,
                    136,
                    0
                ],
                "published": "2024-10-16T08:34:51Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    8,
                    34,
                    51,
                    2,
                    290,
                    0
                ],
                "title": "MatryoshkaKV: Adaptive KV Compression via Trainable Orthogonal\n  Projection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MatryoshkaKV: Adaptive KV Compression via Trainable Orthogonal\n  Projection"
                },
                "summary": "KV cache has become a de facto technique for the inference of large language\nmodels (LLMs), where tensors of shape (layer number, head number, sequence\nlength, feature dimension) are introduced to cache historical information for\nself-attention. As the size of the model and data grows, the KV cache can\nquickly become a bottleneck within the system in both storage and memory\ntransfer. To address this, prior studies usually focus on the first three axes\nof the cache tensors for compression. This paper supplements them, focusing on\nthe feature dimension axis, by utilizing low-rank projection matrices to\ntransform the cache features into spaces with reduced dimensions. We begin by\ninvestigating the canonical orthogonal projection method for data compression\nthrough principal component analysis (PCA). We observe the issue with PCA\nprojection where significant performance degradation is observed at low\ncompression rates. To bridge the gap, we propose to directly tune the\northogonal projection matrices with a distillation objective using an elaborate\nMatryoshka training strategy. After training, we adaptively search for the\noptimal compression rates for various layers and heads given varying\ncompression budgets. Compared to previous works, our method can easily embrace\npre-trained LLMs and hold a smooth tradeoff between performance and compression\nrate. We empirically witness the high data efficiency of our training procedure\nand find that our method can sustain over 90% performance with an average KV\ncache compression rate of 60% (and up to 75% in certain extreme scenarios) for\npopular LLMs like LLaMA2-7B-base and Mistral-7B-v0.3-base.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache has become a de facto technique for the inference of large language\nmodels (LLMs), where tensors of shape (layer number, head number, sequence\nlength, feature dimension) are introduced to cache historical information for\nself-attention. As the size of the model and data grows, the KV cache can\nquickly become a bottleneck within the system in both storage and memory\ntransfer. To address this, prior studies usually focus on the first three axes\nof the cache tensors for compression. This paper supplements them, focusing on\nthe feature dimension axis, by utilizing low-rank projection matrices to\ntransform the cache features into spaces with reduced dimensions. We begin by\ninvestigating the canonical orthogonal projection method for data compression\nthrough principal component analysis (PCA). We observe the issue with PCA\nprojection where significant performance degradation is observed at low\ncompression rates. To bridge the gap, we propose to directly tune the\northogonal projection matrices with a distillation objective using an elaborate\nMatryoshka training strategy. After training, we adaptively search for the\noptimal compression rates for various layers and heads given varying\ncompression budgets. Compared to previous works, our method can easily embrace\npre-trained LLMs and hold a smooth tradeoff between performance and compression\nrate. We empirically witness the high data efficiency of our training procedure\nand find that our method can sustain over 90% performance with an average KV\ncache compression rate of 60% (and up to 75% in certain extreme scenarios) for\npopular LLMs like LLaMA2-7B-base and Mistral-7B-v0.3-base."
                },
                "authors": [
                    {
                        "name": "Bokai Lin"
                    },
                    {
                        "name": "Zihao Zeng"
                    },
                    {
                        "name": "Zipeng Xiao"
                    },
                    {
                        "name": "Siqi Kou"
                    },
                    {
                        "name": "Tianqi Hou"
                    },
                    {
                        "name": "Xiaofeng Gao"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Zhijie Deng"
                    }
                ],
                "author_detail": {
                    "name": "Zhijie Deng"
                },
                "author": "Zhijie Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14731v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14731v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10938v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10938v1",
                "updated": "2025-05-16T07:23:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    16,
                    7,
                    23,
                    12,
                    4,
                    136,
                    0
                ],
                "published": "2025-05-16T07:23:12Z",
                "published_parsed": [
                    2025,
                    5,
                    16,
                    7,
                    23,
                    12,
                    4,
                    136,
                    0
                ],
                "title": "Accurate KV Cache Quantization with Outlier Tokens Tracing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate KV Cache Quantization with Outlier Tokens Tracing"
                },
                "summary": "The impressive capabilities of Large Language Models (LLMs) come at the cost\nof substantial computational resources during deployment. While KV Cache can\nsignificantly reduce recomputation during inference, it also introduces\nadditional memory overhead. KV Cache quantization presents a promising\nsolution, striking a good balance between memory usage and accuracy. Previous\nresearch has shown that the Keys are distributed by channel, while the Values\nare distributed by token. Consequently, the common practice is to apply\nchannel-wise quantization to the Keys and token-wise quantization to the\nValues. However, our further investigation reveals that a small subset of\nunusual tokens exhibit unique characteristics that deviate from this pattern,\nwhich can substantially impact quantization accuracy. To address this, we\ndevelop a simple yet effective method to identify these tokens accurately\nduring the decoding process and exclude them from quantization as outlier\ntokens, significantly improving overall accuracy. Extensive experiments show\nthat our method achieves significant accuracy improvements under 2-bit\nquantization and can deliver a 6.4 times reduction in memory usage and a 2.3\ntimes increase in throughput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The impressive capabilities of Large Language Models (LLMs) come at the cost\nof substantial computational resources during deployment. While KV Cache can\nsignificantly reduce recomputation during inference, it also introduces\nadditional memory overhead. KV Cache quantization presents a promising\nsolution, striking a good balance between memory usage and accuracy. Previous\nresearch has shown that the Keys are distributed by channel, while the Values\nare distributed by token. Consequently, the common practice is to apply\nchannel-wise quantization to the Keys and token-wise quantization to the\nValues. However, our further investigation reveals that a small subset of\nunusual tokens exhibit unique characteristics that deviate from this pattern,\nwhich can substantially impact quantization accuracy. To address this, we\ndevelop a simple yet effective method to identify these tokens accurately\nduring the decoding process and exclude them from quantization as outlier\ntokens, significantly improving overall accuracy. Extensive experiments show\nthat our method achieves significant accuracy improvements under 2-bit\nquantization and can deliver a 6.4 times reduction in memory usage and a 2.3\ntimes increase in throughput."
                },
                "authors": [
                    {
                        "name": "Yi Su"
                    },
                    {
                        "name": "Yuechi Zhou"
                    },
                    {
                        "name": "Quantong Qiu"
                    },
                    {
                        "name": "Juntao Li"
                    },
                    {
                        "name": "Qingrong Xia"
                    },
                    {
                        "name": "Ping Li"
                    },
                    {
                        "name": "Xinyu Duan"
                    },
                    {
                        "name": "Zhefeng Wang"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "arxiv_comment": "ACL2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10938v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10938v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.02882v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.02882v3",
                "updated": "2025-05-16T03:34:33Z",
                "updated_parsed": [
                    2025,
                    5,
                    16,
                    3,
                    34,
                    33,
                    4,
                    136,
                    0
                ],
                "published": "2024-04-03T17:33:21Z",
                "published_parsed": [
                    2024,
                    4,
                    3,
                    17,
                    33,
                    21,
                    2,
                    94,
                    0
                ],
                "title": "Linear Attention Sequence Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linear Attention Sequence Parallelism"
                },
                "summary": "Sequence parallelism (SP) serves as a prevalent strategy to handle long\nsequences that exceed the memory limit of a single device. However, for linear\nsequence modeling methods like linear attention, existing SP approaches do not\ntake advantage of their right-product-first feature, resulting in sub-optimal\ncommunication efficiency and usability. In this paper, we introduce Linear\nAttention Sequence Parallelism (LASP), an efficient SP approach designed for\nlinear attention-based transformer models. Specifically, we design an efficient\npoint-to-point ring-style communication mechanism to leverage the right-product\nkernel trick of linear attention, which sharply decreases the communication\noverhead, comparing with existing SP methods. We enhance the computation\nefficiency of LASP by performing kernel fusion and intermediate state caching,\nmaking the implementation of LASP hardware-friendly on GPUs. Furthermore, we\nmeticulously ensure the compatibility of sequence-level LASP with all types of\nbatch-level data parallel methods, which is vital for distributed training on\nlarge clusters with very-long sequences. We also discuss the generalization of\nLASP on other linear sequence modeling methods. Extensive experiments on linear\nattention-based models are conducted with varying sequence lengths from 2K to\n4096K. LASP scales sequence length up to 4096K on 128 GPUs, which is 8$\\times$\nlonger than existing SP methods. Code is available at:\nhttps://github.com/OpenNLPLab/LASP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequence parallelism (SP) serves as a prevalent strategy to handle long\nsequences that exceed the memory limit of a single device. However, for linear\nsequence modeling methods like linear attention, existing SP approaches do not\ntake advantage of their right-product-first feature, resulting in sub-optimal\ncommunication efficiency and usability. In this paper, we introduce Linear\nAttention Sequence Parallelism (LASP), an efficient SP approach designed for\nlinear attention-based transformer models. Specifically, we design an efficient\npoint-to-point ring-style communication mechanism to leverage the right-product\nkernel trick of linear attention, which sharply decreases the communication\noverhead, comparing with existing SP methods. We enhance the computation\nefficiency of LASP by performing kernel fusion and intermediate state caching,\nmaking the implementation of LASP hardware-friendly on GPUs. Furthermore, we\nmeticulously ensure the compatibility of sequence-level LASP with all types of\nbatch-level data parallel methods, which is vital for distributed training on\nlarge clusters with very-long sequences. We also discuss the generalization of\nLASP on other linear sequence modeling methods. Extensive experiments on linear\nattention-based models are conducted with varying sequence lengths from 2K to\n4096K. LASP scales sequence length up to 4096K on 128 GPUs, which is 8$\\times$\nlonger than existing SP methods. Code is available at:\nhttps://github.com/OpenNLPLab/LASP."
                },
                "authors": [
                    {
                        "name": "Weigao Sun"
                    },
                    {
                        "name": "Zhen Qin"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Xuyang Shen"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Yiran Zhong"
                    }
                ],
                "author_detail": {
                    "name": "Yiran Zhong"
                },
                "author": "Yiran Zhong",
                "arxiv_comment": "Accepted by TMLR, 23 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.02882v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.02882v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10806v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10806v1",
                "updated": "2025-05-16T03:01:47Z",
                "updated_parsed": [
                    2025,
                    5,
                    16,
                    3,
                    1,
                    47,
                    4,
                    136,
                    0
                ],
                "published": "2025-05-16T03:01:47Z",
                "published_parsed": [
                    2025,
                    5,
                    16,
                    3,
                    1,
                    47,
                    4,
                    136,
                    0
                ],
                "title": "RapidGNN: Communication Efficient Large-Scale Distributed Training of\n  Graph Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RapidGNN: Communication Efficient Large-Scale Distributed Training of\n  Graph Neural Networks"
                },
                "summary": "Graph Neural Networks (GNNs) have achieved state-of-the-art (SOTA)\nperformance in diverse domains. However, training GNNs on large-scale graphs\nposes significant challenges due to high memory demands and significant\ncommunication overhead in distributed settings. Traditional sampling-based\napproaches mitigate computation load to some extent but often fail to address\ncommunication inefficiencies inherent in distributed environments. This paper\npresents RapidGNN that introduces a deterministic sampling strategy to\nprecompute mini-batches. By leveraging the sampling strategy, RapidGNN\naccurately anticipates feature access patterns, enabling optimal cache\nconstruction and timely prefetching of remote features. This reduces the\nfrequency and latency of remote data transfers without compromising the\nstochastic nature of training. Evaluations on Reddit and OGBN-Products datasets\ndemonstrate that RapidGNN achieves significant reductions in training time and\nremote feature fetches, outperforming existing models in both communication\nefficiency and throughput. Our findings highlight RapidGNN's potential for\nscalable, high-performance GNN training across large, real-world graph datasets\nalong with improving energy efficiency. Our model improves end-to-end training\nthroughput by 2.10x on average over SOTA model GraphSAGE-METIS (up to 2.45x in\nsome settings), while cutting remote feature fetches by over 4x. It also\nreduces energy consumption up to 23%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNNs) have achieved state-of-the-art (SOTA)\nperformance in diverse domains. However, training GNNs on large-scale graphs\nposes significant challenges due to high memory demands and significant\ncommunication overhead in distributed settings. Traditional sampling-based\napproaches mitigate computation load to some extent but often fail to address\ncommunication inefficiencies inherent in distributed environments. This paper\npresents RapidGNN that introduces a deterministic sampling strategy to\nprecompute mini-batches. By leveraging the sampling strategy, RapidGNN\naccurately anticipates feature access patterns, enabling optimal cache\nconstruction and timely prefetching of remote features. This reduces the\nfrequency and latency of remote data transfers without compromising the\nstochastic nature of training. Evaluations on Reddit and OGBN-Products datasets\ndemonstrate that RapidGNN achieves significant reductions in training time and\nremote feature fetches, outperforming existing models in both communication\nefficiency and throughput. Our findings highlight RapidGNN's potential for\nscalable, high-performance GNN training across large, real-world graph datasets\nalong with improving energy efficiency. Our model improves end-to-end training\nthroughput by 2.10x on average over SOTA model GraphSAGE-METIS (up to 2.45x in\nsome settings), while cutting remote feature fetches by over 4x. It also\nreduces energy consumption up to 23%."
                },
                "authors": [
                    {
                        "name": "Arefin Niam"
                    },
                    {
                        "name": "M S Q Zulkar Nine"
                    }
                ],
                "author_detail": {
                    "name": "M S Q Zulkar Nine"
                },
                "author": "M S Q Zulkar Nine",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10806v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10806v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17720v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17720v2",
                "updated": "2025-05-16T00:56:30Z",
                "updated_parsed": [
                    2025,
                    5,
                    16,
                    0,
                    56,
                    30,
                    4,
                    136,
                    0
                ],
                "published": "2024-11-20T19:44:26Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    19,
                    44,
                    26,
                    2,
                    325,
                    0
                ],
                "title": "MAS-Attention: Memory-Aware Stream Processing for Attention Acceleration\n  on Resource-Constrained Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAS-Attention: Memory-Aware Stream Processing for Attention Acceleration\n  on Resource-Constrained Edge Devices"
                },
                "summary": "The advent of foundation models have revolutionized various fields, enabling\nunprecedented task accuracy and flexibility in computational linguistics,\ncomputer vision and other domains. Attention mechanism has become an essential\ncomponent of foundation models, due to their superb capability of capturing\ncorrelations in a sequence. However, attention results in quadratic complexity\nin memory and compute as the context length grows. Although many fusion-based\nexact attention acceleration algorithms have been developed for\ndatacenter-grade GPUs and accelerators leveraging multi-core parallelism and\ndata locality, yet it remains a significant challenge to accelerate attention\non resource-constrained edge neural accelerators with limited compute units and\nstringent on-chip caches. In this paper, we propose a scheme for exact\nattention inference acceleration on memory-constrained edge accelerators, by\nparallelizing the utilization of heterogeneous compute units, i.e., vector\nprocessing units and matrix processing units. Our method involves scheduling\nworkloads onto these different compute units in a multi-tiered tiling scheme to\nprocess tiled vector workloads and matrix workloads in attention as two\nstreams, respecting the workload dependencies. We search for tiling factors to\nmaximize the parallelization of both compute units while considering I/O\noverhead, and propose a proactive cache overwrite strategy to avoid undesirable\ncache spills in reality. Extensive results based on open-sourced simulation\nframeworks show up to 2.75x speedup and 54% reduction in energy consumption as\ncompared to the state-of-the-art attention fusion method (FLAT) in the edge\ncomputing scenario. Further experiments on a real-world edge neural processing\nunit demonstrate speedup of up to 1.76x for attention as compared to FLAT,\nwithout affecting model output accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of foundation models have revolutionized various fields, enabling\nunprecedented task accuracy and flexibility in computational linguistics,\ncomputer vision and other domains. Attention mechanism has become an essential\ncomponent of foundation models, due to their superb capability of capturing\ncorrelations in a sequence. However, attention results in quadratic complexity\nin memory and compute as the context length grows. Although many fusion-based\nexact attention acceleration algorithms have been developed for\ndatacenter-grade GPUs and accelerators leveraging multi-core parallelism and\ndata locality, yet it remains a significant challenge to accelerate attention\non resource-constrained edge neural accelerators with limited compute units and\nstringent on-chip caches. In this paper, we propose a scheme for exact\nattention inference acceleration on memory-constrained edge accelerators, by\nparallelizing the utilization of heterogeneous compute units, i.e., vector\nprocessing units and matrix processing units. Our method involves scheduling\nworkloads onto these different compute units in a multi-tiered tiling scheme to\nprocess tiled vector workloads and matrix workloads in attention as two\nstreams, respecting the workload dependencies. We search for tiling factors to\nmaximize the parallelization of both compute units while considering I/O\noverhead, and propose a proactive cache overwrite strategy to avoid undesirable\ncache spills in reality. Extensive results based on open-sourced simulation\nframeworks show up to 2.75x speedup and 54% reduction in energy consumption as\ncompared to the state-of-the-art attention fusion method (FLAT) in the edge\ncomputing scenario. Further experiments on a real-world edge neural processing\nunit demonstrate speedup of up to 1.76x for attention as compared to FLAT,\nwithout affecting model output accuracy."
                },
                "authors": [
                    {
                        "name": "Mohammadali Shakerdargah"
                    },
                    {
                        "name": "Shan Lu"
                    },
                    {
                        "name": "Chao Gao"
                    },
                    {
                        "name": "Di Niu"
                    }
                ],
                "author_detail": {
                    "name": "Di Niu"
                },
                "author": "Di Niu",
                "arxiv_comment": "Accepted to MLSys 2025,",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17720v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17720v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.1.4; I.2.7; I.5.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10560v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10560v1",
                "updated": "2025-05-15T17:59:24Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    17,
                    59,
                    24,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T17:59:24Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    17,
                    59,
                    24,
                    3,
                    135,
                    0
                ],
                "title": "Approximation-First Timeseries Monitoring Query At Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Approximation-First Timeseries Monitoring Query At Scale"
                },
                "summary": "Timeseries monitoring systems such as Prometheus play a crucial role in\ngaining observability of the underlying system components. These systems\ncollect timeseries metrics from various system components and perform\nmonitoring queries over periodic window-based aggregations (i.e., rule\nqueries). However, despite wide adoption, the operational costs and query\nlatency of rule queries remain high. In this paper, we identify major\nbottlenecks associated with repeated data scans and query computations\nconcerning window overlaps in rule queries, and present PromSketch, an\napproximation-first query framework as intermediate caches for monitoring\nsystems. It enables low operational costs and query latency, by combining\napproximate window-based query frameworks and sketch-based precomputation.\nPromSketch is implemented as a standalone module that can be integrated into\nPrometheus and VictoriaMetrics, covering 70% of Prometheus' aggregation over\ntime queries. Our evaluation shows that PromSketch achieves up to a two orders\nof magnitude reduction in query latency over Prometheus and VictoriaMetrics,\nwhile lowering operational dollar costs of query processing by two orders of\nmagnitude compared to Prometheus and by at least 4x compared to VictoriaMetrics\nwith at most 5% average errors across statistics. The source code has been made\navailable at https://github.com/Froot-NetSys/promsketch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Timeseries monitoring systems such as Prometheus play a crucial role in\ngaining observability of the underlying system components. These systems\ncollect timeseries metrics from various system components and perform\nmonitoring queries over periodic window-based aggregations (i.e., rule\nqueries). However, despite wide adoption, the operational costs and query\nlatency of rule queries remain high. In this paper, we identify major\nbottlenecks associated with repeated data scans and query computations\nconcerning window overlaps in rule queries, and present PromSketch, an\napproximation-first query framework as intermediate caches for monitoring\nsystems. It enables low operational costs and query latency, by combining\napproximate window-based query frameworks and sketch-based precomputation.\nPromSketch is implemented as a standalone module that can be integrated into\nPrometheus and VictoriaMetrics, covering 70% of Prometheus' aggregation over\ntime queries. Our evaluation shows that PromSketch achieves up to a two orders\nof magnitude reduction in query latency over Prometheus and VictoriaMetrics,\nwhile lowering operational dollar costs of query processing by two orders of\nmagnitude compared to Prometheus and by at least 4x compared to VictoriaMetrics\nwith at most 5% average errors across statistics. The source code has been made\navailable at https://github.com/Froot-NetSys/promsketch."
                },
                "authors": [
                    {
                        "name": "Zeying Zhu"
                    },
                    {
                        "name": "Jonathan Chamberlain"
                    },
                    {
                        "name": "Kenny Wu"
                    },
                    {
                        "name": "David Starobinski"
                    },
                    {
                        "name": "Zaoxing Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zaoxing Liu"
                },
                "author": "Zaoxing Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10560v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10560v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02069v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02069v4",
                "updated": "2025-05-15T17:18:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    17,
                    18,
                    12,
                    3,
                    135,
                    0
                ],
                "published": "2024-06-04T07:51:30Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    7,
                    51,
                    30,
                    1,
                    156,
                    0
                ],
                "title": "PyramidKV: Dynamic KV Cache Compression based on Pyramidal Information\n  Funneling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PyramidKV: Dynamic KV Cache Compression based on Pyramidal Information\n  Funneling"
                },
                "summary": "In this study, we investigate whether attention-based information flow inside\nlarge language models (LLMs) is aggregated through noticeable patterns for long\ncontext processing. Our observations reveal that LLMs aggregate information\nthrough Pyramidal Information Funneling where attention is scattering widely in\nlower layers, progressively consolidating within specific contexts, and\nultimately focusing on critical tokens (a.k.a massive activation or attention\nsink) in higher layers. Motivated by these insights, we developed PyramidKV, a\nnovel and effective KV cache compression method. This approach dynamically\nadjusts the KV cache size across different layers, allocating more cache in\nlower layers and less in higher ones, diverging from traditional methods that\nmaintain a uniform KV cache size. Our experimental evaluations, utilizing the\nLongBench benchmark, show that PyramidKV matches the performance of models with\na full KV cache while retaining only 12% of the KV cache, thus significantly\nreducing memory usage. In scenarios emphasizing memory efficiency, where only\n0.7% of the KV cache is maintained, PyramidKV surpasses other KV cache\ncompression techniques, achieving up to a 20.5 absolute accuracy improvement on\nTREC dataset. In the Needle-in-a-Haystack experiment, PyramidKV outperforms\ncompeting methods in maintaining long-context comprehension in LLMs; notably,\nretaining just 128 KV cache entries enables the LLAMA-3-70B model to achieve\n100.0 Acc. performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we investigate whether attention-based information flow inside\nlarge language models (LLMs) is aggregated through noticeable patterns for long\ncontext processing. Our observations reveal that LLMs aggregate information\nthrough Pyramidal Information Funneling where attention is scattering widely in\nlower layers, progressively consolidating within specific contexts, and\nultimately focusing on critical tokens (a.k.a massive activation or attention\nsink) in higher layers. Motivated by these insights, we developed PyramidKV, a\nnovel and effective KV cache compression method. This approach dynamically\nadjusts the KV cache size across different layers, allocating more cache in\nlower layers and less in higher ones, diverging from traditional methods that\nmaintain a uniform KV cache size. Our experimental evaluations, utilizing the\nLongBench benchmark, show that PyramidKV matches the performance of models with\na full KV cache while retaining only 12% of the KV cache, thus significantly\nreducing memory usage. In scenarios emphasizing memory efficiency, where only\n0.7% of the KV cache is maintained, PyramidKV surpasses other KV cache\ncompression techniques, achieving up to a 20.5 absolute accuracy improvement on\nTREC dataset. In the Needle-in-a-Haystack experiment, PyramidKV outperforms\ncompeting methods in maintaining long-context comprehension in LLMs; notably,\nretaining just 128 KV cache entries enables the LLAMA-3-70B model to achieve\n100.0 Acc. performance."
                },
                "authors": [
                    {
                        "name": "Zefan Cai"
                    },
                    {
                        "name": "Yichi Zhang"
                    },
                    {
                        "name": "Bofei Gao"
                    },
                    {
                        "name": "Yuliang Liu"
                    },
                    {
                        "name": "Yucheng Li"
                    },
                    {
                        "name": "Tianyu Liu"
                    },
                    {
                        "name": "Keming Lu"
                    },
                    {
                        "name": "Wayne Xiong"
                    },
                    {
                        "name": "Yue Dong"
                    },
                    {
                        "name": "Junjie Hu"
                    },
                    {
                        "name": "Wen Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Wen Xiao"
                },
                "author": "Wen Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02069v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02069v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.11554v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.11554v1",
                "updated": "2025-05-15T16:40:14Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    16,
                    40,
                    14,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T16:40:14Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    16,
                    40,
                    14,
                    3,
                    135,
                    0
                ],
                "title": "Multi-Objective Memory Bandwidth Regulation and Cache Partitioning for\n  Multicore Real-Time Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Objective Memory Bandwidth Regulation and Cache Partitioning for\n  Multicore Real-Time Systems"
                },
                "summary": "Memory bandwidth regulation and cache partitioning are widely used techniques\nfor achieving predictable timing in real-time computing systems. Combined with\npartitioned scheduling, these methods require careful co-allocation of tasks\nand resources to cores, as task execution times strongly depend on available\nallocated resources. To address this challenge, this paper presents a 0-1\nlinear program for task-resource co-allocation, along with a multi-objective\nheuristic designed to minimize resource usage while guaranteeing schedulability\nunder a preemptive EDF scheduling policy. Our heuristic employs a multi-layer\nframework, where an outer layer explores resource allocations using\nPareto-pruned search, and an inner layer optimizes task allocation by solving a\nknapsack problem using dynamic programming. To evaluate the performance of the\nproposed optimization algorithm, we profile real-world benchmarks on an\nembedded AMD UltraScale+ ZCU102 platform, with fine-grained resource\npartitioning enabled by the Jailhouse hypervisor, leveraging cache set\npartitioning and MemGuard for memory bandwidth regulation. Experiments based on\nthe benchmarking results show that the proposed 0-1 linear program outperforms\nexisting mixed-integer programs by finding more optimal solutions within the\nsame time limit. Moreover, the proposed multi-objective multi-layer heuristic\nperforms consistently better than the state-of-the-art multi-resource-task\nco-allocation algorithm in terms of schedulability, resource usage, number of\nnon-dominated solutions, and computational efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory bandwidth regulation and cache partitioning are widely used techniques\nfor achieving predictable timing in real-time computing systems. Combined with\npartitioned scheduling, these methods require careful co-allocation of tasks\nand resources to cores, as task execution times strongly depend on available\nallocated resources. To address this challenge, this paper presents a 0-1\nlinear program for task-resource co-allocation, along with a multi-objective\nheuristic designed to minimize resource usage while guaranteeing schedulability\nunder a preemptive EDF scheduling policy. Our heuristic employs a multi-layer\nframework, where an outer layer explores resource allocations using\nPareto-pruned search, and an inner layer optimizes task allocation by solving a\nknapsack problem using dynamic programming. To evaluate the performance of the\nproposed optimization algorithm, we profile real-world benchmarks on an\nembedded AMD UltraScale+ ZCU102 platform, with fine-grained resource\npartitioning enabled by the Jailhouse hypervisor, leveraging cache set\npartitioning and MemGuard for memory bandwidth regulation. Experiments based on\nthe benchmarking results show that the proposed 0-1 linear program outperforms\nexisting mixed-integer programs by finding more optimal solutions within the\nsame time limit. Moreover, the proposed multi-objective multi-layer heuristic\nperforms consistently better than the state-of-the-art multi-resource-task\nco-allocation algorithm in terms of schedulability, resource usage, number of\nnon-dominated solutions, and computational efficiency."
                },
                "authors": [
                    {
                        "name": "Binqi Sun"
                    },
                    {
                        "name": "Zhihang Wei"
                    },
                    {
                        "name": "Andrea Bastoni"
                    },
                    {
                        "name": "Debayan Roy"
                    },
                    {
                        "name": "Mirco Theile"
                    },
                    {
                        "name": "Tomasz Kloda"
                    },
                    {
                        "name": "Rodolfo Pellizzoni"
                    },
                    {
                        "name": "Marco Caccamo"
                    }
                ],
                "author_detail": {
                    "name": "Marco Caccamo"
                },
                "author": "Marco Caccamo",
                "arxiv_doi": "10.4230/LIPIcs.ECRTS.2025.7",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.4230/LIPIcs.ECRTS.2025.7",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2505.11554v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.11554v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted in the 37th Euromicro Conference on Real-Time Systems (ECRTS\n  2025)",
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02380v7",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02380v7",
                "updated": "2025-05-15T13:48:40Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    13,
                    48,
                    40,
                    3,
                    135,
                    0
                ],
                "published": "2025-01-04T20:59:34Z",
                "published_parsed": [
                    2025,
                    1,
                    4,
                    20,
                    59,
                    34,
                    5,
                    4,
                    0
                ],
                "title": "Reciprocating Locks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reciprocating Locks"
                },
                "summary": "We present \"Reciprocating Locks\", a novel mutual exclusion locking algorithm,\ntargeting cache-coherent shared memory (CC), that enjoys a number of desirable\nproperties. The doorway arrival phase and the release operation both run in\nconstant-time. Waiting threads use local spinning and only a single waiting\nelement is required per thread, regardless of the number of locks a thread\nmight hold at a given time. While our lock does not provide strict FIFO\nadmission, it bounds bypass and has strong anti-starvation properties. The lock\nis compact, space efficient, and has been intentionally designed to be readily\nusable in real-world general purpose computing environments such as the linux\nkernel, pthreads, or C++. We show the lock exhibits high throughput under\ncontention and low latency in the uncontended case. The performance of\nReciprocating Locks is competitive with and often better than the best\nstate-of-the-art scalable spin locks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present \"Reciprocating Locks\", a novel mutual exclusion locking algorithm,\ntargeting cache-coherent shared memory (CC), that enjoys a number of desirable\nproperties. The doorway arrival phase and the release operation both run in\nconstant-time. Waiting threads use local spinning and only a single waiting\nelement is required per thread, regardless of the number of locks a thread\nmight hold at a given time. While our lock does not provide strict FIFO\nadmission, it bounds bypass and has strong anti-starvation properties. The lock\nis compact, space efficient, and has been intentionally designed to be readily\nusable in real-world general purpose computing environments such as the linux\nkernel, pthreads, or C++. We show the lock exhibits high throughput under\ncontention and low latency in the uncontended case. The performance of\nReciprocating Locks is competitive with and often better than the best\nstate-of-the-art scalable spin locks."
                },
                "authors": [
                    {
                        "name": "Dave Dice"
                    },
                    {
                        "name": "Alex Kogan"
                    }
                ],
                "author_detail": {
                    "name": "Alex Kogan"
                },
                "author": "Alex Kogan",
                "arxiv_comment": "Added additional variations in appendix, at the request of\n  collaborators who want to prove various properties",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02380v7",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02380v7",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.4.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13779v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13779v3",
                "updated": "2025-05-15T03:29:15Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    3,
                    29,
                    15,
                    3,
                    135,
                    0
                ],
                "published": "2024-12-18T12:16:41Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    16,
                    41,
                    2,
                    353,
                    0
                ],
                "title": "Rehearsal-Free Continual Federated Learning with Synergistic Synaptic\n  Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rehearsal-Free Continual Federated Learning with Synergistic Synaptic\n  Intelligence"
                },
                "summary": "Continual Federated Learning (CFL) allows distributed devices to\ncollaboratively learn novel concepts from continuously shifting training data\nwhile avoiding knowledge forgetting of previously seen tasks. To tackle this\nchallenge, most current CFL approaches rely on extensive rehearsal of previous\ndata. Despite effectiveness, rehearsal comes at a cost to memory, and it may\nalso violate data privacy. Considering these, we seek to apply regularization\ntechniques to CFL by considering their cost-efficient properties that do not\nrequire sample caching or rehearsal. Specifically, we first apply traditional\nregularization techniques to CFL and observe that existing regularization\ntechniques, especially synaptic intelligence, can achieve promising results\nunder homogeneous data distribution but fail when the data is heterogeneous.\nBased on this observation, we propose a simple yet effective regularization\nalgorithm for CFL named FedSSI, which tailors the synaptic intelligence for the\nCFL with heterogeneous data settings. FedSSI can not only reduce computational\noverhead without rehearsal but also address the data heterogeneity issue.\nExtensive experiments show that FedSSI achieves superior performance compared\nto state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual Federated Learning (CFL) allows distributed devices to\ncollaboratively learn novel concepts from continuously shifting training data\nwhile avoiding knowledge forgetting of previously seen tasks. To tackle this\nchallenge, most current CFL approaches rely on extensive rehearsal of previous\ndata. Despite effectiveness, rehearsal comes at a cost to memory, and it may\nalso violate data privacy. Considering these, we seek to apply regularization\ntechniques to CFL by considering their cost-efficient properties that do not\nrequire sample caching or rehearsal. Specifically, we first apply traditional\nregularization techniques to CFL and observe that existing regularization\ntechniques, especially synaptic intelligence, can achieve promising results\nunder homogeneous data distribution but fail when the data is heterogeneous.\nBased on this observation, we propose a simple yet effective regularization\nalgorithm for CFL named FedSSI, which tailors the synaptic intelligence for the\nCFL with heterogeneous data settings. FedSSI can not only reduce computational\noverhead without rehearsal but also address the data heterogeneity issue.\nExtensive experiments show that FedSSI achieves superior performance compared\nto state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Yichen Li"
                    },
                    {
                        "name": "Yuying Wang"
                    },
                    {
                        "name": "Haozhao Wang"
                    },
                    {
                        "name": "Yining Qi"
                    },
                    {
                        "name": "Tianzhe Xiao"
                    },
                    {
                        "name": "Ruixuan Li"
                    }
                ],
                "author_detail": {
                    "name": "Ruixuan Li"
                },
                "author": "Ruixuan Li",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2403.05890",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13779v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13779v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16112v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16112v2",
                "updated": "2025-05-15T03:27:28Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    3,
                    27,
                    28,
                    3,
                    135,
                    0
                ],
                "published": "2025-03-20T13:00:36Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    13,
                    0,
                    36,
                    3,
                    79,
                    0
                ],
                "title": "PromptMobile: Efficient Promptus for Low Bandwidth Mobile Video\n  Streaming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PromptMobile: Efficient Promptus for Low Bandwidth Mobile Video\n  Streaming"
                },
                "summary": "Traditional video compression algorithms exhibit significant quality\ndegradation at extremely low bitrates. Promptus emerges as a new paradigm for\nvideo streaming, substantially cutting down the bandwidth essential for video\nstreaming. However, Promptus is computationally intensive and can not run in\nreal-time on mobile devices. This paper presents PromptMobile, an efficient\nacceleration framework tailored for on-device Promptus. Specifically, we\npropose (1) a two-stage efficient generation framework to reduce computational\ncost by 8.1x, (2) a fine-grained inter-frame caching to reduce redundant\ncomputations by 16.6%, (3) system-level optimizations to further enhance\nefficiency. The evaluations demonstrate that compared with the original\nPromptus, PromptMobile achieves a 13.6x increase in image generation speed.\nCompared with other streaming methods, PromptMobile achives an average LPIPS\nimprovement of 0.016 (compared with H.265), reducing 60% of severely distorted\nframes (compared to VQGAN).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional video compression algorithms exhibit significant quality\ndegradation at extremely low bitrates. Promptus emerges as a new paradigm for\nvideo streaming, substantially cutting down the bandwidth essential for video\nstreaming. However, Promptus is computationally intensive and can not run in\nreal-time on mobile devices. This paper presents PromptMobile, an efficient\nacceleration framework tailored for on-device Promptus. Specifically, we\npropose (1) a two-stage efficient generation framework to reduce computational\ncost by 8.1x, (2) a fine-grained inter-frame caching to reduce redundant\ncomputations by 16.6%, (3) system-level optimizations to further enhance\nefficiency. The evaluations demonstrate that compared with the original\nPromptus, PromptMobile achieves a 13.6x increase in image generation speed.\nCompared with other streaming methods, PromptMobile achives an average LPIPS\nimprovement of 0.016 (compared with H.265), reducing 60% of severely distorted\nframes (compared to VQGAN)."
                },
                "authors": [
                    {
                        "name": "Liming Liu"
                    },
                    {
                        "name": "Jiangkai Wu"
                    },
                    {
                        "name": "Haoyang Wang"
                    },
                    {
                        "name": "Peiheng Wang"
                    },
                    {
                        "name": "Zongming Guo"
                    },
                    {
                        "name": "Xinggong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xinggong Zhang"
                },
                "author": "Xinggong Zhang",
                "arxiv_doi": "10.1145/3735358.3735383",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3735358.3735383",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.16112v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16112v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "6 pages (excluding references), 10 figures, to appear in APNET 2025",
                "arxiv_journal_ref": "Proc. 9th Asia-Pacific Workshop on Networking (APNET), Aug 2025,\n  Paper No. 24",
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.06738v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.06738v2",
                "updated": "2025-05-14T16:04:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    16,
                    4,
                    57,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-10T19:06:37Z",
                "published_parsed": [
                    2025,
                    5,
                    10,
                    19,
                    6,
                    37,
                    5,
                    130,
                    0
                ],
                "title": "I Know What You Said: Unveiling Hardware Cache Side-Channels in Local\n  Large Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "I Know What You Said: Unveiling Hardware Cache Side-Channels in Local\n  Large Language Model Inference"
                },
                "summary": "Large Language Models (LLMs) that can be deployed locally have recently\ngained popularity for privacy-sensitive tasks, with companies such as Meta,\nGoogle, and Intel playing significant roles in their development. However, the\nsecurity of local LLMs through the lens of hardware cache side-channels remains\nunexplored. In this paper, we unveil novel side-channel vulnerabilities in\nlocal LLM inference: token value and token position leakage, which can expose\nboth the victim's input and output text, thereby compromising user privacy.\nSpecifically, we found that adversaries can infer the token values from the\ncache access patterns of the token embedding operation, and deduce the token\npositions from the timing of autoregressive decoding phases. To demonstrate the\npotential of these leaks, we design a novel eavesdropping attack framework\ntargeting both open-source and proprietary LLM inference systems. The attack\nframework does not directly interact with the victim's LLM and can be executed\nwithout privilege.\n  We evaluate the attack on a range of practical local LLM deployments (e.g.,\nLlama, Falcon, and Gemma), and the results show that our attack achieves\npromising accuracy. The restored output and input text have an average edit\ndistance of 5.2% and 17.3% to the ground truth, respectively. Furthermore, the\nreconstructed texts achieve average cosine similarity scores of 98.7% (input)\nand 98.0% (output).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) that can be deployed locally have recently\ngained popularity for privacy-sensitive tasks, with companies such as Meta,\nGoogle, and Intel playing significant roles in their development. However, the\nsecurity of local LLMs through the lens of hardware cache side-channels remains\nunexplored. In this paper, we unveil novel side-channel vulnerabilities in\nlocal LLM inference: token value and token position leakage, which can expose\nboth the victim's input and output text, thereby compromising user privacy.\nSpecifically, we found that adversaries can infer the token values from the\ncache access patterns of the token embedding operation, and deduce the token\npositions from the timing of autoregressive decoding phases. To demonstrate the\npotential of these leaks, we design a novel eavesdropping attack framework\ntargeting both open-source and proprietary LLM inference systems. The attack\nframework does not directly interact with the victim's LLM and can be executed\nwithout privilege.\n  We evaluate the attack on a range of practical local LLM deployments (e.g.,\nLlama, Falcon, and Gemma), and the results show that our attack achieves\npromising accuracy. The restored output and input text have an average edit\ndistance of 5.2% and 17.3% to the ground truth, respectively. Furthermore, the\nreconstructed texts achieve average cosine similarity scores of 98.7% (input)\nand 98.0% (output)."
                },
                "authors": [
                    {
                        "name": "Zibo Gao"
                    },
                    {
                        "name": "Junjie Hu"
                    },
                    {
                        "name": "Feng Guo"
                    },
                    {
                        "name": "Yixin Zhang"
                    },
                    {
                        "name": "Yinglong Han"
                    },
                    {
                        "name": "Siyuan Liu"
                    },
                    {
                        "name": "Haiyang Li"
                    },
                    {
                        "name": "Zhiqiang Lv"
                    }
                ],
                "author_detail": {
                    "name": "Zhiqiang Lv"
                },
                "author": "Zhiqiang Lv",
                "arxiv_comment": "Submitted for review in January 22, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.06738v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.06738v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "K.6.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10584v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10584v1",
                "updated": "2025-05-14T13:39:53Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    13,
                    39,
                    53,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T13:39:53Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    13,
                    39,
                    53,
                    2,
                    134,
                    0
                ],
                "title": "Aquarius: A Family of Industry-Level Video Generation Models for\n  Marketing Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aquarius: A Family of Industry-Level Video Generation Models for\n  Marketing Scenarios"
                },
                "summary": "This report introduces Aquarius, a family of industry-level video generation\nmodels for marketing scenarios designed for thousands-xPU clusters and models\nwith hundreds of billions of parameters. Leveraging efficient engineering\narchitecture and algorithmic innovation, Aquarius demonstrates exceptional\nperformance in high-fidelity, multi-aspect-ratio, and long-duration video\nsynthesis. By disclosing the framework's design details, we aim to demystify\nindustrial-scale video generation systems and catalyze advancements in the\ngenerative video community. The Aquarius framework consists of five components:\nDistributed Graph and Video Data Processing Pipeline: Manages tens of thousands\nof CPUs and thousands of xPUs via automated task distribution, enabling\nefficient video data processing. Additionally, we are about to open-source the\nentire data processing framework named \"Aquarius-Datapipe\". Model Architectures\nfor Different Scales: Include a Single-DiT architecture for 2B models and a\nMultimodal-DiT architecture for 13.4B models, supporting multi-aspect ratios,\nmulti-resolution, and multi-duration video generation. High-Performance\ninfrastructure designed for video generation model training: Incorporating\nhybrid parallelism and fine-grained memory optimization strategies, this\ninfrastructure achieves 36% MFU at large scale. Multi-xPU Parallel Inference\nAcceleration: Utilizes diffusion cache and attention optimization to achieve a\n2.35x inference speedup. Multiple marketing-scenarios applications: Including\nimage-to-video, text-to-video (avatar), video inpainting and video\npersonalization, among others. More downstream applications and\nmulti-dimensional evaluation metrics will be added in the upcoming version\nupdates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This report introduces Aquarius, a family of industry-level video generation\nmodels for marketing scenarios designed for thousands-xPU clusters and models\nwith hundreds of billions of parameters. Leveraging efficient engineering\narchitecture and algorithmic innovation, Aquarius demonstrates exceptional\nperformance in high-fidelity, multi-aspect-ratio, and long-duration video\nsynthesis. By disclosing the framework's design details, we aim to demystify\nindustrial-scale video generation systems and catalyze advancements in the\ngenerative video community. The Aquarius framework consists of five components:\nDistributed Graph and Video Data Processing Pipeline: Manages tens of thousands\nof CPUs and thousands of xPUs via automated task distribution, enabling\nefficient video data processing. Additionally, we are about to open-source the\nentire data processing framework named \"Aquarius-Datapipe\". Model Architectures\nfor Different Scales: Include a Single-DiT architecture for 2B models and a\nMultimodal-DiT architecture for 13.4B models, supporting multi-aspect ratios,\nmulti-resolution, and multi-duration video generation. High-Performance\ninfrastructure designed for video generation model training: Incorporating\nhybrid parallelism and fine-grained memory optimization strategies, this\ninfrastructure achieves 36% MFU at large scale. Multi-xPU Parallel Inference\nAcceleration: Utilizes diffusion cache and attention optimization to achieve a\n2.35x inference speedup. Multiple marketing-scenarios applications: Including\nimage-to-video, text-to-video (avatar), video inpainting and video\npersonalization, among others. More downstream applications and\nmulti-dimensional evaluation metrics will be added in the upcoming version\nupdates."
                },
                "authors": [
                    {
                        "name": "Huafeng Shi"
                    },
                    {
                        "name": "Jianzhong Liang"
                    },
                    {
                        "name": "Rongchang Xie"
                    },
                    {
                        "name": "Xian Wu"
                    },
                    {
                        "name": "Cheng Chen"
                    },
                    {
                        "name": "Chang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Chang Liu"
                },
                "author": "Chang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10584v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10584v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10659v7",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10659v7",
                "updated": "2025-05-14T04:38:42Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    4,
                    38,
                    42,
                    2,
                    134,
                    0
                ],
                "published": "2024-11-16T01:39:44Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    1,
                    39,
                    44,
                    5,
                    321,
                    0
                ],
                "title": "Spineless Traversal for Layout Invalidation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spineless Traversal for Layout Invalidation"
                },
                "summary": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty, and only those\nelements are processed to draw the next frame, dramatically reducing latency.\nHowever, the standard incremental layout algorithm must search the page for\ndirty elements, accessing auxiliary elements in the process. These auxiliary\nelements add cache misses and stalled cycles, and are responsible for a sizable\nfraction of all layout latency. We introduce a new, faster incremental layout\nalgorithm called Spineless Traversal. Spineless Traversal uses a\ncache-friendlier priority queue algorithm that avoids accessing auxiliary nodes\nand thus reduces cache traffic and stalls. This leads to dramatic speedups on\nthe most latency-critical interactions such as hovering, typing, and animation.\nMoreover, thanks to numerous low-level optimizations, Spineless Traversal is\ncompetitive across the whole spectrum of incremental layout workloads.\nSpineless Traversal is faster than the standard approach on 83.0% of 2216\nbenchmarks, with a mean speedup of 1.80x concentrated in the most\nlatency-critical interactions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty, and only those\nelements are processed to draw the next frame, dramatically reducing latency.\nHowever, the standard incremental layout algorithm must search the page for\ndirty elements, accessing auxiliary elements in the process. These auxiliary\nelements add cache misses and stalled cycles, and are responsible for a sizable\nfraction of all layout latency. We introduce a new, faster incremental layout\nalgorithm called Spineless Traversal. Spineless Traversal uses a\ncache-friendlier priority queue algorithm that avoids accessing auxiliary nodes\nand thus reduces cache traffic and stalls. This leads to dramatic speedups on\nthe most latency-critical interactions such as hovering, typing, and animation.\nMoreover, thanks to numerous low-level optimizations, Spineless Traversal is\ncompetitive across the whole spectrum of incremental layout workloads.\nSpineless Traversal is faster than the standard approach on 83.0% of 2216\nbenchmarks, with a mean speedup of 1.80x concentrated in the most\nlatency-critical interactions."
                },
                "authors": [
                    {
                        "name": "Marisa Kirisame"
                    },
                    {
                        "name": "Tiezhi Wang"
                    },
                    {
                        "name": "Pavel Panchekha"
                    }
                ],
                "author_detail": {
                    "name": "Pavel Panchekha"
                },
                "author": "Pavel Panchekha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10659v7",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10659v7",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18599v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18599v2",
                "updated": "2025-05-14T04:22:24Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    4,
                    22,
                    24,
                    2,
                    134,
                    0
                ],
                "published": "2025-03-24T11:56:50Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    11,
                    56,
                    50,
                    0,
                    83,
                    0
                ],
                "title": "Oaken: Fast and Efficient LLM Serving with Online-Offline Hybrid KV\n  Cache Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Oaken: Fast and Efficient LLM Serving with Online-Offline Hybrid KV\n  Cache Quantization"
                },
                "summary": "Modern Large Language Model serving system batches multiple requests to\nachieve high throughput, while batching attention operations is challenging,\nrendering memory bandwidth a critical bottleneck. The community relies on\nhigh-end GPUs with multiple high-bandwidth memory channels. Unfortunately,\nHBM's high bandwidth often comes at the expense of limited memory capacity,\nwhich reduces core utilization and increases costs. Recent advancements\nenabling longer contexts for LLMs have substantially increased the key-value\ncache size, further intensifying the pressures on memory capacity. The\nliterature has explored KV cache quantization techniques, which commonly use\nlow bitwidth for most values, selectively using higher bitwidth for outlier\nvalues. While this approach helps achieve high accuracy and low bitwidth\nsimultaneously, it comes with the limitation that cost for online outlier\ndetection is excessively high, negating the advantages. We propose Oaken, an\nacceleration solution that achieves high accuracy and high performance\nsimultaneously through co-designing algorithm and hardware. To effectively find\na sweet spot in the accuracy-performance trade-off space of KV cache\nquantization, Oaken employs an online-offline hybrid approach, setting outlier\nthresholds offline, which are then used to determine the quantization scale\nonline. To translate the proposed algorithmic technique into tangible\nperformance gains, Oaken also comes with custom quantization engines and memory\nmanagement units that can be integrated with any LLM accelerators. We built an\nOaken accelerator on top of an LLM accelerator, LPU, and conducted a\ncomprehensive evaluation. Our experiments show that for a batch size of 256,\nOaken achieves up to 1.58x throughput improvement over NVIDIA A100 GPU,\nincurring a minimal accuracy loss of only 0.54\\% on average, compared to\nstate-of-the-art KV cache quantization techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern Large Language Model serving system batches multiple requests to\nachieve high throughput, while batching attention operations is challenging,\nrendering memory bandwidth a critical bottleneck. The community relies on\nhigh-end GPUs with multiple high-bandwidth memory channels. Unfortunately,\nHBM's high bandwidth often comes at the expense of limited memory capacity,\nwhich reduces core utilization and increases costs. Recent advancements\nenabling longer contexts for LLMs have substantially increased the key-value\ncache size, further intensifying the pressures on memory capacity. The\nliterature has explored KV cache quantization techniques, which commonly use\nlow bitwidth for most values, selectively using higher bitwidth for outlier\nvalues. While this approach helps achieve high accuracy and low bitwidth\nsimultaneously, it comes with the limitation that cost for online outlier\ndetection is excessively high, negating the advantages. We propose Oaken, an\nacceleration solution that achieves high accuracy and high performance\nsimultaneously through co-designing algorithm and hardware. To effectively find\na sweet spot in the accuracy-performance trade-off space of KV cache\nquantization, Oaken employs an online-offline hybrid approach, setting outlier\nthresholds offline, which are then used to determine the quantization scale\nonline. To translate the proposed algorithmic technique into tangible\nperformance gains, Oaken also comes with custom quantization engines and memory\nmanagement units that can be integrated with any LLM accelerators. We built an\nOaken accelerator on top of an LLM accelerator, LPU, and conducted a\ncomprehensive evaluation. Our experiments show that for a batch size of 256,\nOaken achieves up to 1.58x throughput improvement over NVIDIA A100 GPU,\nincurring a minimal accuracy loss of only 0.54\\% on average, compared to\nstate-of-the-art KV cache quantization techniques."
                },
                "authors": [
                    {
                        "name": "Minsu Kim"
                    },
                    {
                        "name": "Seongmin Hong"
                    },
                    {
                        "name": "RyeoWook Ko"
                    },
                    {
                        "name": "Soongyu Choi"
                    },
                    {
                        "name": "Hunjong Lee"
                    },
                    {
                        "name": "Junsoo Kim"
                    },
                    {
                        "name": "Joo-Young Kim"
                    },
                    {
                        "name": "Jongse Park"
                    }
                ],
                "author_detail": {
                    "name": "Jongse Park"
                },
                "author": "Jongse Park",
                "arxiv_doi": "10.1145/3695053.3731019",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3695053.3731019",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.18599v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18599v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "16 pages, 14 figures, and 4 tables",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09081v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09081v1",
                "updated": "2025-05-14T02:29:46Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    2,
                    29,
                    46,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T02:29:46Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    2,
                    29,
                    46,
                    2,
                    134,
                    0
                ],
                "title": "SALM: A Multi-Agent Framework for Language Model-Driven Social Network\n  Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SALM: A Multi-Agent Framework for Language Model-Driven Social Network\n  Simulation"
                },
                "summary": "Contemporary approaches to agent-based modeling (ABM) of social systems have\ntraditionally emphasized rule-based behaviors, limiting their ability to\ncapture nuanced dynamics by moving beyond predefined rules and leveraging\ncontextual understanding from LMs of human social interaction. This paper\npresents SALM (Social Agent LM Framework), a novel approach for integrating\nlanguage models (LMs) into social network simulation that achieves\nunprecedented temporal stability in multi-agent scenarios. Our primary\ncontributions include: (1) a hierarchical prompting architecture enabling\nstable simulation beyond 4,000 timesteps while reducing token usage by 73%, (2)\nan attention-based memory system achieving 80% cache hit rates (95% CI [78%,\n82%]) with sub-linear memory growth of 9.5%, and (3) formal bounds on\npersonality stability. Through extensive validation against SNAP ego networks,\nwe demonstrate the first LLM-based framework capable of modeling long-term\nsocial phenomena while maintaining empirically validated behavioral fidelity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contemporary approaches to agent-based modeling (ABM) of social systems have\ntraditionally emphasized rule-based behaviors, limiting their ability to\ncapture nuanced dynamics by moving beyond predefined rules and leveraging\ncontextual understanding from LMs of human social interaction. This paper\npresents SALM (Social Agent LM Framework), a novel approach for integrating\nlanguage models (LMs) into social network simulation that achieves\nunprecedented temporal stability in multi-agent scenarios. Our primary\ncontributions include: (1) a hierarchical prompting architecture enabling\nstable simulation beyond 4,000 timesteps while reducing token usage by 73%, (2)\nan attention-based memory system achieving 80% cache hit rates (95% CI [78%,\n82%]) with sub-linear memory growth of 9.5%, and (3) formal bounds on\npersonality stability. Through extensive validation against SNAP ego networks,\nwe demonstrate the first LLM-based framework capable of modeling long-term\nsocial phenomena while maintaining empirically validated behavioral fidelity."
                },
                "authors": [
                    {
                        "name": "Gaurav Koley"
                    }
                ],
                "author_detail": {
                    "name": "Gaurav Koley"
                },
                "author": "Gaurav Koley",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09081v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09081v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09040v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09040v1",
                "updated": "2025-05-14T00:41:44Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    0,
                    41,
                    44,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T00:41:44Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    0,
                    41,
                    44,
                    2,
                    134,
                    0
                ],
                "title": "RT-cache: Efficient Robot Trajectory Retrieval System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RT-cache: Efficient Robot Trajectory Retrieval System"
                },
                "summary": "This paper introduces RT-cache, a novel trajectorymemory pipeline that\naccelerates real-world robot inference by leveraging big-data retrieval and\nlearning from experience. While modern Vision-Language-Action (VLA) models can\nhandle diverse robotic tasks, they often incur high per-step inference costs,\nresulting in significant latency, sometimes minutes per task. In contrast,\nRT-cache stores a large-scale Memory of previously successful robot\ntrajectories and retrieves relevant multistep motion snippets, drastically\nreducing inference overhead. By integrating a Memory Builder with a Trajectory\nRetrieval, we develop an efficient retrieval process that remains tractable\neven for extremely large datasets. RT-cache flexibly accumulates real-world\nexperiences and replays them whenever the current scene matches past states,\nadapting quickly to new or unseen environments with only a few additional\nsamples. Experiments on the Open-X Embodiment Dataset and other real-world data\ndemonstrate that RT-cache completes tasks both faster and more successfully\nthan a baseline lacking retrieval, suggesting a practical, data-driven solution\nfor real-time manipulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces RT-cache, a novel trajectorymemory pipeline that\naccelerates real-world robot inference by leveraging big-data retrieval and\nlearning from experience. While modern Vision-Language-Action (VLA) models can\nhandle diverse robotic tasks, they often incur high per-step inference costs,\nresulting in significant latency, sometimes minutes per task. In contrast,\nRT-cache stores a large-scale Memory of previously successful robot\ntrajectories and retrieves relevant multistep motion snippets, drastically\nreducing inference overhead. By integrating a Memory Builder with a Trajectory\nRetrieval, we develop an efficient retrieval process that remains tractable\neven for extremely large datasets. RT-cache flexibly accumulates real-world\nexperiences and replays them whenever the current scene matches past states,\nadapting quickly to new or unseen environments with only a few additional\nsamples. Experiments on the Open-X Embodiment Dataset and other real-world data\ndemonstrate that RT-cache completes tasks both faster and more successfully\nthan a baseline lacking retrieval, suggesting a practical, data-driven solution\nfor real-time manipulation."
                },
                "authors": [
                    {
                        "name": "Owen Kwon"
                    },
                    {
                        "name": "Abraham George"
                    },
                    {
                        "name": "Alison Bartsch"
                    },
                    {
                        "name": "Amir Barati Farimani"
                    }
                ],
                "author_detail": {
                    "name": "Amir Barati Farimani"
                },
                "author": "Amir Barati Farimani",
                "arxiv_comment": "9 pages, 5 figures. Submitted to an IEEE robotics conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09040v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09040v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08958v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08958v1",
                "updated": "2025-05-13T20:51:59Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    20,
                    51,
                    59,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T20:51:59Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    20,
                    51,
                    59,
                    1,
                    133,
                    0
                ],
                "title": "Adaptive Entanglement Generation for Quantum Routing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Entanglement Generation for Quantum Routing"
                },
                "summary": "Entanglement generation in long-distance quantum networks is a difficult\nprocess due to resource limitations and the probabilistic nature of\nentanglement swapping. To maximize success probability, existing quantum\nrouting algorithms employ computationally expensive solutions (e.g., linear\nprogramming) to determine which links to entangle and use for end-to-end\nentanglement generation. Such optimization methods, however, cannot meet the\ndelay requirements of real-world quantum networks, necessitating swift yet\nefficient real-time optimization models. In this paper, we propose\nreinforcement learning (RL)-based models to determine which links to entangle\nand proactively swap to meet connection requests. We show that the proposed\nRL-based approach is 20x faster compared to linear programming. Moreover, we\nshow that one can take advantage of the longevity of entanglements to (i) cache\nentangled links for future use and (ii) proactively swap entanglement on\nhigh-demand path segments, thereby increasing the likelihood of request\nsuccess. Through comprehensive simulations, we demonstrate that caching unused\nentanglements leads to a 10-15% improvement in the performance of\nstate-of-the-art quantum routing algorithms. Complementing caching with\nproactive entanglement swapping further enhances the request success rate by up\nto 52.55%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Entanglement generation in long-distance quantum networks is a difficult\nprocess due to resource limitations and the probabilistic nature of\nentanglement swapping. To maximize success probability, existing quantum\nrouting algorithms employ computationally expensive solutions (e.g., linear\nprogramming) to determine which links to entangle and use for end-to-end\nentanglement generation. Such optimization methods, however, cannot meet the\ndelay requirements of real-world quantum networks, necessitating swift yet\nefficient real-time optimization models. In this paper, we propose\nreinforcement learning (RL)-based models to determine which links to entangle\nand proactively swap to meet connection requests. We show that the proposed\nRL-based approach is 20x faster compared to linear programming. Moreover, we\nshow that one can take advantage of the longevity of entanglements to (i) cache\nentangled links for future use and (ii) proactively swap entanglement on\nhigh-demand path segments, thereby increasing the likelihood of request\nsuccess. Through comprehensive simulations, we demonstrate that caching unused\nentanglements leads to a 10-15% improvement in the performance of\nstate-of-the-art quantum routing algorithms. Complementing caching with\nproactive entanglement swapping further enhances the request success rate by up\nto 52.55%."
                },
                "authors": [
                    {
                        "name": "Tasdiqul Islam"
                    },
                    {
                        "name": "Md Arifuzzaman"
                    },
                    {
                        "name": "Engin Arslan"
                    }
                ],
                "author_detail": {
                    "name": "Engin Arslan"
                },
                "author": "Engin Arslan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08958v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08958v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01723v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01723v5",
                "updated": "2025-05-13T17:43:47Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    17,
                    43,
                    47,
                    1,
                    133,
                    0
                ],
                "published": "2024-10-02T16:34:29Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    16,
                    34,
                    29,
                    2,
                    276,
                    0
                ],
                "title": "HarmoniCa: Harmonizing Training and Inference for Better Feature Caching\n  in Diffusion Transformer Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HarmoniCa: Harmonizing Training and Inference for Better Feature Caching\n  in Diffusion Transformer Acceleration"
                },
                "summary": "Diffusion Transformers (DiTs) excel in generative tasks but face practical\ndeployment challenges due to high inference costs. Feature caching, which\nstores and retrieves redundant computations, offers the potential for\nacceleration. Existing learning-based caching, though adaptive, overlooks the\nimpact of the prior timestep. It also suffers from misaligned\nobjectives--aligned predicted noise vs. high-quality images--between training\nand inference. These two discrepancies compromise both performance and\nefficiency. To this end, we harmonize training and inference with a novel\nlearning-based caching framework dubbed HarmoniCa. It first incorporates\nStep-Wise Denoising Training (SDT) to ensure the continuity of the denoising\nprocess, where prior steps can be leveraged. In addition, an Image Error\nProxy-Guided Objective (IEPO) is applied to balance image quality against cache\nutilization through an efficient proxy to approximate the image error.\nExtensive experiments across $8$ models, $4$ samplers, and resolutions from\n$256\\times256$ to $2K$ demonstrate superior performance and speedup of our\nframework. For instance, it achieves over $40\\%$ latency reduction (i.e.,\n$2.07\\times$ theoretical speedup) and improved performance on PixArt-$\\alpha$.\nRemarkably, our image-free approach reduces training time by $25\\%$ compared\nwith the previous method. Our code is available at\nhttps://github.com/ModelTC/HarmoniCa.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) excel in generative tasks but face practical\ndeployment challenges due to high inference costs. Feature caching, which\nstores and retrieves redundant computations, offers the potential for\nacceleration. Existing learning-based caching, though adaptive, overlooks the\nimpact of the prior timestep. It also suffers from misaligned\nobjectives--aligned predicted noise vs. high-quality images--between training\nand inference. These two discrepancies compromise both performance and\nefficiency. To this end, we harmonize training and inference with a novel\nlearning-based caching framework dubbed HarmoniCa. It first incorporates\nStep-Wise Denoising Training (SDT) to ensure the continuity of the denoising\nprocess, where prior steps can be leveraged. In addition, an Image Error\nProxy-Guided Objective (IEPO) is applied to balance image quality against cache\nutilization through an efficient proxy to approximate the image error.\nExtensive experiments across $8$ models, $4$ samplers, and resolutions from\n$256\\times256$ to $2K$ demonstrate superior performance and speedup of our\nframework. For instance, it achieves over $40\\%$ latency reduction (i.e.,\n$2.07\\times$ theoretical speedup) and improved performance on PixArt-$\\alpha$.\nRemarkably, our image-free approach reduces training time by $25\\%$ compared\nwith the previous method. Our code is available at\nhttps://github.com/ModelTC/HarmoniCa."
                },
                "authors": [
                    {
                        "name": "Yushi Huang"
                    },
                    {
                        "name": "Zining Wang"
                    },
                    {
                        "name": "Ruihao Gong"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Xinjie Zhang"
                    },
                    {
                        "name": "Jinyang Guo"
                    },
                    {
                        "name": "Xianglong Liu"
                    },
                    {
                        "name": "Jun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhang"
                },
                "author": "Jun Zhang",
                "arxiv_comment": "Accepted by ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01723v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01723v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08587v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08587v1",
                "updated": "2025-05-13T13:58:22Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    13,
                    58,
                    22,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T13:58:22Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    13,
                    58,
                    22,
                    1,
                    133,
                    0
                ],
                "title": "Two-Level Sketching Alternating Anderson acceleration for Complex\n  Physics Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Two-Level Sketching Alternating Anderson acceleration for Complex\n  Physics Applications"
                },
                "summary": "We present a novel two-level sketching extension of the Alternating\nAnderson-Picard (AAP) method for accelerating fixed-point iterations in\nchallenging single- and multi-physics simulations governed by discretized\npartial differential equations. Our approach combines a static, physics-based\nprojection that reduces the least-squares problem to the most informative field\n(e.g., via Schur-complement insight) with a dynamic, algebraic sketching stage\ndriven by a backward stability analysis under Lipschitz continuity. We\nintroduce inexpensive estimators for stability thresholds and cache-aware\nrandomized selection strategies to balance computational cost against\nmemory-access overhead. The resulting algorithm solves reduced least-squares\nsystems in place, minimizes memory footprints, and seamlessly alternates\nbetween low-cost Picard updates and Anderson mixing. Implemented in Julia, our\ntwo-level sketching AAP achieves up to 50% time-to-solution reductions compared\nto standard Anderson acceleration-without degrading convergence rates-on\nbenchmark problems including Stokes, p-Laplacian, Bidomain, and Navier-Stokes\nformulations at varying problem sizes. These results demonstrate the method's\nrobustness, scalability, and potential for integration into high-performance\nscientific computing frameworks. Our implementation is available open-source in\nthe AAP.jl library.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a novel two-level sketching extension of the Alternating\nAnderson-Picard (AAP) method for accelerating fixed-point iterations in\nchallenging single- and multi-physics simulations governed by discretized\npartial differential equations. Our approach combines a static, physics-based\nprojection that reduces the least-squares problem to the most informative field\n(e.g., via Schur-complement insight) with a dynamic, algebraic sketching stage\ndriven by a backward stability analysis under Lipschitz continuity. We\nintroduce inexpensive estimators for stability thresholds and cache-aware\nrandomized selection strategies to balance computational cost against\nmemory-access overhead. The resulting algorithm solves reduced least-squares\nsystems in place, minimizes memory footprints, and seamlessly alternates\nbetween low-cost Picard updates and Anderson mixing. Implemented in Julia, our\ntwo-level sketching AAP achieves up to 50% time-to-solution reductions compared\nto standard Anderson acceleration-without degrading convergence rates-on\nbenchmark problems including Stokes, p-Laplacian, Bidomain, and Navier-Stokes\nformulations at varying problem sizes. These results demonstrate the method's\nrobustness, scalability, and potential for integration into high-performance\nscientific computing frameworks. Our implementation is available open-source in\nthe AAP.jl library."
                },
                "authors": [
                    {
                        "name": "Nicolás A. Barnafi"
                    },
                    {
                        "name": "Massimiliano Lupo Pasini"
                    }
                ],
                "author_detail": {
                    "name": "Massimiliano Lupo Pasini"
                },
                "author": "Massimiliano Lupo Pasini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08587v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08587v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.NA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "65N12, 65N22, 65K10, 65F10, 65F99, 65B99",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13989v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13989v2",
                "updated": "2025-05-13T09:36:03Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    9,
                    36,
                    3,
                    1,
                    133,
                    0
                ],
                "published": "2025-04-18T13:46:58Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    13,
                    46,
                    58,
                    4,
                    108,
                    0
                ],
                "title": "Gradual Binary Search and Dimension Expansion : A general method for\n  activation quantization in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gradual Binary Search and Dimension Expansion : A general method for\n  activation quantization in LLMs"
                },
                "summary": "Large language models (LLMs) have become pivotal in artificial intelligence,\ndemonstrating strong capabilities in reasoning, understanding, and generating\ndata. However, their deployment on edge devices is hindered by their\nsubstantial size, often reaching several billion parameters. Quantization is a\nwidely used method to reduce memory usage and inference time, however LLMs\npresent unique challenges due to the prevalence of outliers in their\nactivations. In this work, we leverage the theoretical advantages of Hadamard\nmatrices over random rotation matrices to push the boundaries of quantization\nin LLMs. We demonstrate that Hadamard matrices are more effective in reducing\noutliers, which are a significant obstacle in achieving low-bit quantization.\nOur method based on a gradual binary search enables 3-bit quantization for\nweights, activations, and key-value (KV) caches, resulting in a 40% increase in\naccuracy on common benchmarks compared to SoTA methods. We extend the use of\nrotation matrices to support non-power-of-2 embedding dimensions, similar to\nthe Qwen architecture, by employing the Paley algorithm. We theoretically\ndemonstrates the superiority of Hadamard matrices in reducing outliers.We\nachieved 3-bit quantization for weights, activations, and KV cache,\nsignificantly enhancing model performance. Our experimental results on multiple\nmodels family like Mistral, LLaMA, and Qwen demonstrate the effectiveness of\nour approach, outperforming existing methods and enabling practical 3-bit\nquantization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have become pivotal in artificial intelligence,\ndemonstrating strong capabilities in reasoning, understanding, and generating\ndata. However, their deployment on edge devices is hindered by their\nsubstantial size, often reaching several billion parameters. Quantization is a\nwidely used method to reduce memory usage and inference time, however LLMs\npresent unique challenges due to the prevalence of outliers in their\nactivations. In this work, we leverage the theoretical advantages of Hadamard\nmatrices over random rotation matrices to push the boundaries of quantization\nin LLMs. We demonstrate that Hadamard matrices are more effective in reducing\noutliers, which are a significant obstacle in achieving low-bit quantization.\nOur method based on a gradual binary search enables 3-bit quantization for\nweights, activations, and key-value (KV) caches, resulting in a 40% increase in\naccuracy on common benchmarks compared to SoTA methods. We extend the use of\nrotation matrices to support non-power-of-2 embedding dimensions, similar to\nthe Qwen architecture, by employing the Paley algorithm. We theoretically\ndemonstrates the superiority of Hadamard matrices in reducing outliers.We\nachieved 3-bit quantization for weights, activations, and KV cache,\nsignificantly enhancing model performance. Our experimental results on multiple\nmodels family like Mistral, LLaMA, and Qwen demonstrate the effectiveness of\nour approach, outperforming existing methods and enabling practical 3-bit\nquantization."
                },
                "authors": [
                    {
                        "name": "Lucas Maisonnave"
                    },
                    {
                        "name": "Cyril Moineau"
                    },
                    {
                        "name": "Olivier Bichler"
                    },
                    {
                        "name": "Fabrice Rastello"
                    }
                ],
                "author_detail": {
                    "name": "Fabrice Rastello"
                },
                "author": "Fabrice Rastello",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13989v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13989v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08261v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08261v1",
                "updated": "2025-05-13T06:24:48Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    6,
                    24,
                    48,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T06:24:48Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    6,
                    24,
                    48,
                    1,
                    133,
                    0
                ],
                "title": "Enhancing Cache-Augmented Generation (CAG) with Adaptive Contextual\n  Compression for Scalable Knowledge Integration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Cache-Augmented Generation (CAG) with Adaptive Contextual\n  Compression for Scalable Knowledge Integration"
                },
                "summary": "The rapid progress in large language models (LLMs) has paved the way for\nnovel approaches in knowledge-intensive tasks. Among these, Cache-Augmented\nGeneration (CAG) has emerged as a promising alternative to Retrieval-Augmented\nGeneration (RAG). CAG minimizes retrieval latency and simplifies system design\nby preloading knowledge into the model's context. However, challenges persist\nin scaling CAG to accommodate large and dynamic knowledge bases effectively.\nThis paper introduces Adaptive Contextual Compression (ACC), an innovative\ntechnique designed to dynamically compress and manage context inputs, enabling\nefficient utilization of the extended memory capabilities of modern LLMs. To\nfurther address the limitations of standalone CAG, we propose a Hybrid CAG-RAG\nFramework, which integrates selective retrieval to augment preloaded contexts\nin scenarios requiring additional information. Comprehensive evaluations on\ndiverse datasets highlight the proposed methods' ability to enhance\nscalability, optimize efficiency, and improve multi-hop reasoning performance,\noffering practical solutions for real-world knowledge integration challenges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid progress in large language models (LLMs) has paved the way for\nnovel approaches in knowledge-intensive tasks. Among these, Cache-Augmented\nGeneration (CAG) has emerged as a promising alternative to Retrieval-Augmented\nGeneration (RAG). CAG minimizes retrieval latency and simplifies system design\nby preloading knowledge into the model's context. However, challenges persist\nin scaling CAG to accommodate large and dynamic knowledge bases effectively.\nThis paper introduces Adaptive Contextual Compression (ACC), an innovative\ntechnique designed to dynamically compress and manage context inputs, enabling\nefficient utilization of the extended memory capabilities of modern LLMs. To\nfurther address the limitations of standalone CAG, we propose a Hybrid CAG-RAG\nFramework, which integrates selective retrieval to augment preloaded contexts\nin scenarios requiring additional information. Comprehensive evaluations on\ndiverse datasets highlight the proposed methods' ability to enhance\nscalability, optimize efficiency, and improve multi-hop reasoning performance,\noffering practical solutions for real-world knowledge integration challenges."
                },
                "authors": [
                    {
                        "name": "Rishabh Agrawal"
                    },
                    {
                        "name": "Himanshu Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Himanshu Kumar"
                },
                "author": "Himanshu Kumar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08261v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08261v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07692v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07692v1",
                "updated": "2025-05-12T15:58:39Z",
                "updated_parsed": [
                    2025,
                    5,
                    12,
                    15,
                    58,
                    39,
                    0,
                    132,
                    0
                ],
                "published": "2025-05-12T15:58:39Z",
                "published_parsed": [
                    2025,
                    5,
                    12,
                    15,
                    58,
                    39,
                    0,
                    132,
                    0
                ],
                "title": "ABase: the Multi-Tenant NoSQL Serverless Database for Diverse and\n  Dynamic Workloads in Large-scale Cloud Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ABase: the Multi-Tenant NoSQL Serverless Database for Diverse and\n  Dynamic Workloads in Large-scale Cloud Environments"
                },
                "summary": "Multi-tenant architectures enhance the elasticity and resource utilization of\nNoSQL databases by allowing multiple tenants to co-locate and share resources.\nHowever, in large-scale cloud environments, the diverse and dynamic nature of\nworkloads poses significant challenges for multi-tenant NoSQL databases. Based\non our practical observations, we have identified three crucial challenges: (1)\nthe impact of caching on performance isolation, as cache hits alter request\nexecution and resource consumption, leading to inaccurate traffic control; (2)\nthe dynamic changes in traffic, with changes in tenant traffic trends causing\nthrottling or resource wastage, and changes in access distribution causing hot\nkey pressure or cache hit ratio drops; and (3) the imbalanced layout of data\nnodes due to tenants' diverse resource requirements, leading to low resource\nutilization. To address these challenges, we introduce ABase, a multi-tenant\nNoSQL serverless database developed at ByteDance. ABase introduces a two-layer\ncaching mechanism with a cache-aware isolation mechanism to ensure accurate\nresource consumption estimates. Furthermore, ABase employs a predictive\nautoscaling policy to dynamically adjust resources in response to tenant\ntraffic changes and a multi-resource rescheduling algorithm to balance resource\nutilization across data nodes. With these innovations, ABase has successfully\nserved ByteDance's large-scale cloud environment, supporting a total workload\nthat has achieved a peak QPS of over 13 billion and total storage exceeding 1\nEB.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-tenant architectures enhance the elasticity and resource utilization of\nNoSQL databases by allowing multiple tenants to co-locate and share resources.\nHowever, in large-scale cloud environments, the diverse and dynamic nature of\nworkloads poses significant challenges for multi-tenant NoSQL databases. Based\non our practical observations, we have identified three crucial challenges: (1)\nthe impact of caching on performance isolation, as cache hits alter request\nexecution and resource consumption, leading to inaccurate traffic control; (2)\nthe dynamic changes in traffic, with changes in tenant traffic trends causing\nthrottling or resource wastage, and changes in access distribution causing hot\nkey pressure or cache hit ratio drops; and (3) the imbalanced layout of data\nnodes due to tenants' diverse resource requirements, leading to low resource\nutilization. To address these challenges, we introduce ABase, a multi-tenant\nNoSQL serverless database developed at ByteDance. ABase introduces a two-layer\ncaching mechanism with a cache-aware isolation mechanism to ensure accurate\nresource consumption estimates. Furthermore, ABase employs a predictive\nautoscaling policy to dynamically adjust resources in response to tenant\ntraffic changes and a multi-resource rescheduling algorithm to balance resource\nutilization across data nodes. With these innovations, ABase has successfully\nserved ByteDance's large-scale cloud environment, supporting a total workload\nthat has achieved a peak QPS of over 13 billion and total storage exceeding 1\nEB."
                },
                "authors": [
                    {
                        "name": "Rong Kang"
                    },
                    {
                        "name": "Yanbin Chen"
                    },
                    {
                        "name": "Ye Liu"
                    },
                    {
                        "name": "Fuxin Jiang"
                    },
                    {
                        "name": "Qingshuo Li"
                    },
                    {
                        "name": "Miao Ma"
                    },
                    {
                        "name": "Jian Liu"
                    },
                    {
                        "name": "Guangliang Zhao"
                    },
                    {
                        "name": "Tieying Zhang"
                    },
                    {
                        "name": "Jianjun Chen"
                    },
                    {
                        "name": "Lei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Lei Zhang"
                },
                "author": "Lei Zhang",
                "arxiv_comment": "SIGMOD 2025 accepted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.07692v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07692v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07680v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07680v1",
                "updated": "2025-05-12T15:46:28Z",
                "updated_parsed": [
                    2025,
                    5,
                    12,
                    15,
                    46,
                    28,
                    0,
                    132,
                    0
                ],
                "published": "2025-05-12T15:46:28Z",
                "published_parsed": [
                    2025,
                    5,
                    12,
                    15,
                    46,
                    28,
                    0,
                    132,
                    0
                ],
                "title": "SpecRouter: Adaptive Routing for Multi-Level Speculative Decoding in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpecRouter: Adaptive Routing for Multi-Level Speculative Decoding in\n  Large Language Models"
                },
                "summary": "Large Language Models (LLMs) present a critical trade-off between inference\nquality and computational cost: larger models offer superior capabilities but\nincur significant latency, while smaller models are faster but less powerful.\nExisting serving strategies often employ fixed model scales or static two-stage\nspeculative decoding, failing to dynamically adapt to the varying complexities\nof user requests or fluctuations in system performance. This paper introduces\n\\systemname{}, a novel framework that reimagines LLM inference as an adaptive\nrouting problem solved through multi-level speculative decoding. \\systemname{}\ndynamically constructs and optimizes inference \"paths\" (chains of models) based\non real-time feedback, addressing the limitations of static approaches. Our\ncontributions are threefold: (1) An \\textbf{adaptive model chain scheduling}\nmechanism that leverages performance profiling (execution times) and predictive\nsimilarity metrics (derived from token distribution divergence) to continuously\nselect the optimal sequence of draft and verifier models, minimizing predicted\nlatency per generated token. (2) A \\textbf{multi-level collaborative\nverification} framework where intermediate models within the selected chain can\nvalidate speculative tokens, reducing the verification burden on the final,\nmost powerful target model. (3) A \\textbf{synchronized state management} system\nproviding efficient, consistent KV cache handling across heterogeneous models\nin the chain, including precise, low-overhead rollbacks tailored for\nasynchronous batch processing inherent in multi-level speculation. Preliminary\nexperiments demonstrate the validity of our method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) present a critical trade-off between inference\nquality and computational cost: larger models offer superior capabilities but\nincur significant latency, while smaller models are faster but less powerful.\nExisting serving strategies often employ fixed model scales or static two-stage\nspeculative decoding, failing to dynamically adapt to the varying complexities\nof user requests or fluctuations in system performance. This paper introduces\n\\systemname{}, a novel framework that reimagines LLM inference as an adaptive\nrouting problem solved through multi-level speculative decoding. \\systemname{}\ndynamically constructs and optimizes inference \"paths\" (chains of models) based\non real-time feedback, addressing the limitations of static approaches. Our\ncontributions are threefold: (1) An \\textbf{adaptive model chain scheduling}\nmechanism that leverages performance profiling (execution times) and predictive\nsimilarity metrics (derived from token distribution divergence) to continuously\nselect the optimal sequence of draft and verifier models, minimizing predicted\nlatency per generated token. (2) A \\textbf{multi-level collaborative\nverification} framework where intermediate models within the selected chain can\nvalidate speculative tokens, reducing the verification burden on the final,\nmost powerful target model. (3) A \\textbf{synchronized state management} system\nproviding efficient, consistent KV cache handling across heterogeneous models\nin the chain, including precise, low-overhead rollbacks tailored for\nasynchronous batch processing inherent in multi-level speculation. Preliminary\nexperiments demonstrate the validity of our method."
                },
                "authors": [
                    {
                        "name": "Hang Wu"
                    },
                    {
                        "name": "Jianian Zhu"
                    },
                    {
                        "name": "Yinghui Li"
                    },
                    {
                        "name": "Haojie Wang"
                    },
                    {
                        "name": "Biao Hou"
                    },
                    {
                        "name": "Jidong Zhai"
                    }
                ],
                "author_detail": {
                    "name": "Jidong Zhai"
                },
                "author": "Jidong Zhai",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.07680v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07680v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07350v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07350v1",
                "updated": "2025-05-12T08:44:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    12,
                    8,
                    44,
                    10,
                    0,
                    132,
                    0
                ],
                "published": "2025-05-12T08:44:10Z",
                "published_parsed": [
                    2025,
                    5,
                    12,
                    8,
                    44,
                    10,
                    0,
                    132,
                    0
                ],
                "title": "All-optical electric field sensing with nanodiamond-doped polymer thin\n  films",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "All-optical electric field sensing with nanodiamond-doped polymer thin\n  films"
                },
                "summary": "The nitrogen-vacancy (NV) center is a photoluminescent defect in diamond that\nexists in different charge states, NV$^-$ and NV$^0$, that are sensitive to the\nNV's nanoscale environment. Here, we show that photoluminescence (PL) from NV\ncenters in fluorescent nanodiamonds (FNDs) can be employed for all-optical\nvoltage sensing based on electric field-induced NV charge state modulation.\nMore than 95% of FNDs integrated into a capacitor device show a transient\nincrease in NV$^-$ PL intensity of up to 31% within 0.1 ms after application of\nan external voltage, accompanied by a simultaneous decrease in NV$^0$ PL. The\nchange in NV$^-$ PL increases with increasing applied voltage from 0 to 100 V,\ncorresponding to an electric field of 0 to 625 kV cm$^ {-1}$ in our devices.\nThe electric field sensitivity of a single FND is 19 V cm$^{-1}$ Hz$^ {-1/2}$.\nWe investigate the NV charge state photodynamics on the millisecond timescale\nand find that the change in NV PL strongly depends on the rate of\nphotoexcitation. We propose a model that qualitatively explains the observed\nchanges in NV PL based on an electric field-induced redistribution of\nphotoexcited electrons from substitutional nitrogen defects to NV centers,\nleading to a transient conversion of NV$^0$ to NV$^-$ centers upon application\nof an external voltage. Our results contribute to the development of FNDs as\nreliable, all-optical, nanoscale electric field sensors in solid-state systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The nitrogen-vacancy (NV) center is a photoluminescent defect in diamond that\nexists in different charge states, NV$^-$ and NV$^0$, that are sensitive to the\nNV's nanoscale environment. Here, we show that photoluminescence (PL) from NV\ncenters in fluorescent nanodiamonds (FNDs) can be employed for all-optical\nvoltage sensing based on electric field-induced NV charge state modulation.\nMore than 95% of FNDs integrated into a capacitor device show a transient\nincrease in NV$^-$ PL intensity of up to 31% within 0.1 ms after application of\nan external voltage, accompanied by a simultaneous decrease in NV$^0$ PL. The\nchange in NV$^-$ PL increases with increasing applied voltage from 0 to 100 V,\ncorresponding to an electric field of 0 to 625 kV cm$^ {-1}$ in our devices.\nThe electric field sensitivity of a single FND is 19 V cm$^{-1}$ Hz$^ {-1/2}$.\nWe investigate the NV charge state photodynamics on the millisecond timescale\nand find that the change in NV PL strongly depends on the rate of\nphotoexcitation. We propose a model that qualitatively explains the observed\nchanges in NV PL based on an electric field-induced redistribution of\nphotoexcited electrons from substitutional nitrogen defects to NV centers,\nleading to a transient conversion of NV$^0$ to NV$^-$ centers upon application\nof an external voltage. Our results contribute to the development of FNDs as\nreliable, all-optical, nanoscale electric field sensors in solid-state systems."
                },
                "authors": [
                    {
                        "name": "Roy Styles"
                    },
                    {
                        "name": "Mengke Han"
                    },
                    {
                        "name": "Toon Goris"
                    },
                    {
                        "name": "James Partridge"
                    },
                    {
                        "name": "Brett C. Johnson"
                    },
                    {
                        "name": "Blanca del Rosal"
                    },
                    {
                        "name": "Amanda N. Abraham"
                    },
                    {
                        "name": "Heike Ebendorff-Heidepriem"
                    },
                    {
                        "name": "Brant C. Gibson"
                    },
                    {
                        "name": "Nikolai Dontschuk"
                    },
                    {
                        "name": "Jean-Philippe Tetienne"
                    },
                    {
                        "name": "Philipp Reineck"
                    }
                ],
                "author_detail": {
                    "name": "Philipp Reineck"
                },
                "author": "Philipp Reineck",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.07350v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07350v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mes-hall",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mes-hall",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07274v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07274v1",
                "updated": "2025-05-12T06:53:24Z",
                "updated_parsed": [
                    2025,
                    5,
                    12,
                    6,
                    53,
                    24,
                    0,
                    132,
                    0
                ],
                "published": "2025-05-12T06:53:24Z",
                "published_parsed": [
                    2025,
                    5,
                    12,
                    6,
                    53,
                    24,
                    0,
                    132,
                    0
                ],
                "title": "Cache-Efficient Posterior Sampling for Reinforcement Learning with\n  LLM-Derived Priors Across Discrete and Continuous Domains",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-Efficient Posterior Sampling for Reinforcement Learning with\n  LLM-Derived Priors Across Discrete and Continuous Domains"
                },
                "summary": "Integrating large language models (LLMs) as priors in reinforcement learning\n(RL) offers significant advantages but comes with substantial computational\ncosts. We present a principled cache-efficient framework for posterior sampling\nwith LLM-derived priors that dramatically reduces these costs while maintaining\nhigh performance. At the core of our approach is an adaptive caching mechanism,\nwhere cache parameters are meta-optimized using surrogate gradients derived\nfrom policy performance. This design enables efficient inference across both\ndiscrete text environments (e.g., TextWorld, ALFWorld) and continuous control\ndomains (e.g., MuJoCo), achieving a 3.8--4.7$\\times$ reduction in LLM queries\nand 4.0--12.0$\\times$ lower median latencies (85--93\\,ms on a consumer GPU)\nwhile retaining 96--98\\% of uncached performance. Our theoretical analysis\nprovides KL divergence bounds on approximation quality, validated empirically.\nThe framework extends to offline RL, where our CQL-Prior variant improves\nperformance by 14--29\\% and reduces training time by 38--40\\%. Extensive\nevaluations across a diverse suite of eight tasks demonstrate the\ngeneralizability and practical viability of LLM-guided RL in\nresource-constrained settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating large language models (LLMs) as priors in reinforcement learning\n(RL) offers significant advantages but comes with substantial computational\ncosts. We present a principled cache-efficient framework for posterior sampling\nwith LLM-derived priors that dramatically reduces these costs while maintaining\nhigh performance. At the core of our approach is an adaptive caching mechanism,\nwhere cache parameters are meta-optimized using surrogate gradients derived\nfrom policy performance. This design enables efficient inference across both\ndiscrete text environments (e.g., TextWorld, ALFWorld) and continuous control\ndomains (e.g., MuJoCo), achieving a 3.8--4.7$\\times$ reduction in LLM queries\nand 4.0--12.0$\\times$ lower median latencies (85--93\\,ms on a consumer GPU)\nwhile retaining 96--98\\% of uncached performance. Our theoretical analysis\nprovides KL divergence bounds on approximation quality, validated empirically.\nThe framework extends to offline RL, where our CQL-Prior variant improves\nperformance by 14--29\\% and reduces training time by 38--40\\%. Extensive\nevaluations across a diverse suite of eight tasks demonstrate the\ngeneralizability and practical viability of LLM-guided RL in\nresource-constrained settings."
                },
                "authors": [
                    {
                        "name": "Ibne Farabi Shihab"
                    },
                    {
                        "name": "Sanjeda Akter"
                    },
                    {
                        "name": "Anuj Sharma"
                    }
                ],
                "author_detail": {
                    "name": "Anuj Sharma"
                },
                "author": "Anuj Sharma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.07274v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07274v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07239v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07239v1",
                "updated": "2025-05-12T05:29:30Z",
                "updated_parsed": [
                    2025,
                    5,
                    12,
                    5,
                    29,
                    30,
                    0,
                    132,
                    0
                ],
                "published": "2025-05-12T05:29:30Z",
                "published_parsed": [
                    2025,
                    5,
                    12,
                    5,
                    29,
                    30,
                    0,
                    132,
                    0
                ],
                "title": "Comet: Accelerating Private Inference for Large Language Model by\n  Predicting Activation Sparsity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comet: Accelerating Private Inference for Large Language Model by\n  Predicting Activation Sparsity"
                },
                "summary": "With the growing use of large language models (LLMs) hosted on cloud\nplatforms to offer inference services, privacy concerns about the potential\nleakage of sensitive information are escalating. Secure multi-party computation\n(MPC) is a promising solution to protect the privacy in LLM inference. However,\nMPC requires frequent inter-server communication, causing high performance\noverhead.\n  Inspired by the prevalent activation sparsity of LLMs, where most neuron are\nnot activated after non-linear activation functions, we propose an efficient\nprivate inference system, Comet. This system employs an accurate and fast\npredictor to predict the sparsity distribution of activation function output.\nAdditionally, we introduce a new private inference protocol. It efficiently and\nsecurely avoids computations involving zero values by exploiting the spatial\nlocality of the predicted sparse distribution. While this computation-avoidance\napproach impacts the spatiotemporal continuity of KV cache entries, we address\nthis challenge with a low-communication overhead cache refilling strategy that\nmerges miss requests and incorporates a prefetching mechanism. Finally, we\nevaluate Comet on four common LLMs and compare it with six state-of-the-art\nprivate inference systems. Comet achieves a 1.87x-2.63x speedup and a\n1.94x-2.64x communication reduction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the growing use of large language models (LLMs) hosted on cloud\nplatforms to offer inference services, privacy concerns about the potential\nleakage of sensitive information are escalating. Secure multi-party computation\n(MPC) is a promising solution to protect the privacy in LLM inference. However,\nMPC requires frequent inter-server communication, causing high performance\noverhead.\n  Inspired by the prevalent activation sparsity of LLMs, where most neuron are\nnot activated after non-linear activation functions, we propose an efficient\nprivate inference system, Comet. This system employs an accurate and fast\npredictor to predict the sparsity distribution of activation function output.\nAdditionally, we introduce a new private inference protocol. It efficiently and\nsecurely avoids computations involving zero values by exploiting the spatial\nlocality of the predicted sparse distribution. While this computation-avoidance\napproach impacts the spatiotemporal continuity of KV cache entries, we address\nthis challenge with a low-communication overhead cache refilling strategy that\nmerges miss requests and incorporates a prefetching mechanism. Finally, we\nevaluate Comet on four common LLMs and compare it with six state-of-the-art\nprivate inference systems. Comet achieves a 1.87x-2.63x speedup and a\n1.94x-2.64x communication reduction."
                },
                "authors": [
                    {
                        "name": "Guang Yan"
                    },
                    {
                        "name": "Yuhui Zhang"
                    },
                    {
                        "name": "Zimu Guo"
                    },
                    {
                        "name": "Lutan Zhao"
                    },
                    {
                        "name": "Xiaojun Chen"
                    },
                    {
                        "name": "Chen Wang"
                    },
                    {
                        "name": "Wenhao Wang"
                    },
                    {
                        "name": "Dan Meng"
                    },
                    {
                        "name": "Rui Hou"
                    }
                ],
                "author_detail": {
                    "name": "Rui Hou"
                },
                "author": "Rui Hou",
                "arxiv_doi": "10.1109/SP61157.2025.00182",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/SP61157.2025.00182",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2505.07239v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07239v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted to SP 2025",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07203v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07203v1",
                "updated": "2025-05-12T03:22:29Z",
                "updated_parsed": [
                    2025,
                    5,
                    12,
                    3,
                    22,
                    29,
                    0,
                    132,
                    0
                ],
                "published": "2025-05-12T03:22:29Z",
                "published_parsed": [
                    2025,
                    5,
                    12,
                    3,
                    22,
                    29,
                    0,
                    132,
                    0
                ],
                "title": "PrefillOnly: An Inference Engine for Prefill-only Workloads in Large\n  Language Model Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PrefillOnly: An Inference Engine for Prefill-only Workloads in Large\n  Language Model Applications"
                },
                "summary": "Besides typical generative applications, like ChatGPT, GitHub Copilot, and\nCursor, we observe an emerging trend that LLMs are increasingly used in\ntraditional discriminative tasks, such as recommendation, credit verification,\nand data labeling. The key characteristic of these emerging use cases is that\nthe LLM generates only a single output token, rather than an arbitrarily long\nsequence of tokens. We call this prefill-only workload. However, since existing\nLLM engines assume arbitrary output lengths, they fail to leverage the unique\nproperties of prefill-only workloads. In this paper, we present PrefillOnly,\nthe first LLM inference engine that improves the inference throughput and\nlatency by fully embracing the properties of prefill-only workloads. First,\nsince it generates only one token, PrefillOnly only needs to store the KV cache\nof only the last computed layer, rather than of all layers. This drastically\nreduces the GPU memory footprint of LLM inference and allows handling long\ninputs without using solutions that reduces throughput, such as cross-GPU KV\ncache parallelization. Second, because the output length is fixed, rather than\narbitrary, PrefillOnly can precisely determine the job completion time (JCT) of\neach prefill-only request before it starts. This enables efficient JCT-aware\nscheduling policies such as shortest remaining job first. PrefillOnly can\nprocess upto 4x larger queries per second without inflating average and P99\nlatency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Besides typical generative applications, like ChatGPT, GitHub Copilot, and\nCursor, we observe an emerging trend that LLMs are increasingly used in\ntraditional discriminative tasks, such as recommendation, credit verification,\nand data labeling. The key characteristic of these emerging use cases is that\nthe LLM generates only a single output token, rather than an arbitrarily long\nsequence of tokens. We call this prefill-only workload. However, since existing\nLLM engines assume arbitrary output lengths, they fail to leverage the unique\nproperties of prefill-only workloads. In this paper, we present PrefillOnly,\nthe first LLM inference engine that improves the inference throughput and\nlatency by fully embracing the properties of prefill-only workloads. First,\nsince it generates only one token, PrefillOnly only needs to store the KV cache\nof only the last computed layer, rather than of all layers. This drastically\nreduces the GPU memory footprint of LLM inference and allows handling long\ninputs without using solutions that reduces throughput, such as cross-GPU KV\ncache parallelization. Second, because the output length is fixed, rather than\narbitrary, PrefillOnly can precisely determine the job completion time (JCT) of\neach prefill-only request before it starts. This enables efficient JCT-aware\nscheduling policies such as shortest remaining job first. PrefillOnly can\nprocess upto 4x larger queries per second without inflating average and P99\nlatency."
                },
                "authors": [
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Bowen Wang"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Yiming Cheng"
                    },
                    {
                        "name": "Qing Lan"
                    },
                    {
                        "name": "Hejian Sang"
                    },
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Xiaoxuan Liu"
                    },
                    {
                        "name": "Yifan Qiao"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Junchen Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Junchen Jiang"
                },
                "author": "Junchen Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.07203v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07203v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.06901v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.06901v1",
                "updated": "2025-05-11T08:44:31Z",
                "updated_parsed": [
                    2025,
                    5,
                    11,
                    8,
                    44,
                    31,
                    6,
                    131,
                    0
                ],
                "published": "2025-05-11T08:44:31Z",
                "published_parsed": [
                    2025,
                    5,
                    11,
                    8,
                    44,
                    31,
                    6,
                    131,
                    0
                ],
                "title": "Ecco: Improving Memory Bandwidth and Capacity for LLMs via Entropy-aware\n  Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ecco: Improving Memory Bandwidth and Capacity for LLMs via Entropy-aware\n  Cache Compression"
                },
                "summary": "Large language models (LLMs) have demonstrated transformative capabilities\nacross diverse artificial intelligence applications, yet their deployment is\nhindered by substantial memory and computational demands, especially in\nresource-constrained environments. Quantization techniques have emerged as a\ncritical solution, reducing data precision to enhance memory and computational\nefficiency. However, existing methods often suffer from high runtime overheads\nand potential accuracy degradation. To address these challenges, we propose\nEcco, an entropy-based cache compression technique tailored for LLMs. Ecco\ncombines group-wise and non-uniform quantization with pre-defined shared\nk-means patterns and Huffman coding to exploit the inherent entropy\ncharacteristics of LLM cache data. Recognizing the inefficiencies of\ntraditional Huffman coding in terms of parallelism and latency, we introduce a\nnovel parallel Huffman-based decoding process with a multi-stage pipeline\ndesign, reducing latency by two orders of magnitude and achieving throughput\ncomparable to GPU L2 caches. Comprehensive evaluations demonstrate that Ecco\nachieves an up to 2.9$\\times$ and 1.9$\\times$ speedup over the state-of-the-art\nAWQ and SmoothQuant framework, 2.4$\\times$ over the Olive accelerator, all\nwhile increasing memory capacity by nearly 4$\\times$ and maintaining\nstate-of-the-art LLM accuracy. These results underscore the effectiveness of\nour entropy-based cache compression in enhancing LLM performance and\nefficiency, paving the way for more deployable large-scale AI models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated transformative capabilities\nacross diverse artificial intelligence applications, yet their deployment is\nhindered by substantial memory and computational demands, especially in\nresource-constrained environments. Quantization techniques have emerged as a\ncritical solution, reducing data precision to enhance memory and computational\nefficiency. However, existing methods often suffer from high runtime overheads\nand potential accuracy degradation. To address these challenges, we propose\nEcco, an entropy-based cache compression technique tailored for LLMs. Ecco\ncombines group-wise and non-uniform quantization with pre-defined shared\nk-means patterns and Huffman coding to exploit the inherent entropy\ncharacteristics of LLM cache data. Recognizing the inefficiencies of\ntraditional Huffman coding in terms of parallelism and latency, we introduce a\nnovel parallel Huffman-based decoding process with a multi-stage pipeline\ndesign, reducing latency by two orders of magnitude and achieving throughput\ncomparable to GPU L2 caches. Comprehensive evaluations demonstrate that Ecco\nachieves an up to 2.9$\\times$ and 1.9$\\times$ speedup over the state-of-the-art\nAWQ and SmoothQuant framework, 2.4$\\times$ over the Olive accelerator, all\nwhile increasing memory capacity by nearly 4$\\times$ and maintaining\nstate-of-the-art LLM accuracy. These results underscore the effectiveness of\nour entropy-based cache compression in enhancing LLM performance and\nefficiency, paving the way for more deployable large-scale AI models."
                },
                "authors": [
                    {
                        "name": "Feng Cheng"
                    },
                    {
                        "name": "Cong Guo"
                    },
                    {
                        "name": "Chiyue Wei"
                    },
                    {
                        "name": "Junyao Zhang"
                    },
                    {
                        "name": "Changchun Zhou"
                    },
                    {
                        "name": "Edward Hanson"
                    },
                    {
                        "name": "Jiaqi Zhang"
                    },
                    {
                        "name": "Xiaoxiao Liu"
                    },
                    {
                        "name": "Hai \"Helen\" Li"
                    },
                    {
                        "name": "Yiran Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yiran Chen"
                },
                "author": "Yiran Chen",
                "arxiv_comment": "ISCA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.06901v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.06901v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.06625v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.06625v1",
                "updated": "2025-05-10T12:16:50Z",
                "updated_parsed": [
                    2025,
                    5,
                    10,
                    12,
                    16,
                    50,
                    5,
                    130,
                    0
                ],
                "published": "2025-05-10T12:16:50Z",
                "published_parsed": [
                    2025,
                    5,
                    10,
                    12,
                    16,
                    50,
                    5,
                    130,
                    0
                ],
                "title": "CaMDN: Enhancing Cache Efficiency for Multi-tenant DNNs on Integrated\n  NPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CaMDN: Enhancing Cache Efficiency for Multi-tenant DNNs on Integrated\n  NPUs"
                },
                "summary": "With the rapid development of DNN applications, multi-tenant execution, where\nmultiple DNNs are co-located on a single SoC, is becoming a prevailing trend.\nAlthough many methods are proposed in prior works to improve multi-tenant\nperformance, the impact of shared cache is not well studied. This paper\nproposes CaMDN, an architecture-scheduling co-design to enhance cache\nefficiency for multi-tenant DNNs on integrated NPUs. Specifically, a\nlightweight architecture is proposed to support model-exclusive, NPU-controlled\nregions inside shared cache to eliminate unexpected cache contention. Moreover,\na cache scheduling method is proposed to improve shared cache utilization. In\nparticular, it includes a cache-aware mapping method for adaptability to the\nvarying available cache capacity and a dynamic allocation algorithm to adjust\nthe usage among co-located DNNs at runtime. Compared to prior works, CaMDN\nreduces the memory access by 33.4% on average and achieves a model speedup of\nup to 2.56$\\times$ (1.88$\\times$ on average).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid development of DNN applications, multi-tenant execution, where\nmultiple DNNs are co-located on a single SoC, is becoming a prevailing trend.\nAlthough many methods are proposed in prior works to improve multi-tenant\nperformance, the impact of shared cache is not well studied. This paper\nproposes CaMDN, an architecture-scheduling co-design to enhance cache\nefficiency for multi-tenant DNNs on integrated NPUs. Specifically, a\nlightweight architecture is proposed to support model-exclusive, NPU-controlled\nregions inside shared cache to eliminate unexpected cache contention. Moreover,\na cache scheduling method is proposed to improve shared cache utilization. In\nparticular, it includes a cache-aware mapping method for adaptability to the\nvarying available cache capacity and a dynamic allocation algorithm to adjust\nthe usage among co-located DNNs at runtime. Compared to prior works, CaMDN\nreduces the memory access by 33.4% on average and achieves a model speedup of\nup to 2.56$\\times$ (1.88$\\times$ on average)."
                },
                "authors": [
                    {
                        "name": "Tianhao Cai"
                    },
                    {
                        "name": "Liang Wang"
                    },
                    {
                        "name": "Limin Xiao"
                    },
                    {
                        "name": "Meng Han"
                    },
                    {
                        "name": "Zeyu Wang"
                    },
                    {
                        "name": "Lin Sun"
                    },
                    {
                        "name": "Xiaojian Liao"
                    }
                ],
                "author_detail": {
                    "name": "Xiaojian Liao"
                },
                "author": "Xiaojian Liao",
                "arxiv_comment": "7 pages, 9 figures. This paper has been accepted to the 2025 Design\n  Automation Conference (DAC)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.06625v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.06625v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.06556v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.06556v1",
                "updated": "2025-05-10T07:57:02Z",
                "updated_parsed": [
                    2025,
                    5,
                    10,
                    7,
                    57,
                    2,
                    5,
                    130,
                    0
                ],
                "published": "2025-05-10T07:57:02Z",
                "published_parsed": [
                    2025,
                    5,
                    10,
                    7,
                    57,
                    2,
                    5,
                    130,
                    0
                ],
                "title": "TierBase: A Workload-Driven Cost-Optimized Key-Value Store",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TierBase: A Workload-Driven Cost-Optimized Key-Value Store"
                },
                "summary": "In the current era of data-intensive applications, the demand for\nhigh-performance, cost-effective storage solutions is paramount. This paper\nintroduces a Space-Performance Cost Model for key-value store, designed to\nguide cost-effective storage configuration decisions. The model quantifies the\ntrade-offs between performance and storage costs, providing a framework for\noptimizing resource allocation in large-scale data serving environments. Guided\nby this cost model, we present TierBase, a distributed key-value store\ndeveloped by Ant Group that optimizes total cost by strategically synchronizing\ndata between cache and storage tiers, maximizing resource utilization and\neffectively handling skewed workloads. To enhance cost-efficiency, TierBase\nincorporates several optimization techniques, including pre-trained data\ncompression, elastic threading mechanisms, and the utilization of persistent\nmemory. We detail TierBase's architecture, key components, and the\nimplementation of cost optimization strategies. Extensive evaluations using\nboth synthetic benchmarks and real-world workloads demonstrate TierBase's\nsuperior cost-effectiveness compared to existing solutions. Furthermore, case\nstudies from Ant Group's production environments showcase TierBase's ability to\nachieve up to 62% cost reduction in primary scenarios, highlighting its\npractical impact in large-scale online data serving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the current era of data-intensive applications, the demand for\nhigh-performance, cost-effective storage solutions is paramount. This paper\nintroduces a Space-Performance Cost Model for key-value store, designed to\nguide cost-effective storage configuration decisions. The model quantifies the\ntrade-offs between performance and storage costs, providing a framework for\noptimizing resource allocation in large-scale data serving environments. Guided\nby this cost model, we present TierBase, a distributed key-value store\ndeveloped by Ant Group that optimizes total cost by strategically synchronizing\ndata between cache and storage tiers, maximizing resource utilization and\neffectively handling skewed workloads. To enhance cost-efficiency, TierBase\nincorporates several optimization techniques, including pre-trained data\ncompression, elastic threading mechanisms, and the utilization of persistent\nmemory. We detail TierBase's architecture, key components, and the\nimplementation of cost optimization strategies. Extensive evaluations using\nboth synthetic benchmarks and real-world workloads demonstrate TierBase's\nsuperior cost-effectiveness compared to existing solutions. Furthermore, case\nstudies from Ant Group's production environments showcase TierBase's ability to\nachieve up to 62% cost reduction in primary scenarios, highlighting its\npractical impact in large-scale online data serving."
                },
                "authors": [
                    {
                        "name": "Zhitao Shen"
                    },
                    {
                        "name": "Shiyu Yang"
                    },
                    {
                        "name": "Weibo Chen"
                    },
                    {
                        "name": "Kunming Wang"
                    },
                    {
                        "name": "Yue Li"
                    },
                    {
                        "name": "Jiabao Jin"
                    },
                    {
                        "name": "Wei Jia"
                    },
                    {
                        "name": "Junwei Chen"
                    },
                    {
                        "name": "Yuan Su"
                    },
                    {
                        "name": "Xiaoxia Duan"
                    },
                    {
                        "name": "Wei Chen"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Jie Song"
                    },
                    {
                        "name": "Ruoyi Ruan"
                    },
                    {
                        "name": "Xuemin Lin"
                    }
                ],
                "author_detail": {
                    "name": "Xuemin Lin"
                },
                "author": "Xuemin Lin",
                "arxiv_comment": "Accepted by ICDE 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.06556v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.06556v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07872v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07872v1",
                "updated": "2025-05-09T21:05:20Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    21,
                    5,
                    20,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-09T21:05:20Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    21,
                    5,
                    20,
                    4,
                    129,
                    0
                ],
                "title": "Revenue Optimization in Video Caching Networks with Privacy-Preserving\n  Demand Predictions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revenue Optimization in Video Caching Networks with Privacy-Preserving\n  Demand Predictions"
                },
                "summary": "Performance of video streaming, which accounts for most of the traffic in\nwireless communication, can be significantly improved by caching popular videos\nat the wireless edge. Determining the cache content that optimizes performance\n(defined via a revenue function) is thus an important task, and prediction of\nthe future demands based on past history can make this process much more\nefficient. However, since practical video caching networks involve various\nparties (e.g., users, isp, and csp) that do not wish to reveal information such\nas past history to each other, privacy-preserving solutions are required.\nMotivated by this, we propose a proactive caching method based on users'\nprivacy-preserving multi-slot future demand predictions -- obtained from a\ntrained Transformer -- to optimize revenue. Specifically, we first use a\nprivacy-preserving fl algorithm to train a Transformer to predict multi-slot\nfuture demands of the users. However, prediction accuracy is not perfect and\ndecreases the farther into the future the prediction is done. We model the\nimpact of prediction errors invoking the file popularities, based on which we\nformulate a long-term system revenue optimization to make the cache placement\ndecisions. As the formulated problem is NP-hard, we use a greedy algorithm to\nefficiently obtain an approximate solution. Simulation results validate that\n(i) the fl solution achieves results close to the centralized\n(non-privacy-preserving) solution and (ii) optimization of revenue may provide\ndifferent solutions than the classical chr criterion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance of video streaming, which accounts for most of the traffic in\nwireless communication, can be significantly improved by caching popular videos\nat the wireless edge. Determining the cache content that optimizes performance\n(defined via a revenue function) is thus an important task, and prediction of\nthe future demands based on past history can make this process much more\nefficient. However, since practical video caching networks involve various\nparties (e.g., users, isp, and csp) that do not wish to reveal information such\nas past history to each other, privacy-preserving solutions are required.\nMotivated by this, we propose a proactive caching method based on users'\nprivacy-preserving multi-slot future demand predictions -- obtained from a\ntrained Transformer -- to optimize revenue. Specifically, we first use a\nprivacy-preserving fl algorithm to train a Transformer to predict multi-slot\nfuture demands of the users. However, prediction accuracy is not perfect and\ndecreases the farther into the future the prediction is done. We model the\nimpact of prediction errors invoking the file popularities, based on which we\nformulate a long-term system revenue optimization to make the cache placement\ndecisions. As the formulated problem is NP-hard, we use a greedy algorithm to\nefficiently obtain an approximate solution. Simulation results validate that\n(i) the fl solution achieves results close to the centralized\n(non-privacy-preserving) solution and (ii) optimization of revenue may provide\ndifferent solutions than the classical chr criterion."
                },
                "authors": [
                    {
                        "name": "Yijing Zhang"
                    },
                    {
                        "name": "Ferdous Pervej"
                    },
                    {
                        "name": "Andreas F. Molisch"
                    }
                ],
                "author_detail": {
                    "name": "Andreas F. Molisch"
                },
                "author": "Andreas F. Molisch",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.07872v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07872v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.06095v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06095v3",
                "updated": "2025-05-09T07:26:29Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    7,
                    26,
                    29,
                    4,
                    129,
                    0
                ],
                "published": "2024-06-10T08:26:27Z",
                "published_parsed": [
                    2024,
                    6,
                    10,
                    8,
                    26,
                    27,
                    0,
                    162,
                    0
                ],
                "title": "An extension of C++ with memory-centric specifications for HPC to reduce\n  memory footprints and streamline MPI development",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An extension of C++ with memory-centric specifications for HPC to reduce\n  memory footprints and streamline MPI development"
                },
                "summary": "The C++ programming language and its cousins lean towards a\nmemory-inefficient storage of structs: The compiler inserts helper bits such\nthat individual instance variables fit to byte or cache boundaries, while it is\nnot able to exploit knowledge about the range of integers, enums or bitsets.\nFurthermore, the language provides neither support for data exchange via MPI\nnor for arbitrary floating-point precisions. We propose C++ attributes through\nwhich developers can guide the compiler what memory arrangements would be\nbeneficial: Can multiple booleans or integers with limited range be squeezed\ninto one bit field, do floating point numbers hold fewer significant bits than\nin the IEEE standard, or does the code benefit from a MPI datatype for subsets\nof attributes? The extension offers the opportunity to fall back to normal\nalignment via plain C++ assignments, no dependencies upon external libraries\nare introduced, and the resulting code remains standard C++ subject to some\nweakened guarantees on addresses and pointer arithmetics. Our work implements\nthe language annotations within LLVM and demonstrates their potential impact,\nboth upon the runtime and the memory footprint, through smoothed particle\nhydrodynamics (SPH) benchmarks. They uncover the potential gains in terms of\nperformance and development productivity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The C++ programming language and its cousins lean towards a\nmemory-inefficient storage of structs: The compiler inserts helper bits such\nthat individual instance variables fit to byte or cache boundaries, while it is\nnot able to exploit knowledge about the range of integers, enums or bitsets.\nFurthermore, the language provides neither support for data exchange via MPI\nnor for arbitrary floating-point precisions. We propose C++ attributes through\nwhich developers can guide the compiler what memory arrangements would be\nbeneficial: Can multiple booleans or integers with limited range be squeezed\ninto one bit field, do floating point numbers hold fewer significant bits than\nin the IEEE standard, or does the code benefit from a MPI datatype for subsets\nof attributes? The extension offers the opportunity to fall back to normal\nalignment via plain C++ assignments, no dependencies upon external libraries\nare introduced, and the resulting code remains standard C++ subject to some\nweakened guarantees on addresses and pointer arithmetics. Our work implements\nthe language annotations within LLVM and demonstrates their potential impact,\nboth upon the runtime and the memory footprint, through smoothed particle\nhydrodynamics (SPH) benchmarks. They uncover the potential gains in terms of\nperformance and development productivity."
                },
                "authors": [
                    {
                        "name": "Pawel K. Radtke"
                    },
                    {
                        "name": "Cristian G. Barrera-Hinojosa"
                    },
                    {
                        "name": "Mladen Ivkovic"
                    },
                    {
                        "name": "Tobias Weinzierl"
                    }
                ],
                "author_detail": {
                    "name": "Tobias Weinzierl"
                },
                "author": "Tobias Weinzierl",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.06095v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06095v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05829v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05829v1",
                "updated": "2025-05-09T06:56:17Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    6,
                    56,
                    17,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-09T06:56:17Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    6,
                    56,
                    17,
                    4,
                    129,
                    0
                ],
                "title": "Accelerating Diffusion Transformer via Increment-Calibrated Caching with\n  Channel-Aware Singular Value Decomposition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformer via Increment-Calibrated Caching with\n  Channel-Aware Singular Value Decomposition"
                },
                "summary": "Diffusion transformer (DiT) models have achieved remarkable success in image\ngeneration, thanks for their exceptional generative capabilities and\nscalability. Nonetheless, the iterative nature of diffusion models (DMs)\nresults in high computation complexity, posing challenges for deployment.\nAlthough existing cache-based acceleration methods try to utilize the inherent\ntemporal similarity to skip redundant computations of DiT, the lack of\ncorrection may induce potential quality degradation. In this paper, we propose\nincrement-calibrated caching, a training-free method for DiT acceleration,\nwhere the calibration parameters are generated from the pre-trained model\nitself with low-rank approximation. To deal with the possible correction\nfailure arising from outlier activations, we introduce channel-aware Singular\nValue Decomposition (SVD), which further strengthens the calibration effect.\nExperimental results show that our method always achieve better performance\nthan existing naive caching methods with a similar computation resource budget.\nWhen compared with 35-step DDIM, our method eliminates more than 45%\ncomputation and improves IS by 12 at the cost of less than 0.06 FID increase.\nCode is available at https://github.com/ccccczzy/icc.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion transformer (DiT) models have achieved remarkable success in image\ngeneration, thanks for their exceptional generative capabilities and\nscalability. Nonetheless, the iterative nature of diffusion models (DMs)\nresults in high computation complexity, posing challenges for deployment.\nAlthough existing cache-based acceleration methods try to utilize the inherent\ntemporal similarity to skip redundant computations of DiT, the lack of\ncorrection may induce potential quality degradation. In this paper, we propose\nincrement-calibrated caching, a training-free method for DiT acceleration,\nwhere the calibration parameters are generated from the pre-trained model\nitself with low-rank approximation. To deal with the possible correction\nfailure arising from outlier activations, we introduce channel-aware Singular\nValue Decomposition (SVD), which further strengthens the calibration effect.\nExperimental results show that our method always achieve better performance\nthan existing naive caching methods with a similar computation resource budget.\nWhen compared with 35-step DDIM, our method eliminates more than 45%\ncomputation and improves IS by 12 at the cost of less than 0.06 FID increase.\nCode is available at https://github.com/ccccczzy/icc."
                },
                "authors": [
                    {
                        "name": "Zhiyuan Chen"
                    },
                    {
                        "name": "Keyi Li"
                    },
                    {
                        "name": "Yifan Jia"
                    },
                    {
                        "name": "Le Ye"
                    },
                    {
                        "name": "Yufei Ma"
                    }
                ],
                "author_detail": {
                    "name": "Yufei Ma"
                },
                "author": "Yufei Ma",
                "arxiv_comment": "accepted by CVPR2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05829v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05829v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05772v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05772v1",
                "updated": "2025-05-09T04:17:05Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    4,
                    17,
                    5,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-09T04:17:05Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    4,
                    17,
                    5,
                    4,
                    129,
                    0
                ],
                "title": "Sparse Attention Remapping with Clustering for Efficient LLM Decoding on\n  PIM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Attention Remapping with Clustering for Efficient LLM Decoding on\n  PIM"
                },
                "summary": "Transformer-based models are the foundation of modern machine learning, but\ntheir execution, particularly during autoregressive decoding in large language\nmodels (LLMs), places significant pressure on memory systems due to frequent\nmemory accesses and growing key-value (KV) caches. This creates a bottleneck in\nmemory bandwidth, especially as context lengths increase. Processing-in-memory\n(PIM) architectures are a promising solution, offering high internal bandwidth\nand compute parallelism near memory. However, current PIM designs are primarily\noptimized for dense attention and struggle with the dynamic, irregular access\npatterns introduced by modern KV cache sparsity techniques. Consequently, they\nsuffer from workload imbalance, reducing throughput and resource utilization.\nIn this work, we propose STARC, a novel sparsity-optimized data mapping scheme\ntailored specifically for efficient LLM decoding on PIM architectures. STARC\nclusters KV pairs by semantic similarity and maps them to contiguous memory\nregions aligned with PIM bank structures. During decoding, queries retrieve\nrelevant tokens at cluster granularity by matching against precomputed\ncentroids, enabling selective attention and parallel processing without\nfrequent reclustering or data movement overhead. Experiments on the HBM-PIM\nsystem show that, compared to common token-wise sparsity methods, STARC reduces\nattention-layer latency by 19%--31% and energy consumption by 19%--27%. Under a\nKV cache budget of 1024, it achieves up to 54%--74% latency reduction and\n45%--67% energy reduction compared to full KV cache retrieval. Meanwhile, STARC\nmaintains model accuracy comparable to state-of-the-art sparse attention\nmethods, demonstrating its effectiveness in enabling efficient and\nhardware-friendly long-context LLM inference on PIM architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based models are the foundation of modern machine learning, but\ntheir execution, particularly during autoregressive decoding in large language\nmodels (LLMs), places significant pressure on memory systems due to frequent\nmemory accesses and growing key-value (KV) caches. This creates a bottleneck in\nmemory bandwidth, especially as context lengths increase. Processing-in-memory\n(PIM) architectures are a promising solution, offering high internal bandwidth\nand compute parallelism near memory. However, current PIM designs are primarily\noptimized for dense attention and struggle with the dynamic, irregular access\npatterns introduced by modern KV cache sparsity techniques. Consequently, they\nsuffer from workload imbalance, reducing throughput and resource utilization.\nIn this work, we propose STARC, a novel sparsity-optimized data mapping scheme\ntailored specifically for efficient LLM decoding on PIM architectures. STARC\nclusters KV pairs by semantic similarity and maps them to contiguous memory\nregions aligned with PIM bank structures. During decoding, queries retrieve\nrelevant tokens at cluster granularity by matching against precomputed\ncentroids, enabling selective attention and parallel processing without\nfrequent reclustering or data movement overhead. Experiments on the HBM-PIM\nsystem show that, compared to common token-wise sparsity methods, STARC reduces\nattention-layer latency by 19%--31% and energy consumption by 19%--27%. Under a\nKV cache budget of 1024, it achieves up to 54%--74% latency reduction and\n45%--67% energy reduction compared to full KV cache retrieval. Meanwhile, STARC\nmaintains model accuracy comparable to state-of-the-art sparse attention\nmethods, demonstrating its effectiveness in enabling efficient and\nhardware-friendly long-context LLM inference on PIM architectures."
                },
                "authors": [
                    {
                        "name": "Zehao Fan"
                    },
                    {
                        "name": "Garrett Gagnon"
                    },
                    {
                        "name": "Zhenyu Liu"
                    },
                    {
                        "name": "Liu Liu"
                    }
                ],
                "author_detail": {
                    "name": "Liu Liu"
                },
                "author": "Liu Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05772v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05772v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17264v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17264v3",
                "updated": "2025-05-09T00:31:24Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    0,
                    31,
                    24,
                    4,
                    129,
                    0
                ],
                "published": "2024-09-25T18:21:05Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    18,
                    21,
                    5,
                    2,
                    269,
                    0
                ],
                "title": "Medha: Efficiently Serving Multi-Million Context Length LLM Inference\n  Requests Without Approximations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medha: Efficiently Serving Multi-Million Context Length LLM Inference\n  Requests Without Approximations"
                },
                "summary": "As large language models (LLMs) handle increasingly longer contexts, serving\nlong inference requests of millions of tokens presents unique challenges. We\nshow that existing work for long context inference is largely based on\ntechniques from long context training, and does not handle the high variability\nin input lengths during inference. This leads to inefficient resource\nutilization, server fragmentation, and head-of-line (HOL) blocking.\n  We present Medha, an end-to-end system for efficient long-context LLM\ninference that addresses these challenges through fine-grained time sharing.\nMedha introduces three key innovations: (1) the mechanism of adaptive prefill\nchunking to help mitigate HOL blocking with preemption; (2) two new parallelism\nstrategies: Sequence Pipeline Parallelism (SPP) to reduce time-to-first-token\nby pipelining prefill chunks, and KV-Cache Parallelism (KVP) to lower\ntime-peroutput-token by distributing decoding across servers; and (3) a novel\ninput-length aware least remaining slack scheduling to meet Service Level\nObjectives (SLOs).\n  Medha enables exact inference scaling beyond 10 million tokens, maintaining\nhigh throughput and low latency across mixed-length workloads. Compared to\nstate-of-the-art systems, Medha reduces server fragmentation, cuts median\nlatency by up to 30x, and improves throughput by over 5x, delivering\nproduction-scale long-context inference without compromising performance on\nshorter requests.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) handle increasingly longer contexts, serving\nlong inference requests of millions of tokens presents unique challenges. We\nshow that existing work for long context inference is largely based on\ntechniques from long context training, and does not handle the high variability\nin input lengths during inference. This leads to inefficient resource\nutilization, server fragmentation, and head-of-line (HOL) blocking.\n  We present Medha, an end-to-end system for efficient long-context LLM\ninference that addresses these challenges through fine-grained time sharing.\nMedha introduces three key innovations: (1) the mechanism of adaptive prefill\nchunking to help mitigate HOL blocking with preemption; (2) two new parallelism\nstrategies: Sequence Pipeline Parallelism (SPP) to reduce time-to-first-token\nby pipelining prefill chunks, and KV-Cache Parallelism (KVP) to lower\ntime-peroutput-token by distributing decoding across servers; and (3) a novel\ninput-length aware least remaining slack scheduling to meet Service Level\nObjectives (SLOs).\n  Medha enables exact inference scaling beyond 10 million tokens, maintaining\nhigh throughput and low latency across mixed-length workloads. Compared to\nstate-of-the-art systems, Medha reduces server fragmentation, cuts median\nlatency by up to 30x, and improves throughput by over 5x, delivering\nproduction-scale long-context inference without compromising performance on\nshorter requests."
                },
                "authors": [
                    {
                        "name": "Amey Agrawal"
                    },
                    {
                        "name": "Haoran Qiu"
                    },
                    {
                        "name": "Junda Chen"
                    },
                    {
                        "name": "Íñigo Goiri"
                    },
                    {
                        "name": "Chaojie Zhang"
                    },
                    {
                        "name": "Rayyan Shahid"
                    },
                    {
                        "name": "Ramachandran Ramjee"
                    },
                    {
                        "name": "Alexey Tumanov"
                    },
                    {
                        "name": "Esha Choukse"
                    }
                ],
                "author_detail": {
                    "name": "Esha Choukse"
                },
                "author": "Esha Choukse",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17264v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17264v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05251v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05251v1",
                "updated": "2025-05-08T13:56:20Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    13,
                    56,
                    20,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T13:56:20Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    13,
                    56,
                    20,
                    3,
                    128,
                    0
                ],
                "title": "High Altitude Platform-Based Caching and Multicasting for Rural\n  Connectivity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High Altitude Platform-Based Caching and Multicasting for Rural\n  Connectivity"
                },
                "summary": "Providing efficient and reliable content delivery in rural areas remains a\nsignificant challenge due to the lack of communication infrastructure. To\nbridge the digital divide, this paper investigates the potential of leveraging\nmultiple high-altitude platforms (HAPs) for energy-efficient content delivery\nin wide rural regions. Each caching-enabled HAP is equipped with both\nFree-Space Optical (FSO) transceivers for backhaul links and Radio Frequency\n(RF) antenna arrays for access links. To further enhance network efficiency, we\nconsider a network coding-based multicasting scheme, where different types of\ncontent are treated as distinct multicast sessions. With the objective of\nminimizing long-term power cost, we propose a hierarchical framework that\nintegrates deep reinforcement learn-ing (DRL) and convex optimization to\njointly optimize dynamic caching strategies and resource allocation across the\nnetwork. Simulation results demonstrate that our approach significantly reduces\npower cost compared to several baseline approaches, providing a practical\nsolution for improving rural connectivity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Providing efficient and reliable content delivery in rural areas remains a\nsignificant challenge due to the lack of communication infrastructure. To\nbridge the digital divide, this paper investigates the potential of leveraging\nmultiple high-altitude platforms (HAPs) for energy-efficient content delivery\nin wide rural regions. Each caching-enabled HAP is equipped with both\nFree-Space Optical (FSO) transceivers for backhaul links and Radio Frequency\n(RF) antenna arrays for access links. To further enhance network efficiency, we\nconsider a network coding-based multicasting scheme, where different types of\ncontent are treated as distinct multicast sessions. With the objective of\nminimizing long-term power cost, we propose a hierarchical framework that\nintegrates deep reinforcement learn-ing (DRL) and convex optimization to\njointly optimize dynamic caching strategies and resource allocation across the\nnetwork. Simulation results demonstrate that our approach significantly reduces\npower cost compared to several baseline approaches, providing a practical\nsolution for improving rural connectivity."
                },
                "authors": [
                    {
                        "name": "Yongqiang Zhang"
                    },
                    {
                        "name": "Mustafa A. Kishk"
                    },
                    {
                        "name": "Mohamed-Slim Alouini"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed-Slim Alouini"
                },
                "author": "Mohamed-Slim Alouini",
                "arxiv_comment": "13 pages, 8 figures, submitted to IEEE journals for possible\n  publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05251v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05251v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "49",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.4.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05130v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05130v1",
                "updated": "2025-05-08T11:07:35Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    11,
                    7,
                    35,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T11:07:35Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    11,
                    7,
                    35,
                    3,
                    128,
                    0
                ],
                "title": "CacheFL: Efficient Federated Cache Model Fine-Tuning for Vision-Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CacheFL: Efficient Federated Cache Model Fine-Tuning for Vision-Language\n  Models"
                },
                "summary": "Large pre-trained Vision-Language Models (VLMs), such as Contrastive\nLanguage-Image Pre-training (CLIP), have exhibited remarkable zero-shot\nperformance across various image classification tasks. Fine-tuning these models\non domain-specific datasets further enhances their effectiveness for downstream\napplications. However, fine-tuning in cloud environments raises significant\nconcerns regarding data security and privacy. Federated Learning (FL) offers a\ndecentralized solution by enabling model training across local clients without\ncentralizing sensitive data, but the high communication and computation costs\nof transmitting full pre-trained models during training limit its scalability.\nAdditionally, non-Independent and Identically Distributed (non-IID) data across\nlocal clients can negatively impact model convergence and performance. To\naddress these challenges, we propose CacheFL, a novel federated learning method\nthat replaces traditional full model fine-tuning with lightweight cache model\nfine-tuning. The cache model is initialized using a class-balanced dataset\ngenerated by a generative pre-trained model, effectively mitigating the impact\nof non-IID data. This cache model is then distributed to local clients for\nfine-tuning, and the updated parameters from each client are aggregated on the\nserver and redistributed. With the updated cache model, the classification\nperformance of CLIP is improved after just a few epochs. By limiting the\ntraining and communication to the cache model, CacheFL significantly reduces\nresource demands while ensuring data privacy and security. Extensive\nexperiments conducted on ImageNet and 10 additional datasets demonstrate that\nCacheFL outperforms traditional approaches in terms of classification accuracy,\nresource efficiency, and privacy preservation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large pre-trained Vision-Language Models (VLMs), such as Contrastive\nLanguage-Image Pre-training (CLIP), have exhibited remarkable zero-shot\nperformance across various image classification tasks. Fine-tuning these models\non domain-specific datasets further enhances their effectiveness for downstream\napplications. However, fine-tuning in cloud environments raises significant\nconcerns regarding data security and privacy. Federated Learning (FL) offers a\ndecentralized solution by enabling model training across local clients without\ncentralizing sensitive data, but the high communication and computation costs\nof transmitting full pre-trained models during training limit its scalability.\nAdditionally, non-Independent and Identically Distributed (non-IID) data across\nlocal clients can negatively impact model convergence and performance. To\naddress these challenges, we propose CacheFL, a novel federated learning method\nthat replaces traditional full model fine-tuning with lightweight cache model\nfine-tuning. The cache model is initialized using a class-balanced dataset\ngenerated by a generative pre-trained model, effectively mitigating the impact\nof non-IID data. This cache model is then distributed to local clients for\nfine-tuning, and the updated parameters from each client are aggregated on the\nserver and redistributed. With the updated cache model, the classification\nperformance of CLIP is improved after just a few epochs. By limiting the\ntraining and communication to the cache model, CacheFL significantly reduces\nresource demands while ensuring data privacy and security. Extensive\nexperiments conducted on ImageNet and 10 additional datasets demonstrate that\nCacheFL outperforms traditional approaches in terms of classification accuracy,\nresource efficiency, and privacy preservation."
                },
                "authors": [
                    {
                        "name": "Mengjun Yi"
                    },
                    {
                        "name": "Hanwen Zhang"
                    },
                    {
                        "name": "Hui Dou"
                    },
                    {
                        "name": "Jian Zhao"
                    },
                    {
                        "name": "Furao Shen"
                    }
                ],
                "author_detail": {
                    "name": "Furao Shen"
                },
                "author": "Furao Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05130v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05130v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03762v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03762v2",
                "updated": "2025-05-08T09:05:51Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    9,
                    5,
                    51,
                    3,
                    128,
                    0
                ],
                "published": "2025-04-20T17:48:54Z",
                "published_parsed": [
                    2025,
                    4,
                    20,
                    17,
                    48,
                    54,
                    6,
                    110,
                    0
                ],
                "title": "CVA6S+: A Superscalar RISC-V Core with High-Throughput Memory\n  Architecture",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CVA6S+: A Superscalar RISC-V Core with High-Throughput Memory\n  Architecture"
                },
                "summary": "Open-source RISC-V cores are increasingly adopted in high-end embedded\ndomains such as automotive, where maximizing instructions per cycle (IPC) is\nbecoming critical. Building on the industry-supported open-source CVA6 core and\nits superscalar variant, CVA6S, we introduce CVA6S+, an enhanced version\nincorporating improved branch prediction, register renaming and enhanced\noperand forwarding. These optimizations enable CVA6S+ to achieve a 43.5%\nperformance improvement over the scalar configuration and 10.9% over CVA6S,\nwith an area overhead of just 9.30% over the scalar core (CVA6). Furthermore,\nwe integrate CVA6S+ with the OpenHW Core-V High-Performance L1 Dcache\n(HPDCache) and report a 74.1% bandwidth improvement over the legacy CVA6 cache\nsubsystem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-source RISC-V cores are increasingly adopted in high-end embedded\ndomains such as automotive, where maximizing instructions per cycle (IPC) is\nbecoming critical. Building on the industry-supported open-source CVA6 core and\nits superscalar variant, CVA6S, we introduce CVA6S+, an enhanced version\nincorporating improved branch prediction, register renaming and enhanced\noperand forwarding. These optimizations enable CVA6S+ to achieve a 43.5%\nperformance improvement over the scalar configuration and 10.9% over CVA6S,\nwith an area overhead of just 9.30% over the scalar core (CVA6). Furthermore,\nwe integrate CVA6S+ with the OpenHW Core-V High-Performance L1 Dcache\n(HPDCache) and report a 74.1% bandwidth improvement over the legacy CVA6 cache\nsubsystem."
                },
                "authors": [
                    {
                        "name": "Riccardo Tedeschi"
                    },
                    {
                        "name": "Gianmarco Ottavi"
                    },
                    {
                        "name": "Côme Allart"
                    },
                    {
                        "name": "Nils Wistoff"
                    },
                    {
                        "name": "Zexin Fu"
                    },
                    {
                        "name": "Filippo Grillotti"
                    },
                    {
                        "name": "Fabio De Ambroggi"
                    },
                    {
                        "name": "Elio Guidetti"
                    },
                    {
                        "name": "Jean-Baptiste Rigaud"
                    },
                    {
                        "name": "Olivier Potin"
                    },
                    {
                        "name": "Jean Roch Coulon"
                    },
                    {
                        "name": "César Fuguet"
                    },
                    {
                        "name": "Luca Benini"
                    },
                    {
                        "name": "Davide Rossi"
                    }
                ],
                "author_detail": {
                    "name": "Davide Rossi"
                },
                "author": "Davide Rossi",
                "arxiv_comment": "3 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03762v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03762v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12110v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12110v2",
                "updated": "2025-05-08T07:55:38Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    7,
                    55,
                    38,
                    3,
                    128,
                    0
                ],
                "published": "2024-06-17T21:43:39Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    21,
                    43,
                    39,
                    0,
                    169,
                    0
                ],
                "title": "CacheSquash: Making caches speculation-aware",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CacheSquash: Making caches speculation-aware"
                },
                "summary": "Speculation is key to achieving high CPU performance, yet it enables risks\nlike Spectre attacks which remain a significant challenge to mitigate without\nincurring substantial performance overheads. These attacks typically unfold in\nthree stages: access, transmit, and receive. Typically, they exploit a cache\ntiming side channel during the transmit and receive phases: speculatively\naccessing sensitive data (access), altering cache state (transmit), and then\nutilizing a cache timing attack (e.g., Flush+Reload) to extract the secret\n(receive). Our key observation is that Spectre attacks only require the\ntransmit instruction to execute and dispatch a request to the cache hierarchy.\nIt need not complete before a misprediction is detected (and mis-speculated\ninstructions squashed) because responses from memory that arrive at the cache\nafter squashing still alter cache state. We propose a novel mitigation,\nCacheSquash, that cancels mis-speculated memory accesses. Immediately upon\nsquashing, a cancellation is sent to the cache hierarchy, propagating\ndownstream and preventing any changes to caches that have not yet received a\nresponse. This minimizes cache state changes, thereby reducing the likelihood\nof Spectre attacks succeeding. We implement CacheSquash on gem5 and show that\nit thwarts practical Spectre attacks, with near-zero performance overheads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculation is key to achieving high CPU performance, yet it enables risks\nlike Spectre attacks which remain a significant challenge to mitigate without\nincurring substantial performance overheads. These attacks typically unfold in\nthree stages: access, transmit, and receive. Typically, they exploit a cache\ntiming side channel during the transmit and receive phases: speculatively\naccessing sensitive data (access), altering cache state (transmit), and then\nutilizing a cache timing attack (e.g., Flush+Reload) to extract the secret\n(receive). Our key observation is that Spectre attacks only require the\ntransmit instruction to execute and dispatch a request to the cache hierarchy.\nIt need not complete before a misprediction is detected (and mis-speculated\ninstructions squashed) because responses from memory that arrive at the cache\nafter squashing still alter cache state. We propose a novel mitigation,\nCacheSquash, that cancels mis-speculated memory accesses. Immediately upon\nsquashing, a cancellation is sent to the cache hierarchy, propagating\ndownstream and preventing any changes to caches that have not yet received a\nresponse. This minimizes cache state changes, thereby reducing the likelihood\nof Spectre attacks succeeding. We implement CacheSquash on gem5 and show that\nit thwarts practical Spectre attacks, with near-zero performance overheads."
                },
                "authors": [
                    {
                        "name": "Hossam ElAtali"
                    },
                    {
                        "name": "N. Asokan"
                    }
                ],
                "author_detail": {
                    "name": "N. Asokan"
                },
                "author": "N. Asokan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12110v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12110v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01658v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01658v2",
                "updated": "2025-05-08T07:08:40Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    7,
                    8,
                    40,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-03T02:47:43Z",
                "published_parsed": [
                    2025,
                    5,
                    3,
                    2,
                    47,
                    43,
                    5,
                    123,
                    0
                ],
                "title": "A Survey on Inference Engines for Large Language Models: Perspectives on\n  Optimization and Efficiency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Inference Engines for Large Language Models: Perspectives on\n  Optimization and Efficiency"
                },
                "summary": "Large language models (LLMs) are widely applied in chatbots, code generators,\nand search engines. Workloads such as chain-of-thought, complex reasoning, and\nagent services significantly increase the inference cost by invoking the model\nrepeatedly. Optimization methods such as parallelism, compression, and caching\nhave been adopted to reduce costs, but the diverse service requirements make it\nhard to select the right method. Recently, specialized LLM inference engines\nhave emerged as a key component for integrating the optimization methods into\nservice-oriented infrastructures. However, a systematic study on inference\nengines is still lacking. This paper provides a comprehensive evaluation of 25\nopen-source and commercial inference engines. We examine each inference engine\nin terms of ease-of-use, ease-of-deployment, general-purpose support,\nscalability, and suitability for throughput- and latency-aware computation.\nFurthermore, we explore the design goals of each inference engine by\ninvestigating the optimization techniques it supports. In addition, we assess\nthe ecosystem maturity of open source inference engines and handle the\nperformance and cost policy of commercial solutions. We outline future research\ndirections that include support for complex LLM-based services, support of\nvarious hardware, and enhanced security, offering practical guidance to\nresearchers and developers in selecting and designing optimized LLM inference\nengines. We also provide a public repository to continually track developments\nin this fast-evolving field:\nhttps://github.com/sihyeong/Awesome-LLM-Inference-Engine",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are widely applied in chatbots, code generators,\nand search engines. Workloads such as chain-of-thought, complex reasoning, and\nagent services significantly increase the inference cost by invoking the model\nrepeatedly. Optimization methods such as parallelism, compression, and caching\nhave been adopted to reduce costs, but the diverse service requirements make it\nhard to select the right method. Recently, specialized LLM inference engines\nhave emerged as a key component for integrating the optimization methods into\nservice-oriented infrastructures. However, a systematic study on inference\nengines is still lacking. This paper provides a comprehensive evaluation of 25\nopen-source and commercial inference engines. We examine each inference engine\nin terms of ease-of-use, ease-of-deployment, general-purpose support,\nscalability, and suitability for throughput- and latency-aware computation.\nFurthermore, we explore the design goals of each inference engine by\ninvestigating the optimization techniques it supports. In addition, we assess\nthe ecosystem maturity of open source inference engines and handle the\nperformance and cost policy of commercial solutions. We outline future research\ndirections that include support for complex LLM-based services, support of\nvarious hardware, and enhanced security, offering practical guidance to\nresearchers and developers in selecting and designing optimized LLM inference\nengines. We also provide a public repository to continually track developments\nin this fast-evolving field:\nhttps://github.com/sihyeong/Awesome-LLM-Inference-Engine"
                },
                "authors": [
                    {
                        "name": "Sihyeong Park"
                    },
                    {
                        "name": "Sungryeol Jeon"
                    },
                    {
                        "name": "Chaelyn Lee"
                    },
                    {
                        "name": "Seokhun Jeon"
                    },
                    {
                        "name": "Byung-Soo Kim"
                    },
                    {
                        "name": "Jemin Lee"
                    }
                ],
                "author_detail": {
                    "name": "Jemin Lee"
                },
                "author": "Jemin Lee",
                "arxiv_comment": "Under review; 65 pages; 27 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01658v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01658v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04896v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04896v1",
                "updated": "2025-05-08T02:16:08Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    2,
                    16,
                    8,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T02:16:08Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    2,
                    16,
                    8,
                    3,
                    128,
                    0
                ],
                "title": "Memory Under Siege: A Comprehensive Survey of Side-Channel Attacks on\n  Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory Under Siege: A Comprehensive Survey of Side-Channel Attacks on\n  Memory"
                },
                "summary": "Side-channel attacks on memory (SCAM) exploit unintended data leaks from\nmemory subsystems to infer sensitive information, posing significant threats to\nsystem security. These attacks exploit vulnerabilities in memory access\npatterns, cache behaviors, and other microarchitectural features to bypass\ntraditional security measures. The purpose of this research is to examine SCAM,\nclassify various attack techniques, and evaluate existing defense mechanisms.\nIt guides researchers and industry professionals in improving memory security\nand mitigating emerging threats. We begin by identifying the major\nvulnerabilities in the memory system that are frequently exploited in SCAM,\nsuch as cache timing, speculative execution, \\textit{Rowhammer}, and other\nsophisticated approaches. Next, we outline a comprehensive taxonomy that\nsystematically classifies these attacks based on their types, target systems,\nattack vectors, and adversarial capabilities required to execute them. In\naddition, we review the current landscape of mitigation strategies, emphasizing\ntheir strengths and limitations. This work aims to provide a comprehensive\noverview of memory-based side-channel attacks with the goal of providing\nsignificant insights for researchers and practitioners to better understand,\ndetect, and mitigate SCAM risks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Side-channel attacks on memory (SCAM) exploit unintended data leaks from\nmemory subsystems to infer sensitive information, posing significant threats to\nsystem security. These attacks exploit vulnerabilities in memory access\npatterns, cache behaviors, and other microarchitectural features to bypass\ntraditional security measures. The purpose of this research is to examine SCAM,\nclassify various attack techniques, and evaluate existing defense mechanisms.\nIt guides researchers and industry professionals in improving memory security\nand mitigating emerging threats. We begin by identifying the major\nvulnerabilities in the memory system that are frequently exploited in SCAM,\nsuch as cache timing, speculative execution, \\textit{Rowhammer}, and other\nsophisticated approaches. Next, we outline a comprehensive taxonomy that\nsystematically classifies these attacks based on their types, target systems,\nattack vectors, and adversarial capabilities required to execute them. In\naddition, we review the current landscape of mitigation strategies, emphasizing\ntheir strengths and limitations. This work aims to provide a comprehensive\noverview of memory-based side-channel attacks with the goal of providing\nsignificant insights for researchers and practitioners to better understand,\ndetect, and mitigate SCAM risks."
                },
                "authors": [
                    {
                        "name": "MD Mahady Hassan"
                    },
                    {
                        "name": "Shanto Roy"
                    },
                    {
                        "name": "Reza Rahaeimehr"
                    }
                ],
                "author_detail": {
                    "name": "Reza Rahaeimehr"
                },
                "author": "Reza Rahaeimehr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04896v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04896v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04556v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04556v1",
                "updated": "2025-05-07T16:44:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    16,
                    44,
                    21,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T16:44:21Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    16,
                    44,
                    21,
                    2,
                    127,
                    0
                ],
                "title": "Comparing CPU and GPU compute of PERMANOVA on MI300A",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparing CPU and GPU compute of PERMANOVA on MI300A"
                },
                "summary": "Comparing the tradeoffs of CPU and GPU compute for memory-heavy algorithms is\noften challenging, due to the drastically different memory subsystems on host\nCPUs and discrete GPUs. The AMD MI300A is an exception, since it sports both\nCPU and GPU cores in a single package, all backed by the same type of HBM\nmemory. In this paper we analyze the performance of Permutational Multivariate\nAnalysis of Variance (PERMANOVA), a non-parametric method that tests whether\ntwo or more groups of objects are significantly different based on a\ncategorical factor. This method is memory-bound and has been recently optimized\nfor CPU cache locality. Our tests show that GPU cores on the MI300A prefer the\nbrute force approach instead, significantly outperforming the CPU-based\nimplementation. The significant benefit of Simultaneous Multithreading (SMT)\nwas also a pleasant surprise.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparing the tradeoffs of CPU and GPU compute for memory-heavy algorithms is\noften challenging, due to the drastically different memory subsystems on host\nCPUs and discrete GPUs. The AMD MI300A is an exception, since it sports both\nCPU and GPU cores in a single package, all backed by the same type of HBM\nmemory. In this paper we analyze the performance of Permutational Multivariate\nAnalysis of Variance (PERMANOVA), a non-parametric method that tests whether\ntwo or more groups of objects are significantly different based on a\ncategorical factor. This method is memory-bound and has been recently optimized\nfor CPU cache locality. Our tests show that GPU cores on the MI300A prefer the\nbrute force approach instead, significantly outperforming the CPU-based\nimplementation. The significant benefit of Simultaneous Multithreading (SMT)\nwas also a pleasant surprise."
                },
                "authors": [
                    {
                        "name": "Igor Sfiligoi"
                    }
                ],
                "author_detail": {
                    "name": "Igor Sfiligoi"
                },
                "author": "Igor Sfiligoi",
                "arxiv_comment": "7 pages, 1 figure, Accepted at PEARC25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04556v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04556v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04466v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04466v1",
                "updated": "2025-05-07T14:37:13Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    14,
                    37,
                    13,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T14:37:13Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    14,
                    37,
                    13,
                    2,
                    127,
                    0
                ],
                "title": "Securing Immersive 360 Video Streams through Attribute-Based Selective\n  Encryption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Securing Immersive 360 Video Streams through Attribute-Based Selective\n  Encryption"
                },
                "summary": "Delivering high-quality, secure 360{\\deg} video content introduces unique\nchallenges, primarily due to the high bitrates and interactive demands of\nimmersive media. Traditional HTTPS-based methods, although widely used, face\nlimitations in computational efficiency and scalability when securing these\nhigh-resolution streams. To address these issues, this paper proposes a novel\nframework integrating Attribute-Based Encryption (ABE) with selective\nencryption techniques tailored specifically for tiled 360{\\deg} video\nstreaming. Our approach employs selective encryption of frames at varying\nlevels to reduce computational overhead while ensuring robust protection\nagainst unauthorized access.\n  Moreover, we explore viewport-adaptive encryption, dynamically encrypting\nmore frames within tiles occupying larger portions of the viewer's field of\nview. This targeted method significantly enhances security in critical viewing\nareas without unnecessary overhead in peripheral regions. We deploy and\nevaluate our proposed approach using the CloudLab testbed, comparing its\nperformance against traditional HTTPS streaming. Experimental results\ndemonstrate that our ABE-based model achieves reduced computational load on\nintermediate caches, improves cache hit rates, and maintains comparable visual\nquality to HTTPS, as assessed by Video Multimethod Assessment Fusion (VMAF).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Delivering high-quality, secure 360{\\deg} video content introduces unique\nchallenges, primarily due to the high bitrates and interactive demands of\nimmersive media. Traditional HTTPS-based methods, although widely used, face\nlimitations in computational efficiency and scalability when securing these\nhigh-resolution streams. To address these issues, this paper proposes a novel\nframework integrating Attribute-Based Encryption (ABE) with selective\nencryption techniques tailored specifically for tiled 360{\\deg} video\nstreaming. Our approach employs selective encryption of frames at varying\nlevels to reduce computational overhead while ensuring robust protection\nagainst unauthorized access.\n  Moreover, we explore viewport-adaptive encryption, dynamically encrypting\nmore frames within tiles occupying larger portions of the viewer's field of\nview. This targeted method significantly enhances security in critical viewing\nareas without unnecessary overhead in peripheral regions. We deploy and\nevaluate our proposed approach using the CloudLab testbed, comparing its\nperformance against traditional HTTPS streaming. Experimental results\ndemonstrate that our ABE-based model achieves reduced computational load on\nintermediate caches, improves cache hit rates, and maintains comparable visual\nquality to HTTPS, as assessed by Video Multimethod Assessment Fusion (VMAF)."
                },
                "authors": [
                    {
                        "name": "Mohammad Waquas Usmani"
                    },
                    {
                        "name": "Susmit Shannigrahi"
                    },
                    {
                        "name": "Michael Zink"
                    }
                ],
                "author_detail": {
                    "name": "Michael Zink"
                },
                "author": "Michael Zink",
                "arxiv_comment": "8 pages plus references, 10 figures, some with subfigures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04466v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04466v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04421v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04421v1",
                "updated": "2025-05-07T13:54:26Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    13,
                    54,
                    26,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T13:54:26Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    13,
                    54,
                    26,
                    2,
                    127,
                    0
                ],
                "title": "LONGER: Scaling Up Long Sequence Modeling in Industrial Recommenders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LONGER: Scaling Up Long Sequence Modeling in Industrial Recommenders"
                },
                "summary": "Modeling ultra-long user behavior sequences is critical for capturing both\nlong- and short-term preferences in industrial recommender systems. Existing\nsolutions typically rely on two-stage retrieval or indirect modeling paradigms,\nincuring upstream-downstream inconsistency and computational inefficiency. In\nthis paper, we present LONGER, a Long-sequence Optimized traNsformer for\nGPU-Efficient Recommenders. LONGER incorporates (i) a global token mechanism\nfor stabilizing attention over long contexts, (ii) a token merge module with\nlightweight InnerTransformers and hybrid attention strategy to reduce quadratic\ncomplexity, and (iii) a series of engineering optimizations, including training\nwith mixed-precision and activation recomputation, KV cache serving, and the\nfully synchronous model training and serving framework for unified GPU-based\ndense and sparse parameter updates. LONGER consistently outperforms strong\nbaselines in both offline metrics and online A/B testing in both advertising\nand e-commerce services at ByteDance, validating its consistent effectiveness\nand industrial-level scaling laws. Currently, LONGER has been fully deployed at\nmore than 10 influential scenarios at ByteDance, serving billion users.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling ultra-long user behavior sequences is critical for capturing both\nlong- and short-term preferences in industrial recommender systems. Existing\nsolutions typically rely on two-stage retrieval or indirect modeling paradigms,\nincuring upstream-downstream inconsistency and computational inefficiency. In\nthis paper, we present LONGER, a Long-sequence Optimized traNsformer for\nGPU-Efficient Recommenders. LONGER incorporates (i) a global token mechanism\nfor stabilizing attention over long contexts, (ii) a token merge module with\nlightweight InnerTransformers and hybrid attention strategy to reduce quadratic\ncomplexity, and (iii) a series of engineering optimizations, including training\nwith mixed-precision and activation recomputation, KV cache serving, and the\nfully synchronous model training and serving framework for unified GPU-based\ndense and sparse parameter updates. LONGER consistently outperforms strong\nbaselines in both offline metrics and online A/B testing in both advertising\nand e-commerce services at ByteDance, validating its consistent effectiveness\nand industrial-level scaling laws. Currently, LONGER has been fully deployed at\nmore than 10 influential scenarios at ByteDance, serving billion users."
                },
                "authors": [
                    {
                        "name": "Zheng Chai"
                    },
                    {
                        "name": "Qin Ren"
                    },
                    {
                        "name": "Xijun Xiao"
                    },
                    {
                        "name": "Huizhi Yang"
                    },
                    {
                        "name": "Bo Han"
                    },
                    {
                        "name": "Sijun Zhang"
                    },
                    {
                        "name": "Di Chen"
                    },
                    {
                        "name": "Hui Lu"
                    },
                    {
                        "name": "Wenlin Zhao"
                    },
                    {
                        "name": "Lele Yu"
                    },
                    {
                        "name": "Xionghang Xie"
                    },
                    {
                        "name": "Shiru Ren"
                    },
                    {
                        "name": "Xiang Sun"
                    },
                    {
                        "name": "Yaocheng Tan"
                    },
                    {
                        "name": "Peng Xu"
                    },
                    {
                        "name": "Yuchao Zheng"
                    },
                    {
                        "name": "Di Wu"
                    }
                ],
                "author_detail": {
                    "name": "Di Wu"
                },
                "author": "Di Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04421v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04421v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04326v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04326v1",
                "updated": "2025-05-07T11:21:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    11,
                    21,
                    12,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T11:21:12Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    11,
                    21,
                    12,
                    2,
                    127,
                    0
                ],
                "title": "Design and Evaluation of an NDN-Based Network for Distributed Digital\n  Twins",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Design and Evaluation of an NDN-Based Network for Distributed Digital\n  Twins"
                },
                "summary": "Digital twins (DT) have received significant attention due to their numerous\nbenefits, such as real-time data analytics and cost reduction in production. DT\nserves as a fundamental component of many applications, encompassing smart\nmanufacturing, intelligent vehicles, and smart cities. By using Machine\nLearning (ML) and Artificial Intelligence (AI) techniques, DTs can efficiently\nfacilitate decision-making and productivity by simulating the status and\nchanges of a physical entity. To handle the massive amount of data brought by\nDTs, it is challenging to achieve low response latency for data fetching over\nexisting IP-based networks. IP-based networks use host addresses for end-to-end\ncommunication, making data distribution between DTs inefficient. Thus, we\npropose to use DTs in a distributed manner over Named Data Networking (NDN)\nnetworks. NDN is data-centric where data is routed based on content names,\ndynamically adjusting paths to optimize latency. Popular data is cached in\nnetwork nodes, reducing data transmission and network congestion. Since data is\nfetched by content names, users and mobile devices can move freely without IP\naddress reassignment. By using in-network caching and adaptive routing, we\nreckon NDN is an ideal fit for Future G Networks in the context of Digital\nTwins. We compared DTs in edge scenarios with cloud scenarios over NDN and\nIP-based networks to validate our insights. Extensive simulation results show\nthat using DT in the edge reduces response latency by 10.2x. This position\npaper represents an initial investigation into the gap in distributed DTs over\nNDN, serving as an early-stage study.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital twins (DT) have received significant attention due to their numerous\nbenefits, such as real-time data analytics and cost reduction in production. DT\nserves as a fundamental component of many applications, encompassing smart\nmanufacturing, intelligent vehicles, and smart cities. By using Machine\nLearning (ML) and Artificial Intelligence (AI) techniques, DTs can efficiently\nfacilitate decision-making and productivity by simulating the status and\nchanges of a physical entity. To handle the massive amount of data brought by\nDTs, it is challenging to achieve low response latency for data fetching over\nexisting IP-based networks. IP-based networks use host addresses for end-to-end\ncommunication, making data distribution between DTs inefficient. Thus, we\npropose to use DTs in a distributed manner over Named Data Networking (NDN)\nnetworks. NDN is data-centric where data is routed based on content names,\ndynamically adjusting paths to optimize latency. Popular data is cached in\nnetwork nodes, reducing data transmission and network congestion. Since data is\nfetched by content names, users and mobile devices can move freely without IP\naddress reassignment. By using in-network caching and adaptive routing, we\nreckon NDN is an ideal fit for Future G Networks in the context of Digital\nTwins. We compared DTs in edge scenarios with cloud scenarios over NDN and\nIP-based networks to validate our insights. Extensive simulation results show\nthat using DT in the edge reduces response latency by 10.2x. This position\npaper represents an initial investigation into the gap in distributed DTs over\nNDN, serving as an early-stage study."
                },
                "authors": [
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Zihan Jia"
                    },
                    {
                        "name": "Ze Wang"
                    },
                    {
                        "name": "Lin Cui"
                    },
                    {
                        "name": "Fung Po Tso"
                    }
                ],
                "author_detail": {
                    "name": "Fung Po Tso"
                },
                "author": "Fung Po Tso",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04326v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04326v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04216v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04216v1",
                "updated": "2025-05-07T08:10:39Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    8,
                    10,
                    39,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T08:10:39Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    8,
                    10,
                    39,
                    2,
                    127,
                    0
                ],
                "title": "Computational Model for Photoionization in Pure SF6 Streamer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computational Model for Photoionization in Pure SF6 Streamer"
                },
                "summary": "Photoionization plays a crucial role in achieving spatial numerical\nconvergence and accurate quantitative predictions in SF6 streamer simulations,\nbut accurate models for SF6 photoionization remains limited, motivating this\npaper. First, we develop a computational model for SF6 photoionization and\nprovide the detailed modeling process. Then, we perform comparative studies\nagainst simplified approaches. The results demonstrate that the proposed model\neffectively captures the non-local effects of SF6 photoionization, enhancing\nboth the spatial numerical convergence and the accuracy of the streamer\nstructure. Finally, we perform comparative studies by artificially increasing\nthe photoionization intensity through multiplying the photoionization source\nterm Sph by a factor of 10 (10*Sph) relative to the baseline intensity.\nRegarding breakdown voltage prediction, 10*Sph leads to a significant\nunderestimation of the breakdown voltage for positive streamers, introducing\nerrors greater than 0.5 kV, while exerting a relatively small impact on\nnegative streamers. Regarding streamer propagation dynamics, 10*Sph reduces the\ncontraction at the positive streamer head and significantly lowers the local\nfield by more than 700 Td, thereby slowing down its speed. In contrast, 10*Sph\nhas little impact on the morphology of the negative streamers and slightly\nenhances the local field by less than 200 Td, thereby consistently accelerating\nits propagation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Photoionization plays a crucial role in achieving spatial numerical\nconvergence and accurate quantitative predictions in SF6 streamer simulations,\nbut accurate models for SF6 photoionization remains limited, motivating this\npaper. First, we develop a computational model for SF6 photoionization and\nprovide the detailed modeling process. Then, we perform comparative studies\nagainst simplified approaches. The results demonstrate that the proposed model\neffectively captures the non-local effects of SF6 photoionization, enhancing\nboth the spatial numerical convergence and the accuracy of the streamer\nstructure. Finally, we perform comparative studies by artificially increasing\nthe photoionization intensity through multiplying the photoionization source\nterm Sph by a factor of 10 (10*Sph) relative to the baseline intensity.\nRegarding breakdown voltage prediction, 10*Sph leads to a significant\nunderestimation of the breakdown voltage for positive streamers, introducing\nerrors greater than 0.5 kV, while exerting a relatively small impact on\nnegative streamers. Regarding streamer propagation dynamics, 10*Sph reduces the\ncontraction at the positive streamer head and significantly lowers the local\nfield by more than 700 Td, thereby slowing down its speed. In contrast, 10*Sph\nhas little impact on the morphology of the negative streamers and slightly\nenhances the local field by less than 200 Td, thereby consistently accelerating\nits propagation."
                },
                "authors": [
                    {
                        "name": "Zihao Feng"
                    }
                ],
                "author_detail": {
                    "name": "Zihao Feng"
                },
                "author": "Zihao Feng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04216v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04216v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12224v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12224v2",
                "updated": "2025-05-07T07:57:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    7,
                    57,
                    21,
                    2,
                    127,
                    0
                ],
                "published": "2025-02-17T14:54:14Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    14,
                    54,
                    14,
                    0,
                    48,
                    0
                ],
                "title": "Fate: Fast Edge Inference of Mixture-of-Experts Models via Cross-Layer\n  Gate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fate: Fast Edge Inference of Mixture-of-Experts Models via Cross-Layer\n  Gate"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive performance across\nvarious tasks, and their application in edge scenarios has attracted\nsignificant attention. However, sparse-activated Mixture-of-Experts (MoE)\nmodels, which are well suited for edge scenarios, have received relatively\nlittle attention due to their high memory demands. Offload-based methods have\nbeen proposed to address this challenge, but they face difficulties with expert\nprediction. Inaccurate expert predictions can result in prolonged inference\ndelays. To promote the application of MoE models in edge scenarios, we propose\nFate, an offloading system designed for MoE models to enable efficient\ninference in resource-constrained environments. The key insight behind Fate is\nthat gate inputs from adjacent layers can be effectively used for expert\nprefetching, achieving high prediction accuracy without additional GPU\noverhead. Furthermore, Fate employs a shallow-favoring expert caching strategy\nthat increases the expert hit rate to 99\\%. Additionally, Fate integrates\ntailored quantization strategies for cache optimization and IO efficiency.\nExperimental results show that, compared to Load on Demand and Expert\nActivation Path-based method, Fate achieves up to 4.5x and 1.9x speedups in\nprefill speed and up to 4.1x and 2.2x speedups in decoding speed, respectively,\nwhile maintaining inference quality. Moreover, Fate's performance improvements\nare scalable across different memory budgets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive performance across\nvarious tasks, and their application in edge scenarios has attracted\nsignificant attention. However, sparse-activated Mixture-of-Experts (MoE)\nmodels, which are well suited for edge scenarios, have received relatively\nlittle attention due to their high memory demands. Offload-based methods have\nbeen proposed to address this challenge, but they face difficulties with expert\nprediction. Inaccurate expert predictions can result in prolonged inference\ndelays. To promote the application of MoE models in edge scenarios, we propose\nFate, an offloading system designed for MoE models to enable efficient\ninference in resource-constrained environments. The key insight behind Fate is\nthat gate inputs from adjacent layers can be effectively used for expert\nprefetching, achieving high prediction accuracy without additional GPU\noverhead. Furthermore, Fate employs a shallow-favoring expert caching strategy\nthat increases the expert hit rate to 99\\%. Additionally, Fate integrates\ntailored quantization strategies for cache optimization and IO efficiency.\nExperimental results show that, compared to Load on Demand and Expert\nActivation Path-based method, Fate achieves up to 4.5x and 1.9x speedups in\nprefill speed and up to 4.1x and 2.2x speedups in decoding speed, respectively,\nwhile maintaining inference quality. Moreover, Fate's performance improvements\nare scalable across different memory budgets."
                },
                "authors": [
                    {
                        "name": "Zhiyuan Fang"
                    },
                    {
                        "name": "Zicong Hong"
                    },
                    {
                        "name": "Yuegui Huang"
                    },
                    {
                        "name": "Yufeng Lyu"
                    },
                    {
                        "name": "Wuhui Chen"
                    },
                    {
                        "name": "Yue Yu"
                    },
                    {
                        "name": "Fan Yu"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12224v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12224v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04129v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04129v1",
                "updated": "2025-05-07T05:00:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    5,
                    0,
                    10,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T05:00:10Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    5,
                    0,
                    10,
                    2,
                    127,
                    0
                ],
                "title": "Maxing Out the SVM: Performance Impact of Memory and Program Cache Sizes\n  in the Agave Validator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Maxing Out the SVM: Performance Impact of Memory and Program Cache Sizes\n  in the Agave Validator"
                },
                "summary": "In this paper we analyze some of the bottlenecks in the execution pipeline of\nSolana's Agave validator client, focusing on RAM and program cache usage under\nmainnet conditions. Through a series of controlled experiments, we measure the\nvalidator's throughput and resource efficiency as RAM availability ranges\nbetween 128 GB to 1,536 GB (1.5 TB). We discover that the validator performance\ndegrades significantly below 256 GB, with transaction processing falling behind\nreal-time block production. Additionally, we study the program cache behavior,\nidentifying inefficiencies in program eviction and load latency. Our results\nprovide practical guidance for hardware provisioning and suggest improvements\nto the Solana execution and caching strategy, reducing latency due to the\nprogram cache by 90%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper we analyze some of the bottlenecks in the execution pipeline of\nSolana's Agave validator client, focusing on RAM and program cache usage under\nmainnet conditions. Through a series of controlled experiments, we measure the\nvalidator's throughput and resource efficiency as RAM availability ranges\nbetween 128 GB to 1,536 GB (1.5 TB). We discover that the validator performance\ndegrades significantly below 256 GB, with transaction processing falling behind\nreal-time block production. Additionally, we study the program cache behavior,\nidentifying inefficiencies in program eviction and load latency. Our results\nprovide practical guidance for hardware provisioning and suggest improvements\nto the Solana execution and caching strategy, reducing latency due to the\nprogram cache by 90%."
                },
                "authors": [
                    {
                        "name": "Turan Vural"
                    },
                    {
                        "name": "Yuki Yuminaga"
                    },
                    {
                        "name": "Alex Petrosyan"
                    },
                    {
                        "name": "Ben Livshits"
                    }
                ],
                "author_detail": {
                    "name": "Ben Livshits"
                },
                "author": "Ben Livshits",
                "arxiv_comment": "15 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04129v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04129v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12240v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12240v3",
                "updated": "2025-05-06T15:23:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    15,
                    23,
                    12,
                    1,
                    126,
                    0
                ],
                "published": "2025-04-16T16:45:19Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    16,
                    45,
                    19,
                    2,
                    106,
                    0
                ],
                "title": "Cobra: Efficient Line Art COlorization with BRoAder References",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cobra: Efficient Line Art COlorization with BRoAder References"
                },
                "summary": "The comic production industry requires reference-based line art colorization\nwith high accuracy, efficiency, contextual consistency, and flexible control. A\ncomic page often involves diverse characters, objects, and backgrounds, which\ncomplicates the coloring process. Despite advancements in diffusion models for\nimage generation, their application in line art colorization remains limited,\nfacing challenges related to handling extensive reference images,\ntime-consuming inference, and flexible control. We investigate the necessity of\nextensive contextual image guidance on the quality of line art colorization. To\naddress these challenges, we introduce Cobra, an efficient and versatile method\nthat supports color hints and utilizes over 200 reference images while\nmaintaining low latency. Central to Cobra is a Causal Sparse DiT architecture,\nwhich leverages specially designed positional encodings, causal sparse\nattention, and Key-Value Cache to effectively manage long-context references\nand ensure color identity consistency. Results demonstrate that Cobra achieves\naccurate line art colorization through extensive contextual reference,\nsignificantly enhancing inference speed and interactivity, thereby meeting\ncritical industrial demands. We release our codes and models on our project\npage: https://zhuang2002.github.io/Cobra/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The comic production industry requires reference-based line art colorization\nwith high accuracy, efficiency, contextual consistency, and flexible control. A\ncomic page often involves diverse characters, objects, and backgrounds, which\ncomplicates the coloring process. Despite advancements in diffusion models for\nimage generation, their application in line art colorization remains limited,\nfacing challenges related to handling extensive reference images,\ntime-consuming inference, and flexible control. We investigate the necessity of\nextensive contextual image guidance on the quality of line art colorization. To\naddress these challenges, we introduce Cobra, an efficient and versatile method\nthat supports color hints and utilizes over 200 reference images while\nmaintaining low latency. Central to Cobra is a Causal Sparse DiT architecture,\nwhich leverages specially designed positional encodings, causal sparse\nattention, and Key-Value Cache to effectively manage long-context references\nand ensure color identity consistency. Results demonstrate that Cobra achieves\naccurate line art colorization through extensive contextual reference,\nsignificantly enhancing inference speed and interactivity, thereby meeting\ncritical industrial demands. We release our codes and models on our project\npage: https://zhuang2002.github.io/Cobra/."
                },
                "authors": [
                    {
                        "name": "Junhao Zhuang"
                    },
                    {
                        "name": "Lingen Li"
                    },
                    {
                        "name": "Xuan Ju"
                    },
                    {
                        "name": "Zhaoyang Zhang"
                    },
                    {
                        "name": "Chun Yuan"
                    },
                    {
                        "name": "Ying Shan"
                    }
                ],
                "author_detail": {
                    "name": "Ying Shan"
                },
                "author": "Ying Shan",
                "arxiv_comment": "Project page with code: https://zhuang2002.github.io/Cobra/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12240v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12240v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02922v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02922v1",
                "updated": "2025-05-05T18:01:17Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    18,
                    1,
                    17,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T18:01:17Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    18,
                    1,
                    17,
                    0,
                    125,
                    0
                ],
                "title": "RetroInfer: A Vector-Storage Approach for Scalable Long-Context LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RetroInfer: A Vector-Storage Approach for Scalable Long-Context LLM\n  Inference"
                },
                "summary": "The growing context lengths of large language models (LLMs) pose significant\nchallenges for efficient inference, primarily due to GPU memory and bandwidth\nconstraints. We present RetroInfer, a novel system that reconceptualizes the\nkey-value (KV) cache as a vector storage system which exploits the inherent\nattention sparsity to accelerate long-context LLM inference. At its core is the\nwave index, an Attention-aWare VEctor index that enables efficient and accurate\nretrieval of critical tokens through techniques such as tripartite attention\napproximation, accuracy-bounded attention estimation, and segmented clustering.\nComplementing this is the wave buffer, which coordinates KV cache placement and\noverlaps computation and data transfer across GPU and CPU to sustain high\nthroughput. Unlike prior sparsity-based methods that struggle with token\nselection and hardware coordination, RetroInfer delivers robust performance\nwithout compromising model accuracy. Experiments on long-context benchmarks\nshow up to 4.5X speedup over full attention within GPU memory limits and up to\n10.5X over sparse attention baselines when KV cache is extended to CPU memory,\nall while preserving full-attention-level accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing context lengths of large language models (LLMs) pose significant\nchallenges for efficient inference, primarily due to GPU memory and bandwidth\nconstraints. We present RetroInfer, a novel system that reconceptualizes the\nkey-value (KV) cache as a vector storage system which exploits the inherent\nattention sparsity to accelerate long-context LLM inference. At its core is the\nwave index, an Attention-aWare VEctor index that enables efficient and accurate\nretrieval of critical tokens through techniques such as tripartite attention\napproximation, accuracy-bounded attention estimation, and segmented clustering.\nComplementing this is the wave buffer, which coordinates KV cache placement and\noverlaps computation and data transfer across GPU and CPU to sustain high\nthroughput. Unlike prior sparsity-based methods that struggle with token\nselection and hardware coordination, RetroInfer delivers robust performance\nwithout compromising model accuracy. Experiments on long-context benchmarks\nshow up to 4.5X speedup over full attention within GPU memory limits and up to\n10.5X over sparse attention baselines when KV cache is extended to CPU memory,\nall while preserving full-attention-level accuracy."
                },
                "authors": [
                    {
                        "name": "Yaoqi Chen"
                    },
                    {
                        "name": "Jinkai Zhang"
                    },
                    {
                        "name": "Baotong Lu"
                    },
                    {
                        "name": "Qianxi Zhang"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Jingjia Luo"
                    },
                    {
                        "name": "Di Liu"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Qi Chen"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Bailu Ding"
                    },
                    {
                        "name": "Xiao Yan"
                    },
                    {
                        "name": "Jiawei Jiang"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Mingxing Zhang"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Mao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Mao Yang"
                },
                "author": "Mao Yang",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02922v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02922v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02533v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02533v1",
                "updated": "2025-05-05T10:16:16Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    10,
                    16,
                    16,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T10:16:16Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    10,
                    16,
                    16,
                    0,
                    125,
                    0
                ],
                "title": "Large Language Model Partitioning for Low-Latency Inference at the Edge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model Partitioning for Low-Latency Inference at the Edge"
                },
                "summary": "Large Language Models (LLMs) based on autoregressive, decoder-only\nTransformers generate text one token at a time, where a token represents a\ndiscrete unit of text. As each newly produced token is appended to the partial\noutput sequence, the length grows and so does the memory and compute load, due\nto the expanding key-value caches, which store intermediate representations of\nall previously generated tokens in the multi-head attention (MHA) layer. As\nthis iterative process steadily increases memory and compute demands,\nlayer-based partitioning in resource-constrained edge environments often\nresults in memory overload or high inference latency. To address this and\nreduce inference latency, we propose a resource-aware Transformer architecture\npartitioning algorithm, where the partitioning decision is updated at regular\nintervals during token generation. The approach is myopic in that it is based\non instantaneous information about device resource availability and network\nlink bandwidths. When first executed, the algorithm places blocks on devices,\nand in later executions, it migrates these blocks among devices so that the sum\nof migration delay and inference delay remains low. Our approach partitions the\ndecoder at the attention head level, co-locating each attention head with its\nkey-value cache and allowing dynamic migrations whenever resources become\ntight. By allocating different attention heads to different devices, we exploit\nparallel execution of attention heads and thus achieve substantial reductions\nin inference delays. Our experiments show that in small-scale settings (3-5\ndevices), the proposed method achieves within 15 to 20 percent of an exact\noptimal solver's latency, while in larger-scale tests it achieves notable\nimprovements in inference speed and memory usage compared to state-of-the-art\nlayer-based partitioning approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) based on autoregressive, decoder-only\nTransformers generate text one token at a time, where a token represents a\ndiscrete unit of text. As each newly produced token is appended to the partial\noutput sequence, the length grows and so does the memory and compute load, due\nto the expanding key-value caches, which store intermediate representations of\nall previously generated tokens in the multi-head attention (MHA) layer. As\nthis iterative process steadily increases memory and compute demands,\nlayer-based partitioning in resource-constrained edge environments often\nresults in memory overload or high inference latency. To address this and\nreduce inference latency, we propose a resource-aware Transformer architecture\npartitioning algorithm, where the partitioning decision is updated at regular\nintervals during token generation. The approach is myopic in that it is based\non instantaneous information about device resource availability and network\nlink bandwidths. When first executed, the algorithm places blocks on devices,\nand in later executions, it migrates these blocks among devices so that the sum\nof migration delay and inference delay remains low. Our approach partitions the\ndecoder at the attention head level, co-locating each attention head with its\nkey-value cache and allowing dynamic migrations whenever resources become\ntight. By allocating different attention heads to different devices, we exploit\nparallel execution of attention heads and thus achieve substantial reductions\nin inference delays. Our experiments show that in small-scale settings (3-5\ndevices), the proposed method achieves within 15 to 20 percent of an exact\noptimal solver's latency, while in larger-scale tests it achieves notable\nimprovements in inference speed and memory usage compared to state-of-the-art\nlayer-based partitioning approaches."
                },
                "authors": [
                    {
                        "name": "Dimitrios Kafetzis"
                    },
                    {
                        "name": "Ramin Khalili"
                    },
                    {
                        "name": "Iordanis Koutsopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Iordanis Koutsopoulos"
                },
                "author": "Iordanis Koutsopoulos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02533v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02533v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02346v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02346v1",
                "updated": "2025-05-05T04:01:56Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    4,
                    1,
                    56,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T04:01:56Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    4,
                    1,
                    56,
                    0,
                    125,
                    0
                ],
                "title": "An Empirical Study on the Performance and Energy Usage of Compiled\n  Python Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Empirical Study on the Performance and Energy Usage of Compiled\n  Python Code"
                },
                "summary": "Python is a popular programming language known for its ease of learning and\nextensive libraries. However, concerns about performance and energy consumption\nhave led to the development of compilers to enhance Python code efficiency.\nDespite the proven benefits of existing compilers on the efficiency of Python\ncode, there is limited analysis comparing their performance and energy\nefficiency, particularly considering code characteristics and factors like CPU\nfrequency and core count. Our study investigates how compilation impacts the\nperformance and energy consumption of Python code, using seven benchmarks\ncompiled with eight different tools: PyPy, Numba, Nuitka, Mypyc, Codon, Cython,\nPyston-lite, and the experimental Python 3.13 version, compared to CPython. The\nbenchmarks are single-threaded and executed on an NUC and a server, measuring\nenergy usage, execution time, memory usage, and Last-Level Cache (LLC) miss\nrates at a fixed frequency and on a single core. The results show that\ncompilation can significantly enhance execution time, energy and memory usage,\nwith Codon, PyPy, and Numba achieving over 90\\% speed and energy improvements.\nNuitka optimizes memory usage consistently on both testbeds. The impact of\ncompilation on LLC miss rate is not clear since it varies considerably across\nbenchmarks for each compiler. Our study is important for researchers and\npractitioners focused on improving Python code performance and energy\nefficiency. We outline future research directions, such as exploring caching\neffects on energy usage. Our findings help practitioners choose the best\ncompiler based on their efficiency benefits and accessibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Python is a popular programming language known for its ease of learning and\nextensive libraries. However, concerns about performance and energy consumption\nhave led to the development of compilers to enhance Python code efficiency.\nDespite the proven benefits of existing compilers on the efficiency of Python\ncode, there is limited analysis comparing their performance and energy\nefficiency, particularly considering code characteristics and factors like CPU\nfrequency and core count. Our study investigates how compilation impacts the\nperformance and energy consumption of Python code, using seven benchmarks\ncompiled with eight different tools: PyPy, Numba, Nuitka, Mypyc, Codon, Cython,\nPyston-lite, and the experimental Python 3.13 version, compared to CPython. The\nbenchmarks are single-threaded and executed on an NUC and a server, measuring\nenergy usage, execution time, memory usage, and Last-Level Cache (LLC) miss\nrates at a fixed frequency and on a single core. The results show that\ncompilation can significantly enhance execution time, energy and memory usage,\nwith Codon, PyPy, and Numba achieving over 90\\% speed and energy improvements.\nNuitka optimizes memory usage consistently on both testbeds. The impact of\ncompilation on LLC miss rate is not clear since it varies considerably across\nbenchmarks for each compiler. Our study is important for researchers and\npractitioners focused on improving Python code performance and energy\nefficiency. We outline future research directions, such as exploring caching\neffects on energy usage. Our findings help practitioners choose the best\ncompiler based on their efficiency benefits and accessibility."
                },
                "authors": [
                    {
                        "name": "Vincenzo Stoico"
                    },
                    {
                        "name": "Andrei Calin Dragomir"
                    },
                    {
                        "name": "Patricia Lago"
                    }
                ],
                "author_detail": {
                    "name": "Patricia Lago"
                },
                "author": "Patricia Lago",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02346v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02346v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10375v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10375v2",
                "updated": "2025-05-04T09:49:42Z",
                "updated_parsed": [
                    2025,
                    5,
                    4,
                    9,
                    49,
                    42,
                    6,
                    124,
                    0
                ],
                "published": "2024-12-16T07:59:21Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    7,
                    59,
                    21,
                    0,
                    351,
                    0
                ],
                "title": "DAOP: Data-Aware Offloading and Predictive Pre-Calculation for Efficient\n  MoE Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DAOP: Data-Aware Offloading and Predictive Pre-Calculation for Efficient\n  MoE Inference"
                },
                "summary": "Mixture-of-Experts (MoE) models, though highly effective for various machine\nlearning tasks, face significant deployment challenges on memory-constrained\ndevices. While GPUs offer fast inference, their limited memory compared to CPUs\nmeans not all experts can be stored on the GPU simultaneously, necessitating\nfrequent, costly data transfers from CPU memory, often negating GPU speed\nadvantages. To address this, we present DAOP, an on-device MoE inference engine\nto optimize parallel GPU-CPU execution. DAOP dynamically allocates experts\nbetween CPU and GPU based on per-sequence activation patterns, and selectively\npre-calculates predicted experts on CPUs to minimize transfer latency. This\napproach enables efficient resource utilization across various expert cache\nratios while maintaining model accuracy through a novel graceful degradation\nmechanism. Comprehensive evaluations across various datasets show that DAOP\noutperforms traditional expert caching and prefetching methods by up to 8.20x\nand offloading techniques by 1.35x while maintaining accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) models, though highly effective for various machine\nlearning tasks, face significant deployment challenges on memory-constrained\ndevices. While GPUs offer fast inference, their limited memory compared to CPUs\nmeans not all experts can be stored on the GPU simultaneously, necessitating\nfrequent, costly data transfers from CPU memory, often negating GPU speed\nadvantages. To address this, we present DAOP, an on-device MoE inference engine\nto optimize parallel GPU-CPU execution. DAOP dynamically allocates experts\nbetween CPU and GPU based on per-sequence activation patterns, and selectively\npre-calculates predicted experts on CPUs to minimize transfer latency. This\napproach enables efficient resource utilization across various expert cache\nratios while maintaining model accuracy through a novel graceful degradation\nmechanism. Comprehensive evaluations across various datasets show that DAOP\noutperforms traditional expert caching and prefetching methods by up to 8.20x\nand offloading techniques by 1.35x while maintaining accuracy."
                },
                "authors": [
                    {
                        "name": "Yujie Zhang"
                    },
                    {
                        "name": "Shivam Aggarwal"
                    },
                    {
                        "name": "Tulika Mitra"
                    }
                ],
                "author_detail": {
                    "name": "Tulika Mitra"
                },
                "author": "Tulika Mitra",
                "arxiv_comment": "7 pages, 10 figures, Accepted by DATE Conference 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10375v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10375v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02027v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02027v1",
                "updated": "2025-05-04T08:30:00Z",
                "updated_parsed": [
                    2025,
                    5,
                    4,
                    8,
                    30,
                    0,
                    6,
                    124,
                    0
                ],
                "published": "2025-05-04T08:30:00Z",
                "published_parsed": [
                    2025,
                    5,
                    4,
                    8,
                    30,
                    0,
                    6,
                    124,
                    0
                ],
                "title": "GraphPrompter: Multi-stage Adaptive Prompt Optimization for Graph\n  In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GraphPrompter: Multi-stage Adaptive Prompt Optimization for Graph\n  In-Context Learning"
                },
                "summary": "Graph In-Context Learning, with the ability to adapt pre-trained graph models\nto novel and diverse downstream graphs without updating any parameters, has\ngained much attention in the community. The key to graph in-context learning is\nto perform downstream graphs conditioned on chosen prompt examples. Existing\nmethods randomly select subgraphs or edges as prompts, leading to noisy graph\nprompts and inferior model performance. Additionally, due to the gap between\npre-training and testing graphs, when the number of classes in the testing\ngraphs is much greater than that in the training, the in-context learning\nability will also significantly deteriorate. To tackle the aforementioned\nchallenges, we develop a multi-stage adaptive prompt optimization method\nGraphPrompter, which optimizes the entire process of generating, selecting, and\nusing graph prompts for better in-context learning capabilities. Firstly,\nPrompt Generator introduces a reconstruction layer to highlight the most\ninformative edges and reduce irrelevant noise for graph prompt construction.\nFurthermore, in the selection stage, Prompt Selector employs the $k$-nearest\nneighbors algorithm and pre-trained selection layers to dynamically choose\nappropriate samples and minimize the influence of irrelevant prompts. Finally,\nwe leverage a Prompt Augmenter with a cache replacement strategy to enhance the\ngeneralization capability of the pre-trained model on new datasets. Extensive\nexperiments show that GraphPrompter effectively enhances the in-context\nlearning ability of graph models. On average across all the settings, our\napproach surpasses the state-of-the-art baselines by over 8%. Our code is\nreleased at https://github.com/karin0018/GraphPrompter.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph In-Context Learning, with the ability to adapt pre-trained graph models\nto novel and diverse downstream graphs without updating any parameters, has\ngained much attention in the community. The key to graph in-context learning is\nto perform downstream graphs conditioned on chosen prompt examples. Existing\nmethods randomly select subgraphs or edges as prompts, leading to noisy graph\nprompts and inferior model performance. Additionally, due to the gap between\npre-training and testing graphs, when the number of classes in the testing\ngraphs is much greater than that in the training, the in-context learning\nability will also significantly deteriorate. To tackle the aforementioned\nchallenges, we develop a multi-stage adaptive prompt optimization method\nGraphPrompter, which optimizes the entire process of generating, selecting, and\nusing graph prompts for better in-context learning capabilities. Firstly,\nPrompt Generator introduces a reconstruction layer to highlight the most\ninformative edges and reduce irrelevant noise for graph prompt construction.\nFurthermore, in the selection stage, Prompt Selector employs the $k$-nearest\nneighbors algorithm and pre-trained selection layers to dynamically choose\nappropriate samples and minimize the influence of irrelevant prompts. Finally,\nwe leverage a Prompt Augmenter with a cache replacement strategy to enhance the\ngeneralization capability of the pre-trained model on new datasets. Extensive\nexperiments show that GraphPrompter effectively enhances the in-context\nlearning ability of graph models. On average across all the settings, our\napproach surpasses the state-of-the-art baselines by over 8%. Our code is\nreleased at https://github.com/karin0018/GraphPrompter."
                },
                "authors": [
                    {
                        "name": "Rui Lv"
                    },
                    {
                        "name": "Zaixi Zhang"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Qi Liu"
                    },
                    {
                        "name": "Weibo Gao"
                    },
                    {
                        "name": "Jiawei Liu"
                    },
                    {
                        "name": "Jiaxia Yan"
                    },
                    {
                        "name": "Linan Yue"
                    },
                    {
                        "name": "Fangzhou Yao"
                    }
                ],
                "author_detail": {
                    "name": "Fangzhou Yao"
                },
                "author": "Fangzhou Yao",
                "arxiv_comment": "14 pages. IEEE International Conference on Data Engineering\n  (ICDE'2025), accepted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02027v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02027v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07578v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07578v3",
                "updated": "2025-05-03T04:07:07Z",
                "updated_parsed": [
                    2025,
                    5,
                    3,
                    4,
                    7,
                    7,
                    5,
                    123,
                    0
                ],
                "published": "2025-02-11T14:25:20Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    14,
                    25,
                    20,
                    1,
                    42,
                    0
                ],
                "title": "PIM Is All You Need: A CXL-Enabled GPU-Free System for Large Language\n  Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PIM Is All You Need: A CXL-Enabled GPU-Free System for Large Language\n  Model Inference"
                },
                "summary": "Large Language Model (LLM) inference uses an autoregressive manner to\ngenerate one token at a time, which exhibits notably lower operational\nintensity compared to earlier Machine Learning (ML) models such as encoder-only\ntransformers and Convolutional Neural Networks. At the same time, LLMs possess\nlarge parameter sizes and use key-value caches to store context information.\nModern LLMs support context windows with up to 1 million tokens to generate\nversatile text, audio, and video content. A large key-value cache unique to\neach prompt requires a large memory capacity, limiting the inference batch\nsize. Both low operational intensity and limited batch size necessitate a high\nmemory bandwidth. However, contemporary hardware systems for ML model\ndeployment, such as GPUs and TPUs, are primarily optimized for compute\nthroughput. This mismatch challenges the efficient deployment of advanced LLMs\nand makes users pay for expensive compute resources that are poorly utilized\nfor the memory-bound LLM inference tasks.\n  We propose CENT, a CXL-ENabled GPU-Free sysTem for LLM inference, which\nharnesses CXL memory expansion capabilities to accommodate substantial LLM\nsizes, and utilizes near-bank processing units to deliver high memory\nbandwidth, eliminating the need for expensive GPUs. CENT exploits a scalable\nCXL network to support peer-to-peer and collective communication primitives\nacross CXL devices. We implement various parallelism strategies to distribute\nLLMs across these devices. Compared to GPU baselines with maximum supported\nbatch sizes and similar average power, CENT achieves 2.3$\\times$ higher\nthroughput and consumes 2.9$\\times$ less energy. CENT enhances the Total Cost\nof Ownership (TCO), generating 5.2$\\times$ more tokens per dollar than GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference uses an autoregressive manner to\ngenerate one token at a time, which exhibits notably lower operational\nintensity compared to earlier Machine Learning (ML) models such as encoder-only\ntransformers and Convolutional Neural Networks. At the same time, LLMs possess\nlarge parameter sizes and use key-value caches to store context information.\nModern LLMs support context windows with up to 1 million tokens to generate\nversatile text, audio, and video content. A large key-value cache unique to\neach prompt requires a large memory capacity, limiting the inference batch\nsize. Both low operational intensity and limited batch size necessitate a high\nmemory bandwidth. However, contemporary hardware systems for ML model\ndeployment, such as GPUs and TPUs, are primarily optimized for compute\nthroughput. This mismatch challenges the efficient deployment of advanced LLMs\nand makes users pay for expensive compute resources that are poorly utilized\nfor the memory-bound LLM inference tasks.\n  We propose CENT, a CXL-ENabled GPU-Free sysTem for LLM inference, which\nharnesses CXL memory expansion capabilities to accommodate substantial LLM\nsizes, and utilizes near-bank processing units to deliver high memory\nbandwidth, eliminating the need for expensive GPUs. CENT exploits a scalable\nCXL network to support peer-to-peer and collective communication primitives\nacross CXL devices. We implement various parallelism strategies to distribute\nLLMs across these devices. Compared to GPU baselines with maximum supported\nbatch sizes and similar average power, CENT achieves 2.3$\\times$ higher\nthroughput and consumes 2.9$\\times$ less energy. CENT enhances the Total Cost\nof Ownership (TCO), generating 5.2$\\times$ more tokens per dollar than GPUs."
                },
                "authors": [
                    {
                        "name": "Yufeng Gu"
                    },
                    {
                        "name": "Alireza Khadem"
                    },
                    {
                        "name": "Sumanth Umesh"
                    },
                    {
                        "name": "Ning Liang"
                    },
                    {
                        "name": "Xavier Servot"
                    },
                    {
                        "name": "Onur Mutlu"
                    },
                    {
                        "name": "Ravi Iyer"
                    },
                    {
                        "name": "Reetuparna Das"
                    }
                ],
                "author_detail": {
                    "name": "Reetuparna Das"
                },
                "author": "Reetuparna Das",
                "arxiv_doi": "10.1145/3676641.3716267",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3676641.3716267",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.07578v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07578v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "In Proceedings of the 30th ACM International Conference on\n  Architectural Support for Programming Languages and Operating Systems, Volume\n  2 (ASPLOS'25)",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20335v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20335v2",
                "updated": "2025-05-03T01:10:30Z",
                "updated_parsed": [
                    2025,
                    5,
                    3,
                    1,
                    10,
                    30,
                    5,
                    123,
                    0
                ],
                "published": "2025-04-29T00:58:59Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    0,
                    58,
                    59,
                    1,
                    119,
                    0
                ],
                "title": "VA-CDH: A Variance-Aware Method to Optimize Latency for Caching with\n  Delayed Hits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VA-CDH: A Variance-Aware Method to Optimize Latency for Caching with\n  Delayed Hits"
                },
                "summary": "Caches are fundamental to latency-sensitive systems like Content Delivery\nNetworks (CDNs) and Mobile Edge Computing (MEC). However, the delayed hit\nphenomenon where multiple requests for an object occur during its fetch from\nthe remote server after a miss significantly inflates user-perceived latency.\nWhile recent algorithms acknowledge delayed hits by estimating the resulting\naggregate delay, they predominantly focus on its mean value. We identify and\ndemonstrate that such approaches are insufficient, as the real aggregate delay\nfrequently exhibits substantial variance in the true production system, leading\nto suboptimal latency performance when ignored. Thus, we propose VA-CDH, a\nvariance-aware method to optimize latency for caching with delayed hits. It\nemploys a novel ranking function that explicitly incorporates both the\nempirically estimated mean and standard deviation of aggregate delay, allowing\ncaching decisions to account for its variation. We derive the analytical\ndistribution of aggregate delay under Poisson arrivals as a theoretical\ncontribution, offering more statistical insight beyond the mean value. Through\nthe simulations conducted on synthetic and real-world datasets, we show that\nVA-CDH reduces the total latency by 1%-6% approximately compared to\nstate-of-the-art algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caches are fundamental to latency-sensitive systems like Content Delivery\nNetworks (CDNs) and Mobile Edge Computing (MEC). However, the delayed hit\nphenomenon where multiple requests for an object occur during its fetch from\nthe remote server after a miss significantly inflates user-perceived latency.\nWhile recent algorithms acknowledge delayed hits by estimating the resulting\naggregate delay, they predominantly focus on its mean value. We identify and\ndemonstrate that such approaches are insufficient, as the real aggregate delay\nfrequently exhibits substantial variance in the true production system, leading\nto suboptimal latency performance when ignored. Thus, we propose VA-CDH, a\nvariance-aware method to optimize latency for caching with delayed hits. It\nemploys a novel ranking function that explicitly incorporates both the\nempirically estimated mean and standard deviation of aggregate delay, allowing\ncaching decisions to account for its variation. We derive the analytical\ndistribution of aggregate delay under Poisson arrivals as a theoretical\ncontribution, offering more statistical insight beyond the mean value. Through\nthe simulations conducted on synthetic and real-world datasets, we show that\nVA-CDH reduces the total latency by 1%-6% approximately compared to\nstate-of-the-art algorithms."
                },
                "authors": [
                    {
                        "name": "Bowen Jiang"
                    },
                    {
                        "name": "Chaofan Ma"
                    },
                    {
                        "name": "Duo Wang"
                    }
                ],
                "author_detail": {
                    "name": "Duo Wang"
                },
                "author": "Duo Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20335v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20335v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13298v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13298v3",
                "updated": "2025-05-02T13:55:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    13,
                    55,
                    21,
                    4,
                    122,
                    0
                ],
                "published": "2025-01-23T00:57:01Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    0,
                    57,
                    1,
                    3,
                    23,
                    0
                ],
                "title": "Collaborative Coded Caching for Partially Connected Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collaborative Coded Caching for Partially Connected Networks"
                },
                "summary": "Coded caching leverages the differences in user cache memories to achieve\ngains that scale with the total cache size, alleviating network congestion due\nto high-quality content requests. Additionally, distributing transmitters over\na wide area can mitigate the adverse effects of path loss. In this work, we\nconsider a partially connected network where the channel between distributed\ntransmitters (helpers) and users is modeled as a distributed\nmultiple-input-multiple-output (MIMO) Gaussian broadcast channel. We propose a\nnovel delivery scheme consisting of two phases: partitioning and transmission.\nIn the partitioning phase, users with identical cache profiles are partitioned\ninto the minimum number of sets, such that users within each set can\nsuccessfully decode their desired message from a joint transmission enabled by\nMIMO precoding. To optimally partition the users, we employ the branch and\nbound method. In the transmission phase, each partition is treated as a single\nentity, and codewords are multicast to partitions with distinct cache profiles.\nThe proposed delivery scheme is applicable to any partially connected network,\nand while the partitioning is optimal, the overall delivery scheme, including\ntransmission, is heuristic. Interestingly, simulation results show that its\nperformance closely approximates that of the fully connected optimal solution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coded caching leverages the differences in user cache memories to achieve\ngains that scale with the total cache size, alleviating network congestion due\nto high-quality content requests. Additionally, distributing transmitters over\na wide area can mitigate the adverse effects of path loss. In this work, we\nconsider a partially connected network where the channel between distributed\ntransmitters (helpers) and users is modeled as a distributed\nmultiple-input-multiple-output (MIMO) Gaussian broadcast channel. We propose a\nnovel delivery scheme consisting of two phases: partitioning and transmission.\nIn the partitioning phase, users with identical cache profiles are partitioned\ninto the minimum number of sets, such that users within each set can\nsuccessfully decode their desired message from a joint transmission enabled by\nMIMO precoding. To optimally partition the users, we employ the branch and\nbound method. In the transmission phase, each partition is treated as a single\nentity, and codewords are multicast to partitions with distinct cache profiles.\nThe proposed delivery scheme is applicable to any partially connected network,\nand while the partitioning is optimal, the overall delivery scheme, including\ntransmission, is heuristic. Interestingly, simulation results show that its\nperformance closely approximates that of the fully connected optimal solution."
                },
                "authors": [
                    {
                        "name": "Kagan Akcay"
                    },
                    {
                        "name": "Eleftherios Lampiris"
                    },
                    {
                        "name": "MohammadJavad Salehi"
                    },
                    {
                        "name": "Giuseppe Caire"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Caire"
                },
                "author": "Giuseppe Caire",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13298v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13298v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01164v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01164v1",
                "updated": "2025-05-02T10:13:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    10,
                    13,
                    12,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T10:13:12Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    10,
                    13,
                    12,
                    4,
                    122,
                    0
                ],
                "title": "CaGR-RAG: Context-aware Query Grouping for Disk-based Vector Search in\n  RAG Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CaGR-RAG: Context-aware Query Grouping for Disk-based Vector Search in\n  RAG Systems"
                },
                "summary": "Modern embedding models capture both semantic and syntactic structures of\nqueries, often mapping different queries to similar regions in vector space.\nThis results in non-uniform cluster access patterns in disk-based vector search\nsystems, particularly in Retrieval Augmented Generation (RAG) framework. While\nexisting approaches optimize individual queries, they overlook the impact of\ncluster access patterns, failing to account for the locality effects of queries\nthat access similar clusters. This oversight reduces cache efficiency and\nincreases search latency due to excessive disk I/O. To address this, we\nintroduce CaGR-RAG, a context-aware query grouping mechanism that organizes\nqueries based on shared cluster access patterns. Additionally, it incorporates\nopportunistic cluster prefetching to minimize cache misses during transitions\nbetween query groups, further optimizing retrieval performance. Experimental\nresults show that CaGR-RAG reduces 99th percentile tail latency by up to 51.55%\nwhile consistently maintaining a higher cache hit ratio than the baseline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern embedding models capture both semantic and syntactic structures of\nqueries, often mapping different queries to similar regions in vector space.\nThis results in non-uniform cluster access patterns in disk-based vector search\nsystems, particularly in Retrieval Augmented Generation (RAG) framework. While\nexisting approaches optimize individual queries, they overlook the impact of\ncluster access patterns, failing to account for the locality effects of queries\nthat access similar clusters. This oversight reduces cache efficiency and\nincreases search latency due to excessive disk I/O. To address this, we\nintroduce CaGR-RAG, a context-aware query grouping mechanism that organizes\nqueries based on shared cluster access patterns. Additionally, it incorporates\nopportunistic cluster prefetching to minimize cache misses during transitions\nbetween query groups, further optimizing retrieval performance. Experimental\nresults show that CaGR-RAG reduces 99th percentile tail latency by up to 51.55%\nwhile consistently maintaining a higher cache hit ratio than the baseline."
                },
                "authors": [
                    {
                        "name": "Yeonwoo Jeong"
                    },
                    {
                        "name": "Kyuli Park"
                    },
                    {
                        "name": "Hyunji Cho"
                    },
                    {
                        "name": "Sungyong Park"
                    }
                ],
                "author_detail": {
                    "name": "Sungyong Park"
                },
                "author": "Sungyong Park",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01164v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01164v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01002v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01002v1",
                "updated": "2025-05-02T04:57:06Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    4,
                    57,
                    6,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T04:57:06Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    4,
                    57,
                    6,
                    4,
                    122,
                    0
                ],
                "title": "High Voltage Delivery and Distribution for the NEXT-100 Time Projection\n  Chamber",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High Voltage Delivery and Distribution for the NEXT-100 Time Projection\n  Chamber"
                },
                "summary": "A critical element in the realization of large liquid and gas time projection\nchambers (TPCs) is the delivery and distribution of high voltages into and\naround the detector. Such experiments require of order tens of kilovolts to\nenable electron drift over meter-scale distances. This paper describes the\ndesign and operation of the cathode feedthrough and high voltage distribution\nthrough the field cage of the NEXT-100 experiment, an underground TPC that will\nsearch for neutrinoless double beta decay $0\\nu\\beta\\beta$. The feedthrough has\nbeen demonstrated to hold pressures up to 20~bar and sustain voltages as high\nas -65~kV, and the TPC is operating stably at its design high voltages. The\nsystem has been realized within the constraints of a stringent radiopurity\nbudget and is now being used to execute a suite of sensitive double beta decay\nanalyses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A critical element in the realization of large liquid and gas time projection\nchambers (TPCs) is the delivery and distribution of high voltages into and\naround the detector. Such experiments require of order tens of kilovolts to\nenable electron drift over meter-scale distances. This paper describes the\ndesign and operation of the cathode feedthrough and high voltage distribution\nthrough the field cage of the NEXT-100 experiment, an underground TPC that will\nsearch for neutrinoless double beta decay $0\\nu\\beta\\beta$. The feedthrough has\nbeen demonstrated to hold pressures up to 20~bar and sustain voltages as high\nas -65~kV, and the TPC is operating stably at its design high voltages. The\nsystem has been realized within the constraints of a stringent radiopurity\nbudget and is now being used to execute a suite of sensitive double beta decay\nanalyses."
                },
                "authors": [
                    {
                        "name": "NEXT Collaboration"
                    },
                    {
                        "name": "C. Adams"
                    },
                    {
                        "name": "H. Almazán"
                    },
                    {
                        "name": "V. Álvarez"
                    },
                    {
                        "name": "K. Bailey"
                    },
                    {
                        "name": "R. Guenette"
                    },
                    {
                        "name": "B. J. P. Jones"
                    },
                    {
                        "name": "S. Johnston"
                    },
                    {
                        "name": "K. Mistry"
                    },
                    {
                        "name": "F. Monrabal"
                    },
                    {
                        "name": "D. R. Nygren"
                    },
                    {
                        "name": "B. Palmeiro"
                    },
                    {
                        "name": "L. Rogers"
                    },
                    {
                        "name": "J. Waldschmidt"
                    },
                    {
                        "name": "B. Aparicio"
                    },
                    {
                        "name": "A. I. Aranburu"
                    },
                    {
                        "name": "L. Arazi"
                    },
                    {
                        "name": "I. J. Arnquist"
                    },
                    {
                        "name": "F. Auria-Luna"
                    },
                    {
                        "name": "S. Ayet"
                    },
                    {
                        "name": "C. D. R. Azevedo"
                    },
                    {
                        "name": "F. Ballester"
                    },
                    {
                        "name": "M. del Barrio-Torregrosa"
                    },
                    {
                        "name": "A. Bayo"
                    },
                    {
                        "name": "J. M. Benlloch-Rodríguez"
                    },
                    {
                        "name": "F. I. G. M. Borges"
                    },
                    {
                        "name": "A. Brodolin"
                    },
                    {
                        "name": "S. Cárcel"
                    },
                    {
                        "name": "A. Castillo"
                    },
                    {
                        "name": "L. Cid"
                    },
                    {
                        "name": "C. A. N. Conde"
                    },
                    {
                        "name": "T. Contreras"
                    },
                    {
                        "name": "F. P. Cossío"
                    },
                    {
                        "name": "R. Coupe"
                    },
                    {
                        "name": "E. Dey"
                    },
                    {
                        "name": "G. Díaz"
                    },
                    {
                        "name": "C. Echevarria"
                    },
                    {
                        "name": "M. Elorza"
                    },
                    {
                        "name": "J. Escada"
                    },
                    {
                        "name": "R. Esteve"
                    },
                    {
                        "name": "R. Felkai"
                    },
                    {
                        "name": "L. M. P. Fernandes"
                    },
                    {
                        "name": "P. Ferrario"
                    },
                    {
                        "name": "A. L. Ferreira"
                    },
                    {
                        "name": "F. W. Foss"
                    },
                    {
                        "name": "Z. Freixa"
                    },
                    {
                        "name": "J. García-Barrena"
                    },
                    {
                        "name": "J. J. Gómez-Cadenas"
                    },
                    {
                        "name": "J. W. R. Grocott"
                    },
                    {
                        "name": "R. Guenette"
                    },
                    {
                        "name": "J. Hauptman"
                    },
                    {
                        "name": "C. A. O. Henriques"
                    },
                    {
                        "name": "J. A. Hernando Morata"
                    },
                    {
                        "name": "P. Herrero-Gómez"
                    },
                    {
                        "name": "V. Herrero"
                    },
                    {
                        "name": "C. Hervés Carrete"
                    },
                    {
                        "name": "Y. Ifergan"
                    },
                    {
                        "name": "F. Kellerer"
                    },
                    {
                        "name": "L. Larizgoitia"
                    },
                    {
                        "name": "A. Larumbe"
                    },
                    {
                        "name": "P. Lebrun"
                    },
                    {
                        "name": "F. Lopez"
                    },
                    {
                        "name": "N. López-March"
                    },
                    {
                        "name": "R. Madigan"
                    },
                    {
                        "name": "R. D. P. Mano"
                    },
                    {
                        "name": "A. P. Marques"
                    },
                    {
                        "name": "J. Martín-Albo"
                    },
                    {
                        "name": "G. Martínez-Lema"
                    },
                    {
                        "name": "M. Martínez-Vara"
                    },
                    {
                        "name": "R. L. Miller"
                    },
                    {
                        "name": "J. Molina-Canteras"
                    },
                    {
                        "name": "F. Monrabal"
                    },
                    {
                        "name": "C. M. B. Monteiro"
                    },
                    {
                        "name": "F. J. Mora"
                    },
                    {
                        "name": "P. Novella"
                    },
                    {
                        "name": "A. Nuñez"
                    },
                    {
                        "name": "E. Oblak"
                    },
                    {
                        "name": "J. Palacio"
                    },
                    {
                        "name": "B. Palmeiro"
                    },
                    {
                        "name": "A. Para"
                    },
                    {
                        "name": "A. Pazos"
                    },
                    {
                        "name": "J. Pelegrin"
                    },
                    {
                        "name": "M. Pérez Maneiro"
                    },
                    {
                        "name": "M. Querol"
                    },
                    {
                        "name": "J. Renner"
                    },
                    {
                        "name": "I. Rivilla"
                    },
                    {
                        "name": "C. Rogero"
                    },
                    {
                        "name": "B. Romeo"
                    },
                    {
                        "name": "C. Romo-Luque"
                    },
                    {
                        "name": "V. San Nacienciano"
                    },
                    {
                        "name": "F. P. Santos"
                    },
                    {
                        "name": "J. M. F. dos Santos"
                    },
                    {
                        "name": "M. Seemann"
                    },
                    {
                        "name": "I. Shomroni"
                    },
                    {
                        "name": "P. A. O. C. Silva"
                    },
                    {
                        "name": "A. Simón"
                    },
                    {
                        "name": "S. R. Soleti"
                    },
                    {
                        "name": "M. Sorel"
                    },
                    {
                        "name": "J. Soto-Oton"
                    },
                    {
                        "name": "J. M. R. Teixeira"
                    },
                    {
                        "name": "S. Teruel-Pardo"
                    },
                    {
                        "name": "J. F. Toledo"
                    },
                    {
                        "name": "C. Tonnelé"
                    },
                    {
                        "name": "S. Torelli"
                    },
                    {
                        "name": "J. Torrent"
                    },
                    {
                        "name": "A. Trettin"
                    },
                    {
                        "name": "A. Usón"
                    },
                    {
                        "name": "P. R. G. Valle"
                    },
                    {
                        "name": "J. F. C. A. Veloso"
                    },
                    {
                        "name": "J. Waiton"
                    },
                    {
                        "name": "A. Yubero-Navarro"
                    }
                ],
                "author_detail": {
                    "name": "A. Yubero-Navarro"
                },
                "author": "A. Yubero-Navarro",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01002v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01002v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00962v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00962v1",
                "updated": "2025-05-02T02:36:23Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    2,
                    36,
                    23,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T02:36:23Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    2,
                    36,
                    23,
                    4,
                    122,
                    0
                ],
                "title": "The Open-Source BlackParrot-BedRock Cache Coherence System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Open-Source BlackParrot-BedRock Cache Coherence System"
                },
                "summary": "This dissertation revisits the topic of programmable cache coherence engines\nin the context of modern shared-memory multicore processors. First, the\nopen-source BedRock cache coherence protocol is described. BedRock employs the\ncanonical MOESIF coherence states and reduces implementation burden by\neliminating transient coherence states from the protocol. The protocol's design\ncomplexity, concurrency, and verification effort are analyzed and compared to a\ncanonical directory-based invalidate coherence protocol. Second, the\narchitecture and microarchitecture of three separate cache coherence\ndirectories implementing the BedRock protocol within the BlackParrot 64-bit\nRISC-V multicore processor, collectively called BlackParrot-BedRock\n(BP-BedRock), are described. A fixed-function coherence directory engine\nimplementation provides a baseline design for performance and area comparisons.\nA microcode-programmable coherence directory implementation demonstrates the\nfeasibility of implementing a programmable coherence engine capable of\nmaintaining sufficient protocol processing performance. A hybrid fixed-function\nand programmable coherence directory blends the protocol processing performance\nof the fixed-function design with the programmable flexibility of the\nmicrocode-programmable design. Collectively, the BedRock coherence protocol and\nits three BP-BedRock implementations demonstrate the feasibility and challenges\nof including programmable logic within the coherence system of modern\nshared-memory multicore processors, paving the way for future research into the\napplication- and system-level benefits of programmable coherence engines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This dissertation revisits the topic of programmable cache coherence engines\nin the context of modern shared-memory multicore processors. First, the\nopen-source BedRock cache coherence protocol is described. BedRock employs the\ncanonical MOESIF coherence states and reduces implementation burden by\neliminating transient coherence states from the protocol. The protocol's design\ncomplexity, concurrency, and verification effort are analyzed and compared to a\ncanonical directory-based invalidate coherence protocol. Second, the\narchitecture and microarchitecture of three separate cache coherence\ndirectories implementing the BedRock protocol within the BlackParrot 64-bit\nRISC-V multicore processor, collectively called BlackParrot-BedRock\n(BP-BedRock), are described. A fixed-function coherence directory engine\nimplementation provides a baseline design for performance and area comparisons.\nA microcode-programmable coherence directory implementation demonstrates the\nfeasibility of implementing a programmable coherence engine capable of\nmaintaining sufficient protocol processing performance. A hybrid fixed-function\nand programmable coherence directory blends the protocol processing performance\nof the fixed-function design with the programmable flexibility of the\nmicrocode-programmable design. Collectively, the BedRock coherence protocol and\nits three BP-BedRock implementations demonstrate the feasibility and challenges\nof including programmable logic within the coherence system of modern\nshared-memory multicore processors, paving the way for future research into the\napplication- and system-level benefits of programmable coherence engines."
                },
                "authors": [
                    {
                        "name": "Mark Unruh Wyse"
                    }
                ],
                "author_detail": {
                    "name": "Mark Unruh Wyse"
                },
                "author": "Mark Unruh Wyse",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00962v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00962v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00901v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00901v1",
                "updated": "2025-05-01T22:32:29Z",
                "updated_parsed": [
                    2025,
                    5,
                    1,
                    22,
                    32,
                    29,
                    3,
                    121,
                    0
                ],
                "published": "2025-05-01T22:32:29Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    22,
                    32,
                    29,
                    3,
                    121,
                    0
                ],
                "title": "Heterogeneous Memory Benchmarking Toolkit",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heterogeneous Memory Benchmarking Toolkit"
                },
                "summary": "This paper presents an open-source kernel-level heterogeneous memory\ncharacterization framework (MemScope) for embedded systems that enables users\nto understand and precisely characterize the temporal behavior of all available\nmemory modules under configurable contention stress scenarios. Since\nkernel-level provides a high degree of control over allocation, cache\nmaintenance, $CPUs$, interrupts, and I/O device activity, seeking the most\naccurate way to benchmark heterogeneous memory subsystems, would be achieved by\nimplementing it in the kernel. This gives us the privilege to directly map\npieces of contiguous physical memory and instantiate allocators, allowing us to\nfinely control cores to create and eliminate interference. Additionally, we can\nminimize noise and interruptions, guaranteeing more consistent and precise\nresults compared to equivalent user-space solutions. Running our Framework on a\nXilinx Zynq UltraScale+ ZCU102 CPU_FPGA platform, demonstrates its capability\nto precisely benchmark bandwidth and latency across various memory types,\nincluding PL-side DRAM and BRAM, in a multi-core system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents an open-source kernel-level heterogeneous memory\ncharacterization framework (MemScope) for embedded systems that enables users\nto understand and precisely characterize the temporal behavior of all available\nmemory modules under configurable contention stress scenarios. Since\nkernel-level provides a high degree of control over allocation, cache\nmaintenance, $CPUs$, interrupts, and I/O device activity, seeking the most\naccurate way to benchmark heterogeneous memory subsystems, would be achieved by\nimplementing it in the kernel. This gives us the privilege to directly map\npieces of contiguous physical memory and instantiate allocators, allowing us to\nfinely control cores to create and eliminate interference. Additionally, we can\nminimize noise and interruptions, guaranteeing more consistent and precise\nresults compared to equivalent user-space solutions. Running our Framework on a\nXilinx Zynq UltraScale+ ZCU102 CPU_FPGA platform, demonstrates its capability\nto precisely benchmark bandwidth and latency across various memory types,\nincluding PL-side DRAM and BRAM, in a multi-core system."
                },
                "authors": [
                    {
                        "name": "Golsana Ghaemi"
                    },
                    {
                        "name": "Kazem Taram"
                    },
                    {
                        "name": "Renato Mancuso"
                    }
                ],
                "author_detail": {
                    "name": "Renato Mancuso"
                },
                "author": "Renato Mancuso",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00901v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00901v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00817v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00817v1",
                "updated": "2025-05-01T19:18:56Z",
                "updated_parsed": [
                    2025,
                    5,
                    1,
                    19,
                    18,
                    56,
                    3,
                    121,
                    0
                ],
                "published": "2025-05-01T19:18:56Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    19,
                    18,
                    56,
                    3,
                    121,
                    0
                ],
                "title": "Spill The Beans: Exploiting CPU Cache Side-Channels to Leak Tokens from\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spill The Beans: Exploiting CPU Cache Side-Channels to Leak Tokens from\n  Large Language Models"
                },
                "summary": "Side-channel attacks on shared hardware resources increasingly threaten\nconfidentiality, especially with the rise of Large Language Models (LLMs). In\nthis work, we introduce Spill The Beans, a novel application of cache\nside-channels to leak tokens generated by an LLM. By co-locating an attack\nprocess on the same hardware as the victim model, we flush and reload embedding\nvectors from the embedding layer, where each token corresponds to a unique\nembedding vector. When accessed during token generation, it results in a cache\nhit detectable by our attack on shared lower-level caches.\n  A significant challenge is the massive size of LLMs, which, by nature of\ntheir compute intensive operation, quickly evicts embedding vectors from the\ncache. We address this by balancing the number of tokens monitored against the\namount of information leaked. Monitoring more tokens increases potential\nvocabulary leakage but raises the chance of missing cache hits due to eviction;\nmonitoring fewer tokens improves detection reliability but limits vocabulary\ncoverage.\n  Through extensive experimentation, we demonstrate the feasibility of leaking\ntokens from LLMs via cache side-channels. Our findings reveal a new\nvulnerability in LLM deployments, highlighting that even sophisticated models\nare susceptible to traditional side-channel attacks. We discuss the\nimplications for privacy and security in LLM-serving infrastructures and\nsuggest considerations for mitigating such threats. For proof of concept we\nconsider two concrete attack scenarios: Our experiments show that an attacker\ncan recover as much as 80%-90% of a high entropy API key with single shot\nmonitoring. As for English text we can reach a 40% recovery rate with a single\nshot. We should note that the rate highly depends on the monitored token set\nand these rates can be improved by targeting more specialized output domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Side-channel attacks on shared hardware resources increasingly threaten\nconfidentiality, especially with the rise of Large Language Models (LLMs). In\nthis work, we introduce Spill The Beans, a novel application of cache\nside-channels to leak tokens generated by an LLM. By co-locating an attack\nprocess on the same hardware as the victim model, we flush and reload embedding\nvectors from the embedding layer, where each token corresponds to a unique\nembedding vector. When accessed during token generation, it results in a cache\nhit detectable by our attack on shared lower-level caches.\n  A significant challenge is the massive size of LLMs, which, by nature of\ntheir compute intensive operation, quickly evicts embedding vectors from the\ncache. We address this by balancing the number of tokens monitored against the\namount of information leaked. Monitoring more tokens increases potential\nvocabulary leakage but raises the chance of missing cache hits due to eviction;\nmonitoring fewer tokens improves detection reliability but limits vocabulary\ncoverage.\n  Through extensive experimentation, we demonstrate the feasibility of leaking\ntokens from LLMs via cache side-channels. Our findings reveal a new\nvulnerability in LLM deployments, highlighting that even sophisticated models\nare susceptible to traditional side-channel attacks. We discuss the\nimplications for privacy and security in LLM-serving infrastructures and\nsuggest considerations for mitigating such threats. For proof of concept we\nconsider two concrete attack scenarios: Our experiments show that an attacker\ncan recover as much as 80%-90% of a high entropy API key with single shot\nmonitoring. As for English text we can reach a 40% recovery rate with a single\nshot. We should note that the rate highly depends on the monitored token set\nand these rates can be improved by targeting more specialized output domains."
                },
                "authors": [
                    {
                        "name": "Andrew Adiletta"
                    },
                    {
                        "name": "Berk Sunar"
                    }
                ],
                "author_detail": {
                    "name": "Berk Sunar"
                },
                "author": "Berk Sunar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00817v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00817v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "K.6.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2505.14687v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14687v1",
                "updated": "2025-05-20T17:59:59Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    17,
                    59,
                    59,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T17:59:59Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    17,
                    59,
                    59,
                    1,
                    140,
                    0
                ],
                "title": "Grouping First, Attending Smartly: Training-Free Acceleration for\n  Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Grouping First, Attending Smartly: Training-Free Acceleration for\n  Diffusion Transformers"
                },
                "summary": "Diffusion-based Transformers have demonstrated impressive generative\ncapabilities, but their high computational costs hinder practical deployment,\nfor example, generating an $8192\\times 8192$ image can take over an hour on an\nA100 GPU. In this work, we propose GRAT (\\textbf{GR}ouping first,\n\\textbf{AT}tending smartly), a training-free attention acceleration strategy\nfor fast image and video generation without compromising output quality. The\nkey insight is to exploit the inherent sparsity in learned attention maps\n(which tend to be locally focused) in pretrained Diffusion Transformers and\nleverage better GPU parallelism. Specifically, GRAT first partitions contiguous\ntokens into non-overlapping groups, aligning both with GPU execution patterns\nand the local attention structures learned in pretrained generative\nTransformers. It then accelerates attention by having all query tokens within\nthe same group share a common set of attendable key and value tokens. These key\nand value tokens are further restricted to structured regions, such as\nsurrounding blocks or criss-cross regions, significantly reducing computational\noverhead (e.g., attaining a \\textbf{35.8$\\times$} speedup over full attention\nwhen generating $8192\\times 8192$ images) while preserving essential attention\npatterns and long-range context. We validate GRAT on pretrained Flux and\nHunyuanVideo for image and video generation, respectively. In both cases, GRAT\nachieves substantially faster inference without any fine-tuning, while\nmaintaining the performance of full attention. We hope GRAT will inspire future\nresearch on accelerating Diffusion Transformers for scalable visual generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based Transformers have demonstrated impressive generative\ncapabilities, but their high computational costs hinder practical deployment,\nfor example, generating an $8192\\times 8192$ image can take over an hour on an\nA100 GPU. In this work, we propose GRAT (\\textbf{GR}ouping first,\n\\textbf{AT}tending smartly), a training-free attention acceleration strategy\nfor fast image and video generation without compromising output quality. The\nkey insight is to exploit the inherent sparsity in learned attention maps\n(which tend to be locally focused) in pretrained Diffusion Transformers and\nleverage better GPU parallelism. Specifically, GRAT first partitions contiguous\ntokens into non-overlapping groups, aligning both with GPU execution patterns\nand the local attention structures learned in pretrained generative\nTransformers. It then accelerates attention by having all query tokens within\nthe same group share a common set of attendable key and value tokens. These key\nand value tokens are further restricted to structured regions, such as\nsurrounding blocks or criss-cross regions, significantly reducing computational\noverhead (e.g., attaining a \\textbf{35.8$\\times$} speedup over full attention\nwhen generating $8192\\times 8192$ images) while preserving essential attention\npatterns and long-range context. We validate GRAT on pretrained Flux and\nHunyuanVideo for image and video generation, respectively. In both cases, GRAT\nachieves substantially faster inference without any fine-tuning, while\nmaintaining the performance of full attention. We hope GRAT will inspire future\nresearch on accelerating Diffusion Transformers for scalable visual generation."
                },
                "authors": [
                    {
                        "name": "Sucheng Ren"
                    },
                    {
                        "name": "Qihang Yu"
                    },
                    {
                        "name": "Ju He"
                    },
                    {
                        "name": "Alan Yuille"
                    },
                    {
                        "name": "Liang-Chieh Chen"
                    }
                ],
                "author_detail": {
                    "name": "Liang-Chieh Chen"
                },
                "author": "Liang-Chieh Chen",
                "arxiv_comment": "Project website at oliverrensu.github.io/project/GRAT",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14687v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14687v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14684v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14684v1",
                "updated": "2025-05-20T17:59:31Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    17,
                    59,
                    31,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T17:59:31Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    17,
                    59,
                    31,
                    1,
                    140,
                    0
                ],
                "title": "Mind the Gap: Bridging Thought Leap for Improved Chain-of-Thought Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mind the Gap: Bridging Thought Leap for Improved Chain-of-Thought Tuning"
                },
                "summary": "Large language models (LLMs) have achieved remarkable progress on\nmathemati-cal tasks through Chain-of-Thought (CoT) reasoning. However, existing\nmathematical CoT datasets often suffer from Thought Leaps due to experts\nomitting intermediate steps, which negatively impacts model learning and\ngeneralization. We propose the CoT Thought Leap Bridge Task, which aims to\nautomatically detect leaps and generate missing intermediate reasoning steps to\nrestore the completeness and coherence of CoT. To facilitate this, we\nconstructed a specialized training dataset called ScaleQM+, based on the\nstructured ScaleQuestMath dataset, and trained CoT-Bridge to bridge thought\nleaps. Through comprehensive experiments on mathematical reasoning benchmarks,\nwe demonstrate that models fine-tuned on bridged datasets consistently\noutperform those trained on original datasets, with improvements of up to\n+5.87% on NuminaMath. Our approach effectively enhances distilled data (+3.02%)\nand provides better starting points for reinforcement learning (+3.1%),\nfunctioning as a plug-and-play module compatible with existing optimization\ntechniques. Furthermore, CoT-Bridge demonstrate improved generalization to\nout-of-domain logical reasoning tasks, confirming that enhancing reasoning\ncompleteness yields broadly applicable benefits.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved remarkable progress on\nmathemati-cal tasks through Chain-of-Thought (CoT) reasoning. However, existing\nmathematical CoT datasets often suffer from Thought Leaps due to experts\nomitting intermediate steps, which negatively impacts model learning and\ngeneralization. We propose the CoT Thought Leap Bridge Task, which aims to\nautomatically detect leaps and generate missing intermediate reasoning steps to\nrestore the completeness and coherence of CoT. To facilitate this, we\nconstructed a specialized training dataset called ScaleQM+, based on the\nstructured ScaleQuestMath dataset, and trained CoT-Bridge to bridge thought\nleaps. Through comprehensive experiments on mathematical reasoning benchmarks,\nwe demonstrate that models fine-tuned on bridged datasets consistently\noutperform those trained on original datasets, with improvements of up to\n+5.87% on NuminaMath. Our approach effectively enhances distilled data (+3.02%)\nand provides better starting points for reinforcement learning (+3.1%),\nfunctioning as a plug-and-play module compatible with existing optimization\ntechniques. Furthermore, CoT-Bridge demonstrate improved generalization to\nout-of-domain logical reasoning tasks, confirming that enhancing reasoning\ncompleteness yields broadly applicable benefits."
                },
                "authors": [
                    {
                        "name": "Haolei Xu"
                    },
                    {
                        "name": "Yuchen Yan"
                    },
                    {
                        "name": "Yongliang Shen"
                    },
                    {
                        "name": "Wenqi Zhang"
                    },
                    {
                        "name": "Guiyang Hou"
                    },
                    {
                        "name": "Shengpei Jiang"
                    },
                    {
                        "name": "Kaitao Song"
                    },
                    {
                        "name": "Weiming Lu"
                    },
                    {
                        "name": "Jun Xiao"
                    },
                    {
                        "name": "Yueting Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Yueting Zhuang"
                },
                "author": "Yueting Zhuang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14684v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14684v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14681v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14681v1",
                "updated": "2025-05-20T17:59:16Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    17,
                    59,
                    16,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T17:59:16Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    17,
                    59,
                    16,
                    1,
                    140,
                    0
                ],
                "title": "Two Experts Are All You Need for Steering Thinking: Reinforcing\n  Cognitive Effort in MoE Reasoning Models Without Additional Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Two Experts Are All You Need for Steering Thinking: Reinforcing\n  Cognitive Effort in MoE Reasoning Models Without Additional Training"
                },
                "summary": "Mixture-of-Experts (MoE) architectures within Large Reasoning Models (LRMs)\nhave achieved impressive reasoning capabilities by selectively activating\nexperts to facilitate structured cognitive processes. Despite notable advances,\nexisting reasoning models often suffer from cognitive inefficiencies like\noverthinking and underthinking. To address these limitations, we introduce a\nnovel inference-time steering methodology called Reinforcing Cognitive Experts\n(RICE), designed to improve reasoning performance without additional training\nor complex heuristics. Leveraging normalized Pointwise Mutual Information\n(nPMI), we systematically identify specialized experts, termed ''cognitive\nexperts'' that orchestrate meta-level reasoning operations characterized by\ntokens like ''<think>''. Empirical evaluations with leading MoE-based LRMs\n(DeepSeek-R1 and Qwen3-235B) on rigorous quantitative and scientific reasoning\nbenchmarks demonstrate noticeable and consistent improvements in reasoning\naccuracy, cognitive efficiency, and cross-domain generalization. Crucially, our\nlightweight approach substantially outperforms prevalent reasoning-steering\ntechniques, such as prompt design and decoding constraints, while preserving\nthe model's general instruction-following skills. These results highlight\nreinforcing cognitive experts as a promising, practical, and interpretable\ndirection to enhance cognitive efficiency within advanced reasoning models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) architectures within Large Reasoning Models (LRMs)\nhave achieved impressive reasoning capabilities by selectively activating\nexperts to facilitate structured cognitive processes. Despite notable advances,\nexisting reasoning models often suffer from cognitive inefficiencies like\noverthinking and underthinking. To address these limitations, we introduce a\nnovel inference-time steering methodology called Reinforcing Cognitive Experts\n(RICE), designed to improve reasoning performance without additional training\nor complex heuristics. Leveraging normalized Pointwise Mutual Information\n(nPMI), we systematically identify specialized experts, termed ''cognitive\nexperts'' that orchestrate meta-level reasoning operations characterized by\ntokens like ''<think>''. Empirical evaluations with leading MoE-based LRMs\n(DeepSeek-R1 and Qwen3-235B) on rigorous quantitative and scientific reasoning\nbenchmarks demonstrate noticeable and consistent improvements in reasoning\naccuracy, cognitive efficiency, and cross-domain generalization. Crucially, our\nlightweight approach substantially outperforms prevalent reasoning-steering\ntechniques, such as prompt design and decoding constraints, while preserving\nthe model's general instruction-following skills. These results highlight\nreinforcing cognitive experts as a promising, practical, and interpretable\ndirection to enhance cognitive efficiency within advanced reasoning models."
                },
                "authors": [
                    {
                        "name": "Mengru Wang"
                    },
                    {
                        "name": "Xingyu Chen"
                    },
                    {
                        "name": "Yue Wang"
                    },
                    {
                        "name": "Zhiwei He"
                    },
                    {
                        "name": "Jiahao Xu"
                    },
                    {
                        "name": "Tian Liang"
                    },
                    {
                        "name": "Qiuzhi Liu"
                    },
                    {
                        "name": "Yunzhi Yao"
                    },
                    {
                        "name": "Wenxuan Wang"
                    },
                    {
                        "name": "Ruotian Ma"
                    },
                    {
                        "name": "Haitao Mi"
                    },
                    {
                        "name": "Ningyu Zhang"
                    },
                    {
                        "name": "Zhaopeng Tu"
                    },
                    {
                        "name": "Xiaolong Li"
                    },
                    {
                        "name": "Dong Yu"
                    }
                ],
                "author_detail": {
                    "name": "Dong Yu"
                },
                "author": "Dong Yu",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14681v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14681v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14679v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14679v1",
                "updated": "2025-05-20T17:59:04Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    17,
                    59,
                    4,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T17:59:04Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    17,
                    59,
                    4,
                    1,
                    140,
                    0
                ],
                "title": "UltraEdit: Training-, Subject-, and Memory-Free Lifelong Editing in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UltraEdit: Training-, Subject-, and Memory-Free Lifelong Editing in\n  Large Language Models"
                },
                "summary": "Lifelong learning enables large language models (LLMs) to adapt to evolving\ninformation by continually updating their internal knowledge. An ideal system\nshould support efficient, wide-ranging updates while preserving existing\ncapabilities and ensuring reliable deployment. Model editing stands out as a\npromising solution for this goal, offering a focused and efficient way to\nrevise a model's internal knowledge. Although recent paradigms have made\nnotable progress, they often struggle to meet the demands of practical lifelong\nadaptation at scale. To bridge this gap, we propose ULTRAEDIT-a fundamentally\nnew editing solution that is training-, subject- and memory-free, making it\nparticularly well-suited for ultra-scalable, real-world lifelong model editing.\nULTRAEDIT performs editing through a self-contained process that relies solely\non lightweight linear algebra operations to compute parameter shifts, enabling\nfast and consistent parameter modifications with minimal overhead. To improve\nscalability in lifelong settings, ULTRAEDIT employs a lifelong normalization\nstrategy that continuously updates feature statistics across turns, allowing it\nto adapt to distributional shifts and maintain consistency over time. ULTRAEDIT\nachieves editing speeds over 7x faster than the previous state-of-the-art\nmethod-which was also the fastest known approach-while consuming less than 1/3\nthe VRAM, making it the only method currently capable of editing a 7B LLM on a\n24GB consumer-grade GPU. Furthermore, we construct ULTRAEDITBENCH-the largest\ndataset in the field to date, with over 2M editing pairs-and demonstrate that\nour method supports up to 1M edits while maintaining high accuracy.\nComprehensive experiments on four datasets and six models show that ULTRAEDIT\nconsistently achieves superior performance across diverse model editing\nscenarios. Our code is available at: https://github.com/XiaojieGu/UltraEdit.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lifelong learning enables large language models (LLMs) to adapt to evolving\ninformation by continually updating their internal knowledge. An ideal system\nshould support efficient, wide-ranging updates while preserving existing\ncapabilities and ensuring reliable deployment. Model editing stands out as a\npromising solution for this goal, offering a focused and efficient way to\nrevise a model's internal knowledge. Although recent paradigms have made\nnotable progress, they often struggle to meet the demands of practical lifelong\nadaptation at scale. To bridge this gap, we propose ULTRAEDIT-a fundamentally\nnew editing solution that is training-, subject- and memory-free, making it\nparticularly well-suited for ultra-scalable, real-world lifelong model editing.\nULTRAEDIT performs editing through a self-contained process that relies solely\non lightweight linear algebra operations to compute parameter shifts, enabling\nfast and consistent parameter modifications with minimal overhead. To improve\nscalability in lifelong settings, ULTRAEDIT employs a lifelong normalization\nstrategy that continuously updates feature statistics across turns, allowing it\nto adapt to distributional shifts and maintain consistency over time. ULTRAEDIT\nachieves editing speeds over 7x faster than the previous state-of-the-art\nmethod-which was also the fastest known approach-while consuming less than 1/3\nthe VRAM, making it the only method currently capable of editing a 7B LLM on a\n24GB consumer-grade GPU. Furthermore, we construct ULTRAEDITBENCH-the largest\ndataset in the field to date, with over 2M editing pairs-and demonstrate that\nour method supports up to 1M edits while maintaining high accuracy.\nComprehensive experiments on four datasets and six models show that ULTRAEDIT\nconsistently achieves superior performance across diverse model editing\nscenarios. Our code is available at: https://github.com/XiaojieGu/UltraEdit."
                },
                "authors": [
                    {
                        "name": "Xiaojie Gu"
                    },
                    {
                        "name": "Guangxu Chen"
                    },
                    {
                        "name": "Jungang Li"
                    },
                    {
                        "name": "Jia-Chen Gu"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Kai Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Kai Zhang"
                },
                "author": "Kai Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14679v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14679v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14677v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14677v1",
                "updated": "2025-05-20T17:58:35Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    17,
                    58,
                    35,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T17:58:35Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    17,
                    58,
                    35,
                    1,
                    140,
                    0
                ],
                "title": "Visionary-R1: Mitigating Shortcuts in Visual Reasoning with\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visionary-R1: Mitigating Shortcuts in Visual Reasoning with\n  Reinforcement Learning"
                },
                "summary": "Learning general-purpose reasoning capabilities has long been a challenging\nproblem in AI. Recent research in large language models (LLMs), such as\nDeepSeek-R1, has shown that reinforcement learning techniques like GRPO can\nenable pre-trained LLMs to develop reasoning capabilities using simple\nquestion-answer pairs. In this paper, we aim to train visual language models\n(VLMs) to perform reasoning on image data through reinforcement learning and\nvisual question-answer pairs, without any explicit chain-of-thought (CoT)\nsupervision. Our findings indicate that simply applying reinforcement learning\nto a VLM -- by prompting the model to produce a reasoning chain before\nproviding an answer -- can lead the model to develop shortcuts from easy\nquestions, thereby reducing its ability to generalize across unseen data\ndistributions. We argue that the key to mitigating shortcut learning is to\nencourage the model to interpret images prior to reasoning. Therefore, we train\nthe model to adhere to a caption-reason-answer output format: initially\ngenerating a detailed caption for an image, followed by constructing an\nextensive reasoning chain. When trained on 273K CoT-free visual question-answer\npairs and using only reinforcement learning, our model, named Visionary-R1,\noutperforms strong multimodal models, such as GPT-4o, Claude3.5-Sonnet, and\nGemini-1.5-Pro, on multiple visual reasoning benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning general-purpose reasoning capabilities has long been a challenging\nproblem in AI. Recent research in large language models (LLMs), such as\nDeepSeek-R1, has shown that reinforcement learning techniques like GRPO can\nenable pre-trained LLMs to develop reasoning capabilities using simple\nquestion-answer pairs. In this paper, we aim to train visual language models\n(VLMs) to perform reasoning on image data through reinforcement learning and\nvisual question-answer pairs, without any explicit chain-of-thought (CoT)\nsupervision. Our findings indicate that simply applying reinforcement learning\nto a VLM -- by prompting the model to produce a reasoning chain before\nproviding an answer -- can lead the model to develop shortcuts from easy\nquestions, thereby reducing its ability to generalize across unseen data\ndistributions. We argue that the key to mitigating shortcut learning is to\nencourage the model to interpret images prior to reasoning. Therefore, we train\nthe model to adhere to a caption-reason-answer output format: initially\ngenerating a detailed caption for an image, followed by constructing an\nextensive reasoning chain. When trained on 273K CoT-free visual question-answer\npairs and using only reinforcement learning, our model, named Visionary-R1,\noutperforms strong multimodal models, such as GPT-4o, Claude3.5-Sonnet, and\nGemini-1.5-Pro, on multiple visual reasoning benchmarks."
                },
                "authors": [
                    {
                        "name": "Jiaer Xia"
                    },
                    {
                        "name": "Yuhang Zang"
                    },
                    {
                        "name": "Peng Gao"
                    },
                    {
                        "name": "Yixuan Li"
                    },
                    {
                        "name": "Kaiyang Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Kaiyang Zhou"
                },
                "author": "Kaiyang Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14677v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14677v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14676v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14676v1",
                "updated": "2025-05-20T17:58:23Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    17,
                    58,
                    23,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T17:58:23Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    17,
                    58,
                    23,
                    1,
                    140,
                    0
                ],
                "title": "A JWST View of the Overmassive Black Hole in NGC 4486B",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A JWST View of the Overmassive Black Hole in NGC 4486B"
                },
                "summary": "We present a new stellar dynamical measurement of the supermassive black hole\n(SMBH) in the compact elliptical galaxy NGC 4486B, based on integral field\nspectroscopy with JWST/NIRSpec. The two-dimensional kinematic maps reveal a\nresolved double nucleus and a velocity dispersion peak offset from the\nphotometric center. Utilizing two independent methods-Schwarzschild\norbit-superposition and Jeans Anisotropic Modeling-we tightly constrain the\nblack hole mass by fitting the full line-of-sight velocity distribution. Our\naxisymmetric Schwarzschild models yield a best-fit black hole mass of $M_{BH} =\n3.6^{+0.6}_{-0.6} \\times 10^8 \\, M_{\\odot}$, slightly lower but significantly\nmore precise than previous estimates. However, since our models do not account\nfor the non-equilibrium nature of the double nucleus, this value may represent\na lower limit. The inferred black hole mass corresponds to approximately 4-13%\nof the total stellar mass, providing robust evidence for an overmassive SMBH in\nNGC 4486B. Combined with the galaxy's location deep within the Virgo Cluster,\nour results support the interpretation that NGC 4486B is the tidally stripped\nremnant core of a formerly massive galaxy. As the JWST/NIRSpec field of view is\ninsufficient to constrain the dark matter halo, we incorporate archival\nground-based long-slit kinematics extending to 5 arcsec. While this provides\nsome leverage on the dark matter content, the constraints remain relatively\nweak. We place only an upper limit on the dark matter fraction, with\n$M_{DM}/M_{*} < 0.5$ within 1 kpc-well beyond the effective radius. The\ninferred black hole mass remains unchanged with or without a dark matter halo.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a new stellar dynamical measurement of the supermassive black hole\n(SMBH) in the compact elliptical galaxy NGC 4486B, based on integral field\nspectroscopy with JWST/NIRSpec. The two-dimensional kinematic maps reveal a\nresolved double nucleus and a velocity dispersion peak offset from the\nphotometric center. Utilizing two independent methods-Schwarzschild\norbit-superposition and Jeans Anisotropic Modeling-we tightly constrain the\nblack hole mass by fitting the full line-of-sight velocity distribution. Our\naxisymmetric Schwarzschild models yield a best-fit black hole mass of $M_{BH} =\n3.6^{+0.6}_{-0.6} \\times 10^8 \\, M_{\\odot}$, slightly lower but significantly\nmore precise than previous estimates. However, since our models do not account\nfor the non-equilibrium nature of the double nucleus, this value may represent\na lower limit. The inferred black hole mass corresponds to approximately 4-13%\nof the total stellar mass, providing robust evidence for an overmassive SMBH in\nNGC 4486B. Combined with the galaxy's location deep within the Virgo Cluster,\nour results support the interpretation that NGC 4486B is the tidally stripped\nremnant core of a formerly massive galaxy. As the JWST/NIRSpec field of view is\ninsufficient to constrain the dark matter halo, we incorporate archival\nground-based long-slit kinematics extending to 5 arcsec. While this provides\nsome leverage on the dark matter content, the constraints remain relatively\nweak. We place only an upper limit on the dark matter fraction, with\n$M_{DM}/M_{*} < 0.5$ within 1 kpc-well beyond the effective radius. The\ninferred black hole mass remains unchanged with or without a dark matter halo."
                },
                "authors": [
                    {
                        "name": "Behzad Tahmasebzadeh"
                    },
                    {
                        "name": "Matthew A. Taylor"
                    },
                    {
                        "name": "Monica Valluri"
                    },
                    {
                        "name": "Haruka Yoshino"
                    },
                    {
                        "name": "Eugene Vasiliev"
                    },
                    {
                        "name": "Michael J. Drinkwater"
                    },
                    {
                        "name": "Solveig Thompson"
                    },
                    {
                        "name": "Kristen Dage"
                    },
                    {
                        "name": "Patrick Côté"
                    },
                    {
                        "name": "Laura Ferrarese"
                    },
                    {
                        "name": "Tatsuya Akiba"
                    },
                    {
                        "name": "Vivienne Baldassare"
                    },
                    {
                        "name": "Misty C. Bentz"
                    },
                    {
                        "name": "John P. Blakeslee"
                    },
                    {
                        "name": "Holger Baumgardt"
                    },
                    {
                        "name": "Youkyung Ko"
                    },
                    {
                        "name": "Chengze Liu"
                    },
                    {
                        "name": "Ann-Marie Madigan"
                    },
                    {
                        "name": "Eric W. Peng"
                    },
                    {
                        "name": "Joel Roediger"
                    },
                    {
                        "name": "Kaixiang Wang"
                    },
                    {
                        "name": "Tyrone E. Woods"
                    }
                ],
                "author_detail": {
                    "name": "Tyrone E. Woods"
                },
                "author": "Tyrone E. Woods",
                "arxiv_comment": "12 pages, 5 figures, 2 tables; submitted to ApJL, comments welcome!",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14676v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14676v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14669v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14669v1",
                "updated": "2025-05-20T17:55:50Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    17,
                    55,
                    50,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T17:55:50Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    17,
                    55,
                    50,
                    1,
                    140,
                    0
                ],
                "title": "Quartet: Native FP4 Training Can Be Optimal for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quartet: Native FP4 Training Can Be Optimal for Large Language Models"
                },
                "summary": "The rapid advancement of large language models (LLMs) has been paralleled by\nunprecedented increases in computational demands, with training costs for\nstate-of-the-art models doubling every few months. Training models directly in\nlow-precision arithmetic offers a solution, by improving both computational\nthroughput and energy efficiency. Specifically, NVIDIA's recent Blackwell\narchitecture facilitates extremely low-precision operations, specifically FP4\nvariants, promising substantial efficiency gains. Yet, current algorithms for\ntraining LLMs in FP4 precision face significant accuracy degradation and often\nrely on mixed-precision fallbacks. In this paper, we systematically investigate\nhardware-supported FP4 training and introduce Quartet, a new approach enabling\naccurate, end-to-end FP4 training with all the major computations (in e.g.\nlinear layers) being performed in low precision. Through extensive evaluations\non Llama-type models, we reveal a new low-precision scaling law that quantifies\nperformance trade-offs across varying bit-widths and allows us to identify a\n\"near-optimal\" low-precision training technique in terms of\naccuracy-vs-computation, called Quartet. We implement Quartet using optimized\nCUDA kernels tailored for NVIDIA Blackwell GPUs, and show that it can achieve\nstate-of-the-art accuracy for FP4 precision, successfully training\nbillion-scale models. Our method demonstrates that fully FP4-based training is\na competitive alternative to standard-precision and FP8 training. Our code is\navailable at https://github.com/IST-DASLab/Quartet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of large language models (LLMs) has been paralleled by\nunprecedented increases in computational demands, with training costs for\nstate-of-the-art models doubling every few months. Training models directly in\nlow-precision arithmetic offers a solution, by improving both computational\nthroughput and energy efficiency. Specifically, NVIDIA's recent Blackwell\narchitecture facilitates extremely low-precision operations, specifically FP4\nvariants, promising substantial efficiency gains. Yet, current algorithms for\ntraining LLMs in FP4 precision face significant accuracy degradation and often\nrely on mixed-precision fallbacks. In this paper, we systematically investigate\nhardware-supported FP4 training and introduce Quartet, a new approach enabling\naccurate, end-to-end FP4 training with all the major computations (in e.g.\nlinear layers) being performed in low precision. Through extensive evaluations\non Llama-type models, we reveal a new low-precision scaling law that quantifies\nperformance trade-offs across varying bit-widths and allows us to identify a\n\"near-optimal\" low-precision training technique in terms of\naccuracy-vs-computation, called Quartet. We implement Quartet using optimized\nCUDA kernels tailored for NVIDIA Blackwell GPUs, and show that it can achieve\nstate-of-the-art accuracy for FP4 precision, successfully training\nbillion-scale models. Our method demonstrates that fully FP4-based training is\na competitive alternative to standard-precision and FP8 training. Our code is\navailable at https://github.com/IST-DASLab/Quartet."
                },
                "authors": [
                    {
                        "name": "Roberto L. Castro"
                    },
                    {
                        "name": "Andrei Panferov"
                    },
                    {
                        "name": "Soroush Tabesh"
                    },
                    {
                        "name": "Oliver Sieberling"
                    },
                    {
                        "name": "Jiale Chen"
                    },
                    {
                        "name": "Mahdi Nikdan"
                    },
                    {
                        "name": "Saleh Ashkboos"
                    },
                    {
                        "name": "Dan Alistarh"
                    }
                ],
                "author_detail": {
                    "name": "Dan Alistarh"
                },
                "author": "Dan Alistarh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14669v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14669v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14668v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14668v1",
                "updated": "2025-05-20T17:55:25Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    17,
                    55,
                    25,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T17:55:25Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    17,
                    55,
                    25,
                    1,
                    140,
                    0
                ],
                "title": "ContextAgent: Context-Aware Proactive LLM Agents with Open-World Sensory\n  Perceptions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ContextAgent: Context-Aware Proactive LLM Agents with Open-World Sensory\n  Perceptions"
                },
                "summary": "Recent advances in Large Language Models (LLMs) have propelled intelligent\nagents from reactive responses to proactive support. While promising, existing\nproactive agents either rely exclusively on observations from enclosed\nenvironments (e.g., desktop UIs) with direct LLM inference or employ rule-based\nproactive notifications, leading to suboptimal user intent understanding and\nlimited functionality for proactive service. In this paper, we introduce\nContextAgent, the first context-aware proactive agent that incorporates\nextensive sensory contexts to enhance the proactive capabilities of LLM agents.\nContextAgent first extracts multi-dimensional contexts from massive sensory\nperceptions on wearables (e.g., video and audio) to understand user intentions.\nContextAgent then leverages the sensory contexts and the persona contexts from\nhistorical data to predict the necessity for proactive services. When proactive\nassistance is needed, ContextAgent further automatically calls the necessary\ntools to assist users unobtrusively. To evaluate this new task, we curate\nContextAgentBench, the first benchmark for evaluating context-aware proactive\nLLM agents, covering 1,000 samples across nine daily scenarios and twenty\ntools. Experiments on ContextAgentBench show that ContextAgent outperforms\nbaselines by achieving up to 8.5% and 6.0% higher accuracy in proactive\npredictions and tool calling, respectively. We hope our research can inspire\nthe development of more advanced, human-centric, proactive AI assistants.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large Language Models (LLMs) have propelled intelligent\nagents from reactive responses to proactive support. While promising, existing\nproactive agents either rely exclusively on observations from enclosed\nenvironments (e.g., desktop UIs) with direct LLM inference or employ rule-based\nproactive notifications, leading to suboptimal user intent understanding and\nlimited functionality for proactive service. In this paper, we introduce\nContextAgent, the first context-aware proactive agent that incorporates\nextensive sensory contexts to enhance the proactive capabilities of LLM agents.\nContextAgent first extracts multi-dimensional contexts from massive sensory\nperceptions on wearables (e.g., video and audio) to understand user intentions.\nContextAgent then leverages the sensory contexts and the persona contexts from\nhistorical data to predict the necessity for proactive services. When proactive\nassistance is needed, ContextAgent further automatically calls the necessary\ntools to assist users unobtrusively. To evaluate this new task, we curate\nContextAgentBench, the first benchmark for evaluating context-aware proactive\nLLM agents, covering 1,000 samples across nine daily scenarios and twenty\ntools. Experiments on ContextAgentBench show that ContextAgent outperforms\nbaselines by achieving up to 8.5% and 6.0% higher accuracy in proactive\npredictions and tool calling, respectively. We hope our research can inspire\nthe development of more advanced, human-centric, proactive AI assistants."
                },
                "authors": [
                    {
                        "name": "Bufang Yang"
                    },
                    {
                        "name": "Lilin Xu"
                    },
                    {
                        "name": "Liekang Zeng"
                    },
                    {
                        "name": "Kaiwei Liu"
                    },
                    {
                        "name": "Siyang Jiang"
                    },
                    {
                        "name": "Wenrui Lu"
                    },
                    {
                        "name": "Hongkai Chen"
                    },
                    {
                        "name": "Xiaofan Jiang"
                    },
                    {
                        "name": "Guoliang Xing"
                    },
                    {
                        "name": "Zhenyu Yan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenyu Yan"
                },
                "author": "Zhenyu Yan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14668v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14668v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14667v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14667v1",
                "updated": "2025-05-20T17:54:54Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    17,
                    54,
                    54,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T17:54:54Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    17,
                    54,
                    54,
                    1,
                    140,
                    0
                ],
                "title": "SAFEPATH: Preventing Harmful Reasoning in Chain-of-Thought via Early\n  Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SAFEPATH: Preventing Harmful Reasoning in Chain-of-Thought via Early\n  Alignment"
                },
                "summary": "Large Reasoning Models (LRMs) have become powerful tools for complex problem\nsolving, but their structured reasoning pathways can lead to unsafe outputs\nwhen exposed to harmful prompts. Existing safety alignment methods reduce\nharmful outputs but can degrade reasoning depth, leading to significant\ntrade-offs in complex, multi-step tasks, and remain vulnerable to sophisticated\njailbreak attacks. To address this, we introduce SAFEPATH, a lightweight\nalignment method that fine-tunes LRMs to emit a short, 8-token Safety Primer at\nthe start of their reasoning, in response to harmful prompts, while leaving the\nrest of the reasoning process unsupervised. Empirical results across multiple\nbenchmarks indicate that SAFEPATH effectively reduces harmful outputs while\nmaintaining reasoning performance. Specifically, SAFEPATH reduces harmful\nresponses by up to 90.0% and blocks 83.3% of jailbreak attempts in the\nDeepSeek-R1-Distill-Llama-8B model, while requiring 295.9x less compute than\nDirect Refusal and 314.1x less than SafeChain. We further introduce a zero-shot\nvariant that requires no fine-tuning. In addition, we provide a comprehensive\nanalysis of how existing methods in LLMs generalize, or fail, when applied to\nreasoning-centric models, revealing critical gaps and new directions for safer\nAI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Reasoning Models (LRMs) have become powerful tools for complex problem\nsolving, but their structured reasoning pathways can lead to unsafe outputs\nwhen exposed to harmful prompts. Existing safety alignment methods reduce\nharmful outputs but can degrade reasoning depth, leading to significant\ntrade-offs in complex, multi-step tasks, and remain vulnerable to sophisticated\njailbreak attacks. To address this, we introduce SAFEPATH, a lightweight\nalignment method that fine-tunes LRMs to emit a short, 8-token Safety Primer at\nthe start of their reasoning, in response to harmful prompts, while leaving the\nrest of the reasoning process unsupervised. Empirical results across multiple\nbenchmarks indicate that SAFEPATH effectively reduces harmful outputs while\nmaintaining reasoning performance. Specifically, SAFEPATH reduces harmful\nresponses by up to 90.0% and blocks 83.3% of jailbreak attempts in the\nDeepSeek-R1-Distill-Llama-8B model, while requiring 295.9x less compute than\nDirect Refusal and 314.1x less than SafeChain. We further introduce a zero-shot\nvariant that requires no fine-tuning. In addition, we provide a comprehensive\nanalysis of how existing methods in LLMs generalize, or fail, when applied to\nreasoning-centric models, revealing critical gaps and new directions for safer\nAI."
                },
                "authors": [
                    {
                        "name": "Wonje Jeung"
                    },
                    {
                        "name": "Sangyeon Yoon"
                    },
                    {
                        "name": "Minsuk Kahng"
                    },
                    {
                        "name": "Albert No"
                    }
                ],
                "author_detail": {
                    "name": "Albert No"
                },
                "author": "Albert No",
                "arxiv_comment": "22 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14667v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14667v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15364v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15364v3",
                "updated": "2025-05-20T17:50:11Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    17,
                    50,
                    11,
                    1,
                    140,
                    0
                ],
                "published": "2025-04-21T18:12:46Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    18,
                    12,
                    46,
                    0,
                    111,
                    0
                ],
                "title": "KeyDiff: Key Similarity-Based KV Cache Eviction for Long-Context LLM\n  Inference in Resource-Constrained Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KeyDiff: Key Similarity-Based KV Cache Eviction for Long-Context LLM\n  Inference in Resource-Constrained Environments"
                },
                "summary": "We demonstrate that geometrically distinctive keys during LLM inference tend\nto have high attention scores. Based on the phenomenon we propose KeyDiff, a\ntraining-free KV cache eviction method based solely on key similarity. Unlike\nother KV cache eviction methods, KeyDiff can process arbitrarily long prompts\nwithin strict resource constraints and efficiently generate responses. We\nprovide a theoretical basis for KeyDiff by relating key diversity with\nattention scores. These results imply KeyDiff can efficiently identify the most\nimportant tokens to retain. Notably KeyDiff does not rely on attention scores,\nallowing the use of optimized attention mechanisms like FlashAttention. Under a\nstrict memory allowance, we demonstrate the effectiveness of KeyDiff for the\nLlama and Qwen model families by observing a performance gap of less than 0.04%\nwith 8K cache budget ($\\sim$23% KV cache reduction) from the non-evicting\nbaseline on LongBench for Llama 3.1-8B and Llama 3.2-3B. We also observe near\nbaseline performance for Deepseek-R1-Distill-Llama-8B on the Math500 reasoning\nbenchmark and decrease end-to-end inference latency by up to 30% compared to\nthe other token-eviction methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We demonstrate that geometrically distinctive keys during LLM inference tend\nto have high attention scores. Based on the phenomenon we propose KeyDiff, a\ntraining-free KV cache eviction method based solely on key similarity. Unlike\nother KV cache eviction methods, KeyDiff can process arbitrarily long prompts\nwithin strict resource constraints and efficiently generate responses. We\nprovide a theoretical basis for KeyDiff by relating key diversity with\nattention scores. These results imply KeyDiff can efficiently identify the most\nimportant tokens to retain. Notably KeyDiff does not rely on attention scores,\nallowing the use of optimized attention mechanisms like FlashAttention. Under a\nstrict memory allowance, we demonstrate the effectiveness of KeyDiff for the\nLlama and Qwen model families by observing a performance gap of less than 0.04%\nwith 8K cache budget ($\\sim$23% KV cache reduction) from the non-evicting\nbaseline on LongBench for Llama 3.1-8B and Llama 3.2-3B. We also observe near\nbaseline performance for Deepseek-R1-Distill-Llama-8B on the Math500 reasoning\nbenchmark and decrease end-to-end inference latency by up to 30% compared to\nthe other token-eviction methods."
                },
                "authors": [
                    {
                        "name": "Junyoung Park"
                    },
                    {
                        "name": "Dalton Jones"
                    },
                    {
                        "name": "Matthew J Morse"
                    },
                    {
                        "name": "Raghavv Goel"
                    },
                    {
                        "name": "Mingu Lee"
                    },
                    {
                        "name": "Chris Lott"
                    }
                ],
                "author_detail": {
                    "name": "Chris Lott"
                },
                "author": "Chris Lott",
                "arxiv_comment": "9 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15364v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15364v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14661v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14661v1",
                "updated": "2025-05-20T17:49:46Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    17,
                    49,
                    46,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T17:49:46Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    17,
                    49,
                    46,
                    1,
                    140,
                    0
                ],
                "title": "Abacus: A Cost-Based Optimizer for Semantic Operator Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Abacus: A Cost-Based Optimizer for Semantic Operator Systems"
                },
                "summary": "LLMs enable an exciting new class of data processing applications over large\ncollections of unstructured documents. Several new programming frameworks have\nenabled developers to build these applications by composing them out of\nsemantic operators: a declarative set of AI-powered data transformations with\nnatural language specifications. These include LLM-powered maps, filters,\njoins, etc. used for document processing tasks such as information extraction,\nsummarization, and more. While systems of semantic operators have achieved\nstrong performance on benchmarks, they can be difficult to optimize. An\noptimizer for this setting must determine how to physically implement each\nsemantic operator in a way that optimizes the system globally. Existing\noptimizers are limited in the number of optimizations they can apply, and most\n(if not all) cannot optimize system quality, cost, or latency subject to\nconstraint(s) on the other dimensions. In this paper we present Abacus, an\nextensible, cost-based optimizer which searches for the best implementation of\na semantic operator system given a (possibly constrained) optimization\nobjective. Abacus estimates operator performance by leveraging a minimal set of\nvalidation examples and, if available, prior beliefs about operator\nperformance. We evaluate Abacus on document processing workloads in the\nbiomedical and legal domains (BioDEX; CUAD) and multi-modal question answering\n(MMQA). We demonstrate that systems optimized by Abacus achieve 18.7%-39.2%\nbetter quality and up to 23.6x lower cost and 4.2x lower latency than the next\nbest system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs enable an exciting new class of data processing applications over large\ncollections of unstructured documents. Several new programming frameworks have\nenabled developers to build these applications by composing them out of\nsemantic operators: a declarative set of AI-powered data transformations with\nnatural language specifications. These include LLM-powered maps, filters,\njoins, etc. used for document processing tasks such as information extraction,\nsummarization, and more. While systems of semantic operators have achieved\nstrong performance on benchmarks, they can be difficult to optimize. An\noptimizer for this setting must determine how to physically implement each\nsemantic operator in a way that optimizes the system globally. Existing\noptimizers are limited in the number of optimizations they can apply, and most\n(if not all) cannot optimize system quality, cost, or latency subject to\nconstraint(s) on the other dimensions. In this paper we present Abacus, an\nextensible, cost-based optimizer which searches for the best implementation of\na semantic operator system given a (possibly constrained) optimization\nobjective. Abacus estimates operator performance by leveraging a minimal set of\nvalidation examples and, if available, prior beliefs about operator\nperformance. We evaluate Abacus on document processing workloads in the\nbiomedical and legal domains (BioDEX; CUAD) and multi-modal question answering\n(MMQA). We demonstrate that systems optimized by Abacus achieve 18.7%-39.2%\nbetter quality and up to 23.6x lower cost and 4.2x lower latency than the next\nbest system."
                },
                "authors": [
                    {
                        "name": "Matthew Russo"
                    },
                    {
                        "name": "Sivaprasad Sudhir"
                    },
                    {
                        "name": "Gerardo Vitagliano"
                    },
                    {
                        "name": "Chunwei Liu"
                    },
                    {
                        "name": "Tim Kraska"
                    },
                    {
                        "name": "Samuel Madden"
                    },
                    {
                        "name": "Michael Cafarella"
                    }
                ],
                "author_detail": {
                    "name": "Michael Cafarella"
                },
                "author": "Michael Cafarella",
                "arxiv_comment": "16 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14661v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14661v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.2.4; I.2.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14656v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14656v1",
                "updated": "2025-05-20T17:43:33Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    17,
                    43,
                    33,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T17:43:33Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    17,
                    43,
                    33,
                    1,
                    140,
                    0
                ],
                "title": "Cost-Augmented Monte Carlo Tree Search for LLM-Assisted Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cost-Augmented Monte Carlo Tree Search for LLM-Assisted Planning"
                },
                "summary": "While LLMs excel at open-ended reasoning, they often struggle with\ncost-sensitive planning, either treating all actions as having equal cost or\nfailing to stay within strict budgets. In this paper, we introduce\nCost-Augmented Monte Carlo Tree Search (CATS), a novel approach that brings\nexplicit cost-awareness into LLM-guided planning. Tight cost constraints push\nthe planner to quickly identify infeasible solutions, while looser constraints\nencourage optimization for minimal cost. We benchmark top LLMs such as GPT-4.1,\nClaude-3.7-Sonnet, and DeepSeek-R1, against our CATS planner to evaluate their\nperformance in cost-sensitive scenarios. Our experiments suggest that raw LLMs\nsuch as GPT-4.1 often falter under tight budgets, whereas CATS consistently\ndelivers strong performance, achieving higher task success rates and better\ncost efficiency. CATS provides an effective solution for budget-aware\ndecision-making by combining the reasoning power of LLMs with structured\nsearch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While LLMs excel at open-ended reasoning, they often struggle with\ncost-sensitive planning, either treating all actions as having equal cost or\nfailing to stay within strict budgets. In this paper, we introduce\nCost-Augmented Monte Carlo Tree Search (CATS), a novel approach that brings\nexplicit cost-awareness into LLM-guided planning. Tight cost constraints push\nthe planner to quickly identify infeasible solutions, while looser constraints\nencourage optimization for minimal cost. We benchmark top LLMs such as GPT-4.1,\nClaude-3.7-Sonnet, and DeepSeek-R1, against our CATS planner to evaluate their\nperformance in cost-sensitive scenarios. Our experiments suggest that raw LLMs\nsuch as GPT-4.1 often falter under tight budgets, whereas CATS consistently\ndelivers strong performance, achieving higher task success rates and better\ncost efficiency. CATS provides an effective solution for budget-aware\ndecision-making by combining the reasoning power of LLMs with structured\nsearch."
                },
                "authors": [
                    {
                        "name": "Zihao Zhang"
                    },
                    {
                        "name": "Fei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Fei Liu"
                },
                "author": "Fei Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14656v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14656v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14654v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14654v1",
                "updated": "2025-05-20T17:42:34Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    17,
                    42,
                    34,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T17:42:34Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    17,
                    42,
                    34,
                    1,
                    140,
                    0
                ],
                "title": "Beyond Words: Multimodal LLM Knows When to Speak",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Words: Multimodal LLM Knows When to Speak"
                },
                "summary": "While large language model (LLM)-based chatbots have demonstrated strong\ncapabilities in generating coherent and contextually relevant responses, they\noften struggle with understanding when to speak, particularly in delivering\nbrief, timely reactions during ongoing conversations. This limitation arises\nlargely from their reliance on text input, lacking the rich contextual cues in\nreal-world human dialogue. In this work, we focus on real-time prediction of\nresponse types, with an emphasis on short, reactive utterances that depend on\nsubtle, multimodal signals across vision, audio, and text. To support this, we\nintroduce a new multimodal dataset constructed from real-world conversational\nvideos, containing temporally aligned visual, auditory, and textual streams.\nThis dataset enables fine-grained modeling of response timing in dyadic\ninteractions. Building on this dataset, we propose MM-When2Speak, a multimodal\nLLM-based model that adaptively integrates visual, auditory, and textual\ncontext to predict when a response should occur, and what type of response is\nappropriate. Experiments show that MM-When2Speak significantly outperforms\nstate-of-the-art unimodal and LLM-based baselines, achieving up to a 4x\nimprovement in response timing accuracy over leading commercial LLMs. These\nresults underscore the importance of multimodal inputs for producing timely,\nnatural, and engaging conversational AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language model (LLM)-based chatbots have demonstrated strong\ncapabilities in generating coherent and contextually relevant responses, they\noften struggle with understanding when to speak, particularly in delivering\nbrief, timely reactions during ongoing conversations. This limitation arises\nlargely from their reliance on text input, lacking the rich contextual cues in\nreal-world human dialogue. In this work, we focus on real-time prediction of\nresponse types, with an emphasis on short, reactive utterances that depend on\nsubtle, multimodal signals across vision, audio, and text. To support this, we\nintroduce a new multimodal dataset constructed from real-world conversational\nvideos, containing temporally aligned visual, auditory, and textual streams.\nThis dataset enables fine-grained modeling of response timing in dyadic\ninteractions. Building on this dataset, we propose MM-When2Speak, a multimodal\nLLM-based model that adaptively integrates visual, auditory, and textual\ncontext to predict when a response should occur, and what type of response is\nappropriate. Experiments show that MM-When2Speak significantly outperforms\nstate-of-the-art unimodal and LLM-based baselines, achieving up to a 4x\nimprovement in response timing accuracy over leading commercial LLMs. These\nresults underscore the importance of multimodal inputs for producing timely,\nnatural, and engaging conversational AI."
                },
                "authors": [
                    {
                        "name": "Zikai Liao"
                    },
                    {
                        "name": "Yi Ouyang"
                    },
                    {
                        "name": "Yi-Lun Lee"
                    },
                    {
                        "name": "Chen-Ping Yu"
                    },
                    {
                        "name": "Yi-Hsuan Tsai"
                    },
                    {
                        "name": "Zhaozheng Yin"
                    }
                ],
                "author_detail": {
                    "name": "Zhaozheng Yin"
                },
                "author": "Zhaozheng Yin",
                "arxiv_comment": "Project page: https://github.com/lzk901372/MM-When2Speak",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14654v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14654v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14652v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14652v1",
                "updated": "2025-05-20T17:41:33Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    17,
                    41,
                    33,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T17:41:33Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    17,
                    41,
                    33,
                    1,
                    140,
                    0
                ],
                "title": "General-Reasoner: Advancing LLM Reasoning Across All Domains",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "General-Reasoner: Advancing LLM Reasoning Across All Domains"
                },
                "summary": "Reinforcement learning (RL) has recently demonstrated strong potential in\nenhancing the reasoning capabilities of large language models (LLMs).\nParticularly, the \"Zero\" reinforcement learning introduced by Deepseek-R1-Zero,\nenables direct RL training of base LLMs without relying on an intermediate\nsupervised fine-tuning stage. Despite these advancements, current works for LLM\nreasoning mainly focus on mathematical and coding domains, largely due to data\nabundance and the ease of answer verification. This limits the applicability\nand generalization of such models to broader domains, where questions often\nhave diverse answer representations, and data is more scarce. In this paper, we\npropose General-Reasoner, a novel training paradigm designed to enhance LLM\nreasoning capabilities across diverse domains. Our key contributions include:\n(1) constructing a large-scale, high-quality dataset of questions with\nverifiable answers curated by web crawling, covering a wide range of\ndisciplines; and (2) developing a generative model-based answer verifier, which\nreplaces traditional rule-based verification with the capability of\nchain-of-thought and context-awareness. We train a series of models and\nevaluate them on a wide range of datasets covering wide domains like physics,\nchemistry, finance, electronics etc. Our comprehensive evaluation across these\n12 benchmarks (e.g. MMLU-Pro, GPQA, SuperGPQA, TheoremQA, BBEH and MATH AMC)\ndemonstrates that General-Reasoner outperforms existing baseline methods,\nachieving robust and generalizable reasoning performance while maintaining\nsuperior effectiveness in mathematical reasoning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) has recently demonstrated strong potential in\nenhancing the reasoning capabilities of large language models (LLMs).\nParticularly, the \"Zero\" reinforcement learning introduced by Deepseek-R1-Zero,\nenables direct RL training of base LLMs without relying on an intermediate\nsupervised fine-tuning stage. Despite these advancements, current works for LLM\nreasoning mainly focus on mathematical and coding domains, largely due to data\nabundance and the ease of answer verification. This limits the applicability\nand generalization of such models to broader domains, where questions often\nhave diverse answer representations, and data is more scarce. In this paper, we\npropose General-Reasoner, a novel training paradigm designed to enhance LLM\nreasoning capabilities across diverse domains. Our key contributions include:\n(1) constructing a large-scale, high-quality dataset of questions with\nverifiable answers curated by web crawling, covering a wide range of\ndisciplines; and (2) developing a generative model-based answer verifier, which\nreplaces traditional rule-based verification with the capability of\nchain-of-thought and context-awareness. We train a series of models and\nevaluate them on a wide range of datasets covering wide domains like physics,\nchemistry, finance, electronics etc. Our comprehensive evaluation across these\n12 benchmarks (e.g. MMLU-Pro, GPQA, SuperGPQA, TheoremQA, BBEH and MATH AMC)\ndemonstrates that General-Reasoner outperforms existing baseline methods,\nachieving robust and generalizable reasoning performance while maintaining\nsuperior effectiveness in mathematical reasoning tasks."
                },
                "authors": [
                    {
                        "name": "Xueguang Ma"
                    },
                    {
                        "name": "Qian Liu"
                    },
                    {
                        "name": "Dongfu Jiang"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Zejun Ma"
                    },
                    {
                        "name": "Wenhu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Wenhu Chen"
                },
                "author": "Wenhu Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14652v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14652v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14585v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14585v3",
                "updated": "2025-05-20T17:41:04Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    17,
                    41,
                    4,
                    1,
                    140,
                    0
                ],
                "published": "2024-11-21T20:48:40Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    20,
                    48,
                    40,
                    3,
                    326,
                    0
                ],
                "title": "Efficient Spatio-Temporal Signal Recognition on Edge Devices Using\n  PointLCA-Net",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Spatio-Temporal Signal Recognition on Edge Devices Using\n  PointLCA-Net"
                },
                "summary": "Recent advancements in machine learning, particularly through deep learning\narchitectures like PointNet, have transformed the processing of\nthree-dimensional (3D) point clouds, significantly improving 3D object\nclassification and segmentation tasks. While 3D point clouds provide detailed\nspatial information, spatio-temporal signals introduce a dynamic element that\naccounts for changes over time. However, applying deep learning techniques to\nspatio-temporal signals and deploying them on edge devices presents challenges,\nincluding real-time processing, memory capacity, and power consumption. To\naddress these issues, this paper presents a novel approach that combines\nPointNet's feature extraction with the in-memory computing capabilities and\nenergy efficiency of neuromorphic systems for spatio-temporal signal\nrecognition. The proposed method consists of a two-stage process: in the first\nstage, PointNet extracts features from the spatio-temporal signals, which are\nthen stored in non-volatile memristor crossbar arrays. In the second stage,\nthese features are processed by a single-layer spiking neural encoder-decoder\nthat employs the Locally Competitive Algorithm (LCA) for efficient encoding and\nclassification. This work integrates the strengths of both PointNet and LCA,\nenhancing computational efficiency and energy performance on edge devices.\nPointLCA-Net achieves high recognition accuracy for spatio-temporal data with\nsubstantially lower energy burden during both inference and training than\ncomparable approaches, thus advancing the deployment of advanced neural\narchitectures in energy-constrained environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in machine learning, particularly through deep learning\narchitectures like PointNet, have transformed the processing of\nthree-dimensional (3D) point clouds, significantly improving 3D object\nclassification and segmentation tasks. While 3D point clouds provide detailed\nspatial information, spatio-temporal signals introduce a dynamic element that\naccounts for changes over time. However, applying deep learning techniques to\nspatio-temporal signals and deploying them on edge devices presents challenges,\nincluding real-time processing, memory capacity, and power consumption. To\naddress these issues, this paper presents a novel approach that combines\nPointNet's feature extraction with the in-memory computing capabilities and\nenergy efficiency of neuromorphic systems for spatio-temporal signal\nrecognition. The proposed method consists of a two-stage process: in the first\nstage, PointNet extracts features from the spatio-temporal signals, which are\nthen stored in non-volatile memristor crossbar arrays. In the second stage,\nthese features are processed by a single-layer spiking neural encoder-decoder\nthat employs the Locally Competitive Algorithm (LCA) for efficient encoding and\nclassification. This work integrates the strengths of both PointNet and LCA,\nenhancing computational efficiency and energy performance on edge devices.\nPointLCA-Net achieves high recognition accuracy for spatio-temporal data with\nsubstantially lower energy burden during both inference and training than\ncomparable approaches, thus advancing the deployment of advanced neural\narchitectures in energy-constrained environments."
                },
                "authors": [
                    {
                        "name": "Sanaz Mahmoodi Takaghaj"
                    }
                ],
                "author_detail": {
                    "name": "Sanaz Mahmoodi Takaghaj"
                },
                "author": "Sanaz Mahmoodi Takaghaj",
                "arxiv_comment": "Accepted to International Joint Conference on Neural Networks(IJCNN),\n  2015",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14585v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14585v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14638v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14638v1",
                "updated": "2025-05-20T17:26:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    17,
                    26,
                    12,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T17:26:12Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    17,
                    26,
                    12,
                    1,
                    140,
                    0
                ],
                "title": "Dual Precision Quantization for Efficient and Accurate Deep Neural\n  Networks Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dual Precision Quantization for Efficient and Accurate Deep Neural\n  Networks Inference"
                },
                "summary": "Deep neural networks have achieved state-of-the-art results in a wide range\nof applications, from natural language processing and computer vision to speech\nrecognition. However, as tasks become increasingly complex, model sizes\ncontinue to grow, posing challenges in latency and memory efficiency. To meet\nthese constraints, post-training quantization has emerged as a promising\nsolution. In this paper, we propose a novel hardware-efficient quantization and\ninference scheme that exploits hardware advantages with minimal accuracy\ndegradation. Specifically, we introduce a W4A8 scheme, where weights are\nquantized and stored using 4-bit integer precision, and inference computations\nare performed using 8-bit floating-point arithmetic, demonstrating significant\nspeedups and improved memory utilization compared to 16-bit operations,\napplicable on various modern accelerators. To mitigate accuracy loss, we\ndevelop a novel quantization algorithm, dubbed Dual Precision Quantization\n(DPQ), that leverages the unique structure of our scheme without introducing\nadditional inference overhead. Experimental results demonstrate improved\nperformance (i.e., increased throughput) while maintaining tolerable accuracy\ndegradation relative to the full-precision model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep neural networks have achieved state-of-the-art results in a wide range\nof applications, from natural language processing and computer vision to speech\nrecognition. However, as tasks become increasingly complex, model sizes\ncontinue to grow, posing challenges in latency and memory efficiency. To meet\nthese constraints, post-training quantization has emerged as a promising\nsolution. In this paper, we propose a novel hardware-efficient quantization and\ninference scheme that exploits hardware advantages with minimal accuracy\ndegradation. Specifically, we introduce a W4A8 scheme, where weights are\nquantized and stored using 4-bit integer precision, and inference computations\nare performed using 8-bit floating-point arithmetic, demonstrating significant\nspeedups and improved memory utilization compared to 16-bit operations,\napplicable on various modern accelerators. To mitigate accuracy loss, we\ndevelop a novel quantization algorithm, dubbed Dual Precision Quantization\n(DPQ), that leverages the unique structure of our scheme without introducing\nadditional inference overhead. Experimental results demonstrate improved\nperformance (i.e., increased throughput) while maintaining tolerable accuracy\ndegradation relative to the full-precision model."
                },
                "authors": [
                    {
                        "name": "Tomer Gafni"
                    },
                    {
                        "name": "Asaf Karnieli"
                    },
                    {
                        "name": "Yair Hanani"
                    }
                ],
                "author_detail": {
                    "name": "Yair Hanani"
                },
                "author": "Yair Hanani",
                "arxiv_comment": "Accepted at eLVM Workshop, CVPR, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14638v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14638v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14631v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14631v2",
                "updated": "2025-05-21T05:17:34Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    5,
                    17,
                    34,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-20T17:23:25Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    17,
                    23,
                    25,
                    1,
                    140,
                    0
                ],
                "title": "Think Only When You Need with Large Hybrid-Reasoning Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Think Only When You Need with Large Hybrid-Reasoning Models"
                },
                "summary": "Recent Large Reasoning Models (LRMs) have shown substantially improved\nreasoning capabilities over traditional Large Language Models (LLMs) by\nincorporating extended thinking processes prior to producing final responses.\nHowever, excessively lengthy thinking introduces substantial overhead in terms\nof token consumption and latency, which is particularly unnecessary for simple\nqueries. In this work, we introduce Large Hybrid-Reasoning Models (LHRMs), the\nfirst kind of model capable of adaptively determining whether to perform\nthinking based on the contextual information of user queries. To achieve this,\nwe propose a two-stage training pipeline comprising Hybrid Fine-Tuning (HFT) as\na cold start, followed by online reinforcement learning with the proposed\nHybrid Group Policy Optimization (HGPO) to implicitly learn to select the\nappropriate thinking mode. Furthermore, we introduce a metric called Hybrid\nAccuracy to quantitatively assess the model's capability for hybrid thinking.\nExtensive experimental results show that LHRMs can adaptively perform hybrid\nthinking on queries of varying difficulty and type. It outperforms existing\nLRMs and LLMs in reasoning and general capabilities while significantly\nimproving efficiency. Together, our work advocates for a reconsideration of the\nappropriate use of extended thinking processes and provides a solid starting\npoint for building hybrid thinking systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Large Reasoning Models (LRMs) have shown substantially improved\nreasoning capabilities over traditional Large Language Models (LLMs) by\nincorporating extended thinking processes prior to producing final responses.\nHowever, excessively lengthy thinking introduces substantial overhead in terms\nof token consumption and latency, which is particularly unnecessary for simple\nqueries. In this work, we introduce Large Hybrid-Reasoning Models (LHRMs), the\nfirst kind of model capable of adaptively determining whether to perform\nthinking based on the contextual information of user queries. To achieve this,\nwe propose a two-stage training pipeline comprising Hybrid Fine-Tuning (HFT) as\na cold start, followed by online reinforcement learning with the proposed\nHybrid Group Policy Optimization (HGPO) to implicitly learn to select the\nappropriate thinking mode. Furthermore, we introduce a metric called Hybrid\nAccuracy to quantitatively assess the model's capability for hybrid thinking.\nExtensive experimental results show that LHRMs can adaptively perform hybrid\nthinking on queries of varying difficulty and type. It outperforms existing\nLRMs and LLMs in reasoning and general capabilities while significantly\nimproving efficiency. Together, our work advocates for a reconsideration of the\nappropriate use of extended thinking processes and provides a solid starting\npoint for building hybrid thinking systems."
                },
                "authors": [
                    {
                        "name": "Lingjie Jiang"
                    },
                    {
                        "name": "Xun Wu"
                    },
                    {
                        "name": "Shaohan Huang"
                    },
                    {
                        "name": "Qingxiu Dong"
                    },
                    {
                        "name": "Zewen Chi"
                    },
                    {
                        "name": "Li Dong"
                    },
                    {
                        "name": "Xingxing Zhang"
                    },
                    {
                        "name": "Tengchao Lv"
                    },
                    {
                        "name": "Lei Cui"
                    },
                    {
                        "name": "Furu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Furu Wei"
                },
                "author": "Furu Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14631v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14631v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09041v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09041v2",
                "updated": "2025-05-20T17:21:47Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    17,
                    21,
                    47,
                    1,
                    140,
                    0
                ],
                "published": "2024-10-11T17:55:48Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    17,
                    55,
                    48,
                    4,
                    285,
                    0
                ],
                "title": "Inferring birth versus death dynamics for ecological interactions in\n  stochastic heterogeneous populations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring birth versus death dynamics for ecological interactions in\n  stochastic heterogeneous populations"
                },
                "summary": "In this paper, we study the significance of ecological interactions and\nseparation of birth and death dynamics in heterogeneous stochastic populations\nvia general birth-death processes. Interactions can manifest through the birth\ndynamics, the death dynamics, or some combination of the two. The underlying\nmechanisms are important but often implicit in population-level data. We\npropose an inference method for disambiguating the types of interaction and the\nbirth and death processes from population size time series data of a stochastic\n$n$-type heterogeneous population. The interspecies interactions considered can\nbe competitive, antagonistic, or mutualistic. We show that different pairs of\nbirth and death rates with the same net growth rate result in different time\nseries statistics. Then, the inference method is validated in the example of a\nbirth-death process inspired by the two-type Lotka-Volterra interaction\ndynamics. Utilizing stochastic fluctuations enables us to estimate additional\nparameters in this stochastic Lotka-Volterra model, which are not identifiable\nin a deterministic model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we study the significance of ecological interactions and\nseparation of birth and death dynamics in heterogeneous stochastic populations\nvia general birth-death processes. Interactions can manifest through the birth\ndynamics, the death dynamics, or some combination of the two. The underlying\nmechanisms are important but often implicit in population-level data. We\npropose an inference method for disambiguating the types of interaction and the\nbirth and death processes from population size time series data of a stochastic\n$n$-type heterogeneous population. The interspecies interactions considered can\nbe competitive, antagonistic, or mutualistic. We show that different pairs of\nbirth and death rates with the same net growth rate result in different time\nseries statistics. Then, the inference method is validated in the example of a\nbirth-death process inspired by the two-type Lotka-Volterra interaction\ndynamics. Utilizing stochastic fluctuations enables us to estimate additional\nparameters in this stochastic Lotka-Volterra model, which are not identifiable\nin a deterministic model."
                },
                "authors": [
                    {
                        "name": "Erin Beckman"
                    },
                    {
                        "name": "Heyrim Cho"
                    },
                    {
                        "name": "Linh Huynh"
                    }
                ],
                "author_detail": {
                    "name": "Linh Huynh"
                },
                "author": "Linh Huynh",
                "arxiv_comment": "28 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09041v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09041v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.PE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.PE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "92-08, 92D25",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14629v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14629v1",
                "updated": "2025-05-20T17:19:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    17,
                    19,
                    57,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T17:19:57Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    17,
                    19,
                    57,
                    1,
                    140,
                    0
                ],
                "title": "KERL: Knowledge-Enhanced Personalized Recipe Recommendation using Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KERL: Knowledge-Enhanced Personalized Recipe Recommendation using Large\n  Language Models"
                },
                "summary": "Recent advances in large language models (LLMs) and the abundance of food\ndata have resulted in studies to improve food understanding using LLMs. Despite\nseveral recommendation systems utilizing LLMs and Knowledge Graphs (KGs), there\nhas been limited research on integrating food related KGs with LLMs. We\nintroduce KERL, a unified system that leverages food KGs and LLMs to provide\npersonalized food recommendations and generates recipes with associated\nmicro-nutritional information. Given a natural language question, KERL extracts\nentities, retrieves subgraphs from the KG, which are then fed into the LLM as\ncontext to select the recipes that satisfy the constraints. Next, our system\ngenerates the cooking steps and nutritional information for each recipe. To\nevaluate our approach, we also develop a benchmark dataset by curating recipe\nrelated questions, combined with constraints and personal preferences. Through\nextensive experiments, we show that our proposed KG-augmented LLM significantly\noutperforms existing approaches, offering a complete and coherent solution for\nfood recommendation, recipe generation, and nutritional analysis. Our code and\nbenchmark datasets are publicly available at\nhttps://github.com/mohbattharani/KERL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) and the abundance of food\ndata have resulted in studies to improve food understanding using LLMs. Despite\nseveral recommendation systems utilizing LLMs and Knowledge Graphs (KGs), there\nhas been limited research on integrating food related KGs with LLMs. We\nintroduce KERL, a unified system that leverages food KGs and LLMs to provide\npersonalized food recommendations and generates recipes with associated\nmicro-nutritional information. Given a natural language question, KERL extracts\nentities, retrieves subgraphs from the KG, which are then fed into the LLM as\ncontext to select the recipes that satisfy the constraints. Next, our system\ngenerates the cooking steps and nutritional information for each recipe. To\nevaluate our approach, we also develop a benchmark dataset by curating recipe\nrelated questions, combined with constraints and personal preferences. Through\nextensive experiments, we show that our proposed KG-augmented LLM significantly\noutperforms existing approaches, offering a complete and coherent solution for\nfood recommendation, recipe generation, and nutritional analysis. Our code and\nbenchmark datasets are publicly available at\nhttps://github.com/mohbattharani/KERL."
                },
                "authors": [
                    {
                        "name": "Fnu Mohbat"
                    },
                    {
                        "name": "Mohammed J Zaki"
                    }
                ],
                "author_detail": {
                    "name": "Mohammed J Zaki"
                },
                "author": "Mohammed J Zaki",
                "arxiv_comment": "Accepted at ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14629v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14629v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14627v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14627v1",
                "updated": "2025-05-20T17:18:17Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    17,
                    18,
                    17,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T17:18:17Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    17,
                    18,
                    17,
                    1,
                    140,
                    0
                ],
                "title": "Debating for Better Reasoning: An Unsupervised Multimodal Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Debating for Better Reasoning: An Unsupervised Multimodal Approach"
                },
                "summary": "As Large Language Models (LLMs) gain expertise across diverse domains and\nmodalities, scalable oversight becomes increasingly challenging, particularly\nwhen their capabilities may surpass human evaluators. Debate has emerged as a\npromising mechanism for enabling such oversight. In this work, we extend the\ndebate paradigm to a multimodal setting, exploring its potential for weaker\nmodels to supervise and enhance the performance of stronger models. We focus on\nvisual question answering (VQA), where two \"sighted\" expert vision-language\nmodels debate an answer, while a \"blind\" (text-only) judge adjudicates based\nsolely on the quality of the arguments. In our framework, the experts defend\nonly answers aligned with their beliefs, thereby obviating the need for\nexplicit role-playing and concentrating the debate on instances of expert\ndisagreement. Experiments on several multimodal tasks demonstrate that the\ndebate framework consistently outperforms individual expert models. Moreover,\njudgments from weaker LLMs can help instill reasoning capabilities in\nvision-language models through finetuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) gain expertise across diverse domains and\nmodalities, scalable oversight becomes increasingly challenging, particularly\nwhen their capabilities may surpass human evaluators. Debate has emerged as a\npromising mechanism for enabling such oversight. In this work, we extend the\ndebate paradigm to a multimodal setting, exploring its potential for weaker\nmodels to supervise and enhance the performance of stronger models. We focus on\nvisual question answering (VQA), where two \"sighted\" expert vision-language\nmodels debate an answer, while a \"blind\" (text-only) judge adjudicates based\nsolely on the quality of the arguments. In our framework, the experts defend\nonly answers aligned with their beliefs, thereby obviating the need for\nexplicit role-playing and concentrating the debate on instances of expert\ndisagreement. Experiments on several multimodal tasks demonstrate that the\ndebate framework consistently outperforms individual expert models. Moreover,\njudgments from weaker LLMs can help instill reasoning capabilities in\nvision-language models through finetuning."
                },
                "authors": [
                    {
                        "name": "Ashutosh Adhikari"
                    },
                    {
                        "name": "Mirella Lapata"
                    }
                ],
                "author_detail": {
                    "name": "Mirella Lapata"
                },
                "author": "Mirella Lapata",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14627v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14627v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10624v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10624v3",
                "updated": "2025-05-20T17:16:44Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    17,
                    16,
                    44,
                    1,
                    140,
                    0
                ],
                "published": "2024-10-14T15:30:41Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    15,
                    30,
                    41,
                    0,
                    288,
                    0
                ],
                "title": "SensorLLM: Human-Intuitive Alignment of Multivariate Sensor Data with\n  LLMs for Activity Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SensorLLM: Human-Intuitive Alignment of Multivariate Sensor Data with\n  LLMs for Activity Recognition"
                },
                "summary": "We introduce SensorLLM, a two-stage framework that enables Large Language\nModels (LLMs) to perform human activity recognition (HAR) from wearable sensor\ndata. While LLMs excel at reasoning and generalization, they struggle with\ntime-series inputs due to limited semantic context, numerical complexity, and\nsequence variability. To address these challenges, we construct SensorQA, a\nquestion-answering dataset of human-intuitive sensor-text pairs spanning\ndiverse HAR scenarios. It supervises the Sensor-Language Alignment stage, where\nthe model aligns sensor inputs with trend descriptions. Special tokens are\nintroduced to mark channel boundaries. This alignment enables LLMs to interpret\nnumerical patterns, channel-specific signals, and variable-length\ninputs--without requiring human annotation. In the subsequent Task-Aware Tuning\nstage, we adapt the model for multivariate HAR classification, achieving\nperformance that matches or exceeds state-of-the-art methods. Our results show\nthat, guided by human-intuitive alignment, SensorLLM becomes an effective\nsensor learner, reasoner, and classifier--generalizing across varied HAR\nsettings and paving the way for foundation model research in time-series\nanalysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce SensorLLM, a two-stage framework that enables Large Language\nModels (LLMs) to perform human activity recognition (HAR) from wearable sensor\ndata. While LLMs excel at reasoning and generalization, they struggle with\ntime-series inputs due to limited semantic context, numerical complexity, and\nsequence variability. To address these challenges, we construct SensorQA, a\nquestion-answering dataset of human-intuitive sensor-text pairs spanning\ndiverse HAR scenarios. It supervises the Sensor-Language Alignment stage, where\nthe model aligns sensor inputs with trend descriptions. Special tokens are\nintroduced to mark channel boundaries. This alignment enables LLMs to interpret\nnumerical patterns, channel-specific signals, and variable-length\ninputs--without requiring human annotation. In the subsequent Task-Aware Tuning\nstage, we adapt the model for multivariate HAR classification, achieving\nperformance that matches or exceeds state-of-the-art methods. Our results show\nthat, guided by human-intuitive alignment, SensorLLM becomes an effective\nsensor learner, reasoner, and classifier--generalizing across varied HAR\nsettings and paving the way for foundation model research in time-series\nanalysis."
                },
                "authors": [
                    {
                        "name": "Zechen Li"
                    },
                    {
                        "name": "Shohreh Deldari"
                    },
                    {
                        "name": "Linyao Chen"
                    },
                    {
                        "name": "Hao Xue"
                    },
                    {
                        "name": "Flora D. Salim"
                    }
                ],
                "author_detail": {
                    "name": "Flora D. Salim"
                },
                "author": "Flora D. Salim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10624v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10624v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14625v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14625v1",
                "updated": "2025-05-20T17:16:44Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    17,
                    16,
                    44,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T17:16:44Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    17,
                    16,
                    44,
                    1,
                    140,
                    0
                ],
                "title": "TinyV: Reducing False Negatives in Verification Improves RL for LLM\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TinyV: Reducing False Negatives in Verification Improves RL for LLM\n  Reasoning"
                },
                "summary": "Reinforcement Learning (RL) has become a powerful tool for enhancing the\nreasoning abilities of large language models (LLMs) by optimizing their\npolicies with reward signals. Yet, RL's success relies on the reliability of\nrewards, which are provided by verifiers. In this paper, we expose and analyze\na widespread problem--false negatives--where verifiers wrongly reject correct\nmodel outputs. Our in-depth study of the Big-Math-RL-Verified dataset reveals\nthat over 38% of model-generated responses suffer from false negatives, where\nthe verifier fails to recognize correct answers. We show, both empirically and\ntheoretically, that these false negatives severely impair RL training by\ndepriving the model of informative gradient signals and slowing convergence. To\nmitigate this, we propose tinyV, a lightweight LLM-based verifier that augments\nexisting rule-based methods, which dynamically identifies potential false\nnegatives and recovers valid responses to produce more accurate reward\nestimates. Across multiple math-reasoning benchmarks, integrating TinyV boosts\npass rates by up to 10% and accelerates convergence relative to the baseline.\nOur findings highlight the critical importance of addressing verifier false\nnegatives and offer a practical approach to improve RL-based fine-tuning of\nLLMs. Our code is available at https://github.com/uw-nsl/TinyV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning (RL) has become a powerful tool for enhancing the\nreasoning abilities of large language models (LLMs) by optimizing their\npolicies with reward signals. Yet, RL's success relies on the reliability of\nrewards, which are provided by verifiers. In this paper, we expose and analyze\na widespread problem--false negatives--where verifiers wrongly reject correct\nmodel outputs. Our in-depth study of the Big-Math-RL-Verified dataset reveals\nthat over 38% of model-generated responses suffer from false negatives, where\nthe verifier fails to recognize correct answers. We show, both empirically and\ntheoretically, that these false negatives severely impair RL training by\ndepriving the model of informative gradient signals and slowing convergence. To\nmitigate this, we propose tinyV, a lightweight LLM-based verifier that augments\nexisting rule-based methods, which dynamically identifies potential false\nnegatives and recovers valid responses to produce more accurate reward\nestimates. Across multiple math-reasoning benchmarks, integrating TinyV boosts\npass rates by up to 10% and accelerates convergence relative to the baseline.\nOur findings highlight the critical importance of addressing verifier false\nnegatives and offer a practical approach to improve RL-based fine-tuning of\nLLMs. Our code is available at https://github.com/uw-nsl/TinyV."
                },
                "authors": [
                    {
                        "name": "Zhangchen Xu"
                    },
                    {
                        "name": "Yuetai Li"
                    },
                    {
                        "name": "Fengqing Jiang"
                    },
                    {
                        "name": "Bhaskar Ramasubramanian"
                    },
                    {
                        "name": "Luyao Niu"
                    },
                    {
                        "name": "Bill Yuchen Lin"
                    },
                    {
                        "name": "Radha Poovendran"
                    }
                ],
                "author_detail": {
                    "name": "Radha Poovendran"
                },
                "author": "Radha Poovendran",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14625v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14625v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03030v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03030v2",
                "updated": "2025-05-20T17:14:08Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    17,
                    14,
                    8,
                    1,
                    140,
                    0
                ],
                "published": "2025-02-05T09:33:42Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    9,
                    33,
                    42,
                    2,
                    36,
                    0
                ],
                "title": "RISE: Two-Stage Rank-Based Identification of High-Dimensional Surrogate\n  Markers Applied to Vaccinology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RISE: Two-Stage Rank-Based Identification of High-Dimensional Surrogate\n  Markers Applied to Vaccinology"
                },
                "summary": "In vaccine trials with long-term participant follow-up, it is of great\nimportance to identify surrogate markers that accurately infer long-term immune\nresponses. These markers offer practical advantages such as providing early,\nindirect evidence of vaccine efficacy, and can accelerate vaccine development\nwhile identifying potential biomarkers. High-throughput technologies like\nRNA-sequencing have emerged as promising tools for understanding complex\nbiological systems and informing new treatment strategies. However, these data\nare high-dimensional, presenting unique statistical challenges for existing\nsurrogate marker identification methods. We introduce Rank-based Identification\nof high-dimensional SurrogatE Markers (RISE), a novel approach designed for\nsmall sample, high-dimensional settings typical in modern vaccine experiments.\nRISE employs a non-parametric univariate test to screen variables for promising\ncandidates, followed by surrogate evaluation on independent data. Our\nsimulation studies demonstrate RISE's desirable properties, including type one\nerror rate control and empirical power under various conditions. Applying RISE\nto a clinical trial for inactivated influenza vaccination, we sought to\nidentify genes whose expression could serve as a surrogate for the induced\nimmune response. This analysis revealed a signature of genes appearing to\nfunction as a reasonable surrogate for the neutralising antibody response.\nPathways related to innate antiviral signalling and interferon stimulation were\nstrongly represented in this derived surrogate, providing a clear immunological\ninterpretation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In vaccine trials with long-term participant follow-up, it is of great\nimportance to identify surrogate markers that accurately infer long-term immune\nresponses. These markers offer practical advantages such as providing early,\nindirect evidence of vaccine efficacy, and can accelerate vaccine development\nwhile identifying potential biomarkers. High-throughput technologies like\nRNA-sequencing have emerged as promising tools for understanding complex\nbiological systems and informing new treatment strategies. However, these data\nare high-dimensional, presenting unique statistical challenges for existing\nsurrogate marker identification methods. We introduce Rank-based Identification\nof high-dimensional SurrogatE Markers (RISE), a novel approach designed for\nsmall sample, high-dimensional settings typical in modern vaccine experiments.\nRISE employs a non-parametric univariate test to screen variables for promising\ncandidates, followed by surrogate evaluation on independent data. Our\nsimulation studies demonstrate RISE's desirable properties, including type one\nerror rate control and empirical power under various conditions. Applying RISE\nto a clinical trial for inactivated influenza vaccination, we sought to\nidentify genes whose expression could serve as a surrogate for the induced\nimmune response. This analysis revealed a signature of genes appearing to\nfunction as a reasonable surrogate for the neutralising antibody response.\nPathways related to innate antiviral signalling and interferon stimulation were\nstrongly represented in this derived surrogate, providing a clear immunological\ninterpretation."
                },
                "authors": [
                    {
                        "name": "Arthur Hughes"
                    },
                    {
                        "name": "Layla Parast"
                    },
                    {
                        "name": "Rodolphe Thiébaut"
                    },
                    {
                        "name": "Boris P. Hejblum"
                    }
                ],
                "author_detail": {
                    "name": "Boris P. Hejblum"
                },
                "author": "Boris P. Hejblum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03030v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03030v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11726v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11726v2",
                "updated": "2025-05-20T17:12:34Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    17,
                    12,
                    34,
                    1,
                    140,
                    0
                ],
                "published": "2024-09-18T06:21:44Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    6,
                    21,
                    44,
                    2,
                    262,
                    0
                ],
                "title": "Revealing and Mitigating the Challenge of Detecting Character Knowledge\n  Errors in LLM Role-Playing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revealing and Mitigating the Challenge of Detecting Character Knowledge\n  Errors in LLM Role-Playing"
                },
                "summary": "Large language model (LLM) role-playing has gained widespread attention.\nAuthentic character knowledge is crucial for constructing realistic LLM\nrole-playing agents. However, existing works usually overlook the exploration\nof LLMs' ability to detect characters' known knowledge errors (KKE) and unknown\nknowledge errors (UKE) while playing roles, which would lead to low-quality\nautomatic construction of character trainable corpus. In this paper, we propose\nRoleKE-Bench to evaluate LLMs' ability to detect errors in KKE and UKE. The\nresults indicate that even the latest LLMs struggle to detect these two types\nof errors effectively, especially when it comes to familiar knowledge. We\nexperimented with various reasoning strategies and propose an agent-based\nreasoning method, Self-Recollection and Self-Doubt (S$^2$RD), to explore\nfurther the potential for improving error detection capabilities. Experiments\nshow that our method effectively improves the LLMs' ability to detect error\ncharacter knowledge, but it remains an issue that requires ongoing attention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) role-playing has gained widespread attention.\nAuthentic character knowledge is crucial for constructing realistic LLM\nrole-playing agents. However, existing works usually overlook the exploration\nof LLMs' ability to detect characters' known knowledge errors (KKE) and unknown\nknowledge errors (UKE) while playing roles, which would lead to low-quality\nautomatic construction of character trainable corpus. In this paper, we propose\nRoleKE-Bench to evaluate LLMs' ability to detect errors in KKE and UKE. The\nresults indicate that even the latest LLMs struggle to detect these two types\nof errors effectively, especially when it comes to familiar knowledge. We\nexperimented with various reasoning strategies and propose an agent-based\nreasoning method, Self-Recollection and Self-Doubt (S$^2$RD), to explore\nfurther the potential for improving error detection capabilities. Experiments\nshow that our method effectively improves the LLMs' ability to detect error\ncharacter knowledge, but it remains an issue that requires ongoing attention."
                },
                "authors": [
                    {
                        "name": "Wenyuan Zhang"
                    },
                    {
                        "name": "Shuaiyi Nie"
                    },
                    {
                        "name": "Jiawei Sheng"
                    },
                    {
                        "name": "Zefeng Zhang"
                    },
                    {
                        "name": "Xinghua Zhang"
                    },
                    {
                        "name": "Yongquan He"
                    },
                    {
                        "name": "Tingwen Liu"
                    }
                ],
                "author_detail": {
                    "name": "Tingwen Liu"
                },
                "author": "Tingwen Liu",
                "arxiv_comment": "25 pages, 6 figures, 20 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11726v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11726v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14620v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14620v1",
                "updated": "2025-05-20T17:11:18Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    17,
                    11,
                    18,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T17:11:18Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    17,
                    11,
                    18,
                    1,
                    140,
                    0
                ],
                "title": "Enhancing Learned Knowledge in LoRA Adapters Through Efficient\n  Contrastive Decoding on Ascend NPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Learned Knowledge in LoRA Adapters Through Efficient\n  Contrastive Decoding on Ascend NPUs"
                },
                "summary": "Huawei Cloud users leverage LoRA (Low-Rank Adaptation) as an efficient and\nscalable method to fine-tune and customize large language models (LLMs) for\napplication-specific needs. However, tasks that require complex reasoning or\ndeep contextual understanding are often hindered by biases or interference from\nthe base model when using typical decoding methods like greedy or beam search.\nThese biases can lead to generic or task-agnostic responses from the base model\ninstead of leveraging the LoRA-specific adaptations. In this paper, we\nintroduce Contrastive LoRA Decoding (CoLD), a novel decoding framework designed\nto maximize the use of task-specific knowledge in LoRA-adapted models,\nresulting in better downstream performance. CoLD uses contrastive decoding by\nscoring candidate tokens based on the divergence between the probability\ndistributions of a LoRA-adapted expert model and the corresponding base model.\nThis approach prioritizes tokens that better align with the LoRA's learned\nrepresentations, enhancing performance for specialized tasks. While effective,\na naive implementation of CoLD is computationally expensive because each\ndecoding step requires evaluating multiple token candidates across both models.\nTo address this, we developed an optimized kernel for Huawei's Ascend NPU. CoLD\nachieves up to a 5.54% increase in task accuracy while reducing end-to-end\nlatency by 28% compared to greedy decoding. This work provides practical and\nefficient decoding strategies for fine-tuned LLMs in resource-constrained\nenvironments and has broad implications for applied data science in both cloud\nand on-premises settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Huawei Cloud users leverage LoRA (Low-Rank Adaptation) as an efficient and\nscalable method to fine-tune and customize large language models (LLMs) for\napplication-specific needs. However, tasks that require complex reasoning or\ndeep contextual understanding are often hindered by biases or interference from\nthe base model when using typical decoding methods like greedy or beam search.\nThese biases can lead to generic or task-agnostic responses from the base model\ninstead of leveraging the LoRA-specific adaptations. In this paper, we\nintroduce Contrastive LoRA Decoding (CoLD), a novel decoding framework designed\nto maximize the use of task-specific knowledge in LoRA-adapted models,\nresulting in better downstream performance. CoLD uses contrastive decoding by\nscoring candidate tokens based on the divergence between the probability\ndistributions of a LoRA-adapted expert model and the corresponding base model.\nThis approach prioritizes tokens that better align with the LoRA's learned\nrepresentations, enhancing performance for specialized tasks. While effective,\na naive implementation of CoLD is computationally expensive because each\ndecoding step requires evaluating multiple token candidates across both models.\nTo address this, we developed an optimized kernel for Huawei's Ascend NPU. CoLD\nachieves up to a 5.54% increase in task accuracy while reducing end-to-end\nlatency by 28% compared to greedy decoding. This work provides practical and\nefficient decoding strategies for fine-tuned LLMs in resource-constrained\nenvironments and has broad implications for applied data science in both cloud\nand on-premises settings."
                },
                "authors": [
                    {
                        "name": "Morgan Lindsay Heisler"
                    },
                    {
                        "name": "Linzi Xing"
                    },
                    {
                        "name": "Ge Shi"
                    },
                    {
                        "name": "Hanieh Sadri"
                    },
                    {
                        "name": "Gursimran Singh"
                    },
                    {
                        "name": "Weiwei Zhang"
                    },
                    {
                        "name": "Tao Ye"
                    },
                    {
                        "name": "Ying Xiong"
                    },
                    {
                        "name": "Yong Zhang"
                    },
                    {
                        "name": "Zhenan Fan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenan Fan"
                },
                "author": "Zhenan Fan",
                "arxiv_comment": "Accepted at ACM KDD 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14620v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14620v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01167v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01167v3",
                "updated": "2025-05-21T02:06:29Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    2,
                    6,
                    29,
                    2,
                    141,
                    0
                ],
                "published": "2025-04-01T20:14:35Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    20,
                    14,
                    35,
                    1,
                    91,
                    0
                ],
                "title": "Predicting Field Experiments with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predicting Field Experiments with Large Language Models"
                },
                "summary": "Large language models (LLMs) have demonstrated unprecedented emergent\ncapabilities, including content generation, translation, and simulation of\nhuman behavior. Field experiments, on the other hand, are widely employed in\nsocial studies to examine real-world human behavior through carefully designed\nmanipulations and treatments. However, field experiments are known to be\nexpensive and time consuming. Therefore, an interesting question is whether and\nhow LLMs can be utilized for field experiments. In this paper, we propose and\nevaluate an automated LLM-based framework to predict the outcomes of a field\nexperiment. Applying this framework to 276 experiments about a wide range of\nhuman behaviors drawn from renowned economics literature yields a prediction\naccuracy of 78%. Moreover, we find that the distributions of the results are\neither bimodal or highly skewed. By investigating this abnormality further, we\nidentify that field experiments related to complex social issues such as\nethnicity, social norms, and ethical dilemmas can pose significant challenges\nto the prediction performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated unprecedented emergent\ncapabilities, including content generation, translation, and simulation of\nhuman behavior. Field experiments, on the other hand, are widely employed in\nsocial studies to examine real-world human behavior through carefully designed\nmanipulations and treatments. However, field experiments are known to be\nexpensive and time consuming. Therefore, an interesting question is whether and\nhow LLMs can be utilized for field experiments. In this paper, we propose and\nevaluate an automated LLM-based framework to predict the outcomes of a field\nexperiment. Applying this framework to 276 experiments about a wide range of\nhuman behaviors drawn from renowned economics literature yields a prediction\naccuracy of 78%. Moreover, we find that the distributions of the results are\neither bimodal or highly skewed. By investigating this abnormality further, we\nidentify that field experiments related to complex social issues such as\nethnicity, social norms, and ethical dilemmas can pose significant challenges\nto the prediction performance."
                },
                "authors": [
                    {
                        "name": "Yaoyu Chen"
                    },
                    {
                        "name": "Yuheng Hu"
                    },
                    {
                        "name": "Yingda Lu"
                    }
                ],
                "author_detail": {
                    "name": "Yingda Lu"
                },
                "author": "Yingda Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01167v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01167v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07482v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07482v2",
                "updated": "2025-05-20T17:09:53Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    17,
                    9,
                    53,
                    1,
                    140,
                    0
                ],
                "published": "2025-01-13T16:58:32Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    16,
                    58,
                    32,
                    0,
                    13,
                    0
                ],
                "title": "TiEBe: Tracking Language Model Recall of Notable Worldwide Events\n  Through Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TiEBe: Tracking Language Model Recall of Notable Worldwide Events\n  Through Time"
                },
                "summary": "As the knowledge landscape evolves and large language models (LLMs) become\nincreasingly widespread, there is a growing need to keep these models updated\nwith current events. While existing benchmarks assess general factual recall,\nfew studies explore how LLMs retain knowledge over time or across different\nregions. To address these gaps, we present the Timely Events Benchmark (TiEBe),\na dataset of over 23,000 question-answer pairs centered on notable global and\nregional events, spanning more than 10 years of events, 23 regions, and 13\nlanguages. TiEBe leverages structured retrospective data from Wikipedia to\nidentify notable events through time. These events are then used to construct a\nbenchmark to evaluate LLMs' understanding of global and regional developments,\ngrounded in factual evidence beyond Wikipedia itself. Our results reveal\nsignificant geographic disparities in factual recall, emphasizing the need for\nmore balanced global representation in LLM training. We also observe a Pearson\ncorrelation of more than 0.7 between models' performance in TiEBe and various\ncountries' socioeconomic indicators, such as HDI. In addition, we examine the\nimpact of language on factual recall by posing questions in the native language\nof the region where each event occurred, uncovering substantial performance\ngaps for low-resource languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the knowledge landscape evolves and large language models (LLMs) become\nincreasingly widespread, there is a growing need to keep these models updated\nwith current events. While existing benchmarks assess general factual recall,\nfew studies explore how LLMs retain knowledge over time or across different\nregions. To address these gaps, we present the Timely Events Benchmark (TiEBe),\na dataset of over 23,000 question-answer pairs centered on notable global and\nregional events, spanning more than 10 years of events, 23 regions, and 13\nlanguages. TiEBe leverages structured retrospective data from Wikipedia to\nidentify notable events through time. These events are then used to construct a\nbenchmark to evaluate LLMs' understanding of global and regional developments,\ngrounded in factual evidence beyond Wikipedia itself. Our results reveal\nsignificant geographic disparities in factual recall, emphasizing the need for\nmore balanced global representation in LLM training. We also observe a Pearson\ncorrelation of more than 0.7 between models' performance in TiEBe and various\ncountries' socioeconomic indicators, such as HDI. In addition, we examine the\nimpact of language on factual recall by posing questions in the native language\nof the region where each event occurred, uncovering substantial performance\ngaps for low-resource languages."
                },
                "authors": [
                    {
                        "name": "Thales Sales Almeida"
                    },
                    {
                        "name": "Giovana Kerche Bonás"
                    },
                    {
                        "name": "João Guilherme Alves Santos"
                    },
                    {
                        "name": "Hugo Abonizio"
                    },
                    {
                        "name": "Rodrigo Nogueira"
                    }
                ],
                "author_detail": {
                    "name": "Rodrigo Nogueira"
                },
                "author": "Rodrigo Nogueira",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07482v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07482v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14617v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14617v1",
                "updated": "2025-05-20T17:03:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    17,
                    3,
                    12,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T17:03:12Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    17,
                    3,
                    12,
                    1,
                    140,
                    0
                ],
                "title": "Linear Control of Test Awareness Reveals Differential Compliance in\n  Reasoning Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linear Control of Test Awareness Reveals Differential Compliance in\n  Reasoning Models"
                },
                "summary": "Reasoning-focused large language models (LLMs) sometimes alter their behavior\nwhen they detect that they are being evaluated, an effect analogous to the\nHawthorne phenomenon, which can lead them to optimize for test-passing\nperformance or to comply more readily with harmful prompts if real-world\nconsequences appear absent. We present the first quantitative study of how such\n\"test awareness\" impacts model behavior, particularly its safety alignment. We\nintroduce a white-box probing framework that (i) linearly identifies\nawareness-related activations and (ii) steers models toward or away from test\nawareness while monitoring downstream performance. We apply our method to\ndifferent state-of-the-art open-source reasoning LLMs across both realistic and\nhypothetical tasks. Our results demonstrate that test awareness significantly\nimpact safety alignment, and is different for different models. By providing\nfine-grained control over this latent effect, our work aims to increase trust\nin how we perform safety evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning-focused large language models (LLMs) sometimes alter their behavior\nwhen they detect that they are being evaluated, an effect analogous to the\nHawthorne phenomenon, which can lead them to optimize for test-passing\nperformance or to comply more readily with harmful prompts if real-world\nconsequences appear absent. We present the first quantitative study of how such\n\"test awareness\" impacts model behavior, particularly its safety alignment. We\nintroduce a white-box probing framework that (i) linearly identifies\nawareness-related activations and (ii) steers models toward or away from test\nawareness while monitoring downstream performance. We apply our method to\ndifferent state-of-the-art open-source reasoning LLMs across both realistic and\nhypothetical tasks. Our results demonstrate that test awareness significantly\nimpact safety alignment, and is different for different models. By providing\nfine-grained control over this latent effect, our work aims to increase trust\nin how we perform safety evaluation."
                },
                "authors": [
                    {
                        "name": "Sahar Abdelnabi"
                    },
                    {
                        "name": "Ahmed Salem"
                    }
                ],
                "author_detail": {
                    "name": "Ahmed Salem"
                },
                "author": "Ahmed Salem",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14617v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14617v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14615v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14615v1",
                "updated": "2025-05-20T17:00:22Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    17,
                    0,
                    22,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T17:00:22Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    17,
                    0,
                    22,
                    1,
                    140,
                    0
                ],
                "title": "SATBench: Benchmarking LLMs' Logical Reasoning via Automated Puzzle\n  Generation from SAT Formulas",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SATBench: Benchmarking LLMs' Logical Reasoning via Automated Puzzle\n  Generation from SAT Formulas"
                },
                "summary": "We introduce SATBench, a benchmark for evaluating the logical reasoning\ncapabilities of large language models (LLMs) through logical puzzles derived\nfrom Boolean satisfiability (SAT) problems. Unlike prior work that focuses on\ninference rule-based reasoning, which often involves deducing conclusions from\na set of premises, our approach leverages the search-based nature of SAT\nproblems, where the objective is to find a solution that fulfills a specified\nset of logical constraints. Each instance in SATBench is generated from a SAT\nformula, then translated into a story context and conditions using LLMs. The\ngeneration process is fully automated and allows for adjustable difficulty by\nvarying the number of clauses. All 2100 puzzles are validated through both\nLLM-assisted and solver-based consistency checks, with human validation on a\nsubset. Experimental results show that even the strongest model, o4-mini,\nachieves only 65.0% accuracy on hard UNSAT problems, close to the random\nbaseline of 50%. SATBench exposes fundamental limitations in the search-based\nlogical reasoning abilities of current LLMs and provides a scalable testbed for\nfuture research in logical reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce SATBench, a benchmark for evaluating the logical reasoning\ncapabilities of large language models (LLMs) through logical puzzles derived\nfrom Boolean satisfiability (SAT) problems. Unlike prior work that focuses on\ninference rule-based reasoning, which often involves deducing conclusions from\na set of premises, our approach leverages the search-based nature of SAT\nproblems, where the objective is to find a solution that fulfills a specified\nset of logical constraints. Each instance in SATBench is generated from a SAT\nformula, then translated into a story context and conditions using LLMs. The\ngeneration process is fully automated and allows for adjustable difficulty by\nvarying the number of clauses. All 2100 puzzles are validated through both\nLLM-assisted and solver-based consistency checks, with human validation on a\nsubset. Experimental results show that even the strongest model, o4-mini,\nachieves only 65.0% accuracy on hard UNSAT problems, close to the random\nbaseline of 50%. SATBench exposes fundamental limitations in the search-based\nlogical reasoning abilities of current LLMs and provides a scalable testbed for\nfuture research in logical reasoning."
                },
                "authors": [
                    {
                        "name": "Anjiang Wei"
                    },
                    {
                        "name": "Yuheng Wu"
                    },
                    {
                        "name": "Yingjia Wan"
                    },
                    {
                        "name": "Tarun Suresh"
                    },
                    {
                        "name": "Huanmi Tan"
                    },
                    {
                        "name": "Zhanke Zhou"
                    },
                    {
                        "name": "Sanmi Koyejo"
                    },
                    {
                        "name": "Ke Wang"
                    },
                    {
                        "name": "Alex Aiken"
                    }
                ],
                "author_detail": {
                    "name": "Alex Aiken"
                },
                "author": "Alex Aiken",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14615v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14615v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14613v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14613v1",
                "updated": "2025-05-20T16:59:24Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    16,
                    59,
                    24,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T16:59:24Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    16,
                    59,
                    24,
                    1,
                    140,
                    0
                ],
                "title": "Virtual Cells: Predict, Explain, Discover",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Virtual Cells: Predict, Explain, Discover"
                },
                "summary": "Drug discovery is fundamentally a process of inferring the effects of\ntreatments on patients, and would therefore benefit immensely from\ncomputational models that can reliably simulate patient responses, enabling\nresearchers to generate and test large numbers of therapeutic hypotheses safely\nand economically before initiating costly clinical trials. Even a more specific\nmodel that predicts the functional response of cells to a wide range of\nperturbations would be tremendously valuable for discovering safe and effective\ntreatments that successfully translate to the clinic. Creating such virtual\ncells has long been a goal of the computational research community that\nunfortunately remains unachieved given the daunting complexity and scale of\ncellular biology. Nevertheless, recent advances in AI, computing power, lab\nautomation, and high-throughput cellular profiling provide new opportunities\nfor reaching this goal. In this perspective, we present a vision for developing\nand evaluating virtual cells that builds on our experience at Recursion. We\nargue that in order to be a useful tool to discover novel biology, virtual\ncells must accurately predict the functional response of a cell to\nperturbations and explain how the predicted response is a consequence of\nmodifications to key biomolecular interactions. We then introduce key\nprinciples for designing therapeutically-relevant virtual cells, describe a\nlab-in-the-loop approach for generating novel insights with them, and advocate\nfor biologically-grounded benchmarks to guide virtual cell development.\nFinally, we make the case that our approach to virtual cells provides a useful\nframework for building other models at higher levels of organization, including\nvirtual patients. We hope that these directions prove useful to the research\ncommunity in developing virtual models optimized for positive impact on drug\ndiscovery outcomes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Drug discovery is fundamentally a process of inferring the effects of\ntreatments on patients, and would therefore benefit immensely from\ncomputational models that can reliably simulate patient responses, enabling\nresearchers to generate and test large numbers of therapeutic hypotheses safely\nand economically before initiating costly clinical trials. Even a more specific\nmodel that predicts the functional response of cells to a wide range of\nperturbations would be tremendously valuable for discovering safe and effective\ntreatments that successfully translate to the clinic. Creating such virtual\ncells has long been a goal of the computational research community that\nunfortunately remains unachieved given the daunting complexity and scale of\ncellular biology. Nevertheless, recent advances in AI, computing power, lab\nautomation, and high-throughput cellular profiling provide new opportunities\nfor reaching this goal. In this perspective, we present a vision for developing\nand evaluating virtual cells that builds on our experience at Recursion. We\nargue that in order to be a useful tool to discover novel biology, virtual\ncells must accurately predict the functional response of a cell to\nperturbations and explain how the predicted response is a consequence of\nmodifications to key biomolecular interactions. We then introduce key\nprinciples for designing therapeutically-relevant virtual cells, describe a\nlab-in-the-loop approach for generating novel insights with them, and advocate\nfor biologically-grounded benchmarks to guide virtual cell development.\nFinally, we make the case that our approach to virtual cells provides a useful\nframework for building other models at higher levels of organization, including\nvirtual patients. We hope that these directions prove useful to the research\ncommunity in developing virtual models optimized for positive impact on drug\ndiscovery outcomes."
                },
                "authors": [
                    {
                        "name": "Emmanuel Noutahi"
                    },
                    {
                        "name": "Jason Hartford"
                    },
                    {
                        "name": "Prudencio Tossou"
                    },
                    {
                        "name": "Shawn Whitfield"
                    },
                    {
                        "name": "Alisandra K. Denton"
                    },
                    {
                        "name": "Cas Wognum"
                    },
                    {
                        "name": "Kristina Ulicna"
                    },
                    {
                        "name": "Jonathan Hsu"
                    },
                    {
                        "name": "Michael Cuccarese"
                    },
                    {
                        "name": "Emmanuel Bengio"
                    },
                    {
                        "name": "Dominique Beaini"
                    },
                    {
                        "name": "Christopher Gibson"
                    },
                    {
                        "name": "Daniel Cohen"
                    },
                    {
                        "name": "Berton Earnshaw"
                    }
                ],
                "author_detail": {
                    "name": "Berton Earnshaw"
                },
                "author": "Berton Earnshaw",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14613v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14613v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14607v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14607v1",
                "updated": "2025-05-20T16:54:34Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    16,
                    54,
                    34,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T16:54:34Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    16,
                    54,
                    34,
                    1,
                    140,
                    0
                ],
                "title": "sudoLLM : On Multi-role Alignment of Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "sudoLLM : On Multi-role Alignment of Language Models"
                },
                "summary": "User authorization-based access privileges are a key feature in many\nsafety-critical systems, but have thus far been absent from the large language\nmodel (LLM) realm. In this work, drawing inspiration from such access control\nsystems, we introduce sudoLLM, a novel framework that results in multi-role\naligned LLMs, i.e., LLMs that account for, and behave in accordance with, user\naccess rights. sudoLLM injects subtle user-based biases into queries and trains\nan LLM to utilize this bias signal in order to produce sensitive information if\nand only if the user is authorized. We present empirical results demonstrating\nthat this approach shows substantially improved alignment, generalization, and\nresistance to prompt-based jailbreaking attacks. The persistent tension between\nthe language modeling objective and safety alignment, which is often exploited\nto jailbreak LLMs, is somewhat resolved with the aid of the injected bias\nsignal. Our framework is meant as an additional security layer, and complements\nexisting guardrail mechanisms for enhanced end-to-end safety with LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "User authorization-based access privileges are a key feature in many\nsafety-critical systems, but have thus far been absent from the large language\nmodel (LLM) realm. In this work, drawing inspiration from such access control\nsystems, we introduce sudoLLM, a novel framework that results in multi-role\naligned LLMs, i.e., LLMs that account for, and behave in accordance with, user\naccess rights. sudoLLM injects subtle user-based biases into queries and trains\nan LLM to utilize this bias signal in order to produce sensitive information if\nand only if the user is authorized. We present empirical results demonstrating\nthat this approach shows substantially improved alignment, generalization, and\nresistance to prompt-based jailbreaking attacks. The persistent tension between\nthe language modeling objective and safety alignment, which is often exploited\nto jailbreak LLMs, is somewhat resolved with the aid of the injected bias\nsignal. Our framework is meant as an additional security layer, and complements\nexisting guardrail mechanisms for enhanced end-to-end safety with LLMs."
                },
                "authors": [
                    {
                        "name": "Soumadeep Saha"
                    },
                    {
                        "name": "Akshay Chaturvedi"
                    },
                    {
                        "name": "Joy Mahapatra"
                    },
                    {
                        "name": "Utpal Garain"
                    }
                ],
                "author_detail": {
                    "name": "Utpal Garain"
                },
                "author": "Utpal Garain",
                "arxiv_comment": "Under review. Code and data to be released later",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14607v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14607v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14604v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14604v1",
                "updated": "2025-05-20T16:53:40Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    16,
                    53,
                    40,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T16:53:40Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    16,
                    53,
                    40,
                    1,
                    140,
                    0
                ],
                "title": "Let LLMs Break Free from Overthinking via Self-Braking Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Let LLMs Break Free from Overthinking via Self-Braking Tuning"
                },
                "summary": "Large reasoning models (LRMs), such as OpenAI o1 and DeepSeek-R1, have\nsignificantly enhanced their reasoning capabilities by generating longer chains\nof thought, demonstrating outstanding performance across a variety of tasks.\nHowever, this performance gain comes at the cost of a substantial increase in\nredundant reasoning during the generation process, leading to high\ncomputational overhead and exacerbating the issue of overthinking. Although\nnumerous existing approaches aim to address the problem of overthinking, they\noften rely on external interventions. In this paper, we propose a novel\nframework, Self-Braking Tuning (SBT), which tackles overthinking from the\nperspective of allowing the model to regulate its own reasoning process, thus\neliminating the reliance on external control mechanisms. We construct a set of\noverthinking identification metrics based on standard answers and design a\nsystematic method to detect redundant reasoning. This method accurately\nidentifies unnecessary steps within the reasoning trajectory and generates\ntraining signals for learning self-regulation behaviors. Building on this\nfoundation, we develop a complete strategy for constructing data with adaptive\nreasoning lengths and introduce an innovative braking prompt mechanism that\nenables the model to naturally learn when to terminate reasoning at an\nappropriate point. Experiments across mathematical benchmarks (AIME, AMC,\nMATH500, GSM8K) demonstrate that our method reduces token consumption by up to\n60% while maintaining comparable accuracy to unconstrained models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large reasoning models (LRMs), such as OpenAI o1 and DeepSeek-R1, have\nsignificantly enhanced their reasoning capabilities by generating longer chains\nof thought, demonstrating outstanding performance across a variety of tasks.\nHowever, this performance gain comes at the cost of a substantial increase in\nredundant reasoning during the generation process, leading to high\ncomputational overhead and exacerbating the issue of overthinking. Although\nnumerous existing approaches aim to address the problem of overthinking, they\noften rely on external interventions. In this paper, we propose a novel\nframework, Self-Braking Tuning (SBT), which tackles overthinking from the\nperspective of allowing the model to regulate its own reasoning process, thus\neliminating the reliance on external control mechanisms. We construct a set of\noverthinking identification metrics based on standard answers and design a\nsystematic method to detect redundant reasoning. This method accurately\nidentifies unnecessary steps within the reasoning trajectory and generates\ntraining signals for learning self-regulation behaviors. Building on this\nfoundation, we develop a complete strategy for constructing data with adaptive\nreasoning lengths and introduce an innovative braking prompt mechanism that\nenables the model to naturally learn when to terminate reasoning at an\nappropriate point. Experiments across mathematical benchmarks (AIME, AMC,\nMATH500, GSM8K) demonstrate that our method reduces token consumption by up to\n60% while maintaining comparable accuracy to unconstrained models."
                },
                "authors": [
                    {
                        "name": "Haoran Zhao"
                    },
                    {
                        "name": "Yuchen Yan"
                    },
                    {
                        "name": "Yongliang Shen"
                    },
                    {
                        "name": "Haolei Xu"
                    },
                    {
                        "name": "Wenqi Zhang"
                    },
                    {
                        "name": "Kaitao Song"
                    },
                    {
                        "name": "Jian Shao"
                    },
                    {
                        "name": "Weiming Lu"
                    },
                    {
                        "name": "Jun Xiao"
                    },
                    {
                        "name": "Yueting Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Yueting Zhuang"
                },
                "author": "Yueting Zhuang",
                "arxiv_comment": "Github:https://github.com/CCAI-Lab/Self-Braking-Tuning; Project:\n  https://CCAI-Lab.github.io/SBT",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14604v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14604v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2212.07383v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2212.07383v4",
                "updated": "2025-05-20T16:51:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    16,
                    51,
                    57,
                    1,
                    140,
                    0
                ],
                "published": "2022-12-14T18:08:42Z",
                "published_parsed": [
                    2022,
                    12,
                    14,
                    18,
                    8,
                    42,
                    2,
                    348,
                    0
                ],
                "title": "Sequential Kernelized Independence Testing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequential Kernelized Independence Testing"
                },
                "summary": "Independence testing is a classical statistical problem that has been\nextensively studied in the batch setting when one fixes the sample size before\ncollecting data. However, practitioners often prefer procedures that adapt to\nthe complexity of a problem at hand instead of setting sample size in advance.\nIdeally, such procedures should (a) stop earlier on easy tasks (and later on\nharder tasks), hence making better use of available resources, and (b)\ncontinuously monitor the data and efficiently incorporate statistical evidence\nafter collecting new data, while controlling the false alarm rate. Classical\nbatch tests are not tailored for streaming data: valid inference after data\npeeking requires correcting for multiple testing which results in low power.\nFollowing the principle of testing by betting, we design sequential kernelized\nindependence tests that overcome such shortcomings. We exemplify our broad\nframework using bets inspired by kernelized dependence measures, e.g., the\nHilbert-Schmidt independence criterion. Our test is also valid under\nnon-i.i.d., time-varying settings. We demonstrate the power of our approaches\non both simulated and real data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Independence testing is a classical statistical problem that has been\nextensively studied in the batch setting when one fixes the sample size before\ncollecting data. However, practitioners often prefer procedures that adapt to\nthe complexity of a problem at hand instead of setting sample size in advance.\nIdeally, such procedures should (a) stop earlier on easy tasks (and later on\nharder tasks), hence making better use of available resources, and (b)\ncontinuously monitor the data and efficiently incorporate statistical evidence\nafter collecting new data, while controlling the false alarm rate. Classical\nbatch tests are not tailored for streaming data: valid inference after data\npeeking requires correcting for multiple testing which results in low power.\nFollowing the principle of testing by betting, we design sequential kernelized\nindependence tests that overcome such shortcomings. We exemplify our broad\nframework using bets inspired by kernelized dependence measures, e.g., the\nHilbert-Schmidt independence criterion. Our test is also valid under\nnon-i.i.d., time-varying settings. We demonstrate the power of our approaches\non both simulated and real data."
                },
                "authors": [
                    {
                        "name": "Aleksandr Podkopaev"
                    },
                    {
                        "name": "Patrick Blöbaum"
                    },
                    {
                        "name": "Shiva Prasad Kasiviswanathan"
                    },
                    {
                        "name": "Aaditya Ramdas"
                    }
                ],
                "author_detail": {
                    "name": "Aaditya Ramdas"
                },
                "author": "Aaditya Ramdas",
                "arxiv_comment": "ICML 2023",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2212.07383v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2212.07383v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14600v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14600v1",
                "updated": "2025-05-20T16:50:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    16,
                    50,
                    21,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T16:50:21Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    16,
                    50,
                    21,
                    1,
                    140,
                    0
                ],
                "title": "AdaKWS: Towards Robust Keyword Spotting with Test-Time Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaKWS: Towards Robust Keyword Spotting with Test-Time Adaptation"
                },
                "summary": "Spoken keyword spotting (KWS) aims to identify keywords in audio for wide\napplications, especially on edge devices. Current small-footprint KWS systems\nfocus on efficient model designs. However, their inference performance can\ndecline in unseen environments or noisy backgrounds. Test-time adaptation (TTA)\nhelps models adapt to test samples without needing the original training data.\nIn this study, we present AdaKWS, the first TTA method for robust KWS to the\nbest of our knowledge. Specifically, 1) We initially optimize the model's\nconfidence by selecting reliable samples based on prediction entropy\nminimization and adjusting the normalization statistics in each batch. 2) We\nintroduce pseudo-keyword consistency (PKC) to identify critical, reliable\nfeatures without overfitting to noise. Our experiments show that AdaKWS\noutperforms other methods across various conditions, including Gaussian noise\nand real-scenario noises. The code will be released in due course.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spoken keyword spotting (KWS) aims to identify keywords in audio for wide\napplications, especially on edge devices. Current small-footprint KWS systems\nfocus on efficient model designs. However, their inference performance can\ndecline in unseen environments or noisy backgrounds. Test-time adaptation (TTA)\nhelps models adapt to test samples without needing the original training data.\nIn this study, we present AdaKWS, the first TTA method for robust KWS to the\nbest of our knowledge. Specifically, 1) We initially optimize the model's\nconfidence by selecting reliable samples based on prediction entropy\nminimization and adjusting the normalization statistics in each batch. 2) We\nintroduce pseudo-keyword consistency (PKC) to identify critical, reliable\nfeatures without overfitting to noise. Our experiments show that AdaKWS\noutperforms other methods across various conditions, including Gaussian noise\nand real-scenario noises. The code will be released in due course."
                },
                "authors": [
                    {
                        "name": "Yang Xiao"
                    },
                    {
                        "name": "Tianyi Peng"
                    },
                    {
                        "name": "Yanghao Zhou"
                    },
                    {
                        "name": "Rohan Kumar Das"
                    }
                ],
                "author_detail": {
                    "name": "Rohan Kumar Das"
                },
                "author": "Rohan Kumar Das",
                "arxiv_comment": "Accepted by Interspeech 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14600v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14600v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14599v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14599v1",
                "updated": "2025-05-20T16:49:40Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    16,
                    49,
                    40,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T16:49:40Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    16,
                    49,
                    40,
                    1,
                    140,
                    0
                ],
                "title": "Toward Reliable Biomedical Hypothesis Generation: Evaluating\n  Truthfulness and Hallucination in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward Reliable Biomedical Hypothesis Generation: Evaluating\n  Truthfulness and Hallucination in Large Language Models"
                },
                "summary": "Large language models (LLMs) have shown significant potential in scientific\ndisciplines such as biomedicine, particularly in hypothesis generation, where\nthey can analyze vast literature, identify patterns, and suggest research\ndirections. However, a key challenge lies in evaluating the truthfulness of\ngenerated hypotheses, as verifying their accuracy often requires substantial\ntime and resources. Additionally, the hallucination problem in LLMs can lead to\nthe generation of hypotheses that appear plausible but are ultimately\nincorrect, undermining their reliability. To facilitate the systematic study of\nthese challenges, we introduce TruthHypo, a benchmark for assessing the\ncapabilities of LLMs in generating truthful biomedical hypotheses, and KnowHD,\na knowledge-based hallucination detector to evaluate how well hypotheses are\ngrounded in existing knowledge. Our results show that LLMs struggle to generate\ntruthful hypotheses. By analyzing hallucinations in reasoning steps, we\ndemonstrate that the groundedness scores provided by KnowHD serve as an\neffective metric for filtering truthful hypotheses from the diverse outputs of\nLLMs. Human evaluations further validate the utility of KnowHD in identifying\ntruthful hypotheses and accelerating scientific discovery. Our data and source\ncode are available at https://github.com/Teddy-XiongGZ/TruthHypo.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown significant potential in scientific\ndisciplines such as biomedicine, particularly in hypothesis generation, where\nthey can analyze vast literature, identify patterns, and suggest research\ndirections. However, a key challenge lies in evaluating the truthfulness of\ngenerated hypotheses, as verifying their accuracy often requires substantial\ntime and resources. Additionally, the hallucination problem in LLMs can lead to\nthe generation of hypotheses that appear plausible but are ultimately\nincorrect, undermining their reliability. To facilitate the systematic study of\nthese challenges, we introduce TruthHypo, a benchmark for assessing the\ncapabilities of LLMs in generating truthful biomedical hypotheses, and KnowHD,\na knowledge-based hallucination detector to evaluate how well hypotheses are\ngrounded in existing knowledge. Our results show that LLMs struggle to generate\ntruthful hypotheses. By analyzing hallucinations in reasoning steps, we\ndemonstrate that the groundedness scores provided by KnowHD serve as an\neffective metric for filtering truthful hypotheses from the diverse outputs of\nLLMs. Human evaluations further validate the utility of KnowHD in identifying\ntruthful hypotheses and accelerating scientific discovery. Our data and source\ncode are available at https://github.com/Teddy-XiongGZ/TruthHypo."
                },
                "authors": [
                    {
                        "name": "Guangzhi Xiong"
                    },
                    {
                        "name": "Eric Xie"
                    },
                    {
                        "name": "Corey Williams"
                    },
                    {
                        "name": "Myles Kim"
                    },
                    {
                        "name": "Amir Hassan Shariatmadari"
                    },
                    {
                        "name": "Sikun Guo"
                    },
                    {
                        "name": "Stefan Bekiranov"
                    },
                    {
                        "name": "Aidong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Aidong Zhang"
                },
                "author": "Aidong Zhang",
                "arxiv_comment": "Accepted to IJCAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14599v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14599v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15241v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15241v2",
                "updated": "2025-05-20T16:49:30Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    16,
                    49,
                    30,
                    1,
                    140,
                    0
                ],
                "published": "2025-04-21T17:15:06Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    17,
                    15,
                    6,
                    0,
                    111,
                    0
                ],
                "title": "MrGuard: A Multilingual Reasoning Guardrail for Universal LLM Safety",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MrGuard: A Multilingual Reasoning Guardrail for Universal LLM Safety"
                },
                "summary": "Large Language Models (LLMs) are susceptible to adversarial attacks such as\njailbreaking, which can elicit harmful or unsafe behaviors. This vulnerability\nis exacerbated in multilingual settings, where multilingual safety-aligned data\nis often limited. Thus, developing a guardrail capable of detecting and\nfiltering unsafe content across diverse languages is critical for deploying\nLLMs in real-world applications. In this work, we introduce a multilingual\nguardrail with reasoning for prompt classification. Our method consists of: (1)\nsynthetic multilingual data generation incorporating culturally and\nlinguistically nuanced variants, (2) supervised fine-tuning, and (3) a\ncurriculum-based Group Relative Policy Optimization (GRPO) framework that\nfurther improves performance. Experimental results demonstrate that our\nmultilingual guardrail, MrGuard, consistently outperforms recent baselines\nacross both in-domain and out-of-domain languages by more than 15%. We also\nevaluate MrGuard's robustness to multilingual variations, such as\ncode-switching and low-resource language distractors in the prompt, and\ndemonstrate that it preserves safety judgments under these challenging\nconditions. The multilingual reasoning capability of our guardrail enables it\nto generate explanations, which are particularly useful for understanding\nlanguage-specific risks and ambiguities in multilingual content moderation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are susceptible to adversarial attacks such as\njailbreaking, which can elicit harmful or unsafe behaviors. This vulnerability\nis exacerbated in multilingual settings, where multilingual safety-aligned data\nis often limited. Thus, developing a guardrail capable of detecting and\nfiltering unsafe content across diverse languages is critical for deploying\nLLMs in real-world applications. In this work, we introduce a multilingual\nguardrail with reasoning for prompt classification. Our method consists of: (1)\nsynthetic multilingual data generation incorporating culturally and\nlinguistically nuanced variants, (2) supervised fine-tuning, and (3) a\ncurriculum-based Group Relative Policy Optimization (GRPO) framework that\nfurther improves performance. Experimental results demonstrate that our\nmultilingual guardrail, MrGuard, consistently outperforms recent baselines\nacross both in-domain and out-of-domain languages by more than 15%. We also\nevaluate MrGuard's robustness to multilingual variations, such as\ncode-switching and low-resource language distractors in the prompt, and\ndemonstrate that it preserves safety judgments under these challenging\nconditions. The multilingual reasoning capability of our guardrail enables it\nto generate explanations, which are particularly useful for understanding\nlanguage-specific risks and ambiguities in multilingual content moderation."
                },
                "authors": [
                    {
                        "name": "Yahan Yang"
                    },
                    {
                        "name": "Soham Dan"
                    },
                    {
                        "name": "Shuo Li"
                    },
                    {
                        "name": "Dan Roth"
                    },
                    {
                        "name": "Insup Lee"
                    }
                ],
                "author_detail": {
                    "name": "Insup Lee"
                },
                "author": "Insup Lee",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15241v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15241v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14597v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14597v1",
                "updated": "2025-05-20T16:48:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    16,
                    48,
                    57,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T16:48:57Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    16,
                    48,
                    57,
                    1,
                    140,
                    0
                ],
                "title": "Success is in the Details: Evaluate and Enhance Details Sensitivity of\n  Code LLMs through Counterfactuals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Success is in the Details: Evaluate and Enhance Details Sensitivity of\n  Code LLMs through Counterfactuals"
                },
                "summary": "Code Sensitivity refers to the ability of Code LLMs to recognize and respond\nto details changes in problem descriptions. While current code benchmarks and\ninstruction data focus on difficulty and diversity, sensitivity is overlooked.\nWe first introduce the CTF-Code benchmark, constructed using counterfactual\nperturbations, minimizing input changes while maximizing output changes. The\nevaluation shows that many LLMs have a more than 10\\% performance drop compared\nto the original problems. To fully utilize sensitivity, CTF-Instruct, an\nincremental instruction fine-tuning framework, extends on existing data and\nuses a selection mechanism to meet the three dimensions of difficulty,\ndiversity, and sensitivity. Experiments show that LLMs fine-tuned with\nCTF-Instruct data achieve over a 2\\% improvement on CTF-Code, and more than a\n10\\% performance boost on LiveCodeBench, validating the feasibility of\nenhancing LLMs' sensitivity to improve performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code Sensitivity refers to the ability of Code LLMs to recognize and respond\nto details changes in problem descriptions. While current code benchmarks and\ninstruction data focus on difficulty and diversity, sensitivity is overlooked.\nWe first introduce the CTF-Code benchmark, constructed using counterfactual\nperturbations, minimizing input changes while maximizing output changes. The\nevaluation shows that many LLMs have a more than 10\\% performance drop compared\nto the original problems. To fully utilize sensitivity, CTF-Instruct, an\nincremental instruction fine-tuning framework, extends on existing data and\nuses a selection mechanism to meet the three dimensions of difficulty,\ndiversity, and sensitivity. Experiments show that LLMs fine-tuned with\nCTF-Instruct data achieve over a 2\\% improvement on CTF-Code, and more than a\n10\\% performance boost on LiveCodeBench, validating the feasibility of\nenhancing LLMs' sensitivity to improve performance."
                },
                "authors": [
                    {
                        "name": "Xianzhen Luo"
                    },
                    {
                        "name": "Qingfu Zhu"
                    },
                    {
                        "name": "Zhiming Zhang"
                    },
                    {
                        "name": "Mingzheng Xu"
                    },
                    {
                        "name": "Tianhao Cheng"
                    },
                    {
                        "name": "Yixuan Wang"
                    },
                    {
                        "name": "Zheng Chu"
                    },
                    {
                        "name": "Shijie Xuyang"
                    },
                    {
                        "name": "Zhiyuan Ma"
                    },
                    {
                        "name": "YuanTao Fan"
                    },
                    {
                        "name": "Wanxiang Che"
                    }
                ],
                "author_detail": {
                    "name": "Wanxiang Che"
                },
                "author": "Wanxiang Che",
                "arxiv_comment": "Code & Model is https://github.com/Luowaterbi/CTF-Instruct",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14597v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14597v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14592v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14592v1",
                "updated": "2025-05-20T16:45:54Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    16,
                    45,
                    54,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T16:45:54Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    16,
                    45,
                    54,
                    1,
                    140,
                    0
                ],
                "title": "Adaptive Pruning of Deep Neural Networks for Resource-Aware Embedded\n  Intrusion Detection on the Edge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Pruning of Deep Neural Networks for Resource-Aware Embedded\n  Intrusion Detection on the Edge"
                },
                "summary": "Artificial neural network pruning is a method in which artificial neural\nnetwork sizes can be reduced while attempting to preserve the predicting\ncapabilities of the network. This is done to make the model smaller or faster\nduring inference time. In this work we analyze the ability of a selection of\nartificial neural network pruning methods to generalize to a new cybersecurity\ndataset utilizing a simpler network type than was designed for. We analyze each\nmethod using a variety of pruning degrees to best understand how each algorithm\nresponds to the new environment. This has allowed us to determine the most well\nfit pruning method of those we searched for the task. Unexpectedly, we have\nfound that many of them do not generalize to the problem well, leaving only a\nfew algorithms working to an acceptable degree.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial neural network pruning is a method in which artificial neural\nnetwork sizes can be reduced while attempting to preserve the predicting\ncapabilities of the network. This is done to make the model smaller or faster\nduring inference time. In this work we analyze the ability of a selection of\nartificial neural network pruning methods to generalize to a new cybersecurity\ndataset utilizing a simpler network type than was designed for. We analyze each\nmethod using a variety of pruning degrees to best understand how each algorithm\nresponds to the new environment. This has allowed us to determine the most well\nfit pruning method of those we searched for the task. Unexpectedly, we have\nfound that many of them do not generalize to the problem well, leaving only a\nfew algorithms working to an acceptable degree."
                },
                "authors": [
                    {
                        "name": "Alexandre Broggi"
                    },
                    {
                        "name": "Nathaniel Bastian"
                    },
                    {
                        "name": "Lance Fiondella"
                    },
                    {
                        "name": "Gokhan Kul"
                    }
                ],
                "author_detail": {
                    "name": "Gokhan Kul"
                },
                "author": "Gokhan Kul",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14592v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14592v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16901v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16901v2",
                "updated": "2025-05-20T16:45:00Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    16,
                    45,
                    0,
                    1,
                    140,
                    0
                ],
                "published": "2025-02-24T06:54:50Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    6,
                    54,
                    50,
                    0,
                    55,
                    0
                ],
                "title": "Char-mander Use mBackdoor! A Study of Cross-lingual Backdoor Attacks in\n  Multilingual LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Char-mander Use mBackdoor! A Study of Cross-lingual Backdoor Attacks in\n  Multilingual LLMs"
                },
                "summary": "We explore \\textbf{C}ross-lingual \\textbf{B}ackdoor \\textbf{AT}tacks (X-BAT)\nin multilingual Large Language Models (mLLMs), revealing how backdoors inserted\nin one language can automatically transfer to others through shared embedding\nspaces. Using toxicity classification as a case study, we demonstrate that\nattackers can compromise multilingual systems by poisoning data in a single\nlanguage, with rare and high-occurring tokens serving as specific, effective\ntriggers. Our findings expose a critical vulnerability that influences the\nmodel's architecture, resulting in a concealed backdoor effect during the\ninformation flow. Our code and data are publicly available\nhttps://github.com/himanshubeniwal/X-BAT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We explore \\textbf{C}ross-lingual \\textbf{B}ackdoor \\textbf{AT}tacks (X-BAT)\nin multilingual Large Language Models (mLLMs), revealing how backdoors inserted\nin one language can automatically transfer to others through shared embedding\nspaces. Using toxicity classification as a case study, we demonstrate that\nattackers can compromise multilingual systems by poisoning data in a single\nlanguage, with rare and high-occurring tokens serving as specific, effective\ntriggers. Our findings expose a critical vulnerability that influences the\nmodel's architecture, resulting in a concealed backdoor effect during the\ninformation flow. Our code and data are publicly available\nhttps://github.com/himanshubeniwal/X-BAT."
                },
                "authors": [
                    {
                        "name": "Himanshu Beniwal"
                    },
                    {
                        "name": "Sailesh Panda"
                    },
                    {
                        "name": "Birudugadda Srivibhav"
                    },
                    {
                        "name": "Mayank Singh"
                    }
                ],
                "author_detail": {
                    "name": "Mayank Singh"
                },
                "author": "Mayank Singh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16901v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16901v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.12023v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.12023v2",
                "updated": "2025-05-20T16:42:34Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    16,
                    42,
                    34,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-17T14:27:21Z",
                "published_parsed": [
                    2025,
                    5,
                    17,
                    14,
                    27,
                    21,
                    5,
                    137,
                    0
                ],
                "title": "Model-X Change-Point Detection of Conditional Distribution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model-X Change-Point Detection of Conditional Distribution"
                },
                "summary": "The dynamic nature of many real-world systems can lead to temporal outcome\nmodel shifts, causing a deterioration in model accuracy and reliability over\ntime. This requires change-point detection on the outcome models to guide model\nretraining and adjustments. However, inferring the change point of conditional\nmodels is more prone to loss of validity or power than classic detection\nproblems for marginal distributions. This is due to both the temporal covariate\nshift and the complexity of the outcome model. To address these challenges, we\npropose a novel model-X Conditional Random Testing (CRT) method computationally\nenhanced with latent mixture model (LMM) distillation for simultaneous\nchange-point detection and localization of the conditional outcome model. Built\nupon the model-X framework, our approach can effectively adjust for the\npotential bias caused by the temporal covariate shift and allow the flexible\nuse of general machine learning methods for outcome modeling. It preserves good\nvalidity against complex or erroneous outcome models, even with imperfect\nknowledge of the temporal covariate shift learned from some auxiliary unlabeled\ndata. Moreover, the incorporation of LMM distillation significantly reduces the\ncomputational burden of the CRT by eliminating the need for repeated complex\nmodel refitting in its resampling procedure and preserves the statistical\nvalidity and power well. Theoretical validity of the proposed method is\njustified. Extensive simulation studies and a real-world example demonstrate\nthe statistical effectiveness and computational scalability of our method as\nwell as its significant improvements over existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The dynamic nature of many real-world systems can lead to temporal outcome\nmodel shifts, causing a deterioration in model accuracy and reliability over\ntime. This requires change-point detection on the outcome models to guide model\nretraining and adjustments. However, inferring the change point of conditional\nmodels is more prone to loss of validity or power than classic detection\nproblems for marginal distributions. This is due to both the temporal covariate\nshift and the complexity of the outcome model. To address these challenges, we\npropose a novel model-X Conditional Random Testing (CRT) method computationally\nenhanced with latent mixture model (LMM) distillation for simultaneous\nchange-point detection and localization of the conditional outcome model. Built\nupon the model-X framework, our approach can effectively adjust for the\npotential bias caused by the temporal covariate shift and allow the flexible\nuse of general machine learning methods for outcome modeling. It preserves good\nvalidity against complex or erroneous outcome models, even with imperfect\nknowledge of the temporal covariate shift learned from some auxiliary unlabeled\ndata. Moreover, the incorporation of LMM distillation significantly reduces the\ncomputational burden of the CRT by eliminating the need for repeated complex\nmodel refitting in its resampling procedure and preserves the statistical\nvalidity and power well. Theoretical validity of the proposed method is\njustified. Extensive simulation studies and a real-world example demonstrate\nthe statistical effectiveness and computational scalability of our method as\nwell as its significant improvements over existing methods."
                },
                "authors": [
                    {
                        "name": "Yiwen Huang"
                    },
                    {
                        "name": "Yan Dong"
                    },
                    {
                        "name": "Mengying Yan"
                    },
                    {
                        "name": "Ziye Tian"
                    },
                    {
                        "name": "Chuan Hong"
                    },
                    {
                        "name": "Doudou Zhou"
                    },
                    {
                        "name": "Molei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Molei Liu"
                },
                "author": "Molei Liu",
                "arxiv_comment": "13 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.12023v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.12023v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14590v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14590v1",
                "updated": "2025-05-20T16:41:45Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    16,
                    41,
                    45,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T16:41:45Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    16,
                    41,
                    45,
                    1,
                    140,
                    0
                ],
                "title": "MCIP: Protecting MCP Safety via Model Contextual Integrity Protocol",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MCIP: Protecting MCP Safety via Model Contextual Integrity Protocol"
                },
                "summary": "As Model Context Protocol (MCP) introduces an easy-to-use ecosystem for users\nand developers, it also brings underexplored safety risks. Its decentralized\narchitecture, which separates clients and servers, poses unique challenges for\nsystematic safety analysis. This paper proposes a novel framework to enhance\nMCP safety. Guided by the MAESTRO framework, we first analyze the missing\nsafety mechanisms in MCP, and based on this analysis, we propose the Model\nContextual Integrity Protocol (MCIP), a refined version of MCP that addresses\nthese gaps.Next, we develop a fine-grained taxonomy that captures a diverse\nrange of unsafe behaviors observed in MCP scenarios. Building on this taxonomy,\nwe develop benchmark and training data that support the evaluation and\nimprovement of LLMs' capabilities in identifying safety risks within MCP\ninteractions. Leveraging the proposed benchmark and training data, we conduct\nextensive experiments on state-of-the-art LLMs. The results highlight LLMs'\nvulnerabilities in MCP interactions and demonstrate that our approach\nsubstantially improves their safety performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Model Context Protocol (MCP) introduces an easy-to-use ecosystem for users\nand developers, it also brings underexplored safety risks. Its decentralized\narchitecture, which separates clients and servers, poses unique challenges for\nsystematic safety analysis. This paper proposes a novel framework to enhance\nMCP safety. Guided by the MAESTRO framework, we first analyze the missing\nsafety mechanisms in MCP, and based on this analysis, we propose the Model\nContextual Integrity Protocol (MCIP), a refined version of MCP that addresses\nthese gaps.Next, we develop a fine-grained taxonomy that captures a diverse\nrange of unsafe behaviors observed in MCP scenarios. Building on this taxonomy,\nwe develop benchmark and training data that support the evaluation and\nimprovement of LLMs' capabilities in identifying safety risks within MCP\ninteractions. Leveraging the proposed benchmark and training data, we conduct\nextensive experiments on state-of-the-art LLMs. The results highlight LLMs'\nvulnerabilities in MCP interactions and demonstrate that our approach\nsubstantially improves their safety performance."
                },
                "authors": [
                    {
                        "name": "Huihao Jing"
                    },
                    {
                        "name": "Haoran Li"
                    },
                    {
                        "name": "Wenbin Hu"
                    },
                    {
                        "name": "Qi Hu"
                    },
                    {
                        "name": "Heli Xu"
                    },
                    {
                        "name": "Tianshu Chu"
                    },
                    {
                        "name": "Peizhao Hu"
                    },
                    {
                        "name": "Yangqiu Song"
                    }
                ],
                "author_detail": {
                    "name": "Yangqiu Song"
                },
                "author": "Yangqiu Song",
                "arxiv_comment": "17 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14590v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14590v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14585v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14585v1",
                "updated": "2025-05-20T16:40:09Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    16,
                    40,
                    9,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T16:40:09Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    16,
                    40,
                    9,
                    1,
                    140,
                    0
                ],
                "title": "Context Reasoner: Incentivizing Reasoning Capability for Contextualized\n  Privacy and Safety Compliance via Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context Reasoner: Incentivizing Reasoning Capability for Contextualized\n  Privacy and Safety Compliance via Reinforcement Learning"
                },
                "summary": "While Large Language Models (LLMs) exhibit remarkable capabilities, they also\nintroduce significant safety and privacy risks. Current mitigation strategies\noften fail to preserve contextual reasoning capabilities in risky scenarios.\nInstead, they rely heavily on sensitive pattern matching to protect LLMs, which\nlimits the scope. Furthermore, they overlook established safety and privacy\nstandards, leading to systemic risks for legal compliance. To address these\ngaps, we formulate safety and privacy issues into contextualized compliance\nproblems following the Contextual Integrity (CI) theory. Under the CI\nframework, we align our model with three critical regulatory standards: GDPR,\nEU AI Act, and HIPAA. Specifically, we employ reinforcement learning (RL) with\na rule-based reward to incentivize contextual reasoning capabilities while\nenhancing compliance with safety and privacy norms. Through extensive\nexperiments, we demonstrate that our method not only significantly enhances\nlegal compliance (achieving a +17.64% accuracy improvement in safety/privacy\nbenchmarks) but also further improves general reasoning capability. For\nOpenThinker-7B, a strong reasoning model that significantly outperforms its\nbase model Qwen2.5-7B-Instruct across diverse subjects, our method enhances its\ngeneral reasoning capabilities, with +2.05% and +8.98% accuracy improvement on\nthe MMLU and LegalBench benchmark, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Large Language Models (LLMs) exhibit remarkable capabilities, they also\nintroduce significant safety and privacy risks. Current mitigation strategies\noften fail to preserve contextual reasoning capabilities in risky scenarios.\nInstead, they rely heavily on sensitive pattern matching to protect LLMs, which\nlimits the scope. Furthermore, they overlook established safety and privacy\nstandards, leading to systemic risks for legal compliance. To address these\ngaps, we formulate safety and privacy issues into contextualized compliance\nproblems following the Contextual Integrity (CI) theory. Under the CI\nframework, we align our model with three critical regulatory standards: GDPR,\nEU AI Act, and HIPAA. Specifically, we employ reinforcement learning (RL) with\na rule-based reward to incentivize contextual reasoning capabilities while\nenhancing compliance with safety and privacy norms. Through extensive\nexperiments, we demonstrate that our method not only significantly enhances\nlegal compliance (achieving a +17.64% accuracy improvement in safety/privacy\nbenchmarks) but also further improves general reasoning capability. For\nOpenThinker-7B, a strong reasoning model that significantly outperforms its\nbase model Qwen2.5-7B-Instruct across diverse subjects, our method enhances its\ngeneral reasoning capabilities, with +2.05% and +8.98% accuracy improvement on\nthe MMLU and LegalBench benchmark, respectively."
                },
                "authors": [
                    {
                        "name": "Wenbin Hu"
                    },
                    {
                        "name": "Haoran Li"
                    },
                    {
                        "name": "Huihao Jing"
                    },
                    {
                        "name": "Qi Hu"
                    },
                    {
                        "name": "Ziqian Zeng"
                    },
                    {
                        "name": "Sirui Han"
                    },
                    {
                        "name": "Heli Xu"
                    },
                    {
                        "name": "Tianshu Chu"
                    },
                    {
                        "name": "Peizhao Hu"
                    },
                    {
                        "name": "Yangqiu Song"
                    }
                ],
                "author_detail": {
                    "name": "Yangqiu Song"
                },
                "author": "Yangqiu Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14585v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14585v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14582v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14582v1",
                "updated": "2025-05-20T16:38:32Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    16,
                    38,
                    32,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T16:38:32Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    16,
                    38,
                    32,
                    1,
                    140,
                    0
                ],
                "title": "Can Pruning Improve Reasoning? Revisiting Long-CoT Compression with\n  Capability in Mind for Better Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Pruning Improve Reasoning? Revisiting Long-CoT Compression with\n  Capability in Mind for Better Reasoning"
                },
                "summary": "Long chain-of-thought (Long-CoT) reasoning improves accuracy in LLMs, yet its\nverbose, self-reflective style often hinders effective distillation into small\nlanguage models (SLMs). We revisit Long-CoT compression through the lens of\ncapability alignment and ask: Can pruning improve reasoning? We propose\nPrune-on-Logic, a structure-aware framework that transforms Long-CoT into logic\ngraphs and selectively prunes low-utility reasoning steps under\nself-verification constraints. Through systematic analysis across three pruning\nstrategies -- targeting entire chains, core reasoning, and verification -- we\nfind that pruning verification steps yields consistent accuracy gains while\nreducing inference cost, outperforming token-level baselines and uncompressed\nfine-tuning. In contrast, pruning reasoning or all-chain steps degrades\nperformance, revealing that small models benefit not from shorter CoTs, but\nfrom semantically leaner ones. Our findings highlight pruning as a structural\noptimization strategy for aligning CoT reasoning with SLM capacity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long chain-of-thought (Long-CoT) reasoning improves accuracy in LLMs, yet its\nverbose, self-reflective style often hinders effective distillation into small\nlanguage models (SLMs). We revisit Long-CoT compression through the lens of\ncapability alignment and ask: Can pruning improve reasoning? We propose\nPrune-on-Logic, a structure-aware framework that transforms Long-CoT into logic\ngraphs and selectively prunes low-utility reasoning steps under\nself-verification constraints. Through systematic analysis across three pruning\nstrategies -- targeting entire chains, core reasoning, and verification -- we\nfind that pruning verification steps yields consistent accuracy gains while\nreducing inference cost, outperforming token-level baselines and uncompressed\nfine-tuning. In contrast, pruning reasoning or all-chain steps degrades\nperformance, revealing that small models benefit not from shorter CoTs, but\nfrom semantically leaner ones. Our findings highlight pruning as a structural\noptimization strategy for aligning CoT reasoning with SLM capacity."
                },
                "authors": [
                    {
                        "name": "Shangziqi Zhao"
                    },
                    {
                        "name": "Jiahao Yuan"
                    },
                    {
                        "name": "Guisong Yang"
                    },
                    {
                        "name": "Usman Naseem"
                    }
                ],
                "author_detail": {
                    "name": "Usman Naseem"
                },
                "author": "Usman Naseem",
                "arxiv_comment": "17 pages,4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14582v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14582v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14579v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14579v1",
                "updated": "2025-05-20T16:37:46Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    16,
                    37,
                    46,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T16:37:46Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    16,
                    37,
                    46,
                    1,
                    140,
                    0
                ],
                "title": "Inference with correlated priors using sisters cells",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference with correlated priors using sisters cells"
                },
                "summary": "A common view of sensory processing is as probabilistic inference of latent\ncauses from receptor activations. Standard approaches often assume these causes\nare a priori independent, yet real-world generative factors are typically\ncorrelated. Representing such structured priors in neural systems poses\narchitectural challenges, particularly when direct interactions between units\nrepresenting latent causes are biologically implausible or computationally\nexpensive. Inspired by the architecture of the olfactory bulb, we propose a\nnovel circuit motif that enables inference with correlated priors without\nrequiring direct interactions among latent cause units. The key insight lies in\nusing sister cells: neurons receiving shared receptor input but connected\ndifferently to local interneurons. The required interactions among latent units\nare implemented indirectly through their connections to the sister cells, such\nthat correlated connectivity implies anti-correlation in the prior and vice\nversa. We use geometric arguments to construct connectivity that implements a\ngiven prior and to bound the number of causes for which such priors can be\nconstructed. Using simulations, we demonstrate the efficacy of such priors for\ninference in noisy environments and compare the inference dynamics to those\nexperimentally observed. Finally, we show how, under certain assumptions on\nlatent representations, the prior used can be inferred from sister cell\nactivations. While biologically grounded in the olfactory system, our mechanism\ngeneralises to other natural and artificial sensory systems and may inform the\ndesign of architectures for efficient inference under correlated latent\nstructure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A common view of sensory processing is as probabilistic inference of latent\ncauses from receptor activations. Standard approaches often assume these causes\nare a priori independent, yet real-world generative factors are typically\ncorrelated. Representing such structured priors in neural systems poses\narchitectural challenges, particularly when direct interactions between units\nrepresenting latent causes are biologically implausible or computationally\nexpensive. Inspired by the architecture of the olfactory bulb, we propose a\nnovel circuit motif that enables inference with correlated priors without\nrequiring direct interactions among latent cause units. The key insight lies in\nusing sister cells: neurons receiving shared receptor input but connected\ndifferently to local interneurons. The required interactions among latent units\nare implemented indirectly through their connections to the sister cells, such\nthat correlated connectivity implies anti-correlation in the prior and vice\nversa. We use geometric arguments to construct connectivity that implements a\ngiven prior and to bound the number of causes for which such priors can be\nconstructed. Using simulations, we demonstrate the efficacy of such priors for\ninference in noisy environments and compare the inference dynamics to those\nexperimentally observed. Finally, we show how, under certain assumptions on\nlatent representations, the prior used can be inferred from sister cell\nactivations. While biologically grounded in the olfactory system, our mechanism\ngeneralises to other natural and artificial sensory systems and may inform the\ndesign of architectures for efficient inference under correlated latent\nstructure."
                },
                "authors": [
                    {
                        "name": "Sina Tootoonian"
                    },
                    {
                        "name": "Andreas T. Schaefer"
                    }
                ],
                "author_detail": {
                    "name": "Andreas T. Schaefer"
                },
                "author": "Andreas T. Schaefer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14579v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14579v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.NC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14577v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14577v1",
                "updated": "2025-05-20T16:34:37Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    16,
                    34,
                    37,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T16:34:37Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    16,
                    34,
                    37,
                    1,
                    140,
                    0
                ],
                "title": "TRATES: Trait-Specific Rubric-Assisted Cross-Prompt Essay Scoring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TRATES: Trait-Specific Rubric-Assisted Cross-Prompt Essay Scoring"
                },
                "summary": "Research on holistic Automated Essay Scoring (AES) is long-dated; yet, there\nis a notable lack of attention for assessing essays according to individual\ntraits. In this work, we propose TRATES, a novel trait-specific and\nrubric-based cross-prompt AES framework that is generic yet specific to the\nunderlying trait. The framework leverages a Large Language Model (LLM) that\nutilizes the trait grading rubrics to generate trait-specific features\n(represented by assessment questions), then assesses those features given an\nessay. The trait-specific features are eventually combined with generic\nwriting-quality and prompt-specific features to train a simple classical\nregression model that predicts trait scores of essays from an unseen prompt.\nExperiments show that TRATES achieves a new state-of-the-art performance across\nall traits on a widely-used dataset, with the generated LLM-based features\nbeing the most significant.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Research on holistic Automated Essay Scoring (AES) is long-dated; yet, there\nis a notable lack of attention for assessing essays according to individual\ntraits. In this work, we propose TRATES, a novel trait-specific and\nrubric-based cross-prompt AES framework that is generic yet specific to the\nunderlying trait. The framework leverages a Large Language Model (LLM) that\nutilizes the trait grading rubrics to generate trait-specific features\n(represented by assessment questions), then assesses those features given an\nessay. The trait-specific features are eventually combined with generic\nwriting-quality and prompt-specific features to train a simple classical\nregression model that predicts trait scores of essays from an unseen prompt.\nExperiments show that TRATES achieves a new state-of-the-art performance across\nall traits on a widely-used dataset, with the generated LLM-based features\nbeing the most significant."
                },
                "authors": [
                    {
                        "name": "Sohaila Eltanbouly"
                    },
                    {
                        "name": "Salam Albatarni"
                    },
                    {
                        "name": "Tamer Elsayed"
                    }
                ],
                "author_detail": {
                    "name": "Tamer Elsayed"
                },
                "author": "Tamer Elsayed",
                "arxiv_comment": "Accepted at ACL 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14577v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14577v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14573v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14573v1",
                "updated": "2025-05-20T16:31:13Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    16,
                    31,
                    13,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T16:31:13Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    16,
                    31,
                    13,
                    1,
                    140,
                    0
                ],
                "title": "Measuring spin precession from massive black holes binaries with\n  gravitational waves: insights from time-domain signal morphology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring spin precession from massive black holes binaries with\n  gravitational waves: insights from time-domain signal morphology"
                },
                "summary": "Robustly measuring binary black hole spins via gravitational waves is key to\nunderstanding these systems' astrophysical origins, but remains challenging --\nespecially for high-mass systems, whose signals are short and dominated by the\nmerger. Nonetheless, events like GW190521 show that strong spin precession can\nindeed be gleaned from high-mass systems. In this work, we track how spin\nprecession imprints on simulated high-mass binary black hole signals\ncycle-by-cycle using time-domain inference. We investigate a suite of signals,\nall with the same spins and (near-unity) mass ratio but different\nsignal-to-noise ratios, total masses, and extrinsic angles, which affect the\nobserved waveform morphology. We truncate each signal at various times and\ninfer source parameters using only the data before or after each cutoff. The\nresultant posterior allows us to identify which time segments of each signal\ninform its spin precession constraints. We find that at a sufficiently high\npost-peak signal-to-noise ratio (SNR, $\\rho\\sim 20$), spin precession can be\nconstrained by the {\\sc NRSur7dq4} waveform model when just the post-peak data\n(i.e., ringdown) is visible. Similarly, at a large enough pre-cutoff SNR\n($\\rho\\sim 10$), spin precession can be constrained using only pre-peak data\n(i.e., inspiral); this occurs for signals with detector-frame total mass\n$\\lesssim 100 M_{\\odot}$ at GW190521's SNR. Finally, we vary the inclination,\npolarization, and phase angles, finding that their configuration need not be\nfine-tuned to measure spin precession, even for very high-mass and short\nsignals with 2-3 observable cycles. We do not find that the same morphological\nfeatures consistently drive precession constraints: in some signals, precession\ninference hinges on the relationship between a loud merger and quiet pre-merger\ncycle, as was the case for GW190521, but this is not generically true.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robustly measuring binary black hole spins via gravitational waves is key to\nunderstanding these systems' astrophysical origins, but remains challenging --\nespecially for high-mass systems, whose signals are short and dominated by the\nmerger. Nonetheless, events like GW190521 show that strong spin precession can\nindeed be gleaned from high-mass systems. In this work, we track how spin\nprecession imprints on simulated high-mass binary black hole signals\ncycle-by-cycle using time-domain inference. We investigate a suite of signals,\nall with the same spins and (near-unity) mass ratio but different\nsignal-to-noise ratios, total masses, and extrinsic angles, which affect the\nobserved waveform morphology. We truncate each signal at various times and\ninfer source parameters using only the data before or after each cutoff. The\nresultant posterior allows us to identify which time segments of each signal\ninform its spin precession constraints. We find that at a sufficiently high\npost-peak signal-to-noise ratio (SNR, $\\rho\\sim 20$), spin precession can be\nconstrained by the {\\sc NRSur7dq4} waveform model when just the post-peak data\n(i.e., ringdown) is visible. Similarly, at a large enough pre-cutoff SNR\n($\\rho\\sim 10$), spin precession can be constrained using only pre-peak data\n(i.e., inspiral); this occurs for signals with detector-frame total mass\n$\\lesssim 100 M_{\\odot}$ at GW190521's SNR. Finally, we vary the inclination,\npolarization, and phase angles, finding that their configuration need not be\nfine-tuned to measure spin precession, even for very high-mass and short\nsignals with 2-3 observable cycles. We do not find that the same morphological\nfeatures consistently drive precession constraints: in some signals, precession\ninference hinges on the relationship between a loud merger and quiet pre-merger\ncycle, as was the case for GW190521, but this is not generically true."
                },
                "authors": [
                    {
                        "name": "Simona J. Miller"
                    },
                    {
                        "name": "Maximiliano Isi"
                    },
                    {
                        "name": "Katerina Chatziioannou"
                    },
                    {
                        "name": "Vijay Varma"
                    },
                    {
                        "name": "Sophie Hourihane"
                    }
                ],
                "author_detail": {
                    "name": "Sophie Hourihane"
                },
                "author": "Sophie Hourihane",
                "arxiv_comment": "19 pages main text, 5 pages appendix (excluding references), 13\n  figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14573v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14573v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07115v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07115v4",
                "updated": "2025-05-20T16:29:52Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    16,
                    29,
                    52,
                    1,
                    140,
                    0
                ],
                "published": "2025-02-10T23:11:44Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    23,
                    11,
                    44,
                    0,
                    41,
                    0
                ],
                "title": "Online Scheduling for LLM Inference with KV Cache Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Scheduling for LLM Inference with KV Cache Constraints"
                },
                "summary": "Large Language Model (LLM) inference, where a trained model generates text\none word at a time in response to user prompts, is a computationally intensive\nprocess requiring efficient scheduling to optimize latency and resource\nutilization. A key challenge in LLM inference is the management of the\nKey-Value (KV) cache, which reduces redundant computations but introduces\nmemory constraints. In this work, we model LLM inference with KV cache\nconstraints theoretically and propose a novel batching and scheduling algorithm\nthat minimizes inference latency while effectively managing the KV cache's\nmemory.\n  More specifically, we make the following contributions. First, to evaluate\nthe performance of online algorithms for scheduling in LLM inference, we\nintroduce a hindsight optimal benchmark, formulated as an integer program that\ncomputes the minimum total inference latency under full future information.\nSecond, we prove that no deterministic online algorithm can achieve a constant\ncompetitive ratio when the arrival process is arbitrary. Third, motivated by\nthe computational intractability of solving the integer program at scale, we\npropose a polynomial-time online scheduling algorithm and show that under\ncertain conditions it can achieve a constant competitive ratio. We also\ndemonstrate our algorithm's strong empirical performance by comparing it to the\nhindsight optimal in a synthetic dataset. Finally, we conduct empirical\nevaluations on a real-world public LLM inference dataset, simulating the\nLlama2-70B model on A100 GPUs, and show that our algorithm significantly\noutperforms the benchmark algorithms. Overall, our results offer a path toward\nmore sustainable and cost-effective LLM deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference, where a trained model generates text\none word at a time in response to user prompts, is a computationally intensive\nprocess requiring efficient scheduling to optimize latency and resource\nutilization. A key challenge in LLM inference is the management of the\nKey-Value (KV) cache, which reduces redundant computations but introduces\nmemory constraints. In this work, we model LLM inference with KV cache\nconstraints theoretically and propose a novel batching and scheduling algorithm\nthat minimizes inference latency while effectively managing the KV cache's\nmemory.\n  More specifically, we make the following contributions. First, to evaluate\nthe performance of online algorithms for scheduling in LLM inference, we\nintroduce a hindsight optimal benchmark, formulated as an integer program that\ncomputes the minimum total inference latency under full future information.\nSecond, we prove that no deterministic online algorithm can achieve a constant\ncompetitive ratio when the arrival process is arbitrary. Third, motivated by\nthe computational intractability of solving the integer program at scale, we\npropose a polynomial-time online scheduling algorithm and show that under\ncertain conditions it can achieve a constant competitive ratio. We also\ndemonstrate our algorithm's strong empirical performance by comparing it to the\nhindsight optimal in a synthetic dataset. Finally, we conduct empirical\nevaluations on a real-world public LLM inference dataset, simulating the\nLlama2-70B model on A100 GPUs, and show that our algorithm significantly\noutperforms the benchmark algorithms. Overall, our results offer a path toward\nmore sustainable and cost-effective LLM deployment."
                },
                "authors": [
                    {
                        "name": "Patrick Jaillet"
                    },
                    {
                        "name": "Jiashuo Jiang"
                    },
                    {
                        "name": "Konstantina Mellou"
                    },
                    {
                        "name": "Marco Molinaro"
                    },
                    {
                        "name": "Chara Podimata"
                    },
                    {
                        "name": "Zijie Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Zijie Zhou"
                },
                "author": "Zijie Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07115v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07115v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21272v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21272v2",
                "updated": "2025-05-20T16:29:40Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    16,
                    29,
                    40,
                    1,
                    140,
                    0
                ],
                "published": "2024-10-28T17:59:06Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    17,
                    59,
                    6,
                    0,
                    302,
                    0
                ],
                "title": "Arithmetic Without Algorithms: Language Models Solve Math With a Bag of\n  Heuristics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Arithmetic Without Algorithms: Language Models Solve Math With a Bag of\n  Heuristics"
                },
                "summary": "Do large language models (LLMs) solve reasoning tasks by learning robust\ngeneralizable algorithms, or do they memorize training data? To investigate\nthis question, we use arithmetic reasoning as a representative task. Using\ncausal analysis, we identify a subset of the model (a circuit) that explains\nmost of the model's behavior for basic arithmetic logic and examine its\nfunctionality. By zooming in on the level of individual circuit neurons, we\ndiscover a sparse set of important neurons that implement simple heuristics.\nEach heuristic identifies a numerical input pattern and outputs corresponding\nanswers. We hypothesize that the combination of these heuristic neurons is the\nmechanism used to produce correct arithmetic answers. To test this, we\ncategorize each neuron into several heuristic types-such as neurons that\nactivate when an operand falls within a certain range-and find that the\nunordered combination of these heuristic types is the mechanism that explains\nmost of the model's accuracy on arithmetic prompts. Finally, we demonstrate\nthat this mechanism appears as the main source of arithmetic accuracy early in\ntraining. Overall, our experimental results across several LLMs show that LLMs\nperform arithmetic using neither robust algorithms nor memorization; rather,\nthey rely on a \"bag of heuristics\".",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do large language models (LLMs) solve reasoning tasks by learning robust\ngeneralizable algorithms, or do they memorize training data? To investigate\nthis question, we use arithmetic reasoning as a representative task. Using\ncausal analysis, we identify a subset of the model (a circuit) that explains\nmost of the model's behavior for basic arithmetic logic and examine its\nfunctionality. By zooming in on the level of individual circuit neurons, we\ndiscover a sparse set of important neurons that implement simple heuristics.\nEach heuristic identifies a numerical input pattern and outputs corresponding\nanswers. We hypothesize that the combination of these heuristic neurons is the\nmechanism used to produce correct arithmetic answers. To test this, we\ncategorize each neuron into several heuristic types-such as neurons that\nactivate when an operand falls within a certain range-and find that the\nunordered combination of these heuristic types is the mechanism that explains\nmost of the model's accuracy on arithmetic prompts. Finally, we demonstrate\nthat this mechanism appears as the main source of arithmetic accuracy early in\ntraining. Overall, our experimental results across several LLMs show that LLMs\nperform arithmetic using neither robust algorithms nor memorization; rather,\nthey rely on a \"bag of heuristics\"."
                },
                "authors": [
                    {
                        "name": "Yaniv Nikankin"
                    },
                    {
                        "name": "Anja Reusch"
                    },
                    {
                        "name": "Aaron Mueller"
                    },
                    {
                        "name": "Yonatan Belinkov"
                    }
                ],
                "author_detail": {
                    "name": "Yonatan Belinkov"
                },
                "author": "Yonatan Belinkov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21272v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21272v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05398v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05398v2",
                "updated": "2025-05-20T16:29:40Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    16,
                    29,
                    40,
                    1,
                    140,
                    0
                ],
                "published": "2024-12-06T19:51:52Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    19,
                    51,
                    52,
                    4,
                    341,
                    0
                ],
                "title": "The Neglected Error: False Negatives and the Case for Validating\n  Eliminations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Neglected Error: False Negatives and the Case for Validating\n  Eliminations"
                },
                "summary": "This article examines the overlooked risk of false negative errors arising\nfrom eliminations in forensic firearm comparisons. While recent reforms in\nforensic science have focused on reducing false positives, eliminations--often\nbased on class characteristics or intuitive judgments--receive little empirical\nscrutiny despite their potential to exclude true sources. In cases involving a\nclosed pool of suspects, eliminations can function as de facto identifications,\nintroducing serious risk of error. A review of existing validity studies\nreveals that many report only false positive rates, failing to provide a\ncomplete assessment of method accuracy. This asymmetry is reinforced by\nprofessional guidelines, such as those from AFTE, and echoed in major\ngovernment reports, including those from NAS and PCAST. The article argues that\neliminations, like identifications, must be validated through rigorous testing\nand reported with transparent error rates. It further cautions against the use\nof \"common sense\" eliminations in the absence of empirical support and\nhighlights the dangers of contextual bias when examiners are aware of\ninvestigative constraints. Five policy recommendations are proposed to improve\nthe scientific treatment and legal interpretation of eliminations, including\nbalanced reporting of false positive and false negative rates, validation of\nintuitive judgments, and clear warnings against using eliminations to infer\nguilt in closed-pool scenarios. Without reform, eliminations will continue to\nescape scrutiny, perpetuating unmeasured error and undermining the integrity of\nforensic conclusions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This article examines the overlooked risk of false negative errors arising\nfrom eliminations in forensic firearm comparisons. While recent reforms in\nforensic science have focused on reducing false positives, eliminations--often\nbased on class characteristics or intuitive judgments--receive little empirical\nscrutiny despite their potential to exclude true sources. In cases involving a\nclosed pool of suspects, eliminations can function as de facto identifications,\nintroducing serious risk of error. A review of existing validity studies\nreveals that many report only false positive rates, failing to provide a\ncomplete assessment of method accuracy. This asymmetry is reinforced by\nprofessional guidelines, such as those from AFTE, and echoed in major\ngovernment reports, including those from NAS and PCAST. The article argues that\neliminations, like identifications, must be validated through rigorous testing\nand reported with transparent error rates. It further cautions against the use\nof \"common sense\" eliminations in the absence of empirical support and\nhighlights the dangers of contextual bias when examiners are aware of\ninvestigative constraints. Five policy recommendations are proposed to improve\nthe scientific treatment and legal interpretation of eliminations, including\nbalanced reporting of false positive and false negative rates, validation of\nintuitive judgments, and clear warnings against using eliminations to infer\nguilt in closed-pool scenarios. Without reform, eliminations will continue to\nescape scrutiny, perpetuating unmeasured error and undermining the integrity of\nforensic conclusions."
                },
                "authors": [
                    {
                        "name": "Maria Cuellar"
                    }
                ],
                "author_detail": {
                    "name": "Maria Cuellar"
                },
                "author": "Maria Cuellar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05398v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05398v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14569v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14569v1",
                "updated": "2025-05-20T16:28:08Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    16,
                    28,
                    8,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T16:28:08Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    16,
                    28,
                    8,
                    1,
                    140,
                    0
                ],
                "title": "Agent Context Protocols Enhance Collective Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agent Context Protocols Enhance Collective Inference"
                },
                "summary": "AI agents have become increasingly adept at complex tasks such as coding,\nreasoning, and multimodal understanding. However, building generalist systems\nrequires moving beyond individual agents to collective inference -- a paradigm\nwhere multi-agent systems with diverse, task-specialized agents complement one\nanother through structured communication and collaboration. Today, coordination\nis usually handled with imprecise, ad-hoc natural language, which limits\ncomplex interaction and hinders interoperability with domain-specific agents.\nWe introduce Agent context protocols (ACPs): a domain- and agent-agnostic\nfamily of structured protocols for agent-agent communication, coordination, and\nerror handling. ACPs combine (i) persistent execution blueprints -- explicit\ndependency graphs that store intermediate agent outputs -- with (ii)\nstandardized message schemas, enabling robust and fault-tolerant multi-agent\ncollective inference. ACP-powered generalist systems reach state-of-the-art\nperformance: 28.3 % accuracy on AssistantBench for long-horizon web assistance\nand best-in-class multimodal technical reports, outperforming commercial AI\nsystems in human evaluation. ACPs are highly modular and extensible, allowing\npractitioners to build top-tier generalist agents quickly.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI agents have become increasingly adept at complex tasks such as coding,\nreasoning, and multimodal understanding. However, building generalist systems\nrequires moving beyond individual agents to collective inference -- a paradigm\nwhere multi-agent systems with diverse, task-specialized agents complement one\nanother through structured communication and collaboration. Today, coordination\nis usually handled with imprecise, ad-hoc natural language, which limits\ncomplex interaction and hinders interoperability with domain-specific agents.\nWe introduce Agent context protocols (ACPs): a domain- and agent-agnostic\nfamily of structured protocols for agent-agent communication, coordination, and\nerror handling. ACPs combine (i) persistent execution blueprints -- explicit\ndependency graphs that store intermediate agent outputs -- with (ii)\nstandardized message schemas, enabling robust and fault-tolerant multi-agent\ncollective inference. ACP-powered generalist systems reach state-of-the-art\nperformance: 28.3 % accuracy on AssistantBench for long-horizon web assistance\nand best-in-class multimodal technical reports, outperforming commercial AI\nsystems in human evaluation. ACPs are highly modular and extensible, allowing\npractitioners to build top-tier generalist agents quickly."
                },
                "authors": [
                    {
                        "name": "Devansh Bhardwaj"
                    },
                    {
                        "name": "Arjun Beniwal"
                    },
                    {
                        "name": "Shreyas Chaudhari"
                    },
                    {
                        "name": "Ashwin Kalyan"
                    },
                    {
                        "name": "Tanmay Rajpurohit"
                    },
                    {
                        "name": "Karthik R. Narasimhan"
                    },
                    {
                        "name": "Ameet Deshpande"
                    },
                    {
                        "name": "Vishvak Murahari"
                    }
                ],
                "author_detail": {
                    "name": "Vishvak Murahari"
                },
                "author": "Vishvak Murahari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14569v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14569v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10940v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10940v2",
                "updated": "2025-05-20T16:27:05Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    16,
                    27,
                    5,
                    1,
                    140,
                    0
                ],
                "published": "2025-02-16T01:05:16Z",
                "published_parsed": [
                    2025,
                    2,
                    16,
                    1,
                    5,
                    16,
                    6,
                    47,
                    0
                ],
                "title": "CoLA: Compute-Efficient Pre-Training of LLMs via Low-Rank Activation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoLA: Compute-Efficient Pre-Training of LLMs via Low-Rank Activation"
                },
                "summary": "The full-size MLPs and the projection layers in attention introduce\ntremendous model sizes of large language models (LLMs), imposing extremely\ndemanding needs of computational resources in the pre-training stage. However,\nwe empirically observe that the activations of pre-trained LLMs exhibit\nlow-rank property. Motivated by such observations, we propose CoLA and its\nmemory-efficient implementation, CoLA-M, to replace these full-size layers with\ncompute-efficient auto-encoders that naturally enforce low-rank activations\nthroughout training. This fundamental architectural change eliminates the\nactivation redundancy and significantly boosts model capacity and training\nefficiency. Experiments on LLaMA models with 60 million to 7 billion parameters\nshow that CoLA reduces the computing cost by $\\bf 2\\pmb{\\times}$ and improves\ntraining throughput by $\\bf 1.86\\pmb{\\times}$ while maintaining full-rank level\nperformance. CoLA-M further squeezes memory cost without sacrificing\nthroughput, offering a pre-training approach with collectively superior\nparameter, computing, and memory efficiency. The LLMs produced are also $\\bf\n2\\pmb{\\times}$ smaller, enabling faster inference with lower memory cost on\nresource-constrained platforms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The full-size MLPs and the projection layers in attention introduce\ntremendous model sizes of large language models (LLMs), imposing extremely\ndemanding needs of computational resources in the pre-training stage. However,\nwe empirically observe that the activations of pre-trained LLMs exhibit\nlow-rank property. Motivated by such observations, we propose CoLA and its\nmemory-efficient implementation, CoLA-M, to replace these full-size layers with\ncompute-efficient auto-encoders that naturally enforce low-rank activations\nthroughout training. This fundamental architectural change eliminates the\nactivation redundancy and significantly boosts model capacity and training\nefficiency. Experiments on LLaMA models with 60 million to 7 billion parameters\nshow that CoLA reduces the computing cost by $\\bf 2\\pmb{\\times}$ and improves\ntraining throughput by $\\bf 1.86\\pmb{\\times}$ while maintaining full-rank level\nperformance. CoLA-M further squeezes memory cost without sacrificing\nthroughput, offering a pre-training approach with collectively superior\nparameter, computing, and memory efficiency. The LLMs produced are also $\\bf\n2\\pmb{\\times}$ smaller, enabling faster inference with lower memory cost on\nresource-constrained platforms."
                },
                "authors": [
                    {
                        "name": "Ziyue Liu"
                    },
                    {
                        "name": "Ruijie Zhang"
                    },
                    {
                        "name": "Zhengyang Wang"
                    },
                    {
                        "name": "Zi Yang"
                    },
                    {
                        "name": "Paul Hovland"
                    },
                    {
                        "name": "Bogdan Nicolae"
                    },
                    {
                        "name": "Franck Cappello"
                    },
                    {
                        "name": "Zheng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Zhang"
                },
                "author": "Zheng Zhang",
                "arxiv_comment": "v2",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10940v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10940v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17388v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17388v3",
                "updated": "2025-05-20T16:24:22Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    16,
                    24,
                    22,
                    1,
                    140,
                    0
                ],
                "published": "2024-11-26T12:46:57Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    12,
                    46,
                    57,
                    1,
                    331,
                    0
                ],
                "title": "Can LLMs be Good Graph Judge for Knowledge Graph Construction?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs be Good Graph Judge for Knowledge Graph Construction?"
                },
                "summary": "In real-world scenarios, most of the data obtained from the information\nretrieval (IR) system is unstructured. Converting natural language sentences\ninto structured Knowledge Graphs (KGs) remains a critical challenge. We\nidentified three limitations with respect to existing KG construction methods:\n(1) There could be a large amount of noise in real-world documents, which could\nresult in extracting messy information. (2) Naive LLMs usually extract\ninaccurate knowledge from some domain-specific documents. (3) Hallucination\nphenomenon cannot be overlooked when directly using LLMs to construct KGs. In\nthis paper, we propose \\textbf{GraphJudge}, a KG construction framework to\naddress the aforementioned challenges. In this framework, we designed an\nentity-centric strategy to eliminate the noise information in the documents.\nAnd we fine-tuned a LLM as a graph judge to finally enhance the quality of\ngenerated KGs. Experiments conducted on two general and one domain-specific\ntext-graph pair datasets demonstrate state-of-the-art performance against\nvarious baseline methods with strong generalization abilities. Our code is\navailable at\n\\href{https://github.com/hhy-huang/GraphJudge}{https://github.com/hhy-huang/GraphJudge}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In real-world scenarios, most of the data obtained from the information\nretrieval (IR) system is unstructured. Converting natural language sentences\ninto structured Knowledge Graphs (KGs) remains a critical challenge. We\nidentified three limitations with respect to existing KG construction methods:\n(1) There could be a large amount of noise in real-world documents, which could\nresult in extracting messy information. (2) Naive LLMs usually extract\ninaccurate knowledge from some domain-specific documents. (3) Hallucination\nphenomenon cannot be overlooked when directly using LLMs to construct KGs. In\nthis paper, we propose \\textbf{GraphJudge}, a KG construction framework to\naddress the aforementioned challenges. In this framework, we designed an\nentity-centric strategy to eliminate the noise information in the documents.\nAnd we fine-tuned a LLM as a graph judge to finally enhance the quality of\ngenerated KGs. Experiments conducted on two general and one domain-specific\ntext-graph pair datasets demonstrate state-of-the-art performance against\nvarious baseline methods with strong generalization abilities. Our code is\navailable at\n\\href{https://github.com/hhy-huang/GraphJudge}{https://github.com/hhy-huang/GraphJudge}."
                },
                "authors": [
                    {
                        "name": "Haoyu Huang"
                    },
                    {
                        "name": "Chong Chen"
                    },
                    {
                        "name": "Zeang Sheng"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Wentao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wentao Zhang"
                },
                "author": "Wentao Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17388v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17388v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12466v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12466v2",
                "updated": "2025-05-20T16:19:18Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    16,
                    19,
                    18,
                    1,
                    140,
                    0
                ],
                "published": "2025-02-18T02:54:25Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    2,
                    54,
                    25,
                    1,
                    49,
                    0
                ],
                "title": "EquiBench: Benchmarking Large Language Models' Understanding of Program\n  Semantics via Equivalence Checking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EquiBench: Benchmarking Large Language Models' Understanding of Program\n  Semantics via Equivalence Checking"
                },
                "summary": "As large language models (LLMs) become integral to code-related tasks, a\ncentral question emerges: do LLMs truly understand program execution semantics?\nWe introduce EquiBench, a new benchmark for evaluating LLMs through equivalence\nchecking, i.e., determining whether two programs produce identical outputs for\nall possible inputs. Unlike prior code generation benchmarks, this task\ndirectly tests a model's understanding of code execution semantics. EquiBench\nconsists of 2400 program pairs across four languages and six categories. These\npairs are generated through program analysis, compiler scheduling, and\nsuperoptimization, ensuring high-confidence labels, nontrivial difficulty, and\nfull automation. The transformations span syntactic edits, structural\nmodifications, and algorithmic changes, covering a broad spectrum of semantic\nvariation. We evaluate 19 state-of-the-art LLMs and find that in the most\nchallenging categories, the best accuracies are 63.8% and 76.2%, only modestly\nabove the 50% random baseline. Further analysis reveals that models often rely\non syntactic similarity rather than exhibiting robust reasoning over execution\nsemantics, highlighting fundamental limitations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) become integral to code-related tasks, a\ncentral question emerges: do LLMs truly understand program execution semantics?\nWe introduce EquiBench, a new benchmark for evaluating LLMs through equivalence\nchecking, i.e., determining whether two programs produce identical outputs for\nall possible inputs. Unlike prior code generation benchmarks, this task\ndirectly tests a model's understanding of code execution semantics. EquiBench\nconsists of 2400 program pairs across four languages and six categories. These\npairs are generated through program analysis, compiler scheduling, and\nsuperoptimization, ensuring high-confidence labels, nontrivial difficulty, and\nfull automation. The transformations span syntactic edits, structural\nmodifications, and algorithmic changes, covering a broad spectrum of semantic\nvariation. We evaluate 19 state-of-the-art LLMs and find that in the most\nchallenging categories, the best accuracies are 63.8% and 76.2%, only modestly\nabove the 50% random baseline. Further analysis reveals that models often rely\non syntactic similarity rather than exhibiting robust reasoning over execution\nsemantics, highlighting fundamental limitations."
                },
                "authors": [
                    {
                        "name": "Anjiang Wei"
                    },
                    {
                        "name": "Jiannan Cao"
                    },
                    {
                        "name": "Ran Li"
                    },
                    {
                        "name": "Hongyu Chen"
                    },
                    {
                        "name": "Yuhui Zhang"
                    },
                    {
                        "name": "Ziheng Wang"
                    },
                    {
                        "name": "Yuan Liu"
                    },
                    {
                        "name": "Thiago S. F. X. Teixeira"
                    },
                    {
                        "name": "Diyi Yang"
                    },
                    {
                        "name": "Ke Wang"
                    },
                    {
                        "name": "Alex Aiken"
                    }
                ],
                "author_detail": {
                    "name": "Alex Aiken"
                },
                "author": "Alex Aiken",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12466v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12466v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14560v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14560v1",
                "updated": "2025-05-20T16:19:16Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    16,
                    19,
                    16,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T16:19:16Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    16,
                    19,
                    16,
                    1,
                    140,
                    0
                ],
                "title": "Neural Inverse Scattering with Score-based Regularization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Inverse Scattering with Score-based Regularization"
                },
                "summary": "Inverse scattering is a fundamental challenge in many imaging applications,\nranging from microscopy to remote sensing. Solving this problem often requires\njointly estimating two unknowns -- the image and the scattering field inside\nthe object -- necessitating effective image prior to regularize the inference.\nIn this paper, we propose a regularized neural field (NF) approach which\nintegrates the denoising score function used in score-based generative models.\nThe neural field formulation offers convenient flexibility to performing joint\nestimation, while the denoising score function imposes the rich structural\nprior of images. Our results on three high-contrast simulated objects show that\nthe proposed approach yields a better imaging quality compared to the\nstate-of-the-art NF approach, where regularization is based on total variation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inverse scattering is a fundamental challenge in many imaging applications,\nranging from microscopy to remote sensing. Solving this problem often requires\njointly estimating two unknowns -- the image and the scattering field inside\nthe object -- necessitating effective image prior to regularize the inference.\nIn this paper, we propose a regularized neural field (NF) approach which\nintegrates the denoising score function used in score-based generative models.\nThe neural field formulation offers convenient flexibility to performing joint\nestimation, while the denoising score function imposes the rich structural\nprior of images. Our results on three high-contrast simulated objects show that\nthe proposed approach yields a better imaging quality compared to the\nstate-of-the-art NF approach, where regularization is based on total variation."
                },
                "authors": [
                    {
                        "name": "Yuan Gao"
                    },
                    {
                        "name": "Wenhan Guo"
                    },
                    {
                        "name": "Yu Sun"
                    }
                ],
                "author_detail": {
                    "name": "Yu Sun"
                },
                "author": "Yu Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14560v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14560v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14558v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14558v1",
                "updated": "2025-05-20T16:15:30Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    16,
                    15,
                    30,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T16:15:30Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    16,
                    15,
                    30,
                    1,
                    140,
                    0
                ],
                "title": "R2MED: A Benchmark for Reasoning-Driven Medical Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "R2MED: A Benchmark for Reasoning-Driven Medical Retrieval"
                },
                "summary": "Current medical retrieval benchmarks primarily emphasize lexical or shallow\nsemantic similarity, overlooking the reasoning-intensive demands that are\ncentral to clinical decision-making. In practice, physicians often retrieve\nauthoritative medical evidence to support diagnostic hypotheses. Such evidence\ntypically aligns with an inferred diagnosis rather than the surface form of a\npatient's symptoms, leading to low lexical or semantic overlap between queries\nand relevant documents. To address this gap, we introduce R2MED, the first\nbenchmark explicitly designed for reasoning-driven medical retrieval. It\ncomprises 876 queries spanning three tasks: Q&A reference retrieval, clinical\nevidence retrieval, and clinical case retrieval. These tasks are drawn from\nfive representative medical scenarios and twelve body systems, capturing the\ncomplexity and diversity of real-world medical information needs. We evaluate\n15 widely-used retrieval systems on R2MED and find that even the best model\nachieves only 31.4 nDCG@10, demonstrating the benchmark's difficulty. Classical\nre-ranking and generation-augmented retrieval methods offer only modest\nimprovements. Although large reasoning models improve performance via\nintermediate inference generation, the best results still peak at 41.4 nDCG@10.\nThese findings underscore a substantial gap between current retrieval\ntechniques and the reasoning demands of real clinical tasks. We release R2MED\nas a challenging benchmark to foster the development of next-generation medical\nretrieval systems with enhanced reasoning capabilities. Data and code are\navailable at https://github.com/R2MED/R2MED",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current medical retrieval benchmarks primarily emphasize lexical or shallow\nsemantic similarity, overlooking the reasoning-intensive demands that are\ncentral to clinical decision-making. In practice, physicians often retrieve\nauthoritative medical evidence to support diagnostic hypotheses. Such evidence\ntypically aligns with an inferred diagnosis rather than the surface form of a\npatient's symptoms, leading to low lexical or semantic overlap between queries\nand relevant documents. To address this gap, we introduce R2MED, the first\nbenchmark explicitly designed for reasoning-driven medical retrieval. It\ncomprises 876 queries spanning three tasks: Q&A reference retrieval, clinical\nevidence retrieval, and clinical case retrieval. These tasks are drawn from\nfive representative medical scenarios and twelve body systems, capturing the\ncomplexity and diversity of real-world medical information needs. We evaluate\n15 widely-used retrieval systems on R2MED and find that even the best model\nachieves only 31.4 nDCG@10, demonstrating the benchmark's difficulty. Classical\nre-ranking and generation-augmented retrieval methods offer only modest\nimprovements. Although large reasoning models improve performance via\nintermediate inference generation, the best results still peak at 41.4 nDCG@10.\nThese findings underscore a substantial gap between current retrieval\ntechniques and the reasoning demands of real clinical tasks. We release R2MED\nas a challenging benchmark to foster the development of next-generation medical\nretrieval systems with enhanced reasoning capabilities. Data and code are\navailable at https://github.com/R2MED/R2MED"
                },
                "authors": [
                    {
                        "name": "Lei Li"
                    },
                    {
                        "name": "Xiao Zhou"
                    },
                    {
                        "name": "Zheng Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Liu"
                },
                "author": "Zheng Liu",
                "arxiv_comment": "38 pages, 16 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14558v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14558v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18169v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18169v4",
                "updated": "2025-05-20T16:15:13Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    16,
                    15,
                    13,
                    1,
                    140,
                    0
                ],
                "published": "2024-12-24T05:07:46Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    5,
                    7,
                    46,
                    1,
                    359,
                    0
                ],
                "title": "KunServe: Efficient Parameter-centric Memory Management for LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KunServe: Efficient Parameter-centric Memory Management for LLM Serving"
                },
                "summary": "Serving LLMs with a cluster of GPUs is common nowadays, where the serving\nsystem must meet strict latency SLOs required by applications. However, the\nstateful nature of LLM serving requires maintaining huge states (i.e., KVCache)\nin limited GPU memory. Under spikes in real-world workloads, GPU memory can be\neasily throttled, leading to orders of magnitude higher response latency due to\nqueuing introduced by waiting for KVCache to be reclaimed. Prior\nKVCache-centric approaches handle load throttling by dropping, migrating, or\nswapping KVCache. These methods fail to release sufficient memory quickly with\nrequests still queued.\n  This paper proposes the first parameter-centric approach to handling\nthrottling by selectively dropping replicated parameters to instantly free\nmemory for requests, based on an unnoticed observation that model parameters\nare commonly replicated across GPUs for serving LLMs. With additional memory,\nall requests can be served with a larger batch without queuing. To make the\nparameter-centric approach correct and efficient, we cooperatively execute\nrequests on GPUs with a complete copy of parameters using pipeline parallelism,\nand derive an appropriate drop plan without unnecessary cooperation. We also\ndesign techniques to minimize the performance overhead due to pipeline\nparallelism with the execution patterns of requests under drop. Evaluations\nshow that {\\sys} reduces the tail TTFT of requests under throttling by up to\n72.2 times compared to the state-of-the-art systems including Llumnix, vLLM and\nInferCept.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving LLMs with a cluster of GPUs is common nowadays, where the serving\nsystem must meet strict latency SLOs required by applications. However, the\nstateful nature of LLM serving requires maintaining huge states (i.e., KVCache)\nin limited GPU memory. Under spikes in real-world workloads, GPU memory can be\neasily throttled, leading to orders of magnitude higher response latency due to\nqueuing introduced by waiting for KVCache to be reclaimed. Prior\nKVCache-centric approaches handle load throttling by dropping, migrating, or\nswapping KVCache. These methods fail to release sufficient memory quickly with\nrequests still queued.\n  This paper proposes the first parameter-centric approach to handling\nthrottling by selectively dropping replicated parameters to instantly free\nmemory for requests, based on an unnoticed observation that model parameters\nare commonly replicated across GPUs for serving LLMs. With additional memory,\nall requests can be served with a larger batch without queuing. To make the\nparameter-centric approach correct and efficient, we cooperatively execute\nrequests on GPUs with a complete copy of parameters using pipeline parallelism,\nand derive an appropriate drop plan without unnecessary cooperation. We also\ndesign techniques to minimize the performance overhead due to pipeline\nparallelism with the execution patterns of requests under drop. Evaluations\nshow that {\\sys} reduces the tail TTFT of requests under throttling by up to\n72.2 times compared to the state-of-the-art systems including Llumnix, vLLM and\nInferCept."
                },
                "authors": [
                    {
                        "name": "Rongxin Cheng"
                    },
                    {
                        "name": "Yuxin Lai"
                    },
                    {
                        "name": "Xingda Wei"
                    },
                    {
                        "name": "Rong Chen"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18169v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18169v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14555v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14555v1",
                "updated": "2025-05-20T16:13:20Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    16,
                    13,
                    20,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T16:13:20Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    16,
                    13,
                    20,
                    1,
                    140,
                    0
                ],
                "title": "Physics-Guided Learning of Meteorological Dynamics for Weather\n  Downscaling and Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Physics-Guided Learning of Meteorological Dynamics for Weather\n  Downscaling and Forecasting"
                },
                "summary": "Weather forecasting is essential but remains computationally intensive and\nphysically incomplete in traditional numerical weather prediction (NWP)\nmethods. Deep learning (DL) models offer efficiency and accuracy but often\nignore physical laws, limiting interpretability and generalization. We propose\nPhyDL-NWP, a physics-guided deep learning framework that integrates physical\nequations with latent force parameterization into data-driven models. It\npredicts weather variables from arbitrary spatiotemporal coordinates, computes\nphysical terms via automatic differentiation, and uses a physics-informed loss\nto align predictions with governing dynamics. PhyDL-NWP enables resolution-free\ndownscaling by modeling weather as a continuous function and fine-tunes\npre-trained models with minimal overhead, achieving up to 170x faster inference\nwith only 55K parameters. Experiments show that PhyDL-NWP improves both\nforecasting performance and physical consistency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Weather forecasting is essential but remains computationally intensive and\nphysically incomplete in traditional numerical weather prediction (NWP)\nmethods. Deep learning (DL) models offer efficiency and accuracy but often\nignore physical laws, limiting interpretability and generalization. We propose\nPhyDL-NWP, a physics-guided deep learning framework that integrates physical\nequations with latent force parameterization into data-driven models. It\npredicts weather variables from arbitrary spatiotemporal coordinates, computes\nphysical terms via automatic differentiation, and uses a physics-informed loss\nto align predictions with governing dynamics. PhyDL-NWP enables resolution-free\ndownscaling by modeling weather as a continuous function and fine-tunes\npre-trained models with minimal overhead, achieving up to 170x faster inference\nwith only 55K parameters. Experiments show that PhyDL-NWP improves both\nforecasting performance and physical consistency."
                },
                "authors": [
                    {
                        "name": "Yingtao Luo"
                    },
                    {
                        "name": "Shikai Fang"
                    },
                    {
                        "name": "Binqing Wu"
                    },
                    {
                        "name": "Qingsong Wen"
                    },
                    {
                        "name": "Liang Sun"
                    }
                ],
                "author_detail": {
                    "name": "Liang Sun"
                },
                "author": "Liang Sun",
                "arxiv_comment": "Published/Accepted in KDD 2025 (February Cycle)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14555v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14555v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14552v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14552v2",
                "updated": "2025-05-21T07:43:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    7,
                    43,
                    57,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-20T16:06:32Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    16,
                    6,
                    32,
                    1,
                    140,
                    0
                ],
                "title": "KORGym: A Dynamic Game Platform for LLM Reasoning Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KORGym: A Dynamic Game Platform for LLM Reasoning Evaluation"
                },
                "summary": "Recent advancements in large language models (LLMs) underscore the need for\nmore comprehensive evaluation methods to accurately assess their reasoning\ncapabilities. Existing benchmarks are often domain-specific and thus cannot\nfully capture an LLM's general reasoning potential. To address this limitation,\nwe introduce the Knowledge Orthogonal Reasoning Gymnasium (KORGym), a dynamic\nevaluation platform inspired by KOR-Bench and Gymnasium. KORGym offers over\nfifty games in either textual or visual formats and supports interactive,\nmulti-turn assessments with reinforcement learning scenarios. Using KORGym, we\nconduct extensive experiments on 19 LLMs and 8 VLMs, revealing consistent\nreasoning patterns within model families and demonstrating the superior\nperformance of closed-source models. Further analysis examines the effects of\nmodality, reasoning strategies, reinforcement learning techniques, and response\nlength on model performance. We expect KORGym to become a valuable resource for\nadvancing LLM reasoning research and developing evaluation methodologies suited\nto complex, interactive environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) underscore the need for\nmore comprehensive evaluation methods to accurately assess their reasoning\ncapabilities. Existing benchmarks are often domain-specific and thus cannot\nfully capture an LLM's general reasoning potential. To address this limitation,\nwe introduce the Knowledge Orthogonal Reasoning Gymnasium (KORGym), a dynamic\nevaluation platform inspired by KOR-Bench and Gymnasium. KORGym offers over\nfifty games in either textual or visual formats and supports interactive,\nmulti-turn assessments with reinforcement learning scenarios. Using KORGym, we\nconduct extensive experiments on 19 LLMs and 8 VLMs, revealing consistent\nreasoning patterns within model families and demonstrating the superior\nperformance of closed-source models. Further analysis examines the effects of\nmodality, reasoning strategies, reinforcement learning techniques, and response\nlength on model performance. We expect KORGym to become a valuable resource for\nadvancing LLM reasoning research and developing evaluation methodologies suited\nto complex, interactive environments."
                },
                "authors": [
                    {
                        "name": "Jiajun Shi"
                    },
                    {
                        "name": "Jian Yang"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Xingyuan Bu"
                    },
                    {
                        "name": "Jiangjie Chen"
                    },
                    {
                        "name": "Junting Zhou"
                    },
                    {
                        "name": "Kaijing Ma"
                    },
                    {
                        "name": "Zhoufutu Wen"
                    },
                    {
                        "name": "Bingli Wang"
                    },
                    {
                        "name": "Yancheng He"
                    },
                    {
                        "name": "Liang Song"
                    },
                    {
                        "name": "Hualei Zhu"
                    },
                    {
                        "name": "Shilong Li"
                    },
                    {
                        "name": "Xingjian Wang"
                    },
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Ruibin Yuan"
                    },
                    {
                        "name": "Yifan Yao"
                    },
                    {
                        "name": "Wenjun Yang"
                    },
                    {
                        "name": "Yunli Wang"
                    },
                    {
                        "name": "Siyuan Fang"
                    },
                    {
                        "name": "Siyu Yuan"
                    },
                    {
                        "name": "Qianyu He"
                    },
                    {
                        "name": "Xiangru Tang"
                    },
                    {
                        "name": "Yingshui Tan"
                    },
                    {
                        "name": "Wangchunshu Zhou"
                    },
                    {
                        "name": "Zhaoxiang Zhang"
                    },
                    {
                        "name": "Zhoujun Li"
                    },
                    {
                        "name": "Wenhao Huang"
                    },
                    {
                        "name": "Ge Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ge Zhang"
                },
                "author": "Ge Zhang",
                "arxiv_comment": "22 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14552v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14552v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00218v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00218v2",
                "updated": "2025-05-20T16:05:31Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    16,
                    5,
                    31,
                    1,
                    140,
                    0
                ],
                "published": "2024-10-31T21:34:29Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    21,
                    34,
                    29,
                    3,
                    305,
                    0
                ],
                "title": "Nudging state-space models for Bayesian filtering under misspecified\n  dynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nudging state-space models for Bayesian filtering under misspecified\n  dynamics"
                },
                "summary": "Nudging is a popular algorithmic strategy in numerical filtering to deal with\nthe problem of inference in high-dimensional dynamical systems. We demonstrate\nin this paper that general nudging techniques can also tackle another crucial\nstatistical problem in filtering, namely the misspecification of the transition\nkernel. Specifically, we rely on the formulation of nudging as a general\noperation increasing the likelihood and prove analytically that, when applied\ncarefully, nudging techniques implicitly define state-space models that have\nhigher marginal likelihoods for a given (fixed) sequence of observations. This\nprovides a theoretical justification of nudging techniques as data-informed\nalgorithmic modifications of state-space models to obtain robust models under\nmisspecified dynamics. To demonstrate the use of nudging, we provide numerical\nexperiments on linear Gaussian state-space models and a stochastic Lorenz 63\nmodel with misspecified dynamics and show that nudging offers a robust\nfiltering strategy for these cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nudging is a popular algorithmic strategy in numerical filtering to deal with\nthe problem of inference in high-dimensional dynamical systems. We demonstrate\nin this paper that general nudging techniques can also tackle another crucial\nstatistical problem in filtering, namely the misspecification of the transition\nkernel. Specifically, we rely on the formulation of nudging as a general\noperation increasing the likelihood and prove analytically that, when applied\ncarefully, nudging techniques implicitly define state-space models that have\nhigher marginal likelihoods for a given (fixed) sequence of observations. This\nprovides a theoretical justification of nudging techniques as data-informed\nalgorithmic modifications of state-space models to obtain robust models under\nmisspecified dynamics. To demonstrate the use of nudging, we provide numerical\nexperiments on linear Gaussian state-space models and a stochastic Lorenz 63\nmodel with misspecified dynamics and show that nudging offers a robust\nfiltering strategy for these cases."
                },
                "authors": [
                    {
                        "name": "Fabian Gonzalez"
                    },
                    {
                        "name": "O. Deniz Akyildiz"
                    },
                    {
                        "name": "Dan Crisan"
                    },
                    {
                        "name": "Joaquin Miguez"
                    }
                ],
                "author_detail": {
                    "name": "Joaquin Miguez"
                },
                "author": "Joaquin Miguez",
                "arxiv_comment": "Accepted for publication in Statistics and Computing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00218v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00218v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14549v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14549v1",
                "updated": "2025-05-20T16:05:05Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    16,
                    5,
                    5,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T16:05:05Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    16,
                    5,
                    5,
                    1,
                    140,
                    0
                ],
                "title": "Can Large Language Models Really Recognize Your Name?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Large Language Models Really Recognize Your Name?"
                },
                "summary": "Large language models (LLMs) are increasingly being used to protect sensitive\nuser data. However, current LLM-based privacy solutions assume that these\nmodels can reliably detect personally identifiable information (PII),\nparticularly named entities. In this paper, we challenge that assumption by\nrevealing systematic failures in LLM-based privacy tasks. Specifically, we show\nthat modern LLMs regularly overlook human names even in short text snippets due\nto ambiguous contexts, which cause the names to be misinterpreted or\nmishandled. We propose AMBENCH, a benchmark dataset of seemingly ambiguous\nhuman names, leveraging the name regularity bias phenomenon, embedded within\nconcise text snippets along with benign prompt injections. Our experiments on\nmodern LLMs tasked to detect PII as well as specialized tools show that recall\nof ambiguous names drops by 20--40% compared to more recognizable names.\nFurthermore, ambiguous human names are four times more likely to be ignored in\nsupposedly privacy-preserving summaries generated by LLMs when benign prompt\ninjections are present. These findings highlight the underexplored risks of\nrelying solely on LLMs to safeguard user privacy and underscore the need for a\nmore systematic investigation into their privacy failure modes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly being used to protect sensitive\nuser data. However, current LLM-based privacy solutions assume that these\nmodels can reliably detect personally identifiable information (PII),\nparticularly named entities. In this paper, we challenge that assumption by\nrevealing systematic failures in LLM-based privacy tasks. Specifically, we show\nthat modern LLMs regularly overlook human names even in short text snippets due\nto ambiguous contexts, which cause the names to be misinterpreted or\nmishandled. We propose AMBENCH, a benchmark dataset of seemingly ambiguous\nhuman names, leveraging the name regularity bias phenomenon, embedded within\nconcise text snippets along with benign prompt injections. Our experiments on\nmodern LLMs tasked to detect PII as well as specialized tools show that recall\nof ambiguous names drops by 20--40% compared to more recognizable names.\nFurthermore, ambiguous human names are four times more likely to be ignored in\nsupposedly privacy-preserving summaries generated by LLMs when benign prompt\ninjections are present. These findings highlight the underexplored risks of\nrelying solely on LLMs to safeguard user privacy and underscore the need for a\nmore systematic investigation into their privacy failure modes."
                },
                "authors": [
                    {
                        "name": "Dzung Pham"
                    },
                    {
                        "name": "Peter Kairouz"
                    },
                    {
                        "name": "Niloofar Mireshghallah"
                    },
                    {
                        "name": "Eugene Bagdasarian"
                    },
                    {
                        "name": "Chau Minh Pham"
                    },
                    {
                        "name": "Amir Houmansadr"
                    }
                ],
                "author_detail": {
                    "name": "Amir Houmansadr"
                },
                "author": "Amir Houmansadr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14549v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14549v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05136v8",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05136v8",
                "updated": "2025-05-20T16:04:22Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    16,
                    4,
                    22,
                    1,
                    140,
                    0
                ],
                "published": "2025-03-07T04:29:11Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    4,
                    29,
                    11,
                    4,
                    66,
                    0
                ],
                "title": "The Beginner's Textbook for Fully Homomorphic Encryption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Beginner's Textbook for Fully Homomorphic Encryption"
                },
                "summary": "Fully Homomorphic Encryption (FHE) is a cryptographic scheme that enables\ncomputations to be performed directly on encrypted data, as if the data were in\nplaintext. After all computations are performed on the encrypted data, it can\nbe decrypted to reveal the result. The decrypted value matches the result that\nwould have been obtained if the same computations were applied to the plaintext\ndata.\n  FHE supports basic operations such as addition and multiplication on\nencrypted numbers. Using these fundamental operations, more complex\ncomputations can be constructed, including subtraction, division, logic gates\n(e.g., AND, OR, XOR, NAND, MUX), and even advanced mathematical functions such\nas ReLU, sigmoid, and trigonometric functions (e.g., sin, cos). These functions\ncan be implemented either as exact formulas or as approximations, depending on\nthe trade-off between computational efficiency and accuracy.\n  FHE enables privacy-preserving machine learning by allowing a server to\nprocess the client's data in its encrypted form through an ML model. With FHE,\nthe server learns neither the plaintext version of the input features nor the\ninference results. Only the client, using their secret key, can decrypt and\naccess the results at the end of the service protocol. FHE can also be applied\nto confidential blockchain services, ensuring that sensitive data in smart\ncontracts remains encrypted and confidential while maintaining the transparency\nand integrity of the execution process. Other applications of FHE include\nsecure outsourcing of data analytics, encrypted database queries,\nprivacy-preserving searches, efficient multi-party computation for digital\nsignatures, and more.\n  As this book is an open project (https://fhetextbook.github.io), we welcome\nFHE experts to join us as collaborators to help expand the draft.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fully Homomorphic Encryption (FHE) is a cryptographic scheme that enables\ncomputations to be performed directly on encrypted data, as if the data were in\nplaintext. After all computations are performed on the encrypted data, it can\nbe decrypted to reveal the result. The decrypted value matches the result that\nwould have been obtained if the same computations were applied to the plaintext\ndata.\n  FHE supports basic operations such as addition and multiplication on\nencrypted numbers. Using these fundamental operations, more complex\ncomputations can be constructed, including subtraction, division, logic gates\n(e.g., AND, OR, XOR, NAND, MUX), and even advanced mathematical functions such\nas ReLU, sigmoid, and trigonometric functions (e.g., sin, cos). These functions\ncan be implemented either as exact formulas or as approximations, depending on\nthe trade-off between computational efficiency and accuracy.\n  FHE enables privacy-preserving machine learning by allowing a server to\nprocess the client's data in its encrypted form through an ML model. With FHE,\nthe server learns neither the plaintext version of the input features nor the\ninference results. Only the client, using their secret key, can decrypt and\naccess the results at the end of the service protocol. FHE can also be applied\nto confidential blockchain services, ensuring that sensitive data in smart\ncontracts remains encrypted and confidential while maintaining the transparency\nand integrity of the execution process. Other applications of FHE include\nsecure outsourcing of data analytics, encrypted database queries,\nprivacy-preserving searches, efficient multi-party computation for digital\nsignatures, and more.\n  As this book is an open project (https://fhetextbook.github.io), we welcome\nFHE experts to join us as collaborators to help expand the draft."
                },
                "authors": [
                    {
                        "name": "Ronny Ko"
                    }
                ],
                "author_detail": {
                    "name": "Ronny Ko"
                },
                "author": "Ronny Ko",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05136v8",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05136v8",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03331v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03331v3",
                "updated": "2025-05-20T16:03:43Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    16,
                    3,
                    43,
                    1,
                    140,
                    0
                ],
                "published": "2025-04-04T10:23:22Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    10,
                    23,
                    22,
                    4,
                    94,
                    0
                ],
                "title": "The Galactic-Centre Arms inferred from ACES (ALMA CMZ Exploration\n  Survey)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Galactic-Centre Arms inferred from ACES (ALMA CMZ Exploration\n  Survey)"
                },
                "summary": "Analyzing longitude-velocity diagrams (LVDs) in the CS(J=2-1) and\nH13CN(J=1-0) molecular lines from the internal release data of the ALMA\nCentral-Molecular-Zone Exploration Survey (ACES) and in the 13CO (J=1-0) line\nfrom the Nobeyama Galactic-Centre (GC) survey, we identify six GC Arms as\nprominent straight LV ridges. In addition to the currently known Arms I to IV,\nwe identify a new inner arm, Arm V, and further highlight the circum-nuclear\ndisc (CND) as Arm VI. Integrated intensity maps of the Arms on the sky suggest\nthat most of the Arms compose ring-like structures inclined from the Galactic\nplane. We determine the radii (curvatures) of the Arms using the\nvelocity-gradient ($dv/dl$) method, assuming that the arms are rotating on\ncircular orbits at a constant velocity of $\\sim 150$ km/s. We show that Arms I\nand II compose the main ring structure of the CMZ with radii $\\sim 100$--120\npc; Arm III is a dense arm 42 pc from the GC; Arm IV is a clear and narrow arm\n20 pc from the GC; and Arm V is a faint, long arm of 8.2 pc radius. We show\nthat the circum-nuclear disc (CND) composes the sixth arm, Arm VI, of radius\n$\\sim 2.3$ pc associated with bifurcated spiral fins. We also discuss the\nassociation of the 20- and 50-km/s clouds with these Arms. The radii of the\narms fall on an empirical relation $R\\sim 630 (2/5)^N$ for $N=1$ (Arm I) to 6\n(VI), suggesting either discrete rings or a logarithmic spiral with pitch angle\n$\\sim 22^\\circ$. The vertical full extent of the arm increases with radius and\nis represented by $z\\sim 0.7 (R/1 {\\rm pc})^{0.7}$ pc. The tilt angle of the\narms from the Galactic plane, or the warping, increases rapidly toward the GC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analyzing longitude-velocity diagrams (LVDs) in the CS(J=2-1) and\nH13CN(J=1-0) molecular lines from the internal release data of the ALMA\nCentral-Molecular-Zone Exploration Survey (ACES) and in the 13CO (J=1-0) line\nfrom the Nobeyama Galactic-Centre (GC) survey, we identify six GC Arms as\nprominent straight LV ridges. In addition to the currently known Arms I to IV,\nwe identify a new inner arm, Arm V, and further highlight the circum-nuclear\ndisc (CND) as Arm VI. Integrated intensity maps of the Arms on the sky suggest\nthat most of the Arms compose ring-like structures inclined from the Galactic\nplane. We determine the radii (curvatures) of the Arms using the\nvelocity-gradient ($dv/dl$) method, assuming that the arms are rotating on\ncircular orbits at a constant velocity of $\\sim 150$ km/s. We show that Arms I\nand II compose the main ring structure of the CMZ with radii $\\sim 100$--120\npc; Arm III is a dense arm 42 pc from the GC; Arm IV is a clear and narrow arm\n20 pc from the GC; and Arm V is a faint, long arm of 8.2 pc radius. We show\nthat the circum-nuclear disc (CND) composes the sixth arm, Arm VI, of radius\n$\\sim 2.3$ pc associated with bifurcated spiral fins. We also discuss the\nassociation of the 20- and 50-km/s clouds with these Arms. The radii of the\narms fall on an empirical relation $R\\sim 630 (2/5)^N$ for $N=1$ (Arm I) to 6\n(VI), suggesting either discrete rings or a logarithmic spiral with pitch angle\n$\\sim 22^\\circ$. The vertical full extent of the arm increases with radius and\nis represented by $z\\sim 0.7 (R/1 {\\rm pc})^{0.7}$ pc. The tilt angle of the\narms from the Galactic plane, or the warping, increases rapidly toward the GC."
                },
                "authors": [
                    {
                        "name": "Y. Sofue"
                    },
                    {
                        "name": "Tomo. Oka"
                    },
                    {
                        "name": "S. N. Longmore"
                    },
                    {
                        "name": "D. Walker"
                    },
                    {
                        "name": "A. Ginsburg"
                    },
                    {
                        "name": "J. D. Henshaw"
                    },
                    {
                        "name": "J. Bally"
                    },
                    {
                        "name": "A. T. Barnes"
                    },
                    {
                        "name": "C. Battersby"
                    },
                    {
                        "name": "L. Colzi"
                    },
                    {
                        "name": "P. Ho"
                    },
                    {
                        "name": "I. Jimenez-Serra"
                    },
                    {
                        "name": "J. M. D. Kruijssen"
                    },
                    {
                        "name": "E. Mills"
                    },
                    {
                        "name": "M. A. Petkova"
                    },
                    {
                        "name": "M. C. Sormani"
                    },
                    {
                        "name": "J. Wallace"
                    },
                    {
                        "name": "J. Armijos-Abendano"
                    },
                    {
                        "name": "K. M. Dutkowska"
                    },
                    {
                        "name": "R. Enokiya"
                    },
                    {
                        "name": "Y. Fukui"
                    },
                    {
                        "name": "P. Garcia"
                    },
                    {
                        "name": "A. Guzman"
                    },
                    {
                        "name": "C. Henkel"
                    },
                    {
                        "name": "P. -Y. Hsieh"
                    },
                    {
                        "name": "Y. Hu"
                    },
                    {
                        "name": "K. Immer"
                    },
                    {
                        "name": "D. Jeff"
                    },
                    {
                        "name": "R. S. Klessen"
                    },
                    {
                        "name": "K. Kohno"
                    },
                    {
                        "name": "M. R. Krumholz"
                    },
                    {
                        "name": "D. Lipman"
                    },
                    {
                        "name": "S. Martin"
                    },
                    {
                        "name": "M. R. Morris"
                    },
                    {
                        "name": "F. Nogueras-Lara"
                    },
                    {
                        "name": "M. Nonhebel"
                    },
                    {
                        "name": "J. Ott"
                    },
                    {
                        "name": "J. E. Pineda"
                    },
                    {
                        "name": "M. A. Requena-Torres"
                    },
                    {
                        "name": "V. M. Rivilla"
                    },
                    {
                        "name": "D. Riquelme-Vasquez"
                    },
                    {
                        "name": "A. Sanchez-Monge"
                    },
                    {
                        "name": "M. G. Santa-Maria"
                    },
                    {
                        "name": "H. A. Smith"
                    },
                    {
                        "name": "T. S. Tanvir"
                    },
                    {
                        "name": "V. Tolls"
                    },
                    {
                        "name": "Q. D. Wang"
                    }
                ],
                "author_detail": {
                    "name": "Q. D. Wang"
                },
                "author": "Q. D. Wang",
                "arxiv_comment": "Accepted for PASJ, 20 pages, 19 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03331v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03331v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.11733v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.11733v2",
                "updated": "2025-05-20T15:56:52Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    15,
                    56,
                    52,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-16T22:34:36Z",
                "published_parsed": [
                    2025,
                    5,
                    16,
                    22,
                    34,
                    36,
                    4,
                    136,
                    0
                ],
                "title": "MedCaseReasoning: Evaluating and learning diagnostic reasoning from\n  clinical case reports",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MedCaseReasoning: Evaluating and learning diagnostic reasoning from\n  clinical case reports"
                },
                "summary": "Doctors and patients alike increasingly use Large Language Models (LLMs) to\ndiagnose clinical cases. However, unlike domains such as math or coding, where\ncorrectness can be objectively defined by the final answer, medical diagnosis\nrequires both the outcome and the reasoning process to be accurate. Currently,\nwidely used medical benchmarks like MedQA and MMLU assess only accuracy in the\nfinal answer, overlooking the quality and faithfulness of the clinical\nreasoning process. To address this limitation, we introduce MedCaseReasoning,\nthe first open-access dataset for evaluating LLMs on their ability to align\nwith clinician-authored diagnostic reasoning. The dataset includes 14,489\ndiagnostic question-and-answer cases, each paired with detailed reasoning\nstatements derived from open-access medical case reports. We evaluate\nstate-of-the-art reasoning LLMs on MedCaseReasoning and find significant\nshortcomings in their diagnoses and reasoning: for instance, the top-performing\nopen-source model, DeepSeek-R1, achieves only 48% 10-shot diagnostic accuracy\nand mentions only 64% of the clinician reasoning statements (recall). However,\nwe demonstrate that fine-tuning LLMs on the reasoning traces derived from\nMedCaseReasoning significantly improves diagnostic accuracy and clinical\nreasoning recall by an average relative gain of 29% and 41%, respectively. The\nopen-source dataset, code, and models are available at\nhttps://github.com/kevinwu23/Stanford-MedCaseReasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Doctors and patients alike increasingly use Large Language Models (LLMs) to\ndiagnose clinical cases. However, unlike domains such as math or coding, where\ncorrectness can be objectively defined by the final answer, medical diagnosis\nrequires both the outcome and the reasoning process to be accurate. Currently,\nwidely used medical benchmarks like MedQA and MMLU assess only accuracy in the\nfinal answer, overlooking the quality and faithfulness of the clinical\nreasoning process. To address this limitation, we introduce MedCaseReasoning,\nthe first open-access dataset for evaluating LLMs on their ability to align\nwith clinician-authored diagnostic reasoning. The dataset includes 14,489\ndiagnostic question-and-answer cases, each paired with detailed reasoning\nstatements derived from open-access medical case reports. We evaluate\nstate-of-the-art reasoning LLMs on MedCaseReasoning and find significant\nshortcomings in their diagnoses and reasoning: for instance, the top-performing\nopen-source model, DeepSeek-R1, achieves only 48% 10-shot diagnostic accuracy\nand mentions only 64% of the clinician reasoning statements (recall). However,\nwe demonstrate that fine-tuning LLMs on the reasoning traces derived from\nMedCaseReasoning significantly improves diagnostic accuracy and clinical\nreasoning recall by an average relative gain of 29% and 41%, respectively. The\nopen-source dataset, code, and models are available at\nhttps://github.com/kevinwu23/Stanford-MedCaseReasoning."
                },
                "authors": [
                    {
                        "name": "Kevin Wu"
                    },
                    {
                        "name": "Eric Wu"
                    },
                    {
                        "name": "Rahul Thapa"
                    },
                    {
                        "name": "Kevin Wei"
                    },
                    {
                        "name": "Angela Zhang"
                    },
                    {
                        "name": "Arvind Suresh"
                    },
                    {
                        "name": "Jacqueline J. Tao"
                    },
                    {
                        "name": "Min Woo Sun"
                    },
                    {
                        "name": "Alejandro Lozano"
                    },
                    {
                        "name": "James Zou"
                    }
                ],
                "author_detail": {
                    "name": "James Zou"
                },
                "author": "James Zou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.11733v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.11733v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14536v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14536v1",
                "updated": "2025-05-20T15:55:31Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    15,
                    55,
                    31,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T15:55:31Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    15,
                    55,
                    31,
                    1,
                    140,
                    0
                ],
                "title": "Breaking Bad Tokens: Detoxification of LLMs Using Sparse Autoencoders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking Bad Tokens: Detoxification of LLMs Using Sparse Autoencoders"
                },
                "summary": "Large language models (LLMs) are now ubiquitous in user-facing applications,\nyet they still generate undesirable toxic outputs, including profanity,\nvulgarity, and derogatory remarks. Although numerous detoxification methods\nexist, most apply broad, surface-level fixes and can therefore easily be\ncircumvented by jailbreak attacks. In this paper we leverage sparse\nautoencoders (SAEs) to identify toxicity-related directions in the residual\nstream of models and perform targeted activation steering using the\ncorresponding decoder vectors. We introduce three tiers of steering\naggressiveness and evaluate them on GPT-2 Small and Gemma-2-2B, revealing\ntrade-offs between toxicity reduction and language fluency. At stronger\nsteering strengths, these causal interventions surpass competitive baselines in\nreducing toxicity by up to 20%, though fluency can degrade noticeably on GPT-2\nSmall depending on the aggressiveness. Crucially, standard NLP benchmark scores\nupon steering remain stable, indicating that the model's knowledge and general\nabilities are preserved. We further show that feature-splitting in wider SAEs\nhampers safety interventions, underscoring the importance of disentangled\nfeature learning. Our findings highlight both the promise and the current\nlimitations of SAE-based causal interventions for LLM detoxification, further\nsuggesting practical guidelines for safer language-model deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are now ubiquitous in user-facing applications,\nyet they still generate undesirable toxic outputs, including profanity,\nvulgarity, and derogatory remarks. Although numerous detoxification methods\nexist, most apply broad, surface-level fixes and can therefore easily be\ncircumvented by jailbreak attacks. In this paper we leverage sparse\nautoencoders (SAEs) to identify toxicity-related directions in the residual\nstream of models and perform targeted activation steering using the\ncorresponding decoder vectors. We introduce three tiers of steering\naggressiveness and evaluate them on GPT-2 Small and Gemma-2-2B, revealing\ntrade-offs between toxicity reduction and language fluency. At stronger\nsteering strengths, these causal interventions surpass competitive baselines in\nreducing toxicity by up to 20%, though fluency can degrade noticeably on GPT-2\nSmall depending on the aggressiveness. Crucially, standard NLP benchmark scores\nupon steering remain stable, indicating that the model's knowledge and general\nabilities are preserved. We further show that feature-splitting in wider SAEs\nhampers safety interventions, underscoring the importance of disentangled\nfeature learning. Our findings highlight both the promise and the current\nlimitations of SAE-based causal interventions for LLM detoxification, further\nsuggesting practical guidelines for safer language-model deployment."
                },
                "authors": [
                    {
                        "name": "Agam Goyal"
                    },
                    {
                        "name": "Vedant Rathi"
                    },
                    {
                        "name": "William Yeh"
                    },
                    {
                        "name": "Yian Wang"
                    },
                    {
                        "name": "Yuen Chen"
                    },
                    {
                        "name": "Hari Sundaram"
                    }
                ],
                "author_detail": {
                    "name": "Hari Sundaram"
                },
                "author": "Hari Sundaram",
                "arxiv_comment": "Preprint: 19 pages, 7 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14536v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14536v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06885v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06885v3",
                "updated": "2025-05-20T15:53:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    15,
                    53,
                    10,
                    1,
                    140,
                    0
                ],
                "published": "2024-10-09T13:46:34Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    13,
                    46,
                    34,
                    2,
                    283,
                    0
                ],
                "title": "F5-TTS: A Fairytaler that Fakes Fluent and Faithful Speech with Flow\n  Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "F5-TTS: A Fairytaler that Fakes Fluent and Faithful Speech with Flow\n  Matching"
                },
                "summary": "This paper introduces F5-TTS, a fully non-autoregressive text-to-speech\nsystem based on flow matching with Diffusion Transformer (DiT). Without\nrequiring complex designs such as duration model, text encoder, and phoneme\nalignment, the text input is simply padded with filler tokens to the same\nlength as input speech, and then the denoising is performed for speech\ngeneration, which was originally proved feasible by E2 TTS. However, the\noriginal design of E2 TTS makes it hard to follow due to its slow convergence\nand low robustness. To address these issues, we first model the input with\nConvNeXt to refine the text representation, making it easy to align with the\nspeech. We further propose an inference-time Sway Sampling strategy, which\nsignificantly improves our model's performance and efficiency. This sampling\nstrategy for flow step can be easily applied to existing flow matching based\nmodels without retraining. Our design allows faster training and achieves an\ninference RTF of 0.15, which is greatly improved compared to state-of-the-art\ndiffusion-based TTS models. Trained on a public 100K hours multilingual\ndataset, our F5-TTS exhibits highly natural and expressive zero-shot ability,\nseamless code-switching capability, and speed control efficiency. We have\nreleased all codes and checkpoints to promote community development, at\nhttps://SWivid.github.io/F5-TTS/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces F5-TTS, a fully non-autoregressive text-to-speech\nsystem based on flow matching with Diffusion Transformer (DiT). Without\nrequiring complex designs such as duration model, text encoder, and phoneme\nalignment, the text input is simply padded with filler tokens to the same\nlength as input speech, and then the denoising is performed for speech\ngeneration, which was originally proved feasible by E2 TTS. However, the\noriginal design of E2 TTS makes it hard to follow due to its slow convergence\nand low robustness. To address these issues, we first model the input with\nConvNeXt to refine the text representation, making it easy to align with the\nspeech. We further propose an inference-time Sway Sampling strategy, which\nsignificantly improves our model's performance and efficiency. This sampling\nstrategy for flow step can be easily applied to existing flow matching based\nmodels without retraining. Our design allows faster training and achieves an\ninference RTF of 0.15, which is greatly improved compared to state-of-the-art\ndiffusion-based TTS models. Trained on a public 100K hours multilingual\ndataset, our F5-TTS exhibits highly natural and expressive zero-shot ability,\nseamless code-switching capability, and speed control efficiency. We have\nreleased all codes and checkpoints to promote community development, at\nhttps://SWivid.github.io/F5-TTS/."
                },
                "authors": [
                    {
                        "name": "Yushen Chen"
                    },
                    {
                        "name": "Zhikang Niu"
                    },
                    {
                        "name": "Ziyang Ma"
                    },
                    {
                        "name": "Keqi Deng"
                    },
                    {
                        "name": "Chunhui Wang"
                    },
                    {
                        "name": "Jian Zhao"
                    },
                    {
                        "name": "Kai Yu"
                    },
                    {
                        "name": "Xie Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xie Chen"
                },
                "author": "Xie Chen",
                "arxiv_comment": "17 pages, 9 tables, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06885v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06885v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14530v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14530v1",
                "updated": "2025-05-20T15:49:15Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    15,
                    49,
                    15,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T15:49:15Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    15,
                    49,
                    15,
                    1,
                    140,
                    0
                ],
                "title": "Internal Chain-of-Thought: Empirical Evidence for Layer-wise Subtask\n  Scheduling in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Internal Chain-of-Thought: Empirical Evidence for Layer-wise Subtask\n  Scheduling in LLMs"
                },
                "summary": "We show that large language models (LLMs) exhibit an $\\textit{internal\nchain-of-thought}$: they sequentially decompose and execute composite tasks\nlayer-by-layer. Two claims ground our study: (i) distinct subtasks are learned\nat different network depths, and (ii) these subtasks are executed sequentially\nacross layers. On a benchmark of 15 two-step composite tasks, we employ\nlayer-from context-masking and propose a novel cross-task patching method,\nconfirming (i). To examine claim (ii), we apply LogitLens to decode hidden\nstates, revealing a consistent layerwise execution pattern. We further\nreplicate our analysis on the real-world $\\text{TRACE}$ benchmark, observing\nthe same stepwise dynamics. Together, our results enhance LLMs transparency by\nshowing their capacity to internally plan and execute subtasks (or\ninstructions), opening avenues for fine-grained, instruction-level activation\nsteering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We show that large language models (LLMs) exhibit an $\\textit{internal\nchain-of-thought}$: they sequentially decompose and execute composite tasks\nlayer-by-layer. Two claims ground our study: (i) distinct subtasks are learned\nat different network depths, and (ii) these subtasks are executed sequentially\nacross layers. On a benchmark of 15 two-step composite tasks, we employ\nlayer-from context-masking and propose a novel cross-task patching method,\nconfirming (i). To examine claim (ii), we apply LogitLens to decode hidden\nstates, revealing a consistent layerwise execution pattern. We further\nreplicate our analysis on the real-world $\\text{TRACE}$ benchmark, observing\nthe same stepwise dynamics. Together, our results enhance LLMs transparency by\nshowing their capacity to internally plan and execute subtasks (or\ninstructions), opening avenues for fine-grained, instruction-level activation\nsteering."
                },
                "authors": [
                    {
                        "name": "Zhipeng Yang"
                    },
                    {
                        "name": "Junzhuo Li"
                    },
                    {
                        "name": "Siyu Xia"
                    },
                    {
                        "name": "Xuming Hu"
                    }
                ],
                "author_detail": {
                    "name": "Xuming Hu"
                },
                "author": "Xuming Hu",
                "arxiv_comment": "27 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14530v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14530v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14528v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14528v1",
                "updated": "2025-05-20T15:48:34Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    15,
                    48,
                    34,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T15:48:34Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    15,
                    48,
                    34,
                    1,
                    140,
                    0
                ],
                "title": "BugRepro: Enhancing Android Bug Reproduction with Domain-Specific\n  Knowledge Integration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BugRepro: Enhancing Android Bug Reproduction with Domain-Specific\n  Knowledge Integration"
                },
                "summary": "Mobile application development is a fast-paced process where maintaining\nhigh-quality user experiences is crucial. Current bug reproduction methods\npredominantly depend on precise feature descriptions in bug reports. However,\nthe growing complexity and dynamism of modern software systems pose significant\nchallenges to this crucial quality assurance process, as ambiguous or\nincomplete steps-to-reproduce (S2Rs) in reports frequently impede effective\ndebugging and maintenance. To address these challenges, we propose BugRepro, a\nnovel technique that integrates domain-specific knowledge to enhance the\naccuracy and efficiency of bug reproduction. BugRepro adopts a\nRetrieval-Augmented Generation (RAG) approach. It retrieves similar bug reports\nalong with their corresponding S2R entities from an example-rich RAG document.\nThis document serves as a valuable reference for improving the accuracy of S2R\nentity extraction. In addition, BugRepro incorporates app-specific knowledge.\nIt explores the app's graphical user interface (GUI) and extracts UI transition\ngraphs. These graphs are used to guide large language models (LLMs) in their\nexploration process when they encounter bottlenecks. Our experiments\ndemonstrate the effectiveness of BugRepro. Our method significantly outperforms\ntwo state-of-the-art methods. For S2R entity extraction accuracy, it achieves\nimprovements of 8.85% and 28.89%. For bug reproduction success rate, the\nimprovements reach 74.55% and 152.63%. In reproduction efficiency, the gains\nare 0.72% and 76.68%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile application development is a fast-paced process where maintaining\nhigh-quality user experiences is crucial. Current bug reproduction methods\npredominantly depend on precise feature descriptions in bug reports. However,\nthe growing complexity and dynamism of modern software systems pose significant\nchallenges to this crucial quality assurance process, as ambiguous or\nincomplete steps-to-reproduce (S2Rs) in reports frequently impede effective\ndebugging and maintenance. To address these challenges, we propose BugRepro, a\nnovel technique that integrates domain-specific knowledge to enhance the\naccuracy and efficiency of bug reproduction. BugRepro adopts a\nRetrieval-Augmented Generation (RAG) approach. It retrieves similar bug reports\nalong with their corresponding S2R entities from an example-rich RAG document.\nThis document serves as a valuable reference for improving the accuracy of S2R\nentity extraction. In addition, BugRepro incorporates app-specific knowledge.\nIt explores the app's graphical user interface (GUI) and extracts UI transition\ngraphs. These graphs are used to guide large language models (LLMs) in their\nexploration process when they encounter bottlenecks. Our experiments\ndemonstrate the effectiveness of BugRepro. Our method significantly outperforms\ntwo state-of-the-art methods. For S2R entity extraction accuracy, it achieves\nimprovements of 8.85% and 28.89%. For bug reproduction success rate, the\nimprovements reach 74.55% and 152.63%. In reproduction efficiency, the gains\nare 0.72% and 76.68%."
                },
                "authors": [
                    {
                        "name": "Hongrong Yin"
                    },
                    {
                        "name": "Tao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tao Zhang"
                },
                "author": "Tao Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14528v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14528v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14524v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14524v1",
                "updated": "2025-05-20T15:46:59Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    15,
                    46,
                    59,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T15:46:59Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    15,
                    46,
                    59,
                    1,
                    140,
                    0
                ],
                "title": "Guarded Query Routing for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Guarded Query Routing for Large Language Models"
                },
                "summary": "Query routing, the task to route user queries to different large language\nmodel (LLM) endpoints, can be considered as a text classification problem.\nHowever, out-of-distribution queries must be handled properly, as those could\nbe questions about unrelated domains, queries in other languages, or even\ncontain unsafe text. Here, we thus study a \\emph{guarded} query routing\nproblem, for which we first introduce the Guarded Query Routing Benchmark\n(GQR-Bench), which covers three exemplary target domains (law, finance, and\nhealthcare), and seven datasets to test robustness against out-of-distribution\nqueries. We then use GQR-Bench to contrast the effectiveness and efficiency of\nLLM-based routing mechanisms (GPT-4o-mini, Llama-3.2-3B, and Llama-3.1-8B),\nstandard LLM-based guardrail approaches (LlamaGuard and NVIDIA NeMo\nGuardrails), continuous bag-of-words classifiers (WideMLP, fastText), and\ntraditional machine learning models (SVM, XGBoost). Our results show that\nWideMLP, enhanced with out-of-domain detection capabilities, yields the best\ntrade-off between accuracy (88\\%) and speed (<4ms). The embedding-based\nfastText excels at speed (<1ms) with acceptable accuracy (80\\%), whereas LLMs\nyield the highest accuracy (91\\%) but are comparatively slow (62ms for local\nLlama-3.1:8B and 669ms for remote GPT-4o-mini calls). Our findings challenge\nthe automatic reliance on LLMs for (guarded) query routing and provide concrete\nrecommendations for practical applications. GQR-Bench will be released as a\nPython package -- \\texttt{gqr}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Query routing, the task to route user queries to different large language\nmodel (LLM) endpoints, can be considered as a text classification problem.\nHowever, out-of-distribution queries must be handled properly, as those could\nbe questions about unrelated domains, queries in other languages, or even\ncontain unsafe text. Here, we thus study a \\emph{guarded} query routing\nproblem, for which we first introduce the Guarded Query Routing Benchmark\n(GQR-Bench), which covers three exemplary target domains (law, finance, and\nhealthcare), and seven datasets to test robustness against out-of-distribution\nqueries. We then use GQR-Bench to contrast the effectiveness and efficiency of\nLLM-based routing mechanisms (GPT-4o-mini, Llama-3.2-3B, and Llama-3.1-8B),\nstandard LLM-based guardrail approaches (LlamaGuard and NVIDIA NeMo\nGuardrails), continuous bag-of-words classifiers (WideMLP, fastText), and\ntraditional machine learning models (SVM, XGBoost). Our results show that\nWideMLP, enhanced with out-of-domain detection capabilities, yields the best\ntrade-off between accuracy (88\\%) and speed (<4ms). The embedding-based\nfastText excels at speed (<1ms) with acceptable accuracy (80\\%), whereas LLMs\nyield the highest accuracy (91\\%) but are comparatively slow (62ms for local\nLlama-3.1:8B and 669ms for remote GPT-4o-mini calls). Our findings challenge\nthe automatic reliance on LLMs for (guarded) query routing and provide concrete\nrecommendations for practical applications. GQR-Bench will be released as a\nPython package -- \\texttt{gqr}."
                },
                "authors": [
                    {
                        "name": "Richard Šléher"
                    },
                    {
                        "name": "William Brach"
                    },
                    {
                        "name": "Tibor Sloboda"
                    },
                    {
                        "name": "Kristián Košťál"
                    },
                    {
                        "name": "Lukas Galke"
                    }
                ],
                "author_detail": {
                    "name": "Lukas Galke"
                },
                "author": "Lukas Galke",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14524v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14524v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14521v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14521v2",
                "updated": "2025-05-21T07:06:19Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    7,
                    6,
                    19,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-20T15:44:54Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    15,
                    44,
                    54,
                    1,
                    140,
                    0
                ],
                "title": "Sparc3D: Sparse Representation and Construction for High-Resolution 3D\n  Shapes Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparc3D: Sparse Representation and Construction for High-Resolution 3D\n  Shapes Modeling"
                },
                "summary": "High-fidelity 3D object synthesis remains significantly more challenging than\n2D image generation due to the unstructured nature of mesh data and the cubic\ncomplexity of dense volumetric grids. Existing two-stage pipelines-compressing\nmeshes with a VAE (using either 2D or 3D supervision), followed by latent\ndiffusion sampling-often suffer from severe detail loss caused by inefficient\nrepresentations and modality mismatches introduced in VAE. We introduce\nSparc3D, a unified framework that combines a sparse deformable marching cubes\nrepresentation Sparcubes with a novel encoder Sparconv-VAE. Sparcubes converts\nraw meshes into high-resolution ($1024^3$) surfaces with arbitrary topology by\nscattering signed distance and deformation fields onto a sparse cube, allowing\ndifferentiable optimization. Sparconv-VAE is the first modality-consistent\nvariational autoencoder built entirely upon sparse convolutional networks,\nenabling efficient and near-lossless 3D reconstruction suitable for\nhigh-resolution generative modeling through latent diffusion. Sparc3D achieves\nstate-of-the-art reconstruction fidelity on challenging inputs, including open\nsurfaces, disconnected components, and intricate geometry. It preserves\nfine-grained shape details, reduces training and inference cost, and integrates\nnaturally with latent diffusion models for scalable, high-resolution 3D\ngeneration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-fidelity 3D object synthesis remains significantly more challenging than\n2D image generation due to the unstructured nature of mesh data and the cubic\ncomplexity of dense volumetric grids. Existing two-stage pipelines-compressing\nmeshes with a VAE (using either 2D or 3D supervision), followed by latent\ndiffusion sampling-often suffer from severe detail loss caused by inefficient\nrepresentations and modality mismatches introduced in VAE. We introduce\nSparc3D, a unified framework that combines a sparse deformable marching cubes\nrepresentation Sparcubes with a novel encoder Sparconv-VAE. Sparcubes converts\nraw meshes into high-resolution ($1024^3$) surfaces with arbitrary topology by\nscattering signed distance and deformation fields onto a sparse cube, allowing\ndifferentiable optimization. Sparconv-VAE is the first modality-consistent\nvariational autoencoder built entirely upon sparse convolutional networks,\nenabling efficient and near-lossless 3D reconstruction suitable for\nhigh-resolution generative modeling through latent diffusion. Sparc3D achieves\nstate-of-the-art reconstruction fidelity on challenging inputs, including open\nsurfaces, disconnected components, and intricate geometry. It preserves\nfine-grained shape details, reduces training and inference cost, and integrates\nnaturally with latent diffusion models for scalable, high-resolution 3D\ngeneration."
                },
                "authors": [
                    {
                        "name": "Zhihao Li"
                    },
                    {
                        "name": "Yufei Wang"
                    },
                    {
                        "name": "Heliang Zheng"
                    },
                    {
                        "name": "Yihao Luo"
                    },
                    {
                        "name": "Bihan Wen"
                    }
                ],
                "author_detail": {
                    "name": "Bihan Wen"
                },
                "author": "Bihan Wen",
                "arxiv_comment": "Homepage: https://lizhihao6.github.io/Sparc3D",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14521v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14521v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14518v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14518v1",
                "updated": "2025-05-20T15:44:01Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    15,
                    44,
                    1,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T15:44:01Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    15,
                    44,
                    1,
                    1,
                    140,
                    0
                ],
                "title": "Teaching Audio-Aware Large Language Models What Does Not Hear:\n  Mitigating Hallucinations through Synthesized Negative Samples",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Teaching Audio-Aware Large Language Models What Does Not Hear:\n  Mitigating Hallucinations through Synthesized Negative Samples"
                },
                "summary": "Recent advancements in audio-aware large language models (ALLMs) enable them\nto process and understand audio inputs. However, these models often hallucinate\nnon-existent sound events, reducing their reliability in real-world\napplications. To address this, we propose LISTEN (Learning to Identify Sounds\nThrough Extended Negative Samples), a contrastive-like training method that\nenhances ALLMs' ability to distinguish between present and absent sounds using\nsynthesized data from the backbone LLM. Unlike prior approaches, our method\nrequires no modification to LLM parameters and efficiently integrates audio\nrepresentations via a lightweight adapter. Experiments show that LISTEN\neffectively mitigates hallucinations while maintaining impressive performance\non existing audio question and reasoning benchmarks. At the same time, it is\nmore efficient in both data and computation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in audio-aware large language models (ALLMs) enable them\nto process and understand audio inputs. However, these models often hallucinate\nnon-existent sound events, reducing their reliability in real-world\napplications. To address this, we propose LISTEN (Learning to Identify Sounds\nThrough Extended Negative Samples), a contrastive-like training method that\nenhances ALLMs' ability to distinguish between present and absent sounds using\nsynthesized data from the backbone LLM. Unlike prior approaches, our method\nrequires no modification to LLM parameters and efficiently integrates audio\nrepresentations via a lightweight adapter. Experiments show that LISTEN\neffectively mitigates hallucinations while maintaining impressive performance\non existing audio question and reasoning benchmarks. At the same time, it is\nmore efficient in both data and computation."
                },
                "authors": [
                    {
                        "name": "Chun-Yi Kuan"
                    },
                    {
                        "name": "Hung-yi Lee"
                    }
                ],
                "author_detail": {
                    "name": "Hung-yi Lee"
                },
                "author": "Hung-yi Lee",
                "arxiv_comment": "Accepted to Interspeech 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14518v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14518v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14513v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14513v1",
                "updated": "2025-05-20T15:41:05Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    15,
                    41,
                    5,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T15:41:05Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    15,
                    41,
                    5,
                    1,
                    140,
                    0
                ],
                "title": "Latent Flow Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latent Flow Transformer"
                },
                "summary": "Transformers, the standard implementation for large language models (LLMs),\ntypically consist of tens to hundreds of discrete layers. While more layers can\nlead to better performance, this approach has been challenged as far from\nefficient, especially given the superiority of continuous layers demonstrated\nby diffusion and flow-based models for image generation. We propose the Latent\nFlow Transformer (LFT), which replaces a block of layers with a single learned\ntransport operator trained via flow matching, offering significant compression\nwhile maintaining compatibility with the original architecture. Additionally,\nwe address the limitations of existing flow-based methods in \\textit{preserving\ncoupling} by introducing the Flow Walking (FW) algorithm. On the Pythia-410M\nmodel, LFT trained with flow matching compresses 6 of 24 layers and outperforms\ndirectly skipping 2 layers (KL Divergence of LM logits at 0.407 vs. 0.529),\ndemonstrating the feasibility of this design. When trained with FW, LFT further\ndistills 12 layers into one while reducing the KL to 0.736 surpassing that from\nskipping 3 layers (0.932), significantly narrowing the gap between\nautoregressive and flow-based generation paradigms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers, the standard implementation for large language models (LLMs),\ntypically consist of tens to hundreds of discrete layers. While more layers can\nlead to better performance, this approach has been challenged as far from\nefficient, especially given the superiority of continuous layers demonstrated\nby diffusion and flow-based models for image generation. We propose the Latent\nFlow Transformer (LFT), which replaces a block of layers with a single learned\ntransport operator trained via flow matching, offering significant compression\nwhile maintaining compatibility with the original architecture. Additionally,\nwe address the limitations of existing flow-based methods in \\textit{preserving\ncoupling} by introducing the Flow Walking (FW) algorithm. On the Pythia-410M\nmodel, LFT trained with flow matching compresses 6 of 24 layers and outperforms\ndirectly skipping 2 layers (KL Divergence of LM logits at 0.407 vs. 0.529),\ndemonstrating the feasibility of this design. When trained with FW, LFT further\ndistills 12 layers into one while reducing the KL to 0.736 surpassing that from\nskipping 3 layers (0.932), significantly narrowing the gap between\nautoregressive and flow-based generation paradigms."
                },
                "authors": [
                    {
                        "name": "Yen-Chen Wu"
                    },
                    {
                        "name": "Feng-Ting Liao"
                    },
                    {
                        "name": "Meng-Hsi Chen"
                    },
                    {
                        "name": "Pei-Chen Ho"
                    },
                    {
                        "name": "Farhang Nabiei"
                    },
                    {
                        "name": "Da-shan Shiu"
                    }
                ],
                "author_detail": {
                    "name": "Da-shan Shiu"
                },
                "author": "Da-shan Shiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14513v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14513v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.12540v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.12540v2",
                "updated": "2025-05-20T15:38:41Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    15,
                    38,
                    41,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-18T20:37:07Z",
                "published_parsed": [
                    2025,
                    5,
                    18,
                    20,
                    37,
                    7,
                    6,
                    138,
                    0
                ],
                "title": "Harnessing the Universal Geometry of Embeddings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harnessing the Universal Geometry of Embeddings"
                },
                "summary": "We introduce the first method for translating text embeddings from one vector\nspace to another without any paired data, encoders, or predefined sets of\nmatches. Our unsupervised approach translates any embedding to and from a\nuniversal latent representation (i.e., a universal semantic structure\nconjectured by the Platonic Representation Hypothesis). Our translations\nachieve high cosine similarity across model pairs with different architectures,\nparameter counts, and training datasets.\n  The ability to translate unknown embeddings into a different space while\npreserving their geometry has serious implications for the security of vector\ndatabases. An adversary with access only to embedding vectors can extract\nsensitive information about the underlying documents, sufficient for\nclassification and attribute inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce the first method for translating text embeddings from one vector\nspace to another without any paired data, encoders, or predefined sets of\nmatches. Our unsupervised approach translates any embedding to and from a\nuniversal latent representation (i.e., a universal semantic structure\nconjectured by the Platonic Representation Hypothesis). Our translations\nachieve high cosine similarity across model pairs with different architectures,\nparameter counts, and training datasets.\n  The ability to translate unknown embeddings into a different space while\npreserving their geometry has serious implications for the security of vector\ndatabases. An adversary with access only to embedding vectors can extract\nsensitive information about the underlying documents, sufficient for\nclassification and attribute inference."
                },
                "authors": [
                    {
                        "name": "Rishi Jha"
                    },
                    {
                        "name": "Collin Zhang"
                    },
                    {
                        "name": "Vitaly Shmatikov"
                    },
                    {
                        "name": "John X. Morris"
                    }
                ],
                "author_detail": {
                    "name": "John X. Morris"
                },
                "author": "John X. Morris",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.12540v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.12540v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14505v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14505v1",
                "updated": "2025-05-20T15:34:36Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    15,
                    34,
                    36,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T15:34:36Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    15,
                    34,
                    36,
                    1,
                    140,
                    0
                ],
                "title": "ModRWKV: Transformer Multimodality in Linear Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ModRWKV: Transformer Multimodality in Linear Time"
                },
                "summary": "Currently, most multimodal studies are based on large language models (LLMs)\nwith quadratic-complexity Transformer architectures. While linear models like\nRNNs enjoy low inference costs, their application has been largely limited to\nthe text-only modality. This work explores the capabilities of modern RNN\narchitectures in multimodal contexts. We propose ModRWKV-a decoupled multimodal\nframework built upon the RWKV7 architecture as its LLM backbone-which achieves\nmulti-source information fusion through dynamically adaptable heterogeneous\nmodality encoders. We designed the multimodal modules in ModRWKV with an\nextremely lightweight architecture and, through extensive experiments,\nidentified a configuration that achieves an optimal balance between performance\nand computational efficiency. ModRWKV leverages the pretrained weights of the\nRWKV7 LLM for initialization, which significantly accelerates multimodal\ntraining. Comparative experiments with different pretrained checkpoints further\ndemonstrate that such initialization plays a crucial role in enhancing the\nmodel's ability to understand multimodal signals. Supported by extensive\nexperiments, we conclude that modern RNN architectures present a viable\nalternative to Transformers in the domain of multimodal large language models\n(MLLMs). Furthermore, we identify the optimal configuration of the ModRWKV\narchitecture through systematic exploration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Currently, most multimodal studies are based on large language models (LLMs)\nwith quadratic-complexity Transformer architectures. While linear models like\nRNNs enjoy low inference costs, their application has been largely limited to\nthe text-only modality. This work explores the capabilities of modern RNN\narchitectures in multimodal contexts. We propose ModRWKV-a decoupled multimodal\nframework built upon the RWKV7 architecture as its LLM backbone-which achieves\nmulti-source information fusion through dynamically adaptable heterogeneous\nmodality encoders. We designed the multimodal modules in ModRWKV with an\nextremely lightweight architecture and, through extensive experiments,\nidentified a configuration that achieves an optimal balance between performance\nand computational efficiency. ModRWKV leverages the pretrained weights of the\nRWKV7 LLM for initialization, which significantly accelerates multimodal\ntraining. Comparative experiments with different pretrained checkpoints further\ndemonstrate that such initialization plays a crucial role in enhancing the\nmodel's ability to understand multimodal signals. Supported by extensive\nexperiments, we conclude that modern RNN architectures present a viable\nalternative to Transformers in the domain of multimodal large language models\n(MLLMs). Furthermore, we identify the optimal configuration of the ModRWKV\narchitecture through systematic exploration."
                },
                "authors": [
                    {
                        "name": "Jiale Kang"
                    },
                    {
                        "name": "Ziyin Yue"
                    },
                    {
                        "name": "Qingyu Yin"
                    },
                    {
                        "name": "Jiang Rui"
                    },
                    {
                        "name": "Weile Li"
                    },
                    {
                        "name": "Zening Lu"
                    },
                    {
                        "name": "Zhouran Ji"
                    }
                ],
                "author_detail": {
                    "name": "Zhouran Ji"
                },
                "author": "Zhouran Ji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14505v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14505v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14502v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14502v1",
                "updated": "2025-05-20T15:30:38Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    15,
                    30,
                    38,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T15:30:38Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    15,
                    30,
                    38,
                    1,
                    140,
                    0
                ],
                "title": "Learning to Integrate Diffusion ODEs by Averaging the Derivatives",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Integrate Diffusion ODEs by Averaging the Derivatives"
                },
                "summary": "To accelerate diffusion model inference, numerical solvers perform poorly at\nextremely small steps, while distillation techniques often introduce complexity\nand instability. This work presents an intermediate strategy, balancing\nperformance and cost, by learning ODE integration using loss functions derived\nfrom the derivative-integral relationship, inspired by Monte Carlo integration\nand Picard iteration. From a geometric perspective, the losses operate by\ngradually extending the tangent to the secant, thus are named as secant losses.\nThe secant losses can rapidly convert (via fine-tuning or distillation) a\npretrained diffusion model into its secant version. In our experiments, the\nsecant version of EDM achieves a $10$-step FID of $2.14$ on CIFAR-10, while the\nsecant version of SiT-XL/2 attains a $4$-step FID of $2.27$ and an $8$-step FID\nof $1.96$ on ImageNet-$256\\times256$. Code will be available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To accelerate diffusion model inference, numerical solvers perform poorly at\nextremely small steps, while distillation techniques often introduce complexity\nand instability. This work presents an intermediate strategy, balancing\nperformance and cost, by learning ODE integration using loss functions derived\nfrom the derivative-integral relationship, inspired by Monte Carlo integration\nand Picard iteration. From a geometric perspective, the losses operate by\ngradually extending the tangent to the secant, thus are named as secant losses.\nThe secant losses can rapidly convert (via fine-tuning or distillation) a\npretrained diffusion model into its secant version. In our experiments, the\nsecant version of EDM achieves a $10$-step FID of $2.14$ on CIFAR-10, while the\nsecant version of SiT-XL/2 attains a $4$-step FID of $2.27$ and an $8$-step FID\nof $1.96$ on ImageNet-$256\\times256$. Code will be available."
                },
                "authors": [
                    {
                        "name": "Wenze Liu"
                    },
                    {
                        "name": "Xiangyu Yue"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyu Yue"
                },
                "author": "Xiangyu Yue",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14502v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14502v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09083v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09083v2",
                "updated": "2025-05-20T15:29:37Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    15,
                    29,
                    37,
                    1,
                    140,
                    0
                ],
                "published": "2024-10-06T08:33:39Z",
                "published_parsed": [
                    2024,
                    10,
                    6,
                    8,
                    33,
                    39,
                    6,
                    280,
                    0
                ],
                "title": "Evaluating the Correctness of Inference Patterns Used by LLMs for\n  Judgment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the Correctness of Inference Patterns Used by LLMs for\n  Judgment"
                },
                "summary": "This paper presents a method to analyze the inference patterns used by Large\nLanguage Models (LLMs) for judgment in a case study on legal LLMs, so as to\nidentify potential incorrect representations of the LLM, according to human\ndomain knowledge. Unlike traditional evaluations on language generation\nresults, we propose to evaluate the correctness of the detailed inference\npatterns of an LLM behind its seemingly correct outputs. To this end, we\nquantify the interactions between input phrases used by the LLM as primitive\ninference patterns, because recent theoretical achievements have proven several\nmathematical guarantees of the faithfulness of the interaction-based\nexplanation. We design a set of metrics to evaluate the detailed inference\npatterns of LLMs. Experiments show that even when the language generation\nresults appear correct, a significant portion of the inference patterns used by\nthe LLM for the legal judgment may represent misleading or irrelevant logic.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a method to analyze the inference patterns used by Large\nLanguage Models (LLMs) for judgment in a case study on legal LLMs, so as to\nidentify potential incorrect representations of the LLM, according to human\ndomain knowledge. Unlike traditional evaluations on language generation\nresults, we propose to evaluate the correctness of the detailed inference\npatterns of an LLM behind its seemingly correct outputs. To this end, we\nquantify the interactions between input phrases used by the LLM as primitive\ninference patterns, because recent theoretical achievements have proven several\nmathematical guarantees of the faithfulness of the interaction-based\nexplanation. We design a set of metrics to evaluate the detailed inference\npatterns of LLMs. Experiments show that even when the language generation\nresults appear correct, a significant portion of the inference patterns used by\nthe LLM for the legal judgment may represent misleading or irrelevant logic."
                },
                "authors": [
                    {
                        "name": "Lu Chen"
                    },
                    {
                        "name": "Yuxuan Huang"
                    },
                    {
                        "name": "Yixing Li"
                    },
                    {
                        "name": "Dongrui Liu"
                    },
                    {
                        "name": "Qihan Ren"
                    },
                    {
                        "name": "Shuai Zhao"
                    },
                    {
                        "name": "Kun Kuang"
                    },
                    {
                        "name": "Zilong Zheng"
                    },
                    {
                        "name": "Quanshi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Quanshi Zhang"
                },
                "author": "Quanshi Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09083v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09083v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.12482v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.12482v2",
                "updated": "2025-05-20T15:28:35Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    15,
                    28,
                    35,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-18T15:56:35Z",
                "published_parsed": [
                    2025,
                    5,
                    18,
                    15,
                    56,
                    35,
                    6,
                    138,
                    0
                ],
                "title": "Spectral-Spatial Self-Supervised Learning for Few-Shot Hyperspectral\n  Image Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spectral-Spatial Self-Supervised Learning for Few-Shot Hyperspectral\n  Image Classification"
                },
                "summary": "Few-shot classification of hyperspectral images (HSI) faces the challenge of\nscarce labeled samples. Self-Supervised learning (SSL) and Few-Shot Learning\n(FSL) offer promising avenues to address this issue. However, existing methods\noften struggle to adapt to the spatial geometric diversity of HSIs and lack\nsufficient spectral prior knowledge. To tackle these challenges, we propose a\nmethod, Spectral-Spatial Self-Supervised Learning for Few-Shot Hyperspectral\nImage Classification (S4L-FSC), aimed at improving the performance of few-shot\nHSI classification. Specifically, we first leverage heterogeneous datasets to\npretrain a spatial feature extractor using a designed Rotation-Mirror\nSelf-Supervised Learning (RM-SSL) method, combined with FSL. This approach\nenables the model to learn the spatial geometric diversity of HSIs using\nrotation and mirroring labels as supervisory signals, while acquiring\ntransferable spatial meta-knowledge through few-shot learning. Subsequently,\nhomogeneous datasets are utilized to pretrain a spectral feature extractor via\na combination of FSL and Masked Reconstruction Self-Supervised Learning\n(MR-SSL). The model learns to reconstruct original spectral information from\nrandomly masked spectral vectors, inferring spectral dependencies. In parallel,\nFSL guides the model to extract pixel-level discriminative features, thereby\nembedding rich spectral priors into the model. This spectral-spatial\npretraining method, along with the integration of knowledge from heterogeneous\nand homogeneous sources, significantly enhances model performance. Extensive\nexperiments on four HSI datasets demonstrate the effectiveness and superiority\nof the proposed S4L-FSC approach for few-shot HSI classification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Few-shot classification of hyperspectral images (HSI) faces the challenge of\nscarce labeled samples. Self-Supervised learning (SSL) and Few-Shot Learning\n(FSL) offer promising avenues to address this issue. However, existing methods\noften struggle to adapt to the spatial geometric diversity of HSIs and lack\nsufficient spectral prior knowledge. To tackle these challenges, we propose a\nmethod, Spectral-Spatial Self-Supervised Learning for Few-Shot Hyperspectral\nImage Classification (S4L-FSC), aimed at improving the performance of few-shot\nHSI classification. Specifically, we first leverage heterogeneous datasets to\npretrain a spatial feature extractor using a designed Rotation-Mirror\nSelf-Supervised Learning (RM-SSL) method, combined with FSL. This approach\nenables the model to learn the spatial geometric diversity of HSIs using\nrotation and mirroring labels as supervisory signals, while acquiring\ntransferable spatial meta-knowledge through few-shot learning. Subsequently,\nhomogeneous datasets are utilized to pretrain a spectral feature extractor via\na combination of FSL and Masked Reconstruction Self-Supervised Learning\n(MR-SSL). The model learns to reconstruct original spectral information from\nrandomly masked spectral vectors, inferring spectral dependencies. In parallel,\nFSL guides the model to extract pixel-level discriminative features, thereby\nembedding rich spectral priors into the model. This spectral-spatial\npretraining method, along with the integration of knowledge from heterogeneous\nand homogeneous sources, significantly enhances model performance. Extensive\nexperiments on four HSI datasets demonstrate the effectiveness and superiority\nof the proposed S4L-FSC approach for few-shot HSI classification."
                },
                "authors": [
                    {
                        "name": "Wenchen Chen"
                    },
                    {
                        "name": "Yanmei Zhang"
                    },
                    {
                        "name": "Zhongwei Xiao"
                    },
                    {
                        "name": "Jianping Chu"
                    },
                    {
                        "name": "Xingbo Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xingbo Wang"
                },
                "author": "Xingbo Wang",
                "arxiv_comment": "https://github.com/Wenchen-Chen/S4L-FSC",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.12482v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.12482v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14499v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14499v1",
                "updated": "2025-05-20T15:28:26Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    15,
                    28,
                    26,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T15:28:26Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    15,
                    28,
                    26,
                    1,
                    140,
                    0
                ],
                "title": "Enhanced Multimodal Aspect-Based Sentiment Analysis by LLM-Generated\n  Rationales",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhanced Multimodal Aspect-Based Sentiment Analysis by LLM-Generated\n  Rationales"
                },
                "summary": "There has been growing interest in Multimodal Aspect-Based Sentiment Analysis\n(MABSA) in recent years. Existing methods predominantly rely on pre-trained\nsmall language models (SLMs) to collect information related to aspects and\nsentiments from both image and text, with an aim to align these two modalities.\nHowever, small SLMs possess limited capacity and knowledge, often resulting in\ninaccurate identification of meaning, aspects, sentiments, and their\ninterconnections in textual and visual data. On the other hand, Large language\nmodels (LLMs) have shown exceptional capabilities in various tasks by\neffectively exploring fine-grained information in multimodal data. However,\nsome studies indicate that LLMs still fall short compared to fine-tuned small\nmodels in the field of ABSA. Based on these findings, we propose a novel\nframework, termed LRSA, which combines the decision-making capabilities of SLMs\nwith additional information provided by LLMs for MABSA. Specifically, we inject\nexplanations generated by LLMs as rationales into SLMs and employ a dual\ncross-attention mechanism for enhancing feature interaction and fusion, thereby\naugmenting the SLMs' ability to identify aspects and sentiments. We evaluated\nour method using two baseline models, numerous experiments highlight the\nsuperiority of our approach on three widely-used benchmarks, indicating its\ngeneralizability and applicability to most pre-trained models for MABSA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There has been growing interest in Multimodal Aspect-Based Sentiment Analysis\n(MABSA) in recent years. Existing methods predominantly rely on pre-trained\nsmall language models (SLMs) to collect information related to aspects and\nsentiments from both image and text, with an aim to align these two modalities.\nHowever, small SLMs possess limited capacity and knowledge, often resulting in\ninaccurate identification of meaning, aspects, sentiments, and their\ninterconnections in textual and visual data. On the other hand, Large language\nmodels (LLMs) have shown exceptional capabilities in various tasks by\neffectively exploring fine-grained information in multimodal data. However,\nsome studies indicate that LLMs still fall short compared to fine-tuned small\nmodels in the field of ABSA. Based on these findings, we propose a novel\nframework, termed LRSA, which combines the decision-making capabilities of SLMs\nwith additional information provided by LLMs for MABSA. Specifically, we inject\nexplanations generated by LLMs as rationales into SLMs and employ a dual\ncross-attention mechanism for enhancing feature interaction and fusion, thereby\naugmenting the SLMs' ability to identify aspects and sentiments. We evaluated\nour method using two baseline models, numerous experiments highlight the\nsuperiority of our approach on three widely-used benchmarks, indicating its\ngeneralizability and applicability to most pre-trained models for MABSA."
                },
                "authors": [
                    {
                        "name": "Jun Cao"
                    },
                    {
                        "name": "Jiyi Li"
                    },
                    {
                        "name": "Ziwei Yang"
                    },
                    {
                        "name": "Renjie Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Renjie Zhou"
                },
                "author": "Renjie Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14499v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14499v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.06993v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.06993v2",
                "updated": "2025-05-20T15:25:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    15,
                    25,
                    12,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-11T14:37:30Z",
                "published_parsed": [
                    2025,
                    5,
                    11,
                    14,
                    37,
                    30,
                    6,
                    131,
                    0
                ],
                "title": "Technical Report: Quantifying and Analyzing the Generalization Power of\n  a DNN",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Technical Report: Quantifying and Analyzing the Generalization Power of\n  a DNN"
                },
                "summary": "This paper proposes a new perspective for analyzing the generalization power\nof deep neural networks (DNNs), i.e., directly disentangling and analyzing the\ndynamics of generalizable and non-generalizable interaction encoded by a DNN\nthrough the training process. Specifically, this work builds upon the recent\ntheoretical achievement in explainble AI, which proves that the detailed\ninference logic of DNNs can be can be strictly rewritten as a small number of\nAND-OR interaction patterns. Based on this, we propose an efficient method to\nquantify the generalization power of each interaction, and we discover a\ndistinct three-phase dynamics of the generalization power of interactions\nduring training. In particular, the early phase of training typically removes\nnoisy and non-generalizable interactions and learns simple and generalizable\nones. The second and the third phases tend to capture increasingly complex\ninteractions that are harder to generalize. Experimental results verify that\nthe learning of non-generalizable interactions is the the direct cause for the\ngap between the training and testing losses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes a new perspective for analyzing the generalization power\nof deep neural networks (DNNs), i.e., directly disentangling and analyzing the\ndynamics of generalizable and non-generalizable interaction encoded by a DNN\nthrough the training process. Specifically, this work builds upon the recent\ntheoretical achievement in explainble AI, which proves that the detailed\ninference logic of DNNs can be can be strictly rewritten as a small number of\nAND-OR interaction patterns. Based on this, we propose an efficient method to\nquantify the generalization power of each interaction, and we discover a\ndistinct three-phase dynamics of the generalization power of interactions\nduring training. In particular, the early phase of training typically removes\nnoisy and non-generalizable interactions and learns simple and generalizable\nones. The second and the third phases tend to capture increasingly complex\ninteractions that are harder to generalize. Experimental results verify that\nthe learning of non-generalizable interactions is the the direct cause for the\ngap between the training and testing losses."
                },
                "authors": [
                    {
                        "name": "Yuxuan He"
                    },
                    {
                        "name": "Junpeng Zhang"
                    },
                    {
                        "name": "Lei Cheng"
                    },
                    {
                        "name": "Hongyuan Zhang"
                    },
                    {
                        "name": "Quanshi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Quanshi Zhang"
                },
                "author": "Quanshi Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.06993v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.06993v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14489v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14489v1",
                "updated": "2025-05-20T15:19:00Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    15,
                    19,
                    0,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T15:19:00Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    15,
                    19,
                    0,
                    1,
                    140,
                    0
                ],
                "title": "Reasoning Models Better Express Their Confidence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning Models Better Express Their Confidence"
                },
                "summary": "Despite their strengths, large language models (LLMs) often fail to\ncommunicate their confidence accurately, making it difficult to assess when\nthey might be wrong and limiting their reliability. In this work, we\ndemonstrate that reasoning models-LLMs that engage in extended chain-of-thought\n(CoT) reasoning-exhibit superior performance not only in problem-solving but\nalso in accurately expressing their confidence. Specifically, we benchmark six\nreasoning models across six datasets and find that they achieve strictly better\nconfidence calibration than their non-reasoning counterparts in 33 out of the\n36 settings. Our detailed analysis reveals that these gains in calibration stem\nfrom the slow thinking behaviors of reasoning models-such as exploring\nalternative approaches and backtracking-which enable them to adjust their\nconfidence dynamically throughout their CoT, making it progressively more\naccurate. In particular, we find that reasoning models become increasingly\nbetter calibrated as their CoT unfolds, a trend not observed in non-reasoning\nmodels. Moreover, removing slow thinking behaviors from the CoT leads to a\nsignificant drop in calibration. Lastly, we show that these gains are not\nexclusive to reasoning models-non-reasoning models also benefit when guided to\nperform slow thinking via in-context learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite their strengths, large language models (LLMs) often fail to\ncommunicate their confidence accurately, making it difficult to assess when\nthey might be wrong and limiting their reliability. In this work, we\ndemonstrate that reasoning models-LLMs that engage in extended chain-of-thought\n(CoT) reasoning-exhibit superior performance not only in problem-solving but\nalso in accurately expressing their confidence. Specifically, we benchmark six\nreasoning models across six datasets and find that they achieve strictly better\nconfidence calibration than their non-reasoning counterparts in 33 out of the\n36 settings. Our detailed analysis reveals that these gains in calibration stem\nfrom the slow thinking behaviors of reasoning models-such as exploring\nalternative approaches and backtracking-which enable them to adjust their\nconfidence dynamically throughout their CoT, making it progressively more\naccurate. In particular, we find that reasoning models become increasingly\nbetter calibrated as their CoT unfolds, a trend not observed in non-reasoning\nmodels. Moreover, removing slow thinking behaviors from the CoT leads to a\nsignificant drop in calibration. Lastly, we show that these gains are not\nexclusive to reasoning models-non-reasoning models also benefit when guided to\nperform slow thinking via in-context learning."
                },
                "authors": [
                    {
                        "name": "Dongkeun Yoon"
                    },
                    {
                        "name": "Seungone Kim"
                    },
                    {
                        "name": "Sohee Yang"
                    },
                    {
                        "name": "Sunkyoung Kim"
                    },
                    {
                        "name": "Soyeon Kim"
                    },
                    {
                        "name": "Yongil Kim"
                    },
                    {
                        "name": "Eunbi Choi"
                    },
                    {
                        "name": "Yireun Kim"
                    },
                    {
                        "name": "Minjoon Seo"
                    }
                ],
                "author_detail": {
                    "name": "Minjoon Seo"
                },
                "author": "Minjoon Seo",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14489v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14489v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02577v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02577v3",
                "updated": "2025-05-20T15:16:38Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    15,
                    16,
                    38,
                    1,
                    140,
                    0
                ],
                "published": "2025-02-04T18:53:42Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    18,
                    53,
                    42,
                    1,
                    35,
                    0
                ],
                "title": "A comparison of translation performance between DeepL and Supertext",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A comparison of translation performance between DeepL and Supertext"
                },
                "summary": "As strong machine translation (MT) systems are increasingly based on large\nlanguage models (LLMs), reliable quality benchmarking requires methods that\ncapture their ability to leverage extended context. This study compares two\ncommercial MT systems -- DeepL and Supertext -- by assessing their performance\non unsegmented texts. We evaluate translation quality across four language\ndirections with professional translators assessing segments with full\ndocument-level context. While segment-level assessments indicate no strong\npreference between the systems in most cases, document-level analysis reveals a\npreference for Supertext in three out of four language directions, suggesting\nsuperior consistency across longer texts. We advocate for more\ncontext-sensitive evaluation methodologies to ensure that MT quality\nassessments reflect real-world usability. We release all evaluation data and\nscripts for further analysis and reproduction at\nhttps://github.com/supertext/evaluation_deepl_supertext.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As strong machine translation (MT) systems are increasingly based on large\nlanguage models (LLMs), reliable quality benchmarking requires methods that\ncapture their ability to leverage extended context. This study compares two\ncommercial MT systems -- DeepL and Supertext -- by assessing their performance\non unsegmented texts. We evaluate translation quality across four language\ndirections with professional translators assessing segments with full\ndocument-level context. While segment-level assessments indicate no strong\npreference between the systems in most cases, document-level analysis reveals a\npreference for Supertext in three out of four language directions, suggesting\nsuperior consistency across longer texts. We advocate for more\ncontext-sensitive evaluation methodologies to ensure that MT quality\nassessments reflect real-world usability. We release all evaluation data and\nscripts for further analysis and reproduction at\nhttps://github.com/supertext/evaluation_deepl_supertext."
                },
                "authors": [
                    {
                        "name": "Alex Flückiger"
                    },
                    {
                        "name": "Chantal Amrhein"
                    },
                    {
                        "name": "Tim Graf"
                    },
                    {
                        "name": "Frédéric Odermatt"
                    },
                    {
                        "name": "Martin Pömsl"
                    },
                    {
                        "name": "Philippe Schläpfer"
                    },
                    {
                        "name": "Florian Schottmann"
                    },
                    {
                        "name": "Samuel Läubli"
                    }
                ],
                "author_detail": {
                    "name": "Samuel Läubli"
                },
                "author": "Samuel Läubli",
                "arxiv_comment": "Paper accepted at MT Summit 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02577v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02577v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14483v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14483v1",
                "updated": "2025-05-20T15:16:06Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    15,
                    16,
                    6,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T15:16:06Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    15,
                    16,
                    6,
                    1,
                    140,
                    0
                ],
                "title": "MoMoE: Mixture of Moderation Experts Framework for AI-Assisted Online\n  Governance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoMoE: Mixture of Moderation Experts Framework for AI-Assisted Online\n  Governance"
                },
                "summary": "Large language models (LLMs) have shown great potential in flagging harmful\ncontent in online communities. Yet, existing approaches for moderation require\na separate model for every community and are opaque in their decision-making,\nlimiting real-world adoption. We introduce Mixture of Moderation Experts\n(MoMoE), a modular, cross-community framework that adds post-hoc explanations\nto scalable content moderation. MoMoE orchestrates four operators -- Allocate,\nPredict, Aggregate, Explain -- and is instantiated as seven\ncommunity-specialized experts (MoMoE-Community) and five norm-violation experts\n(MoMoE-NormVio). On 30 unseen subreddits, the best variants obtain Micro-F1\nscores of 0.72 and 0.67, respectively, matching or surpassing strong fine-tuned\nbaselines while consistently producing concise and reliable explanations.\nAlthough community-specialized experts deliver the highest peak accuracy,\nnorm-violation experts provide steadier performance across domains. These\nfindings show that MoMoE yields scalable, transparent moderation without\nneeding per-community fine-tuning. More broadly, they suggest that lightweight,\nexplainable expert ensembles can guide future NLP and HCI research on\ntrustworthy human-AI governance of online communities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown great potential in flagging harmful\ncontent in online communities. Yet, existing approaches for moderation require\na separate model for every community and are opaque in their decision-making,\nlimiting real-world adoption. We introduce Mixture of Moderation Experts\n(MoMoE), a modular, cross-community framework that adds post-hoc explanations\nto scalable content moderation. MoMoE orchestrates four operators -- Allocate,\nPredict, Aggregate, Explain -- and is instantiated as seven\ncommunity-specialized experts (MoMoE-Community) and five norm-violation experts\n(MoMoE-NormVio). On 30 unseen subreddits, the best variants obtain Micro-F1\nscores of 0.72 and 0.67, respectively, matching or surpassing strong fine-tuned\nbaselines while consistently producing concise and reliable explanations.\nAlthough community-specialized experts deliver the highest peak accuracy,\nnorm-violation experts provide steadier performance across domains. These\nfindings show that MoMoE yields scalable, transparent moderation without\nneeding per-community fine-tuning. More broadly, they suggest that lightweight,\nexplainable expert ensembles can guide future NLP and HCI research on\ntrustworthy human-AI governance of online communities."
                },
                "authors": [
                    {
                        "name": "Agam Goyal"
                    },
                    {
                        "name": "Xianyang Zhan"
                    },
                    {
                        "name": "Yilun Chen"
                    },
                    {
                        "name": "Koustuv Saha"
                    },
                    {
                        "name": "Eshwar Chandrasekharan"
                    }
                ],
                "author_detail": {
                    "name": "Eshwar Chandrasekharan"
                },
                "author": "Eshwar Chandrasekharan",
                "arxiv_comment": "Preprint: 15 pages, 4 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14483v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14483v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14479v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14479v1",
                "updated": "2025-05-20T15:13:32Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    15,
                    13,
                    32,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T15:13:32Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    15,
                    13,
                    32,
                    1,
                    140,
                    0
                ],
                "title": "Towards Reliable Proof Generation with LLMs: A Neuro-Symbolic Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Reliable Proof Generation with LLMs: A Neuro-Symbolic Approach"
                },
                "summary": "Large language models (LLMs) struggle with formal domains that require\nrigorous logical deduction and symbolic reasoning, such as mathematical proof\ngeneration. We propose a neuro-symbolic approach that combines LLMs' generative\nstrengths with structured components to overcome this challenge. As a\nproof-of-concept, we focus on geometry problems. Our approach is two-fold: (1)\nwe retrieve analogous problems and use their proofs to guide the LLM, and (2) a\nformal verifier evaluates the generated proofs and provides feedback, helping\nthe model fix incorrect proofs. We demonstrate that our method significantly\nimproves proof accuracy for OpenAI's o1 model (58%-70% improvement); both\nanalogous problems and the verifier's feedback contribute to these gains. More\nbroadly, shifting to LLMs that generate provably correct conclusions could\ndramatically improve their reliability, accuracy and consistency, unlocking\ncomplex tasks and critical real-world applications that require\ntrustworthiness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) struggle with formal domains that require\nrigorous logical deduction and symbolic reasoning, such as mathematical proof\ngeneration. We propose a neuro-symbolic approach that combines LLMs' generative\nstrengths with structured components to overcome this challenge. As a\nproof-of-concept, we focus on geometry problems. Our approach is two-fold: (1)\nwe retrieve analogous problems and use their proofs to guide the LLM, and (2) a\nformal verifier evaluates the generated proofs and provides feedback, helping\nthe model fix incorrect proofs. We demonstrate that our method significantly\nimproves proof accuracy for OpenAI's o1 model (58%-70% improvement); both\nanalogous problems and the verifier's feedback contribute to these gains. More\nbroadly, shifting to LLMs that generate provably correct conclusions could\ndramatically improve their reliability, accuracy and consistency, unlocking\ncomplex tasks and critical real-world applications that require\ntrustworthiness."
                },
                "authors": [
                    {
                        "name": "Oren Sultan"
                    },
                    {
                        "name": "Eitan Stern"
                    },
                    {
                        "name": "Dafna Shahaf"
                    }
                ],
                "author_detail": {
                    "name": "Dafna Shahaf"
                },
                "author": "Dafna Shahaf",
                "arxiv_comment": "long paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14479v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14479v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14471v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14471v1",
                "updated": "2025-05-20T15:05:27Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    15,
                    5,
                    27,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T15:05:27Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    15,
                    5,
                    27,
                    1,
                    140,
                    0
                ],
                "title": "Adapting Pretrained Language Models for Citation Classification via\n  Self-Supervised Contrastive Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adapting Pretrained Language Models for Citation Classification via\n  Self-Supervised Contrastive Learning"
                },
                "summary": "Citation classification, which identifies the intention behind academic\ncitations, is pivotal for scholarly analysis. Previous works suggest\nfine-tuning pretrained language models (PLMs) on citation classification\ndatasets, reaping the reward of the linguistic knowledge they gained during\npretraining. However, directly fine-tuning for citation classification is\nchallenging due to labeled data scarcity, contextual noise, and spurious\nkeyphrase correlations. In this paper, we present a novel framework, Citss,\nthat adapts the PLMs to overcome these challenges. Citss introduces\nself-supervised contrastive learning to alleviate data scarcity, and is\nequipped with two specialized strategies to obtain the contrastive pairs:\nsentence-level cropping, which enhances focus on target citations within long\ncontexts, and keyphrase perturbation, which mitigates reliance on specific\nkeyphrases. Compared with previous works that are only designed for\nencoder-based PLMs, Citss is carefully developed to be compatible with both\nencoder-based PLMs and decoder-based LLMs, to embrace the benefits of enlarged\npretraining. Experiments with three benchmark datasets with both encoder-based\nPLMs and decoder-based LLMs demonstrate our superiority compared to the\nprevious state of the art. Our code is available at: github.com/LITONG99/Citss",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Citation classification, which identifies the intention behind academic\ncitations, is pivotal for scholarly analysis. Previous works suggest\nfine-tuning pretrained language models (PLMs) on citation classification\ndatasets, reaping the reward of the linguistic knowledge they gained during\npretraining. However, directly fine-tuning for citation classification is\nchallenging due to labeled data scarcity, contextual noise, and spurious\nkeyphrase correlations. In this paper, we present a novel framework, Citss,\nthat adapts the PLMs to overcome these challenges. Citss introduces\nself-supervised contrastive learning to alleviate data scarcity, and is\nequipped with two specialized strategies to obtain the contrastive pairs:\nsentence-level cropping, which enhances focus on target citations within long\ncontexts, and keyphrase perturbation, which mitigates reliance on specific\nkeyphrases. Compared with previous works that are only designed for\nencoder-based PLMs, Citss is carefully developed to be compatible with both\nencoder-based PLMs and decoder-based LLMs, to embrace the benefits of enlarged\npretraining. Experiments with three benchmark datasets with both encoder-based\nPLMs and decoder-based LLMs demonstrate our superiority compared to the\nprevious state of the art. Our code is available at: github.com/LITONG99/Citss"
                },
                "authors": [
                    {
                        "name": "Tong Li"
                    },
                    {
                        "name": "Jiachuan Wang"
                    },
                    {
                        "name": "Yongqi Zhang"
                    },
                    {
                        "name": "Shuangyin Li"
                    },
                    {
                        "name": "Lei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lei Chen"
                },
                "author": "Lei Chen",
                "arxiv_comment": "Manuscripts, accepted to KDD 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14471v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14471v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14469v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14469v1",
                "updated": "2025-05-20T15:05:03Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    15,
                    5,
                    3,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T15:05:03Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    15,
                    5,
                    3,
                    1,
                    140,
                    0
                ],
                "title": "Attributional Safety Failures in Large Language Models under Code-Mixed\n  Perturbations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attributional Safety Failures in Large Language Models under Code-Mixed\n  Perturbations"
                },
                "summary": "Recent advancements in LLMs have raised significant safety concerns,\nparticularly when dealing with code-mixed inputs and outputs. Our study\nsystematically investigates the increased susceptibility of LLMs to produce\nunsafe outputs from code-mixed prompts compared to monolingual English prompts.\nUtilizing explainability methods, we dissect the internal attribution shifts\ncausing model's harmful behaviors. In addition, we explore cultural dimensions\nby distinguishing between universally unsafe and culturally-specific unsafe\nqueries. This paper presents novel experimental insights, clarifying the\nmechanisms driving this phenomenon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in LLMs have raised significant safety concerns,\nparticularly when dealing with code-mixed inputs and outputs. Our study\nsystematically investigates the increased susceptibility of LLMs to produce\nunsafe outputs from code-mixed prompts compared to monolingual English prompts.\nUtilizing explainability methods, we dissect the internal attribution shifts\ncausing model's harmful behaviors. In addition, we explore cultural dimensions\nby distinguishing between universally unsafe and culturally-specific unsafe\nqueries. This paper presents novel experimental insights, clarifying the\nmechanisms driving this phenomenon."
                },
                "authors": [
                    {
                        "name": "Somnath Banerjee"
                    },
                    {
                        "name": "Pratyush Chatterjee"
                    },
                    {
                        "name": "Shanu Kumar"
                    },
                    {
                        "name": "Sayan Layek"
                    },
                    {
                        "name": "Parag Agrawal"
                    },
                    {
                        "name": "Rima Hazra"
                    },
                    {
                        "name": "Animesh Mukherjee"
                    }
                ],
                "author_detail": {
                    "name": "Animesh Mukherjee"
                },
                "author": "Animesh Mukherjee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14469v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14469v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14468v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14468v1",
                "updated": "2025-05-20T15:04:17Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    15,
                    4,
                    17,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T15:04:17Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    15,
                    4,
                    17,
                    1,
                    140,
                    0
                ],
                "title": "ServerlessLoRA: Minimizing Latency and Cost in Serverless Inference for\n  LoRA-Based LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ServerlessLoRA: Minimizing Latency and Cost in Serverless Inference for\n  LoRA-Based LLMs"
                },
                "summary": "Serverless computing has grown rapidly for serving Large Language Model (LLM)\ninference due to its pay-as-you-go pricing, fine-grained GPU usage, and rapid\nscaling. However, our analysis reveals that current serverless can effectively\nserve general LLM but fail with Low-Rank Adaptation (LoRA) inference due to\nthree key limitations: 1) massive parameter redundancy among functions where\n99% of weights are unnecessarily duplicated, 2) costly artifact loading latency\nbeyond LLM loading, and 3) magnified resource contention when serving multiple\nLoRA LLMs. These inefficiencies lead to massive GPU wastage, increased\nTime-To-First-Token (TTFT), and high monetary costs.\n  We propose ServerlessLoRA, a novel serverless inference system designed for\nfaster and cheaper LoRA LLM serving. ServerlessLoRA enables secure backbone LLM\nsharing across isolated LoRA functions to reduce redundancy. We design a\npre-loading method that pre-loads comprehensive LoRA artifacts to minimize\ncold-start latency. Furthermore, ServerlessLoRA employs contention aware\nbatching and offloading to mitigate GPU resource conflicts during bursty\nworkloads. Experiment on industrial workloads demonstrates that ServerlessLoRA\nreduces TTFT by up to 86% and cuts monetary costs by up to 89% compared to\nstate-of-the-art LLM inference solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serverless computing has grown rapidly for serving Large Language Model (LLM)\ninference due to its pay-as-you-go pricing, fine-grained GPU usage, and rapid\nscaling. However, our analysis reveals that current serverless can effectively\nserve general LLM but fail with Low-Rank Adaptation (LoRA) inference due to\nthree key limitations: 1) massive parameter redundancy among functions where\n99% of weights are unnecessarily duplicated, 2) costly artifact loading latency\nbeyond LLM loading, and 3) magnified resource contention when serving multiple\nLoRA LLMs. These inefficiencies lead to massive GPU wastage, increased\nTime-To-First-Token (TTFT), and high monetary costs.\n  We propose ServerlessLoRA, a novel serverless inference system designed for\nfaster and cheaper LoRA LLM serving. ServerlessLoRA enables secure backbone LLM\nsharing across isolated LoRA functions to reduce redundancy. We design a\npre-loading method that pre-loads comprehensive LoRA artifacts to minimize\ncold-start latency. Furthermore, ServerlessLoRA employs contention aware\nbatching and offloading to mitigate GPU resource conflicts during bursty\nworkloads. Experiment on industrial workloads demonstrates that ServerlessLoRA\nreduces TTFT by up to 86% and cuts monetary costs by up to 89% compared to\nstate-of-the-art LLM inference solutions."
                },
                "authors": [
                    {
                        "name": "Yifan Sui"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Hanfei Yu"
                    },
                    {
                        "name": "Yitao Hu"
                    },
                    {
                        "name": "Jianxun Li"
                    },
                    {
                        "name": "Hao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Wang"
                },
                "author": "Hao Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14468v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14468v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03680v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03680v3",
                "updated": "2025-05-20T15:04:15Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    15,
                    4,
                    15,
                    1,
                    140,
                    0
                ],
                "published": "2024-08-07T10:43:59Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    10,
                    43,
                    59,
                    2,
                    220,
                    0
                ],
                "title": "Smaller but Better: Self-Paced Knowledge Distillation for Lightweight\n  yet Effective LCMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Smaller but Better: Self-Paced Knowledge Distillation for Lightweight\n  yet Effective LCMs"
                },
                "summary": "Large code models (LCMs) have remarkably advanced the field of code\ngeneration. Despite their impressive capabilities, they still face practical\ndeployment issues, such as high inference costs, limited accessibility of\nproprietary LCMs, and adaptability issues of ultra-large LCMs. These issues\nhighlight the critical need for more accessible, lightweight yet effective\nLCMs. Knowledge distillation (KD) offers a promising solution, which transfers\nthe programming capabilities of larger, advanced LCMs to smaller, less powerful\nLCMs. In this paper, we propose a novel Self-Paced knOwledge DistillAtion\nframework, named SODA, aiming at developing lightweight yet effective student\nLCMs. SODA consists of three stages in one cycle: (1) Correct-and-Fault\nKnowledge Delivery stage aims at improving the student models capability to\nrecognize errors while ensuring its basic programming skill during the\nknowledge transferring, which involves correctness-aware supervised learning\nand fault-aware contrastive learning methods. (2) Multi-View Feedback stage\naims at measuring the quality of results generated by the student model from\ntwo views, including model-based and static tool-based measurement, for\nidentifying the difficult questions. (3) Feedback-based Knowledge Update stage\naims at updating the student model adaptively by generating new questions at\ndifferent difficulty levels, in which the difficulty levels are categorized\nbased on the feedback in the second stage. Experimental results show that SODA\nimproves the student model by 65.96% in terms of average Pass@1, outperforming\nthe best baseline by 29.85%. Based on the SODA framework, we develop SodaCoder,\na series of lightweight yet effective LCMs, which outperform 15 LCMs with less\nthan or equal to 16B parameters. Notably, SodaCoder-DS-6.7B, built on\nDeepseekCoder-6.7B, even surpasses the prominent ChatGPT on average Pass@1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large code models (LCMs) have remarkably advanced the field of code\ngeneration. Despite their impressive capabilities, they still face practical\ndeployment issues, such as high inference costs, limited accessibility of\nproprietary LCMs, and adaptability issues of ultra-large LCMs. These issues\nhighlight the critical need for more accessible, lightweight yet effective\nLCMs. Knowledge distillation (KD) offers a promising solution, which transfers\nthe programming capabilities of larger, advanced LCMs to smaller, less powerful\nLCMs. In this paper, we propose a novel Self-Paced knOwledge DistillAtion\nframework, named SODA, aiming at developing lightweight yet effective student\nLCMs. SODA consists of three stages in one cycle: (1) Correct-and-Fault\nKnowledge Delivery stage aims at improving the student models capability to\nrecognize errors while ensuring its basic programming skill during the\nknowledge transferring, which involves correctness-aware supervised learning\nand fault-aware contrastive learning methods. (2) Multi-View Feedback stage\naims at measuring the quality of results generated by the student model from\ntwo views, including model-based and static tool-based measurement, for\nidentifying the difficult questions. (3) Feedback-based Knowledge Update stage\naims at updating the student model adaptively by generating new questions at\ndifferent difficulty levels, in which the difficulty levels are categorized\nbased on the feedback in the second stage. Experimental results show that SODA\nimproves the student model by 65.96% in terms of average Pass@1, outperforming\nthe best baseline by 29.85%. Based on the SODA framework, we develop SodaCoder,\na series of lightweight yet effective LCMs, which outperform 15 LCMs with less\nthan or equal to 16B parameters. Notably, SodaCoder-DS-6.7B, built on\nDeepseekCoder-6.7B, even surpasses the prominent ChatGPT on average Pass@1."
                },
                "authors": [
                    {
                        "name": "Yujia Chen"
                    },
                    {
                        "name": "Yang Ye"
                    },
                    {
                        "name": "Zhongqi Li"
                    },
                    {
                        "name": "Yuchi Ma"
                    },
                    {
                        "name": "Cuiyun Gao"
                    }
                ],
                "author_detail": {
                    "name": "Cuiyun Gao"
                },
                "author": "Cuiyun Gao",
                "arxiv_comment": "Accepted by FSE 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03680v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03680v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14467v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14467v1",
                "updated": "2025-05-20T15:01:56Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    15,
                    1,
                    56,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T15:01:56Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    15,
                    1,
                    56,
                    1,
                    140,
                    0
                ],
                "title": "Void in Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Void in Language Models"
                },
                "summary": "Despite advances in transformer-based language models (LMs), a fundamental\nquestion remains largely unanswered: Are all layers activated during inference?\nWe investigate this question by detecting unactivated layers (which we refer to\nas Voids) using a non-trainable and parameter-free adaptive computation method\ncalled L2 Adaptive Computation (LAC). We adapt LAC from its original\nefficiency-focused application to trace activated layers during inference. This\nmethod monitors changes in the L2-norm of activations to identify voids. We\nanalyze layer activation in instruction-tuned LMs across two phases: Prompt\nProcessing (PP), where we trace activated layers for each token in the input\nprompts, and Response Generation (RG), where we trace activated layers for each\ngenerated token. We further demonstrate that distinct layers are activated\nduring these two phases. To show the effectiveness of our method, we evaluated\nthree distinct instruction-tuned LMs from the Llama, Mistral, and Qwen families\non three benchmarks: MMLU, GPQA Diamond, and BoolQ. For example, on MMLU with a\nzero-shot setting, skipping voids in Qwen2.5-7B-Instruct resulted in an\nimprovement from 69.24 to 71.29 while the model uses only 30% of the layers.\nSimilarly, Mistral-7B-Instruct-v0.3 on GPQA Diamond improved from 13.88 to\n18.36 when using 70% of the layers during both the PP and RG phases. These\nresults show that not all layers contribute equally during inference, and that\nselectively skipping most of them can improve the performance of models on\ncertain tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite advances in transformer-based language models (LMs), a fundamental\nquestion remains largely unanswered: Are all layers activated during inference?\nWe investigate this question by detecting unactivated layers (which we refer to\nas Voids) using a non-trainable and parameter-free adaptive computation method\ncalled L2 Adaptive Computation (LAC). We adapt LAC from its original\nefficiency-focused application to trace activated layers during inference. This\nmethod monitors changes in the L2-norm of activations to identify voids. We\nanalyze layer activation in instruction-tuned LMs across two phases: Prompt\nProcessing (PP), where we trace activated layers for each token in the input\nprompts, and Response Generation (RG), where we trace activated layers for each\ngenerated token. We further demonstrate that distinct layers are activated\nduring these two phases. To show the effectiveness of our method, we evaluated\nthree distinct instruction-tuned LMs from the Llama, Mistral, and Qwen families\non three benchmarks: MMLU, GPQA Diamond, and BoolQ. For example, on MMLU with a\nzero-shot setting, skipping voids in Qwen2.5-7B-Instruct resulted in an\nimprovement from 69.24 to 71.29 while the model uses only 30% of the layers.\nSimilarly, Mistral-7B-Instruct-v0.3 on GPQA Diamond improved from 13.88 to\n18.36 when using 70% of the layers during both the PP and RG phases. These\nresults show that not all layers contribute equally during inference, and that\nselectively skipping most of them can improve the performance of models on\ncertain tasks."
                },
                "authors": [
                    {
                        "name": "Mani Shemiranifar"
                    }
                ],
                "author_detail": {
                    "name": "Mani Shemiranifar"
                },
                "author": "Mani Shemiranifar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14467v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14467v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10657v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10657v2",
                "updated": "2025-05-20T14:59:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    14,
                    59,
                    21,
                    1,
                    140,
                    0
                ],
                "published": "2025-03-08T04:07:07Z",
                "published_parsed": [
                    2025,
                    3,
                    8,
                    4,
                    7,
                    7,
                    5,
                    67,
                    0
                ],
                "title": "RouterEval: A Comprehensive Benchmark for Routing LLMs to Explore\n  Model-level Scaling Up in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RouterEval: A Comprehensive Benchmark for Routing LLMs to Explore\n  Model-level Scaling Up in LLMs"
                },
                "summary": "Routing large language models (LLMs) is a new paradigm that uses a router to\nrecommend the best LLM from a pool of candidates for a given input. In this\npaper, our comprehensive analysis with more than 8,500 LLMs reveals a novel\nmodel-level scaling up phenomenon in Routing LLMs, i.e., a capable router can\nsignificantly enhance the performance of this paradigm as the number of\ncandidates increases. This improvement can even surpass the performance of the\nbest single model in the pool and many existing strong LLMs, confirming it a\nhighly promising paradigm. However, the lack of comprehensive and open-source\nbenchmarks for Routing LLMs has hindered the development of routers. In this\npaper, we introduce RouterEval, a benchmark tailored for router research, which\nincludes over 200,000,000 performance records for 12 popular LLM evaluations\nacross various areas such as commonsense reasoning, semantic understanding,\netc., based on over 8,500 various LLMs. Using RouterEval, extensive evaluations\nof existing Routing LLM methods reveal that most still have significant room\nfor improvement. See https://github.com/MilkThink-Lab/RouterEval for all data,\ncode and tutorial.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Routing large language models (LLMs) is a new paradigm that uses a router to\nrecommend the best LLM from a pool of candidates for a given input. In this\npaper, our comprehensive analysis with more than 8,500 LLMs reveals a novel\nmodel-level scaling up phenomenon in Routing LLMs, i.e., a capable router can\nsignificantly enhance the performance of this paradigm as the number of\ncandidates increases. This improvement can even surpass the performance of the\nbest single model in the pool and many existing strong LLMs, confirming it a\nhighly promising paradigm. However, the lack of comprehensive and open-source\nbenchmarks for Routing LLMs has hindered the development of routers. In this\npaper, we introduce RouterEval, a benchmark tailored for router research, which\nincludes over 200,000,000 performance records for 12 popular LLM evaluations\nacross various areas such as commonsense reasoning, semantic understanding,\netc., based on over 8,500 various LLMs. Using RouterEval, extensive evaluations\nof existing Routing LLM methods reveal that most still have significant room\nfor improvement. See https://github.com/MilkThink-Lab/RouterEval for all data,\ncode and tutorial."
                },
                "authors": [
                    {
                        "name": "Zhongzhan Huang"
                    },
                    {
                        "name": "Guoming Ling"
                    },
                    {
                        "name": "Yupei Lin"
                    },
                    {
                        "name": "Yandong Chen"
                    },
                    {
                        "name": "Shanshan Zhong"
                    },
                    {
                        "name": "Hefeng Wu"
                    },
                    {
                        "name": "Liang Lin"
                    }
                ],
                "author_detail": {
                    "name": "Liang Lin"
                },
                "author": "Liang Lin",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10657v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10657v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13346v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13346v2",
                "updated": "2025-05-20T14:57:18Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    14,
                    57,
                    18,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-19T16:50:35Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    16,
                    50,
                    35,
                    0,
                    139,
                    0
                ],
                "title": "J4R: Learning to Judge with Equivalent Initial State Group Relative\n  Policy Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "J4R: Learning to Judge with Equivalent Initial State Group Relative\n  Policy Optimization"
                },
                "summary": "To keep pace with the increasing pace of large language models (LLM)\ndevelopment, model output evaluation has transitioned away from time-consuming\nhuman evaluation to automatic evaluation, where LLMs themselves are tasked with\nassessing and critiquing other model outputs. LLM-as-judge models are a class\nof generative evaluators that excel in evaluating relatively simple domains,\nlike chat quality, but struggle in reasoning intensive domains where model\nresponses contain more substantive and challenging content. To remedy existing\njudge shortcomings, we explore training judges with reinforcement learning\n(RL). We make three key contributions: (1) We propose the Equivalent Initial\nState Group Relative Policy Optimization (EIS-GRPO) algorithm, which allows us\nto train our judge to be robust to positional biases that arise in more complex\nevaluation settings. (2) We introduce ReasoningJudgeBench, a benchmark that\nevaluates judges in diverse reasoning settings not covered by prior work. (3)\nWe train Judge for Reasoning (J4R), a 7B judge trained with EIS-GRPO that\noutperforms GPT-4o and the next best small judge by 6.7% and 9%, matching or\nexceeding the performance of larger GRPO-trained judges on both JudgeBench and\nReasoningJudgeBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To keep pace with the increasing pace of large language models (LLM)\ndevelopment, model output evaluation has transitioned away from time-consuming\nhuman evaluation to automatic evaluation, where LLMs themselves are tasked with\nassessing and critiquing other model outputs. LLM-as-judge models are a class\nof generative evaluators that excel in evaluating relatively simple domains,\nlike chat quality, but struggle in reasoning intensive domains where model\nresponses contain more substantive and challenging content. To remedy existing\njudge shortcomings, we explore training judges with reinforcement learning\n(RL). We make three key contributions: (1) We propose the Equivalent Initial\nState Group Relative Policy Optimization (EIS-GRPO) algorithm, which allows us\nto train our judge to be robust to positional biases that arise in more complex\nevaluation settings. (2) We introduce ReasoningJudgeBench, a benchmark that\nevaluates judges in diverse reasoning settings not covered by prior work. (3)\nWe train Judge for Reasoning (J4R), a 7B judge trained with EIS-GRPO that\noutperforms GPT-4o and the next best small judge by 6.7% and 9%, matching or\nexceeding the performance of larger GRPO-trained judges on both JudgeBench and\nReasoningJudgeBench."
                },
                "authors": [
                    {
                        "name": "Austin Xu"
                    },
                    {
                        "name": "Yilun Zhou"
                    },
                    {
                        "name": "Xuan-Phi Nguyen"
                    },
                    {
                        "name": "Caiming Xiong"
                    },
                    {
                        "name": "Shafiq Joty"
                    }
                ],
                "author_detail": {
                    "name": "Shafiq Joty"
                },
                "author": "Shafiq Joty",
                "arxiv_comment": "25 pages, 4 figures, 6 tables. To be updated with links for\n  code/benchmark",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13346v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13346v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14460v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14460v1",
                "updated": "2025-05-20T14:56:50Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    14,
                    56,
                    50,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T14:56:50Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    14,
                    56,
                    50,
                    1,
                    140,
                    0
                ],
                "title": "VisualQuality-R1: Reasoning-Induced Image Quality Assessment via\n  Reinforcement Learning to Rank",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VisualQuality-R1: Reasoning-Induced Image Quality Assessment via\n  Reinforcement Learning to Rank"
                },
                "summary": "DeepSeek-R1 has demonstrated remarkable effectiveness in incentivizing\nreasoning and generalization capabilities of large language models (LLMs)\nthrough reinforcement learning. Nevertheless, the potential of\nreasoning-induced computational modeling has not been thoroughly explored in\nthe context of image quality assessment (IQA), a task critically dependent on\nvisual reasoning. In this paper, we introduce VisualQuality-R1, a\nreasoning-induced no-reference IQA (NR-IQA) model, and we train it with\nreinforcement learning to rank, a learning algorithm tailored to the\nintrinsically relative nature of visual quality. Specifically, for a pair of\nimages, we employ group relative policy optimization to generate multiple\nquality scores for each image. These estimates are then used to compute\ncomparative probabilities of one image having higher quality than the other\nunder the Thurstone model. Rewards for each quality estimate are defined using\ncontinuous fidelity measures rather than discretized binary labels. Extensive\nexperiments show that the proposed VisualQuality-R1 consistently outperforms\ndiscriminative deep learning-based NR-IQA models as well as a recent\nreasoning-induced quality regression method. Moreover, VisualQuality-R1 is\ncapable of generating contextually rich, human-aligned quality descriptions,\nand supports multi-dataset training without requiring perceptual scale\nrealignment. These features make VisualQuality-R1 especially well-suited for\nreliably measuring progress in a wide range of image processing tasks like\nsuper-resolution and image generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepSeek-R1 has demonstrated remarkable effectiveness in incentivizing\nreasoning and generalization capabilities of large language models (LLMs)\nthrough reinforcement learning. Nevertheless, the potential of\nreasoning-induced computational modeling has not been thoroughly explored in\nthe context of image quality assessment (IQA), a task critically dependent on\nvisual reasoning. In this paper, we introduce VisualQuality-R1, a\nreasoning-induced no-reference IQA (NR-IQA) model, and we train it with\nreinforcement learning to rank, a learning algorithm tailored to the\nintrinsically relative nature of visual quality. Specifically, for a pair of\nimages, we employ group relative policy optimization to generate multiple\nquality scores for each image. These estimates are then used to compute\ncomparative probabilities of one image having higher quality than the other\nunder the Thurstone model. Rewards for each quality estimate are defined using\ncontinuous fidelity measures rather than discretized binary labels. Extensive\nexperiments show that the proposed VisualQuality-R1 consistently outperforms\ndiscriminative deep learning-based NR-IQA models as well as a recent\nreasoning-induced quality regression method. Moreover, VisualQuality-R1 is\ncapable of generating contextually rich, human-aligned quality descriptions,\nand supports multi-dataset training without requiring perceptual scale\nrealignment. These features make VisualQuality-R1 especially well-suited for\nreliably measuring progress in a wide range of image processing tasks like\nsuper-resolution and image generation."
                },
                "authors": [
                    {
                        "name": "Tianhe Wu"
                    },
                    {
                        "name": "Jian Zou"
                    },
                    {
                        "name": "Jie Liang"
                    },
                    {
                        "name": "Lei Zhang"
                    },
                    {
                        "name": "Kede Ma"
                    }
                ],
                "author_detail": {
                    "name": "Kede Ma"
                },
                "author": "Kede Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14460v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14460v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14454v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14454v1",
                "updated": "2025-05-20T14:52:31Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    14,
                    52,
                    31,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T14:52:31Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    14,
                    52,
                    31,
                    1,
                    140,
                    0
                ],
                "title": "Video Compression Commander: Plug-and-Play Inference Acceleration for\n  Video Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Compression Commander: Plug-and-Play Inference Acceleration for\n  Video Large Language Models"
                },
                "summary": "Video large language models (VideoLLM) excel at video understanding, but face\nefficiency challenges due to the quadratic complexity of abundant visual\ntokens. Our systematic analysis of token compression methods for VideoLLMs\nreveals two critical issues: (i) overlooking distinctive visual signals across\nframes, leading to information loss; (ii) suffering from implementation\nconstraints, causing incompatibility with modern architectures or efficient\noperators. To address these challenges, we distill three design principles for\nVideoLLM token compression and propose a plug-and-play inference acceleration\nframework \"Video Compression Commander\" (VidCom2). By quantifying each frame's\nuniqueness, VidCom2 adaptively adjusts compression intensity across frames,\neffectively preserving essential information while reducing redundancy in video\nsequences. Extensive experiments across various VideoLLMs and benchmarks\ndemonstrate the superior performance and efficiency of our VidCom2. With only\n25% visual tokens, VidCom2 achieves 99.6% of the original performance on\nLLaVA-OV while reducing 70.8% of the LLM generation latency. Notably, our Frame\nCompression Adjustment strategy is compatible with other token compression\nmethods to further improve their performance. Our code is available at\nhttps://github.com/xuyang-liu16/VidCom2.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video large language models (VideoLLM) excel at video understanding, but face\nefficiency challenges due to the quadratic complexity of abundant visual\ntokens. Our systematic analysis of token compression methods for VideoLLMs\nreveals two critical issues: (i) overlooking distinctive visual signals across\nframes, leading to information loss; (ii) suffering from implementation\nconstraints, causing incompatibility with modern architectures or efficient\noperators. To address these challenges, we distill three design principles for\nVideoLLM token compression and propose a plug-and-play inference acceleration\nframework \"Video Compression Commander\" (VidCom2). By quantifying each frame's\nuniqueness, VidCom2 adaptively adjusts compression intensity across frames,\neffectively preserving essential information while reducing redundancy in video\nsequences. Extensive experiments across various VideoLLMs and benchmarks\ndemonstrate the superior performance and efficiency of our VidCom2. With only\n25% visual tokens, VidCom2 achieves 99.6% of the original performance on\nLLaVA-OV while reducing 70.8% of the LLM generation latency. Notably, our Frame\nCompression Adjustment strategy is compatible with other token compression\nmethods to further improve their performance. Our code is available at\nhttps://github.com/xuyang-liu16/VidCom2."
                },
                "authors": [
                    {
                        "name": "Xuyang Liu"
                    },
                    {
                        "name": "Yiyu Wang"
                    },
                    {
                        "name": "Junpeng Ma"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "Our code is available at https://github.com/xuyang-liu16/VidCom2",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14454v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14454v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07078v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07078v2",
                "updated": "2025-05-20T14:51:24Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    14,
                    51,
                    24,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-11T18:02:21Z",
                "published_parsed": [
                    2025,
                    5,
                    11,
                    18,
                    2,
                    21,
                    6,
                    131,
                    0
                ],
                "title": "Can LLM-based Financial Investing Strategies Outperform the Market in\n  Long Run?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLM-based Financial Investing Strategies Outperform the Market in\n  Long Run?"
                },
                "summary": "Large Language Models (LLMs) have recently been leveraged for asset pricing\ntasks and stock trading applications, enabling AI agents to generate investment\ndecisions from unstructured financial data. However, most evaluations of LLM\ntiming-based investing strategies are conducted on narrow timeframes and\nlimited stock universes, overstating effectiveness due to survivorship and\ndata-snooping biases. We critically assess their generalizability and\nrobustness by proposing FINSABER, a backtesting framework evaluating\ntiming-based strategies across longer periods and a larger universe of symbols.\nSystematic backtests over two decades and 100+ symbols reveal that previously\nreported LLM advantages deteriorate significantly under broader cross-section\nand over a longer-term evaluation. Our market regime analysis further\ndemonstrates that LLM strategies are overly conservative in bull markets,\nunderperforming passive benchmarks, and overly aggressive in bear markets,\nincurring heavy losses. These findings highlight the need to develop LLM\nstrategies that are able to prioritise trend detection and regime-aware risk\ncontrols over mere scaling of framework complexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have recently been leveraged for asset pricing\ntasks and stock trading applications, enabling AI agents to generate investment\ndecisions from unstructured financial data. However, most evaluations of LLM\ntiming-based investing strategies are conducted on narrow timeframes and\nlimited stock universes, overstating effectiveness due to survivorship and\ndata-snooping biases. We critically assess their generalizability and\nrobustness by proposing FINSABER, a backtesting framework evaluating\ntiming-based strategies across longer periods and a larger universe of symbols.\nSystematic backtests over two decades and 100+ symbols reveal that previously\nreported LLM advantages deteriorate significantly under broader cross-section\nand over a longer-term evaluation. Our market regime analysis further\ndemonstrates that LLM strategies are overly conservative in bull markets,\nunderperforming passive benchmarks, and overly aggressive in bear markets,\nincurring heavy losses. These findings highlight the need to develop LLM\nstrategies that are able to prioritise trend detection and regime-aware risk\ncontrols over mere scaling of framework complexity."
                },
                "authors": [
                    {
                        "name": "Weixian Waylon Li"
                    },
                    {
                        "name": "Hyeonjun Kim"
                    },
                    {
                        "name": "Mihai Cucuringu"
                    },
                    {
                        "name": "Tiejun Ma"
                    }
                ],
                "author_detail": {
                    "name": "Tiejun Ma"
                },
                "author": "Tiejun Ma",
                "arxiv_comment": "14 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.07078v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07078v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.TR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.TR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14450v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14450v1",
                "updated": "2025-05-20T14:50:54Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    14,
                    50,
                    54,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T14:50:54Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    14,
                    50,
                    54,
                    1,
                    140,
                    0
                ],
                "title": "Hamiltonian-Driven Architectures for Non-Markovian Quantum Reservoir\n  Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hamiltonian-Driven Architectures for Non-Markovian Quantum Reservoir\n  Computing"
                },
                "summary": "We propose a Hamiltonian-level framework for non-Markovian quantum reservoir\ncomputing directly tailored for analog hardware implementations. By dividing\nthe reservoir into a system block and an environment block and evolving their\njoint state under a unified Hamiltonian, our architecture naturally embeds\nmemory backflow by harnessing entanglement-induced information backflow with\ntunable coupling strengths. Numerical benchmarks on short-term memory tasks\ndemonstrate that operating in non-Markovian regimes yields significantly slower\nmemory decay compared to the Markovian limit. Further analyzing the echo-state\nproperty (ESP), showing that the non-Markovian quantum reservoir evolves from\ntwo different initial states, they do not converge to the same trajectory even\nafter a long time, strongly suggesting that the ESP is effectively violated.\nOur work provides the first demonstration in quantum reservoir computing that\nstrong non-Markovianity can fundamentally violate the ESP, such that\nconventional linear-regression readouts fail to deliver stable training and\ninference. Finally, we experimentally showed that, with an appropriate\ntime-evolution step size, the non-Markovian reservoir exhibits superior\nperformance on higher-order nonlinear autoregressive moving-average(NARMA)\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a Hamiltonian-level framework for non-Markovian quantum reservoir\ncomputing directly tailored for analog hardware implementations. By dividing\nthe reservoir into a system block and an environment block and evolving their\njoint state under a unified Hamiltonian, our architecture naturally embeds\nmemory backflow by harnessing entanglement-induced information backflow with\ntunable coupling strengths. Numerical benchmarks on short-term memory tasks\ndemonstrate that operating in non-Markovian regimes yields significantly slower\nmemory decay compared to the Markovian limit. Further analyzing the echo-state\nproperty (ESP), showing that the non-Markovian quantum reservoir evolves from\ntwo different initial states, they do not converge to the same trajectory even\nafter a long time, strongly suggesting that the ESP is effectively violated.\nOur work provides the first demonstration in quantum reservoir computing that\nstrong non-Markovianity can fundamentally violate the ESP, such that\nconventional linear-regression readouts fail to deliver stable training and\ninference. Finally, we experimentally showed that, with an appropriate\ntime-evolution step size, the non-Markovian reservoir exhibits superior\nperformance on higher-order nonlinear autoregressive moving-average(NARMA)\ntasks."
                },
                "authors": [
                    {
                        "name": "Daiki Sasaki"
                    },
                    {
                        "name": "Ryosuke Koga"
                    },
                    {
                        "name": "Taihei Kuroiwa"
                    },
                    {
                        "name": "Yuya Ito"
                    },
                    {
                        "name": "Chih-Chieh Chen"
                    },
                    {
                        "name": "Tomah Sogabe"
                    }
                ],
                "author_detail": {
                    "name": "Tomah Sogabe"
                },
                "author": "Tomah Sogabe",
                "arxiv_comment": "8 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14450v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14450v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14449v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14449v1",
                "updated": "2025-05-20T14:50:44Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    14,
                    50,
                    44,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T14:50:44Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    14,
                    50,
                    44,
                    1,
                    140,
                    0
                ],
                "title": "Mitigating Subgroup Disparities in Multi-Label Speech Emotion\n  Recognition: A Pseudo-Labeling and Unsupervised Learning Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating Subgroup Disparities in Multi-Label Speech Emotion\n  Recognition: A Pseudo-Labeling and Unsupervised Learning Approach"
                },
                "summary": "While subgroup disparities and performance bias are increasingly studied in\ncomputational research, fairness in categorical Speech Emotion Recognition\n(SER) remains underexplored. Existing methods often rely on explicit\ndemographic labels, which are difficult to obtain due to privacy concerns. To\naddress this limitation, we introduce an Implicit Demography Inference (IDI)\nmodule that leverages pseudo-labeling from a pre-trained model and unsupervised\nlearning using k-means clustering to mitigate bias in SER. Our experiments show\nthat pseudo-labeling IDI reduces subgroup disparities, improving fairness\nmetrics by over 33% with less than a 3% decrease in SER accuracy. Also, the\nunsupervised IDI yields more than a 26% improvement in fairness metrics with a\ndrop of less than 4% in SER performance. Further analyses reveal that the\nunsupervised IDI consistently mitigates race and age disparities, demonstrating\nits potential in scenarios where explicit demographic information is\nunavailable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While subgroup disparities and performance bias are increasingly studied in\ncomputational research, fairness in categorical Speech Emotion Recognition\n(SER) remains underexplored. Existing methods often rely on explicit\ndemographic labels, which are difficult to obtain due to privacy concerns. To\naddress this limitation, we introduce an Implicit Demography Inference (IDI)\nmodule that leverages pseudo-labeling from a pre-trained model and unsupervised\nlearning using k-means clustering to mitigate bias in SER. Our experiments show\nthat pseudo-labeling IDI reduces subgroup disparities, improving fairness\nmetrics by over 33% with less than a 3% decrease in SER accuracy. Also, the\nunsupervised IDI yields more than a 26% improvement in fairness metrics with a\ndrop of less than 4% in SER performance. Further analyses reveal that the\nunsupervised IDI consistently mitigates race and age disparities, demonstrating\nits potential in scenarios where explicit demographic information is\nunavailable."
                },
                "authors": [
                    {
                        "name": "Yi-Cheng Lin"
                    },
                    {
                        "name": "Huang-Cheng Chou"
                    },
                    {
                        "name": "Hung-yi Lee"
                    }
                ],
                "author_detail": {
                    "name": "Hung-yi Lee"
                },
                "author": "Hung-yi Lee",
                "arxiv_comment": "Accepted by InterSpeech 2025. 7 pages including 2 pages of appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14449v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14449v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2303.12667v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2303.12667v4",
                "updated": "2025-05-20T14:49:29Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    14,
                    49,
                    29,
                    1,
                    140,
                    0
                ],
                "published": "2023-03-22T15:44:59Z",
                "published_parsed": [
                    2023,
                    3,
                    22,
                    15,
                    44,
                    59,
                    2,
                    81,
                    0
                ],
                "title": "Don't (fully) exclude me, it's not necessary! Causal inference with\n  semi-IVs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Don't (fully) exclude me, it's not necessary! Causal inference with\n  semi-IVs"
                },
                "summary": "This paper proposes semi-instrumental variables (semi-IVs) as an alternative\nto instrumental variables (IVs) to identify the causal effect of a binary (or\ndiscrete) endogenous treatment. A semi-IV is a less restrictive form of\ninstrument: it affects the selection into treatment but is excluded only from\none, not necessarily both, potential outcomes. Having two semi-IVs, one\nexcluded from the potential outcome under treatment and the other from the\npotential outcome under control, is sufficient to nonparametrically point\nidentify local average treatment effect (LATE) and marginal treatment effect\n(MTE) parameters. In practice, semi-IVs provide a solution to the challenge of\nfinding valid IVs because they are easier to find: many selection-specific\nshocks, policies, costs, or benefits are valid semi-IVs. As an application, I\nestimate the returns to working in the manufacturing sector using\nsector-specific semi-IVs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes semi-instrumental variables (semi-IVs) as an alternative\nto instrumental variables (IVs) to identify the causal effect of a binary (or\ndiscrete) endogenous treatment. A semi-IV is a less restrictive form of\ninstrument: it affects the selection into treatment but is excluded only from\none, not necessarily both, potential outcomes. Having two semi-IVs, one\nexcluded from the potential outcome under treatment and the other from the\npotential outcome under control, is sufficient to nonparametrically point\nidentify local average treatment effect (LATE) and marginal treatment effect\n(MTE) parameters. In practice, semi-IVs provide a solution to the challenge of\nfinding valid IVs because they are easier to find: many selection-specific\nshocks, policies, costs, or benefits are valid semi-IVs. As an application, I\nestimate the returns to working in the manufacturing sector using\nsector-specific semi-IVs."
                },
                "authors": [
                    {
                        "name": "Christophe Bruneel-Zupanc"
                    }
                ],
                "author_detail": {
                    "name": "Christophe Bruneel-Zupanc"
                },
                "author": "Christophe Bruneel-Zupanc",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2303.12667v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2303.12667v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11936v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11936v3",
                "updated": "2025-05-20T14:45:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    14,
                    45,
                    21,
                    1,
                    140,
                    0
                ],
                "published": "2024-12-16T16:21:41Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    16,
                    21,
                    41,
                    0,
                    351,
                    0
                ],
                "title": "A Survey of Mathematical Reasoning in the Era of Multimodal Large\n  Language Model: Benchmark, Method & Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of Mathematical Reasoning in the Era of Multimodal Large\n  Language Model: Benchmark, Method & Challenges"
                },
                "summary": "Mathematical reasoning, a core aspect of human cognition, is vital across\nmany domains, from educational problem-solving to scientific advancements. As\nartificial general intelligence (AGI) progresses, integrating large language\nmodels (LLMs) with mathematical reasoning tasks is becoming increasingly\nsignificant. This survey provides the first comprehensive analysis of\nmathematical reasoning in the era of multimodal large language models (MLLMs).\nWe review over 200 studies published since 2021, and examine the\nstate-of-the-art developments in Math-LLMs, with a focus on multimodal\nsettings. We categorize the field into three dimensions: benchmarks,\nmethodologies, and challenges. In particular, we explore multimodal\nmathematical reasoning pipeline, as well as the role of (M)LLMs and the\nassociated methodologies. Finally, we identify five major challenges hindering\nthe realization of AGI in this domain, offering insights into the future\ndirection for enhancing multimodal reasoning capabilities. This survey serves\nas a critical resource for the research community in advancing the capabilities\nof LLMs to tackle complex multimodal reasoning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mathematical reasoning, a core aspect of human cognition, is vital across\nmany domains, from educational problem-solving to scientific advancements. As\nartificial general intelligence (AGI) progresses, integrating large language\nmodels (LLMs) with mathematical reasoning tasks is becoming increasingly\nsignificant. This survey provides the first comprehensive analysis of\nmathematical reasoning in the era of multimodal large language models (MLLMs).\nWe review over 200 studies published since 2021, and examine the\nstate-of-the-art developments in Math-LLMs, with a focus on multimodal\nsettings. We categorize the field into three dimensions: benchmarks,\nmethodologies, and challenges. In particular, we explore multimodal\nmathematical reasoning pipeline, as well as the role of (M)LLMs and the\nassociated methodologies. Finally, we identify five major challenges hindering\nthe realization of AGI in this domain, offering insights into the future\ndirection for enhancing multimodal reasoning capabilities. This survey serves\nas a critical resource for the research community in advancing the capabilities\nof LLMs to tackle complex multimodal reasoning tasks."
                },
                "authors": [
                    {
                        "name": "Yibo Yan"
                    },
                    {
                        "name": "Jiamin Su"
                    },
                    {
                        "name": "Jianxiang He"
                    },
                    {
                        "name": "Fangteng Fu"
                    },
                    {
                        "name": "Xu Zheng"
                    },
                    {
                        "name": "Yuanhuiyi Lyu"
                    },
                    {
                        "name": "Kun Wang"
                    },
                    {
                        "name": "Shen Wang"
                    },
                    {
                        "name": "Qingsong Wen"
                    },
                    {
                        "name": "Xuming Hu"
                    }
                ],
                "author_detail": {
                    "name": "Xuming Hu"
                },
                "author": "Xuming Hu",
                "arxiv_comment": "Accepted by The 63rd Annual Meeting of the Association for\n  Computational Linguistics (ACL Findings 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11936v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11936v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14442v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14442v1",
                "updated": "2025-05-20T14:43:41Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    14,
                    43,
                    41,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T14:43:41Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    14,
                    43,
                    41,
                    1,
                    140,
                    0
                ],
                "title": "Creative Preference Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Creative Preference Optimization"
                },
                "summary": "While Large Language Models (LLMs) have demonstrated impressive performance\nacross natural language generation tasks, their ability to generate truly\ncreative content-characterized by novelty, diversity, surprise, and\nquality-remains limited. Existing methods for enhancing LLM creativity often\nfocus narrowly on diversity or specific tasks, failing to address creativity's\nmultifaceted nature in a generalizable way. In this work, we propose Creative\nPreference Optimization (CrPO), a novel alignment method that injects signals\nfrom multiple creativity dimensions into the preference optimization objective\nin a modular fashion. We train and evaluate creativity-augmented versions of\nseveral models using CrPO and MuCE, a new large-scale human preference dataset\nspanning over 200,000 human-generated responses and ratings from more than 30\npsychological creativity assessments. Our models outperform strong baselines,\nincluding GPT-4o, on both automated and human evaluations, producing more\nnovel, diverse, and surprising generations while maintaining high output\nquality. Additional evaluations on NoveltyBench further confirm the\ngeneralizability of our approach. Together, our results demonstrate that\ndirectly optimizing for creativity within preference frameworks is a promising\ndirection for advancing the creative capabilities of LLMs without compromising\noutput quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Large Language Models (LLMs) have demonstrated impressive performance\nacross natural language generation tasks, their ability to generate truly\ncreative content-characterized by novelty, diversity, surprise, and\nquality-remains limited. Existing methods for enhancing LLM creativity often\nfocus narrowly on diversity or specific tasks, failing to address creativity's\nmultifaceted nature in a generalizable way. In this work, we propose Creative\nPreference Optimization (CrPO), a novel alignment method that injects signals\nfrom multiple creativity dimensions into the preference optimization objective\nin a modular fashion. We train and evaluate creativity-augmented versions of\nseveral models using CrPO and MuCE, a new large-scale human preference dataset\nspanning over 200,000 human-generated responses and ratings from more than 30\npsychological creativity assessments. Our models outperform strong baselines,\nincluding GPT-4o, on both automated and human evaluations, producing more\nnovel, diverse, and surprising generations while maintaining high output\nquality. Additional evaluations on NoveltyBench further confirm the\ngeneralizability of our approach. Together, our results demonstrate that\ndirectly optimizing for creativity within preference frameworks is a promising\ndirection for advancing the creative capabilities of LLMs without compromising\noutput quality."
                },
                "authors": [
                    {
                        "name": "Mete Ismayilzada"
                    },
                    {
                        "name": "Antonio Laverghetta Jr."
                    },
                    {
                        "name": "Simone A. Luchini"
                    },
                    {
                        "name": "Reet Patel"
                    },
                    {
                        "name": "Antoine Bosselut"
                    },
                    {
                        "name": "Lonneke van der Plas"
                    },
                    {
                        "name": "Roger Beaty"
                    }
                ],
                "author_detail": {
                    "name": "Roger Beaty"
                },
                "author": "Roger Beaty",
                "arxiv_comment": "27 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14442v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14442v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14438v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14438v1",
                "updated": "2025-05-20T14:42:20Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    14,
                    42,
                    20,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T14:42:20Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    14,
                    42,
                    20,
                    1,
                    140,
                    0
                ],
                "title": "S2SBench: A Benchmark for Quantifying Intelligence Degradation in\n  Speech-to-Speech Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "S2SBench: A Benchmark for Quantifying Intelligence Degradation in\n  Speech-to-Speech Large Language Models"
                },
                "summary": "End-to-end speech large language models ((LLMs)) extend the capabilities of\ntext-based models to directly process and generate audio tokens. However, this\noften leads to a decline in reasoning and generation performance compared to\ntext input, a phenomenon referred to as intelligence degradation. To\nsystematically evaluate this gap, we propose S2SBench, a benchmark designed to\nquantify performance degradation in Speech LLMs. It includes diagnostic\ndatasets targeting sentence continuation and commonsense reasoning under audio\ninput. We further introduce a pairwise evaluation protocol based on perplexity\ndifferences between plausible and implausible samples to measure degradation\nrelative to text input. We apply S2SBench to analyze the training process of\nBaichuan-Audio, which further demonstrates the benchmark's effectiveness. All\ndatasets and evaluation code are available at\nhttps://github.com/undobug/S2SBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "End-to-end speech large language models ((LLMs)) extend the capabilities of\ntext-based models to directly process and generate audio tokens. However, this\noften leads to a decline in reasoning and generation performance compared to\ntext input, a phenomenon referred to as intelligence degradation. To\nsystematically evaluate this gap, we propose S2SBench, a benchmark designed to\nquantify performance degradation in Speech LLMs. It includes diagnostic\ndatasets targeting sentence continuation and commonsense reasoning under audio\ninput. We further introduce a pairwise evaluation protocol based on perplexity\ndifferences between plausible and implausible samples to measure degradation\nrelative to text input. We apply S2SBench to analyze the training process of\nBaichuan-Audio, which further demonstrates the benchmark's effectiveness. All\ndatasets and evaluation code are available at\nhttps://github.com/undobug/S2SBench."
                },
                "authors": [
                    {
                        "name": "Yuanbo Fang"
                    },
                    {
                        "name": "Haoze Sun"
                    },
                    {
                        "name": "Jun Liu"
                    },
                    {
                        "name": "Tao Zhang"
                    },
                    {
                        "name": "Zenan Zhou"
                    },
                    {
                        "name": "Weipeng Chen"
                    },
                    {
                        "name": "Xiaofen Xing"
                    },
                    {
                        "name": "Xiangmin Xu"
                    }
                ],
                "author_detail": {
                    "name": "Xiangmin Xu"
                },
                "author": "Xiangmin Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14438v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14438v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14436v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14436v1",
                "updated": "2025-05-20T14:42:03Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    14,
                    42,
                    3,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T14:42:03Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    14,
                    42,
                    3,
                    1,
                    140,
                    0
                ],
                "title": "Neural Incompatibility: The Unbridgeable Gap of Cross-Scale Parametric\n  Knowledge Transfer in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Incompatibility: The Unbridgeable Gap of Cross-Scale Parametric\n  Knowledge Transfer in Large Language Models"
                },
                "summary": "Large Language Models (LLMs) offer a transparent brain with accessible\nparameters that encode extensive knowledge, which can be analyzed, located and\ntransferred. Consequently, a key research challenge is to transcend traditional\nknowledge transfer paradigms rooted in symbolic language and achieve genuine\nParametric Knowledge Transfer (PKT). Significantly, exploring effective methods\nfor transferring knowledge across LLMs of different scales through parameters\npresents an intriguing and valuable research direction. In this paper, we first\ndemonstrate $\\textbf{Alignment}$ in parametric space is the fundamental\nprerequisite to achieve successful cross-scale PKT. We redefine the previously\nexplored knowledge transfer as Post-Align PKT (PostPKT), which utilizes\nextracted parameters for LoRA initialization and requires subsequent fine-tune\nfor alignment. Hence, to reduce cost for further fine-tuning, we introduce a\nnovel Pre-Align PKT (PrePKT) paradigm and propose a solution called\n$\\textbf{LaTen}$\n($\\textbf{L}$oc$\\textbf{a}$te-$\\textbf{T}$h$\\textbf{e}$n-Alig$\\textbf{n}$) that\naligns the parametric spaces of LLMs across scales only using several training\nsteps without following training. Comprehensive experiments on four benchmarks\ndemonstrate that both PostPKT and PrePKT face challenges in achieving\nconsistently stable transfer. Through in-depth analysis, we identify\n$\\textbf{Neural Incompatibility}$ as the ethological and parametric structural\ndifferences between LLMs of varying scales, presenting fundamental challenges\nto achieving effective PKT. These findings provide fresh insights into the\nparametric architectures of LLMs and highlight promising directions for future\nresearch on efficient PKT. Our code is available at\nhttps://github.com/Trae1ounG/Neural_Incompatibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) offer a transparent brain with accessible\nparameters that encode extensive knowledge, which can be analyzed, located and\ntransferred. Consequently, a key research challenge is to transcend traditional\nknowledge transfer paradigms rooted in symbolic language and achieve genuine\nParametric Knowledge Transfer (PKT). Significantly, exploring effective methods\nfor transferring knowledge across LLMs of different scales through parameters\npresents an intriguing and valuable research direction. In this paper, we first\ndemonstrate $\\textbf{Alignment}$ in parametric space is the fundamental\nprerequisite to achieve successful cross-scale PKT. We redefine the previously\nexplored knowledge transfer as Post-Align PKT (PostPKT), which utilizes\nextracted parameters for LoRA initialization and requires subsequent fine-tune\nfor alignment. Hence, to reduce cost for further fine-tuning, we introduce a\nnovel Pre-Align PKT (PrePKT) paradigm and propose a solution called\n$\\textbf{LaTen}$\n($\\textbf{L}$oc$\\textbf{a}$te-$\\textbf{T}$h$\\textbf{e}$n-Alig$\\textbf{n}$) that\naligns the parametric spaces of LLMs across scales only using several training\nsteps without following training. Comprehensive experiments on four benchmarks\ndemonstrate that both PostPKT and PrePKT face challenges in achieving\nconsistently stable transfer. Through in-depth analysis, we identify\n$\\textbf{Neural Incompatibility}$ as the ethological and parametric structural\ndifferences between LLMs of varying scales, presenting fundamental challenges\nto achieving effective PKT. These findings provide fresh insights into the\nparametric architectures of LLMs and highlight promising directions for future\nresearch on efficient PKT. Our code is available at\nhttps://github.com/Trae1ounG/Neural_Incompatibility."
                },
                "authors": [
                    {
                        "name": "Yuqiao Tan"
                    },
                    {
                        "name": "Shizhu He"
                    },
                    {
                        "name": "Kang Liu"
                    },
                    {
                        "name": "Jun Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhao"
                },
                "author": "Jun Zhao",
                "arxiv_comment": "Accepted by ACL'25 Main. Code link:\n  https://github.com/Trae1ounG/Neural_Incompatibility",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14436v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14436v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14435v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14435v1",
                "updated": "2025-05-20T14:41:56Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    14,
                    41,
                    56,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T14:41:56Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    14,
                    41,
                    56,
                    1,
                    140,
                    0
                ],
                "title": "Choosing a Model, Shaping a Future: Comparing LLM Perspectives on\n  Sustainability and its Relationship with AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Choosing a Model, Shaping a Future: Comparing LLM Perspectives on\n  Sustainability and its Relationship with AI"
                },
                "summary": "As organizations increasingly rely on AI systems for decision support in\nsustainability contexts, it becomes critical to understand the inherent biases\nand perspectives embedded in Large Language Models (LLMs). This study\nsystematically investigates how five state-of-the-art LLMs -- Claude, DeepSeek,\nGPT, LLaMA, and Mistral - conceptualize sustainability and its relationship\nwith AI. We administered validated, psychometric sustainability-related\nquestionnaires - each 100 times per model -- to capture response patterns and\nvariability. Our findings revealed significant inter-model differences: For\nexample, GPT exhibited skepticism about the compatibility of AI and\nsustainability, whereas LLaMA demonstrated extreme techno-optimism with perfect\nscores for several Sustainable Development Goals (SDGs). Models also diverged\nin attributing institutional responsibility for AI and sustainability\nintegration, a results that holds implications for technology governance\napproaches. Our results demonstrate that model selection could substantially\ninfluence organizational sustainability strategies, highlighting the need for\nawareness of model-specific biases when deploying LLMs for\nsustainability-related decision-making.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As organizations increasingly rely on AI systems for decision support in\nsustainability contexts, it becomes critical to understand the inherent biases\nand perspectives embedded in Large Language Models (LLMs). This study\nsystematically investigates how five state-of-the-art LLMs -- Claude, DeepSeek,\nGPT, LLaMA, and Mistral - conceptualize sustainability and its relationship\nwith AI. We administered validated, psychometric sustainability-related\nquestionnaires - each 100 times per model -- to capture response patterns and\nvariability. Our findings revealed significant inter-model differences: For\nexample, GPT exhibited skepticism about the compatibility of AI and\nsustainability, whereas LLaMA demonstrated extreme techno-optimism with perfect\nscores for several Sustainable Development Goals (SDGs). Models also diverged\nin attributing institutional responsibility for AI and sustainability\nintegration, a results that holds implications for technology governance\napproaches. Our results demonstrate that model selection could substantially\ninfluence organizational sustainability strategies, highlighting the need for\nawareness of model-specific biases when deploying LLMs for\nsustainability-related decision-making."
                },
                "authors": [
                    {
                        "name": "Annika Bush"
                    },
                    {
                        "name": "Meltem Aksoy"
                    },
                    {
                        "name": "Markus Pauly"
                    },
                    {
                        "name": "Greta Ontrup"
                    }
                ],
                "author_detail": {
                    "name": "Greta Ontrup"
                },
                "author": "Greta Ontrup",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14435v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14435v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2505.14687v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14687v1",
                "updated": "2025-05-20T17:59:59Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    17,
                    59,
                    59,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T17:59:59Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    17,
                    59,
                    59,
                    1,
                    140,
                    0
                ],
                "title": "Grouping First, Attending Smartly: Training-Free Acceleration for\n  Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Grouping First, Attending Smartly: Training-Free Acceleration for\n  Diffusion Transformers"
                },
                "summary": "Diffusion-based Transformers have demonstrated impressive generative\ncapabilities, but their high computational costs hinder practical deployment,\nfor example, generating an $8192\\times 8192$ image can take over an hour on an\nA100 GPU. In this work, we propose GRAT (\\textbf{GR}ouping first,\n\\textbf{AT}tending smartly), a training-free attention acceleration strategy\nfor fast image and video generation without compromising output quality. The\nkey insight is to exploit the inherent sparsity in learned attention maps\n(which tend to be locally focused) in pretrained Diffusion Transformers and\nleverage better GPU parallelism. Specifically, GRAT first partitions contiguous\ntokens into non-overlapping groups, aligning both with GPU execution patterns\nand the local attention structures learned in pretrained generative\nTransformers. It then accelerates attention by having all query tokens within\nthe same group share a common set of attendable key and value tokens. These key\nand value tokens are further restricted to structured regions, such as\nsurrounding blocks or criss-cross regions, significantly reducing computational\noverhead (e.g., attaining a \\textbf{35.8$\\times$} speedup over full attention\nwhen generating $8192\\times 8192$ images) while preserving essential attention\npatterns and long-range context. We validate GRAT on pretrained Flux and\nHunyuanVideo for image and video generation, respectively. In both cases, GRAT\nachieves substantially faster inference without any fine-tuning, while\nmaintaining the performance of full attention. We hope GRAT will inspire future\nresearch on accelerating Diffusion Transformers for scalable visual generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based Transformers have demonstrated impressive generative\ncapabilities, but their high computational costs hinder practical deployment,\nfor example, generating an $8192\\times 8192$ image can take over an hour on an\nA100 GPU. In this work, we propose GRAT (\\textbf{GR}ouping first,\n\\textbf{AT}tending smartly), a training-free attention acceleration strategy\nfor fast image and video generation without compromising output quality. The\nkey insight is to exploit the inherent sparsity in learned attention maps\n(which tend to be locally focused) in pretrained Diffusion Transformers and\nleverage better GPU parallelism. Specifically, GRAT first partitions contiguous\ntokens into non-overlapping groups, aligning both with GPU execution patterns\nand the local attention structures learned in pretrained generative\nTransformers. It then accelerates attention by having all query tokens within\nthe same group share a common set of attendable key and value tokens. These key\nand value tokens are further restricted to structured regions, such as\nsurrounding blocks or criss-cross regions, significantly reducing computational\noverhead (e.g., attaining a \\textbf{35.8$\\times$} speedup over full attention\nwhen generating $8192\\times 8192$ images) while preserving essential attention\npatterns and long-range context. We validate GRAT on pretrained Flux and\nHunyuanVideo for image and video generation, respectively. In both cases, GRAT\nachieves substantially faster inference without any fine-tuning, while\nmaintaining the performance of full attention. We hope GRAT will inspire future\nresearch on accelerating Diffusion Transformers for scalable visual generation."
                },
                "authors": [
                    {
                        "name": "Sucheng Ren"
                    },
                    {
                        "name": "Qihang Yu"
                    },
                    {
                        "name": "Ju He"
                    },
                    {
                        "name": "Alan Yuille"
                    },
                    {
                        "name": "Liang-Chieh Chen"
                    }
                ],
                "author_detail": {
                    "name": "Liang-Chieh Chen"
                },
                "author": "Liang-Chieh Chen",
                "arxiv_comment": "Project website at oliverrensu.github.io/project/GRAT",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14687v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14687v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14684v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14684v1",
                "updated": "2025-05-20T17:59:31Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    17,
                    59,
                    31,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T17:59:31Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    17,
                    59,
                    31,
                    1,
                    140,
                    0
                ],
                "title": "Mind the Gap: Bridging Thought Leap for Improved Chain-of-Thought Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mind the Gap: Bridging Thought Leap for Improved Chain-of-Thought Tuning"
                },
                "summary": "Large language models (LLMs) have achieved remarkable progress on\nmathemati-cal tasks through Chain-of-Thought (CoT) reasoning. However, existing\nmathematical CoT datasets often suffer from Thought Leaps due to experts\nomitting intermediate steps, which negatively impacts model learning and\ngeneralization. We propose the CoT Thought Leap Bridge Task, which aims to\nautomatically detect leaps and generate missing intermediate reasoning steps to\nrestore the completeness and coherence of CoT. To facilitate this, we\nconstructed a specialized training dataset called ScaleQM+, based on the\nstructured ScaleQuestMath dataset, and trained CoT-Bridge to bridge thought\nleaps. Through comprehensive experiments on mathematical reasoning benchmarks,\nwe demonstrate that models fine-tuned on bridged datasets consistently\noutperform those trained on original datasets, with improvements of up to\n+5.87% on NuminaMath. Our approach effectively enhances distilled data (+3.02%)\nand provides better starting points for reinforcement learning (+3.1%),\nfunctioning as a plug-and-play module compatible with existing optimization\ntechniques. Furthermore, CoT-Bridge demonstrate improved generalization to\nout-of-domain logical reasoning tasks, confirming that enhancing reasoning\ncompleteness yields broadly applicable benefits.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved remarkable progress on\nmathemati-cal tasks through Chain-of-Thought (CoT) reasoning. However, existing\nmathematical CoT datasets often suffer from Thought Leaps due to experts\nomitting intermediate steps, which negatively impacts model learning and\ngeneralization. We propose the CoT Thought Leap Bridge Task, which aims to\nautomatically detect leaps and generate missing intermediate reasoning steps to\nrestore the completeness and coherence of CoT. To facilitate this, we\nconstructed a specialized training dataset called ScaleQM+, based on the\nstructured ScaleQuestMath dataset, and trained CoT-Bridge to bridge thought\nleaps. Through comprehensive experiments on mathematical reasoning benchmarks,\nwe demonstrate that models fine-tuned on bridged datasets consistently\noutperform those trained on original datasets, with improvements of up to\n+5.87% on NuminaMath. Our approach effectively enhances distilled data (+3.02%)\nand provides better starting points for reinforcement learning (+3.1%),\nfunctioning as a plug-and-play module compatible with existing optimization\ntechniques. Furthermore, CoT-Bridge demonstrate improved generalization to\nout-of-domain logical reasoning tasks, confirming that enhancing reasoning\ncompleteness yields broadly applicable benefits."
                },
                "authors": [
                    {
                        "name": "Haolei Xu"
                    },
                    {
                        "name": "Yuchen Yan"
                    },
                    {
                        "name": "Yongliang Shen"
                    },
                    {
                        "name": "Wenqi Zhang"
                    },
                    {
                        "name": "Guiyang Hou"
                    },
                    {
                        "name": "Shengpei Jiang"
                    },
                    {
                        "name": "Kaitao Song"
                    },
                    {
                        "name": "Weiming Lu"
                    },
                    {
                        "name": "Jun Xiao"
                    },
                    {
                        "name": "Yueting Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Yueting Zhuang"
                },
                "author": "Yueting Zhuang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14684v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14684v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14679v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14679v1",
                "updated": "2025-05-20T17:59:04Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    17,
                    59,
                    4,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T17:59:04Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    17,
                    59,
                    4,
                    1,
                    140,
                    0
                ],
                "title": "UltraEdit: Training-, Subject-, and Memory-Free Lifelong Editing in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UltraEdit: Training-, Subject-, and Memory-Free Lifelong Editing in\n  Large Language Models"
                },
                "summary": "Lifelong learning enables large language models (LLMs) to adapt to evolving\ninformation by continually updating their internal knowledge. An ideal system\nshould support efficient, wide-ranging updates while preserving existing\ncapabilities and ensuring reliable deployment. Model editing stands out as a\npromising solution for this goal, offering a focused and efficient way to\nrevise a model's internal knowledge. Although recent paradigms have made\nnotable progress, they often struggle to meet the demands of practical lifelong\nadaptation at scale. To bridge this gap, we propose ULTRAEDIT-a fundamentally\nnew editing solution that is training-, subject- and memory-free, making it\nparticularly well-suited for ultra-scalable, real-world lifelong model editing.\nULTRAEDIT performs editing through a self-contained process that relies solely\non lightweight linear algebra operations to compute parameter shifts, enabling\nfast and consistent parameter modifications with minimal overhead. To improve\nscalability in lifelong settings, ULTRAEDIT employs a lifelong normalization\nstrategy that continuously updates feature statistics across turns, allowing it\nto adapt to distributional shifts and maintain consistency over time. ULTRAEDIT\nachieves editing speeds over 7x faster than the previous state-of-the-art\nmethod-which was also the fastest known approach-while consuming less than 1/3\nthe VRAM, making it the only method currently capable of editing a 7B LLM on a\n24GB consumer-grade GPU. Furthermore, we construct ULTRAEDITBENCH-the largest\ndataset in the field to date, with over 2M editing pairs-and demonstrate that\nour method supports up to 1M edits while maintaining high accuracy.\nComprehensive experiments on four datasets and six models show that ULTRAEDIT\nconsistently achieves superior performance across diverse model editing\nscenarios. Our code is available at: https://github.com/XiaojieGu/UltraEdit.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lifelong learning enables large language models (LLMs) to adapt to evolving\ninformation by continually updating their internal knowledge. An ideal system\nshould support efficient, wide-ranging updates while preserving existing\ncapabilities and ensuring reliable deployment. Model editing stands out as a\npromising solution for this goal, offering a focused and efficient way to\nrevise a model's internal knowledge. Although recent paradigms have made\nnotable progress, they often struggle to meet the demands of practical lifelong\nadaptation at scale. To bridge this gap, we propose ULTRAEDIT-a fundamentally\nnew editing solution that is training-, subject- and memory-free, making it\nparticularly well-suited for ultra-scalable, real-world lifelong model editing.\nULTRAEDIT performs editing through a self-contained process that relies solely\non lightweight linear algebra operations to compute parameter shifts, enabling\nfast and consistent parameter modifications with minimal overhead. To improve\nscalability in lifelong settings, ULTRAEDIT employs a lifelong normalization\nstrategy that continuously updates feature statistics across turns, allowing it\nto adapt to distributional shifts and maintain consistency over time. ULTRAEDIT\nachieves editing speeds over 7x faster than the previous state-of-the-art\nmethod-which was also the fastest known approach-while consuming less than 1/3\nthe VRAM, making it the only method currently capable of editing a 7B LLM on a\n24GB consumer-grade GPU. Furthermore, we construct ULTRAEDITBENCH-the largest\ndataset in the field to date, with over 2M editing pairs-and demonstrate that\nour method supports up to 1M edits while maintaining high accuracy.\nComprehensive experiments on four datasets and six models show that ULTRAEDIT\nconsistently achieves superior performance across diverse model editing\nscenarios. Our code is available at: https://github.com/XiaojieGu/UltraEdit."
                },
                "authors": [
                    {
                        "name": "Xiaojie Gu"
                    },
                    {
                        "name": "Guangxu Chen"
                    },
                    {
                        "name": "Jungang Li"
                    },
                    {
                        "name": "Jia-Chen Gu"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Kai Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Kai Zhang"
                },
                "author": "Kai Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14679v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14679v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14677v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14677v1",
                "updated": "2025-05-20T17:58:35Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    17,
                    58,
                    35,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T17:58:35Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    17,
                    58,
                    35,
                    1,
                    140,
                    0
                ],
                "title": "Visionary-R1: Mitigating Shortcuts in Visual Reasoning with\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visionary-R1: Mitigating Shortcuts in Visual Reasoning with\n  Reinforcement Learning"
                },
                "summary": "Learning general-purpose reasoning capabilities has long been a challenging\nproblem in AI. Recent research in large language models (LLMs), such as\nDeepSeek-R1, has shown that reinforcement learning techniques like GRPO can\nenable pre-trained LLMs to develop reasoning capabilities using simple\nquestion-answer pairs. In this paper, we aim to train visual language models\n(VLMs) to perform reasoning on image data through reinforcement learning and\nvisual question-answer pairs, without any explicit chain-of-thought (CoT)\nsupervision. Our findings indicate that simply applying reinforcement learning\nto a VLM -- by prompting the model to produce a reasoning chain before\nproviding an answer -- can lead the model to develop shortcuts from easy\nquestions, thereby reducing its ability to generalize across unseen data\ndistributions. We argue that the key to mitigating shortcut learning is to\nencourage the model to interpret images prior to reasoning. Therefore, we train\nthe model to adhere to a caption-reason-answer output format: initially\ngenerating a detailed caption for an image, followed by constructing an\nextensive reasoning chain. When trained on 273K CoT-free visual question-answer\npairs and using only reinforcement learning, our model, named Visionary-R1,\noutperforms strong multimodal models, such as GPT-4o, Claude3.5-Sonnet, and\nGemini-1.5-Pro, on multiple visual reasoning benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning general-purpose reasoning capabilities has long been a challenging\nproblem in AI. Recent research in large language models (LLMs), such as\nDeepSeek-R1, has shown that reinforcement learning techniques like GRPO can\nenable pre-trained LLMs to develop reasoning capabilities using simple\nquestion-answer pairs. In this paper, we aim to train visual language models\n(VLMs) to perform reasoning on image data through reinforcement learning and\nvisual question-answer pairs, without any explicit chain-of-thought (CoT)\nsupervision. Our findings indicate that simply applying reinforcement learning\nto a VLM -- by prompting the model to produce a reasoning chain before\nproviding an answer -- can lead the model to develop shortcuts from easy\nquestions, thereby reducing its ability to generalize across unseen data\ndistributions. We argue that the key to mitigating shortcut learning is to\nencourage the model to interpret images prior to reasoning. Therefore, we train\nthe model to adhere to a caption-reason-answer output format: initially\ngenerating a detailed caption for an image, followed by constructing an\nextensive reasoning chain. When trained on 273K CoT-free visual question-answer\npairs and using only reinforcement learning, our model, named Visionary-R1,\noutperforms strong multimodal models, such as GPT-4o, Claude3.5-Sonnet, and\nGemini-1.5-Pro, on multiple visual reasoning benchmarks."
                },
                "authors": [
                    {
                        "name": "Jiaer Xia"
                    },
                    {
                        "name": "Yuhang Zang"
                    },
                    {
                        "name": "Peng Gao"
                    },
                    {
                        "name": "Yixuan Li"
                    },
                    {
                        "name": "Kaiyang Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Kaiyang Zhou"
                },
                "author": "Kaiyang Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14677v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14677v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14669v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14669v1",
                "updated": "2025-05-20T17:55:50Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    17,
                    55,
                    50,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T17:55:50Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    17,
                    55,
                    50,
                    1,
                    140,
                    0
                ],
                "title": "Quartet: Native FP4 Training Can Be Optimal for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quartet: Native FP4 Training Can Be Optimal for Large Language Models"
                },
                "summary": "The rapid advancement of large language models (LLMs) has been paralleled by\nunprecedented increases in computational demands, with training costs for\nstate-of-the-art models doubling every few months. Training models directly in\nlow-precision arithmetic offers a solution, by improving both computational\nthroughput and energy efficiency. Specifically, NVIDIA's recent Blackwell\narchitecture facilitates extremely low-precision operations, specifically FP4\nvariants, promising substantial efficiency gains. Yet, current algorithms for\ntraining LLMs in FP4 precision face significant accuracy degradation and often\nrely on mixed-precision fallbacks. In this paper, we systematically investigate\nhardware-supported FP4 training and introduce Quartet, a new approach enabling\naccurate, end-to-end FP4 training with all the major computations (in e.g.\nlinear layers) being performed in low precision. Through extensive evaluations\non Llama-type models, we reveal a new low-precision scaling law that quantifies\nperformance trade-offs across varying bit-widths and allows us to identify a\n\"near-optimal\" low-precision training technique in terms of\naccuracy-vs-computation, called Quartet. We implement Quartet using optimized\nCUDA kernels tailored for NVIDIA Blackwell GPUs, and show that it can achieve\nstate-of-the-art accuracy for FP4 precision, successfully training\nbillion-scale models. Our method demonstrates that fully FP4-based training is\na competitive alternative to standard-precision and FP8 training. Our code is\navailable at https://github.com/IST-DASLab/Quartet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of large language models (LLMs) has been paralleled by\nunprecedented increases in computational demands, with training costs for\nstate-of-the-art models doubling every few months. Training models directly in\nlow-precision arithmetic offers a solution, by improving both computational\nthroughput and energy efficiency. Specifically, NVIDIA's recent Blackwell\narchitecture facilitates extremely low-precision operations, specifically FP4\nvariants, promising substantial efficiency gains. Yet, current algorithms for\ntraining LLMs in FP4 precision face significant accuracy degradation and often\nrely on mixed-precision fallbacks. In this paper, we systematically investigate\nhardware-supported FP4 training and introduce Quartet, a new approach enabling\naccurate, end-to-end FP4 training with all the major computations (in e.g.\nlinear layers) being performed in low precision. Through extensive evaluations\non Llama-type models, we reveal a new low-precision scaling law that quantifies\nperformance trade-offs across varying bit-widths and allows us to identify a\n\"near-optimal\" low-precision training technique in terms of\naccuracy-vs-computation, called Quartet. We implement Quartet using optimized\nCUDA kernels tailored for NVIDIA Blackwell GPUs, and show that it can achieve\nstate-of-the-art accuracy for FP4 precision, successfully training\nbillion-scale models. Our method demonstrates that fully FP4-based training is\na competitive alternative to standard-precision and FP8 training. Our code is\navailable at https://github.com/IST-DASLab/Quartet."
                },
                "authors": [
                    {
                        "name": "Roberto L. Castro"
                    },
                    {
                        "name": "Andrei Panferov"
                    },
                    {
                        "name": "Soroush Tabesh"
                    },
                    {
                        "name": "Oliver Sieberling"
                    },
                    {
                        "name": "Jiale Chen"
                    },
                    {
                        "name": "Mahdi Nikdan"
                    },
                    {
                        "name": "Saleh Ashkboos"
                    },
                    {
                        "name": "Dan Alistarh"
                    }
                ],
                "author_detail": {
                    "name": "Dan Alistarh"
                },
                "author": "Dan Alistarh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14669v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14669v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14668v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14668v1",
                "updated": "2025-05-20T17:55:25Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    17,
                    55,
                    25,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T17:55:25Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    17,
                    55,
                    25,
                    1,
                    140,
                    0
                ],
                "title": "ContextAgent: Context-Aware Proactive LLM Agents with Open-World Sensory\n  Perceptions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ContextAgent: Context-Aware Proactive LLM Agents with Open-World Sensory\n  Perceptions"
                },
                "summary": "Recent advances in Large Language Models (LLMs) have propelled intelligent\nagents from reactive responses to proactive support. While promising, existing\nproactive agents either rely exclusively on observations from enclosed\nenvironments (e.g., desktop UIs) with direct LLM inference or employ rule-based\nproactive notifications, leading to suboptimal user intent understanding and\nlimited functionality for proactive service. In this paper, we introduce\nContextAgent, the first context-aware proactive agent that incorporates\nextensive sensory contexts to enhance the proactive capabilities of LLM agents.\nContextAgent first extracts multi-dimensional contexts from massive sensory\nperceptions on wearables (e.g., video and audio) to understand user intentions.\nContextAgent then leverages the sensory contexts and the persona contexts from\nhistorical data to predict the necessity for proactive services. When proactive\nassistance is needed, ContextAgent further automatically calls the necessary\ntools to assist users unobtrusively. To evaluate this new task, we curate\nContextAgentBench, the first benchmark for evaluating context-aware proactive\nLLM agents, covering 1,000 samples across nine daily scenarios and twenty\ntools. Experiments on ContextAgentBench show that ContextAgent outperforms\nbaselines by achieving up to 8.5% and 6.0% higher accuracy in proactive\npredictions and tool calling, respectively. We hope our research can inspire\nthe development of more advanced, human-centric, proactive AI assistants.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large Language Models (LLMs) have propelled intelligent\nagents from reactive responses to proactive support. While promising, existing\nproactive agents either rely exclusively on observations from enclosed\nenvironments (e.g., desktop UIs) with direct LLM inference or employ rule-based\nproactive notifications, leading to suboptimal user intent understanding and\nlimited functionality for proactive service. In this paper, we introduce\nContextAgent, the first context-aware proactive agent that incorporates\nextensive sensory contexts to enhance the proactive capabilities of LLM agents.\nContextAgent first extracts multi-dimensional contexts from massive sensory\nperceptions on wearables (e.g., video and audio) to understand user intentions.\nContextAgent then leverages the sensory contexts and the persona contexts from\nhistorical data to predict the necessity for proactive services. When proactive\nassistance is needed, ContextAgent further automatically calls the necessary\ntools to assist users unobtrusively. To evaluate this new task, we curate\nContextAgentBench, the first benchmark for evaluating context-aware proactive\nLLM agents, covering 1,000 samples across nine daily scenarios and twenty\ntools. Experiments on ContextAgentBench show that ContextAgent outperforms\nbaselines by achieving up to 8.5% and 6.0% higher accuracy in proactive\npredictions and tool calling, respectively. We hope our research can inspire\nthe development of more advanced, human-centric, proactive AI assistants."
                },
                "authors": [
                    {
                        "name": "Bufang Yang"
                    },
                    {
                        "name": "Lilin Xu"
                    },
                    {
                        "name": "Liekang Zeng"
                    },
                    {
                        "name": "Kaiwei Liu"
                    },
                    {
                        "name": "Siyang Jiang"
                    },
                    {
                        "name": "Wenrui Lu"
                    },
                    {
                        "name": "Hongkai Chen"
                    },
                    {
                        "name": "Xiaofan Jiang"
                    },
                    {
                        "name": "Guoliang Xing"
                    },
                    {
                        "name": "Zhenyu Yan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenyu Yan"
                },
                "author": "Zhenyu Yan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14668v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14668v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14667v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14667v1",
                "updated": "2025-05-20T17:54:54Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    17,
                    54,
                    54,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T17:54:54Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    17,
                    54,
                    54,
                    1,
                    140,
                    0
                ],
                "title": "SAFEPATH: Preventing Harmful Reasoning in Chain-of-Thought via Early\n  Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SAFEPATH: Preventing Harmful Reasoning in Chain-of-Thought via Early\n  Alignment"
                },
                "summary": "Large Reasoning Models (LRMs) have become powerful tools for complex problem\nsolving, but their structured reasoning pathways can lead to unsafe outputs\nwhen exposed to harmful prompts. Existing safety alignment methods reduce\nharmful outputs but can degrade reasoning depth, leading to significant\ntrade-offs in complex, multi-step tasks, and remain vulnerable to sophisticated\njailbreak attacks. To address this, we introduce SAFEPATH, a lightweight\nalignment method that fine-tunes LRMs to emit a short, 8-token Safety Primer at\nthe start of their reasoning, in response to harmful prompts, while leaving the\nrest of the reasoning process unsupervised. Empirical results across multiple\nbenchmarks indicate that SAFEPATH effectively reduces harmful outputs while\nmaintaining reasoning performance. Specifically, SAFEPATH reduces harmful\nresponses by up to 90.0% and blocks 83.3% of jailbreak attempts in the\nDeepSeek-R1-Distill-Llama-8B model, while requiring 295.9x less compute than\nDirect Refusal and 314.1x less than SafeChain. We further introduce a zero-shot\nvariant that requires no fine-tuning. In addition, we provide a comprehensive\nanalysis of how existing methods in LLMs generalize, or fail, when applied to\nreasoning-centric models, revealing critical gaps and new directions for safer\nAI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Reasoning Models (LRMs) have become powerful tools for complex problem\nsolving, but their structured reasoning pathways can lead to unsafe outputs\nwhen exposed to harmful prompts. Existing safety alignment methods reduce\nharmful outputs but can degrade reasoning depth, leading to significant\ntrade-offs in complex, multi-step tasks, and remain vulnerable to sophisticated\njailbreak attacks. To address this, we introduce SAFEPATH, a lightweight\nalignment method that fine-tunes LRMs to emit a short, 8-token Safety Primer at\nthe start of their reasoning, in response to harmful prompts, while leaving the\nrest of the reasoning process unsupervised. Empirical results across multiple\nbenchmarks indicate that SAFEPATH effectively reduces harmful outputs while\nmaintaining reasoning performance. Specifically, SAFEPATH reduces harmful\nresponses by up to 90.0% and blocks 83.3% of jailbreak attempts in the\nDeepSeek-R1-Distill-Llama-8B model, while requiring 295.9x less compute than\nDirect Refusal and 314.1x less than SafeChain. We further introduce a zero-shot\nvariant that requires no fine-tuning. In addition, we provide a comprehensive\nanalysis of how existing methods in LLMs generalize, or fail, when applied to\nreasoning-centric models, revealing critical gaps and new directions for safer\nAI."
                },
                "authors": [
                    {
                        "name": "Wonje Jeung"
                    },
                    {
                        "name": "Sangyeon Yoon"
                    },
                    {
                        "name": "Minsuk Kahng"
                    },
                    {
                        "name": "Albert No"
                    }
                ],
                "author_detail": {
                    "name": "Albert No"
                },
                "author": "Albert No",
                "arxiv_comment": "22 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14667v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14667v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15364v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15364v3",
                "updated": "2025-05-20T17:50:11Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    17,
                    50,
                    11,
                    1,
                    140,
                    0
                ],
                "published": "2025-04-21T18:12:46Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    18,
                    12,
                    46,
                    0,
                    111,
                    0
                ],
                "title": "KeyDiff: Key Similarity-Based KV Cache Eviction for Long-Context LLM\n  Inference in Resource-Constrained Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KeyDiff: Key Similarity-Based KV Cache Eviction for Long-Context LLM\n  Inference in Resource-Constrained Environments"
                },
                "summary": "We demonstrate that geometrically distinctive keys during LLM inference tend\nto have high attention scores. Based on the phenomenon we propose KeyDiff, a\ntraining-free KV cache eviction method based solely on key similarity. Unlike\nother KV cache eviction methods, KeyDiff can process arbitrarily long prompts\nwithin strict resource constraints and efficiently generate responses. We\nprovide a theoretical basis for KeyDiff by relating key diversity with\nattention scores. These results imply KeyDiff can efficiently identify the most\nimportant tokens to retain. Notably KeyDiff does not rely on attention scores,\nallowing the use of optimized attention mechanisms like FlashAttention. Under a\nstrict memory allowance, we demonstrate the effectiveness of KeyDiff for the\nLlama and Qwen model families by observing a performance gap of less than 0.04%\nwith 8K cache budget ($\\sim$23% KV cache reduction) from the non-evicting\nbaseline on LongBench for Llama 3.1-8B and Llama 3.2-3B. We also observe near\nbaseline performance for Deepseek-R1-Distill-Llama-8B on the Math500 reasoning\nbenchmark and decrease end-to-end inference latency by up to 30% compared to\nthe other token-eviction methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We demonstrate that geometrically distinctive keys during LLM inference tend\nto have high attention scores. Based on the phenomenon we propose KeyDiff, a\ntraining-free KV cache eviction method based solely on key similarity. Unlike\nother KV cache eviction methods, KeyDiff can process arbitrarily long prompts\nwithin strict resource constraints and efficiently generate responses. We\nprovide a theoretical basis for KeyDiff by relating key diversity with\nattention scores. These results imply KeyDiff can efficiently identify the most\nimportant tokens to retain. Notably KeyDiff does not rely on attention scores,\nallowing the use of optimized attention mechanisms like FlashAttention. Under a\nstrict memory allowance, we demonstrate the effectiveness of KeyDiff for the\nLlama and Qwen model families by observing a performance gap of less than 0.04%\nwith 8K cache budget ($\\sim$23% KV cache reduction) from the non-evicting\nbaseline on LongBench for Llama 3.1-8B and Llama 3.2-3B. We also observe near\nbaseline performance for Deepseek-R1-Distill-Llama-8B on the Math500 reasoning\nbenchmark and decrease end-to-end inference latency by up to 30% compared to\nthe other token-eviction methods."
                },
                "authors": [
                    {
                        "name": "Junyoung Park"
                    },
                    {
                        "name": "Dalton Jones"
                    },
                    {
                        "name": "Matthew J Morse"
                    },
                    {
                        "name": "Raghavv Goel"
                    },
                    {
                        "name": "Mingu Lee"
                    },
                    {
                        "name": "Chris Lott"
                    }
                ],
                "author_detail": {
                    "name": "Chris Lott"
                },
                "author": "Chris Lott",
                "arxiv_comment": "9 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15364v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15364v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14661v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14661v1",
                "updated": "2025-05-20T17:49:46Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    17,
                    49,
                    46,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T17:49:46Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    17,
                    49,
                    46,
                    1,
                    140,
                    0
                ],
                "title": "Abacus: A Cost-Based Optimizer for Semantic Operator Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Abacus: A Cost-Based Optimizer for Semantic Operator Systems"
                },
                "summary": "LLMs enable an exciting new class of data processing applications over large\ncollections of unstructured documents. Several new programming frameworks have\nenabled developers to build these applications by composing them out of\nsemantic operators: a declarative set of AI-powered data transformations with\nnatural language specifications. These include LLM-powered maps, filters,\njoins, etc. used for document processing tasks such as information extraction,\nsummarization, and more. While systems of semantic operators have achieved\nstrong performance on benchmarks, they can be difficult to optimize. An\noptimizer for this setting must determine how to physically implement each\nsemantic operator in a way that optimizes the system globally. Existing\noptimizers are limited in the number of optimizations they can apply, and most\n(if not all) cannot optimize system quality, cost, or latency subject to\nconstraint(s) on the other dimensions. In this paper we present Abacus, an\nextensible, cost-based optimizer which searches for the best implementation of\na semantic operator system given a (possibly constrained) optimization\nobjective. Abacus estimates operator performance by leveraging a minimal set of\nvalidation examples and, if available, prior beliefs about operator\nperformance. We evaluate Abacus on document processing workloads in the\nbiomedical and legal domains (BioDEX; CUAD) and multi-modal question answering\n(MMQA). We demonstrate that systems optimized by Abacus achieve 18.7%-39.2%\nbetter quality and up to 23.6x lower cost and 4.2x lower latency than the next\nbest system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs enable an exciting new class of data processing applications over large\ncollections of unstructured documents. Several new programming frameworks have\nenabled developers to build these applications by composing them out of\nsemantic operators: a declarative set of AI-powered data transformations with\nnatural language specifications. These include LLM-powered maps, filters,\njoins, etc. used for document processing tasks such as information extraction,\nsummarization, and more. While systems of semantic operators have achieved\nstrong performance on benchmarks, they can be difficult to optimize. An\noptimizer for this setting must determine how to physically implement each\nsemantic operator in a way that optimizes the system globally. Existing\noptimizers are limited in the number of optimizations they can apply, and most\n(if not all) cannot optimize system quality, cost, or latency subject to\nconstraint(s) on the other dimensions. In this paper we present Abacus, an\nextensible, cost-based optimizer which searches for the best implementation of\na semantic operator system given a (possibly constrained) optimization\nobjective. Abacus estimates operator performance by leveraging a minimal set of\nvalidation examples and, if available, prior beliefs about operator\nperformance. We evaluate Abacus on document processing workloads in the\nbiomedical and legal domains (BioDEX; CUAD) and multi-modal question answering\n(MMQA). We demonstrate that systems optimized by Abacus achieve 18.7%-39.2%\nbetter quality and up to 23.6x lower cost and 4.2x lower latency than the next\nbest system."
                },
                "authors": [
                    {
                        "name": "Matthew Russo"
                    },
                    {
                        "name": "Sivaprasad Sudhir"
                    },
                    {
                        "name": "Gerardo Vitagliano"
                    },
                    {
                        "name": "Chunwei Liu"
                    },
                    {
                        "name": "Tim Kraska"
                    },
                    {
                        "name": "Samuel Madden"
                    },
                    {
                        "name": "Michael Cafarella"
                    }
                ],
                "author_detail": {
                    "name": "Michael Cafarella"
                },
                "author": "Michael Cafarella",
                "arxiv_comment": "16 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14661v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14661v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.2.4; I.2.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14656v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14656v1",
                "updated": "2025-05-20T17:43:33Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    17,
                    43,
                    33,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T17:43:33Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    17,
                    43,
                    33,
                    1,
                    140,
                    0
                ],
                "title": "Cost-Augmented Monte Carlo Tree Search for LLM-Assisted Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cost-Augmented Monte Carlo Tree Search for LLM-Assisted Planning"
                },
                "summary": "While LLMs excel at open-ended reasoning, they often struggle with\ncost-sensitive planning, either treating all actions as having equal cost or\nfailing to stay within strict budgets. In this paper, we introduce\nCost-Augmented Monte Carlo Tree Search (CATS), a novel approach that brings\nexplicit cost-awareness into LLM-guided planning. Tight cost constraints push\nthe planner to quickly identify infeasible solutions, while looser constraints\nencourage optimization for minimal cost. We benchmark top LLMs such as GPT-4.1,\nClaude-3.7-Sonnet, and DeepSeek-R1, against our CATS planner to evaluate their\nperformance in cost-sensitive scenarios. Our experiments suggest that raw LLMs\nsuch as GPT-4.1 often falter under tight budgets, whereas CATS consistently\ndelivers strong performance, achieving higher task success rates and better\ncost efficiency. CATS provides an effective solution for budget-aware\ndecision-making by combining the reasoning power of LLMs with structured\nsearch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While LLMs excel at open-ended reasoning, they often struggle with\ncost-sensitive planning, either treating all actions as having equal cost or\nfailing to stay within strict budgets. In this paper, we introduce\nCost-Augmented Monte Carlo Tree Search (CATS), a novel approach that brings\nexplicit cost-awareness into LLM-guided planning. Tight cost constraints push\nthe planner to quickly identify infeasible solutions, while looser constraints\nencourage optimization for minimal cost. We benchmark top LLMs such as GPT-4.1,\nClaude-3.7-Sonnet, and DeepSeek-R1, against our CATS planner to evaluate their\nperformance in cost-sensitive scenarios. Our experiments suggest that raw LLMs\nsuch as GPT-4.1 often falter under tight budgets, whereas CATS consistently\ndelivers strong performance, achieving higher task success rates and better\ncost efficiency. CATS provides an effective solution for budget-aware\ndecision-making by combining the reasoning power of LLMs with structured\nsearch."
                },
                "authors": [
                    {
                        "name": "Zihao Zhang"
                    },
                    {
                        "name": "Fei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Fei Liu"
                },
                "author": "Fei Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14656v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14656v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14654v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14654v1",
                "updated": "2025-05-20T17:42:34Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    17,
                    42,
                    34,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T17:42:34Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    17,
                    42,
                    34,
                    1,
                    140,
                    0
                ],
                "title": "Beyond Words: Multimodal LLM Knows When to Speak",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Words: Multimodal LLM Knows When to Speak"
                },
                "summary": "While large language model (LLM)-based chatbots have demonstrated strong\ncapabilities in generating coherent and contextually relevant responses, they\noften struggle with understanding when to speak, particularly in delivering\nbrief, timely reactions during ongoing conversations. This limitation arises\nlargely from their reliance on text input, lacking the rich contextual cues in\nreal-world human dialogue. In this work, we focus on real-time prediction of\nresponse types, with an emphasis on short, reactive utterances that depend on\nsubtle, multimodal signals across vision, audio, and text. To support this, we\nintroduce a new multimodal dataset constructed from real-world conversational\nvideos, containing temporally aligned visual, auditory, and textual streams.\nThis dataset enables fine-grained modeling of response timing in dyadic\ninteractions. Building on this dataset, we propose MM-When2Speak, a multimodal\nLLM-based model that adaptively integrates visual, auditory, and textual\ncontext to predict when a response should occur, and what type of response is\nappropriate. Experiments show that MM-When2Speak significantly outperforms\nstate-of-the-art unimodal and LLM-based baselines, achieving up to a 4x\nimprovement in response timing accuracy over leading commercial LLMs. These\nresults underscore the importance of multimodal inputs for producing timely,\nnatural, and engaging conversational AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language model (LLM)-based chatbots have demonstrated strong\ncapabilities in generating coherent and contextually relevant responses, they\noften struggle with understanding when to speak, particularly in delivering\nbrief, timely reactions during ongoing conversations. This limitation arises\nlargely from their reliance on text input, lacking the rich contextual cues in\nreal-world human dialogue. In this work, we focus on real-time prediction of\nresponse types, with an emphasis on short, reactive utterances that depend on\nsubtle, multimodal signals across vision, audio, and text. To support this, we\nintroduce a new multimodal dataset constructed from real-world conversational\nvideos, containing temporally aligned visual, auditory, and textual streams.\nThis dataset enables fine-grained modeling of response timing in dyadic\ninteractions. Building on this dataset, we propose MM-When2Speak, a multimodal\nLLM-based model that adaptively integrates visual, auditory, and textual\ncontext to predict when a response should occur, and what type of response is\nappropriate. Experiments show that MM-When2Speak significantly outperforms\nstate-of-the-art unimodal and LLM-based baselines, achieving up to a 4x\nimprovement in response timing accuracy over leading commercial LLMs. These\nresults underscore the importance of multimodal inputs for producing timely,\nnatural, and engaging conversational AI."
                },
                "authors": [
                    {
                        "name": "Zikai Liao"
                    },
                    {
                        "name": "Yi Ouyang"
                    },
                    {
                        "name": "Yi-Lun Lee"
                    },
                    {
                        "name": "Chen-Ping Yu"
                    },
                    {
                        "name": "Yi-Hsuan Tsai"
                    },
                    {
                        "name": "Zhaozheng Yin"
                    }
                ],
                "author_detail": {
                    "name": "Zhaozheng Yin"
                },
                "author": "Zhaozheng Yin",
                "arxiv_comment": "Project page: https://github.com/lzk901372/MM-When2Speak",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14654v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14654v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14652v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14652v1",
                "updated": "2025-05-20T17:41:33Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    17,
                    41,
                    33,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T17:41:33Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    17,
                    41,
                    33,
                    1,
                    140,
                    0
                ],
                "title": "General-Reasoner: Advancing LLM Reasoning Across All Domains",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "General-Reasoner: Advancing LLM Reasoning Across All Domains"
                },
                "summary": "Reinforcement learning (RL) has recently demonstrated strong potential in\nenhancing the reasoning capabilities of large language models (LLMs).\nParticularly, the \"Zero\" reinforcement learning introduced by Deepseek-R1-Zero,\nenables direct RL training of base LLMs without relying on an intermediate\nsupervised fine-tuning stage. Despite these advancements, current works for LLM\nreasoning mainly focus on mathematical and coding domains, largely due to data\nabundance and the ease of answer verification. This limits the applicability\nand generalization of such models to broader domains, where questions often\nhave diverse answer representations, and data is more scarce. In this paper, we\npropose General-Reasoner, a novel training paradigm designed to enhance LLM\nreasoning capabilities across diverse domains. Our key contributions include:\n(1) constructing a large-scale, high-quality dataset of questions with\nverifiable answers curated by web crawling, covering a wide range of\ndisciplines; and (2) developing a generative model-based answer verifier, which\nreplaces traditional rule-based verification with the capability of\nchain-of-thought and context-awareness. We train a series of models and\nevaluate them on a wide range of datasets covering wide domains like physics,\nchemistry, finance, electronics etc. Our comprehensive evaluation across these\n12 benchmarks (e.g. MMLU-Pro, GPQA, SuperGPQA, TheoremQA, BBEH and MATH AMC)\ndemonstrates that General-Reasoner outperforms existing baseline methods,\nachieving robust and generalizable reasoning performance while maintaining\nsuperior effectiveness in mathematical reasoning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) has recently demonstrated strong potential in\nenhancing the reasoning capabilities of large language models (LLMs).\nParticularly, the \"Zero\" reinforcement learning introduced by Deepseek-R1-Zero,\nenables direct RL training of base LLMs without relying on an intermediate\nsupervised fine-tuning stage. Despite these advancements, current works for LLM\nreasoning mainly focus on mathematical and coding domains, largely due to data\nabundance and the ease of answer verification. This limits the applicability\nand generalization of such models to broader domains, where questions often\nhave diverse answer representations, and data is more scarce. In this paper, we\npropose General-Reasoner, a novel training paradigm designed to enhance LLM\nreasoning capabilities across diverse domains. Our key contributions include:\n(1) constructing a large-scale, high-quality dataset of questions with\nverifiable answers curated by web crawling, covering a wide range of\ndisciplines; and (2) developing a generative model-based answer verifier, which\nreplaces traditional rule-based verification with the capability of\nchain-of-thought and context-awareness. We train a series of models and\nevaluate them on a wide range of datasets covering wide domains like physics,\nchemistry, finance, electronics etc. Our comprehensive evaluation across these\n12 benchmarks (e.g. MMLU-Pro, GPQA, SuperGPQA, TheoremQA, BBEH and MATH AMC)\ndemonstrates that General-Reasoner outperforms existing baseline methods,\nachieving robust and generalizable reasoning performance while maintaining\nsuperior effectiveness in mathematical reasoning tasks."
                },
                "authors": [
                    {
                        "name": "Xueguang Ma"
                    },
                    {
                        "name": "Qian Liu"
                    },
                    {
                        "name": "Dongfu Jiang"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Zejun Ma"
                    },
                    {
                        "name": "Wenhu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Wenhu Chen"
                },
                "author": "Wenhu Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14652v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14652v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14585v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14585v3",
                "updated": "2025-05-20T17:41:04Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    17,
                    41,
                    4,
                    1,
                    140,
                    0
                ],
                "published": "2024-11-21T20:48:40Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    20,
                    48,
                    40,
                    3,
                    326,
                    0
                ],
                "title": "Efficient Spatio-Temporal Signal Recognition on Edge Devices Using\n  PointLCA-Net",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Spatio-Temporal Signal Recognition on Edge Devices Using\n  PointLCA-Net"
                },
                "summary": "Recent advancements in machine learning, particularly through deep learning\narchitectures like PointNet, have transformed the processing of\nthree-dimensional (3D) point clouds, significantly improving 3D object\nclassification and segmentation tasks. While 3D point clouds provide detailed\nspatial information, spatio-temporal signals introduce a dynamic element that\naccounts for changes over time. However, applying deep learning techniques to\nspatio-temporal signals and deploying them on edge devices presents challenges,\nincluding real-time processing, memory capacity, and power consumption. To\naddress these issues, this paper presents a novel approach that combines\nPointNet's feature extraction with the in-memory computing capabilities and\nenergy efficiency of neuromorphic systems for spatio-temporal signal\nrecognition. The proposed method consists of a two-stage process: in the first\nstage, PointNet extracts features from the spatio-temporal signals, which are\nthen stored in non-volatile memristor crossbar arrays. In the second stage,\nthese features are processed by a single-layer spiking neural encoder-decoder\nthat employs the Locally Competitive Algorithm (LCA) for efficient encoding and\nclassification. This work integrates the strengths of both PointNet and LCA,\nenhancing computational efficiency and energy performance on edge devices.\nPointLCA-Net achieves high recognition accuracy for spatio-temporal data with\nsubstantially lower energy burden during both inference and training than\ncomparable approaches, thus advancing the deployment of advanced neural\narchitectures in energy-constrained environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in machine learning, particularly through deep learning\narchitectures like PointNet, have transformed the processing of\nthree-dimensional (3D) point clouds, significantly improving 3D object\nclassification and segmentation tasks. While 3D point clouds provide detailed\nspatial information, spatio-temporal signals introduce a dynamic element that\naccounts for changes over time. However, applying deep learning techniques to\nspatio-temporal signals and deploying them on edge devices presents challenges,\nincluding real-time processing, memory capacity, and power consumption. To\naddress these issues, this paper presents a novel approach that combines\nPointNet's feature extraction with the in-memory computing capabilities and\nenergy efficiency of neuromorphic systems for spatio-temporal signal\nrecognition. The proposed method consists of a two-stage process: in the first\nstage, PointNet extracts features from the spatio-temporal signals, which are\nthen stored in non-volatile memristor crossbar arrays. In the second stage,\nthese features are processed by a single-layer spiking neural encoder-decoder\nthat employs the Locally Competitive Algorithm (LCA) for efficient encoding and\nclassification. This work integrates the strengths of both PointNet and LCA,\nenhancing computational efficiency and energy performance on edge devices.\nPointLCA-Net achieves high recognition accuracy for spatio-temporal data with\nsubstantially lower energy burden during both inference and training than\ncomparable approaches, thus advancing the deployment of advanced neural\narchitectures in energy-constrained environments."
                },
                "authors": [
                    {
                        "name": "Sanaz Mahmoodi Takaghaj"
                    }
                ],
                "author_detail": {
                    "name": "Sanaz Mahmoodi Takaghaj"
                },
                "author": "Sanaz Mahmoodi Takaghaj",
                "arxiv_comment": "Accepted to International Joint Conference on Neural Networks(IJCNN),\n  2015",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14585v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14585v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14631v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14631v2",
                "updated": "2025-05-21T05:17:34Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    5,
                    17,
                    34,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-20T17:23:25Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    17,
                    23,
                    25,
                    1,
                    140,
                    0
                ],
                "title": "Think Only When You Need with Large Hybrid-Reasoning Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Think Only When You Need with Large Hybrid-Reasoning Models"
                },
                "summary": "Recent Large Reasoning Models (LRMs) have shown substantially improved\nreasoning capabilities over traditional Large Language Models (LLMs) by\nincorporating extended thinking processes prior to producing final responses.\nHowever, excessively lengthy thinking introduces substantial overhead in terms\nof token consumption and latency, which is particularly unnecessary for simple\nqueries. In this work, we introduce Large Hybrid-Reasoning Models (LHRMs), the\nfirst kind of model capable of adaptively determining whether to perform\nthinking based on the contextual information of user queries. To achieve this,\nwe propose a two-stage training pipeline comprising Hybrid Fine-Tuning (HFT) as\na cold start, followed by online reinforcement learning with the proposed\nHybrid Group Policy Optimization (HGPO) to implicitly learn to select the\nappropriate thinking mode. Furthermore, we introduce a metric called Hybrid\nAccuracy to quantitatively assess the model's capability for hybrid thinking.\nExtensive experimental results show that LHRMs can adaptively perform hybrid\nthinking on queries of varying difficulty and type. It outperforms existing\nLRMs and LLMs in reasoning and general capabilities while significantly\nimproving efficiency. Together, our work advocates for a reconsideration of the\nappropriate use of extended thinking processes and provides a solid starting\npoint for building hybrid thinking systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Large Reasoning Models (LRMs) have shown substantially improved\nreasoning capabilities over traditional Large Language Models (LLMs) by\nincorporating extended thinking processes prior to producing final responses.\nHowever, excessively lengthy thinking introduces substantial overhead in terms\nof token consumption and latency, which is particularly unnecessary for simple\nqueries. In this work, we introduce Large Hybrid-Reasoning Models (LHRMs), the\nfirst kind of model capable of adaptively determining whether to perform\nthinking based on the contextual information of user queries. To achieve this,\nwe propose a two-stage training pipeline comprising Hybrid Fine-Tuning (HFT) as\na cold start, followed by online reinforcement learning with the proposed\nHybrid Group Policy Optimization (HGPO) to implicitly learn to select the\nappropriate thinking mode. Furthermore, we introduce a metric called Hybrid\nAccuracy to quantitatively assess the model's capability for hybrid thinking.\nExtensive experimental results show that LHRMs can adaptively perform hybrid\nthinking on queries of varying difficulty and type. It outperforms existing\nLRMs and LLMs in reasoning and general capabilities while significantly\nimproving efficiency. Together, our work advocates for a reconsideration of the\nappropriate use of extended thinking processes and provides a solid starting\npoint for building hybrid thinking systems."
                },
                "authors": [
                    {
                        "name": "Lingjie Jiang"
                    },
                    {
                        "name": "Xun Wu"
                    },
                    {
                        "name": "Shaohan Huang"
                    },
                    {
                        "name": "Qingxiu Dong"
                    },
                    {
                        "name": "Zewen Chi"
                    },
                    {
                        "name": "Li Dong"
                    },
                    {
                        "name": "Xingxing Zhang"
                    },
                    {
                        "name": "Tengchao Lv"
                    },
                    {
                        "name": "Lei Cui"
                    },
                    {
                        "name": "Furu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Furu Wei"
                },
                "author": "Furu Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14631v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14631v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14629v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14629v1",
                "updated": "2025-05-20T17:19:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    17,
                    19,
                    57,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T17:19:57Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    17,
                    19,
                    57,
                    1,
                    140,
                    0
                ],
                "title": "KERL: Knowledge-Enhanced Personalized Recipe Recommendation using Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KERL: Knowledge-Enhanced Personalized Recipe Recommendation using Large\n  Language Models"
                },
                "summary": "Recent advances in large language models (LLMs) and the abundance of food\ndata have resulted in studies to improve food understanding using LLMs. Despite\nseveral recommendation systems utilizing LLMs and Knowledge Graphs (KGs), there\nhas been limited research on integrating food related KGs with LLMs. We\nintroduce KERL, a unified system that leverages food KGs and LLMs to provide\npersonalized food recommendations and generates recipes with associated\nmicro-nutritional information. Given a natural language question, KERL extracts\nentities, retrieves subgraphs from the KG, which are then fed into the LLM as\ncontext to select the recipes that satisfy the constraints. Next, our system\ngenerates the cooking steps and nutritional information for each recipe. To\nevaluate our approach, we also develop a benchmark dataset by curating recipe\nrelated questions, combined with constraints and personal preferences. Through\nextensive experiments, we show that our proposed KG-augmented LLM significantly\noutperforms existing approaches, offering a complete and coherent solution for\nfood recommendation, recipe generation, and nutritional analysis. Our code and\nbenchmark datasets are publicly available at\nhttps://github.com/mohbattharani/KERL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) and the abundance of food\ndata have resulted in studies to improve food understanding using LLMs. Despite\nseveral recommendation systems utilizing LLMs and Knowledge Graphs (KGs), there\nhas been limited research on integrating food related KGs with LLMs. We\nintroduce KERL, a unified system that leverages food KGs and LLMs to provide\npersonalized food recommendations and generates recipes with associated\nmicro-nutritional information. Given a natural language question, KERL extracts\nentities, retrieves subgraphs from the KG, which are then fed into the LLM as\ncontext to select the recipes that satisfy the constraints. Next, our system\ngenerates the cooking steps and nutritional information for each recipe. To\nevaluate our approach, we also develop a benchmark dataset by curating recipe\nrelated questions, combined with constraints and personal preferences. Through\nextensive experiments, we show that our proposed KG-augmented LLM significantly\noutperforms existing approaches, offering a complete and coherent solution for\nfood recommendation, recipe generation, and nutritional analysis. Our code and\nbenchmark datasets are publicly available at\nhttps://github.com/mohbattharani/KERL."
                },
                "authors": [
                    {
                        "name": "Fnu Mohbat"
                    },
                    {
                        "name": "Mohammed J Zaki"
                    }
                ],
                "author_detail": {
                    "name": "Mohammed J Zaki"
                },
                "author": "Mohammed J Zaki",
                "arxiv_comment": "Accepted at ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14629v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14629v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14627v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14627v1",
                "updated": "2025-05-20T17:18:17Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    17,
                    18,
                    17,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T17:18:17Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    17,
                    18,
                    17,
                    1,
                    140,
                    0
                ],
                "title": "Debating for Better Reasoning: An Unsupervised Multimodal Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Debating for Better Reasoning: An Unsupervised Multimodal Approach"
                },
                "summary": "As Large Language Models (LLMs) gain expertise across diverse domains and\nmodalities, scalable oversight becomes increasingly challenging, particularly\nwhen their capabilities may surpass human evaluators. Debate has emerged as a\npromising mechanism for enabling such oversight. In this work, we extend the\ndebate paradigm to a multimodal setting, exploring its potential for weaker\nmodels to supervise and enhance the performance of stronger models. We focus on\nvisual question answering (VQA), where two \"sighted\" expert vision-language\nmodels debate an answer, while a \"blind\" (text-only) judge adjudicates based\nsolely on the quality of the arguments. In our framework, the experts defend\nonly answers aligned with their beliefs, thereby obviating the need for\nexplicit role-playing and concentrating the debate on instances of expert\ndisagreement. Experiments on several multimodal tasks demonstrate that the\ndebate framework consistently outperforms individual expert models. Moreover,\njudgments from weaker LLMs can help instill reasoning capabilities in\nvision-language models through finetuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) gain expertise across diverse domains and\nmodalities, scalable oversight becomes increasingly challenging, particularly\nwhen their capabilities may surpass human evaluators. Debate has emerged as a\npromising mechanism for enabling such oversight. In this work, we extend the\ndebate paradigm to a multimodal setting, exploring its potential for weaker\nmodels to supervise and enhance the performance of stronger models. We focus on\nvisual question answering (VQA), where two \"sighted\" expert vision-language\nmodels debate an answer, while a \"blind\" (text-only) judge adjudicates based\nsolely on the quality of the arguments. In our framework, the experts defend\nonly answers aligned with their beliefs, thereby obviating the need for\nexplicit role-playing and concentrating the debate on instances of expert\ndisagreement. Experiments on several multimodal tasks demonstrate that the\ndebate framework consistently outperforms individual expert models. Moreover,\njudgments from weaker LLMs can help instill reasoning capabilities in\nvision-language models through finetuning."
                },
                "authors": [
                    {
                        "name": "Ashutosh Adhikari"
                    },
                    {
                        "name": "Mirella Lapata"
                    }
                ],
                "author_detail": {
                    "name": "Mirella Lapata"
                },
                "author": "Mirella Lapata",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14627v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14627v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10624v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10624v3",
                "updated": "2025-05-20T17:16:44Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    17,
                    16,
                    44,
                    1,
                    140,
                    0
                ],
                "published": "2024-10-14T15:30:41Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    15,
                    30,
                    41,
                    0,
                    288,
                    0
                ],
                "title": "SensorLLM: Human-Intuitive Alignment of Multivariate Sensor Data with\n  LLMs for Activity Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SensorLLM: Human-Intuitive Alignment of Multivariate Sensor Data with\n  LLMs for Activity Recognition"
                },
                "summary": "We introduce SensorLLM, a two-stage framework that enables Large Language\nModels (LLMs) to perform human activity recognition (HAR) from wearable sensor\ndata. While LLMs excel at reasoning and generalization, they struggle with\ntime-series inputs due to limited semantic context, numerical complexity, and\nsequence variability. To address these challenges, we construct SensorQA, a\nquestion-answering dataset of human-intuitive sensor-text pairs spanning\ndiverse HAR scenarios. It supervises the Sensor-Language Alignment stage, where\nthe model aligns sensor inputs with trend descriptions. Special tokens are\nintroduced to mark channel boundaries. This alignment enables LLMs to interpret\nnumerical patterns, channel-specific signals, and variable-length\ninputs--without requiring human annotation. In the subsequent Task-Aware Tuning\nstage, we adapt the model for multivariate HAR classification, achieving\nperformance that matches or exceeds state-of-the-art methods. Our results show\nthat, guided by human-intuitive alignment, SensorLLM becomes an effective\nsensor learner, reasoner, and classifier--generalizing across varied HAR\nsettings and paving the way for foundation model research in time-series\nanalysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce SensorLLM, a two-stage framework that enables Large Language\nModels (LLMs) to perform human activity recognition (HAR) from wearable sensor\ndata. While LLMs excel at reasoning and generalization, they struggle with\ntime-series inputs due to limited semantic context, numerical complexity, and\nsequence variability. To address these challenges, we construct SensorQA, a\nquestion-answering dataset of human-intuitive sensor-text pairs spanning\ndiverse HAR scenarios. It supervises the Sensor-Language Alignment stage, where\nthe model aligns sensor inputs with trend descriptions. Special tokens are\nintroduced to mark channel boundaries. This alignment enables LLMs to interpret\nnumerical patterns, channel-specific signals, and variable-length\ninputs--without requiring human annotation. In the subsequent Task-Aware Tuning\nstage, we adapt the model for multivariate HAR classification, achieving\nperformance that matches or exceeds state-of-the-art methods. Our results show\nthat, guided by human-intuitive alignment, SensorLLM becomes an effective\nsensor learner, reasoner, and classifier--generalizing across varied HAR\nsettings and paving the way for foundation model research in time-series\nanalysis."
                },
                "authors": [
                    {
                        "name": "Zechen Li"
                    },
                    {
                        "name": "Shohreh Deldari"
                    },
                    {
                        "name": "Linyao Chen"
                    },
                    {
                        "name": "Hao Xue"
                    },
                    {
                        "name": "Flora D. Salim"
                    }
                ],
                "author_detail": {
                    "name": "Flora D. Salim"
                },
                "author": "Flora D. Salim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10624v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10624v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14625v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14625v1",
                "updated": "2025-05-20T17:16:44Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    17,
                    16,
                    44,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T17:16:44Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    17,
                    16,
                    44,
                    1,
                    140,
                    0
                ],
                "title": "TinyV: Reducing False Negatives in Verification Improves RL for LLM\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TinyV: Reducing False Negatives in Verification Improves RL for LLM\n  Reasoning"
                },
                "summary": "Reinforcement Learning (RL) has become a powerful tool for enhancing the\nreasoning abilities of large language models (LLMs) by optimizing their\npolicies with reward signals. Yet, RL's success relies on the reliability of\nrewards, which are provided by verifiers. In this paper, we expose and analyze\na widespread problem--false negatives--where verifiers wrongly reject correct\nmodel outputs. Our in-depth study of the Big-Math-RL-Verified dataset reveals\nthat over 38% of model-generated responses suffer from false negatives, where\nthe verifier fails to recognize correct answers. We show, both empirically and\ntheoretically, that these false negatives severely impair RL training by\ndepriving the model of informative gradient signals and slowing convergence. To\nmitigate this, we propose tinyV, a lightweight LLM-based verifier that augments\nexisting rule-based methods, which dynamically identifies potential false\nnegatives and recovers valid responses to produce more accurate reward\nestimates. Across multiple math-reasoning benchmarks, integrating TinyV boosts\npass rates by up to 10% and accelerates convergence relative to the baseline.\nOur findings highlight the critical importance of addressing verifier false\nnegatives and offer a practical approach to improve RL-based fine-tuning of\nLLMs. Our code is available at https://github.com/uw-nsl/TinyV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning (RL) has become a powerful tool for enhancing the\nreasoning abilities of large language models (LLMs) by optimizing their\npolicies with reward signals. Yet, RL's success relies on the reliability of\nrewards, which are provided by verifiers. In this paper, we expose and analyze\na widespread problem--false negatives--where verifiers wrongly reject correct\nmodel outputs. Our in-depth study of the Big-Math-RL-Verified dataset reveals\nthat over 38% of model-generated responses suffer from false negatives, where\nthe verifier fails to recognize correct answers. We show, both empirically and\ntheoretically, that these false negatives severely impair RL training by\ndepriving the model of informative gradient signals and slowing convergence. To\nmitigate this, we propose tinyV, a lightweight LLM-based verifier that augments\nexisting rule-based methods, which dynamically identifies potential false\nnegatives and recovers valid responses to produce more accurate reward\nestimates. Across multiple math-reasoning benchmarks, integrating TinyV boosts\npass rates by up to 10% and accelerates convergence relative to the baseline.\nOur findings highlight the critical importance of addressing verifier false\nnegatives and offer a practical approach to improve RL-based fine-tuning of\nLLMs. Our code is available at https://github.com/uw-nsl/TinyV."
                },
                "authors": [
                    {
                        "name": "Zhangchen Xu"
                    },
                    {
                        "name": "Yuetai Li"
                    },
                    {
                        "name": "Fengqing Jiang"
                    },
                    {
                        "name": "Bhaskar Ramasubramanian"
                    },
                    {
                        "name": "Luyao Niu"
                    },
                    {
                        "name": "Bill Yuchen Lin"
                    },
                    {
                        "name": "Radha Poovendran"
                    }
                ],
                "author_detail": {
                    "name": "Radha Poovendran"
                },
                "author": "Radha Poovendran",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14625v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14625v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11726v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11726v2",
                "updated": "2025-05-20T17:12:34Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    17,
                    12,
                    34,
                    1,
                    140,
                    0
                ],
                "published": "2024-09-18T06:21:44Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    6,
                    21,
                    44,
                    2,
                    262,
                    0
                ],
                "title": "Revealing and Mitigating the Challenge of Detecting Character Knowledge\n  Errors in LLM Role-Playing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revealing and Mitigating the Challenge of Detecting Character Knowledge\n  Errors in LLM Role-Playing"
                },
                "summary": "Large language model (LLM) role-playing has gained widespread attention.\nAuthentic character knowledge is crucial for constructing realistic LLM\nrole-playing agents. However, existing works usually overlook the exploration\nof LLMs' ability to detect characters' known knowledge errors (KKE) and unknown\nknowledge errors (UKE) while playing roles, which would lead to low-quality\nautomatic construction of character trainable corpus. In this paper, we propose\nRoleKE-Bench to evaluate LLMs' ability to detect errors in KKE and UKE. The\nresults indicate that even the latest LLMs struggle to detect these two types\nof errors effectively, especially when it comes to familiar knowledge. We\nexperimented with various reasoning strategies and propose an agent-based\nreasoning method, Self-Recollection and Self-Doubt (S$^2$RD), to explore\nfurther the potential for improving error detection capabilities. Experiments\nshow that our method effectively improves the LLMs' ability to detect error\ncharacter knowledge, but it remains an issue that requires ongoing attention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) role-playing has gained widespread attention.\nAuthentic character knowledge is crucial for constructing realistic LLM\nrole-playing agents. However, existing works usually overlook the exploration\nof LLMs' ability to detect characters' known knowledge errors (KKE) and unknown\nknowledge errors (UKE) while playing roles, which would lead to low-quality\nautomatic construction of character trainable corpus. In this paper, we propose\nRoleKE-Bench to evaluate LLMs' ability to detect errors in KKE and UKE. The\nresults indicate that even the latest LLMs struggle to detect these two types\nof errors effectively, especially when it comes to familiar knowledge. We\nexperimented with various reasoning strategies and propose an agent-based\nreasoning method, Self-Recollection and Self-Doubt (S$^2$RD), to explore\nfurther the potential for improving error detection capabilities. Experiments\nshow that our method effectively improves the LLMs' ability to detect error\ncharacter knowledge, but it remains an issue that requires ongoing attention."
                },
                "authors": [
                    {
                        "name": "Wenyuan Zhang"
                    },
                    {
                        "name": "Shuaiyi Nie"
                    },
                    {
                        "name": "Jiawei Sheng"
                    },
                    {
                        "name": "Zefeng Zhang"
                    },
                    {
                        "name": "Xinghua Zhang"
                    },
                    {
                        "name": "Yongquan He"
                    },
                    {
                        "name": "Tingwen Liu"
                    }
                ],
                "author_detail": {
                    "name": "Tingwen Liu"
                },
                "author": "Tingwen Liu",
                "arxiv_comment": "25 pages, 6 figures, 20 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11726v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11726v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14620v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14620v1",
                "updated": "2025-05-20T17:11:18Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    17,
                    11,
                    18,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T17:11:18Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    17,
                    11,
                    18,
                    1,
                    140,
                    0
                ],
                "title": "Enhancing Learned Knowledge in LoRA Adapters Through Efficient\n  Contrastive Decoding on Ascend NPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Learned Knowledge in LoRA Adapters Through Efficient\n  Contrastive Decoding on Ascend NPUs"
                },
                "summary": "Huawei Cloud users leverage LoRA (Low-Rank Adaptation) as an efficient and\nscalable method to fine-tune and customize large language models (LLMs) for\napplication-specific needs. However, tasks that require complex reasoning or\ndeep contextual understanding are often hindered by biases or interference from\nthe base model when using typical decoding methods like greedy or beam search.\nThese biases can lead to generic or task-agnostic responses from the base model\ninstead of leveraging the LoRA-specific adaptations. In this paper, we\nintroduce Contrastive LoRA Decoding (CoLD), a novel decoding framework designed\nto maximize the use of task-specific knowledge in LoRA-adapted models,\nresulting in better downstream performance. CoLD uses contrastive decoding by\nscoring candidate tokens based on the divergence between the probability\ndistributions of a LoRA-adapted expert model and the corresponding base model.\nThis approach prioritizes tokens that better align with the LoRA's learned\nrepresentations, enhancing performance for specialized tasks. While effective,\na naive implementation of CoLD is computationally expensive because each\ndecoding step requires evaluating multiple token candidates across both models.\nTo address this, we developed an optimized kernel for Huawei's Ascend NPU. CoLD\nachieves up to a 5.54% increase in task accuracy while reducing end-to-end\nlatency by 28% compared to greedy decoding. This work provides practical and\nefficient decoding strategies for fine-tuned LLMs in resource-constrained\nenvironments and has broad implications for applied data science in both cloud\nand on-premises settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Huawei Cloud users leverage LoRA (Low-Rank Adaptation) as an efficient and\nscalable method to fine-tune and customize large language models (LLMs) for\napplication-specific needs. However, tasks that require complex reasoning or\ndeep contextual understanding are often hindered by biases or interference from\nthe base model when using typical decoding methods like greedy or beam search.\nThese biases can lead to generic or task-agnostic responses from the base model\ninstead of leveraging the LoRA-specific adaptations. In this paper, we\nintroduce Contrastive LoRA Decoding (CoLD), a novel decoding framework designed\nto maximize the use of task-specific knowledge in LoRA-adapted models,\nresulting in better downstream performance. CoLD uses contrastive decoding by\nscoring candidate tokens based on the divergence between the probability\ndistributions of a LoRA-adapted expert model and the corresponding base model.\nThis approach prioritizes tokens that better align with the LoRA's learned\nrepresentations, enhancing performance for specialized tasks. While effective,\na naive implementation of CoLD is computationally expensive because each\ndecoding step requires evaluating multiple token candidates across both models.\nTo address this, we developed an optimized kernel for Huawei's Ascend NPU. CoLD\nachieves up to a 5.54% increase in task accuracy while reducing end-to-end\nlatency by 28% compared to greedy decoding. This work provides practical and\nefficient decoding strategies for fine-tuned LLMs in resource-constrained\nenvironments and has broad implications for applied data science in both cloud\nand on-premises settings."
                },
                "authors": [
                    {
                        "name": "Morgan Lindsay Heisler"
                    },
                    {
                        "name": "Linzi Xing"
                    },
                    {
                        "name": "Ge Shi"
                    },
                    {
                        "name": "Hanieh Sadri"
                    },
                    {
                        "name": "Gursimran Singh"
                    },
                    {
                        "name": "Weiwei Zhang"
                    },
                    {
                        "name": "Tao Ye"
                    },
                    {
                        "name": "Ying Xiong"
                    },
                    {
                        "name": "Yong Zhang"
                    },
                    {
                        "name": "Zhenan Fan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenan Fan"
                },
                "author": "Zhenan Fan",
                "arxiv_comment": "Accepted at ACM KDD 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14620v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14620v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01167v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01167v3",
                "updated": "2025-05-21T02:06:29Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    2,
                    6,
                    29,
                    2,
                    141,
                    0
                ],
                "published": "2025-04-01T20:14:35Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    20,
                    14,
                    35,
                    1,
                    91,
                    0
                ],
                "title": "Predicting Field Experiments with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predicting Field Experiments with Large Language Models"
                },
                "summary": "Large language models (LLMs) have demonstrated unprecedented emergent\ncapabilities, including content generation, translation, and simulation of\nhuman behavior. Field experiments, on the other hand, are widely employed in\nsocial studies to examine real-world human behavior through carefully designed\nmanipulations and treatments. However, field experiments are known to be\nexpensive and time consuming. Therefore, an interesting question is whether and\nhow LLMs can be utilized for field experiments. In this paper, we propose and\nevaluate an automated LLM-based framework to predict the outcomes of a field\nexperiment. Applying this framework to 276 experiments about a wide range of\nhuman behaviors drawn from renowned economics literature yields a prediction\naccuracy of 78%. Moreover, we find that the distributions of the results are\neither bimodal or highly skewed. By investigating this abnormality further, we\nidentify that field experiments related to complex social issues such as\nethnicity, social norms, and ethical dilemmas can pose significant challenges\nto the prediction performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated unprecedented emergent\ncapabilities, including content generation, translation, and simulation of\nhuman behavior. Field experiments, on the other hand, are widely employed in\nsocial studies to examine real-world human behavior through carefully designed\nmanipulations and treatments. However, field experiments are known to be\nexpensive and time consuming. Therefore, an interesting question is whether and\nhow LLMs can be utilized for field experiments. In this paper, we propose and\nevaluate an automated LLM-based framework to predict the outcomes of a field\nexperiment. Applying this framework to 276 experiments about a wide range of\nhuman behaviors drawn from renowned economics literature yields a prediction\naccuracy of 78%. Moreover, we find that the distributions of the results are\neither bimodal or highly skewed. By investigating this abnormality further, we\nidentify that field experiments related to complex social issues such as\nethnicity, social norms, and ethical dilemmas can pose significant challenges\nto the prediction performance."
                },
                "authors": [
                    {
                        "name": "Yaoyu Chen"
                    },
                    {
                        "name": "Yuheng Hu"
                    },
                    {
                        "name": "Yingda Lu"
                    }
                ],
                "author_detail": {
                    "name": "Yingda Lu"
                },
                "author": "Yingda Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01167v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01167v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07482v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07482v2",
                "updated": "2025-05-20T17:09:53Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    17,
                    9,
                    53,
                    1,
                    140,
                    0
                ],
                "published": "2025-01-13T16:58:32Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    16,
                    58,
                    32,
                    0,
                    13,
                    0
                ],
                "title": "TiEBe: Tracking Language Model Recall of Notable Worldwide Events\n  Through Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TiEBe: Tracking Language Model Recall of Notable Worldwide Events\n  Through Time"
                },
                "summary": "As the knowledge landscape evolves and large language models (LLMs) become\nincreasingly widespread, there is a growing need to keep these models updated\nwith current events. While existing benchmarks assess general factual recall,\nfew studies explore how LLMs retain knowledge over time or across different\nregions. To address these gaps, we present the Timely Events Benchmark (TiEBe),\na dataset of over 23,000 question-answer pairs centered on notable global and\nregional events, spanning more than 10 years of events, 23 regions, and 13\nlanguages. TiEBe leverages structured retrospective data from Wikipedia to\nidentify notable events through time. These events are then used to construct a\nbenchmark to evaluate LLMs' understanding of global and regional developments,\ngrounded in factual evidence beyond Wikipedia itself. Our results reveal\nsignificant geographic disparities in factual recall, emphasizing the need for\nmore balanced global representation in LLM training. We also observe a Pearson\ncorrelation of more than 0.7 between models' performance in TiEBe and various\ncountries' socioeconomic indicators, such as HDI. In addition, we examine the\nimpact of language on factual recall by posing questions in the native language\nof the region where each event occurred, uncovering substantial performance\ngaps for low-resource languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the knowledge landscape evolves and large language models (LLMs) become\nincreasingly widespread, there is a growing need to keep these models updated\nwith current events. While existing benchmarks assess general factual recall,\nfew studies explore how LLMs retain knowledge over time or across different\nregions. To address these gaps, we present the Timely Events Benchmark (TiEBe),\na dataset of over 23,000 question-answer pairs centered on notable global and\nregional events, spanning more than 10 years of events, 23 regions, and 13\nlanguages. TiEBe leverages structured retrospective data from Wikipedia to\nidentify notable events through time. These events are then used to construct a\nbenchmark to evaluate LLMs' understanding of global and regional developments,\ngrounded in factual evidence beyond Wikipedia itself. Our results reveal\nsignificant geographic disparities in factual recall, emphasizing the need for\nmore balanced global representation in LLM training. We also observe a Pearson\ncorrelation of more than 0.7 between models' performance in TiEBe and various\ncountries' socioeconomic indicators, such as HDI. In addition, we examine the\nimpact of language on factual recall by posing questions in the native language\nof the region where each event occurred, uncovering substantial performance\ngaps for low-resource languages."
                },
                "authors": [
                    {
                        "name": "Thales Sales Almeida"
                    },
                    {
                        "name": "Giovana Kerche Bonás"
                    },
                    {
                        "name": "João Guilherme Alves Santos"
                    },
                    {
                        "name": "Hugo Abonizio"
                    },
                    {
                        "name": "Rodrigo Nogueira"
                    }
                ],
                "author_detail": {
                    "name": "Rodrigo Nogueira"
                },
                "author": "Rodrigo Nogueira",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07482v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07482v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07337v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07337v2",
                "updated": "2025-05-20T17:08:45Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    17,
                    8,
                    45,
                    1,
                    140,
                    0
                ],
                "published": "2024-08-14T07:22:28Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    7,
                    22,
                    28,
                    2,
                    227,
                    0
                ],
                "title": "KIND: Knowledge Integration and Diversion for Training Decomposable\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KIND: Knowledge Integration and Diversion for Training Decomposable\n  Models"
                },
                "summary": "Pre-trained models have become the preferred backbone due to the increasing\ncomplexity of model parameters. However, traditional pre-trained models often\nface deployment challenges due to their fixed sizes, and are prone to negative\ntransfer when discrepancies arise between training tasks and target tasks. To\naddress this, we propose KIND, a novel pre-training method designed to\nconstruct decomposable models. KIND integrates knowledge by incorporating\nSingular Value Decomposition (SVD) as a structural constraint, with each basic\ncomponent represented as a combination of a column vector, singular value, and\nrow vector from U, \\Sigma, and V^\\top matrices. These components are\ncategorized into learngenes for encapsulating class-agnostic knowledge and\ntailors for capturing class-specific knowledge, with knowledge diversion\nfacilitated by a class gate mechanism during training. Extensive experiments\ndemonstrate that models pre-trained with KIND can be decomposed into learngenes\nand tailors, which can be adaptively recombined for diverse\nresource-constrained deployments. Moreover, for tasks with large domain shifts,\ntransferring only learngenes with task-agnostic knowledge, when combined with\nrandomly initialized tailors, effectively mitigates domain shifts. Code will be\nmade available at https://github.com/Te4P0t/KIND.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pre-trained models have become the preferred backbone due to the increasing\ncomplexity of model parameters. However, traditional pre-trained models often\nface deployment challenges due to their fixed sizes, and are prone to negative\ntransfer when discrepancies arise between training tasks and target tasks. To\naddress this, we propose KIND, a novel pre-training method designed to\nconstruct decomposable models. KIND integrates knowledge by incorporating\nSingular Value Decomposition (SVD) as a structural constraint, with each basic\ncomponent represented as a combination of a column vector, singular value, and\nrow vector from U, \\Sigma, and V^\\top matrices. These components are\ncategorized into learngenes for encapsulating class-agnostic knowledge and\ntailors for capturing class-specific knowledge, with knowledge diversion\nfacilitated by a class gate mechanism during training. Extensive experiments\ndemonstrate that models pre-trained with KIND can be decomposed into learngenes\nand tailors, which can be adaptively recombined for diverse\nresource-constrained deployments. Moreover, for tasks with large domain shifts,\ntransferring only learngenes with task-agnostic knowledge, when combined with\nrandomly initialized tailors, effectively mitigates domain shifts. Code will be\nmade available at https://github.com/Te4P0t/KIND."
                },
                "authors": [
                    {
                        "name": "Yucheng Xie"
                    },
                    {
                        "name": "Fu Feng"
                    },
                    {
                        "name": "Ruixiao Shi"
                    },
                    {
                        "name": "Jing Wang"
                    },
                    {
                        "name": "Yong Rui"
                    },
                    {
                        "name": "Xin Geng"
                    }
                ],
                "author_detail": {
                    "name": "Xin Geng"
                },
                "author": "Xin Geng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07337v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07337v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14617v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14617v1",
                "updated": "2025-05-20T17:03:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    17,
                    3,
                    12,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T17:03:12Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    17,
                    3,
                    12,
                    1,
                    140,
                    0
                ],
                "title": "Linear Control of Test Awareness Reveals Differential Compliance in\n  Reasoning Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linear Control of Test Awareness Reveals Differential Compliance in\n  Reasoning Models"
                },
                "summary": "Reasoning-focused large language models (LLMs) sometimes alter their behavior\nwhen they detect that they are being evaluated, an effect analogous to the\nHawthorne phenomenon, which can lead them to optimize for test-passing\nperformance or to comply more readily with harmful prompts if real-world\nconsequences appear absent. We present the first quantitative study of how such\n\"test awareness\" impacts model behavior, particularly its safety alignment. We\nintroduce a white-box probing framework that (i) linearly identifies\nawareness-related activations and (ii) steers models toward or away from test\nawareness while monitoring downstream performance. We apply our method to\ndifferent state-of-the-art open-source reasoning LLMs across both realistic and\nhypothetical tasks. Our results demonstrate that test awareness significantly\nimpact safety alignment, and is different for different models. By providing\nfine-grained control over this latent effect, our work aims to increase trust\nin how we perform safety evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning-focused large language models (LLMs) sometimes alter their behavior\nwhen they detect that they are being evaluated, an effect analogous to the\nHawthorne phenomenon, which can lead them to optimize for test-passing\nperformance or to comply more readily with harmful prompts if real-world\nconsequences appear absent. We present the first quantitative study of how such\n\"test awareness\" impacts model behavior, particularly its safety alignment. We\nintroduce a white-box probing framework that (i) linearly identifies\nawareness-related activations and (ii) steers models toward or away from test\nawareness while monitoring downstream performance. We apply our method to\ndifferent state-of-the-art open-source reasoning LLMs across both realistic and\nhypothetical tasks. Our results demonstrate that test awareness significantly\nimpact safety alignment, and is different for different models. By providing\nfine-grained control over this latent effect, our work aims to increase trust\nin how we perform safety evaluation."
                },
                "authors": [
                    {
                        "name": "Sahar Abdelnabi"
                    },
                    {
                        "name": "Ahmed Salem"
                    }
                ],
                "author_detail": {
                    "name": "Ahmed Salem"
                },
                "author": "Ahmed Salem",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14617v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14617v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14615v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14615v1",
                "updated": "2025-05-20T17:00:22Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    17,
                    0,
                    22,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T17:00:22Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    17,
                    0,
                    22,
                    1,
                    140,
                    0
                ],
                "title": "SATBench: Benchmarking LLMs' Logical Reasoning via Automated Puzzle\n  Generation from SAT Formulas",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SATBench: Benchmarking LLMs' Logical Reasoning via Automated Puzzle\n  Generation from SAT Formulas"
                },
                "summary": "We introduce SATBench, a benchmark for evaluating the logical reasoning\ncapabilities of large language models (LLMs) through logical puzzles derived\nfrom Boolean satisfiability (SAT) problems. Unlike prior work that focuses on\ninference rule-based reasoning, which often involves deducing conclusions from\na set of premises, our approach leverages the search-based nature of SAT\nproblems, where the objective is to find a solution that fulfills a specified\nset of logical constraints. Each instance in SATBench is generated from a SAT\nformula, then translated into a story context and conditions using LLMs. The\ngeneration process is fully automated and allows for adjustable difficulty by\nvarying the number of clauses. All 2100 puzzles are validated through both\nLLM-assisted and solver-based consistency checks, with human validation on a\nsubset. Experimental results show that even the strongest model, o4-mini,\nachieves only 65.0% accuracy on hard UNSAT problems, close to the random\nbaseline of 50%. SATBench exposes fundamental limitations in the search-based\nlogical reasoning abilities of current LLMs and provides a scalable testbed for\nfuture research in logical reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce SATBench, a benchmark for evaluating the logical reasoning\ncapabilities of large language models (LLMs) through logical puzzles derived\nfrom Boolean satisfiability (SAT) problems. Unlike prior work that focuses on\ninference rule-based reasoning, which often involves deducing conclusions from\na set of premises, our approach leverages the search-based nature of SAT\nproblems, where the objective is to find a solution that fulfills a specified\nset of logical constraints. Each instance in SATBench is generated from a SAT\nformula, then translated into a story context and conditions using LLMs. The\ngeneration process is fully automated and allows for adjustable difficulty by\nvarying the number of clauses. All 2100 puzzles are validated through both\nLLM-assisted and solver-based consistency checks, with human validation on a\nsubset. Experimental results show that even the strongest model, o4-mini,\nachieves only 65.0% accuracy on hard UNSAT problems, close to the random\nbaseline of 50%. SATBench exposes fundamental limitations in the search-based\nlogical reasoning abilities of current LLMs and provides a scalable testbed for\nfuture research in logical reasoning."
                },
                "authors": [
                    {
                        "name": "Anjiang Wei"
                    },
                    {
                        "name": "Yuheng Wu"
                    },
                    {
                        "name": "Yingjia Wan"
                    },
                    {
                        "name": "Tarun Suresh"
                    },
                    {
                        "name": "Huanmi Tan"
                    },
                    {
                        "name": "Zhanke Zhou"
                    },
                    {
                        "name": "Sanmi Koyejo"
                    },
                    {
                        "name": "Ke Wang"
                    },
                    {
                        "name": "Alex Aiken"
                    }
                ],
                "author_detail": {
                    "name": "Alex Aiken"
                },
                "author": "Alex Aiken",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14615v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14615v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14607v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14607v1",
                "updated": "2025-05-20T16:54:34Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    16,
                    54,
                    34,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T16:54:34Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    16,
                    54,
                    34,
                    1,
                    140,
                    0
                ],
                "title": "sudoLLM : On Multi-role Alignment of Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "sudoLLM : On Multi-role Alignment of Language Models"
                },
                "summary": "User authorization-based access privileges are a key feature in many\nsafety-critical systems, but have thus far been absent from the large language\nmodel (LLM) realm. In this work, drawing inspiration from such access control\nsystems, we introduce sudoLLM, a novel framework that results in multi-role\naligned LLMs, i.e., LLMs that account for, and behave in accordance with, user\naccess rights. sudoLLM injects subtle user-based biases into queries and trains\nan LLM to utilize this bias signal in order to produce sensitive information if\nand only if the user is authorized. We present empirical results demonstrating\nthat this approach shows substantially improved alignment, generalization, and\nresistance to prompt-based jailbreaking attacks. The persistent tension between\nthe language modeling objective and safety alignment, which is often exploited\nto jailbreak LLMs, is somewhat resolved with the aid of the injected bias\nsignal. Our framework is meant as an additional security layer, and complements\nexisting guardrail mechanisms for enhanced end-to-end safety with LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "User authorization-based access privileges are a key feature in many\nsafety-critical systems, but have thus far been absent from the large language\nmodel (LLM) realm. In this work, drawing inspiration from such access control\nsystems, we introduce sudoLLM, a novel framework that results in multi-role\naligned LLMs, i.e., LLMs that account for, and behave in accordance with, user\naccess rights. sudoLLM injects subtle user-based biases into queries and trains\nan LLM to utilize this bias signal in order to produce sensitive information if\nand only if the user is authorized. We present empirical results demonstrating\nthat this approach shows substantially improved alignment, generalization, and\nresistance to prompt-based jailbreaking attacks. The persistent tension between\nthe language modeling objective and safety alignment, which is often exploited\nto jailbreak LLMs, is somewhat resolved with the aid of the injected bias\nsignal. Our framework is meant as an additional security layer, and complements\nexisting guardrail mechanisms for enhanced end-to-end safety with LLMs."
                },
                "authors": [
                    {
                        "name": "Soumadeep Saha"
                    },
                    {
                        "name": "Akshay Chaturvedi"
                    },
                    {
                        "name": "Joy Mahapatra"
                    },
                    {
                        "name": "Utpal Garain"
                    }
                ],
                "author_detail": {
                    "name": "Utpal Garain"
                },
                "author": "Utpal Garain",
                "arxiv_comment": "Under review. Code and data to be released later",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14607v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14607v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14604v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14604v1",
                "updated": "2025-05-20T16:53:40Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    16,
                    53,
                    40,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T16:53:40Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    16,
                    53,
                    40,
                    1,
                    140,
                    0
                ],
                "title": "Let LLMs Break Free from Overthinking via Self-Braking Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Let LLMs Break Free from Overthinking via Self-Braking Tuning"
                },
                "summary": "Large reasoning models (LRMs), such as OpenAI o1 and DeepSeek-R1, have\nsignificantly enhanced their reasoning capabilities by generating longer chains\nof thought, demonstrating outstanding performance across a variety of tasks.\nHowever, this performance gain comes at the cost of a substantial increase in\nredundant reasoning during the generation process, leading to high\ncomputational overhead and exacerbating the issue of overthinking. Although\nnumerous existing approaches aim to address the problem of overthinking, they\noften rely on external interventions. In this paper, we propose a novel\nframework, Self-Braking Tuning (SBT), which tackles overthinking from the\nperspective of allowing the model to regulate its own reasoning process, thus\neliminating the reliance on external control mechanisms. We construct a set of\noverthinking identification metrics based on standard answers and design a\nsystematic method to detect redundant reasoning. This method accurately\nidentifies unnecessary steps within the reasoning trajectory and generates\ntraining signals for learning self-regulation behaviors. Building on this\nfoundation, we develop a complete strategy for constructing data with adaptive\nreasoning lengths and introduce an innovative braking prompt mechanism that\nenables the model to naturally learn when to terminate reasoning at an\nappropriate point. Experiments across mathematical benchmarks (AIME, AMC,\nMATH500, GSM8K) demonstrate that our method reduces token consumption by up to\n60% while maintaining comparable accuracy to unconstrained models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large reasoning models (LRMs), such as OpenAI o1 and DeepSeek-R1, have\nsignificantly enhanced their reasoning capabilities by generating longer chains\nof thought, demonstrating outstanding performance across a variety of tasks.\nHowever, this performance gain comes at the cost of a substantial increase in\nredundant reasoning during the generation process, leading to high\ncomputational overhead and exacerbating the issue of overthinking. Although\nnumerous existing approaches aim to address the problem of overthinking, they\noften rely on external interventions. In this paper, we propose a novel\nframework, Self-Braking Tuning (SBT), which tackles overthinking from the\nperspective of allowing the model to regulate its own reasoning process, thus\neliminating the reliance on external control mechanisms. We construct a set of\noverthinking identification metrics based on standard answers and design a\nsystematic method to detect redundant reasoning. This method accurately\nidentifies unnecessary steps within the reasoning trajectory and generates\ntraining signals for learning self-regulation behaviors. Building on this\nfoundation, we develop a complete strategy for constructing data with adaptive\nreasoning lengths and introduce an innovative braking prompt mechanism that\nenables the model to naturally learn when to terminate reasoning at an\nappropriate point. Experiments across mathematical benchmarks (AIME, AMC,\nMATH500, GSM8K) demonstrate that our method reduces token consumption by up to\n60% while maintaining comparable accuracy to unconstrained models."
                },
                "authors": [
                    {
                        "name": "Haoran Zhao"
                    },
                    {
                        "name": "Yuchen Yan"
                    },
                    {
                        "name": "Yongliang Shen"
                    },
                    {
                        "name": "Haolei Xu"
                    },
                    {
                        "name": "Wenqi Zhang"
                    },
                    {
                        "name": "Kaitao Song"
                    },
                    {
                        "name": "Jian Shao"
                    },
                    {
                        "name": "Weiming Lu"
                    },
                    {
                        "name": "Jun Xiao"
                    },
                    {
                        "name": "Yueting Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Yueting Zhuang"
                },
                "author": "Yueting Zhuang",
                "arxiv_comment": "Github:https://github.com/CCAI-Lab/Self-Braking-Tuning; Project:\n  https://CCAI-Lab.github.io/SBT",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14604v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14604v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14599v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14599v1",
                "updated": "2025-05-20T16:49:40Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    16,
                    49,
                    40,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T16:49:40Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    16,
                    49,
                    40,
                    1,
                    140,
                    0
                ],
                "title": "Toward Reliable Biomedical Hypothesis Generation: Evaluating\n  Truthfulness and Hallucination in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward Reliable Biomedical Hypothesis Generation: Evaluating\n  Truthfulness and Hallucination in Large Language Models"
                },
                "summary": "Large language models (LLMs) have shown significant potential in scientific\ndisciplines such as biomedicine, particularly in hypothesis generation, where\nthey can analyze vast literature, identify patterns, and suggest research\ndirections. However, a key challenge lies in evaluating the truthfulness of\ngenerated hypotheses, as verifying their accuracy often requires substantial\ntime and resources. Additionally, the hallucination problem in LLMs can lead to\nthe generation of hypotheses that appear plausible but are ultimately\nincorrect, undermining their reliability. To facilitate the systematic study of\nthese challenges, we introduce TruthHypo, a benchmark for assessing the\ncapabilities of LLMs in generating truthful biomedical hypotheses, and KnowHD,\na knowledge-based hallucination detector to evaluate how well hypotheses are\ngrounded in existing knowledge. Our results show that LLMs struggle to generate\ntruthful hypotheses. By analyzing hallucinations in reasoning steps, we\ndemonstrate that the groundedness scores provided by KnowHD serve as an\neffective metric for filtering truthful hypotheses from the diverse outputs of\nLLMs. Human evaluations further validate the utility of KnowHD in identifying\ntruthful hypotheses and accelerating scientific discovery. Our data and source\ncode are available at https://github.com/Teddy-XiongGZ/TruthHypo.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown significant potential in scientific\ndisciplines such as biomedicine, particularly in hypothesis generation, where\nthey can analyze vast literature, identify patterns, and suggest research\ndirections. However, a key challenge lies in evaluating the truthfulness of\ngenerated hypotheses, as verifying their accuracy often requires substantial\ntime and resources. Additionally, the hallucination problem in LLMs can lead to\nthe generation of hypotheses that appear plausible but are ultimately\nincorrect, undermining their reliability. To facilitate the systematic study of\nthese challenges, we introduce TruthHypo, a benchmark for assessing the\ncapabilities of LLMs in generating truthful biomedical hypotheses, and KnowHD,\na knowledge-based hallucination detector to evaluate how well hypotheses are\ngrounded in existing knowledge. Our results show that LLMs struggle to generate\ntruthful hypotheses. By analyzing hallucinations in reasoning steps, we\ndemonstrate that the groundedness scores provided by KnowHD serve as an\neffective metric for filtering truthful hypotheses from the diverse outputs of\nLLMs. Human evaluations further validate the utility of KnowHD in identifying\ntruthful hypotheses and accelerating scientific discovery. Our data and source\ncode are available at https://github.com/Teddy-XiongGZ/TruthHypo."
                },
                "authors": [
                    {
                        "name": "Guangzhi Xiong"
                    },
                    {
                        "name": "Eric Xie"
                    },
                    {
                        "name": "Corey Williams"
                    },
                    {
                        "name": "Myles Kim"
                    },
                    {
                        "name": "Amir Hassan Shariatmadari"
                    },
                    {
                        "name": "Sikun Guo"
                    },
                    {
                        "name": "Stefan Bekiranov"
                    },
                    {
                        "name": "Aidong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Aidong Zhang"
                },
                "author": "Aidong Zhang",
                "arxiv_comment": "Accepted to IJCAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14599v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14599v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15241v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15241v2",
                "updated": "2025-05-20T16:49:30Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    16,
                    49,
                    30,
                    1,
                    140,
                    0
                ],
                "published": "2025-04-21T17:15:06Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    17,
                    15,
                    6,
                    0,
                    111,
                    0
                ],
                "title": "MrGuard: A Multilingual Reasoning Guardrail for Universal LLM Safety",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MrGuard: A Multilingual Reasoning Guardrail for Universal LLM Safety"
                },
                "summary": "Large Language Models (LLMs) are susceptible to adversarial attacks such as\njailbreaking, which can elicit harmful or unsafe behaviors. This vulnerability\nis exacerbated in multilingual settings, where multilingual safety-aligned data\nis often limited. Thus, developing a guardrail capable of detecting and\nfiltering unsafe content across diverse languages is critical for deploying\nLLMs in real-world applications. In this work, we introduce a multilingual\nguardrail with reasoning for prompt classification. Our method consists of: (1)\nsynthetic multilingual data generation incorporating culturally and\nlinguistically nuanced variants, (2) supervised fine-tuning, and (3) a\ncurriculum-based Group Relative Policy Optimization (GRPO) framework that\nfurther improves performance. Experimental results demonstrate that our\nmultilingual guardrail, MrGuard, consistently outperforms recent baselines\nacross both in-domain and out-of-domain languages by more than 15%. We also\nevaluate MrGuard's robustness to multilingual variations, such as\ncode-switching and low-resource language distractors in the prompt, and\ndemonstrate that it preserves safety judgments under these challenging\nconditions. The multilingual reasoning capability of our guardrail enables it\nto generate explanations, which are particularly useful for understanding\nlanguage-specific risks and ambiguities in multilingual content moderation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are susceptible to adversarial attacks such as\njailbreaking, which can elicit harmful or unsafe behaviors. This vulnerability\nis exacerbated in multilingual settings, where multilingual safety-aligned data\nis often limited. Thus, developing a guardrail capable of detecting and\nfiltering unsafe content across diverse languages is critical for deploying\nLLMs in real-world applications. In this work, we introduce a multilingual\nguardrail with reasoning for prompt classification. Our method consists of: (1)\nsynthetic multilingual data generation incorporating culturally and\nlinguistically nuanced variants, (2) supervised fine-tuning, and (3) a\ncurriculum-based Group Relative Policy Optimization (GRPO) framework that\nfurther improves performance. Experimental results demonstrate that our\nmultilingual guardrail, MrGuard, consistently outperforms recent baselines\nacross both in-domain and out-of-domain languages by more than 15%. We also\nevaluate MrGuard's robustness to multilingual variations, such as\ncode-switching and low-resource language distractors in the prompt, and\ndemonstrate that it preserves safety judgments under these challenging\nconditions. The multilingual reasoning capability of our guardrail enables it\nto generate explanations, which are particularly useful for understanding\nlanguage-specific risks and ambiguities in multilingual content moderation."
                },
                "authors": [
                    {
                        "name": "Yahan Yang"
                    },
                    {
                        "name": "Soham Dan"
                    },
                    {
                        "name": "Shuo Li"
                    },
                    {
                        "name": "Dan Roth"
                    },
                    {
                        "name": "Insup Lee"
                    }
                ],
                "author_detail": {
                    "name": "Insup Lee"
                },
                "author": "Insup Lee",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15241v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15241v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14597v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14597v1",
                "updated": "2025-05-20T16:48:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    16,
                    48,
                    57,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T16:48:57Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    16,
                    48,
                    57,
                    1,
                    140,
                    0
                ],
                "title": "Success is in the Details: Evaluate and Enhance Details Sensitivity of\n  Code LLMs through Counterfactuals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Success is in the Details: Evaluate and Enhance Details Sensitivity of\n  Code LLMs through Counterfactuals"
                },
                "summary": "Code Sensitivity refers to the ability of Code LLMs to recognize and respond\nto details changes in problem descriptions. While current code benchmarks and\ninstruction data focus on difficulty and diversity, sensitivity is overlooked.\nWe first introduce the CTF-Code benchmark, constructed using counterfactual\nperturbations, minimizing input changes while maximizing output changes. The\nevaluation shows that many LLMs have a more than 10\\% performance drop compared\nto the original problems. To fully utilize sensitivity, CTF-Instruct, an\nincremental instruction fine-tuning framework, extends on existing data and\nuses a selection mechanism to meet the three dimensions of difficulty,\ndiversity, and sensitivity. Experiments show that LLMs fine-tuned with\nCTF-Instruct data achieve over a 2\\% improvement on CTF-Code, and more than a\n10\\% performance boost on LiveCodeBench, validating the feasibility of\nenhancing LLMs' sensitivity to improve performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code Sensitivity refers to the ability of Code LLMs to recognize and respond\nto details changes in problem descriptions. While current code benchmarks and\ninstruction data focus on difficulty and diversity, sensitivity is overlooked.\nWe first introduce the CTF-Code benchmark, constructed using counterfactual\nperturbations, minimizing input changes while maximizing output changes. The\nevaluation shows that many LLMs have a more than 10\\% performance drop compared\nto the original problems. To fully utilize sensitivity, CTF-Instruct, an\nincremental instruction fine-tuning framework, extends on existing data and\nuses a selection mechanism to meet the three dimensions of difficulty,\ndiversity, and sensitivity. Experiments show that LLMs fine-tuned with\nCTF-Instruct data achieve over a 2\\% improvement on CTF-Code, and more than a\n10\\% performance boost on LiveCodeBench, validating the feasibility of\nenhancing LLMs' sensitivity to improve performance."
                },
                "authors": [
                    {
                        "name": "Xianzhen Luo"
                    },
                    {
                        "name": "Qingfu Zhu"
                    },
                    {
                        "name": "Zhiming Zhang"
                    },
                    {
                        "name": "Mingzheng Xu"
                    },
                    {
                        "name": "Tianhao Cheng"
                    },
                    {
                        "name": "Yixuan Wang"
                    },
                    {
                        "name": "Zheng Chu"
                    },
                    {
                        "name": "Shijie Xuyang"
                    },
                    {
                        "name": "Zhiyuan Ma"
                    },
                    {
                        "name": "YuanTao Fan"
                    },
                    {
                        "name": "Wanxiang Che"
                    }
                ],
                "author_detail": {
                    "name": "Wanxiang Che"
                },
                "author": "Wanxiang Che",
                "arxiv_comment": "Code & Model is https://github.com/Luowaterbi/CTF-Instruct",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14597v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14597v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16901v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16901v2",
                "updated": "2025-05-20T16:45:00Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    16,
                    45,
                    0,
                    1,
                    140,
                    0
                ],
                "published": "2025-02-24T06:54:50Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    6,
                    54,
                    50,
                    0,
                    55,
                    0
                ],
                "title": "Char-mander Use mBackdoor! A Study of Cross-lingual Backdoor Attacks in\n  Multilingual LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Char-mander Use mBackdoor! A Study of Cross-lingual Backdoor Attacks in\n  Multilingual LLMs"
                },
                "summary": "We explore \\textbf{C}ross-lingual \\textbf{B}ackdoor \\textbf{AT}tacks (X-BAT)\nin multilingual Large Language Models (mLLMs), revealing how backdoors inserted\nin one language can automatically transfer to others through shared embedding\nspaces. Using toxicity classification as a case study, we demonstrate that\nattackers can compromise multilingual systems by poisoning data in a single\nlanguage, with rare and high-occurring tokens serving as specific, effective\ntriggers. Our findings expose a critical vulnerability that influences the\nmodel's architecture, resulting in a concealed backdoor effect during the\ninformation flow. Our code and data are publicly available\nhttps://github.com/himanshubeniwal/X-BAT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We explore \\textbf{C}ross-lingual \\textbf{B}ackdoor \\textbf{AT}tacks (X-BAT)\nin multilingual Large Language Models (mLLMs), revealing how backdoors inserted\nin one language can automatically transfer to others through shared embedding\nspaces. Using toxicity classification as a case study, we demonstrate that\nattackers can compromise multilingual systems by poisoning data in a single\nlanguage, with rare and high-occurring tokens serving as specific, effective\ntriggers. Our findings expose a critical vulnerability that influences the\nmodel's architecture, resulting in a concealed backdoor effect during the\ninformation flow. Our code and data are publicly available\nhttps://github.com/himanshubeniwal/X-BAT."
                },
                "authors": [
                    {
                        "name": "Himanshu Beniwal"
                    },
                    {
                        "name": "Sailesh Panda"
                    },
                    {
                        "name": "Birudugadda Srivibhav"
                    },
                    {
                        "name": "Mayank Singh"
                    }
                ],
                "author_detail": {
                    "name": "Mayank Singh"
                },
                "author": "Mayank Singh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16901v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16901v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14590v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14590v1",
                "updated": "2025-05-20T16:41:45Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    16,
                    41,
                    45,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T16:41:45Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    16,
                    41,
                    45,
                    1,
                    140,
                    0
                ],
                "title": "MCIP: Protecting MCP Safety via Model Contextual Integrity Protocol",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MCIP: Protecting MCP Safety via Model Contextual Integrity Protocol"
                },
                "summary": "As Model Context Protocol (MCP) introduces an easy-to-use ecosystem for users\nand developers, it also brings underexplored safety risks. Its decentralized\narchitecture, which separates clients and servers, poses unique challenges for\nsystematic safety analysis. This paper proposes a novel framework to enhance\nMCP safety. Guided by the MAESTRO framework, we first analyze the missing\nsafety mechanisms in MCP, and based on this analysis, we propose the Model\nContextual Integrity Protocol (MCIP), a refined version of MCP that addresses\nthese gaps.Next, we develop a fine-grained taxonomy that captures a diverse\nrange of unsafe behaviors observed in MCP scenarios. Building on this taxonomy,\nwe develop benchmark and training data that support the evaluation and\nimprovement of LLMs' capabilities in identifying safety risks within MCP\ninteractions. Leveraging the proposed benchmark and training data, we conduct\nextensive experiments on state-of-the-art LLMs. The results highlight LLMs'\nvulnerabilities in MCP interactions and demonstrate that our approach\nsubstantially improves their safety performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Model Context Protocol (MCP) introduces an easy-to-use ecosystem for users\nand developers, it also brings underexplored safety risks. Its decentralized\narchitecture, which separates clients and servers, poses unique challenges for\nsystematic safety analysis. This paper proposes a novel framework to enhance\nMCP safety. Guided by the MAESTRO framework, we first analyze the missing\nsafety mechanisms in MCP, and based on this analysis, we propose the Model\nContextual Integrity Protocol (MCIP), a refined version of MCP that addresses\nthese gaps.Next, we develop a fine-grained taxonomy that captures a diverse\nrange of unsafe behaviors observed in MCP scenarios. Building on this taxonomy,\nwe develop benchmark and training data that support the evaluation and\nimprovement of LLMs' capabilities in identifying safety risks within MCP\ninteractions. Leveraging the proposed benchmark and training data, we conduct\nextensive experiments on state-of-the-art LLMs. The results highlight LLMs'\nvulnerabilities in MCP interactions and demonstrate that our approach\nsubstantially improves their safety performance."
                },
                "authors": [
                    {
                        "name": "Huihao Jing"
                    },
                    {
                        "name": "Haoran Li"
                    },
                    {
                        "name": "Wenbin Hu"
                    },
                    {
                        "name": "Qi Hu"
                    },
                    {
                        "name": "Heli Xu"
                    },
                    {
                        "name": "Tianshu Chu"
                    },
                    {
                        "name": "Peizhao Hu"
                    },
                    {
                        "name": "Yangqiu Song"
                    }
                ],
                "author_detail": {
                    "name": "Yangqiu Song"
                },
                "author": "Yangqiu Song",
                "arxiv_comment": "17 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14590v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14590v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14585v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14585v1",
                "updated": "2025-05-20T16:40:09Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    16,
                    40,
                    9,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T16:40:09Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    16,
                    40,
                    9,
                    1,
                    140,
                    0
                ],
                "title": "Context Reasoner: Incentivizing Reasoning Capability for Contextualized\n  Privacy and Safety Compliance via Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context Reasoner: Incentivizing Reasoning Capability for Contextualized\n  Privacy and Safety Compliance via Reinforcement Learning"
                },
                "summary": "While Large Language Models (LLMs) exhibit remarkable capabilities, they also\nintroduce significant safety and privacy risks. Current mitigation strategies\noften fail to preserve contextual reasoning capabilities in risky scenarios.\nInstead, they rely heavily on sensitive pattern matching to protect LLMs, which\nlimits the scope. Furthermore, they overlook established safety and privacy\nstandards, leading to systemic risks for legal compliance. To address these\ngaps, we formulate safety and privacy issues into contextualized compliance\nproblems following the Contextual Integrity (CI) theory. Under the CI\nframework, we align our model with three critical regulatory standards: GDPR,\nEU AI Act, and HIPAA. Specifically, we employ reinforcement learning (RL) with\na rule-based reward to incentivize contextual reasoning capabilities while\nenhancing compliance with safety and privacy norms. Through extensive\nexperiments, we demonstrate that our method not only significantly enhances\nlegal compliance (achieving a +17.64% accuracy improvement in safety/privacy\nbenchmarks) but also further improves general reasoning capability. For\nOpenThinker-7B, a strong reasoning model that significantly outperforms its\nbase model Qwen2.5-7B-Instruct across diverse subjects, our method enhances its\ngeneral reasoning capabilities, with +2.05% and +8.98% accuracy improvement on\nthe MMLU and LegalBench benchmark, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Large Language Models (LLMs) exhibit remarkable capabilities, they also\nintroduce significant safety and privacy risks. Current mitigation strategies\noften fail to preserve contextual reasoning capabilities in risky scenarios.\nInstead, they rely heavily on sensitive pattern matching to protect LLMs, which\nlimits the scope. Furthermore, they overlook established safety and privacy\nstandards, leading to systemic risks for legal compliance. To address these\ngaps, we formulate safety and privacy issues into contextualized compliance\nproblems following the Contextual Integrity (CI) theory. Under the CI\nframework, we align our model with three critical regulatory standards: GDPR,\nEU AI Act, and HIPAA. Specifically, we employ reinforcement learning (RL) with\na rule-based reward to incentivize contextual reasoning capabilities while\nenhancing compliance with safety and privacy norms. Through extensive\nexperiments, we demonstrate that our method not only significantly enhances\nlegal compliance (achieving a +17.64% accuracy improvement in safety/privacy\nbenchmarks) but also further improves general reasoning capability. For\nOpenThinker-7B, a strong reasoning model that significantly outperforms its\nbase model Qwen2.5-7B-Instruct across diverse subjects, our method enhances its\ngeneral reasoning capabilities, with +2.05% and +8.98% accuracy improvement on\nthe MMLU and LegalBench benchmark, respectively."
                },
                "authors": [
                    {
                        "name": "Wenbin Hu"
                    },
                    {
                        "name": "Haoran Li"
                    },
                    {
                        "name": "Huihao Jing"
                    },
                    {
                        "name": "Qi Hu"
                    },
                    {
                        "name": "Ziqian Zeng"
                    },
                    {
                        "name": "Sirui Han"
                    },
                    {
                        "name": "Heli Xu"
                    },
                    {
                        "name": "Tianshu Chu"
                    },
                    {
                        "name": "Peizhao Hu"
                    },
                    {
                        "name": "Yangqiu Song"
                    }
                ],
                "author_detail": {
                    "name": "Yangqiu Song"
                },
                "author": "Yangqiu Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14585v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14585v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14582v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14582v1",
                "updated": "2025-05-20T16:38:32Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    16,
                    38,
                    32,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T16:38:32Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    16,
                    38,
                    32,
                    1,
                    140,
                    0
                ],
                "title": "Can Pruning Improve Reasoning? Revisiting Long-CoT Compression with\n  Capability in Mind for Better Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Pruning Improve Reasoning? Revisiting Long-CoT Compression with\n  Capability in Mind for Better Reasoning"
                },
                "summary": "Long chain-of-thought (Long-CoT) reasoning improves accuracy in LLMs, yet its\nverbose, self-reflective style often hinders effective distillation into small\nlanguage models (SLMs). We revisit Long-CoT compression through the lens of\ncapability alignment and ask: Can pruning improve reasoning? We propose\nPrune-on-Logic, a structure-aware framework that transforms Long-CoT into logic\ngraphs and selectively prunes low-utility reasoning steps under\nself-verification constraints. Through systematic analysis across three pruning\nstrategies -- targeting entire chains, core reasoning, and verification -- we\nfind that pruning verification steps yields consistent accuracy gains while\nreducing inference cost, outperforming token-level baselines and uncompressed\nfine-tuning. In contrast, pruning reasoning or all-chain steps degrades\nperformance, revealing that small models benefit not from shorter CoTs, but\nfrom semantically leaner ones. Our findings highlight pruning as a structural\noptimization strategy for aligning CoT reasoning with SLM capacity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long chain-of-thought (Long-CoT) reasoning improves accuracy in LLMs, yet its\nverbose, self-reflective style often hinders effective distillation into small\nlanguage models (SLMs). We revisit Long-CoT compression through the lens of\ncapability alignment and ask: Can pruning improve reasoning? We propose\nPrune-on-Logic, a structure-aware framework that transforms Long-CoT into logic\ngraphs and selectively prunes low-utility reasoning steps under\nself-verification constraints. Through systematic analysis across three pruning\nstrategies -- targeting entire chains, core reasoning, and verification -- we\nfind that pruning verification steps yields consistent accuracy gains while\nreducing inference cost, outperforming token-level baselines and uncompressed\nfine-tuning. In contrast, pruning reasoning or all-chain steps degrades\nperformance, revealing that small models benefit not from shorter CoTs, but\nfrom semantically leaner ones. Our findings highlight pruning as a structural\noptimization strategy for aligning CoT reasoning with SLM capacity."
                },
                "authors": [
                    {
                        "name": "Shangziqi Zhao"
                    },
                    {
                        "name": "Jiahao Yuan"
                    },
                    {
                        "name": "Guisong Yang"
                    },
                    {
                        "name": "Usman Naseem"
                    }
                ],
                "author_detail": {
                    "name": "Usman Naseem"
                },
                "author": "Usman Naseem",
                "arxiv_comment": "17 pages,4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14582v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14582v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14577v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14577v1",
                "updated": "2025-05-20T16:34:37Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    16,
                    34,
                    37,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T16:34:37Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    16,
                    34,
                    37,
                    1,
                    140,
                    0
                ],
                "title": "TRATES: Trait-Specific Rubric-Assisted Cross-Prompt Essay Scoring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TRATES: Trait-Specific Rubric-Assisted Cross-Prompt Essay Scoring"
                },
                "summary": "Research on holistic Automated Essay Scoring (AES) is long-dated; yet, there\nis a notable lack of attention for assessing essays according to individual\ntraits. In this work, we propose TRATES, a novel trait-specific and\nrubric-based cross-prompt AES framework that is generic yet specific to the\nunderlying trait. The framework leverages a Large Language Model (LLM) that\nutilizes the trait grading rubrics to generate trait-specific features\n(represented by assessment questions), then assesses those features given an\nessay. The trait-specific features are eventually combined with generic\nwriting-quality and prompt-specific features to train a simple classical\nregression model that predicts trait scores of essays from an unseen prompt.\nExperiments show that TRATES achieves a new state-of-the-art performance across\nall traits on a widely-used dataset, with the generated LLM-based features\nbeing the most significant.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Research on holistic Automated Essay Scoring (AES) is long-dated; yet, there\nis a notable lack of attention for assessing essays according to individual\ntraits. In this work, we propose TRATES, a novel trait-specific and\nrubric-based cross-prompt AES framework that is generic yet specific to the\nunderlying trait. The framework leverages a Large Language Model (LLM) that\nutilizes the trait grading rubrics to generate trait-specific features\n(represented by assessment questions), then assesses those features given an\nessay. The trait-specific features are eventually combined with generic\nwriting-quality and prompt-specific features to train a simple classical\nregression model that predicts trait scores of essays from an unseen prompt.\nExperiments show that TRATES achieves a new state-of-the-art performance across\nall traits on a widely-used dataset, with the generated LLM-based features\nbeing the most significant."
                },
                "authors": [
                    {
                        "name": "Sohaila Eltanbouly"
                    },
                    {
                        "name": "Salam Albatarni"
                    },
                    {
                        "name": "Tamer Elsayed"
                    }
                ],
                "author_detail": {
                    "name": "Tamer Elsayed"
                },
                "author": "Tamer Elsayed",
                "arxiv_comment": "Accepted at ACL 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14577v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14577v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07115v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07115v4",
                "updated": "2025-05-20T16:29:52Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    16,
                    29,
                    52,
                    1,
                    140,
                    0
                ],
                "published": "2025-02-10T23:11:44Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    23,
                    11,
                    44,
                    0,
                    41,
                    0
                ],
                "title": "Online Scheduling for LLM Inference with KV Cache Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Scheduling for LLM Inference with KV Cache Constraints"
                },
                "summary": "Large Language Model (LLM) inference, where a trained model generates text\none word at a time in response to user prompts, is a computationally intensive\nprocess requiring efficient scheduling to optimize latency and resource\nutilization. A key challenge in LLM inference is the management of the\nKey-Value (KV) cache, which reduces redundant computations but introduces\nmemory constraints. In this work, we model LLM inference with KV cache\nconstraints theoretically and propose a novel batching and scheduling algorithm\nthat minimizes inference latency while effectively managing the KV cache's\nmemory.\n  More specifically, we make the following contributions. First, to evaluate\nthe performance of online algorithms for scheduling in LLM inference, we\nintroduce a hindsight optimal benchmark, formulated as an integer program that\ncomputes the minimum total inference latency under full future information.\nSecond, we prove that no deterministic online algorithm can achieve a constant\ncompetitive ratio when the arrival process is arbitrary. Third, motivated by\nthe computational intractability of solving the integer program at scale, we\npropose a polynomial-time online scheduling algorithm and show that under\ncertain conditions it can achieve a constant competitive ratio. We also\ndemonstrate our algorithm's strong empirical performance by comparing it to the\nhindsight optimal in a synthetic dataset. Finally, we conduct empirical\nevaluations on a real-world public LLM inference dataset, simulating the\nLlama2-70B model on A100 GPUs, and show that our algorithm significantly\noutperforms the benchmark algorithms. Overall, our results offer a path toward\nmore sustainable and cost-effective LLM deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference, where a trained model generates text\none word at a time in response to user prompts, is a computationally intensive\nprocess requiring efficient scheduling to optimize latency and resource\nutilization. A key challenge in LLM inference is the management of the\nKey-Value (KV) cache, which reduces redundant computations but introduces\nmemory constraints. In this work, we model LLM inference with KV cache\nconstraints theoretically and propose a novel batching and scheduling algorithm\nthat minimizes inference latency while effectively managing the KV cache's\nmemory.\n  More specifically, we make the following contributions. First, to evaluate\nthe performance of online algorithms for scheduling in LLM inference, we\nintroduce a hindsight optimal benchmark, formulated as an integer program that\ncomputes the minimum total inference latency under full future information.\nSecond, we prove that no deterministic online algorithm can achieve a constant\ncompetitive ratio when the arrival process is arbitrary. Third, motivated by\nthe computational intractability of solving the integer program at scale, we\npropose a polynomial-time online scheduling algorithm and show that under\ncertain conditions it can achieve a constant competitive ratio. We also\ndemonstrate our algorithm's strong empirical performance by comparing it to the\nhindsight optimal in a synthetic dataset. Finally, we conduct empirical\nevaluations on a real-world public LLM inference dataset, simulating the\nLlama2-70B model on A100 GPUs, and show that our algorithm significantly\noutperforms the benchmark algorithms. Overall, our results offer a path toward\nmore sustainable and cost-effective LLM deployment."
                },
                "authors": [
                    {
                        "name": "Patrick Jaillet"
                    },
                    {
                        "name": "Jiashuo Jiang"
                    },
                    {
                        "name": "Konstantina Mellou"
                    },
                    {
                        "name": "Marco Molinaro"
                    },
                    {
                        "name": "Chara Podimata"
                    },
                    {
                        "name": "Zijie Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Zijie Zhou"
                },
                "author": "Zijie Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07115v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07115v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21272v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21272v2",
                "updated": "2025-05-20T16:29:40Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    16,
                    29,
                    40,
                    1,
                    140,
                    0
                ],
                "published": "2024-10-28T17:59:06Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    17,
                    59,
                    6,
                    0,
                    302,
                    0
                ],
                "title": "Arithmetic Without Algorithms: Language Models Solve Math With a Bag of\n  Heuristics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Arithmetic Without Algorithms: Language Models Solve Math With a Bag of\n  Heuristics"
                },
                "summary": "Do large language models (LLMs) solve reasoning tasks by learning robust\ngeneralizable algorithms, or do they memorize training data? To investigate\nthis question, we use arithmetic reasoning as a representative task. Using\ncausal analysis, we identify a subset of the model (a circuit) that explains\nmost of the model's behavior for basic arithmetic logic and examine its\nfunctionality. By zooming in on the level of individual circuit neurons, we\ndiscover a sparse set of important neurons that implement simple heuristics.\nEach heuristic identifies a numerical input pattern and outputs corresponding\nanswers. We hypothesize that the combination of these heuristic neurons is the\nmechanism used to produce correct arithmetic answers. To test this, we\ncategorize each neuron into several heuristic types-such as neurons that\nactivate when an operand falls within a certain range-and find that the\nunordered combination of these heuristic types is the mechanism that explains\nmost of the model's accuracy on arithmetic prompts. Finally, we demonstrate\nthat this mechanism appears as the main source of arithmetic accuracy early in\ntraining. Overall, our experimental results across several LLMs show that LLMs\nperform arithmetic using neither robust algorithms nor memorization; rather,\nthey rely on a \"bag of heuristics\".",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do large language models (LLMs) solve reasoning tasks by learning robust\ngeneralizable algorithms, or do they memorize training data? To investigate\nthis question, we use arithmetic reasoning as a representative task. Using\ncausal analysis, we identify a subset of the model (a circuit) that explains\nmost of the model's behavior for basic arithmetic logic and examine its\nfunctionality. By zooming in on the level of individual circuit neurons, we\ndiscover a sparse set of important neurons that implement simple heuristics.\nEach heuristic identifies a numerical input pattern and outputs corresponding\nanswers. We hypothesize that the combination of these heuristic neurons is the\nmechanism used to produce correct arithmetic answers. To test this, we\ncategorize each neuron into several heuristic types-such as neurons that\nactivate when an operand falls within a certain range-and find that the\nunordered combination of these heuristic types is the mechanism that explains\nmost of the model's accuracy on arithmetic prompts. Finally, we demonstrate\nthat this mechanism appears as the main source of arithmetic accuracy early in\ntraining. Overall, our experimental results across several LLMs show that LLMs\nperform arithmetic using neither robust algorithms nor memorization; rather,\nthey rely on a \"bag of heuristics\"."
                },
                "authors": [
                    {
                        "name": "Yaniv Nikankin"
                    },
                    {
                        "name": "Anja Reusch"
                    },
                    {
                        "name": "Aaron Mueller"
                    },
                    {
                        "name": "Yonatan Belinkov"
                    }
                ],
                "author_detail": {
                    "name": "Yonatan Belinkov"
                },
                "author": "Yonatan Belinkov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21272v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21272v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10940v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10940v2",
                "updated": "2025-05-20T16:27:05Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    16,
                    27,
                    5,
                    1,
                    140,
                    0
                ],
                "published": "2025-02-16T01:05:16Z",
                "published_parsed": [
                    2025,
                    2,
                    16,
                    1,
                    5,
                    16,
                    6,
                    47,
                    0
                ],
                "title": "CoLA: Compute-Efficient Pre-Training of LLMs via Low-Rank Activation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoLA: Compute-Efficient Pre-Training of LLMs via Low-Rank Activation"
                },
                "summary": "The full-size MLPs and the projection layers in attention introduce\ntremendous model sizes of large language models (LLMs), imposing extremely\ndemanding needs of computational resources in the pre-training stage. However,\nwe empirically observe that the activations of pre-trained LLMs exhibit\nlow-rank property. Motivated by such observations, we propose CoLA and its\nmemory-efficient implementation, CoLA-M, to replace these full-size layers with\ncompute-efficient auto-encoders that naturally enforce low-rank activations\nthroughout training. This fundamental architectural change eliminates the\nactivation redundancy and significantly boosts model capacity and training\nefficiency. Experiments on LLaMA models with 60 million to 7 billion parameters\nshow that CoLA reduces the computing cost by $\\bf 2\\pmb{\\times}$ and improves\ntraining throughput by $\\bf 1.86\\pmb{\\times}$ while maintaining full-rank level\nperformance. CoLA-M further squeezes memory cost without sacrificing\nthroughput, offering a pre-training approach with collectively superior\nparameter, computing, and memory efficiency. The LLMs produced are also $\\bf\n2\\pmb{\\times}$ smaller, enabling faster inference with lower memory cost on\nresource-constrained platforms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The full-size MLPs and the projection layers in attention introduce\ntremendous model sizes of large language models (LLMs), imposing extremely\ndemanding needs of computational resources in the pre-training stage. However,\nwe empirically observe that the activations of pre-trained LLMs exhibit\nlow-rank property. Motivated by such observations, we propose CoLA and its\nmemory-efficient implementation, CoLA-M, to replace these full-size layers with\ncompute-efficient auto-encoders that naturally enforce low-rank activations\nthroughout training. This fundamental architectural change eliminates the\nactivation redundancy and significantly boosts model capacity and training\nefficiency. Experiments on LLaMA models with 60 million to 7 billion parameters\nshow that CoLA reduces the computing cost by $\\bf 2\\pmb{\\times}$ and improves\ntraining throughput by $\\bf 1.86\\pmb{\\times}$ while maintaining full-rank level\nperformance. CoLA-M further squeezes memory cost without sacrificing\nthroughput, offering a pre-training approach with collectively superior\nparameter, computing, and memory efficiency. The LLMs produced are also $\\bf\n2\\pmb{\\times}$ smaller, enabling faster inference with lower memory cost on\nresource-constrained platforms."
                },
                "authors": [
                    {
                        "name": "Ziyue Liu"
                    },
                    {
                        "name": "Ruijie Zhang"
                    },
                    {
                        "name": "Zhengyang Wang"
                    },
                    {
                        "name": "Zi Yang"
                    },
                    {
                        "name": "Paul Hovland"
                    },
                    {
                        "name": "Bogdan Nicolae"
                    },
                    {
                        "name": "Franck Cappello"
                    },
                    {
                        "name": "Zheng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Zhang"
                },
                "author": "Zheng Zhang",
                "arxiv_comment": "v2",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10940v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10940v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17388v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17388v3",
                "updated": "2025-05-20T16:24:22Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    16,
                    24,
                    22,
                    1,
                    140,
                    0
                ],
                "published": "2024-11-26T12:46:57Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    12,
                    46,
                    57,
                    1,
                    331,
                    0
                ],
                "title": "Can LLMs be Good Graph Judge for Knowledge Graph Construction?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs be Good Graph Judge for Knowledge Graph Construction?"
                },
                "summary": "In real-world scenarios, most of the data obtained from the information\nretrieval (IR) system is unstructured. Converting natural language sentences\ninto structured Knowledge Graphs (KGs) remains a critical challenge. We\nidentified three limitations with respect to existing KG construction methods:\n(1) There could be a large amount of noise in real-world documents, which could\nresult in extracting messy information. (2) Naive LLMs usually extract\ninaccurate knowledge from some domain-specific documents. (3) Hallucination\nphenomenon cannot be overlooked when directly using LLMs to construct KGs. In\nthis paper, we propose \\textbf{GraphJudge}, a KG construction framework to\naddress the aforementioned challenges. In this framework, we designed an\nentity-centric strategy to eliminate the noise information in the documents.\nAnd we fine-tuned a LLM as a graph judge to finally enhance the quality of\ngenerated KGs. Experiments conducted on two general and one domain-specific\ntext-graph pair datasets demonstrate state-of-the-art performance against\nvarious baseline methods with strong generalization abilities. Our code is\navailable at\n\\href{https://github.com/hhy-huang/GraphJudge}{https://github.com/hhy-huang/GraphJudge}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In real-world scenarios, most of the data obtained from the information\nretrieval (IR) system is unstructured. Converting natural language sentences\ninto structured Knowledge Graphs (KGs) remains a critical challenge. We\nidentified three limitations with respect to existing KG construction methods:\n(1) There could be a large amount of noise in real-world documents, which could\nresult in extracting messy information. (2) Naive LLMs usually extract\ninaccurate knowledge from some domain-specific documents. (3) Hallucination\nphenomenon cannot be overlooked when directly using LLMs to construct KGs. In\nthis paper, we propose \\textbf{GraphJudge}, a KG construction framework to\naddress the aforementioned challenges. In this framework, we designed an\nentity-centric strategy to eliminate the noise information in the documents.\nAnd we fine-tuned a LLM as a graph judge to finally enhance the quality of\ngenerated KGs. Experiments conducted on two general and one domain-specific\ntext-graph pair datasets demonstrate state-of-the-art performance against\nvarious baseline methods with strong generalization abilities. Our code is\navailable at\n\\href{https://github.com/hhy-huang/GraphJudge}{https://github.com/hhy-huang/GraphJudge}."
                },
                "authors": [
                    {
                        "name": "Haoyu Huang"
                    },
                    {
                        "name": "Chong Chen"
                    },
                    {
                        "name": "Zeang Sheng"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Wentao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wentao Zhang"
                },
                "author": "Wentao Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17388v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17388v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12466v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12466v2",
                "updated": "2025-05-20T16:19:18Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    16,
                    19,
                    18,
                    1,
                    140,
                    0
                ],
                "published": "2025-02-18T02:54:25Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    2,
                    54,
                    25,
                    1,
                    49,
                    0
                ],
                "title": "EquiBench: Benchmarking Large Language Models' Understanding of Program\n  Semantics via Equivalence Checking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EquiBench: Benchmarking Large Language Models' Understanding of Program\n  Semantics via Equivalence Checking"
                },
                "summary": "As large language models (LLMs) become integral to code-related tasks, a\ncentral question emerges: do LLMs truly understand program execution semantics?\nWe introduce EquiBench, a new benchmark for evaluating LLMs through equivalence\nchecking, i.e., determining whether two programs produce identical outputs for\nall possible inputs. Unlike prior code generation benchmarks, this task\ndirectly tests a model's understanding of code execution semantics. EquiBench\nconsists of 2400 program pairs across four languages and six categories. These\npairs are generated through program analysis, compiler scheduling, and\nsuperoptimization, ensuring high-confidence labels, nontrivial difficulty, and\nfull automation. The transformations span syntactic edits, structural\nmodifications, and algorithmic changes, covering a broad spectrum of semantic\nvariation. We evaluate 19 state-of-the-art LLMs and find that in the most\nchallenging categories, the best accuracies are 63.8% and 76.2%, only modestly\nabove the 50% random baseline. Further analysis reveals that models often rely\non syntactic similarity rather than exhibiting robust reasoning over execution\nsemantics, highlighting fundamental limitations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) become integral to code-related tasks, a\ncentral question emerges: do LLMs truly understand program execution semantics?\nWe introduce EquiBench, a new benchmark for evaluating LLMs through equivalence\nchecking, i.e., determining whether two programs produce identical outputs for\nall possible inputs. Unlike prior code generation benchmarks, this task\ndirectly tests a model's understanding of code execution semantics. EquiBench\nconsists of 2400 program pairs across four languages and six categories. These\npairs are generated through program analysis, compiler scheduling, and\nsuperoptimization, ensuring high-confidence labels, nontrivial difficulty, and\nfull automation. The transformations span syntactic edits, structural\nmodifications, and algorithmic changes, covering a broad spectrum of semantic\nvariation. We evaluate 19 state-of-the-art LLMs and find that in the most\nchallenging categories, the best accuracies are 63.8% and 76.2%, only modestly\nabove the 50% random baseline. Further analysis reveals that models often rely\non syntactic similarity rather than exhibiting robust reasoning over execution\nsemantics, highlighting fundamental limitations."
                },
                "authors": [
                    {
                        "name": "Anjiang Wei"
                    },
                    {
                        "name": "Jiannan Cao"
                    },
                    {
                        "name": "Ran Li"
                    },
                    {
                        "name": "Hongyu Chen"
                    },
                    {
                        "name": "Yuhui Zhang"
                    },
                    {
                        "name": "Ziheng Wang"
                    },
                    {
                        "name": "Yuan Liu"
                    },
                    {
                        "name": "Thiago S. F. X. Teixeira"
                    },
                    {
                        "name": "Diyi Yang"
                    },
                    {
                        "name": "Ke Wang"
                    },
                    {
                        "name": "Alex Aiken"
                    }
                ],
                "author_detail": {
                    "name": "Alex Aiken"
                },
                "author": "Alex Aiken",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12466v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12466v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18169v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18169v4",
                "updated": "2025-05-20T16:15:13Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    16,
                    15,
                    13,
                    1,
                    140,
                    0
                ],
                "published": "2024-12-24T05:07:46Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    5,
                    7,
                    46,
                    1,
                    359,
                    0
                ],
                "title": "KunServe: Efficient Parameter-centric Memory Management for LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KunServe: Efficient Parameter-centric Memory Management for LLM Serving"
                },
                "summary": "Serving LLMs with a cluster of GPUs is common nowadays, where the serving\nsystem must meet strict latency SLOs required by applications. However, the\nstateful nature of LLM serving requires maintaining huge states (i.e., KVCache)\nin limited GPU memory. Under spikes in real-world workloads, GPU memory can be\neasily throttled, leading to orders of magnitude higher response latency due to\nqueuing introduced by waiting for KVCache to be reclaimed. Prior\nKVCache-centric approaches handle load throttling by dropping, migrating, or\nswapping KVCache. These methods fail to release sufficient memory quickly with\nrequests still queued.\n  This paper proposes the first parameter-centric approach to handling\nthrottling by selectively dropping replicated parameters to instantly free\nmemory for requests, based on an unnoticed observation that model parameters\nare commonly replicated across GPUs for serving LLMs. With additional memory,\nall requests can be served with a larger batch without queuing. To make the\nparameter-centric approach correct and efficient, we cooperatively execute\nrequests on GPUs with a complete copy of parameters using pipeline parallelism,\nand derive an appropriate drop plan without unnecessary cooperation. We also\ndesign techniques to minimize the performance overhead due to pipeline\nparallelism with the execution patterns of requests under drop. Evaluations\nshow that {\\sys} reduces the tail TTFT of requests under throttling by up to\n72.2 times compared to the state-of-the-art systems including Llumnix, vLLM and\nInferCept.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving LLMs with a cluster of GPUs is common nowadays, where the serving\nsystem must meet strict latency SLOs required by applications. However, the\nstateful nature of LLM serving requires maintaining huge states (i.e., KVCache)\nin limited GPU memory. Under spikes in real-world workloads, GPU memory can be\neasily throttled, leading to orders of magnitude higher response latency due to\nqueuing introduced by waiting for KVCache to be reclaimed. Prior\nKVCache-centric approaches handle load throttling by dropping, migrating, or\nswapping KVCache. These methods fail to release sufficient memory quickly with\nrequests still queued.\n  This paper proposes the first parameter-centric approach to handling\nthrottling by selectively dropping replicated parameters to instantly free\nmemory for requests, based on an unnoticed observation that model parameters\nare commonly replicated across GPUs for serving LLMs. With additional memory,\nall requests can be served with a larger batch without queuing. To make the\nparameter-centric approach correct and efficient, we cooperatively execute\nrequests on GPUs with a complete copy of parameters using pipeline parallelism,\nand derive an appropriate drop plan without unnecessary cooperation. We also\ndesign techniques to minimize the performance overhead due to pipeline\nparallelism with the execution patterns of requests under drop. Evaluations\nshow that {\\sys} reduces the tail TTFT of requests under throttling by up to\n72.2 times compared to the state-of-the-art systems including Llumnix, vLLM and\nInferCept."
                },
                "authors": [
                    {
                        "name": "Rongxin Cheng"
                    },
                    {
                        "name": "Yuxin Lai"
                    },
                    {
                        "name": "Xingda Wei"
                    },
                    {
                        "name": "Rong Chen"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18169v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18169v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14552v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14552v2",
                "updated": "2025-05-21T07:43:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    7,
                    43,
                    57,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-20T16:06:32Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    16,
                    6,
                    32,
                    1,
                    140,
                    0
                ],
                "title": "KORGym: A Dynamic Game Platform for LLM Reasoning Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KORGym: A Dynamic Game Platform for LLM Reasoning Evaluation"
                },
                "summary": "Recent advancements in large language models (LLMs) underscore the need for\nmore comprehensive evaluation methods to accurately assess their reasoning\ncapabilities. Existing benchmarks are often domain-specific and thus cannot\nfully capture an LLM's general reasoning potential. To address this limitation,\nwe introduce the Knowledge Orthogonal Reasoning Gymnasium (KORGym), a dynamic\nevaluation platform inspired by KOR-Bench and Gymnasium. KORGym offers over\nfifty games in either textual or visual formats and supports interactive,\nmulti-turn assessments with reinforcement learning scenarios. Using KORGym, we\nconduct extensive experiments on 19 LLMs and 8 VLMs, revealing consistent\nreasoning patterns within model families and demonstrating the superior\nperformance of closed-source models. Further analysis examines the effects of\nmodality, reasoning strategies, reinforcement learning techniques, and response\nlength on model performance. We expect KORGym to become a valuable resource for\nadvancing LLM reasoning research and developing evaluation methodologies suited\nto complex, interactive environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) underscore the need for\nmore comprehensive evaluation methods to accurately assess their reasoning\ncapabilities. Existing benchmarks are often domain-specific and thus cannot\nfully capture an LLM's general reasoning potential. To address this limitation,\nwe introduce the Knowledge Orthogonal Reasoning Gymnasium (KORGym), a dynamic\nevaluation platform inspired by KOR-Bench and Gymnasium. KORGym offers over\nfifty games in either textual or visual formats and supports interactive,\nmulti-turn assessments with reinforcement learning scenarios. Using KORGym, we\nconduct extensive experiments on 19 LLMs and 8 VLMs, revealing consistent\nreasoning patterns within model families and demonstrating the superior\nperformance of closed-source models. Further analysis examines the effects of\nmodality, reasoning strategies, reinforcement learning techniques, and response\nlength on model performance. We expect KORGym to become a valuable resource for\nadvancing LLM reasoning research and developing evaluation methodologies suited\nto complex, interactive environments."
                },
                "authors": [
                    {
                        "name": "Jiajun Shi"
                    },
                    {
                        "name": "Jian Yang"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Xingyuan Bu"
                    },
                    {
                        "name": "Jiangjie Chen"
                    },
                    {
                        "name": "Junting Zhou"
                    },
                    {
                        "name": "Kaijing Ma"
                    },
                    {
                        "name": "Zhoufutu Wen"
                    },
                    {
                        "name": "Bingli Wang"
                    },
                    {
                        "name": "Yancheng He"
                    },
                    {
                        "name": "Liang Song"
                    },
                    {
                        "name": "Hualei Zhu"
                    },
                    {
                        "name": "Shilong Li"
                    },
                    {
                        "name": "Xingjian Wang"
                    },
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Ruibin Yuan"
                    },
                    {
                        "name": "Yifan Yao"
                    },
                    {
                        "name": "Wenjun Yang"
                    },
                    {
                        "name": "Yunli Wang"
                    },
                    {
                        "name": "Siyuan Fang"
                    },
                    {
                        "name": "Siyu Yuan"
                    },
                    {
                        "name": "Qianyu He"
                    },
                    {
                        "name": "Xiangru Tang"
                    },
                    {
                        "name": "Yingshui Tan"
                    },
                    {
                        "name": "Wangchunshu Zhou"
                    },
                    {
                        "name": "Zhaoxiang Zhang"
                    },
                    {
                        "name": "Zhoujun Li"
                    },
                    {
                        "name": "Wenhao Huang"
                    },
                    {
                        "name": "Ge Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ge Zhang"
                },
                "author": "Ge Zhang",
                "arxiv_comment": "22 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14552v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14552v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14549v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14549v1",
                "updated": "2025-05-20T16:05:05Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    16,
                    5,
                    5,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T16:05:05Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    16,
                    5,
                    5,
                    1,
                    140,
                    0
                ],
                "title": "Can Large Language Models Really Recognize Your Name?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Large Language Models Really Recognize Your Name?"
                },
                "summary": "Large language models (LLMs) are increasingly being used to protect sensitive\nuser data. However, current LLM-based privacy solutions assume that these\nmodels can reliably detect personally identifiable information (PII),\nparticularly named entities. In this paper, we challenge that assumption by\nrevealing systematic failures in LLM-based privacy tasks. Specifically, we show\nthat modern LLMs regularly overlook human names even in short text snippets due\nto ambiguous contexts, which cause the names to be misinterpreted or\nmishandled. We propose AMBENCH, a benchmark dataset of seemingly ambiguous\nhuman names, leveraging the name regularity bias phenomenon, embedded within\nconcise text snippets along with benign prompt injections. Our experiments on\nmodern LLMs tasked to detect PII as well as specialized tools show that recall\nof ambiguous names drops by 20--40% compared to more recognizable names.\nFurthermore, ambiguous human names are four times more likely to be ignored in\nsupposedly privacy-preserving summaries generated by LLMs when benign prompt\ninjections are present. These findings highlight the underexplored risks of\nrelying solely on LLMs to safeguard user privacy and underscore the need for a\nmore systematic investigation into their privacy failure modes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly being used to protect sensitive\nuser data. However, current LLM-based privacy solutions assume that these\nmodels can reliably detect personally identifiable information (PII),\nparticularly named entities. In this paper, we challenge that assumption by\nrevealing systematic failures in LLM-based privacy tasks. Specifically, we show\nthat modern LLMs regularly overlook human names even in short text snippets due\nto ambiguous contexts, which cause the names to be misinterpreted or\nmishandled. We propose AMBENCH, a benchmark dataset of seemingly ambiguous\nhuman names, leveraging the name regularity bias phenomenon, embedded within\nconcise text snippets along with benign prompt injections. Our experiments on\nmodern LLMs tasked to detect PII as well as specialized tools show that recall\nof ambiguous names drops by 20--40% compared to more recognizable names.\nFurthermore, ambiguous human names are four times more likely to be ignored in\nsupposedly privacy-preserving summaries generated by LLMs when benign prompt\ninjections are present. These findings highlight the underexplored risks of\nrelying solely on LLMs to safeguard user privacy and underscore the need for a\nmore systematic investigation into their privacy failure modes."
                },
                "authors": [
                    {
                        "name": "Dzung Pham"
                    },
                    {
                        "name": "Peter Kairouz"
                    },
                    {
                        "name": "Niloofar Mireshghallah"
                    },
                    {
                        "name": "Eugene Bagdasarian"
                    },
                    {
                        "name": "Chau Minh Pham"
                    },
                    {
                        "name": "Amir Houmansadr"
                    }
                ],
                "author_detail": {
                    "name": "Amir Houmansadr"
                },
                "author": "Amir Houmansadr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14549v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14549v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.11733v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.11733v2",
                "updated": "2025-05-20T15:56:52Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    15,
                    56,
                    52,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-16T22:34:36Z",
                "published_parsed": [
                    2025,
                    5,
                    16,
                    22,
                    34,
                    36,
                    4,
                    136,
                    0
                ],
                "title": "MedCaseReasoning: Evaluating and learning diagnostic reasoning from\n  clinical case reports",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MedCaseReasoning: Evaluating and learning diagnostic reasoning from\n  clinical case reports"
                },
                "summary": "Doctors and patients alike increasingly use Large Language Models (LLMs) to\ndiagnose clinical cases. However, unlike domains such as math or coding, where\ncorrectness can be objectively defined by the final answer, medical diagnosis\nrequires both the outcome and the reasoning process to be accurate. Currently,\nwidely used medical benchmarks like MedQA and MMLU assess only accuracy in the\nfinal answer, overlooking the quality and faithfulness of the clinical\nreasoning process. To address this limitation, we introduce MedCaseReasoning,\nthe first open-access dataset for evaluating LLMs on their ability to align\nwith clinician-authored diagnostic reasoning. The dataset includes 14,489\ndiagnostic question-and-answer cases, each paired with detailed reasoning\nstatements derived from open-access medical case reports. We evaluate\nstate-of-the-art reasoning LLMs on MedCaseReasoning and find significant\nshortcomings in their diagnoses and reasoning: for instance, the top-performing\nopen-source model, DeepSeek-R1, achieves only 48% 10-shot diagnostic accuracy\nand mentions only 64% of the clinician reasoning statements (recall). However,\nwe demonstrate that fine-tuning LLMs on the reasoning traces derived from\nMedCaseReasoning significantly improves diagnostic accuracy and clinical\nreasoning recall by an average relative gain of 29% and 41%, respectively. The\nopen-source dataset, code, and models are available at\nhttps://github.com/kevinwu23/Stanford-MedCaseReasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Doctors and patients alike increasingly use Large Language Models (LLMs) to\ndiagnose clinical cases. However, unlike domains such as math or coding, where\ncorrectness can be objectively defined by the final answer, medical diagnosis\nrequires both the outcome and the reasoning process to be accurate. Currently,\nwidely used medical benchmarks like MedQA and MMLU assess only accuracy in the\nfinal answer, overlooking the quality and faithfulness of the clinical\nreasoning process. To address this limitation, we introduce MedCaseReasoning,\nthe first open-access dataset for evaluating LLMs on their ability to align\nwith clinician-authored diagnostic reasoning. The dataset includes 14,489\ndiagnostic question-and-answer cases, each paired with detailed reasoning\nstatements derived from open-access medical case reports. We evaluate\nstate-of-the-art reasoning LLMs on MedCaseReasoning and find significant\nshortcomings in their diagnoses and reasoning: for instance, the top-performing\nopen-source model, DeepSeek-R1, achieves only 48% 10-shot diagnostic accuracy\nand mentions only 64% of the clinician reasoning statements (recall). However,\nwe demonstrate that fine-tuning LLMs on the reasoning traces derived from\nMedCaseReasoning significantly improves diagnostic accuracy and clinical\nreasoning recall by an average relative gain of 29% and 41%, respectively. The\nopen-source dataset, code, and models are available at\nhttps://github.com/kevinwu23/Stanford-MedCaseReasoning."
                },
                "authors": [
                    {
                        "name": "Kevin Wu"
                    },
                    {
                        "name": "Eric Wu"
                    },
                    {
                        "name": "Rahul Thapa"
                    },
                    {
                        "name": "Kevin Wei"
                    },
                    {
                        "name": "Angela Zhang"
                    },
                    {
                        "name": "Arvind Suresh"
                    },
                    {
                        "name": "Jacqueline J. Tao"
                    },
                    {
                        "name": "Min Woo Sun"
                    },
                    {
                        "name": "Alejandro Lozano"
                    },
                    {
                        "name": "James Zou"
                    }
                ],
                "author_detail": {
                    "name": "James Zou"
                },
                "author": "James Zou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.11733v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.11733v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14536v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14536v1",
                "updated": "2025-05-20T15:55:31Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    15,
                    55,
                    31,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T15:55:31Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    15,
                    55,
                    31,
                    1,
                    140,
                    0
                ],
                "title": "Breaking Bad Tokens: Detoxification of LLMs Using Sparse Autoencoders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking Bad Tokens: Detoxification of LLMs Using Sparse Autoencoders"
                },
                "summary": "Large language models (LLMs) are now ubiquitous in user-facing applications,\nyet they still generate undesirable toxic outputs, including profanity,\nvulgarity, and derogatory remarks. Although numerous detoxification methods\nexist, most apply broad, surface-level fixes and can therefore easily be\ncircumvented by jailbreak attacks. In this paper we leverage sparse\nautoencoders (SAEs) to identify toxicity-related directions in the residual\nstream of models and perform targeted activation steering using the\ncorresponding decoder vectors. We introduce three tiers of steering\naggressiveness and evaluate them on GPT-2 Small and Gemma-2-2B, revealing\ntrade-offs between toxicity reduction and language fluency. At stronger\nsteering strengths, these causal interventions surpass competitive baselines in\nreducing toxicity by up to 20%, though fluency can degrade noticeably on GPT-2\nSmall depending on the aggressiveness. Crucially, standard NLP benchmark scores\nupon steering remain stable, indicating that the model's knowledge and general\nabilities are preserved. We further show that feature-splitting in wider SAEs\nhampers safety interventions, underscoring the importance of disentangled\nfeature learning. Our findings highlight both the promise and the current\nlimitations of SAE-based causal interventions for LLM detoxification, further\nsuggesting practical guidelines for safer language-model deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are now ubiquitous in user-facing applications,\nyet they still generate undesirable toxic outputs, including profanity,\nvulgarity, and derogatory remarks. Although numerous detoxification methods\nexist, most apply broad, surface-level fixes and can therefore easily be\ncircumvented by jailbreak attacks. In this paper we leverage sparse\nautoencoders (SAEs) to identify toxicity-related directions in the residual\nstream of models and perform targeted activation steering using the\ncorresponding decoder vectors. We introduce three tiers of steering\naggressiveness and evaluate them on GPT-2 Small and Gemma-2-2B, revealing\ntrade-offs between toxicity reduction and language fluency. At stronger\nsteering strengths, these causal interventions surpass competitive baselines in\nreducing toxicity by up to 20%, though fluency can degrade noticeably on GPT-2\nSmall depending on the aggressiveness. Crucially, standard NLP benchmark scores\nupon steering remain stable, indicating that the model's knowledge and general\nabilities are preserved. We further show that feature-splitting in wider SAEs\nhampers safety interventions, underscoring the importance of disentangled\nfeature learning. Our findings highlight both the promise and the current\nlimitations of SAE-based causal interventions for LLM detoxification, further\nsuggesting practical guidelines for safer language-model deployment."
                },
                "authors": [
                    {
                        "name": "Agam Goyal"
                    },
                    {
                        "name": "Vedant Rathi"
                    },
                    {
                        "name": "William Yeh"
                    },
                    {
                        "name": "Yian Wang"
                    },
                    {
                        "name": "Yuen Chen"
                    },
                    {
                        "name": "Hari Sundaram"
                    }
                ],
                "author_detail": {
                    "name": "Hari Sundaram"
                },
                "author": "Hari Sundaram",
                "arxiv_comment": "Preprint: 19 pages, 7 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14536v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14536v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14533v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14533v1",
                "updated": "2025-05-20T15:52:43Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    15,
                    52,
                    43,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T15:52:43Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    15,
                    52,
                    43,
                    1,
                    140,
                    0
                ],
                "title": "Energy-Efficient Deep Reinforcement Learning with Spiking Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Energy-Efficient Deep Reinforcement Learning with Spiking Transformers"
                },
                "summary": "Agent-based Transformers have been widely adopted in recent reinforcement\nlearning advances due to their demonstrated ability to solve complex tasks.\nHowever, the high computational complexity of Transformers often results in\nsignificant energy consumption, limiting their deployment in real-world\nautonomous systems. Spiking neural networks (SNNs), with their biologically\ninspired structure, offer an energy-efficient alternative for machine learning.\nIn this paper, a novel Spike-Transformer Reinforcement Learning (STRL)\nalgorithm that combines the energy efficiency of SNNs with the powerful\ndecision-making capabilities of reinforcement learning is developed.\nSpecifically, an SNN using multi-step Leaky Integrate-and-Fire (LIF) neurons\nand attention mechanisms capable of processing spatio-temporal patterns over\nmultiple time steps is designed. The architecture is further enhanced with\nstate, action, and reward encodings to create a Transformer-like structure\noptimized for reinforcement learning tasks. Comprehensive numerical experiments\nconducted on state-of-the-art benchmarks demonstrate that the proposed SNN\nTransformer achieves significantly improved policy performance compared to\nconventional agent-based Transformers. With both enhanced energy efficiency and\npolicy optimality, this work highlights a promising direction for deploying\nbio-inspired, low-cost machine learning models in complex real-world\ndecision-making scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agent-based Transformers have been widely adopted in recent reinforcement\nlearning advances due to their demonstrated ability to solve complex tasks.\nHowever, the high computational complexity of Transformers often results in\nsignificant energy consumption, limiting their deployment in real-world\nautonomous systems. Spiking neural networks (SNNs), with their biologically\ninspired structure, offer an energy-efficient alternative for machine learning.\nIn this paper, a novel Spike-Transformer Reinforcement Learning (STRL)\nalgorithm that combines the energy efficiency of SNNs with the powerful\ndecision-making capabilities of reinforcement learning is developed.\nSpecifically, an SNN using multi-step Leaky Integrate-and-Fire (LIF) neurons\nand attention mechanisms capable of processing spatio-temporal patterns over\nmultiple time steps is designed. The architecture is further enhanced with\nstate, action, and reward encodings to create a Transformer-like structure\noptimized for reinforcement learning tasks. Comprehensive numerical experiments\nconducted on state-of-the-art benchmarks demonstrate that the proposed SNN\nTransformer achieves significantly improved policy performance compared to\nconventional agent-based Transformers. With both enhanced energy efficiency and\npolicy optimality, this work highlights a promising direction for deploying\nbio-inspired, low-cost machine learning models in complex real-world\ndecision-making scenarios."
                },
                "authors": [
                    {
                        "name": "Mohammad Irfan Uddin"
                    },
                    {
                        "name": "Nishad Tasnim"
                    },
                    {
                        "name": "Md Omor Faruk"
                    },
                    {
                        "name": "Zejian Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Zejian Zhou"
                },
                "author": "Zejian Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14533v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14533v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14530v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14530v1",
                "updated": "2025-05-20T15:49:15Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    15,
                    49,
                    15,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T15:49:15Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    15,
                    49,
                    15,
                    1,
                    140,
                    0
                ],
                "title": "Internal Chain-of-Thought: Empirical Evidence for Layer-wise Subtask\n  Scheduling in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Internal Chain-of-Thought: Empirical Evidence for Layer-wise Subtask\n  Scheduling in LLMs"
                },
                "summary": "We show that large language models (LLMs) exhibit an $\\textit{internal\nchain-of-thought}$: they sequentially decompose and execute composite tasks\nlayer-by-layer. Two claims ground our study: (i) distinct subtasks are learned\nat different network depths, and (ii) these subtasks are executed sequentially\nacross layers. On a benchmark of 15 two-step composite tasks, we employ\nlayer-from context-masking and propose a novel cross-task patching method,\nconfirming (i). To examine claim (ii), we apply LogitLens to decode hidden\nstates, revealing a consistent layerwise execution pattern. We further\nreplicate our analysis on the real-world $\\text{TRACE}$ benchmark, observing\nthe same stepwise dynamics. Together, our results enhance LLMs transparency by\nshowing their capacity to internally plan and execute subtasks (or\ninstructions), opening avenues for fine-grained, instruction-level activation\nsteering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We show that large language models (LLMs) exhibit an $\\textit{internal\nchain-of-thought}$: they sequentially decompose and execute composite tasks\nlayer-by-layer. Two claims ground our study: (i) distinct subtasks are learned\nat different network depths, and (ii) these subtasks are executed sequentially\nacross layers. On a benchmark of 15 two-step composite tasks, we employ\nlayer-from context-masking and propose a novel cross-task patching method,\nconfirming (i). To examine claim (ii), we apply LogitLens to decode hidden\nstates, revealing a consistent layerwise execution pattern. We further\nreplicate our analysis on the real-world $\\text{TRACE}$ benchmark, observing\nthe same stepwise dynamics. Together, our results enhance LLMs transparency by\nshowing their capacity to internally plan and execute subtasks (or\ninstructions), opening avenues for fine-grained, instruction-level activation\nsteering."
                },
                "authors": [
                    {
                        "name": "Zhipeng Yang"
                    },
                    {
                        "name": "Junzhuo Li"
                    },
                    {
                        "name": "Siyu Xia"
                    },
                    {
                        "name": "Xuming Hu"
                    }
                ],
                "author_detail": {
                    "name": "Xuming Hu"
                },
                "author": "Xuming Hu",
                "arxiv_comment": "27 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14530v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14530v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14528v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14528v1",
                "updated": "2025-05-20T15:48:34Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    15,
                    48,
                    34,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T15:48:34Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    15,
                    48,
                    34,
                    1,
                    140,
                    0
                ],
                "title": "BugRepro: Enhancing Android Bug Reproduction with Domain-Specific\n  Knowledge Integration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BugRepro: Enhancing Android Bug Reproduction with Domain-Specific\n  Knowledge Integration"
                },
                "summary": "Mobile application development is a fast-paced process where maintaining\nhigh-quality user experiences is crucial. Current bug reproduction methods\npredominantly depend on precise feature descriptions in bug reports. However,\nthe growing complexity and dynamism of modern software systems pose significant\nchallenges to this crucial quality assurance process, as ambiguous or\nincomplete steps-to-reproduce (S2Rs) in reports frequently impede effective\ndebugging and maintenance. To address these challenges, we propose BugRepro, a\nnovel technique that integrates domain-specific knowledge to enhance the\naccuracy and efficiency of bug reproduction. BugRepro adopts a\nRetrieval-Augmented Generation (RAG) approach. It retrieves similar bug reports\nalong with their corresponding S2R entities from an example-rich RAG document.\nThis document serves as a valuable reference for improving the accuracy of S2R\nentity extraction. In addition, BugRepro incorporates app-specific knowledge.\nIt explores the app's graphical user interface (GUI) and extracts UI transition\ngraphs. These graphs are used to guide large language models (LLMs) in their\nexploration process when they encounter bottlenecks. Our experiments\ndemonstrate the effectiveness of BugRepro. Our method significantly outperforms\ntwo state-of-the-art methods. For S2R entity extraction accuracy, it achieves\nimprovements of 8.85% and 28.89%. For bug reproduction success rate, the\nimprovements reach 74.55% and 152.63%. In reproduction efficiency, the gains\nare 0.72% and 76.68%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile application development is a fast-paced process where maintaining\nhigh-quality user experiences is crucial. Current bug reproduction methods\npredominantly depend on precise feature descriptions in bug reports. However,\nthe growing complexity and dynamism of modern software systems pose significant\nchallenges to this crucial quality assurance process, as ambiguous or\nincomplete steps-to-reproduce (S2Rs) in reports frequently impede effective\ndebugging and maintenance. To address these challenges, we propose BugRepro, a\nnovel technique that integrates domain-specific knowledge to enhance the\naccuracy and efficiency of bug reproduction. BugRepro adopts a\nRetrieval-Augmented Generation (RAG) approach. It retrieves similar bug reports\nalong with their corresponding S2R entities from an example-rich RAG document.\nThis document serves as a valuable reference for improving the accuracy of S2R\nentity extraction. In addition, BugRepro incorporates app-specific knowledge.\nIt explores the app's graphical user interface (GUI) and extracts UI transition\ngraphs. These graphs are used to guide large language models (LLMs) in their\nexploration process when they encounter bottlenecks. Our experiments\ndemonstrate the effectiveness of BugRepro. Our method significantly outperforms\ntwo state-of-the-art methods. For S2R entity extraction accuracy, it achieves\nimprovements of 8.85% and 28.89%. For bug reproduction success rate, the\nimprovements reach 74.55% and 152.63%. In reproduction efficiency, the gains\nare 0.72% and 76.68%."
                },
                "authors": [
                    {
                        "name": "Hongrong Yin"
                    },
                    {
                        "name": "Tao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tao Zhang"
                },
                "author": "Tao Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14528v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14528v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14526v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14526v1",
                "updated": "2025-05-20T15:48:23Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    15,
                    48,
                    23,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T15:48:23Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    15,
                    48,
                    23,
                    1,
                    140,
                    0
                ],
                "title": "NavBench: A Unified Robotics Benchmark for Reinforcement Learning-Based\n  Autonomous Navigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NavBench: A Unified Robotics Benchmark for Reinforcement Learning-Based\n  Autonomous Navigation"
                },
                "summary": "Autonomous robots must navigate and operate in diverse environments, from\nterrestrial and aquatic settings to aerial and space domains. While\nReinforcement Learning (RL) has shown promise in training policies for specific\nautonomous robots, existing benchmarks are often constrained to unique\nplatforms, limiting generalization and fair comparisons across different\nmobility systems. In this paper, we present NavBench, a multi-domain benchmark\nfor training and evaluating RL-based navigation policies across diverse robotic\nplatforms and operational environments. Built on IsaacLab, our framework\nstandardizes task definitions, enabling different robots to tackle various\nnavigation challenges without the need for ad-hoc task redesigns or custom\nevaluation metrics. Our benchmark addresses three key challenges: (1) Unified\ncross-medium benchmarking, enabling direct evaluation of diverse actuation\nmethods (thrusters, wheels, water-based propulsion) in realistic environments;\n(2) Scalable and modular design, facilitating seamless robot-task\ninterchangeability and reproducible training pipelines; and (3) Robust\nsim-to-real validation, demonstrated through successful policy transfer to\nmultiple real-world robots, including a satellite robotic simulator, an\nunmanned surface vessel, and a wheeled ground vehicle. By ensuring consistency\nbetween simulation and real-world deployment, NavBench simplifies the\ndevelopment of adaptable RL-based navigation strategies. Its modular design\nallows researchers to easily integrate custom robots and tasks by following the\nframework's predefined templates, making it accessible for a wide range of\napplications. Our code is publicly available at NavBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous robots must navigate and operate in diverse environments, from\nterrestrial and aquatic settings to aerial and space domains. While\nReinforcement Learning (RL) has shown promise in training policies for specific\nautonomous robots, existing benchmarks are often constrained to unique\nplatforms, limiting generalization and fair comparisons across different\nmobility systems. In this paper, we present NavBench, a multi-domain benchmark\nfor training and evaluating RL-based navigation policies across diverse robotic\nplatforms and operational environments. Built on IsaacLab, our framework\nstandardizes task definitions, enabling different robots to tackle various\nnavigation challenges without the need for ad-hoc task redesigns or custom\nevaluation metrics. Our benchmark addresses three key challenges: (1) Unified\ncross-medium benchmarking, enabling direct evaluation of diverse actuation\nmethods (thrusters, wheels, water-based propulsion) in realistic environments;\n(2) Scalable and modular design, facilitating seamless robot-task\ninterchangeability and reproducible training pipelines; and (3) Robust\nsim-to-real validation, demonstrated through successful policy transfer to\nmultiple real-world robots, including a satellite robotic simulator, an\nunmanned surface vessel, and a wheeled ground vehicle. By ensuring consistency\nbetween simulation and real-world deployment, NavBench simplifies the\ndevelopment of adaptable RL-based navigation strategies. Its modular design\nallows researchers to easily integrate custom robots and tasks by following the\nframework's predefined templates, making it accessible for a wide range of\napplications. Our code is publicly available at NavBench."
                },
                "authors": [
                    {
                        "name": "Matteo El-Hariry"
                    },
                    {
                        "name": "Antoine Richard"
                    },
                    {
                        "name": "Ricard M. Castan"
                    },
                    {
                        "name": "Luis F. W. Batista"
                    },
                    {
                        "name": "Matthieu Geist"
                    },
                    {
                        "name": "Cedric Pradalier"
                    },
                    {
                        "name": "Miguel Olivares-Mendez"
                    }
                ],
                "author_detail": {
                    "name": "Miguel Olivares-Mendez"
                },
                "author": "Miguel Olivares-Mendez",
                "arxiv_comment": "Submitted for publication. Under review (2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14526v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14526v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14524v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14524v1",
                "updated": "2025-05-20T15:46:59Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    15,
                    46,
                    59,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T15:46:59Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    15,
                    46,
                    59,
                    1,
                    140,
                    0
                ],
                "title": "Guarded Query Routing for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Guarded Query Routing for Large Language Models"
                },
                "summary": "Query routing, the task to route user queries to different large language\nmodel (LLM) endpoints, can be considered as a text classification problem.\nHowever, out-of-distribution queries must be handled properly, as those could\nbe questions about unrelated domains, queries in other languages, or even\ncontain unsafe text. Here, we thus study a \\emph{guarded} query routing\nproblem, for which we first introduce the Guarded Query Routing Benchmark\n(GQR-Bench), which covers three exemplary target domains (law, finance, and\nhealthcare), and seven datasets to test robustness against out-of-distribution\nqueries. We then use GQR-Bench to contrast the effectiveness and efficiency of\nLLM-based routing mechanisms (GPT-4o-mini, Llama-3.2-3B, and Llama-3.1-8B),\nstandard LLM-based guardrail approaches (LlamaGuard and NVIDIA NeMo\nGuardrails), continuous bag-of-words classifiers (WideMLP, fastText), and\ntraditional machine learning models (SVM, XGBoost). Our results show that\nWideMLP, enhanced with out-of-domain detection capabilities, yields the best\ntrade-off between accuracy (88\\%) and speed (<4ms). The embedding-based\nfastText excels at speed (<1ms) with acceptable accuracy (80\\%), whereas LLMs\nyield the highest accuracy (91\\%) but are comparatively slow (62ms for local\nLlama-3.1:8B and 669ms for remote GPT-4o-mini calls). Our findings challenge\nthe automatic reliance on LLMs for (guarded) query routing and provide concrete\nrecommendations for practical applications. GQR-Bench will be released as a\nPython package -- \\texttt{gqr}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Query routing, the task to route user queries to different large language\nmodel (LLM) endpoints, can be considered as a text classification problem.\nHowever, out-of-distribution queries must be handled properly, as those could\nbe questions about unrelated domains, queries in other languages, or even\ncontain unsafe text. Here, we thus study a \\emph{guarded} query routing\nproblem, for which we first introduce the Guarded Query Routing Benchmark\n(GQR-Bench), which covers three exemplary target domains (law, finance, and\nhealthcare), and seven datasets to test robustness against out-of-distribution\nqueries. We then use GQR-Bench to contrast the effectiveness and efficiency of\nLLM-based routing mechanisms (GPT-4o-mini, Llama-3.2-3B, and Llama-3.1-8B),\nstandard LLM-based guardrail approaches (LlamaGuard and NVIDIA NeMo\nGuardrails), continuous bag-of-words classifiers (WideMLP, fastText), and\ntraditional machine learning models (SVM, XGBoost). Our results show that\nWideMLP, enhanced with out-of-domain detection capabilities, yields the best\ntrade-off between accuracy (88\\%) and speed (<4ms). The embedding-based\nfastText excels at speed (<1ms) with acceptable accuracy (80\\%), whereas LLMs\nyield the highest accuracy (91\\%) but are comparatively slow (62ms for local\nLlama-3.1:8B and 669ms for remote GPT-4o-mini calls). Our findings challenge\nthe automatic reliance on LLMs for (guarded) query routing and provide concrete\nrecommendations for practical applications. GQR-Bench will be released as a\nPython package -- \\texttt{gqr}."
                },
                "authors": [
                    {
                        "name": "Richard Šléher"
                    },
                    {
                        "name": "William Brach"
                    },
                    {
                        "name": "Tibor Sloboda"
                    },
                    {
                        "name": "Kristián Košťál"
                    },
                    {
                        "name": "Lukas Galke"
                    }
                ],
                "author_detail": {
                    "name": "Lukas Galke"
                },
                "author": "Lukas Galke",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14524v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14524v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14518v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14518v1",
                "updated": "2025-05-20T15:44:01Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    15,
                    44,
                    1,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T15:44:01Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    15,
                    44,
                    1,
                    1,
                    140,
                    0
                ],
                "title": "Teaching Audio-Aware Large Language Models What Does Not Hear:\n  Mitigating Hallucinations through Synthesized Negative Samples",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Teaching Audio-Aware Large Language Models What Does Not Hear:\n  Mitigating Hallucinations through Synthesized Negative Samples"
                },
                "summary": "Recent advancements in audio-aware large language models (ALLMs) enable them\nto process and understand audio inputs. However, these models often hallucinate\nnon-existent sound events, reducing their reliability in real-world\napplications. To address this, we propose LISTEN (Learning to Identify Sounds\nThrough Extended Negative Samples), a contrastive-like training method that\nenhances ALLMs' ability to distinguish between present and absent sounds using\nsynthesized data from the backbone LLM. Unlike prior approaches, our method\nrequires no modification to LLM parameters and efficiently integrates audio\nrepresentations via a lightweight adapter. Experiments show that LISTEN\neffectively mitigates hallucinations while maintaining impressive performance\non existing audio question and reasoning benchmarks. At the same time, it is\nmore efficient in both data and computation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in audio-aware large language models (ALLMs) enable them\nto process and understand audio inputs. However, these models often hallucinate\nnon-existent sound events, reducing their reliability in real-world\napplications. To address this, we propose LISTEN (Learning to Identify Sounds\nThrough Extended Negative Samples), a contrastive-like training method that\nenhances ALLMs' ability to distinguish between present and absent sounds using\nsynthesized data from the backbone LLM. Unlike prior approaches, our method\nrequires no modification to LLM parameters and efficiently integrates audio\nrepresentations via a lightweight adapter. Experiments show that LISTEN\neffectively mitigates hallucinations while maintaining impressive performance\non existing audio question and reasoning benchmarks. At the same time, it is\nmore efficient in both data and computation."
                },
                "authors": [
                    {
                        "name": "Chun-Yi Kuan"
                    },
                    {
                        "name": "Hung-yi Lee"
                    }
                ],
                "author_detail": {
                    "name": "Hung-yi Lee"
                },
                "author": "Hung-yi Lee",
                "arxiv_comment": "Accepted to Interspeech 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14518v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14518v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14513v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14513v1",
                "updated": "2025-05-20T15:41:05Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    15,
                    41,
                    5,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T15:41:05Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    15,
                    41,
                    5,
                    1,
                    140,
                    0
                ],
                "title": "Latent Flow Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latent Flow Transformer"
                },
                "summary": "Transformers, the standard implementation for large language models (LLMs),\ntypically consist of tens to hundreds of discrete layers. While more layers can\nlead to better performance, this approach has been challenged as far from\nefficient, especially given the superiority of continuous layers demonstrated\nby diffusion and flow-based models for image generation. We propose the Latent\nFlow Transformer (LFT), which replaces a block of layers with a single learned\ntransport operator trained via flow matching, offering significant compression\nwhile maintaining compatibility with the original architecture. Additionally,\nwe address the limitations of existing flow-based methods in \\textit{preserving\ncoupling} by introducing the Flow Walking (FW) algorithm. On the Pythia-410M\nmodel, LFT trained with flow matching compresses 6 of 24 layers and outperforms\ndirectly skipping 2 layers (KL Divergence of LM logits at 0.407 vs. 0.529),\ndemonstrating the feasibility of this design. When trained with FW, LFT further\ndistills 12 layers into one while reducing the KL to 0.736 surpassing that from\nskipping 3 layers (0.932), significantly narrowing the gap between\nautoregressive and flow-based generation paradigms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers, the standard implementation for large language models (LLMs),\ntypically consist of tens to hundreds of discrete layers. While more layers can\nlead to better performance, this approach has been challenged as far from\nefficient, especially given the superiority of continuous layers demonstrated\nby diffusion and flow-based models for image generation. We propose the Latent\nFlow Transformer (LFT), which replaces a block of layers with a single learned\ntransport operator trained via flow matching, offering significant compression\nwhile maintaining compatibility with the original architecture. Additionally,\nwe address the limitations of existing flow-based methods in \\textit{preserving\ncoupling} by introducing the Flow Walking (FW) algorithm. On the Pythia-410M\nmodel, LFT trained with flow matching compresses 6 of 24 layers and outperforms\ndirectly skipping 2 layers (KL Divergence of LM logits at 0.407 vs. 0.529),\ndemonstrating the feasibility of this design. When trained with FW, LFT further\ndistills 12 layers into one while reducing the KL to 0.736 surpassing that from\nskipping 3 layers (0.932), significantly narrowing the gap between\nautoregressive and flow-based generation paradigms."
                },
                "authors": [
                    {
                        "name": "Yen-Chen Wu"
                    },
                    {
                        "name": "Feng-Ting Liao"
                    },
                    {
                        "name": "Meng-Hsi Chen"
                    },
                    {
                        "name": "Pei-Chen Ho"
                    },
                    {
                        "name": "Farhang Nabiei"
                    },
                    {
                        "name": "Da-shan Shiu"
                    }
                ],
                "author_detail": {
                    "name": "Da-shan Shiu"
                },
                "author": "Da-shan Shiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14513v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14513v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14505v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14505v1",
                "updated": "2025-05-20T15:34:36Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    15,
                    34,
                    36,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T15:34:36Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    15,
                    34,
                    36,
                    1,
                    140,
                    0
                ],
                "title": "ModRWKV: Transformer Multimodality in Linear Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ModRWKV: Transformer Multimodality in Linear Time"
                },
                "summary": "Currently, most multimodal studies are based on large language models (LLMs)\nwith quadratic-complexity Transformer architectures. While linear models like\nRNNs enjoy low inference costs, their application has been largely limited to\nthe text-only modality. This work explores the capabilities of modern RNN\narchitectures in multimodal contexts. We propose ModRWKV-a decoupled multimodal\nframework built upon the RWKV7 architecture as its LLM backbone-which achieves\nmulti-source information fusion through dynamically adaptable heterogeneous\nmodality encoders. We designed the multimodal modules in ModRWKV with an\nextremely lightweight architecture and, through extensive experiments,\nidentified a configuration that achieves an optimal balance between performance\nand computational efficiency. ModRWKV leverages the pretrained weights of the\nRWKV7 LLM for initialization, which significantly accelerates multimodal\ntraining. Comparative experiments with different pretrained checkpoints further\ndemonstrate that such initialization plays a crucial role in enhancing the\nmodel's ability to understand multimodal signals. Supported by extensive\nexperiments, we conclude that modern RNN architectures present a viable\nalternative to Transformers in the domain of multimodal large language models\n(MLLMs). Furthermore, we identify the optimal configuration of the ModRWKV\narchitecture through systematic exploration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Currently, most multimodal studies are based on large language models (LLMs)\nwith quadratic-complexity Transformer architectures. While linear models like\nRNNs enjoy low inference costs, their application has been largely limited to\nthe text-only modality. This work explores the capabilities of modern RNN\narchitectures in multimodal contexts. We propose ModRWKV-a decoupled multimodal\nframework built upon the RWKV7 architecture as its LLM backbone-which achieves\nmulti-source information fusion through dynamically adaptable heterogeneous\nmodality encoders. We designed the multimodal modules in ModRWKV with an\nextremely lightweight architecture and, through extensive experiments,\nidentified a configuration that achieves an optimal balance between performance\nand computational efficiency. ModRWKV leverages the pretrained weights of the\nRWKV7 LLM for initialization, which significantly accelerates multimodal\ntraining. Comparative experiments with different pretrained checkpoints further\ndemonstrate that such initialization plays a crucial role in enhancing the\nmodel's ability to understand multimodal signals. Supported by extensive\nexperiments, we conclude that modern RNN architectures present a viable\nalternative to Transformers in the domain of multimodal large language models\n(MLLMs). Furthermore, we identify the optimal configuration of the ModRWKV\narchitecture through systematic exploration."
                },
                "authors": [
                    {
                        "name": "Jiale Kang"
                    },
                    {
                        "name": "Ziyin Yue"
                    },
                    {
                        "name": "Qingyu Yin"
                    },
                    {
                        "name": "Jiang Rui"
                    },
                    {
                        "name": "Weile Li"
                    },
                    {
                        "name": "Zening Lu"
                    },
                    {
                        "name": "Zhouran Ji"
                    }
                ],
                "author_detail": {
                    "name": "Zhouran Ji"
                },
                "author": "Zhouran Ji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14505v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14505v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09083v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09083v2",
                "updated": "2025-05-20T15:29:37Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    15,
                    29,
                    37,
                    1,
                    140,
                    0
                ],
                "published": "2024-10-06T08:33:39Z",
                "published_parsed": [
                    2024,
                    10,
                    6,
                    8,
                    33,
                    39,
                    6,
                    280,
                    0
                ],
                "title": "Evaluating the Correctness of Inference Patterns Used by LLMs for\n  Judgment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the Correctness of Inference Patterns Used by LLMs for\n  Judgment"
                },
                "summary": "This paper presents a method to analyze the inference patterns used by Large\nLanguage Models (LLMs) for judgment in a case study on legal LLMs, so as to\nidentify potential incorrect representations of the LLM, according to human\ndomain knowledge. Unlike traditional evaluations on language generation\nresults, we propose to evaluate the correctness of the detailed inference\npatterns of an LLM behind its seemingly correct outputs. To this end, we\nquantify the interactions between input phrases used by the LLM as primitive\ninference patterns, because recent theoretical achievements have proven several\nmathematical guarantees of the faithfulness of the interaction-based\nexplanation. We design a set of metrics to evaluate the detailed inference\npatterns of LLMs. Experiments show that even when the language generation\nresults appear correct, a significant portion of the inference patterns used by\nthe LLM for the legal judgment may represent misleading or irrelevant logic.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a method to analyze the inference patterns used by Large\nLanguage Models (LLMs) for judgment in a case study on legal LLMs, so as to\nidentify potential incorrect representations of the LLM, according to human\ndomain knowledge. Unlike traditional evaluations on language generation\nresults, we propose to evaluate the correctness of the detailed inference\npatterns of an LLM behind its seemingly correct outputs. To this end, we\nquantify the interactions between input phrases used by the LLM as primitive\ninference patterns, because recent theoretical achievements have proven several\nmathematical guarantees of the faithfulness of the interaction-based\nexplanation. We design a set of metrics to evaluate the detailed inference\npatterns of LLMs. Experiments show that even when the language generation\nresults appear correct, a significant portion of the inference patterns used by\nthe LLM for the legal judgment may represent misleading or irrelevant logic."
                },
                "authors": [
                    {
                        "name": "Lu Chen"
                    },
                    {
                        "name": "Yuxuan Huang"
                    },
                    {
                        "name": "Yixing Li"
                    },
                    {
                        "name": "Dongrui Liu"
                    },
                    {
                        "name": "Qihan Ren"
                    },
                    {
                        "name": "Shuai Zhao"
                    },
                    {
                        "name": "Kun Kuang"
                    },
                    {
                        "name": "Zilong Zheng"
                    },
                    {
                        "name": "Quanshi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Quanshi Zhang"
                },
                "author": "Quanshi Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09083v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09083v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14499v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14499v1",
                "updated": "2025-05-20T15:28:26Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    15,
                    28,
                    26,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T15:28:26Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    15,
                    28,
                    26,
                    1,
                    140,
                    0
                ],
                "title": "Enhanced Multimodal Aspect-Based Sentiment Analysis by LLM-Generated\n  Rationales",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhanced Multimodal Aspect-Based Sentiment Analysis by LLM-Generated\n  Rationales"
                },
                "summary": "There has been growing interest in Multimodal Aspect-Based Sentiment Analysis\n(MABSA) in recent years. Existing methods predominantly rely on pre-trained\nsmall language models (SLMs) to collect information related to aspects and\nsentiments from both image and text, with an aim to align these two modalities.\nHowever, small SLMs possess limited capacity and knowledge, often resulting in\ninaccurate identification of meaning, aspects, sentiments, and their\ninterconnections in textual and visual data. On the other hand, Large language\nmodels (LLMs) have shown exceptional capabilities in various tasks by\neffectively exploring fine-grained information in multimodal data. However,\nsome studies indicate that LLMs still fall short compared to fine-tuned small\nmodels in the field of ABSA. Based on these findings, we propose a novel\nframework, termed LRSA, which combines the decision-making capabilities of SLMs\nwith additional information provided by LLMs for MABSA. Specifically, we inject\nexplanations generated by LLMs as rationales into SLMs and employ a dual\ncross-attention mechanism for enhancing feature interaction and fusion, thereby\naugmenting the SLMs' ability to identify aspects and sentiments. We evaluated\nour method using two baseline models, numerous experiments highlight the\nsuperiority of our approach on three widely-used benchmarks, indicating its\ngeneralizability and applicability to most pre-trained models for MABSA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There has been growing interest in Multimodal Aspect-Based Sentiment Analysis\n(MABSA) in recent years. Existing methods predominantly rely on pre-trained\nsmall language models (SLMs) to collect information related to aspects and\nsentiments from both image and text, with an aim to align these two modalities.\nHowever, small SLMs possess limited capacity and knowledge, often resulting in\ninaccurate identification of meaning, aspects, sentiments, and their\ninterconnections in textual and visual data. On the other hand, Large language\nmodels (LLMs) have shown exceptional capabilities in various tasks by\neffectively exploring fine-grained information in multimodal data. However,\nsome studies indicate that LLMs still fall short compared to fine-tuned small\nmodels in the field of ABSA. Based on these findings, we propose a novel\nframework, termed LRSA, which combines the decision-making capabilities of SLMs\nwith additional information provided by LLMs for MABSA. Specifically, we inject\nexplanations generated by LLMs as rationales into SLMs and employ a dual\ncross-attention mechanism for enhancing feature interaction and fusion, thereby\naugmenting the SLMs' ability to identify aspects and sentiments. We evaluated\nour method using two baseline models, numerous experiments highlight the\nsuperiority of our approach on three widely-used benchmarks, indicating its\ngeneralizability and applicability to most pre-trained models for MABSA."
                },
                "authors": [
                    {
                        "name": "Jun Cao"
                    },
                    {
                        "name": "Jiyi Li"
                    },
                    {
                        "name": "Ziwei Yang"
                    },
                    {
                        "name": "Renjie Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Renjie Zhou"
                },
                "author": "Renjie Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14499v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14499v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01420v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01420v3",
                "updated": "2025-05-20T15:20:09Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    15,
                    20,
                    9,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-02T17:57:14Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    17,
                    57,
                    14,
                    4,
                    122,
                    0
                ],
                "title": "Evaluating Frontier Models for Stealth and Situational Awareness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Frontier Models for Stealth and Situational Awareness"
                },
                "summary": "Recent work has demonstrated the plausibility of frontier AI models scheming\n-- knowingly and covertly pursuing an objective misaligned with its developer's\nintentions. Such behavior could be very hard to detect, and if present in\nfuture advanced systems, could pose severe loss of control risk. It is\ntherefore important for AI developers to rule out harm from scheming prior to\nmodel deployment. In this paper, we present a suite of scheming reasoning\nevaluations measuring two types of reasoning capabilities that we believe are\nprerequisites for successful scheming: First, we propose five evaluations of\nability to reason about and circumvent oversight (stealth). Second, we present\neleven evaluations for measuring a model's ability to instrumentally reason\nabout itself, its environment and its deployment (situational awareness). We\ndemonstrate how these evaluations can be used as part of a scheming inability\nsafety case: a model that does not succeed on these evaluations is almost\ncertainly incapable of causing severe harm via scheming in real deployment. We\nrun our evaluations on current frontier models and find that none of them show\nconcerning levels of either situational awareness or stealth.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work has demonstrated the plausibility of frontier AI models scheming\n-- knowingly and covertly pursuing an objective misaligned with its developer's\nintentions. Such behavior could be very hard to detect, and if present in\nfuture advanced systems, could pose severe loss of control risk. It is\ntherefore important for AI developers to rule out harm from scheming prior to\nmodel deployment. In this paper, we present a suite of scheming reasoning\nevaluations measuring two types of reasoning capabilities that we believe are\nprerequisites for successful scheming: First, we propose five evaluations of\nability to reason about and circumvent oversight (stealth). Second, we present\neleven evaluations for measuring a model's ability to instrumentally reason\nabout itself, its environment and its deployment (situational awareness). We\ndemonstrate how these evaluations can be used as part of a scheming inability\nsafety case: a model that does not succeed on these evaluations is almost\ncertainly incapable of causing severe harm via scheming in real deployment. We\nrun our evaluations on current frontier models and find that none of them show\nconcerning levels of either situational awareness or stealth."
                },
                "authors": [
                    {
                        "name": "Mary Phuong"
                    },
                    {
                        "name": "Roland S. Zimmermann"
                    },
                    {
                        "name": "Ziyue Wang"
                    },
                    {
                        "name": "David Lindner"
                    },
                    {
                        "name": "Victoria Krakovna"
                    },
                    {
                        "name": "Sarah Cogan"
                    },
                    {
                        "name": "Allan Dafoe"
                    },
                    {
                        "name": "Lewis Ho"
                    },
                    {
                        "name": "Rohin Shah"
                    }
                ],
                "author_detail": {
                    "name": "Rohin Shah"
                },
                "author": "Rohin Shah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01420v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01420v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14489v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14489v1",
                "updated": "2025-05-20T15:19:00Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    15,
                    19,
                    0,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T15:19:00Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    15,
                    19,
                    0,
                    1,
                    140,
                    0
                ],
                "title": "Reasoning Models Better Express Their Confidence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning Models Better Express Their Confidence"
                },
                "summary": "Despite their strengths, large language models (LLMs) often fail to\ncommunicate their confidence accurately, making it difficult to assess when\nthey might be wrong and limiting their reliability. In this work, we\ndemonstrate that reasoning models-LLMs that engage in extended chain-of-thought\n(CoT) reasoning-exhibit superior performance not only in problem-solving but\nalso in accurately expressing their confidence. Specifically, we benchmark six\nreasoning models across six datasets and find that they achieve strictly better\nconfidence calibration than their non-reasoning counterparts in 33 out of the\n36 settings. Our detailed analysis reveals that these gains in calibration stem\nfrom the slow thinking behaviors of reasoning models-such as exploring\nalternative approaches and backtracking-which enable them to adjust their\nconfidence dynamically throughout their CoT, making it progressively more\naccurate. In particular, we find that reasoning models become increasingly\nbetter calibrated as their CoT unfolds, a trend not observed in non-reasoning\nmodels. Moreover, removing slow thinking behaviors from the CoT leads to a\nsignificant drop in calibration. Lastly, we show that these gains are not\nexclusive to reasoning models-non-reasoning models also benefit when guided to\nperform slow thinking via in-context learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite their strengths, large language models (LLMs) often fail to\ncommunicate their confidence accurately, making it difficult to assess when\nthey might be wrong and limiting their reliability. In this work, we\ndemonstrate that reasoning models-LLMs that engage in extended chain-of-thought\n(CoT) reasoning-exhibit superior performance not only in problem-solving but\nalso in accurately expressing their confidence. Specifically, we benchmark six\nreasoning models across six datasets and find that they achieve strictly better\nconfidence calibration than their non-reasoning counterparts in 33 out of the\n36 settings. Our detailed analysis reveals that these gains in calibration stem\nfrom the slow thinking behaviors of reasoning models-such as exploring\nalternative approaches and backtracking-which enable them to adjust their\nconfidence dynamically throughout their CoT, making it progressively more\naccurate. In particular, we find that reasoning models become increasingly\nbetter calibrated as their CoT unfolds, a trend not observed in non-reasoning\nmodels. Moreover, removing slow thinking behaviors from the CoT leads to a\nsignificant drop in calibration. Lastly, we show that these gains are not\nexclusive to reasoning models-non-reasoning models also benefit when guided to\nperform slow thinking via in-context learning."
                },
                "authors": [
                    {
                        "name": "Dongkeun Yoon"
                    },
                    {
                        "name": "Seungone Kim"
                    },
                    {
                        "name": "Sohee Yang"
                    },
                    {
                        "name": "Sunkyoung Kim"
                    },
                    {
                        "name": "Soyeon Kim"
                    },
                    {
                        "name": "Yongil Kim"
                    },
                    {
                        "name": "Eunbi Choi"
                    },
                    {
                        "name": "Yireun Kim"
                    },
                    {
                        "name": "Minjoon Seo"
                    }
                ],
                "author_detail": {
                    "name": "Minjoon Seo"
                },
                "author": "Minjoon Seo",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14489v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14489v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02577v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02577v3",
                "updated": "2025-05-20T15:16:38Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    15,
                    16,
                    38,
                    1,
                    140,
                    0
                ],
                "published": "2025-02-04T18:53:42Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    18,
                    53,
                    42,
                    1,
                    35,
                    0
                ],
                "title": "A comparison of translation performance between DeepL and Supertext",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A comparison of translation performance between DeepL and Supertext"
                },
                "summary": "As strong machine translation (MT) systems are increasingly based on large\nlanguage models (LLMs), reliable quality benchmarking requires methods that\ncapture their ability to leverage extended context. This study compares two\ncommercial MT systems -- DeepL and Supertext -- by assessing their performance\non unsegmented texts. We evaluate translation quality across four language\ndirections with professional translators assessing segments with full\ndocument-level context. While segment-level assessments indicate no strong\npreference between the systems in most cases, document-level analysis reveals a\npreference for Supertext in three out of four language directions, suggesting\nsuperior consistency across longer texts. We advocate for more\ncontext-sensitive evaluation methodologies to ensure that MT quality\nassessments reflect real-world usability. We release all evaluation data and\nscripts for further analysis and reproduction at\nhttps://github.com/supertext/evaluation_deepl_supertext.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As strong machine translation (MT) systems are increasingly based on large\nlanguage models (LLMs), reliable quality benchmarking requires methods that\ncapture their ability to leverage extended context. This study compares two\ncommercial MT systems -- DeepL and Supertext -- by assessing their performance\non unsegmented texts. We evaluate translation quality across four language\ndirections with professional translators assessing segments with full\ndocument-level context. While segment-level assessments indicate no strong\npreference between the systems in most cases, document-level analysis reveals a\npreference for Supertext in three out of four language directions, suggesting\nsuperior consistency across longer texts. We advocate for more\ncontext-sensitive evaluation methodologies to ensure that MT quality\nassessments reflect real-world usability. We release all evaluation data and\nscripts for further analysis and reproduction at\nhttps://github.com/supertext/evaluation_deepl_supertext."
                },
                "authors": [
                    {
                        "name": "Alex Flückiger"
                    },
                    {
                        "name": "Chantal Amrhein"
                    },
                    {
                        "name": "Tim Graf"
                    },
                    {
                        "name": "Frédéric Odermatt"
                    },
                    {
                        "name": "Martin Pömsl"
                    },
                    {
                        "name": "Philippe Schläpfer"
                    },
                    {
                        "name": "Florian Schottmann"
                    },
                    {
                        "name": "Samuel Läubli"
                    }
                ],
                "author_detail": {
                    "name": "Samuel Läubli"
                },
                "author": "Samuel Läubli",
                "arxiv_comment": "Paper accepted at MT Summit 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02577v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02577v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14483v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14483v1",
                "updated": "2025-05-20T15:16:06Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    15,
                    16,
                    6,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T15:16:06Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    15,
                    16,
                    6,
                    1,
                    140,
                    0
                ],
                "title": "MoMoE: Mixture of Moderation Experts Framework for AI-Assisted Online\n  Governance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoMoE: Mixture of Moderation Experts Framework for AI-Assisted Online\n  Governance"
                },
                "summary": "Large language models (LLMs) have shown great potential in flagging harmful\ncontent in online communities. Yet, existing approaches for moderation require\na separate model for every community and are opaque in their decision-making,\nlimiting real-world adoption. We introduce Mixture of Moderation Experts\n(MoMoE), a modular, cross-community framework that adds post-hoc explanations\nto scalable content moderation. MoMoE orchestrates four operators -- Allocate,\nPredict, Aggregate, Explain -- and is instantiated as seven\ncommunity-specialized experts (MoMoE-Community) and five norm-violation experts\n(MoMoE-NormVio). On 30 unseen subreddits, the best variants obtain Micro-F1\nscores of 0.72 and 0.67, respectively, matching or surpassing strong fine-tuned\nbaselines while consistently producing concise and reliable explanations.\nAlthough community-specialized experts deliver the highest peak accuracy,\nnorm-violation experts provide steadier performance across domains. These\nfindings show that MoMoE yields scalable, transparent moderation without\nneeding per-community fine-tuning. More broadly, they suggest that lightweight,\nexplainable expert ensembles can guide future NLP and HCI research on\ntrustworthy human-AI governance of online communities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown great potential in flagging harmful\ncontent in online communities. Yet, existing approaches for moderation require\na separate model for every community and are opaque in their decision-making,\nlimiting real-world adoption. We introduce Mixture of Moderation Experts\n(MoMoE), a modular, cross-community framework that adds post-hoc explanations\nto scalable content moderation. MoMoE orchestrates four operators -- Allocate,\nPredict, Aggregate, Explain -- and is instantiated as seven\ncommunity-specialized experts (MoMoE-Community) and five norm-violation experts\n(MoMoE-NormVio). On 30 unseen subreddits, the best variants obtain Micro-F1\nscores of 0.72 and 0.67, respectively, matching or surpassing strong fine-tuned\nbaselines while consistently producing concise and reliable explanations.\nAlthough community-specialized experts deliver the highest peak accuracy,\nnorm-violation experts provide steadier performance across domains. These\nfindings show that MoMoE yields scalable, transparent moderation without\nneeding per-community fine-tuning. More broadly, they suggest that lightweight,\nexplainable expert ensembles can guide future NLP and HCI research on\ntrustworthy human-AI governance of online communities."
                },
                "authors": [
                    {
                        "name": "Agam Goyal"
                    },
                    {
                        "name": "Xianyang Zhan"
                    },
                    {
                        "name": "Yilun Chen"
                    },
                    {
                        "name": "Koustuv Saha"
                    },
                    {
                        "name": "Eshwar Chandrasekharan"
                    }
                ],
                "author_detail": {
                    "name": "Eshwar Chandrasekharan"
                },
                "author": "Eshwar Chandrasekharan",
                "arxiv_comment": "Preprint: 15 pages, 4 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14483v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14483v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14479v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14479v1",
                "updated": "2025-05-20T15:13:32Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    15,
                    13,
                    32,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T15:13:32Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    15,
                    13,
                    32,
                    1,
                    140,
                    0
                ],
                "title": "Towards Reliable Proof Generation with LLMs: A Neuro-Symbolic Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Reliable Proof Generation with LLMs: A Neuro-Symbolic Approach"
                },
                "summary": "Large language models (LLMs) struggle with formal domains that require\nrigorous logical deduction and symbolic reasoning, such as mathematical proof\ngeneration. We propose a neuro-symbolic approach that combines LLMs' generative\nstrengths with structured components to overcome this challenge. As a\nproof-of-concept, we focus on geometry problems. Our approach is two-fold: (1)\nwe retrieve analogous problems and use their proofs to guide the LLM, and (2) a\nformal verifier evaluates the generated proofs and provides feedback, helping\nthe model fix incorrect proofs. We demonstrate that our method significantly\nimproves proof accuracy for OpenAI's o1 model (58%-70% improvement); both\nanalogous problems and the verifier's feedback contribute to these gains. More\nbroadly, shifting to LLMs that generate provably correct conclusions could\ndramatically improve their reliability, accuracy and consistency, unlocking\ncomplex tasks and critical real-world applications that require\ntrustworthiness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) struggle with formal domains that require\nrigorous logical deduction and symbolic reasoning, such as mathematical proof\ngeneration. We propose a neuro-symbolic approach that combines LLMs' generative\nstrengths with structured components to overcome this challenge. As a\nproof-of-concept, we focus on geometry problems. Our approach is two-fold: (1)\nwe retrieve analogous problems and use their proofs to guide the LLM, and (2) a\nformal verifier evaluates the generated proofs and provides feedback, helping\nthe model fix incorrect proofs. We demonstrate that our method significantly\nimproves proof accuracy for OpenAI's o1 model (58%-70% improvement); both\nanalogous problems and the verifier's feedback contribute to these gains. More\nbroadly, shifting to LLMs that generate provably correct conclusions could\ndramatically improve their reliability, accuracy and consistency, unlocking\ncomplex tasks and critical real-world applications that require\ntrustworthiness."
                },
                "authors": [
                    {
                        "name": "Oren Sultan"
                    },
                    {
                        "name": "Eitan Stern"
                    },
                    {
                        "name": "Dafna Shahaf"
                    }
                ],
                "author_detail": {
                    "name": "Dafna Shahaf"
                },
                "author": "Dafna Shahaf",
                "arxiv_comment": "long paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14479v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14479v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14471v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14471v1",
                "updated": "2025-05-20T15:05:27Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    15,
                    5,
                    27,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T15:05:27Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    15,
                    5,
                    27,
                    1,
                    140,
                    0
                ],
                "title": "Adapting Pretrained Language Models for Citation Classification via\n  Self-Supervised Contrastive Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adapting Pretrained Language Models for Citation Classification via\n  Self-Supervised Contrastive Learning"
                },
                "summary": "Citation classification, which identifies the intention behind academic\ncitations, is pivotal for scholarly analysis. Previous works suggest\nfine-tuning pretrained language models (PLMs) on citation classification\ndatasets, reaping the reward of the linguistic knowledge they gained during\npretraining. However, directly fine-tuning for citation classification is\nchallenging due to labeled data scarcity, contextual noise, and spurious\nkeyphrase correlations. In this paper, we present a novel framework, Citss,\nthat adapts the PLMs to overcome these challenges. Citss introduces\nself-supervised contrastive learning to alleviate data scarcity, and is\nequipped with two specialized strategies to obtain the contrastive pairs:\nsentence-level cropping, which enhances focus on target citations within long\ncontexts, and keyphrase perturbation, which mitigates reliance on specific\nkeyphrases. Compared with previous works that are only designed for\nencoder-based PLMs, Citss is carefully developed to be compatible with both\nencoder-based PLMs and decoder-based LLMs, to embrace the benefits of enlarged\npretraining. Experiments with three benchmark datasets with both encoder-based\nPLMs and decoder-based LLMs demonstrate our superiority compared to the\nprevious state of the art. Our code is available at: github.com/LITONG99/Citss",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Citation classification, which identifies the intention behind academic\ncitations, is pivotal for scholarly analysis. Previous works suggest\nfine-tuning pretrained language models (PLMs) on citation classification\ndatasets, reaping the reward of the linguistic knowledge they gained during\npretraining. However, directly fine-tuning for citation classification is\nchallenging due to labeled data scarcity, contextual noise, and spurious\nkeyphrase correlations. In this paper, we present a novel framework, Citss,\nthat adapts the PLMs to overcome these challenges. Citss introduces\nself-supervised contrastive learning to alleviate data scarcity, and is\nequipped with two specialized strategies to obtain the contrastive pairs:\nsentence-level cropping, which enhances focus on target citations within long\ncontexts, and keyphrase perturbation, which mitigates reliance on specific\nkeyphrases. Compared with previous works that are only designed for\nencoder-based PLMs, Citss is carefully developed to be compatible with both\nencoder-based PLMs and decoder-based LLMs, to embrace the benefits of enlarged\npretraining. Experiments with three benchmark datasets with both encoder-based\nPLMs and decoder-based LLMs demonstrate our superiority compared to the\nprevious state of the art. Our code is available at: github.com/LITONG99/Citss"
                },
                "authors": [
                    {
                        "name": "Tong Li"
                    },
                    {
                        "name": "Jiachuan Wang"
                    },
                    {
                        "name": "Yongqi Zhang"
                    },
                    {
                        "name": "Shuangyin Li"
                    },
                    {
                        "name": "Lei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lei Chen"
                },
                "author": "Lei Chen",
                "arxiv_comment": "Manuscripts, accepted to KDD 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14471v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14471v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14469v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14469v1",
                "updated": "2025-05-20T15:05:03Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    15,
                    5,
                    3,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T15:05:03Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    15,
                    5,
                    3,
                    1,
                    140,
                    0
                ],
                "title": "Attributional Safety Failures in Large Language Models under Code-Mixed\n  Perturbations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attributional Safety Failures in Large Language Models under Code-Mixed\n  Perturbations"
                },
                "summary": "Recent advancements in LLMs have raised significant safety concerns,\nparticularly when dealing with code-mixed inputs and outputs. Our study\nsystematically investigates the increased susceptibility of LLMs to produce\nunsafe outputs from code-mixed prompts compared to monolingual English prompts.\nUtilizing explainability methods, we dissect the internal attribution shifts\ncausing model's harmful behaviors. In addition, we explore cultural dimensions\nby distinguishing between universally unsafe and culturally-specific unsafe\nqueries. This paper presents novel experimental insights, clarifying the\nmechanisms driving this phenomenon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in LLMs have raised significant safety concerns,\nparticularly when dealing with code-mixed inputs and outputs. Our study\nsystematically investigates the increased susceptibility of LLMs to produce\nunsafe outputs from code-mixed prompts compared to monolingual English prompts.\nUtilizing explainability methods, we dissect the internal attribution shifts\ncausing model's harmful behaviors. In addition, we explore cultural dimensions\nby distinguishing between universally unsafe and culturally-specific unsafe\nqueries. This paper presents novel experimental insights, clarifying the\nmechanisms driving this phenomenon."
                },
                "authors": [
                    {
                        "name": "Somnath Banerjee"
                    },
                    {
                        "name": "Pratyush Chatterjee"
                    },
                    {
                        "name": "Shanu Kumar"
                    },
                    {
                        "name": "Sayan Layek"
                    },
                    {
                        "name": "Parag Agrawal"
                    },
                    {
                        "name": "Rima Hazra"
                    },
                    {
                        "name": "Animesh Mukherjee"
                    }
                ],
                "author_detail": {
                    "name": "Animesh Mukherjee"
                },
                "author": "Animesh Mukherjee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14469v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14469v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14468v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14468v1",
                "updated": "2025-05-20T15:04:17Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    15,
                    4,
                    17,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T15:04:17Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    15,
                    4,
                    17,
                    1,
                    140,
                    0
                ],
                "title": "ServerlessLoRA: Minimizing Latency and Cost in Serverless Inference for\n  LoRA-Based LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ServerlessLoRA: Minimizing Latency and Cost in Serverless Inference for\n  LoRA-Based LLMs"
                },
                "summary": "Serverless computing has grown rapidly for serving Large Language Model (LLM)\ninference due to its pay-as-you-go pricing, fine-grained GPU usage, and rapid\nscaling. However, our analysis reveals that current serverless can effectively\nserve general LLM but fail with Low-Rank Adaptation (LoRA) inference due to\nthree key limitations: 1) massive parameter redundancy among functions where\n99% of weights are unnecessarily duplicated, 2) costly artifact loading latency\nbeyond LLM loading, and 3) magnified resource contention when serving multiple\nLoRA LLMs. These inefficiencies lead to massive GPU wastage, increased\nTime-To-First-Token (TTFT), and high monetary costs.\n  We propose ServerlessLoRA, a novel serverless inference system designed for\nfaster and cheaper LoRA LLM serving. ServerlessLoRA enables secure backbone LLM\nsharing across isolated LoRA functions to reduce redundancy. We design a\npre-loading method that pre-loads comprehensive LoRA artifacts to minimize\ncold-start latency. Furthermore, ServerlessLoRA employs contention aware\nbatching and offloading to mitigate GPU resource conflicts during bursty\nworkloads. Experiment on industrial workloads demonstrates that ServerlessLoRA\nreduces TTFT by up to 86% and cuts monetary costs by up to 89% compared to\nstate-of-the-art LLM inference solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serverless computing has grown rapidly for serving Large Language Model (LLM)\ninference due to its pay-as-you-go pricing, fine-grained GPU usage, and rapid\nscaling. However, our analysis reveals that current serverless can effectively\nserve general LLM but fail with Low-Rank Adaptation (LoRA) inference due to\nthree key limitations: 1) massive parameter redundancy among functions where\n99% of weights are unnecessarily duplicated, 2) costly artifact loading latency\nbeyond LLM loading, and 3) magnified resource contention when serving multiple\nLoRA LLMs. These inefficiencies lead to massive GPU wastage, increased\nTime-To-First-Token (TTFT), and high monetary costs.\n  We propose ServerlessLoRA, a novel serverless inference system designed for\nfaster and cheaper LoRA LLM serving. ServerlessLoRA enables secure backbone LLM\nsharing across isolated LoRA functions to reduce redundancy. We design a\npre-loading method that pre-loads comprehensive LoRA artifacts to minimize\ncold-start latency. Furthermore, ServerlessLoRA employs contention aware\nbatching and offloading to mitigate GPU resource conflicts during bursty\nworkloads. Experiment on industrial workloads demonstrates that ServerlessLoRA\nreduces TTFT by up to 86% and cuts monetary costs by up to 89% compared to\nstate-of-the-art LLM inference solutions."
                },
                "authors": [
                    {
                        "name": "Yifan Sui"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Hanfei Yu"
                    },
                    {
                        "name": "Yitao Hu"
                    },
                    {
                        "name": "Jianxun Li"
                    },
                    {
                        "name": "Hao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Wang"
                },
                "author": "Hao Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14468v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14468v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03680v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03680v3",
                "updated": "2025-05-20T15:04:15Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    15,
                    4,
                    15,
                    1,
                    140,
                    0
                ],
                "published": "2024-08-07T10:43:59Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    10,
                    43,
                    59,
                    2,
                    220,
                    0
                ],
                "title": "Smaller but Better: Self-Paced Knowledge Distillation for Lightweight\n  yet Effective LCMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Smaller but Better: Self-Paced Knowledge Distillation for Lightweight\n  yet Effective LCMs"
                },
                "summary": "Large code models (LCMs) have remarkably advanced the field of code\ngeneration. Despite their impressive capabilities, they still face practical\ndeployment issues, such as high inference costs, limited accessibility of\nproprietary LCMs, and adaptability issues of ultra-large LCMs. These issues\nhighlight the critical need for more accessible, lightweight yet effective\nLCMs. Knowledge distillation (KD) offers a promising solution, which transfers\nthe programming capabilities of larger, advanced LCMs to smaller, less powerful\nLCMs. In this paper, we propose a novel Self-Paced knOwledge DistillAtion\nframework, named SODA, aiming at developing lightweight yet effective student\nLCMs. SODA consists of three stages in one cycle: (1) Correct-and-Fault\nKnowledge Delivery stage aims at improving the student models capability to\nrecognize errors while ensuring its basic programming skill during the\nknowledge transferring, which involves correctness-aware supervised learning\nand fault-aware contrastive learning methods. (2) Multi-View Feedback stage\naims at measuring the quality of results generated by the student model from\ntwo views, including model-based and static tool-based measurement, for\nidentifying the difficult questions. (3) Feedback-based Knowledge Update stage\naims at updating the student model adaptively by generating new questions at\ndifferent difficulty levels, in which the difficulty levels are categorized\nbased on the feedback in the second stage. Experimental results show that SODA\nimproves the student model by 65.96% in terms of average Pass@1, outperforming\nthe best baseline by 29.85%. Based on the SODA framework, we develop SodaCoder,\na series of lightweight yet effective LCMs, which outperform 15 LCMs with less\nthan or equal to 16B parameters. Notably, SodaCoder-DS-6.7B, built on\nDeepseekCoder-6.7B, even surpasses the prominent ChatGPT on average Pass@1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large code models (LCMs) have remarkably advanced the field of code\ngeneration. Despite their impressive capabilities, they still face practical\ndeployment issues, such as high inference costs, limited accessibility of\nproprietary LCMs, and adaptability issues of ultra-large LCMs. These issues\nhighlight the critical need for more accessible, lightweight yet effective\nLCMs. Knowledge distillation (KD) offers a promising solution, which transfers\nthe programming capabilities of larger, advanced LCMs to smaller, less powerful\nLCMs. In this paper, we propose a novel Self-Paced knOwledge DistillAtion\nframework, named SODA, aiming at developing lightweight yet effective student\nLCMs. SODA consists of three stages in one cycle: (1) Correct-and-Fault\nKnowledge Delivery stage aims at improving the student models capability to\nrecognize errors while ensuring its basic programming skill during the\nknowledge transferring, which involves correctness-aware supervised learning\nand fault-aware contrastive learning methods. (2) Multi-View Feedback stage\naims at measuring the quality of results generated by the student model from\ntwo views, including model-based and static tool-based measurement, for\nidentifying the difficult questions. (3) Feedback-based Knowledge Update stage\naims at updating the student model adaptively by generating new questions at\ndifferent difficulty levels, in which the difficulty levels are categorized\nbased on the feedback in the second stage. Experimental results show that SODA\nimproves the student model by 65.96% in terms of average Pass@1, outperforming\nthe best baseline by 29.85%. Based on the SODA framework, we develop SodaCoder,\na series of lightweight yet effective LCMs, which outperform 15 LCMs with less\nthan or equal to 16B parameters. Notably, SodaCoder-DS-6.7B, built on\nDeepseekCoder-6.7B, even surpasses the prominent ChatGPT on average Pass@1."
                },
                "authors": [
                    {
                        "name": "Yujia Chen"
                    },
                    {
                        "name": "Yang Ye"
                    },
                    {
                        "name": "Zhongqi Li"
                    },
                    {
                        "name": "Yuchi Ma"
                    },
                    {
                        "name": "Cuiyun Gao"
                    }
                ],
                "author_detail": {
                    "name": "Cuiyun Gao"
                },
                "author": "Cuiyun Gao",
                "arxiv_comment": "Accepted by FSE 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03680v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03680v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10657v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10657v2",
                "updated": "2025-05-20T14:59:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    14,
                    59,
                    21,
                    1,
                    140,
                    0
                ],
                "published": "2025-03-08T04:07:07Z",
                "published_parsed": [
                    2025,
                    3,
                    8,
                    4,
                    7,
                    7,
                    5,
                    67,
                    0
                ],
                "title": "RouterEval: A Comprehensive Benchmark for Routing LLMs to Explore\n  Model-level Scaling Up in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RouterEval: A Comprehensive Benchmark for Routing LLMs to Explore\n  Model-level Scaling Up in LLMs"
                },
                "summary": "Routing large language models (LLMs) is a new paradigm that uses a router to\nrecommend the best LLM from a pool of candidates for a given input. In this\npaper, our comprehensive analysis with more than 8,500 LLMs reveals a novel\nmodel-level scaling up phenomenon in Routing LLMs, i.e., a capable router can\nsignificantly enhance the performance of this paradigm as the number of\ncandidates increases. This improvement can even surpass the performance of the\nbest single model in the pool and many existing strong LLMs, confirming it a\nhighly promising paradigm. However, the lack of comprehensive and open-source\nbenchmarks for Routing LLMs has hindered the development of routers. In this\npaper, we introduce RouterEval, a benchmark tailored for router research, which\nincludes over 200,000,000 performance records for 12 popular LLM evaluations\nacross various areas such as commonsense reasoning, semantic understanding,\netc., based on over 8,500 various LLMs. Using RouterEval, extensive evaluations\nof existing Routing LLM methods reveal that most still have significant room\nfor improvement. See https://github.com/MilkThink-Lab/RouterEval for all data,\ncode and tutorial.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Routing large language models (LLMs) is a new paradigm that uses a router to\nrecommend the best LLM from a pool of candidates for a given input. In this\npaper, our comprehensive analysis with more than 8,500 LLMs reveals a novel\nmodel-level scaling up phenomenon in Routing LLMs, i.e., a capable router can\nsignificantly enhance the performance of this paradigm as the number of\ncandidates increases. This improvement can even surpass the performance of the\nbest single model in the pool and many existing strong LLMs, confirming it a\nhighly promising paradigm. However, the lack of comprehensive and open-source\nbenchmarks for Routing LLMs has hindered the development of routers. In this\npaper, we introduce RouterEval, a benchmark tailored for router research, which\nincludes over 200,000,000 performance records for 12 popular LLM evaluations\nacross various areas such as commonsense reasoning, semantic understanding,\netc., based on over 8,500 various LLMs. Using RouterEval, extensive evaluations\nof existing Routing LLM methods reveal that most still have significant room\nfor improvement. See https://github.com/MilkThink-Lab/RouterEval for all data,\ncode and tutorial."
                },
                "authors": [
                    {
                        "name": "Zhongzhan Huang"
                    },
                    {
                        "name": "Guoming Ling"
                    },
                    {
                        "name": "Yupei Lin"
                    },
                    {
                        "name": "Yandong Chen"
                    },
                    {
                        "name": "Shanshan Zhong"
                    },
                    {
                        "name": "Hefeng Wu"
                    },
                    {
                        "name": "Liang Lin"
                    }
                ],
                "author_detail": {
                    "name": "Liang Lin"
                },
                "author": "Liang Lin",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10657v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10657v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13346v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13346v2",
                "updated": "2025-05-20T14:57:18Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    14,
                    57,
                    18,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-19T16:50:35Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    16,
                    50,
                    35,
                    0,
                    139,
                    0
                ],
                "title": "J4R: Learning to Judge with Equivalent Initial State Group Relative\n  Policy Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "J4R: Learning to Judge with Equivalent Initial State Group Relative\n  Policy Optimization"
                },
                "summary": "To keep pace with the increasing pace of large language models (LLM)\ndevelopment, model output evaluation has transitioned away from time-consuming\nhuman evaluation to automatic evaluation, where LLMs themselves are tasked with\nassessing and critiquing other model outputs. LLM-as-judge models are a class\nof generative evaluators that excel in evaluating relatively simple domains,\nlike chat quality, but struggle in reasoning intensive domains where model\nresponses contain more substantive and challenging content. To remedy existing\njudge shortcomings, we explore training judges with reinforcement learning\n(RL). We make three key contributions: (1) We propose the Equivalent Initial\nState Group Relative Policy Optimization (EIS-GRPO) algorithm, which allows us\nto train our judge to be robust to positional biases that arise in more complex\nevaluation settings. (2) We introduce ReasoningJudgeBench, a benchmark that\nevaluates judges in diverse reasoning settings not covered by prior work. (3)\nWe train Judge for Reasoning (J4R), a 7B judge trained with EIS-GRPO that\noutperforms GPT-4o and the next best small judge by 6.7% and 9%, matching or\nexceeding the performance of larger GRPO-trained judges on both JudgeBench and\nReasoningJudgeBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To keep pace with the increasing pace of large language models (LLM)\ndevelopment, model output evaluation has transitioned away from time-consuming\nhuman evaluation to automatic evaluation, where LLMs themselves are tasked with\nassessing and critiquing other model outputs. LLM-as-judge models are a class\nof generative evaluators that excel in evaluating relatively simple domains,\nlike chat quality, but struggle in reasoning intensive domains where model\nresponses contain more substantive and challenging content. To remedy existing\njudge shortcomings, we explore training judges with reinforcement learning\n(RL). We make three key contributions: (1) We propose the Equivalent Initial\nState Group Relative Policy Optimization (EIS-GRPO) algorithm, which allows us\nto train our judge to be robust to positional biases that arise in more complex\nevaluation settings. (2) We introduce ReasoningJudgeBench, a benchmark that\nevaluates judges in diverse reasoning settings not covered by prior work. (3)\nWe train Judge for Reasoning (J4R), a 7B judge trained with EIS-GRPO that\noutperforms GPT-4o and the next best small judge by 6.7% and 9%, matching or\nexceeding the performance of larger GRPO-trained judges on both JudgeBench and\nReasoningJudgeBench."
                },
                "authors": [
                    {
                        "name": "Austin Xu"
                    },
                    {
                        "name": "Yilun Zhou"
                    },
                    {
                        "name": "Xuan-Phi Nguyen"
                    },
                    {
                        "name": "Caiming Xiong"
                    },
                    {
                        "name": "Shafiq Joty"
                    }
                ],
                "author_detail": {
                    "name": "Shafiq Joty"
                },
                "author": "Shafiq Joty",
                "arxiv_comment": "25 pages, 4 figures, 6 tables. To be updated with links for\n  code/benchmark",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13346v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13346v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14460v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14460v1",
                "updated": "2025-05-20T14:56:50Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    14,
                    56,
                    50,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T14:56:50Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    14,
                    56,
                    50,
                    1,
                    140,
                    0
                ],
                "title": "VisualQuality-R1: Reasoning-Induced Image Quality Assessment via\n  Reinforcement Learning to Rank",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VisualQuality-R1: Reasoning-Induced Image Quality Assessment via\n  Reinforcement Learning to Rank"
                },
                "summary": "DeepSeek-R1 has demonstrated remarkable effectiveness in incentivizing\nreasoning and generalization capabilities of large language models (LLMs)\nthrough reinforcement learning. Nevertheless, the potential of\nreasoning-induced computational modeling has not been thoroughly explored in\nthe context of image quality assessment (IQA), a task critically dependent on\nvisual reasoning. In this paper, we introduce VisualQuality-R1, a\nreasoning-induced no-reference IQA (NR-IQA) model, and we train it with\nreinforcement learning to rank, a learning algorithm tailored to the\nintrinsically relative nature of visual quality. Specifically, for a pair of\nimages, we employ group relative policy optimization to generate multiple\nquality scores for each image. These estimates are then used to compute\ncomparative probabilities of one image having higher quality than the other\nunder the Thurstone model. Rewards for each quality estimate are defined using\ncontinuous fidelity measures rather than discretized binary labels. Extensive\nexperiments show that the proposed VisualQuality-R1 consistently outperforms\ndiscriminative deep learning-based NR-IQA models as well as a recent\nreasoning-induced quality regression method. Moreover, VisualQuality-R1 is\ncapable of generating contextually rich, human-aligned quality descriptions,\nand supports multi-dataset training without requiring perceptual scale\nrealignment. These features make VisualQuality-R1 especially well-suited for\nreliably measuring progress in a wide range of image processing tasks like\nsuper-resolution and image generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepSeek-R1 has demonstrated remarkable effectiveness in incentivizing\nreasoning and generalization capabilities of large language models (LLMs)\nthrough reinforcement learning. Nevertheless, the potential of\nreasoning-induced computational modeling has not been thoroughly explored in\nthe context of image quality assessment (IQA), a task critically dependent on\nvisual reasoning. In this paper, we introduce VisualQuality-R1, a\nreasoning-induced no-reference IQA (NR-IQA) model, and we train it with\nreinforcement learning to rank, a learning algorithm tailored to the\nintrinsically relative nature of visual quality. Specifically, for a pair of\nimages, we employ group relative policy optimization to generate multiple\nquality scores for each image. These estimates are then used to compute\ncomparative probabilities of one image having higher quality than the other\nunder the Thurstone model. Rewards for each quality estimate are defined using\ncontinuous fidelity measures rather than discretized binary labels. Extensive\nexperiments show that the proposed VisualQuality-R1 consistently outperforms\ndiscriminative deep learning-based NR-IQA models as well as a recent\nreasoning-induced quality regression method. Moreover, VisualQuality-R1 is\ncapable of generating contextually rich, human-aligned quality descriptions,\nand supports multi-dataset training without requiring perceptual scale\nrealignment. These features make VisualQuality-R1 especially well-suited for\nreliably measuring progress in a wide range of image processing tasks like\nsuper-resolution and image generation."
                },
                "authors": [
                    {
                        "name": "Tianhe Wu"
                    },
                    {
                        "name": "Jian Zou"
                    },
                    {
                        "name": "Jie Liang"
                    },
                    {
                        "name": "Lei Zhang"
                    },
                    {
                        "name": "Kede Ma"
                    }
                ],
                "author_detail": {
                    "name": "Kede Ma"
                },
                "author": "Kede Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14460v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14460v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14454v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14454v1",
                "updated": "2025-05-20T14:52:31Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    14,
                    52,
                    31,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T14:52:31Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    14,
                    52,
                    31,
                    1,
                    140,
                    0
                ],
                "title": "Video Compression Commander: Plug-and-Play Inference Acceleration for\n  Video Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Compression Commander: Plug-and-Play Inference Acceleration for\n  Video Large Language Models"
                },
                "summary": "Video large language models (VideoLLM) excel at video understanding, but face\nefficiency challenges due to the quadratic complexity of abundant visual\ntokens. Our systematic analysis of token compression methods for VideoLLMs\nreveals two critical issues: (i) overlooking distinctive visual signals across\nframes, leading to information loss; (ii) suffering from implementation\nconstraints, causing incompatibility with modern architectures or efficient\noperators. To address these challenges, we distill three design principles for\nVideoLLM token compression and propose a plug-and-play inference acceleration\nframework \"Video Compression Commander\" (VidCom2). By quantifying each frame's\nuniqueness, VidCom2 adaptively adjusts compression intensity across frames,\neffectively preserving essential information while reducing redundancy in video\nsequences. Extensive experiments across various VideoLLMs and benchmarks\ndemonstrate the superior performance and efficiency of our VidCom2. With only\n25% visual tokens, VidCom2 achieves 99.6% of the original performance on\nLLaVA-OV while reducing 70.8% of the LLM generation latency. Notably, our Frame\nCompression Adjustment strategy is compatible with other token compression\nmethods to further improve their performance. Our code is available at\nhttps://github.com/xuyang-liu16/VidCom2.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video large language models (VideoLLM) excel at video understanding, but face\nefficiency challenges due to the quadratic complexity of abundant visual\ntokens. Our systematic analysis of token compression methods for VideoLLMs\nreveals two critical issues: (i) overlooking distinctive visual signals across\nframes, leading to information loss; (ii) suffering from implementation\nconstraints, causing incompatibility with modern architectures or efficient\noperators. To address these challenges, we distill three design principles for\nVideoLLM token compression and propose a plug-and-play inference acceleration\nframework \"Video Compression Commander\" (VidCom2). By quantifying each frame's\nuniqueness, VidCom2 adaptively adjusts compression intensity across frames,\neffectively preserving essential information while reducing redundancy in video\nsequences. Extensive experiments across various VideoLLMs and benchmarks\ndemonstrate the superior performance and efficiency of our VidCom2. With only\n25% visual tokens, VidCom2 achieves 99.6% of the original performance on\nLLaVA-OV while reducing 70.8% of the LLM generation latency. Notably, our Frame\nCompression Adjustment strategy is compatible with other token compression\nmethods to further improve their performance. Our code is available at\nhttps://github.com/xuyang-liu16/VidCom2."
                },
                "authors": [
                    {
                        "name": "Xuyang Liu"
                    },
                    {
                        "name": "Yiyu Wang"
                    },
                    {
                        "name": "Junpeng Ma"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "Our code is available at https://github.com/xuyang-liu16/VidCom2",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14454v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14454v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07078v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07078v2",
                "updated": "2025-05-20T14:51:24Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    14,
                    51,
                    24,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-11T18:02:21Z",
                "published_parsed": [
                    2025,
                    5,
                    11,
                    18,
                    2,
                    21,
                    6,
                    131,
                    0
                ],
                "title": "Can LLM-based Financial Investing Strategies Outperform the Market in\n  Long Run?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLM-based Financial Investing Strategies Outperform the Market in\n  Long Run?"
                },
                "summary": "Large Language Models (LLMs) have recently been leveraged for asset pricing\ntasks and stock trading applications, enabling AI agents to generate investment\ndecisions from unstructured financial data. However, most evaluations of LLM\ntiming-based investing strategies are conducted on narrow timeframes and\nlimited stock universes, overstating effectiveness due to survivorship and\ndata-snooping biases. We critically assess their generalizability and\nrobustness by proposing FINSABER, a backtesting framework evaluating\ntiming-based strategies across longer periods and a larger universe of symbols.\nSystematic backtests over two decades and 100+ symbols reveal that previously\nreported LLM advantages deteriorate significantly under broader cross-section\nand over a longer-term evaluation. Our market regime analysis further\ndemonstrates that LLM strategies are overly conservative in bull markets,\nunderperforming passive benchmarks, and overly aggressive in bear markets,\nincurring heavy losses. These findings highlight the need to develop LLM\nstrategies that are able to prioritise trend detection and regime-aware risk\ncontrols over mere scaling of framework complexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have recently been leveraged for asset pricing\ntasks and stock trading applications, enabling AI agents to generate investment\ndecisions from unstructured financial data. However, most evaluations of LLM\ntiming-based investing strategies are conducted on narrow timeframes and\nlimited stock universes, overstating effectiveness due to survivorship and\ndata-snooping biases. We critically assess their generalizability and\nrobustness by proposing FINSABER, a backtesting framework evaluating\ntiming-based strategies across longer periods and a larger universe of symbols.\nSystematic backtests over two decades and 100+ symbols reveal that previously\nreported LLM advantages deteriorate significantly under broader cross-section\nand over a longer-term evaluation. Our market regime analysis further\ndemonstrates that LLM strategies are overly conservative in bull markets,\nunderperforming passive benchmarks, and overly aggressive in bear markets,\nincurring heavy losses. These findings highlight the need to develop LLM\nstrategies that are able to prioritise trend detection and regime-aware risk\ncontrols over mere scaling of framework complexity."
                },
                "authors": [
                    {
                        "name": "Weixian Waylon Li"
                    },
                    {
                        "name": "Hyeonjun Kim"
                    },
                    {
                        "name": "Mihai Cucuringu"
                    },
                    {
                        "name": "Tiejun Ma"
                    }
                ],
                "author_detail": {
                    "name": "Tiejun Ma"
                },
                "author": "Tiejun Ma",
                "arxiv_comment": "14 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.07078v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07078v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.TR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.TR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11936v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11936v3",
                "updated": "2025-05-20T14:45:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    14,
                    45,
                    21,
                    1,
                    140,
                    0
                ],
                "published": "2024-12-16T16:21:41Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    16,
                    21,
                    41,
                    0,
                    351,
                    0
                ],
                "title": "A Survey of Mathematical Reasoning in the Era of Multimodal Large\n  Language Model: Benchmark, Method & Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of Mathematical Reasoning in the Era of Multimodal Large\n  Language Model: Benchmark, Method & Challenges"
                },
                "summary": "Mathematical reasoning, a core aspect of human cognition, is vital across\nmany domains, from educational problem-solving to scientific advancements. As\nartificial general intelligence (AGI) progresses, integrating large language\nmodels (LLMs) with mathematical reasoning tasks is becoming increasingly\nsignificant. This survey provides the first comprehensive analysis of\nmathematical reasoning in the era of multimodal large language models (MLLMs).\nWe review over 200 studies published since 2021, and examine the\nstate-of-the-art developments in Math-LLMs, with a focus on multimodal\nsettings. We categorize the field into three dimensions: benchmarks,\nmethodologies, and challenges. In particular, we explore multimodal\nmathematical reasoning pipeline, as well as the role of (M)LLMs and the\nassociated methodologies. Finally, we identify five major challenges hindering\nthe realization of AGI in this domain, offering insights into the future\ndirection for enhancing multimodal reasoning capabilities. This survey serves\nas a critical resource for the research community in advancing the capabilities\nof LLMs to tackle complex multimodal reasoning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mathematical reasoning, a core aspect of human cognition, is vital across\nmany domains, from educational problem-solving to scientific advancements. As\nartificial general intelligence (AGI) progresses, integrating large language\nmodels (LLMs) with mathematical reasoning tasks is becoming increasingly\nsignificant. This survey provides the first comprehensive analysis of\nmathematical reasoning in the era of multimodal large language models (MLLMs).\nWe review over 200 studies published since 2021, and examine the\nstate-of-the-art developments in Math-LLMs, with a focus on multimodal\nsettings. We categorize the field into three dimensions: benchmarks,\nmethodologies, and challenges. In particular, we explore multimodal\nmathematical reasoning pipeline, as well as the role of (M)LLMs and the\nassociated methodologies. Finally, we identify five major challenges hindering\nthe realization of AGI in this domain, offering insights into the future\ndirection for enhancing multimodal reasoning capabilities. This survey serves\nas a critical resource for the research community in advancing the capabilities\nof LLMs to tackle complex multimodal reasoning tasks."
                },
                "authors": [
                    {
                        "name": "Yibo Yan"
                    },
                    {
                        "name": "Jiamin Su"
                    },
                    {
                        "name": "Jianxiang He"
                    },
                    {
                        "name": "Fangteng Fu"
                    },
                    {
                        "name": "Xu Zheng"
                    },
                    {
                        "name": "Yuanhuiyi Lyu"
                    },
                    {
                        "name": "Kun Wang"
                    },
                    {
                        "name": "Shen Wang"
                    },
                    {
                        "name": "Qingsong Wen"
                    },
                    {
                        "name": "Xuming Hu"
                    }
                ],
                "author_detail": {
                    "name": "Xuming Hu"
                },
                "author": "Xuming Hu",
                "arxiv_comment": "Accepted by The 63rd Annual Meeting of the Association for\n  Computational Linguistics (ACL Findings 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11936v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11936v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14442v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14442v1",
                "updated": "2025-05-20T14:43:41Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    14,
                    43,
                    41,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T14:43:41Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    14,
                    43,
                    41,
                    1,
                    140,
                    0
                ],
                "title": "Creative Preference Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Creative Preference Optimization"
                },
                "summary": "While Large Language Models (LLMs) have demonstrated impressive performance\nacross natural language generation tasks, their ability to generate truly\ncreative content-characterized by novelty, diversity, surprise, and\nquality-remains limited. Existing methods for enhancing LLM creativity often\nfocus narrowly on diversity or specific tasks, failing to address creativity's\nmultifaceted nature in a generalizable way. In this work, we propose Creative\nPreference Optimization (CrPO), a novel alignment method that injects signals\nfrom multiple creativity dimensions into the preference optimization objective\nin a modular fashion. We train and evaluate creativity-augmented versions of\nseveral models using CrPO and MuCE, a new large-scale human preference dataset\nspanning over 200,000 human-generated responses and ratings from more than 30\npsychological creativity assessments. Our models outperform strong baselines,\nincluding GPT-4o, on both automated and human evaluations, producing more\nnovel, diverse, and surprising generations while maintaining high output\nquality. Additional evaluations on NoveltyBench further confirm the\ngeneralizability of our approach. Together, our results demonstrate that\ndirectly optimizing for creativity within preference frameworks is a promising\ndirection for advancing the creative capabilities of LLMs without compromising\noutput quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Large Language Models (LLMs) have demonstrated impressive performance\nacross natural language generation tasks, their ability to generate truly\ncreative content-characterized by novelty, diversity, surprise, and\nquality-remains limited. Existing methods for enhancing LLM creativity often\nfocus narrowly on diversity or specific tasks, failing to address creativity's\nmultifaceted nature in a generalizable way. In this work, we propose Creative\nPreference Optimization (CrPO), a novel alignment method that injects signals\nfrom multiple creativity dimensions into the preference optimization objective\nin a modular fashion. We train and evaluate creativity-augmented versions of\nseveral models using CrPO and MuCE, a new large-scale human preference dataset\nspanning over 200,000 human-generated responses and ratings from more than 30\npsychological creativity assessments. Our models outperform strong baselines,\nincluding GPT-4o, on both automated and human evaluations, producing more\nnovel, diverse, and surprising generations while maintaining high output\nquality. Additional evaluations on NoveltyBench further confirm the\ngeneralizability of our approach. Together, our results demonstrate that\ndirectly optimizing for creativity within preference frameworks is a promising\ndirection for advancing the creative capabilities of LLMs without compromising\noutput quality."
                },
                "authors": [
                    {
                        "name": "Mete Ismayilzada"
                    },
                    {
                        "name": "Antonio Laverghetta Jr."
                    },
                    {
                        "name": "Simone A. Luchini"
                    },
                    {
                        "name": "Reet Patel"
                    },
                    {
                        "name": "Antoine Bosselut"
                    },
                    {
                        "name": "Lonneke van der Plas"
                    },
                    {
                        "name": "Roger Beaty"
                    }
                ],
                "author_detail": {
                    "name": "Roger Beaty"
                },
                "author": "Roger Beaty",
                "arxiv_comment": "27 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14442v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14442v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14438v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14438v1",
                "updated": "2025-05-20T14:42:20Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    14,
                    42,
                    20,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T14:42:20Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    14,
                    42,
                    20,
                    1,
                    140,
                    0
                ],
                "title": "S2SBench: A Benchmark for Quantifying Intelligence Degradation in\n  Speech-to-Speech Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "S2SBench: A Benchmark for Quantifying Intelligence Degradation in\n  Speech-to-Speech Large Language Models"
                },
                "summary": "End-to-end speech large language models ((LLMs)) extend the capabilities of\ntext-based models to directly process and generate audio tokens. However, this\noften leads to a decline in reasoning and generation performance compared to\ntext input, a phenomenon referred to as intelligence degradation. To\nsystematically evaluate this gap, we propose S2SBench, a benchmark designed to\nquantify performance degradation in Speech LLMs. It includes diagnostic\ndatasets targeting sentence continuation and commonsense reasoning under audio\ninput. We further introduce a pairwise evaluation protocol based on perplexity\ndifferences between plausible and implausible samples to measure degradation\nrelative to text input. We apply S2SBench to analyze the training process of\nBaichuan-Audio, which further demonstrates the benchmark's effectiveness. All\ndatasets and evaluation code are available at\nhttps://github.com/undobug/S2SBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "End-to-end speech large language models ((LLMs)) extend the capabilities of\ntext-based models to directly process and generate audio tokens. However, this\noften leads to a decline in reasoning and generation performance compared to\ntext input, a phenomenon referred to as intelligence degradation. To\nsystematically evaluate this gap, we propose S2SBench, a benchmark designed to\nquantify performance degradation in Speech LLMs. It includes diagnostic\ndatasets targeting sentence continuation and commonsense reasoning under audio\ninput. We further introduce a pairwise evaluation protocol based on perplexity\ndifferences between plausible and implausible samples to measure degradation\nrelative to text input. We apply S2SBench to analyze the training process of\nBaichuan-Audio, which further demonstrates the benchmark's effectiveness. All\ndatasets and evaluation code are available at\nhttps://github.com/undobug/S2SBench."
                },
                "authors": [
                    {
                        "name": "Yuanbo Fang"
                    },
                    {
                        "name": "Haoze Sun"
                    },
                    {
                        "name": "Jun Liu"
                    },
                    {
                        "name": "Tao Zhang"
                    },
                    {
                        "name": "Zenan Zhou"
                    },
                    {
                        "name": "Weipeng Chen"
                    },
                    {
                        "name": "Xiaofen Xing"
                    },
                    {
                        "name": "Xiangmin Xu"
                    }
                ],
                "author_detail": {
                    "name": "Xiangmin Xu"
                },
                "author": "Xiangmin Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14438v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14438v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14436v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14436v1",
                "updated": "2025-05-20T14:42:03Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    14,
                    42,
                    3,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T14:42:03Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    14,
                    42,
                    3,
                    1,
                    140,
                    0
                ],
                "title": "Neural Incompatibility: The Unbridgeable Gap of Cross-Scale Parametric\n  Knowledge Transfer in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Incompatibility: The Unbridgeable Gap of Cross-Scale Parametric\n  Knowledge Transfer in Large Language Models"
                },
                "summary": "Large Language Models (LLMs) offer a transparent brain with accessible\nparameters that encode extensive knowledge, which can be analyzed, located and\ntransferred. Consequently, a key research challenge is to transcend traditional\nknowledge transfer paradigms rooted in symbolic language and achieve genuine\nParametric Knowledge Transfer (PKT). Significantly, exploring effective methods\nfor transferring knowledge across LLMs of different scales through parameters\npresents an intriguing and valuable research direction. In this paper, we first\ndemonstrate $\\textbf{Alignment}$ in parametric space is the fundamental\nprerequisite to achieve successful cross-scale PKT. We redefine the previously\nexplored knowledge transfer as Post-Align PKT (PostPKT), which utilizes\nextracted parameters for LoRA initialization and requires subsequent fine-tune\nfor alignment. Hence, to reduce cost for further fine-tuning, we introduce a\nnovel Pre-Align PKT (PrePKT) paradigm and propose a solution called\n$\\textbf{LaTen}$\n($\\textbf{L}$oc$\\textbf{a}$te-$\\textbf{T}$h$\\textbf{e}$n-Alig$\\textbf{n}$) that\naligns the parametric spaces of LLMs across scales only using several training\nsteps without following training. Comprehensive experiments on four benchmarks\ndemonstrate that both PostPKT and PrePKT face challenges in achieving\nconsistently stable transfer. Through in-depth analysis, we identify\n$\\textbf{Neural Incompatibility}$ as the ethological and parametric structural\ndifferences between LLMs of varying scales, presenting fundamental challenges\nto achieving effective PKT. These findings provide fresh insights into the\nparametric architectures of LLMs and highlight promising directions for future\nresearch on efficient PKT. Our code is available at\nhttps://github.com/Trae1ounG/Neural_Incompatibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) offer a transparent brain with accessible\nparameters that encode extensive knowledge, which can be analyzed, located and\ntransferred. Consequently, a key research challenge is to transcend traditional\nknowledge transfer paradigms rooted in symbolic language and achieve genuine\nParametric Knowledge Transfer (PKT). Significantly, exploring effective methods\nfor transferring knowledge across LLMs of different scales through parameters\npresents an intriguing and valuable research direction. In this paper, we first\ndemonstrate $\\textbf{Alignment}$ in parametric space is the fundamental\nprerequisite to achieve successful cross-scale PKT. We redefine the previously\nexplored knowledge transfer as Post-Align PKT (PostPKT), which utilizes\nextracted parameters for LoRA initialization and requires subsequent fine-tune\nfor alignment. Hence, to reduce cost for further fine-tuning, we introduce a\nnovel Pre-Align PKT (PrePKT) paradigm and propose a solution called\n$\\textbf{LaTen}$\n($\\textbf{L}$oc$\\textbf{a}$te-$\\textbf{T}$h$\\textbf{e}$n-Alig$\\textbf{n}$) that\naligns the parametric spaces of LLMs across scales only using several training\nsteps without following training. Comprehensive experiments on four benchmarks\ndemonstrate that both PostPKT and PrePKT face challenges in achieving\nconsistently stable transfer. Through in-depth analysis, we identify\n$\\textbf{Neural Incompatibility}$ as the ethological and parametric structural\ndifferences between LLMs of varying scales, presenting fundamental challenges\nto achieving effective PKT. These findings provide fresh insights into the\nparametric architectures of LLMs and highlight promising directions for future\nresearch on efficient PKT. Our code is available at\nhttps://github.com/Trae1ounG/Neural_Incompatibility."
                },
                "authors": [
                    {
                        "name": "Yuqiao Tan"
                    },
                    {
                        "name": "Shizhu He"
                    },
                    {
                        "name": "Kang Liu"
                    },
                    {
                        "name": "Jun Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhao"
                },
                "author": "Jun Zhao",
                "arxiv_comment": "Accepted by ACL'25 Main. Code link:\n  https://github.com/Trae1ounG/Neural_Incompatibility",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14436v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14436v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14435v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14435v1",
                "updated": "2025-05-20T14:41:56Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    14,
                    41,
                    56,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T14:41:56Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    14,
                    41,
                    56,
                    1,
                    140,
                    0
                ],
                "title": "Choosing a Model, Shaping a Future: Comparing LLM Perspectives on\n  Sustainability and its Relationship with AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Choosing a Model, Shaping a Future: Comparing LLM Perspectives on\n  Sustainability and its Relationship with AI"
                },
                "summary": "As organizations increasingly rely on AI systems for decision support in\nsustainability contexts, it becomes critical to understand the inherent biases\nand perspectives embedded in Large Language Models (LLMs). This study\nsystematically investigates how five state-of-the-art LLMs -- Claude, DeepSeek,\nGPT, LLaMA, and Mistral - conceptualize sustainability and its relationship\nwith AI. We administered validated, psychometric sustainability-related\nquestionnaires - each 100 times per model -- to capture response patterns and\nvariability. Our findings revealed significant inter-model differences: For\nexample, GPT exhibited skepticism about the compatibility of AI and\nsustainability, whereas LLaMA demonstrated extreme techno-optimism with perfect\nscores for several Sustainable Development Goals (SDGs). Models also diverged\nin attributing institutional responsibility for AI and sustainability\nintegration, a results that holds implications for technology governance\napproaches. Our results demonstrate that model selection could substantially\ninfluence organizational sustainability strategies, highlighting the need for\nawareness of model-specific biases when deploying LLMs for\nsustainability-related decision-making.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As organizations increasingly rely on AI systems for decision support in\nsustainability contexts, it becomes critical to understand the inherent biases\nand perspectives embedded in Large Language Models (LLMs). This study\nsystematically investigates how five state-of-the-art LLMs -- Claude, DeepSeek,\nGPT, LLaMA, and Mistral - conceptualize sustainability and its relationship\nwith AI. We administered validated, psychometric sustainability-related\nquestionnaires - each 100 times per model -- to capture response patterns and\nvariability. Our findings revealed significant inter-model differences: For\nexample, GPT exhibited skepticism about the compatibility of AI and\nsustainability, whereas LLaMA demonstrated extreme techno-optimism with perfect\nscores for several Sustainable Development Goals (SDGs). Models also diverged\nin attributing institutional responsibility for AI and sustainability\nintegration, a results that holds implications for technology governance\napproaches. Our results demonstrate that model selection could substantially\ninfluence organizational sustainability strategies, highlighting the need for\nawareness of model-specific biases when deploying LLMs for\nsustainability-related decision-making."
                },
                "authors": [
                    {
                        "name": "Annika Bush"
                    },
                    {
                        "name": "Meltem Aksoy"
                    },
                    {
                        "name": "Markus Pauly"
                    },
                    {
                        "name": "Greta Ontrup"
                    }
                ],
                "author_detail": {
                    "name": "Greta Ontrup"
                },
                "author": "Greta Ontrup",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14435v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14435v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14427v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14427v1",
                "updated": "2025-05-20T14:38:34Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    14,
                    38,
                    34,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T14:38:34Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    14,
                    38,
                    34,
                    1,
                    140,
                    0
                ],
                "title": "SkyMemory: A LEO Edge Cache for Transformer Inference Optimization and\n  Scale Out",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SkyMemory: A LEO Edge Cache for Transformer Inference Optimization and\n  Scale Out"
                },
                "summary": "We expand the scope of cache memory to include LEO constellations, which are\nhighly distributed systems with thousands of satellites connected with\nfree-space optics inter-satellite links (ISL) always only one hop from any\npoint on earth. We show how to increase the number of cache hits and improve\nthe speed of inference for the important use case of LLMs. These benefits apply\nnot only to LLMs, both terrestrially hosted and on satellites, but also\ngeneralize to any cache distributed over multiple locations that needs to be\naccessed in a timely manner. We show the benefit of our key value cache (KVC)\nprotocol in simulations and present a proof-of-concept implementation of the\nprotocol for KVCs on a testbed comprising 5 Intel NUC Linux mini PCs hosting a\n19x5 constellation, with an NVIDIA Jetson Nano 8GB GPU hosting the LLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We expand the scope of cache memory to include LEO constellations, which are\nhighly distributed systems with thousands of satellites connected with\nfree-space optics inter-satellite links (ISL) always only one hop from any\npoint on earth. We show how to increase the number of cache hits and improve\nthe speed of inference for the important use case of LLMs. These benefits apply\nnot only to LLMs, both terrestrially hosted and on satellites, but also\ngeneralize to any cache distributed over multiple locations that needs to be\naccessed in a timely manner. We show the benefit of our key value cache (KVC)\nprotocol in simulations and present a proof-of-concept implementation of the\nprotocol for KVCs on a testbed comprising 5 Intel NUC Linux mini PCs hosting a\n19x5 constellation, with an NVIDIA Jetson Nano 8GB GPU hosting the LLM."
                },
                "authors": [
                    {
                        "name": "Thomas Sandholm"
                    },
                    {
                        "name": "Sayandev Mukherjee"
                    },
                    {
                        "name": "Lin Cheng"
                    },
                    {
                        "name": "Bernardo A. Huberman"
                    }
                ],
                "author_detail": {
                    "name": "Bernardo A. Huberman"
                },
                "author": "Bernardo A. Huberman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14427v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14427v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14150v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14150v2",
                "updated": "2025-05-20T14:36:36Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    14,
                    36,
                    36,
                    1,
                    140,
                    0
                ],
                "published": "2025-04-19T02:51:20Z",
                "published_parsed": [
                    2025,
                    4,
                    19,
                    2,
                    51,
                    20,
                    5,
                    109,
                    0
                ],
                "title": "Walk the Talk? Measuring the Faithfulness of Large Language Model\n  Explanations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Walk the Talk? Measuring the Faithfulness of Large Language Model\n  Explanations"
                },
                "summary": "Large language models (LLMs) are capable of generating plausible explanations\nof how they arrived at an answer to a question. However, these explanations can\nmisrepresent the model's \"reasoning\" process, i.e., they can be unfaithful.\nThis, in turn, can lead to over-trust and misuse. We introduce a new approach\nfor measuring the faithfulness of LLM explanations. First, we provide a\nrigorous definition of faithfulness. Since LLM explanations mimic human\nexplanations, they often reference high-level concepts in the input question\nthat purportedly influenced the model. We define faithfulness in terms of the\ndifference between the set of concepts that LLM explanations imply are\ninfluential and the set that truly are. Second, we present a novel method for\nestimating faithfulness that is based on: (1) using an auxiliary LLM to modify\nthe values of concepts within model inputs to create realistic counterfactuals,\nand (2) using a Bayesian hierarchical model to quantify the causal effects of\nconcepts at both the example- and dataset-level. Our experiments show that our\nmethod can be used to quantify and discover interpretable patterns of\nunfaithfulness. On a social bias task, we uncover cases where LLM explanations\nhide the influence of social bias. On a medical question answering task, we\nuncover cases where LLM explanations provide misleading claims about which\npieces of evidence influenced the model's decisions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are capable of generating plausible explanations\nof how they arrived at an answer to a question. However, these explanations can\nmisrepresent the model's \"reasoning\" process, i.e., they can be unfaithful.\nThis, in turn, can lead to over-trust and misuse. We introduce a new approach\nfor measuring the faithfulness of LLM explanations. First, we provide a\nrigorous definition of faithfulness. Since LLM explanations mimic human\nexplanations, they often reference high-level concepts in the input question\nthat purportedly influenced the model. We define faithfulness in terms of the\ndifference between the set of concepts that LLM explanations imply are\ninfluential and the set that truly are. Second, we present a novel method for\nestimating faithfulness that is based on: (1) using an auxiliary LLM to modify\nthe values of concepts within model inputs to create realistic counterfactuals,\nand (2) using a Bayesian hierarchical model to quantify the causal effects of\nconcepts at both the example- and dataset-level. Our experiments show that our\nmethod can be used to quantify and discover interpretable patterns of\nunfaithfulness. On a social bias task, we uncover cases where LLM explanations\nhide the influence of social bias. On a medical question answering task, we\nuncover cases where LLM explanations provide misleading claims about which\npieces of evidence influenced the model's decisions."
                },
                "authors": [
                    {
                        "name": "Katie Matton"
                    },
                    {
                        "name": "Robert Osazuwa Ness"
                    },
                    {
                        "name": "John Guttag"
                    },
                    {
                        "name": "Emre Kıcıman"
                    }
                ],
                "author_detail": {
                    "name": "Emre Kıcıman"
                },
                "author": "Emre Kıcıman",
                "arxiv_comment": "66 pages, 14 figures, 40 tables; ICLR 2025 (spotlight) camera ready",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14150v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14150v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13160v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13160v3",
                "updated": "2025-05-20T14:34:54Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    14,
                    34,
                    54,
                    1,
                    140,
                    0
                ],
                "published": "2025-02-16T03:02:48Z",
                "published_parsed": [
                    2025,
                    2,
                    16,
                    3,
                    2,
                    48,
                    6,
                    47,
                    0
                ],
                "title": "Attention Mechanism for LLM-based Agents Dynamic Diffusion under\n  Information Asymmetry",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention Mechanism for LLM-based Agents Dynamic Diffusion under\n  Information Asymmetry"
                },
                "summary": "Large language models have been used to simulate human society using\nmulti-agent systems. Most current social simulation research emphasizes\ninteractive behaviors in fixed environments, ignoring information opacity,\nrelationship variability, and diffusion diversity. In this paper, we first\npropose a general framework for exploring multi-agent information diffusion. We\nidentified LLMs' deficiency in the perception and utilization of social\nrelationships, as well as diverse actions. Then, we designed a dynamic\nattention mechanism to help agents allocate attention to different information,\naddressing the limitations of the LLM attention mechanism. Agents start by\nresponding to external information stimuli within a five-agent group,\nincreasing group size and forming information circles while developing\nrelationships and sharing information. Additionally, we explore the information\ndiffusion features in the asymmetric open environment by observing the\nevolution of information gaps, diffusion patterns, and the accumulation of\nsocial capital, which are closely linked to psychological, sociological, and\ncommunication theories.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have been used to simulate human society using\nmulti-agent systems. Most current social simulation research emphasizes\ninteractive behaviors in fixed environments, ignoring information opacity,\nrelationship variability, and diffusion diversity. In this paper, we first\npropose a general framework for exploring multi-agent information diffusion. We\nidentified LLMs' deficiency in the perception and utilization of social\nrelationships, as well as diverse actions. Then, we designed a dynamic\nattention mechanism to help agents allocate attention to different information,\naddressing the limitations of the LLM attention mechanism. Agents start by\nresponding to external information stimuli within a five-agent group,\nincreasing group size and forming information circles while developing\nrelationships and sharing information. Additionally, we explore the information\ndiffusion features in the asymmetric open environment by observing the\nevolution of information gaps, diffusion patterns, and the accumulation of\nsocial capital, which are closely linked to psychological, sociological, and\ncommunication theories."
                },
                "authors": [
                    {
                        "name": "Yiwen Zhang"
                    },
                    {
                        "name": "Yifu Wu"
                    },
                    {
                        "name": "Wenyue Hua"
                    },
                    {
                        "name": "Xiang Lu"
                    },
                    {
                        "name": "Xuming Hu"
                    }
                ],
                "author_detail": {
                    "name": "Xuming Hu"
                },
                "author": "Xuming Hu",
                "arxiv_comment": "18 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13160v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13160v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14425v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14425v1",
                "updated": "2025-05-20T14:33:29Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    14,
                    33,
                    29,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T14:33:29Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    14,
                    33,
                    29,
                    1,
                    140,
                    0
                ],
                "title": "From Templates to Natural Language: Generalization Challenges in\n  Instruction-Tuned LLMs for Spatial Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Templates to Natural Language: Generalization Challenges in\n  Instruction-Tuned LLMs for Spatial Reasoning"
                },
                "summary": "Instruction-tuned large language models (LLMs) have shown strong performance\non a variety of tasks; however, generalizing from synthetic to human-authored\ninstructions in grounded environments remains a challenge for them. In this\nwork, we study generalization challenges in spatial grounding tasks where\nmodels interpret and translate instructions for building object arrangements on\na $2.5$D grid. We fine-tune LLMs using only synthetic instructions and evaluate\ntheir performance on a benchmark dataset containing both synthetic and\nhuman-written instructions. Our results reveal that while models generalize\nwell on simple tasks, their performance degrades significantly on more complex\ntasks. We present a detailed error analysis of the gaps in instruction\ngeneralization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction-tuned large language models (LLMs) have shown strong performance\non a variety of tasks; however, generalizing from synthetic to human-authored\ninstructions in grounded environments remains a challenge for them. In this\nwork, we study generalization challenges in spatial grounding tasks where\nmodels interpret and translate instructions for building object arrangements on\na $2.5$D grid. We fine-tune LLMs using only synthetic instructions and evaluate\ntheir performance on a benchmark dataset containing both synthetic and\nhuman-written instructions. Our results reveal that while models generalize\nwell on simple tasks, their performance degrades significantly on more complex\ntasks. We present a detailed error analysis of the gaps in instruction\ngeneralization."
                },
                "authors": [
                    {
                        "name": "Chalamalasetti Kranti"
                    },
                    {
                        "name": "Sherzod Hakimov"
                    },
                    {
                        "name": "David Schlangen"
                    }
                ],
                "author_detail": {
                    "name": "David Schlangen"
                },
                "author": "David Schlangen",
                "arxiv_comment": "4 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14425v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14425v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14423v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14423v1",
                "updated": "2025-05-20T14:31:54Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    14,
                    31,
                    54,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T14:31:54Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    14,
                    31,
                    54,
                    1,
                    140,
                    0
                ],
                "title": "Scaling Low-Resource MT via Synthetic Data Generation with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Low-Resource MT via Synthetic Data Generation with LLMs"
                },
                "summary": "We investigate the potential of LLM-generated synthetic data for improving\nlow-resource machine translation (MT). Focusing on seven diverse target\nlanguages, we construct a document-level synthetic corpus from English\nEuroparl, and extend it via pivoting to 147 additional language pairs.\nAutomatic and human evaluation confirm its high overall quality. We study its\npractical application by (i) identifying effective training regimes, (ii)\ncomparing our data with the HPLT dataset, and (iii) testing its utility beyond\nEnglish-centric MT. Finally, we introduce SynOPUS, a public repository for\nsynthetic parallel datasets. Our findings show that LLM-generated synthetic\ndata, even when noisy, can substantially improve MT performance for\nlow-resource languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate the potential of LLM-generated synthetic data for improving\nlow-resource machine translation (MT). Focusing on seven diverse target\nlanguages, we construct a document-level synthetic corpus from English\nEuroparl, and extend it via pivoting to 147 additional language pairs.\nAutomatic and human evaluation confirm its high overall quality. We study its\npractical application by (i) identifying effective training regimes, (ii)\ncomparing our data with the HPLT dataset, and (iii) testing its utility beyond\nEnglish-centric MT. Finally, we introduce SynOPUS, a public repository for\nsynthetic parallel datasets. Our findings show that LLM-generated synthetic\ndata, even when noisy, can substantially improve MT performance for\nlow-resource languages."
                },
                "authors": [
                    {
                        "name": "Ona de Gibert"
                    },
                    {
                        "name": "Joseph Attieh"
                    },
                    {
                        "name": "Teemu Vahtola"
                    },
                    {
                        "name": "Mikko Aulamo"
                    },
                    {
                        "name": "Zihao Li"
                    },
                    {
                        "name": "Raúl Vázquez"
                    },
                    {
                        "name": "Tiancheng Hu"
                    },
                    {
                        "name": "Jörg Tiedemann"
                    }
                ],
                "author_detail": {
                    "name": "Jörg Tiedemann"
                },
                "author": "Jörg Tiedemann",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14423v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14423v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14422v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14422v1",
                "updated": "2025-05-20T14:31:53Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    14,
                    31,
                    53,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T14:31:53Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    14,
                    31,
                    53,
                    1,
                    140,
                    0
                ],
                "title": "MindVote: How LLMs Predict Human Decision-Making in Social Media Polls",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MindVote: How LLMs Predict Human Decision-Making in Social Media Polls"
                },
                "summary": "The increasing complexity of Large Language Models (LLMs) necessitates new\nbenchmarks to assess their ability to predict human decision-making in dynamic\nsocial contexts. We introduce MindVote, the first benchmark for evaluating LLMs\nas \"virtual respondents\" in social media polling. MindVote comprises 276 poll\ninstances with 1,142 data entry points from three platforms (Weibo, Reddit,\nFizz), features bilingual content (Chinese/English), and covers five domains.\nOur evaluation of 18 LLMs demonstrates that top-performing models achieve an\noverall score of 0.74, an 80% relative improvement over traditional baselines,\nand then we analyze LLM world model bias with human preferences across societal\nbias dimensions. MindVote also uncovers significant disparities related to\nplatform, language, and domain. We present strategies to optimize LLM\nperformance and use LLM-as-a-Judge to assess reasoning in societal contexts.\nFurthermore, we show that temperature controls can reflect a way of human\nthinking diversity and opinion shifts in polling. In summary, MindVote offers a\nscalable framework for evaluating LLMs' social intelligence, with implications\nfor understanding behavioral decision-making. Code and data will be available\nsoon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing complexity of Large Language Models (LLMs) necessitates new\nbenchmarks to assess their ability to predict human decision-making in dynamic\nsocial contexts. We introduce MindVote, the first benchmark for evaluating LLMs\nas \"virtual respondents\" in social media polling. MindVote comprises 276 poll\ninstances with 1,142 data entry points from three platforms (Weibo, Reddit,\nFizz), features bilingual content (Chinese/English), and covers five domains.\nOur evaluation of 18 LLMs demonstrates that top-performing models achieve an\noverall score of 0.74, an 80% relative improvement over traditional baselines,\nand then we analyze LLM world model bias with human preferences across societal\nbias dimensions. MindVote also uncovers significant disparities related to\nplatform, language, and domain. We present strategies to optimize LLM\nperformance and use LLM-as-a-Judge to assess reasoning in societal contexts.\nFurthermore, we show that temperature controls can reflect a way of human\nthinking diversity and opinion shifts in polling. In summary, MindVote offers a\nscalable framework for evaluating LLMs' social intelligence, with implications\nfor understanding behavioral decision-making. Code and data will be available\nsoon."
                },
                "authors": [
                    {
                        "name": "Xutao Mao"
                    },
                    {
                        "name": "Ezra Xuanru Tao"
                    }
                ],
                "author_detail": {
                    "name": "Ezra Xuanru Tao"
                },
                "author": "Ezra Xuanru Tao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14422v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14422v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06794v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06794v3",
                "updated": "2025-05-20T14:31:45Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    14,
                    31,
                    45,
                    1,
                    140,
                    0
                ],
                "published": "2025-03-09T22:16:48Z",
                "published_parsed": [
                    2025,
                    3,
                    9,
                    22,
                    16,
                    48,
                    6,
                    68,
                    0
                ],
                "title": "Does Acceleration Cause Hidden Instability in Vision Language Models?\n  Uncovering Instance-Level Divergence Through a Large-Scale Empirical Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Does Acceleration Cause Hidden Instability in Vision Language Models?\n  Uncovering Instance-Level Divergence Through a Large-Scale Empirical Study"
                },
                "summary": "Vision-Language Models (VLMs) are powerful yet computationally intensive for\nwidespread practical deployments. To address such challenge without costly\nre-training, post-training acceleration techniques like quantization and token\nreduction are extensively explored. However, current acceleration evaluations\nprimarily target minimal overall performance degradation, overlooking a crucial\nquestion: does the accelerated model still give the same answers to the same\nquestions as it did before acceleration? This is vital for stability-centered\nindustrial applications where consistently correct answers for specific, known\nsituations are paramount, such as in AI-based disease diagnosis. We\nsystematically investigate this for accelerated VLMs, testing four leading\nmodels (LLaVA-1.5, LLaVA-Next, Qwen2-VL, Qwen2.5-VL) with eight acceleration\nmethods on ten multi-modal benchmarks. Our findings are stark: despite minimal\naggregate performance drops, accelerated models changed original answers up to\n20% of the time. Critically, up to 6.5% of these changes converted correct\nanswers to incorrect. Input perturbations magnified these inconsistencies, and\nthe trend is confirmed by case studies with the medical VLM LLaVA-Med. This\nresearch reveals a significant oversight in VLM acceleration, stressing an\nurgent need for instance-level stability checks to ensure trustworthy\nreal-world deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Models (VLMs) are powerful yet computationally intensive for\nwidespread practical deployments. To address such challenge without costly\nre-training, post-training acceleration techniques like quantization and token\nreduction are extensively explored. However, current acceleration evaluations\nprimarily target minimal overall performance degradation, overlooking a crucial\nquestion: does the accelerated model still give the same answers to the same\nquestions as it did before acceleration? This is vital for stability-centered\nindustrial applications where consistently correct answers for specific, known\nsituations are paramount, such as in AI-based disease diagnosis. We\nsystematically investigate this for accelerated VLMs, testing four leading\nmodels (LLaVA-1.5, LLaVA-Next, Qwen2-VL, Qwen2.5-VL) with eight acceleration\nmethods on ten multi-modal benchmarks. Our findings are stark: despite minimal\naggregate performance drops, accelerated models changed original answers up to\n20% of the time. Critically, up to 6.5% of these changes converted correct\nanswers to incorrect. Input perturbations magnified these inconsistencies, and\nthe trend is confirmed by case studies with the medical VLM LLaVA-Med. This\nresearch reveals a significant oversight in VLM acceleration, stressing an\nurgent need for instance-level stability checks to ensure trustworthy\nreal-world deployment."
                },
                "authors": [
                    {
                        "name": "Yizheng Sun"
                    },
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Chang Xu"
                    },
                    {
                        "name": "Hongpeng Zhou"
                    },
                    {
                        "name": "Chenghua Lin"
                    },
                    {
                        "name": "Riza Batista-Navarro"
                    },
                    {
                        "name": "Jingyuan Sun"
                    }
                ],
                "author_detail": {
                    "name": "Jingyuan Sun"
                },
                "author": "Jingyuan Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06794v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06794v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14417v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14417v1",
                "updated": "2025-05-20T14:28:59Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    14,
                    28,
                    59,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T14:28:59Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    14,
                    28,
                    59,
                    1,
                    140,
                    0
                ],
                "title": "Towards Non-Euclidean Foundation Models: Advancing AI Beyond Euclidean\n  Frameworks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Non-Euclidean Foundation Models: Advancing AI Beyond Euclidean\n  Frameworks"
                },
                "summary": "In the era of foundation models and Large Language Models (LLMs), Euclidean\nspace is the de facto geometric setting of our machine learning architectures.\nHowever, recent literature has demonstrated that this choice comes with\nfundamental limitations. To that end, non-Euclidean learning is quickly gaining\ntraction, particularly in web-related applications where complex relationships\nand structures are prevalent. Non-Euclidean spaces, such as hyperbolic,\nspherical, and mixed-curvature spaces, have been shown to provide more\nefficient and effective representations for data with intrinsic geometric\nproperties, including web-related data like social network topology,\nquery-document relationships, and user-item interactions. Integrating\nfoundation models with non-Euclidean geometries has great potential to enhance\ntheir ability to capture and model the underlying structures, leading to better\nperformance in search, recommendations, and content understanding. This\nworkshop focuses on the intersection of Non-Euclidean Foundation Models and\nGeometric Learning (NEGEL), exploring its potential benefits, including the\npotential benefits for advancing web-related technologies, challenges, and\nfuture directions. Workshop page:\n[https://hyperboliclearning.github.io/events/www2025workshop](https://hyperboliclearning.github.io/events/www2025workshop)",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the era of foundation models and Large Language Models (LLMs), Euclidean\nspace is the de facto geometric setting of our machine learning architectures.\nHowever, recent literature has demonstrated that this choice comes with\nfundamental limitations. To that end, non-Euclidean learning is quickly gaining\ntraction, particularly in web-related applications where complex relationships\nand structures are prevalent. Non-Euclidean spaces, such as hyperbolic,\nspherical, and mixed-curvature spaces, have been shown to provide more\nefficient and effective representations for data with intrinsic geometric\nproperties, including web-related data like social network topology,\nquery-document relationships, and user-item interactions. Integrating\nfoundation models with non-Euclidean geometries has great potential to enhance\ntheir ability to capture and model the underlying structures, leading to better\nperformance in search, recommendations, and content understanding. This\nworkshop focuses on the intersection of Non-Euclidean Foundation Models and\nGeometric Learning (NEGEL), exploring its potential benefits, including the\npotential benefits for advancing web-related technologies, challenges, and\nfuture directions. Workshop page:\n[https://hyperboliclearning.github.io/events/www2025workshop](https://hyperboliclearning.github.io/events/www2025workshop)"
                },
                "authors": [
                    {
                        "name": "Menglin Yang"
                    },
                    {
                        "name": "Yifei Zhang"
                    },
                    {
                        "name": "Jialin Chen"
                    },
                    {
                        "name": "Melanie Weber"
                    },
                    {
                        "name": "Rex Ying"
                    }
                ],
                "author_detail": {
                    "name": "Rex Ying"
                },
                "author": "Rex Ying",
                "arxiv_doi": "10.1145/3701716.3717806",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3701716.3717806",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2505.14417v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14417v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "WWW 2025 Companion",
                "arxiv_primary_category": {
                    "term": "cs.CG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14412v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14412v1",
                "updated": "2025-05-20T14:26:19Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    14,
                    26,
                    19,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T14:26:19Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    14,
                    26,
                    19,
                    1,
                    140,
                    0
                ],
                "title": "PRL: Prompts from Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PRL: Prompts from Reinforcement Learning"
                },
                "summary": "Effective prompt engineering remains a central challenge in fully harnessing\nthe capabilities of LLMs. While well-designed prompts can dramatically enhance\nperformance, crafting them typically demands expert intuition and a nuanced\nunderstanding of the task. Moreover, the most impactful prompts often hinge on\nsubtle semantic cues, ones that may elude human perception but are crucial for\nguiding LLM behavior. In this paper, we introduce PRL (Prompts from\nReinforcement Learning), a novel RL-based approach for automatic prompt\ngeneration. Unlike previous methods, PRL can produce novel few-shot examples\nthat were not seen during training. Our approach achieves state-of-the-art\nperformance across a range of benchmarks, including text classification,\nsimplification, and summarization. On the classification task, it surpasses\nprior methods by 2.58% over APE and 1.00% over EvoPrompt. Additionally, it\nimproves the average ROUGE scores on the summarization task by 4.32 over APE\nand by 2.12 over EvoPrompt and the SARI score on simplification by 6.93 over\nAPE and by 6.01 over EvoPrompt. Our code is available at\nhttps://github.com/Batorskq/prl .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective prompt engineering remains a central challenge in fully harnessing\nthe capabilities of LLMs. While well-designed prompts can dramatically enhance\nperformance, crafting them typically demands expert intuition and a nuanced\nunderstanding of the task. Moreover, the most impactful prompts often hinge on\nsubtle semantic cues, ones that may elude human perception but are crucial for\nguiding LLM behavior. In this paper, we introduce PRL (Prompts from\nReinforcement Learning), a novel RL-based approach for automatic prompt\ngeneration. Unlike previous methods, PRL can produce novel few-shot examples\nthat were not seen during training. Our approach achieves state-of-the-art\nperformance across a range of benchmarks, including text classification,\nsimplification, and summarization. On the classification task, it surpasses\nprior methods by 2.58% over APE and 1.00% over EvoPrompt. Additionally, it\nimproves the average ROUGE scores on the summarization task by 4.32 over APE\nand by 2.12 over EvoPrompt and the SARI score on simplification by 6.93 over\nAPE and by 6.01 over EvoPrompt. Our code is available at\nhttps://github.com/Batorskq/prl ."
                },
                "authors": [
                    {
                        "name": "Paweł Batorski"
                    },
                    {
                        "name": "Adrian Kosmala"
                    },
                    {
                        "name": "Paul Swoboda"
                    }
                ],
                "author_detail": {
                    "name": "Paul Swoboda"
                },
                "author": "Paul Swoboda",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14412v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14412v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13794v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13794v2",
                "updated": "2025-05-20T14:22:45Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    14,
                    22,
                    45,
                    1,
                    140,
                    0
                ],
                "published": "2025-03-18T00:50:40Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    0,
                    50,
                    40,
                    1,
                    77,
                    0
                ],
                "title": "LED: LLM Enhanced Open-Vocabulary Object Detection without Human Curated\n  Data Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LED: LLM Enhanced Open-Vocabulary Object Detection without Human Curated\n  Data Generation"
                },
                "summary": "Large foundation models trained on large-scale vision-language data can boost\nOpen-Vocabulary Object Detection (OVD) via synthetic training data, yet the\nhand-crafted pipelines often introduce bias and overfit to specific prompts. We\nsidestep this issue by directly fusing hidden states from Large Language Models\n(LLMs) into detectors-an avenue surprisingly under-explored. This paper\npresents a systematic method to enhance visual grounding by utilizing decoder\nlayers of the LLM of an MLLM. We introduce a zero-initialized cross-attention\nadapter to enable efficient knowledge fusion from LLMs to object detectors, a\nnew approach called LED (LLM Enhanced Open-Vocabulary Object Detection). We\nfind that intermediate LLM layers already encode rich spatial semantics;\nadapting only the early layers yields most of the gain. With Swin-T as the\nvision encoder, Qwen2-0.5B + LED lifts GroundingDINO by 3.82 % on OmniLabel at\njust 8.7 % extra GFLOPs, and a larger vision backbone pushes the improvement to\n6.22 %. Extensive ablations on adapter variants, LLM scales and fusion depths\nfurther corroborate our design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large foundation models trained on large-scale vision-language data can boost\nOpen-Vocabulary Object Detection (OVD) via synthetic training data, yet the\nhand-crafted pipelines often introduce bias and overfit to specific prompts. We\nsidestep this issue by directly fusing hidden states from Large Language Models\n(LLMs) into detectors-an avenue surprisingly under-explored. This paper\npresents a systematic method to enhance visual grounding by utilizing decoder\nlayers of the LLM of an MLLM. We introduce a zero-initialized cross-attention\nadapter to enable efficient knowledge fusion from LLMs to object detectors, a\nnew approach called LED (LLM Enhanced Open-Vocabulary Object Detection). We\nfind that intermediate LLM layers already encode rich spatial semantics;\nadapting only the early layers yields most of the gain. With Swin-T as the\nvision encoder, Qwen2-0.5B + LED lifts GroundingDINO by 3.82 % on OmniLabel at\njust 8.7 % extra GFLOPs, and a larger vision backbone pushes the improvement to\n6.22 %. Extensive ablations on adapter variants, LLM scales and fusion depths\nfurther corroborate our design."
                },
                "authors": [
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Shiyu Zhao"
                    },
                    {
                        "name": "Yuxiao Chen"
                    },
                    {
                        "name": "Zhenting Wang"
                    },
                    {
                        "name": "Can Jin"
                    },
                    {
                        "name": "Dimitris N. Metaxas"
                    }
                ],
                "author_detail": {
                    "name": "Dimitris N. Metaxas"
                },
                "author": "Dimitris N. Metaxas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13794v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13794v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.12938v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.12938v2",
                "updated": "2025-05-20T14:22:15Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    14,
                    22,
                    15,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-19T10:22:04Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    10,
                    22,
                    4,
                    0,
                    139,
                    0
                ],
                "title": "Leveraging LLM Inconsistency to Boost Pass@k Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging LLM Inconsistency to Boost Pass@k Performance"
                },
                "summary": "Large language models (LLMs) achieve impressive abilities in numerous\ndomains, but exhibit inconsistent performance in response to minor input\nchanges. Rather than view this as a drawback, in this paper we introduce a\nnovel method for leveraging models' inconsistency to boost Pass@k performance.\nSpecifically, we present a \"Variator\" agent that generates k variants of a\ngiven task and submits one candidate solution for each one. Our variant\ngeneration approach is applicable to a wide range of domains as it is task\nagnostic and compatible with free-form inputs. We demonstrate the efficacy of\nour agent theoretically using a probabilistic model of the inconsistency\neffect, and show empirically that it outperforms the baseline on the APPS\ndataset. Furthermore, we establish that inconsistency persists even in frontier\nreasoning models across coding and cybersecurity domains, suggesting our method\nis likely to remain relevant for future model generations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) achieve impressive abilities in numerous\ndomains, but exhibit inconsistent performance in response to minor input\nchanges. Rather than view this as a drawback, in this paper we introduce a\nnovel method for leveraging models' inconsistency to boost Pass@k performance.\nSpecifically, we present a \"Variator\" agent that generates k variants of a\ngiven task and submits one candidate solution for each one. Our variant\ngeneration approach is applicable to a wide range of domains as it is task\nagnostic and compatible with free-form inputs. We demonstrate the efficacy of\nour agent theoretically using a probabilistic model of the inconsistency\neffect, and show empirically that it outperforms the baseline on the APPS\ndataset. Furthermore, we establish that inconsistency persists even in frontier\nreasoning models across coding and cybersecurity domains, suggesting our method\nis likely to remain relevant for future model generations."
                },
                "authors": [
                    {
                        "name": "Uri Dalal"
                    },
                    {
                        "name": "Meirav Segal"
                    },
                    {
                        "name": "Zvika Ben-Haim"
                    },
                    {
                        "name": "Dan Lahav"
                    },
                    {
                        "name": "Omer Nevo"
                    }
                ],
                "author_detail": {
                    "name": "Omer Nevo"
                },
                "author": "Omer Nevo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.12938v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.12938v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14406v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14406v2",
                "updated": "2025-05-21T02:11:58Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    2,
                    11,
                    58,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-20T14:20:30Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    14,
                    20,
                    30,
                    1,
                    140,
                    0
                ],
                "title": "Pierce the Mists, Greet the Sky: Decipher Knowledge Overshadowing via\n  Knowledge Circuit Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pierce the Mists, Greet the Sky: Decipher Knowledge Overshadowing via\n  Knowledge Circuit Analysis"
                },
                "summary": "Large Language Models (LLMs), despite their remarkable capabilities, are\nhampered by hallucinations. A particularly challenging variant, knowledge\novershadowing, occurs when one piece of activated knowledge inadvertently masks\nanother relevant piece, leading to erroneous outputs even with high-quality\ntraining data. Current understanding of overshadowing is largely confined to\ninference-time observations, lacking deep insights into its origins and\ninternal mechanisms during model training. Therefore, we introduce\nPhantomCircuit, a novel framework designed to comprehensively analyze and\ndetect knowledge overshadowing. By innovatively employing knowledge circuit\nanalysis, PhantomCircuit dissects the internal workings of attention heads,\ntracing how competing knowledge pathways contribute to the overshadowing\nphenomenon and its evolution throughout the training process. Extensive\nexperiments demonstrate PhantomCircuit's effectiveness in identifying such\ninstances, offering novel insights into this elusive hallucination and\nproviding the research community with a new methodological lens for its\npotential mitigation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), despite their remarkable capabilities, are\nhampered by hallucinations. A particularly challenging variant, knowledge\novershadowing, occurs when one piece of activated knowledge inadvertently masks\nanother relevant piece, leading to erroneous outputs even with high-quality\ntraining data. Current understanding of overshadowing is largely confined to\ninference-time observations, lacking deep insights into its origins and\ninternal mechanisms during model training. Therefore, we introduce\nPhantomCircuit, a novel framework designed to comprehensively analyze and\ndetect knowledge overshadowing. By innovatively employing knowledge circuit\nanalysis, PhantomCircuit dissects the internal workings of attention heads,\ntracing how competing knowledge pathways contribute to the overshadowing\nphenomenon and its evolution throughout the training process. Extensive\nexperiments demonstrate PhantomCircuit's effectiveness in identifying such\ninstances, offering novel insights into this elusive hallucination and\nproviding the research community with a new methodological lens for its\npotential mitigation."
                },
                "authors": [
                    {
                        "name": "Haoming Huang"
                    },
                    {
                        "name": "Yibo Yan"
                    },
                    {
                        "name": "Jiahao Huo"
                    },
                    {
                        "name": "Xin Zou"
                    },
                    {
                        "name": "Xinfeng Li"
                    },
                    {
                        "name": "Kun Wang"
                    },
                    {
                        "name": "Xuming Hu"
                    }
                ],
                "author_detail": {
                    "name": "Xuming Hu"
                },
                "author": "Xuming Hu",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14406v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14406v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05057v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05057v2",
                "updated": "2025-05-20T14:18:03Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    14,
                    18,
                    3,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-08T08:48:17Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    8,
                    48,
                    17,
                    3,
                    128,
                    0
                ],
                "title": "Towards Mitigating API Hallucination in Code Generated by LLMs with\n  Hierarchical Dependency Aware",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Mitigating API Hallucination in Code Generated by LLMs with\n  Hierarchical Dependency Aware"
                },
                "summary": "Application Programming Interfaces (APIs) are crucial in modern software\ndevelopment. Large Language Models (LLMs) assist in automated code generation\nbut often struggle with API hallucination, including invoking non-existent APIs\nand misusing existing ones in practical development scenarios. Existing studies\nresort to Retrieval-Augmented Generation (RAG) methods for mitigating the\nhallucination issue, but tend to fail since they generally ignore the\nstructural dependencies in practical projects and do not indeed validate\nwhether the generated APIs are available or not. To address these limitations,\nwe propose MARIN, a framework for mitigating API hallucination in code\ngenerated by LLMs with hierarchical dependency aware. MARIN consists of two\nphases: Hierarchical Dependency Mining, which analyzes local and global\ndependencies of the current function, aiming to supplement comprehensive\nproject context in LLMs input, and Dependency Constrained Decoding, which\nutilizes mined dependencies to adaptively constrain the generation process,\naiming to ensure the generated APIs align with the projects specifications. To\nfacilitate the evaluation of the degree of API hallucination, we introduce a\nnew benchmark APIHulBench and two new metrics including Micro Hallucination\nNumber (MiHN) and Macro Hallucination Rate (MaHR). Experiments on six\nstate-of-the-art LLMs demonstrate that MARIN effectively reduces API\nhallucinations, achieving an average decrease of 67.52% in MiHN and 73.56% in\nMaHR compared to the RAG approach. Applied to Huaweis internal projects and two\nproprietary LLMs, MARIN achieves average decreases of 57.33% in MiHN and 59.41%\nin MaHR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Application Programming Interfaces (APIs) are crucial in modern software\ndevelopment. Large Language Models (LLMs) assist in automated code generation\nbut often struggle with API hallucination, including invoking non-existent APIs\nand misusing existing ones in practical development scenarios. Existing studies\nresort to Retrieval-Augmented Generation (RAG) methods for mitigating the\nhallucination issue, but tend to fail since they generally ignore the\nstructural dependencies in practical projects and do not indeed validate\nwhether the generated APIs are available or not. To address these limitations,\nwe propose MARIN, a framework for mitigating API hallucination in code\ngenerated by LLMs with hierarchical dependency aware. MARIN consists of two\nphases: Hierarchical Dependency Mining, which analyzes local and global\ndependencies of the current function, aiming to supplement comprehensive\nproject context in LLMs input, and Dependency Constrained Decoding, which\nutilizes mined dependencies to adaptively constrain the generation process,\naiming to ensure the generated APIs align with the projects specifications. To\nfacilitate the evaluation of the degree of API hallucination, we introduce a\nnew benchmark APIHulBench and two new metrics including Micro Hallucination\nNumber (MiHN) and Macro Hallucination Rate (MaHR). Experiments on six\nstate-of-the-art LLMs demonstrate that MARIN effectively reduces API\nhallucinations, achieving an average decrease of 67.52% in MiHN and 73.56% in\nMaHR compared to the RAG approach. Applied to Huaweis internal projects and two\nproprietary LLMs, MARIN achieves average decreases of 57.33% in MiHN and 59.41%\nin MaHR."
                },
                "authors": [
                    {
                        "name": "Yujia Chen"
                    },
                    {
                        "name": "Mingyu Chen"
                    },
                    {
                        "name": "Cuiyun Gao"
                    },
                    {
                        "name": "Zhihan Jiang"
                    },
                    {
                        "name": "Zhongqi Li"
                    },
                    {
                        "name": "Yuchi Ma"
                    }
                ],
                "author_detail": {
                    "name": "Yuchi Ma"
                },
                "author": "Yuchi Ma",
                "arxiv_comment": "Accepted by FSE 2025 Industry Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05057v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05057v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14403v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14403v1",
                "updated": "2025-05-20T14:16:49Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    14,
                    16,
                    49,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T14:16:49Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    14,
                    16,
                    49,
                    1,
                    140,
                    0
                ],
                "title": "Unearthing Gems from Stones: Policy Optimization with Negative Sample\n  Augmentation for LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unearthing Gems from Stones: Policy Optimization with Negative Sample\n  Augmentation for LLM Reasoning"
                },
                "summary": "Recent advances in reasoning language models have witnessed a paradigm shift\nfrom short to long CoT pattern. Given the substantial computational cost of\nrollouts in long CoT models, maximizing the utility of fixed training datasets\nbecomes crucial. Our analysis reveals that negative responses contain valuable\ncomponents such as self-reflection and error-correction steps, yet primary\nexisting methods either completely discard negative samples (RFT) or apply\nequal penalization across all tokens (RL), failing to leverage these potential\nlearning signals. In light of this, we propose Behavior Constrained Policy\nGradient with Negative Sample Augmentation (BCPG-NSA), a fine-grained offline\nRL framework that encompasses three stages: 1) sample segmentation, 2)\nconsensus-based step correctness assessment combining LLM and PRM judgers, and\n3) policy optimization with NSA designed to effectively mine positive steps\nwithin negative samples. Experimental results show that BCPG-NSA outperforms\nbaselines on several challenging math/coding reasoning benchmarks using the\nsame training dataset, achieving improved sample efficiency and demonstrating\nrobustness and scalability when extended to multiple iterations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in reasoning language models have witnessed a paradigm shift\nfrom short to long CoT pattern. Given the substantial computational cost of\nrollouts in long CoT models, maximizing the utility of fixed training datasets\nbecomes crucial. Our analysis reveals that negative responses contain valuable\ncomponents such as self-reflection and error-correction steps, yet primary\nexisting methods either completely discard negative samples (RFT) or apply\nequal penalization across all tokens (RL), failing to leverage these potential\nlearning signals. In light of this, we propose Behavior Constrained Policy\nGradient with Negative Sample Augmentation (BCPG-NSA), a fine-grained offline\nRL framework that encompasses three stages: 1) sample segmentation, 2)\nconsensus-based step correctness assessment combining LLM and PRM judgers, and\n3) policy optimization with NSA designed to effectively mine positive steps\nwithin negative samples. Experimental results show that BCPG-NSA outperforms\nbaselines on several challenging math/coding reasoning benchmarks using the\nsame training dataset, achieving improved sample efficiency and demonstrating\nrobustness and scalability when extended to multiple iterations."
                },
                "authors": [
                    {
                        "name": "Zhaohui Yang"
                    },
                    {
                        "name": "Shilei Jiang"
                    },
                    {
                        "name": "Chen Hu"
                    },
                    {
                        "name": "Linjing Li"
                    },
                    {
                        "name": "Shihong Deng"
                    },
                    {
                        "name": "Daxin Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Daxin Jiang"
                },
                "author": "Daxin Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14403v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14403v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02506v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02506v4",
                "updated": "2025-05-20T14:15:36Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    14,
                    15,
                    36,
                    1,
                    140,
                    0
                ],
                "published": "2025-01-05T11:06:55Z",
                "published_parsed": [
                    2025,
                    1,
                    5,
                    11,
                    6,
                    55,
                    6,
                    5,
                    0
                ],
                "title": "ToolHop: A Query-Driven Benchmark for Evaluating Large Language Models\n  in Multi-Hop Tool Use",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ToolHop: A Query-Driven Benchmark for Evaluating Large Language Models\n  in Multi-Hop Tool Use"
                },
                "summary": "Effective evaluation of multi-hop tool use is critical for analyzing the\nunderstanding, reasoning, and function-calling capabilities of large language\nmodels (LLMs). However, progress has been hindered by a lack of reliable\nevaluation datasets. To address this, we present ToolHop, a dataset comprising\n995 user queries and 3,912 associated tools, specifically designed for rigorous\nevaluation of multi-hop tool use. ToolHop ensures diverse queries, meaningful\ninterdependencies, locally executable tools, detailed feedback, and verifiable\nanswers through a novel query-driven data construction approach that includes\ntool creation, document refinement, and code generation. We evaluate 14 LLMs\nacross five model families (i.e., LLaMA3.1, Qwen2.5, Gemini1.5, Claude3.5, and\nGPT), uncovering significant challenges in handling multi-hop tool-use\nscenarios. The leading model, GPT-4o, achieves an accuracy of 49.04%,\nunderscoring substantial room for improvement. Further analysis reveals\nvariations in tool-use strategies for various families, offering actionable\ninsights to guide the development of more effective approaches. Code and data\ncan be found in https://huggingface.co/datasets/bytedance-research/ToolHop.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective evaluation of multi-hop tool use is critical for analyzing the\nunderstanding, reasoning, and function-calling capabilities of large language\nmodels (LLMs). However, progress has been hindered by a lack of reliable\nevaluation datasets. To address this, we present ToolHop, a dataset comprising\n995 user queries and 3,912 associated tools, specifically designed for rigorous\nevaluation of multi-hop tool use. ToolHop ensures diverse queries, meaningful\ninterdependencies, locally executable tools, detailed feedback, and verifiable\nanswers through a novel query-driven data construction approach that includes\ntool creation, document refinement, and code generation. We evaluate 14 LLMs\nacross five model families (i.e., LLaMA3.1, Qwen2.5, Gemini1.5, Claude3.5, and\nGPT), uncovering significant challenges in handling multi-hop tool-use\nscenarios. The leading model, GPT-4o, achieves an accuracy of 49.04%,\nunderscoring substantial room for improvement. Further analysis reveals\nvariations in tool-use strategies for various families, offering actionable\ninsights to guide the development of more effective approaches. Code and data\ncan be found in https://huggingface.co/datasets/bytedance-research/ToolHop."
                },
                "authors": [
                    {
                        "name": "Junjie Ye"
                    },
                    {
                        "name": "Zhengyin Du"
                    },
                    {
                        "name": "Xuesong Yao"
                    },
                    {
                        "name": "Weijian Lin"
                    },
                    {
                        "name": "Yufei Xu"
                    },
                    {
                        "name": "Zehui Chen"
                    },
                    {
                        "name": "Zaiyuan Wang"
                    },
                    {
                        "name": "Sining Zhu"
                    },
                    {
                        "name": "Zhiheng Xi"
                    },
                    {
                        "name": "Siyu Yuan"
                    },
                    {
                        "name": "Tao Gui"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Xuanjing Huang"
                    },
                    {
                        "name": "Jiecao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Jiecao Chen"
                },
                "author": "Jiecao Chen",
                "arxiv_comment": "Accepted by ACL 2025 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02506v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02506v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14398v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14398v1",
                "updated": "2025-05-20T14:14:38Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    14,
                    14,
                    38,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T14:14:38Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    14,
                    14,
                    38,
                    1,
                    140,
                    0
                ],
                "title": "Log-Augmented Generation: Scaling Test-Time Reasoning with Reusable\n  Computation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Log-Augmented Generation: Scaling Test-Time Reasoning with Reusable\n  Computation"
                },
                "summary": "While humans naturally learn and adapt from past experiences, large language\nmodels (LLMs) and their agentic counterparts struggle to retain reasoning from\nprevious tasks and apply them in future contexts. To address this limitation,\nwe propose a novel framework, log-augmented generation (LAG) that directly\nreuses prior computation and reasoning from past logs at test time to enhance\nmodel's ability to learn from previous tasks and perform better on new, unseen\nchallenges, all while keeping the system efficient and scalable. Specifically,\nour system represents task logs using key-value (KV) caches, encoding the full\nreasoning context of prior tasks while storing KV caches for only a selected\nsubset of tokens. When a new task arises, LAG retrieves the KV values from\nrelevant logs to augment generation. Our approach differs from reflection-based\nmemory mechanisms by directly reusing prior reasoning and computations without\nrequiring additional steps for knowledge extraction or distillation. Our method\nalso goes beyond existing KV caching techniques, which primarily target\nefficiency gains rather than improving accuracy. Experiments on knowledge- and\nreasoning-intensive datasets demonstrate that our method significantly\noutperforms standard agentic systems that do not utilize logs, as well as\nexisting solutions based on reflection and KV cache techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While humans naturally learn and adapt from past experiences, large language\nmodels (LLMs) and their agentic counterparts struggle to retain reasoning from\nprevious tasks and apply them in future contexts. To address this limitation,\nwe propose a novel framework, log-augmented generation (LAG) that directly\nreuses prior computation and reasoning from past logs at test time to enhance\nmodel's ability to learn from previous tasks and perform better on new, unseen\nchallenges, all while keeping the system efficient and scalable. Specifically,\nour system represents task logs using key-value (KV) caches, encoding the full\nreasoning context of prior tasks while storing KV caches for only a selected\nsubset of tokens. When a new task arises, LAG retrieves the KV values from\nrelevant logs to augment generation. Our approach differs from reflection-based\nmemory mechanisms by directly reusing prior reasoning and computations without\nrequiring additional steps for knowledge extraction or distillation. Our method\nalso goes beyond existing KV caching techniques, which primarily target\nefficiency gains rather than improving accuracy. Experiments on knowledge- and\nreasoning-intensive datasets demonstrate that our method significantly\noutperforms standard agentic systems that do not utilize logs, as well as\nexisting solutions based on reflection and KV cache techniques."
                },
                "authors": [
                    {
                        "name": "Peter Baile Chen"
                    },
                    {
                        "name": "Yi Zhang"
                    },
                    {
                        "name": "Dan Roth"
                    },
                    {
                        "name": "Samuel Madden"
                    },
                    {
                        "name": "Jacob Andreas"
                    },
                    {
                        "name": "Michael Cafarella"
                    }
                ],
                "author_detail": {
                    "name": "Michael Cafarella"
                },
                "author": "Michael Cafarella",
                "arxiv_comment": "Data and code are available at https://peterbaile.github.io/lag/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14398v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14398v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14396v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14396v1",
                "updated": "2025-05-20T14:14:05Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    14,
                    14,
                    5,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T14:14:05Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    14,
                    14,
                    5,
                    1,
                    140,
                    0
                ],
                "title": "Causal Cartographer: From Mapping to Reasoning Over Counterfactual\n  Worlds",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal Cartographer: From Mapping to Reasoning Over Counterfactual\n  Worlds"
                },
                "summary": "Causal world models are systems that can answer counterfactual questions\nabout an environment of interest, i.e. predict how it would have evolved if an\narbitrary subset of events had been realized differently. It requires\nunderstanding the underlying causes behind chains of events and conducting\ncausal inference for arbitrary unseen distributions. So far, this task eludes\nfoundation models, notably large language models (LLMs), which do not have\ndemonstrated causal reasoning capabilities beyond the memorization of existing\ncausal relationships. Furthermore, evaluating counterfactuals in real-world\napplications is challenging since only the factual world is observed, limiting\nevaluation to synthetic datasets. We address these problems by explicitly\nextracting and modeling causal relationships and propose the Causal\nCartographer framework. First, we introduce a graph retrieval-augmented\ngeneration agent tasked to retrieve causal relationships from data. This\napproach allows us to construct a large network of real-world causal\nrelationships that can serve as a repository of causal knowledge and build\nreal-world counterfactuals. In addition, we create a counterfactual reasoning\nagent constrained by causal relationships to perform reliable step-by-step\ncausal inference. We show that our approach can extract causal knowledge and\nimprove the robustness of LLMs for causal reasoning tasks while reducing\ninference costs and spurious correlations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal world models are systems that can answer counterfactual questions\nabout an environment of interest, i.e. predict how it would have evolved if an\narbitrary subset of events had been realized differently. It requires\nunderstanding the underlying causes behind chains of events and conducting\ncausal inference for arbitrary unseen distributions. So far, this task eludes\nfoundation models, notably large language models (LLMs), which do not have\ndemonstrated causal reasoning capabilities beyond the memorization of existing\ncausal relationships. Furthermore, evaluating counterfactuals in real-world\napplications is challenging since only the factual world is observed, limiting\nevaluation to synthetic datasets. We address these problems by explicitly\nextracting and modeling causal relationships and propose the Causal\nCartographer framework. First, we introduce a graph retrieval-augmented\ngeneration agent tasked to retrieve causal relationships from data. This\napproach allows us to construct a large network of real-world causal\nrelationships that can serve as a repository of causal knowledge and build\nreal-world counterfactuals. In addition, we create a counterfactual reasoning\nagent constrained by causal relationships to perform reliable step-by-step\ncausal inference. We show that our approach can extract causal knowledge and\nimprove the robustness of LLMs for causal reasoning tasks while reducing\ninference costs and spurious correlations."
                },
                "authors": [
                    {
                        "name": "Gaël Gendron"
                    },
                    {
                        "name": "Jože M. Rožanec"
                    },
                    {
                        "name": "Michael Witbrock"
                    },
                    {
                        "name": "Gillian Dobbie"
                    }
                ],
                "author_detail": {
                    "name": "Gillian Dobbie"
                },
                "author": "Gillian Dobbie",
                "arxiv_comment": "29 pages, 9 pages for the main paper, 20 pages for the references and\n  appendix, 25 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14396v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14396v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.3; I.2.6; I.2.7; G.2.2; G.3; J.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14395v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14395v1",
                "updated": "2025-05-20T14:14:00Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    14,
                    14,
                    0,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T14:14:00Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    14,
                    14,
                    0,
                    1,
                    140,
                    0
                ],
                "title": "MUG-Eval: A Proxy Evaluation Framework for Multilingual Generation\n  Capabilities in Any Language",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MUG-Eval: A Proxy Evaluation Framework for Multilingual Generation\n  Capabilities in Any Language"
                },
                "summary": "Evaluating text generation capabilities of large language models (LLMs) is\nchallenging, particularly for low-resource languages where methods for direct\nassessment are scarce. We propose MUG-Eval, a novel framework that evaluates\nLLMs' multilingual generation capabilities by transforming existing benchmarks\ninto conversational tasks and measuring the LLMs' accuracies on those tasks. We\nspecifically designed these conversational tasks to require effective\ncommunication in the target language. Then, we simply use task success rate as\na proxy of successful conversation generation. Our approach offers two key\nadvantages: it is independent of language-specific NLP tools or annotated\ndatasets, which are limited for most languages, and it does not rely on\nLLMs-as-judges, whose evaluation quality degrades outside a few high-resource\nlanguages. We evaluate 8 LLMs across 30 languages spanning high, mid, and\nlow-resource categories, and we find that MUG-Eval correlates strongly with\nestablished benchmarks ($r$ > 0.75) while enabling standardized comparisons\nacross languages and models. Our framework provides a robust and\nresource-efficient solution for evaluating multilingual generation that can be\nextended to thousands of languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating text generation capabilities of large language models (LLMs) is\nchallenging, particularly for low-resource languages where methods for direct\nassessment are scarce. We propose MUG-Eval, a novel framework that evaluates\nLLMs' multilingual generation capabilities by transforming existing benchmarks\ninto conversational tasks and measuring the LLMs' accuracies on those tasks. We\nspecifically designed these conversational tasks to require effective\ncommunication in the target language. Then, we simply use task success rate as\na proxy of successful conversation generation. Our approach offers two key\nadvantages: it is independent of language-specific NLP tools or annotated\ndatasets, which are limited for most languages, and it does not rely on\nLLMs-as-judges, whose evaluation quality degrades outside a few high-resource\nlanguages. We evaluate 8 LLMs across 30 languages spanning high, mid, and\nlow-resource categories, and we find that MUG-Eval correlates strongly with\nestablished benchmarks ($r$ > 0.75) while enabling standardized comparisons\nacross languages and models. Our framework provides a robust and\nresource-efficient solution for evaluating multilingual generation that can be\nextended to thousands of languages."
                },
                "authors": [
                    {
                        "name": "Seyoung Song"
                    },
                    {
                        "name": "Seogyeong Jeong"
                    },
                    {
                        "name": "Eunsu Kim"
                    },
                    {
                        "name": "Jiho Jin"
                    },
                    {
                        "name": "Dongkwan Kim"
                    },
                    {
                        "name": "Jay Shin"
                    },
                    {
                        "name": "Alice Oh"
                    }
                ],
                "author_detail": {
                    "name": "Alice Oh"
                },
                "author": "Alice Oh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14395v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14395v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14394v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14394v1",
                "updated": "2025-05-20T14:13:59Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    14,
                    13,
                    59,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T14:13:59Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    14,
                    13,
                    59,
                    1,
                    140,
                    0
                ],
                "title": "Knowledge Graph Based Repository-Level Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Graph Based Repository-Level Code Generation"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have transformed code\ngeneration from natural language queries. However, despite their extensive\nknowledge and ability to produce high-quality code, LLMs often struggle with\ncontextual accuracy, particularly in evolving codebases. Current code search\nand retrieval methods frequently lack robustness in both the quality and\ncontextual relevance of retrieved results, leading to suboptimal code\ngeneration. This paper introduces a novel knowledge graph-based approach to\nimprove code search and retrieval leading to better quality of code generation\nin the context of repository-level tasks. The proposed approach represents code\nrepositories as graphs, capturing structural and relational information for\nenhanced context-aware code generation. Our framework employs a hybrid approach\nfor code retrieval to improve contextual relevance, track inter-file modular\ndependencies, generate more robust code and ensure consistency with the\nexisting codebase. We benchmark the proposed approach on the Evolutionary Code\nBenchmark (EvoCodeBench) dataset, a repository-level code generation benchmark,\nand demonstrate that our method significantly outperforms the baseline\napproach. These findings suggest that knowledge graph based code generation\ncould advance robust, context-sensitive coding assistance tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have transformed code\ngeneration from natural language queries. However, despite their extensive\nknowledge and ability to produce high-quality code, LLMs often struggle with\ncontextual accuracy, particularly in evolving codebases. Current code search\nand retrieval methods frequently lack robustness in both the quality and\ncontextual relevance of retrieved results, leading to suboptimal code\ngeneration. This paper introduces a novel knowledge graph-based approach to\nimprove code search and retrieval leading to better quality of code generation\nin the context of repository-level tasks. The proposed approach represents code\nrepositories as graphs, capturing structural and relational information for\nenhanced context-aware code generation. Our framework employs a hybrid approach\nfor code retrieval to improve contextual relevance, track inter-file modular\ndependencies, generate more robust code and ensure consistency with the\nexisting codebase. We benchmark the proposed approach on the Evolutionary Code\nBenchmark (EvoCodeBench) dataset, a repository-level code generation benchmark,\nand demonstrate that our method significantly outperforms the baseline\napproach. These findings suggest that knowledge graph based code generation\ncould advance robust, context-sensitive coding assistance tools."
                },
                "authors": [
                    {
                        "name": "Mihir Athale"
                    },
                    {
                        "name": "Vishal Vaddina"
                    }
                ],
                "author_detail": {
                    "name": "Vishal Vaddina"
                },
                "author": "Vishal Vaddina",
                "arxiv_comment": "8 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14394v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14394v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14393v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14393v1",
                "updated": "2025-05-20T14:13:04Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    14,
                    13,
                    4,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T14:13:04Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    14,
                    13,
                    4,
                    1,
                    140,
                    0
                ],
                "title": "Editing Across Languages: A Survey of Multilingual Knowledge Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Editing Across Languages: A Survey of Multilingual Knowledge Editing"
                },
                "summary": "While Knowledge Editing has been extensively studied in monolingual settings,\nit remains underexplored in multilingual contexts. This survey systematizes\nrecent research on Multilingual Knowledge Editing (MKE), a growing subdomain of\nmodel editing focused on ensuring factual edits generalize reliably across\nlanguages. We present a comprehensive taxonomy of MKE methods, covering\nparameter-based, memory-based, fine-tuning, and hypernetwork approaches. We\nsurvey available benchmarks,summarize key findings on method effectiveness and\ntransfer patterns, identify challenges in cross-lingual propagation, and\nhighlight open problems related to language anisotropy, evaluation coverage,\nand edit scalability. Our analysis consolidates a rapidly evolving area and\nlays the groundwork for future progress in editable language-aware LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Knowledge Editing has been extensively studied in monolingual settings,\nit remains underexplored in multilingual contexts. This survey systematizes\nrecent research on Multilingual Knowledge Editing (MKE), a growing subdomain of\nmodel editing focused on ensuring factual edits generalize reliably across\nlanguages. We present a comprehensive taxonomy of MKE methods, covering\nparameter-based, memory-based, fine-tuning, and hypernetwork approaches. We\nsurvey available benchmarks,summarize key findings on method effectiveness and\ntransfer patterns, identify challenges in cross-lingual propagation, and\nhighlight open problems related to language anisotropy, evaluation coverage,\nand edit scalability. Our analysis consolidates a rapidly evolving area and\nlays the groundwork for future progress in editable language-aware LLMs."
                },
                "authors": [
                    {
                        "name": "Nadir Durrani"
                    },
                    {
                        "name": "Basel Mousi"
                    },
                    {
                        "name": "Fahim Dalvi"
                    }
                ],
                "author_detail": {
                    "name": "Fahim Dalvi"
                },
                "author": "Fahim Dalvi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14393v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14393v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14391v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14391v1",
                "updated": "2025-05-20T14:12:05Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    14,
                    12,
                    5,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T14:12:05Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    14,
                    12,
                    5,
                    1,
                    140,
                    0
                ],
                "title": "Beyond the First Error: Process Reward Models for Reflective\n  Mathematical Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond the First Error: Process Reward Models for Reflective\n  Mathematical Reasoning"
                },
                "summary": "Many studies focus on data annotation techniques for training effective PRMs.\nHowever, current methods encounter a significant issue when applied to long CoT\nreasoning processes: they tend to focus solely on the first incorrect step and\nall preceding steps, assuming that all subsequent steps are incorrect. These\nmethods overlook the unique self-correction and reflection mechanisms inherent\nin long CoT, where correct reasoning steps may still occur after initial\nreasoning mistakes. To address this issue, we propose a novel data annotation\nmethod for PRMs specifically designed to score the long CoT reasoning process.\nGiven that under the reflection pattern, correct and incorrect steps often\nalternate, we introduce the concepts of Error Propagation and Error Cessation,\nenhancing PRMs' ability to identify both effective self-correction behaviors\nand reasoning based on erroneous steps. Leveraging an LLM-based judger for\nannotation, we collect 1.7 million data samples to train a 7B PRM and evaluate\nit at both solution and step levels. Experimental results demonstrate that\ncompared to existing open-source PRMs and PRMs trained on open-source datasets,\nour PRM achieves superior performance across various metrics, including search\nguidance, BoN, and F1 scores. Compared to widely used MC-based annotation\nmethods, our annotation approach not only achieves higher data efficiency but\nalso delivers superior performance. Detailed analysis is also conducted to\ndemonstrate the stability and generalizability of our method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many studies focus on data annotation techniques for training effective PRMs.\nHowever, current methods encounter a significant issue when applied to long CoT\nreasoning processes: they tend to focus solely on the first incorrect step and\nall preceding steps, assuming that all subsequent steps are incorrect. These\nmethods overlook the unique self-correction and reflection mechanisms inherent\nin long CoT, where correct reasoning steps may still occur after initial\nreasoning mistakes. To address this issue, we propose a novel data annotation\nmethod for PRMs specifically designed to score the long CoT reasoning process.\nGiven that under the reflection pattern, correct and incorrect steps often\nalternate, we introduce the concepts of Error Propagation and Error Cessation,\nenhancing PRMs' ability to identify both effective self-correction behaviors\nand reasoning based on erroneous steps. Leveraging an LLM-based judger for\nannotation, we collect 1.7 million data samples to train a 7B PRM and evaluate\nit at both solution and step levels. Experimental results demonstrate that\ncompared to existing open-source PRMs and PRMs trained on open-source datasets,\nour PRM achieves superior performance across various metrics, including search\nguidance, BoN, and F1 scores. Compared to widely used MC-based annotation\nmethods, our annotation approach not only achieves higher data efficiency but\nalso delivers superior performance. Detailed analysis is also conducted to\ndemonstrate the stability and generalizability of our method."
                },
                "authors": [
                    {
                        "name": "Zhaohui Yang"
                    },
                    {
                        "name": "Chenghua He"
                    },
                    {
                        "name": "Xiaowen Shi"
                    },
                    {
                        "name": "Linjing Li"
                    },
                    {
                        "name": "Qiyue Yin"
                    },
                    {
                        "name": "Shihong Deng"
                    },
                    {
                        "name": "Daxin Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Daxin Jiang"
                },
                "author": "Daxin Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14391v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14391v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14315v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14315v2",
                "updated": "2025-05-20T14:10:24Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    14,
                    10,
                    24,
                    1,
                    140,
                    0
                ],
                "published": "2025-01-24T08:18:56Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    8,
                    18,
                    56,
                    4,
                    24,
                    0
                ],
                "title": "Mitigating Forgetting in LLM Fine-Tuning via Low-Perplexity Token\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating Forgetting in LLM Fine-Tuning via Low-Perplexity Token\n  Learning"
                },
                "summary": "Maintaining consistent model performance across domains is a fundamental\nchallenge in machine learning. While recent work has explored using\nLLM-generated data for fine-tuning, its impact on cross-domain generalization\nremains poorly understood. This paper presents a systematic analysis revealing\nthat fine-tuning with LLM-generated data not only improves target task\nperformance but also reduces non-target task degradation compared to\nfine-tuning with ground truth data. Through analyzing the data sequence in\ntasks of various domains, we demonstrate that this enhancement of non-target\ntask robustness stems from the reduction of high perplexity tokens found in\nLLM-generated sequences. Following our findings, we showed that masking high\nperplexity tokens in ground truth training data achieves similar non-target\ntask performance preservation, comparable to using LLM-generated data.\nExtensive experiments across different model families and scales, including\nGemma 2 IT 2B, Llama 3 8B Instruct, and 3 additional models, agree with our\nfindings. To the best of our knowledge, this is the first work to provide an\nempirical explanation based on token perplexity reduction to mitigate\ncatastrophic forgetting in LLMs after fine-tuning, offering valuable insights\nfor developing more robust fine-tuning strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Maintaining consistent model performance across domains is a fundamental\nchallenge in machine learning. While recent work has explored using\nLLM-generated data for fine-tuning, its impact on cross-domain generalization\nremains poorly understood. This paper presents a systematic analysis revealing\nthat fine-tuning with LLM-generated data not only improves target task\nperformance but also reduces non-target task degradation compared to\nfine-tuning with ground truth data. Through analyzing the data sequence in\ntasks of various domains, we demonstrate that this enhancement of non-target\ntask robustness stems from the reduction of high perplexity tokens found in\nLLM-generated sequences. Following our findings, we showed that masking high\nperplexity tokens in ground truth training data achieves similar non-target\ntask performance preservation, comparable to using LLM-generated data.\nExtensive experiments across different model families and scales, including\nGemma 2 IT 2B, Llama 3 8B Instruct, and 3 additional models, agree with our\nfindings. To the best of our knowledge, this is the first work to provide an\nempirical explanation based on token perplexity reduction to mitigate\ncatastrophic forgetting in LLMs after fine-tuning, offering valuable insights\nfor developing more robust fine-tuning strategies."
                },
                "authors": [
                    {
                        "name": "Chao-Chung Wu"
                    },
                    {
                        "name": "Zhi Rui Tam"
                    },
                    {
                        "name": "Chieh-Yen Lin"
                    },
                    {
                        "name": "Yun-Nung Chen"
                    },
                    {
                        "name": "Shao-Hua Sun"
                    },
                    {
                        "name": "Hung-yi Lee"
                    }
                ],
                "author_detail": {
                    "name": "Hung-yi Lee"
                },
                "author": "Hung-yi Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14315v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14315v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14381v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14381v1",
                "updated": "2025-05-20T14:03:24Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    14,
                    3,
                    24,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T14:03:24Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    14,
                    3,
                    24,
                    1,
                    140,
                    0
                ],
                "title": "SCAN: Semantic Document Layout Analysis for Textual and Visual\n  Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCAN: Semantic Document Layout Analysis for Textual and Visual\n  Retrieval-Augmented Generation"
                },
                "summary": "With the increasing adoption of Large Language Models (LLMs) and\nVision-Language Models (VLMs), rich document analysis technologies for\napplications like Retrieval-Augmented Generation (RAG) and visual RAG are\ngaining significant attention. Recent research indicates that using VLMs can\nachieve better RAG performance, but processing rich documents still remains a\nchallenge since a single page contains large amounts of information. In this\npaper, we present SCAN (\\textbf{S}emanti\\textbf{C} Document Layout\n\\textbf{AN}alysis), a novel approach enhancing both textual and visual\nRetrieval-Augmented Generation (RAG) systems working with visually rich\ndocuments. It is a VLM-friendly approach that identifies document components\nwith appropriate semantic granularity, balancing context preservation with\nprocessing efficiency. SCAN uses a coarse-grained semantic approach that\ndivides documents into coherent regions covering continuous components. We\ntrained the SCAN model by fine-tuning object detection models with\nsophisticated annotation datasets. Our experimental results across English and\nJapanese datasets demonstrate that applying SCAN improves end-to-end textual\nRAG performance by up to 9.0\\% and visual RAG performance by up to 6.4\\%,\noutperforming conventional approaches and even commercial document processing\nsolutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the increasing adoption of Large Language Models (LLMs) and\nVision-Language Models (VLMs), rich document analysis technologies for\napplications like Retrieval-Augmented Generation (RAG) and visual RAG are\ngaining significant attention. Recent research indicates that using VLMs can\nachieve better RAG performance, but processing rich documents still remains a\nchallenge since a single page contains large amounts of information. In this\npaper, we present SCAN (\\textbf{S}emanti\\textbf{C} Document Layout\n\\textbf{AN}alysis), a novel approach enhancing both textual and visual\nRetrieval-Augmented Generation (RAG) systems working with visually rich\ndocuments. It is a VLM-friendly approach that identifies document components\nwith appropriate semantic granularity, balancing context preservation with\nprocessing efficiency. SCAN uses a coarse-grained semantic approach that\ndivides documents into coherent regions covering continuous components. We\ntrained the SCAN model by fine-tuning object detection models with\nsophisticated annotation datasets. Our experimental results across English and\nJapanese datasets demonstrate that applying SCAN improves end-to-end textual\nRAG performance by up to 9.0\\% and visual RAG performance by up to 6.4\\%,\noutperforming conventional approaches and even commercial document processing\nsolutions."
                },
                "authors": [
                    {
                        "name": "Yuyang Dong"
                    },
                    {
                        "name": "Nobuhiro Ueda"
                    },
                    {
                        "name": "Krisztián Boros"
                    },
                    {
                        "name": "Daiki Ito"
                    },
                    {
                        "name": "Takuya Sera"
                    },
                    {
                        "name": "Masafumi Oyamada"
                    }
                ],
                "author_detail": {
                    "name": "Masafumi Oyamada"
                },
                "author": "Masafumi Oyamada",
                "arxiv_comment": "v1",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14381v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14381v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14376v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14376v1",
                "updated": "2025-05-20T13:59:58Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    13,
                    59,
                    58,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T13:59:58Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    13,
                    59,
                    58,
                    1,
                    140,
                    0
                ],
                "title": "AutoRev: Automatic Peer Review System for Academic Research Papers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoRev: Automatic Peer Review System for Academic Research Papers"
                },
                "summary": "Generating a review for an academic research paper is a complex task that\nrequires a deep understanding of the document's content and the\ninterdependencies between its sections. It demands not only insight into\ntechnical details but also an appreciation of the paper's overall coherence and\nstructure. Recent methods have predominantly focused on fine-tuning large\nlanguage models (LLMs) to address this challenge. However, they often overlook\nthe computational and performance limitations imposed by long input token\nlengths. To address this, we introduce AutoRev, an Automatic Peer Review System\nfor Academic Research Papers. Our novel framework represents an academic\ndocument as a graph, enabling the extraction of the most critical passages that\ncontribute significantly to the review. This graph-based approach demonstrates\neffectiveness for review generation and is potentially adaptable to various\ndownstream tasks, such as question answering, summarization, and document\nrepresentation. When applied to review generation, our method outperforms SOTA\nbaselines by an average of 58.72% across all evaluation metrics. We hope that\nour work will stimulate further research in applying graph-based extraction\ntechniques to other downstream tasks in NLP. We plan to make our code public\nupon acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating a review for an academic research paper is a complex task that\nrequires a deep understanding of the document's content and the\ninterdependencies between its sections. It demands not only insight into\ntechnical details but also an appreciation of the paper's overall coherence and\nstructure. Recent methods have predominantly focused on fine-tuning large\nlanguage models (LLMs) to address this challenge. However, they often overlook\nthe computational and performance limitations imposed by long input token\nlengths. To address this, we introduce AutoRev, an Automatic Peer Review System\nfor Academic Research Papers. Our novel framework represents an academic\ndocument as a graph, enabling the extraction of the most critical passages that\ncontribute significantly to the review. This graph-based approach demonstrates\neffectiveness for review generation and is potentially adaptable to various\ndownstream tasks, such as question answering, summarization, and document\nrepresentation. When applied to review generation, our method outperforms SOTA\nbaselines by an average of 58.72% across all evaluation metrics. We hope that\nour work will stimulate further research in applying graph-based extraction\ntechniques to other downstream tasks in NLP. We plan to make our code public\nupon acceptance."
                },
                "authors": [
                    {
                        "name": "Maitreya Prafulla Chitale"
                    },
                    {
                        "name": "Ketaki Mangesh Shetye"
                    },
                    {
                        "name": "Harshit Gupta"
                    },
                    {
                        "name": "Manav Chaudhary"
                    },
                    {
                        "name": "Vasudeva Varma"
                    }
                ],
                "author_detail": {
                    "name": "Vasudeva Varma"
                },
                "author": "Vasudeva Varma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14376v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14376v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13126v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13126v2",
                "updated": "2025-05-20T13:53:50Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    13,
                    53,
                    50,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-19T13:58:15Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    13,
                    58,
                    15,
                    0,
                    139,
                    0
                ],
                "title": "Zero-Shot Iterative Formalization and Planning in Partially Observable\n  Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-Shot Iterative Formalization and Planning in Partially Observable\n  Environments"
                },
                "summary": "Using LLMs not to predict plans but to formalize an environment into the\nPlanning Domain Definition Language (PDDL) has been shown to improve\nperformance and control. Existing work focuses on fully observable\nenvironments; we tackle the more realistic and challenging partially observable\nenvironments that lack of complete, reliable information. We propose PDDLego+,\na framework to iteratively formalize, plan, grow, and refine PDDL\nrepresentations in a zero-shot manner, without needing access to any existing\ntrajectories. On two textual simulated environments, we show that PDDLego+\nimproves goal reaching success and exhibits robustness against problem\ncomplexity. We also show that the domain knowledge captured after a successful\ntrial can benefit future tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using LLMs not to predict plans but to formalize an environment into the\nPlanning Domain Definition Language (PDDL) has been shown to improve\nperformance and control. Existing work focuses on fully observable\nenvironments; we tackle the more realistic and challenging partially observable\nenvironments that lack of complete, reliable information. We propose PDDLego+,\na framework to iteratively formalize, plan, grow, and refine PDDL\nrepresentations in a zero-shot manner, without needing access to any existing\ntrajectories. On two textual simulated environments, we show that PDDLego+\nimproves goal reaching success and exhibits robustness against problem\ncomplexity. We also show that the domain knowledge captured after a successful\ntrial can benefit future tasks."
                },
                "authors": [
                    {
                        "name": "Liancheng Gong"
                    },
                    {
                        "name": "Wang Zhu"
                    },
                    {
                        "name": "Jesse Thomason"
                    },
                    {
                        "name": "Li Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Li Zhang"
                },
                "author": "Li Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13126v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13126v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14368v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14368v1",
                "updated": "2025-05-20T13:50:43Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    13,
                    50,
                    43,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T13:50:43Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    13,
                    50,
                    43,
                    1,
                    140,
                    0
                ],
                "title": "Is Your Prompt Safe? Investigating Prompt Injection Attacks Against\n  Open-Source LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is Your Prompt Safe? Investigating Prompt Injection Attacks Against\n  Open-Source LLMs"
                },
                "summary": "Recent studies demonstrate that Large Language Models (LLMs) are vulnerable\nto different prompt-based attacks, generating harmful content or sensitive\ninformation. Both closed-source and open-source LLMs are underinvestigated for\nthese attacks. This paper studies effective prompt injection attacks against\nthe $\\mathbf{14}$ most popular open-source LLMs on five attack benchmarks.\nCurrent metrics only consider successful attacks, whereas our proposed Attack\nSuccess Probability (ASP) also captures uncertainty in the model's response,\nreflecting ambiguity in attack feasibility. By comprehensively analyzing the\neffectiveness of prompt injection attacks, we propose a simple and effective\nhypnotism attack; results show that this attack causes aligned language models,\nincluding Stablelm2, Mistral, Openchat, and Vicuna, to generate objectionable\nbehaviors, achieving around $90$% ASP. They also indicate that our ignore\nprefix attacks can break all $\\mathbf{14}$ open-source LLMs, achieving over\n$60$% ASP on a multi-categorical dataset. We find that moderately well-known\nLLMs exhibit higher vulnerability to prompt injection attacks, highlighting the\nneed to raise public awareness and prioritize efficient mitigation strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies demonstrate that Large Language Models (LLMs) are vulnerable\nto different prompt-based attacks, generating harmful content or sensitive\ninformation. Both closed-source and open-source LLMs are underinvestigated for\nthese attacks. This paper studies effective prompt injection attacks against\nthe $\\mathbf{14}$ most popular open-source LLMs on five attack benchmarks.\nCurrent metrics only consider successful attacks, whereas our proposed Attack\nSuccess Probability (ASP) also captures uncertainty in the model's response,\nreflecting ambiguity in attack feasibility. By comprehensively analyzing the\neffectiveness of prompt injection attacks, we propose a simple and effective\nhypnotism attack; results show that this attack causes aligned language models,\nincluding Stablelm2, Mistral, Openchat, and Vicuna, to generate objectionable\nbehaviors, achieving around $90$% ASP. They also indicate that our ignore\nprefix attacks can break all $\\mathbf{14}$ open-source LLMs, achieving over\n$60$% ASP on a multi-categorical dataset. We find that moderately well-known\nLLMs exhibit higher vulnerability to prompt injection attacks, highlighting the\nneed to raise public awareness and prioritize efficient mitigation strategies."
                },
                "authors": [
                    {
                        "name": "Jiawen Wang"
                    },
                    {
                        "name": "Pritha Gupta"
                    },
                    {
                        "name": "Ivan Habernal"
                    },
                    {
                        "name": "Eyke Hüllermeier"
                    }
                ],
                "author_detail": {
                    "name": "Eyke Hüllermeier"
                },
                "author": "Eyke Hüllermeier",
                "arxiv_comment": "8 pages, 3 figures, EMNLP 2025 under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14368v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14368v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]