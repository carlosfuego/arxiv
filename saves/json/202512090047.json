[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2512.05916v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05916v1",
                "title": "KQ-SVD: Compressing the KV Cache with Provable Guarantees on Attention Fidelity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KQ-SVD: Compressing the KV Cache with Provable Guarantees on Attention Fidelity"
                },
                "updated": "2025-12-05T17:51:10Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    17,
                    51,
                    10,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05916v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05916v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The Key-Value (KV) cache is central to the efficiency of transformer-based large language models (LLMs), storing previously computed vectors to accelerate inference. Yet, as sequence length and batch size grow, the cache becomes a major memory bottleneck. Prior compression methods typically apply low-rank decomposition to keys alone or attempt to jointly embed queries and keys, but both approaches neglect that attention fundamentally depends on their inner products. In this work, we prove that such strategies are suboptimal for approximating the attention matrix. We introduce KQ-SVD, a simple and computationally efficient method that directly performs an optimal low-rank decomposition of the attention matrix via a closed-form solution. By targeting the true source of redundancy, KQ-SVD preserves attention outputs with higher fidelity under compression. Extensive evaluations on LLaMA and Mistral models demonstrate that our approach consistently delivers superior projection quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Key-Value (KV) cache is central to the efficiency of transformer-based large language models (LLMs), storing previously computed vectors to accelerate inference. Yet, as sequence length and batch size grow, the cache becomes a major memory bottleneck. Prior compression methods typically apply low-rank decomposition to keys alone or attempt to jointly embed queries and keys, but both approaches neglect that attention fundamentally depends on their inner products. In this work, we prove that such strategies are suboptimal for approximating the attention matrix. We introduce KQ-SVD, a simple and computationally efficient method that directly performs an optimal low-rank decomposition of the attention matrix via a closed-form solution. By targeting the true source of redundancy, KQ-SVD preserves attention outputs with higher fidelity under compression. Extensive evaluations on LLaMA and Mistral models demonstrate that our approach consistently delivers superior projection quality."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T17:51:10Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    17,
                    51,
                    10,
                    4,
                    339,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Damien Lesens"
                    },
                    {
                        "name": "Beheshteh T. Rakhshan"
                    },
                    {
                        "name": "Guillaume Rabusseau"
                    }
                ],
                "author_detail": {
                    "name": "Guillaume Rabusseau"
                },
                "author": "Guillaume Rabusseau"
            },
            {
                "id": "http://arxiv.org/abs/2512.01678v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.01678v3",
                "title": "Morphling: Fast, Fused, and Flexible GNN Training at Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Morphling: Fast, Fused, and Flexible GNN Training at Scale"
                },
                "updated": "2025-12-05T16:07:38Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    16,
                    7,
                    38,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.01678v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.01678v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Graph Neural Networks (GNNs) present a fundamental hardware challenge by fusing irregular, memory-bound graph traversals with regular, compute-intensive dense matrix operations. While frameworks such as PyTorch Geometric (PyG) and Deep Graph Library (DGL) prioritize high-level usability, they fail to address these divergent execution characteristics. As a result, they rely on generic kernels that suffer from poor cache locality, excessive memory movement, and substantial intermediate allocations. To address these limitations, we present Morphling, a domain-specific code synthesizer designed to bridge this gap. Morphling compiles high-level GNN specifications into portable, backend-specialized implementations targeting OpenMP, CUDA, and MPI. It achieves this by instantiating a library of optimized, architecture-aware primitives tailored to each execution environment. Morphling also incorporates a runtime sparsity-aware execution engine that dynamically selects dense or sparse execution paths using input feature statistics, reducing unnecessary computation on zero-valued entries. We evaluate Morphling on eleven real-world datasets spanning diverse graph structures, feature dimensionalities, and sparsity regimes. Morphling improves per-epoch training throughput by an average of 20X on CPUs, 19X on GPUs, and 6X in distributed settings over PyG and DGL, with peak speedups reaching 66X. Morphling's memory-efficient layouts further reduce peak memory consumption by up to 15X, enabling large-scale GNN training on commodity hardware. These findings demonstrate that specialized, architecture-aware code synthesis provides an effective and scalable path toward high-performance GNN execution across diverse parallel and distributed platforms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNNs) present a fundamental hardware challenge by fusing irregular, memory-bound graph traversals with regular, compute-intensive dense matrix operations. While frameworks such as PyTorch Geometric (PyG) and Deep Graph Library (DGL) prioritize high-level usability, they fail to address these divergent execution characteristics. As a result, they rely on generic kernels that suffer from poor cache locality, excessive memory movement, and substantial intermediate allocations. To address these limitations, we present Morphling, a domain-specific code synthesizer designed to bridge this gap. Morphling compiles high-level GNN specifications into portable, backend-specialized implementations targeting OpenMP, CUDA, and MPI. It achieves this by instantiating a library of optimized, architecture-aware primitives tailored to each execution environment. Morphling also incorporates a runtime sparsity-aware execution engine that dynamically selects dense or sparse execution paths using input feature statistics, reducing unnecessary computation on zero-valued entries. We evaluate Morphling on eleven real-world datasets spanning diverse graph structures, feature dimensionalities, and sparsity regimes. Morphling improves per-epoch training throughput by an average of 20X on CPUs, 19X on GPUs, and 6X in distributed settings over PyG and DGL, with peak speedups reaching 66X. Morphling's memory-efficient layouts further reduce peak memory consumption by up to 15X, enabling large-scale GNN training on commodity hardware. These findings demonstrate that specialized, architecture-aware code synthesis provides an effective and scalable path toward high-performance GNN execution across diverse parallel and distributed platforms."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-01T13:45:03Z",
                "published_parsed": [
                    2025,
                    12,
                    1,
                    13,
                    45,
                    3,
                    0,
                    335,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Anubhab"
                    },
                    {
                        "name": "Rupesh Nasre"
                    }
                ],
                "author_detail": {
                    "name": "Rupesh Nasre"
                },
                "author": "Rupesh Nasre"
            },
            {
                "id": "http://arxiv.org/abs/2512.04677v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.04677v2",
                "title": "Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length"
                },
                "updated": "2025-12-05T06:32:30Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    6,
                    32,
                    30,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.04677v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.04677v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Existing diffusion-based video generation methods are fundamentally constrained by sequential computation and long-horizon inconsistency, limiting their practical adoption in real-time, streaming audio-driven avatar synthesis. We present Live Avatar, an algorithm-system co-designed framework that enables efficient, high-fidelity, and infinite-length avatar generation using a 14-billion-parameter diffusion model. Our approach introduces Timestep-forcing Pipeline Parallelism (TPP), a distributed inference paradigm that pipelines denoising steps across multiple GPUs, effectively breaking the autoregressive bottleneck and ensuring stable, low-latency real-time streaming. To further enhance temporal consistency and mitigate identity drift and color artifacts, we propose the Rolling Sink Frame Mechanism (RSFM), which maintains sequence fidelity by dynamically recalibrating appearance using a cached reference image. Additionally, we leverage Self-Forcing Distribution Matching Distillation to facilitate causal, streamable adaptation of large-scale models without sacrificing visual quality. Live Avatar demonstrates state-of-the-art performance, reaching 20 FPS end-to-end generation on 5 H800 GPUs, and, to the best of our knowledge, is the first to achieve practical, real-time, high-fidelity avatar generation at this scale. Our work establishes a new paradigm for deploying advanced diffusion models in industrial long-form video synthesis applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing diffusion-based video generation methods are fundamentally constrained by sequential computation and long-horizon inconsistency, limiting their practical adoption in real-time, streaming audio-driven avatar synthesis. We present Live Avatar, an algorithm-system co-designed framework that enables efficient, high-fidelity, and infinite-length avatar generation using a 14-billion-parameter diffusion model. Our approach introduces Timestep-forcing Pipeline Parallelism (TPP), a distributed inference paradigm that pipelines denoising steps across multiple GPUs, effectively breaking the autoregressive bottleneck and ensuring stable, low-latency real-time streaming. To further enhance temporal consistency and mitigate identity drift and color artifacts, we propose the Rolling Sink Frame Mechanism (RSFM), which maintains sequence fidelity by dynamically recalibrating appearance using a cached reference image. Additionally, we leverage Self-Forcing Distribution Matching Distillation to facilitate causal, streamable adaptation of large-scale models without sacrificing visual quality. Live Avatar demonstrates state-of-the-art performance, reaching 20 FPS end-to-end generation on 5 H800 GPUs, and, to the best of our knowledge, is the first to achieve practical, real-time, high-fidelity avatar generation at this scale. Our work establishes a new paradigm for deploying advanced diffusion models in industrial long-form video synthesis applications."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-04T11:11:24Z",
                "published_parsed": [
                    2025,
                    12,
                    4,
                    11,
                    11,
                    24,
                    3,
                    338,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Yubo Huang"
                    },
                    {
                        "name": "Hailong Guo"
                    },
                    {
                        "name": "Fangtai Wu"
                    },
                    {
                        "name": "Shifeng Zhang"
                    },
                    {
                        "name": "Shijie Huang"
                    },
                    {
                        "name": "Qijun Gan"
                    },
                    {
                        "name": "Lin Liu"
                    },
                    {
                        "name": "Sirui Zhao"
                    },
                    {
                        "name": "Enhong Chen"
                    },
                    {
                        "name": "Jiaming Liu"
                    },
                    {
                        "name": "Steven Hoi"
                    }
                ],
                "author_detail": {
                    "name": "Steven Hoi"
                },
                "author": "Steven Hoi"
            },
            {
                "id": "http://arxiv.org/abs/2510.09665v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.09665v2",
                "title": "LMCache: An Efficient KV Cache Layer for Enterprise-Scale LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LMCache: An Efficient KV Cache Layer for Enterprise-Scale LLM Inference"
                },
                "updated": "2025-12-05T04:52:54Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    4,
                    52,
                    54,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.09665v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.09665v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "KV cache has traditionally been stored in GPU memory to accelerate the decoding phase of large language model (LLM) inference. However, it is increasingly necessary to move KV caches outside GPU devices, to enable cache reuse across different queries and inference engines. Our real-world usage statistics confirm this trend: over time, the total KV cache stored by users has grown rapidly, far exceeding the capacity of GPU memory. Despite this need, there lacks an efficient solution for offloading and transferring KV caches. We present LMCACHE, the first and so far the most efficient open-source KV caching solution, which extracts and stores KV caches generated by modern LLM engines (vLLM and SGLang) out of the GPU memory and shares them across engines and queries. LMCACHE supports both cache offloading (prefix reuse across queries) and prefill-decode (PD) disaggregation (cross-engine/GPU cache transfer). LMCACHE's high performance and wide adoption stem from the following contributions: (1) highly optimized KV cache data movement powered by batched data movement operations, compute and I/O pipelining; (2) a modular KV cache connector component, decoupling LMCACHE from the rapid evolution of inference engines; (3) a first-class control API for flexible cache orchestration across GPU, CPU, storage, and network layers. Our evaluation shows that combining LMCACHE with vLLM achieves up to 15x improvement in throughput across workloads such as multi-round question answering and document analysis. Large-scale adoption of LMCACHE in enterprise settings provides us valuable insights, for example, fetching KV cache from remote storage has unsurprisingly benefits to prefill delay, and that context truncation, which is a widely applied technique in industry, can greatly reduce prefix cache hit ratio by half. The source code of LMCACHE is at: https://github.com/LMCache/LMCache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache has traditionally been stored in GPU memory to accelerate the decoding phase of large language model (LLM) inference. However, it is increasingly necessary to move KV caches outside GPU devices, to enable cache reuse across different queries and inference engines. Our real-world usage statistics confirm this trend: over time, the total KV cache stored by users has grown rapidly, far exceeding the capacity of GPU memory. Despite this need, there lacks an efficient solution for offloading and transferring KV caches. We present LMCACHE, the first and so far the most efficient open-source KV caching solution, which extracts and stores KV caches generated by modern LLM engines (vLLM and SGLang) out of the GPU memory and shares them across engines and queries. LMCACHE supports both cache offloading (prefix reuse across queries) and prefill-decode (PD) disaggregation (cross-engine/GPU cache transfer). LMCACHE's high performance and wide adoption stem from the following contributions: (1) highly optimized KV cache data movement powered by batched data movement operations, compute and I/O pipelining; (2) a modular KV cache connector component, decoupling LMCACHE from the rapid evolution of inference engines; (3) a first-class control API for flexible cache orchestration across GPU, CPU, storage, and network layers. Our evaluation shows that combining LMCACHE with vLLM achieves up to 15x improvement in throughput across workloads such as multi-round question answering and document analysis. Large-scale adoption of LMCACHE in enterprise settings provides us valuable insights, for example, fetching KV cache from remote storage has unsurprisingly benefits to prefill delay, and that context truncation, which is a widely applied technique in industry, can greatly reduce prefix cache hit ratio by half. The source code of LMCACHE is at: https://github.com/LMCache/LMCache."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-08T00:15:04Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    0,
                    15,
                    4,
                    2,
                    281,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Yuwei An"
                    },
                    {
                        "name": "Xiaokun Chen"
                    },
                    {
                        "name": "Shaoting Feng"
                    },
                    {
                        "name": "Yuyang Huang"
                    },
                    {
                        "name": "Samuel Shen"
                    },
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Junchen Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Junchen Jiang"
                },
                "author": "Junchen Jiang"
            },
            {
                "id": "http://arxiv.org/abs/2511.21958v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.21958v2",
                "title": "Clock2Q+: A Simple and Efficient Replacement Algorithm for Metadata Cache in VMware vSAN",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clock2Q+: A Simple and Efficient Replacement Algorithm for Metadata Cache in VMware vSAN"
                },
                "updated": "2025-12-05T02:13:10Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    2,
                    13,
                    10,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.21958v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.21958v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Cache replacement algorithms are critical building blocks of storage systems. This paper examines the characteristics of metadata caches and argues that they inherently exhibit correlated references, even when the corresponding data accesses do not contain correlated references. The presence of correlated references reduces the effectiveness of cache replacement algorithms because these references are often mistakenly categorized as hot blocks. Clock2Q+ is specifically designed for metadata caches and has been implemented in vSAN and VDFS, two flagship storage products of VMware by Broadcom. Similar to S3-FIFO, Clock2Q+ uses three queues; however, Clock2Q+ introduces a correlation window in the Small FIFO queue, where blocks in this window do not set the reference bit. This simple enhancement allows Clock2Q+ to outperform state-of-the-art replacement algorithms. Compared to S3-FIFO, the second-best performing algorithm, Clock2Q+ achieves up to a 28.5% lower miss ratio on metadata traces. Clock2Q+ possesses the essential properties required for large-scale storage systems: it has low CPU overhead on cache hits, low memory overhead, scales efficiently to multiple CPUs, and is both easy to tune and implement. Additionally, Clock2Q+ outperforms state-of-the-art cache replacement algorithms on data traces as well.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache replacement algorithms are critical building blocks of storage systems. This paper examines the characteristics of metadata caches and argues that they inherently exhibit correlated references, even when the corresponding data accesses do not contain correlated references. The presence of correlated references reduces the effectiveness of cache replacement algorithms because these references are often mistakenly categorized as hot blocks. Clock2Q+ is specifically designed for metadata caches and has been implemented in vSAN and VDFS, two flagship storage products of VMware by Broadcom. Similar to S3-FIFO, Clock2Q+ uses three queues; however, Clock2Q+ introduces a correlation window in the Small FIFO queue, where blocks in this window do not set the reference bit. This simple enhancement allows Clock2Q+ to outperform state-of-the-art replacement algorithms. Compared to S3-FIFO, the second-best performing algorithm, Clock2Q+ achieves up to a 28.5% lower miss ratio on metadata traces. Clock2Q+ possesses the essential properties required for large-scale storage systems: it has low CPU overhead on cache hits, low memory overhead, scales efficiently to multiple CPUs, and is both easy to tune and implement. Additionally, Clock2Q+ outperforms state-of-the-art cache replacement algorithms on data traces as well."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-26T22:34:26Z",
                "published_parsed": [
                    2025,
                    11,
                    26,
                    22,
                    34,
                    26,
                    2,
                    330,
                    0
                ],
                "arxiv_comment": "12 pages, 14 figures",
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Yiyan Zhai"
                    },
                    {
                        "name": "Bintang Dwi Marthen"
                    },
                    {
                        "name": "Sarath Balivada"
                    },
                    {
                        "name": "Vamsi Sudhakar Bojji"
                    },
                    {
                        "name": "Eric Knauft"
                    },
                    {
                        "name": "Jitender Rohilla"
                    },
                    {
                        "name": "Jiaqi Zuo"
                    },
                    {
                        "name": "Quanxing Liu"
                    },
                    {
                        "name": "Maxime Austruy"
                    },
                    {
                        "name": "Wenguang Wang"
                    },
                    {
                        "name": "Juncheng Yang"
                    }
                ],
                "author_detail": {
                    "name": "Juncheng Yang"
                },
                "author": "Juncheng Yang"
            },
            {
                "id": "http://arxiv.org/abs/2511.14748v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.14748v2",
                "title": "Cloud-Native Vector Search: A Comprehensive Performance Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cloud-Native Vector Search: A Comprehensive Performance Analysis"
                },
                "updated": "2025-12-05T00:04:45Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    0,
                    4,
                    45,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.14748v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.14748v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Vector search has been widely employed in recommender system and retrieval-augmented-generation pipelines, commonly performed with vector indexes to efficiently find similar items in large datasets. Recent growths in both data and task complexity have motivated placing vector indexes onto remote storage -- cloud-native vector search, which cloud providers have recently introduced services for. Yet, despite varying workload characteristics and various available vector index forms, providers default to using cluster-based indexes, which on paper do adapt well to differences between disk and cloud-based environment: their fetch granularities and lack of notable intra-query dependencies aligns with the large optimal fetch sizes and minimizes costly round-trips (i.e., as opposed to graph-based indexes) to remote storage, respectively.\n  This paper systematically studies cloud-native vector search: What and how should indexes be built and used for on-cloud vector search? We analyze bottlenecks of two common index classes, cluster and graph indexes, on remote storage, and show that despite current standardized adoption of cluster indexes on the cloud, graph indexes are favored in workloads requiring high concurrency and recall, or operating on high-dimensional data or large datatypes. We further find that on-cloud search demands significantly different indexing and search parameterizations versus on-disk search for optimal performance. Finally, we incorporate existing cloud-based caching setups into vector search and find that certain index optimizations work against caching, and study how this can be mitigated to maximize gains under various available cache sizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vector search has been widely employed in recommender system and retrieval-augmented-generation pipelines, commonly performed with vector indexes to efficiently find similar items in large datasets. Recent growths in both data and task complexity have motivated placing vector indexes onto remote storage -- cloud-native vector search, which cloud providers have recently introduced services for. Yet, despite varying workload characteristics and various available vector index forms, providers default to using cluster-based indexes, which on paper do adapt well to differences between disk and cloud-based environment: their fetch granularities and lack of notable intra-query dependencies aligns with the large optimal fetch sizes and minimizes costly round-trips (i.e., as opposed to graph-based indexes) to remote storage, respectively.\n  This paper systematically studies cloud-native vector search: What and how should indexes be built and used for on-cloud vector search? We analyze bottlenecks of two common index classes, cluster and graph indexes, on remote storage, and show that despite current standardized adoption of cluster indexes on the cloud, graph indexes are favored in workloads requiring high concurrency and recall, or operating on high-dimensional data or large datatypes. We further find that on-cloud search demands significantly different indexing and search parameterizations versus on-disk search for optimal performance. Finally, we incorporate existing cloud-based caching setups into vector search and find that certain index optimizations work against caching, and study how this can be mitigated to maximize gains under various available cache sizes."
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-18T18:50:15Z",
                "published_parsed": [
                    2025,
                    11,
                    18,
                    18,
                    50,
                    15,
                    1,
                    322,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB"
                },
                "authors": [
                    {
                        "name": "Zhaoheng Li"
                    },
                    {
                        "name": "Wei Ding"
                    },
                    {
                        "name": "Silu Huang"
                    },
                    {
                        "name": "Zikang Wang"
                    },
                    {
                        "name": "Yuanjin Lin"
                    },
                    {
                        "name": "Ke Wu"
                    },
                    {
                        "name": "Yongjoo Park"
                    },
                    {
                        "name": "Jianjun Chen"
                    }
                ],
                "author_detail": {
                    "name": "Jianjun Chen"
                },
                "author": "Jianjun Chen"
            },
            {
                "id": "http://arxiv.org/abs/2512.05081v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05081v1",
                "title": "Deep Forcing: Training-Free Long Video Generation with Deep Sink and Participative Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Forcing: Training-Free Long Video Generation with Deep Sink and Participative Compression"
                },
                "updated": "2025-12-04T18:46:44Z",
                "updated_parsed": [
                    2025,
                    12,
                    4,
                    18,
                    46,
                    44,
                    3,
                    338,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05081v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05081v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recent advances in autoregressive video diffusion have enabled real-time frame streaming, yet existing solutions still suffer from temporal repetition, drift, and motion deceleration. We find that naively applying StreamingLLM-style attention sinks to video diffusion leads to fidelity degradation and motion stagnation. To overcome this, we introduce Deep Forcing, which consists of two training-free mechanisms that address this without any fine-tuning. Specifically, 1) Deep Sink dedicates half of the sliding window to persistent sink tokens and re-aligns their temporal RoPE phase to the current timeline, stabilizing global context during long rollouts. 2) Participative Compression performs importance-aware KV cache pruning that preserves only tokens actively participating in recent attention while safely discarding redundant and degraded history, minimizing error accumulation under out-of-distribution length generation. Together, these components enable over 12x extrapolation (e.g. 5s-trained to 60s+ generation) with better imaging quality than LongLive, better aesthetic quality than RollingForcing, almost maintaining overall consistency, and substantial gains in dynamic degree, all while maintaining real-time generation. Our results demonstrate that training-free KV-cache management can match or exceed training-based approaches for autoregressively streaming long-video generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in autoregressive video diffusion have enabled real-time frame streaming, yet existing solutions still suffer from temporal repetition, drift, and motion deceleration. We find that naively applying StreamingLLM-style attention sinks to video diffusion leads to fidelity degradation and motion stagnation. To overcome this, we introduce Deep Forcing, which consists of two training-free mechanisms that address this without any fine-tuning. Specifically, 1) Deep Sink dedicates half of the sliding window to persistent sink tokens and re-aligns their temporal RoPE phase to the current timeline, stabilizing global context during long rollouts. 2) Participative Compression performs importance-aware KV cache pruning that preserves only tokens actively participating in recent attention while safely discarding redundant and degraded history, minimizing error accumulation under out-of-distribution length generation. Together, these components enable over 12x extrapolation (e.g. 5s-trained to 60s+ generation) with better imaging quality than LongLive, better aesthetic quality than RollingForcing, almost maintaining overall consistency, and substantial gains in dynamic degree, all while maintaining real-time generation. Our results demonstrate that training-free KV-cache management can match or exceed training-based approaches for autoregressively streaming long-video generation."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-04T18:46:44Z",
                "published_parsed": [
                    2025,
                    12,
                    4,
                    18,
                    46,
                    44,
                    3,
                    338,
                    0
                ],
                "arxiv_comment": "Project Page: https://cvlab-kaist.github.io/DeepForcing/",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Jung Yi"
                    },
                    {
                        "name": "Wooseok Jang"
                    },
                    {
                        "name": "Paul Hyunbin Cho"
                    },
                    {
                        "name": "Jisu Nam"
                    },
                    {
                        "name": "Heeji Yoon"
                    },
                    {
                        "name": "Seungryong Kim"
                    }
                ],
                "author_detail": {
                    "name": "Seungryong Kim"
                },
                "author": "Seungryong Kim"
            },
            {
                "id": "http://arxiv.org/abs/2508.10875v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.10875v2",
                "title": "A Survey on Diffusion Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Diffusion Language Models"
                },
                "updated": "2025-12-04T17:57:10Z",
                "updated_parsed": [
                    2025,
                    12,
                    4,
                    17,
                    57,
                    10,
                    3,
                    338,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.10875v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.10875v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Diffusion Language Models (DLMs) are rapidly emerging as a powerful and promising alternative to the dominant autoregressive (AR) paradigm. By generating tokens in parallel through an iterative denoising process, DLMs possess inherent advantages in reducing inference latency and capturing bidirectional context, thereby enabling fine-grained control over the generation process. While achieving a several-fold speed-up, recent advancements have allowed DLMs to show performance comparable to their autoregressive counterparts, making them a compelling choice for various natural language processing tasks. In this survey, we provide a holistic overview of the current DLM landscape. We trace its evolution and relationship with other paradigms, such as autoregressive and masked language models, and cover both foundational principles and state-of-the-art models. Our work offers an up-to-date, comprehensive taxonomy and an in-depth analysis of current techniques, from pre-training strategies to advanced post-training methods. Another contribution of this survey is a thorough review of DLM inference strategies and optimizations, including improvements in decoding parallelism, caching mechanisms, and generation quality. We also highlight the latest approaches to multimodal extensions of DLMs and delineate their applications across various practical scenarios. Furthermore, our discussion addresses the limitations and challenges of DLMs, including efficiency, long-sequence handling, and infrastructure requirements, while outlining future research directions to sustain progress in this rapidly evolving field. Project GitHub is available at https://github.com/VILA-Lab/Awesome-DLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Language Models (DLMs) are rapidly emerging as a powerful and promising alternative to the dominant autoregressive (AR) paradigm. By generating tokens in parallel through an iterative denoising process, DLMs possess inherent advantages in reducing inference latency and capturing bidirectional context, thereby enabling fine-grained control over the generation process. While achieving a several-fold speed-up, recent advancements have allowed DLMs to show performance comparable to their autoregressive counterparts, making them a compelling choice for various natural language processing tasks. In this survey, we provide a holistic overview of the current DLM landscape. We trace its evolution and relationship with other paradigms, such as autoregressive and masked language models, and cover both foundational principles and state-of-the-art models. Our work offers an up-to-date, comprehensive taxonomy and an in-depth analysis of current techniques, from pre-training strategies to advanced post-training methods. Another contribution of this survey is a thorough review of DLM inference strategies and optimizations, including improvements in decoding parallelism, caching mechanisms, and generation quality. We also highlight the latest approaches to multimodal extensions of DLMs and delineate their applications across various practical scenarios. Furthermore, our discussion addresses the limitations and challenges of DLMs, including efficiency, long-sequence handling, and infrastructure requirements, while outlining future research directions to sustain progress in this rapidly evolving field. Project GitHub is available at https://github.com/VILA-Lab/Awesome-DLMs."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-14T17:47:22Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    17,
                    47,
                    22,
                    3,
                    226,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Tianyi Li"
                    },
                    {
                        "name": "Mingda Chen"
                    },
                    {
                        "name": "Bowei Guo"
                    },
                    {
                        "name": "Zhiqiang Shen"
                    }
                ],
                "author_detail": {
                    "name": "Zhiqiang Shen"
                },
                "author": "Zhiqiang Shen"
            },
            {
                "id": "http://arxiv.org/abs/2512.04939v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.04939v1",
                "title": "LiteVGGT: Boosting Vanilla VGGT via Geometry-aware Cached Token Merging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LiteVGGT: Boosting Vanilla VGGT via Geometry-aware Cached Token Merging"
                },
                "updated": "2025-12-04T16:07:02Z",
                "updated_parsed": [
                    2025,
                    12,
                    4,
                    16,
                    7,
                    2,
                    3,
                    338,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.04939v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.04939v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "3D vision foundation models like Visual Geometry Grounded Transformer (VGGT) have advanced greatly in geometric perception. However, it is time-consuming and memory-intensive for long sequences, limiting application to large-scale scenes beyond hundreds of images. To address this, we propose LiteVGGT, achieving up to 10x speedup and substantial memory reduction, enabling efficient processing of 1000-image scenes. We derive two key insights for 3D reconstruction: (1) tokens from local image regions have inherent geometric correlations, leading to high similarity and computational redundancy; (2) token similarity across adjacent network layers remains stable, allowing for reusable merge decisions. Guided by these, we design a simple yet efficient strategy, dubbed geometry-aware cached token merging. We analyze each token's geometric importance, optimizing anchor token selection to better preserve key information for reconstruction. We also cache and reuse merge indices across layers, substantially reducing latency with minimal accuracy impact. This strategy retains VGGT's core performance, enabling efficient fine-tuning and FP8 quantization for further gains. Extensive experiments validate LiteVGGT's effectiveness, scalability, and robustness. Project page: https://garlicba.github.io/LiteVGGT/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D vision foundation models like Visual Geometry Grounded Transformer (VGGT) have advanced greatly in geometric perception. However, it is time-consuming and memory-intensive for long sequences, limiting application to large-scale scenes beyond hundreds of images. To address this, we propose LiteVGGT, achieving up to 10x speedup and substantial memory reduction, enabling efficient processing of 1000-image scenes. We derive two key insights for 3D reconstruction: (1) tokens from local image regions have inherent geometric correlations, leading to high similarity and computational redundancy; (2) token similarity across adjacent network layers remains stable, allowing for reusable merge decisions. Guided by these, we design a simple yet efficient strategy, dubbed geometry-aware cached token merging. We analyze each token's geometric importance, optimizing anchor token selection to better preserve key information for reconstruction. We also cache and reuse merge indices across layers, substantially reducing latency with minimal accuracy impact. This strategy retains VGGT's core performance, enabling efficient fine-tuning and FP8 quantization for further gains. Extensive experiments validate LiteVGGT's effectiveness, scalability, and robustness. Project page: https://garlicba.github.io/LiteVGGT/"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-04T16:07:02Z",
                "published_parsed": [
                    2025,
                    12,
                    4,
                    16,
                    7,
                    2,
                    3,
                    338,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Zhijian Shu"
                    },
                    {
                        "name": "Cheng Lin"
                    },
                    {
                        "name": "Tao Xie"
                    },
                    {
                        "name": "Wei Yin"
                    },
                    {
                        "name": "Ben Li"
                    },
                    {
                        "name": "Zhiyuan Pu"
                    },
                    {
                        "name": "Weize Li"
                    },
                    {
                        "name": "Yao Yao"
                    },
                    {
                        "name": "Xun Cao"
                    },
                    {
                        "name": "Xiaoyang Guo"
                    },
                    {
                        "name": "Xiao-Xiao Long"
                    }
                ],
                "author_detail": {
                    "name": "Xiao-Xiao Long"
                },
                "author": "Xiao-Xiao Long"
            },
            {
                "id": "http://arxiv.org/abs/2512.04857v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.04857v1",
                "title": "Autoregressive Image Generation Needs Only a Few Lines of Cached Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive Image Generation Needs Only a Few Lines of Cached Tokens"
                },
                "updated": "2025-12-04T14:41:21Z",
                "updated_parsed": [
                    2025,
                    12,
                    4,
                    14,
                    41,
                    21,
                    3,
                    338,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.04857v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.04857v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Autoregressive (AR) visual generation has emerged as a powerful paradigm for image and multimodal synthesis, owing to its scalability and generality. However, existing AR image generation suffers from severe memory bottlenecks due to the need to cache all previously generated visual tokens during decoding, leading to both high storage requirements and low throughput. In this paper, we introduce \\textbf{LineAR}, a novel, training-free progressive key-value (KV) cache compression pipeline for autoregressive image generation. By fully exploiting the intrinsic characteristics of visual attention, LineAR manages the cache at the line level using a 2D view, preserving the visual dependency regions while progressively evicting less-informative tokens that are harmless for subsequent line generation, guided by inter-line attention. LineAR enables efficient autoregressive (AR) image generation by utilizing only a few lines of cache, achieving both memory savings and throughput speedup, while maintaining or even improving generation quality. Extensive experiments across six autoregressive image generation models, including class-conditional and text-to-image generation, validate its effectiveness and generality. LineAR improves ImageNet FID from 2.77 to 2.68 and COCO FID from 23.85 to 22.86 on LlamaGen-XL and Janus-Pro-1B, while retaining only 1/6 KV cache. It also improves DPG on Lumina-mGPT-768 with just 1/8 KV cache. Additionally, LineAR achieves significant memory and throughput gains, including up to 67.61% memory reduction and 7.57x speedup on LlamaGen-XL, and 39.66% memory reduction and 5.62x speedup on Janus-Pro-7B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive (AR) visual generation has emerged as a powerful paradigm for image and multimodal synthesis, owing to its scalability and generality. However, existing AR image generation suffers from severe memory bottlenecks due to the need to cache all previously generated visual tokens during decoding, leading to both high storage requirements and low throughput. In this paper, we introduce \\textbf{LineAR}, a novel, training-free progressive key-value (KV) cache compression pipeline for autoregressive image generation. By fully exploiting the intrinsic characteristics of visual attention, LineAR manages the cache at the line level using a 2D view, preserving the visual dependency regions while progressively evicting less-informative tokens that are harmless for subsequent line generation, guided by inter-line attention. LineAR enables efficient autoregressive (AR) image generation by utilizing only a few lines of cache, achieving both memory savings and throughput speedup, while maintaining or even improving generation quality. Extensive experiments across six autoregressive image generation models, including class-conditional and text-to-image generation, validate its effectiveness and generality. LineAR improves ImageNet FID from 2.77 to 2.68 and COCO FID from 23.85 to 22.86 on LlamaGen-XL and Janus-Pro-1B, while retaining only 1/6 KV cache. It also improves DPG on Lumina-mGPT-768 with just 1/8 KV cache. Additionally, LineAR achieves significant memory and throughput gains, including up to 67.61% memory reduction and 7.57x speedup on LlamaGen-XL, and 39.66% memory reduction and 5.62x speedup on Janus-Pro-7B."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-04T14:41:21Z",
                "published_parsed": [
                    2025,
                    12,
                    4,
                    14,
                    41,
                    21,
                    3,
                    338,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Ziran Qin"
                    },
                    {
                        "name": "Youru Lv"
                    },
                    {
                        "name": "Mingbao Lin"
                    },
                    {
                        "name": "Zeren Zhang"
                    },
                    {
                        "name": "Chanfan Gan"
                    },
                    {
                        "name": "Tieyuan Chen"
                    },
                    {
                        "name": "Weiyao Lin"
                    }
                ],
                "author_detail": {
                    "name": "Weiyao Lin"
                },
                "author": "Weiyao Lin"
            },
            {
                "id": "http://arxiv.org/abs/2512.03397v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.03397v2",
                "title": "Surfel-LIO: Fast LiDAR-Inertial Odometry with Pre-computed Surfels and Hierarchical Z-order Voxel Hashing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surfel-LIO: Fast LiDAR-Inertial Odometry with Pre-computed Surfels and Hierarchical Z-order Voxel Hashing"
                },
                "updated": "2025-12-04T12:53:34Z",
                "updated_parsed": [
                    2025,
                    12,
                    4,
                    12,
                    53,
                    34,
                    3,
                    338,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.03397v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.03397v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "LiDAR-inertial odometry (LIO) is an active research area, as it enables accurate real-time state estimation in GPS-denied environments. Recent advances in map data structures and spatial indexing have significantly improved the efficiency of LIO systems. Nevertheless, we observe that two aspects may still leave room for improvement: (1) nearest neighbor search often requires examining multiple spatial units to gather sufficient points for plane fitting, and (2) plane parameters are typically recomputed at every iteration despite unchanged map geometry. Motivated by these observations, we propose Surfel-LIO, which employs a hierarchical voxel structure (hVox) with pre-computed surfel representation. This design enables O(1) correspondence retrieval without runtime neighbor enumeration or plane fitting, combined with Z-order curve encoding for cache-friendly spatial indexing. Experimental results on the M3DGR dataset demonstrate that our method achieves significantly faster processing speed compared to recent state-of-the-art methods while maintaining comparable state estimation accuracy. Our implementation is publicly available at https://github.com/93won/lidar_inertial_odometry.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LiDAR-inertial odometry (LIO) is an active research area, as it enables accurate real-time state estimation in GPS-denied environments. Recent advances in map data structures and spatial indexing have significantly improved the efficiency of LIO systems. Nevertheless, we observe that two aspects may still leave room for improvement: (1) nearest neighbor search often requires examining multiple spatial units to gather sufficient points for plane fitting, and (2) plane parameters are typically recomputed at every iteration despite unchanged map geometry. Motivated by these observations, we propose Surfel-LIO, which employs a hierarchical voxel structure (hVox) with pre-computed surfel representation. This design enables O(1) correspondence retrieval without runtime neighbor enumeration or plane fitting, combined with Z-order curve encoding for cache-friendly spatial indexing. Experimental results on the M3DGR dataset demonstrate that our method achieves significantly faster processing speed compared to recent state-of-the-art methods while maintaining comparable state estimation accuracy. Our implementation is publicly available at https://github.com/93won/lidar_inertial_odometry."
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-03T03:07:08Z",
                "published_parsed": [
                    2025,
                    12,
                    3,
                    3,
                    7,
                    8,
                    2,
                    337,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "authors": [
                    {
                        "name": "Seungwon Choi"
                    },
                    {
                        "name": "Dong-Gyu Park"
                    },
                    {
                        "name": "Seo-Yeon Hwang"
                    },
                    {
                        "name": "Tae-Wan Kim"
                    }
                ],
                "author_detail": {
                    "name": "Tae-Wan Kim"
                },
                "author": "Tae-Wan Kim"
            },
            {
                "id": "http://arxiv.org/abs/2511.22421v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.22421v2",
                "title": "Semantic-Aware Caching for Efficient Image Generation in Edge Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic-Aware Caching for Efficient Image Generation in Edge Computing"
                },
                "updated": "2025-12-04T07:11:48Z",
                "updated_parsed": [
                    2025,
                    12,
                    4,
                    7,
                    11,
                    48,
                    3,
                    338,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.22421v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.22421v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Text-to-image generation employing diffusion models has attained significant popularity due to its capability to produce high-quality images that adhere to textual prompts. However, the integration of diffusion models faces critical challenges into resource-constrained mobile and edge environments because it requires multiple denoising steps from the original random noise. A practical way to speed up denoising is to initialize the process with a noised reference image that is similar to the target, since both images share similar layouts, structures, and details, allowing for fewer denoising steps. Based on this idea, we present CacheGenius, a hybrid image generation system in edge computing that accelerates generation by combining text-toimage and image-to-image workflows. It generates images from user text prompts using cached reference images. CacheGenius introduces a semantic-aware classified storage scheme and a request-scheduling algorithm that ensures semantic alignment between references and targets. To ensure sustained performance, it employs a cache maintenance policy that proactively evicts obsolete entries via correlation analysis. Evaluated in a distributed edge computing system, CacheGenius reduces generation latency by 41% and computational costs by 48% relative to baselines, while maintaining competitive evaluation metrics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-image generation employing diffusion models has attained significant popularity due to its capability to produce high-quality images that adhere to textual prompts. However, the integration of diffusion models faces critical challenges into resource-constrained mobile and edge environments because it requires multiple denoising steps from the original random noise. A practical way to speed up denoising is to initialize the process with a noised reference image that is similar to the target, since both images share similar layouts, structures, and details, allowing for fewer denoising steps. Based on this idea, we present CacheGenius, a hybrid image generation system in edge computing that accelerates generation by combining text-toimage and image-to-image workflows. It generates images from user text prompts using cached reference images. CacheGenius introduces a semantic-aware classified storage scheme and a request-scheduling algorithm that ensures semantic alignment between references and targets. To ensure sustained performance, it employs a cache maintenance policy that proactively evicts obsolete entries via correlation analysis. Evaluated in a distributed edge computing system, CacheGenius reduces generation latency by 41% and computational costs by 48% relative to baselines, while maintaining competitive evaluation metrics."
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-27T12:58:25Z",
                "published_parsed": [
                    2025,
                    11,
                    27,
                    12,
                    58,
                    25,
                    3,
                    331,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI"
                },
                "authors": [
                    {
                        "name": "Hanshuai Cui"
                    },
                    {
                        "name": "Zhiqing Tang"
                    },
                    {
                        "name": "Zhi Yao"
                    },
                    {
                        "name": "Weijia Jia"
                    },
                    {
                        "name": "Wei Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Wei Zhao"
                },
                "author": "Wei Zhao"
            },
            {
                "id": "http://arxiv.org/abs/2512.04515v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.04515v1",
                "title": "EgoLCD: Egocentric Video Generation with Long Context Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EgoLCD: Egocentric Video Generation with Long Context Diffusion"
                },
                "updated": "2025-12-04T06:53:01Z",
                "updated_parsed": [
                    2025,
                    12,
                    4,
                    6,
                    53,
                    1,
                    3,
                    338,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.04515v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.04515v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Generating long, coherent egocentric videos is difficult, as hand-object interactions and procedural tasks require reliable long-term memory. Existing autoregressive models suffer from content drift, where object identity and scene semantics degrade over time. To address this challenge, we introduce EgoLCD, an end-to-end framework for egocentric long-context video generation that treats long video synthesis as a problem of efficient and stable memory management. EgoLCD combines a Long-Term Sparse KV Cache for stable global context with an attention-based short-term memory, extended by LoRA for local adaptation. A Memory Regulation Loss enforces consistent memory usage, and Structured Narrative Prompting provides explicit temporal guidance. Extensive experiments on the EgoVid-5M benchmark demonstrate that EgoLCD achieves state-of-the-art performance in both perceptual quality and temporal consistency, effectively mitigating generative forgetting and representing a significant step toward building scalable world models for embodied AI. Code: https://github.com/AIGeeksGroup/EgoLCD. Website: https://aigeeksgroup.github.io/EgoLCD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating long, coherent egocentric videos is difficult, as hand-object interactions and procedural tasks require reliable long-term memory. Existing autoregressive models suffer from content drift, where object identity and scene semantics degrade over time. To address this challenge, we introduce EgoLCD, an end-to-end framework for egocentric long-context video generation that treats long video synthesis as a problem of efficient and stable memory management. EgoLCD combines a Long-Term Sparse KV Cache for stable global context with an attention-based short-term memory, extended by LoRA for local adaptation. A Memory Regulation Loss enforces consistent memory usage, and Structured Narrative Prompting provides explicit temporal guidance. Extensive experiments on the EgoVid-5M benchmark demonstrate that EgoLCD achieves state-of-the-art performance in both perceptual quality and temporal consistency, effectively mitigating generative forgetting and representing a significant step toward building scalable world models for embodied AI. Code: https://github.com/AIGeeksGroup/EgoLCD. Website: https://aigeeksgroup.github.io/EgoLCD."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-04T06:53:01Z",
                "published_parsed": [
                    2025,
                    12,
                    4,
                    6,
                    53,
                    1,
                    3,
                    338,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Liuzhou Zhang"
                    },
                    {
                        "name": "Jiarui Ye"
                    },
                    {
                        "name": "Yuanlei Wang"
                    },
                    {
                        "name": "Ming Zhong"
                    },
                    {
                        "name": "Mingju Cao"
                    },
                    {
                        "name": "Wanke Xia"
                    },
                    {
                        "name": "Bowen Zeng"
                    },
                    {
                        "name": "Zeyu Zhang"
                    },
                    {
                        "name": "Hao Tang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Tang"
                },
                "author": "Hao Tang"
            },
            {
                "id": "http://arxiv.org/abs/2511.08003v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.08003v2",
                "title": "Sharp Eyes and Memory for VideoLLMs: Information-Aware Visual Token Pruning for Efficient and Reliable VideoLLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sharp Eyes and Memory for VideoLLMs: Information-Aware Visual Token Pruning for Efficient and Reliable VideoLLM Reasoning"
                },
                "updated": "2025-12-04T06:19:39Z",
                "updated_parsed": [
                    2025,
                    12,
                    4,
                    6,
                    19,
                    39,
                    3,
                    338,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.08003v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.08003v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Current Video Large Language Models (VideoLLMs) suffer from quadratic computational complexity and key-value cache scaling, due to their reliance on processing excessive redundant visual tokens. To address this problem, we propose SharpV, a minimalist and efficient method for adaptive pruning of visual tokens and KV cache. Different from most uniform compression approaches, SharpV dynamically adjusts pruning ratios based on spatial-temporal information. Remarkably, this adaptive mechanism occasionally achieves performance gains over dense models, offering a novel paradigm for adaptive pruning. During the KV cache pruning stage, based on observations of visual information degradation, SharpV prunes degraded visual features via a self-calibration manner, guided by similarity to original visual features. In this way, SharpV achieves hierarchical cache pruning from the perspective of information bottleneck, offering a new insight into VideoLLMs' information flow. Experiments on multiple public benchmarks demonstrate the superiority of SharpV. Moreover, to the best of our knowledge, SharpV is notably the first two-stage pruning framework that operates without requiring access to exposed attention scores, ensuring full compatibility with hardware acceleration techniques like Flash Attention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current Video Large Language Models (VideoLLMs) suffer from quadratic computational complexity and key-value cache scaling, due to their reliance on processing excessive redundant visual tokens. To address this problem, we propose SharpV, a minimalist and efficient method for adaptive pruning of visual tokens and KV cache. Different from most uniform compression approaches, SharpV dynamically adjusts pruning ratios based on spatial-temporal information. Remarkably, this adaptive mechanism occasionally achieves performance gains over dense models, offering a novel paradigm for adaptive pruning. During the KV cache pruning stage, based on observations of visual information degradation, SharpV prunes degraded visual features via a self-calibration manner, guided by similarity to original visual features. In this way, SharpV achieves hierarchical cache pruning from the perspective of information bottleneck, offering a new insight into VideoLLMs' information flow. Experiments on multiple public benchmarks demonstrate the superiority of SharpV. Moreover, to the best of our knowledge, SharpV is notably the first two-stage pruning framework that operates without requiring access to exposed attention scores, ensuring full compatibility with hardware acceleration techniques like Flash Attention."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-11T09:07:40Z",
                "published_parsed": [
                    2025,
                    11,
                    11,
                    9,
                    7,
                    40,
                    1,
                    315,
                    0
                ],
                "arxiv_comment": "The 40th Annual AAAI Conference on Artificial Intelligence (AAAI-26) Poster",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Jialong Qin"
                    },
                    {
                        "name": "Xin Zou"
                    },
                    {
                        "name": "Di Lu"
                    },
                    {
                        "name": "Yibo Yan"
                    },
                    {
                        "name": "Xuming Hu"
                    }
                ],
                "author_detail": {
                    "name": "Xuming Hu"
                },
                "author": "Xuming Hu"
            },
            {
                "id": "http://arxiv.org/abs/2512.04226v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.04226v1",
                "title": "tritonBLAS: Triton-based Analytical Approach for GEMM Kernel Parameter Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "tritonBLAS: Triton-based Analytical Approach for GEMM Kernel Parameter Selection"
                },
                "updated": "2025-12-03T19:46:11Z",
                "updated_parsed": [
                    2025,
                    12,
                    3,
                    19,
                    46,
                    11,
                    2,
                    337,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.04226v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.04226v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We present tritonBLAS, a fast and deterministic analytical model that uses architectural parameters like the cache hierarchy, and relative code and data placement to generate performant GPU GEMM kernels. tritonBLAS explicitly models the relationship between architectural topology, matrix shapes, and algorithmic blocking behavior to predict near-optimal configurations without runtime autotuning. Based on this model, we developed and implemented a lightweight GEMM framework entirely within Triton. We evaluate the performance of tritonBLAS across a diverse set of GEMM problem sizes on modern GPUs. tritonBLAS achieves over 95% of the performance of autotuning solutions, while reducing autotuning time to zero. This makes tritonBLAS a practical drop-in replacement for empirical tuning in production HPC and ML workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present tritonBLAS, a fast and deterministic analytical model that uses architectural parameters like the cache hierarchy, and relative code and data placement to generate performant GPU GEMM kernels. tritonBLAS explicitly models the relationship between architectural topology, matrix shapes, and algorithmic blocking behavior to predict near-optimal configurations without runtime autotuning. Based on this model, we developed and implemented a lightweight GEMM framework entirely within Triton. We evaluate the performance of tritonBLAS across a diverse set of GEMM problem sizes on modern GPUs. tritonBLAS achieves over 95% of the performance of autotuning solutions, while reducing autotuning time to zero. This makes tritonBLAS a practical drop-in replacement for empirical tuning in production HPC and ML workloads."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-03T19:46:11Z",
                "published_parsed": [
                    2025,
                    12,
                    3,
                    19,
                    46,
                    11,
                    2,
                    337,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Ryan Swann"
                    },
                    {
                        "name": "Muhammad Osama"
                    },
                    {
                        "name": "Xiaohu Guo"
                    },
                    {
                        "name": "Bryant Nelson"
                    },
                    {
                        "name": "Lixun Zhang"
                    },
                    {
                        "name": "Alex Brown"
                    },
                    {
                        "name": "Yen Ong"
                    },
                    {
                        "name": "Ali Yazdani"
                    },
                    {
                        "name": "Sean Siddens"
                    },
                    {
                        "name": "Ganesh Dasika"
                    },
                    {
                        "name": "Alex Underwood"
                    }
                ],
                "author_detail": {
                    "name": "Alex Underwood"
                },
                "author": "Alex Underwood"
            },
            {
                "id": "http://arxiv.org/abs/2512.04040v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.04040v1",
                "title": "RELIC: Interactive Video World Model with Long-Horizon Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RELIC: Interactive Video World Model with Long-Horizon Memory"
                },
                "updated": "2025-12-03T18:29:20Z",
                "updated_parsed": [
                    2025,
                    12,
                    3,
                    18,
                    29,
                    20,
                    2,
                    337,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.04040v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.04040v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "A truly interactive world model requires three key ingredients: real-time long-horizon streaming, consistent spatial memory, and precise user control. However, most existing approaches address only one of these aspects in isolation, as achieving all three simultaneously is highly challenging-for example, long-term memory mechanisms often degrade real-time performance. In this work, we present RELIC, a unified framework that tackles these three challenges altogether. Given a single image and a text description, RELIC enables memory-aware, long-duration exploration of arbitrary scenes in real time. Built upon recent autoregressive video-diffusion distillation techniques, our model represents long-horizon memory using highly compressed historical latent tokens encoded with both relative actions and absolute camera poses within the KV cache. This compact, camera-aware memory structure supports implicit 3D-consistent content retrieval and enforces long-term coherence with minimal computational overhead. In parallel, we fine-tune a bidirectional teacher video model to generate sequences beyond its original 5-second training horizon, and transform it into a causal student generator using a new memory-efficient self-forcing paradigm that enables full-context distillation over long-duration teacher as well as long student self-rollouts. Implemented as a 14B-parameter model and trained on a curated Unreal Engine-rendered dataset, RELIC achieves real-time generation at 16 FPS while demonstrating more accurate action following, more stable long-horizon streaming, and more robust spatial-memory retrieval compared with prior work. These capabilities establish RELIC as a strong foundation for the next generation of interactive world modeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A truly interactive world model requires three key ingredients: real-time long-horizon streaming, consistent spatial memory, and precise user control. However, most existing approaches address only one of these aspects in isolation, as achieving all three simultaneously is highly challenging-for example, long-term memory mechanisms often degrade real-time performance. In this work, we present RELIC, a unified framework that tackles these three challenges altogether. Given a single image and a text description, RELIC enables memory-aware, long-duration exploration of arbitrary scenes in real time. Built upon recent autoregressive video-diffusion distillation techniques, our model represents long-horizon memory using highly compressed historical latent tokens encoded with both relative actions and absolute camera poses within the KV cache. This compact, camera-aware memory structure supports implicit 3D-consistent content retrieval and enforces long-term coherence with minimal computational overhead. In parallel, we fine-tune a bidirectional teacher video model to generate sequences beyond its original 5-second training horizon, and transform it into a causal student generator using a new memory-efficient self-forcing paradigm that enables full-context distillation over long-duration teacher as well as long student self-rollouts. Implemented as a 14B-parameter model and trained on a curated Unreal Engine-rendered dataset, RELIC achieves real-time generation at 16 FPS while demonstrating more accurate action following, more stable long-horizon streaming, and more robust spatial-memory retrieval compared with prior work. These capabilities establish RELIC as a strong foundation for the next generation of interactive world modeling."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-03T18:29:20Z",
                "published_parsed": [
                    2025,
                    12,
                    3,
                    18,
                    29,
                    20,
                    2,
                    337,
                    0
                ],
                "arxiv_comment": "22 pages",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Yicong Hong"
                    },
                    {
                        "name": "Yiqun Mei"
                    },
                    {
                        "name": "Chongjian Ge"
                    },
                    {
                        "name": "Yiran Xu"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Sai Bi"
                    },
                    {
                        "name": "Yannick Hold-Geoffroy"
                    },
                    {
                        "name": "Mike Roberts"
                    },
                    {
                        "name": "Matthew Fisher"
                    },
                    {
                        "name": "Eli Shechtman"
                    },
                    {
                        "name": "Kalyan Sunkavalli"
                    },
                    {
                        "name": "Feng Liu"
                    },
                    {
                        "name": "Zhengqi Li"
                    },
                    {
                        "name": "Hao Tan"
                    }
                ],
                "author_detail": {
                    "name": "Hao Tan"
                },
                "author": "Hao Tan"
            },
            {
                "id": "http://arxiv.org/abs/2512.04033v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.04033v1",
                "title": "Demonstration of KV-Class \\b{eta}-Ga2O3 Trench Junction Barrier Schottky Diodes with SpaceModulated Junction Termination Extension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demonstration of KV-Class \\b{eta}-Ga2O3 Trench Junction Barrier Schottky Diodes with SpaceModulated Junction Termination Extension"
                },
                "updated": "2025-12-03T18:17:12Z",
                "updated_parsed": [
                    2025,
                    12,
                    3,
                    18,
                    17,
                    12,
                    2,
                    337,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.04033v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.04033v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "In this work, we report on the design and fabrication of p-NiO/Ga2O3 trench junction barrier schottky diodes (JBSD) integrated with space-modulated junction termination extension (SM-JTE) and compare the performance with planar Ni/Ga2O3 schottky diodes (SBDs) and p-NiO/Ga2O3 heterojunction diodes (HJDs). The JBSDs achieved breakdown voltages exceeding 1.8 kV along with low leakage currents (<10-2 A/cm2), while displaying low turn on voltage (VON) of ~1V, which is similar to that of planar Ni/Ga2O3 SBDs. The fabricated devices showed excellent forward characteristics with low differential on-resistance (Ron,sp) ranging from 4-10.5 m-cm2, for fin width between 0.6- 1.25 microns. Best performing device with fin width of 0.85m showed a unipolar figure of merit (FOM) of ~0.7GW/cm2. This work showcases the benefits of trench JBS design along with SM-JTE edge-termination for efficient high-performance kilovolt-class \\b{eta}- Ga2O3 diodes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we report on the design and fabrication of p-NiO/Ga2O3 trench junction barrier schottky diodes (JBSD) integrated with space-modulated junction termination extension (SM-JTE) and compare the performance with planar Ni/Ga2O3 schottky diodes (SBDs) and p-NiO/Ga2O3 heterojunction diodes (HJDs). The JBSDs achieved breakdown voltages exceeding 1.8 kV along with low leakage currents (<10-2 A/cm2), while displaying low turn on voltage (VON) of ~1V, which is similar to that of planar Ni/Ga2O3 SBDs. The fabricated devices showed excellent forward characteristics with low differential on-resistance (Ron,sp) ranging from 4-10.5 m-cm2, for fin width between 0.6- 1.25 microns. Best performing device with fin width of 0.85m showed a unipolar figure of merit (FOM) of ~0.7GW/cm2. This work showcases the benefits of trench JBS design along with SM-JTE edge-termination for efficient high-performance kilovolt-class \\b{eta}- Ga2O3 diodes."
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-03T18:17:12Z",
                "published_parsed": [
                    2025,
                    12,
                    3,
                    18,
                    17,
                    12,
                    2,
                    337,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph"
                },
                "authors": [
                    {
                        "name": "Advait Gilankar"
                    },
                    {
                        "name": "Julian Gervassi-Saga"
                    },
                    {
                        "name": "Martha R. McCartney"
                    },
                    {
                        "name": "Nabasindhu Das"
                    },
                    {
                        "name": "David Malcolm McComas"
                    },
                    {
                        "name": "David J. Smith"
                    },
                    {
                        "name": "Nidhin Kurian Kalarickal"
                    }
                ],
                "author_detail": {
                    "name": "Nidhin Kurian Kalarickal"
                },
                "author": "Nidhin Kurian Kalarickal"
            },
            {
                "id": "http://arxiv.org/abs/2512.04025v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.04025v1",
                "title": "PSA: Pyramid Sparse Attention for Efficient Video Understanding and Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PSA: Pyramid Sparse Attention for Efficient Video Understanding and Generation"
                },
                "updated": "2025-12-03T18:02:11Z",
                "updated_parsed": [
                    2025,
                    12,
                    3,
                    18,
                    2,
                    11,
                    2,
                    337,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.04025v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.04025v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Attention mechanisms are the core of foundation models, but their quadratic complexity remains a critical bottleneck for scaling. This challenge has driven the development of efficient attention mechanisms, with sparsity emerging as the dominant paradigm. Current methods typically retain or discard entire key-value blocks with binary masks, resulting in substantial information loss under high sparsity. To mitigate this gap, we present Pyramid Sparse Attention (PSA), a versatile module applicable to both video understanding and generation tasks. Instead of binary masking, PSA introduces multi-level pooled KV representations, enabling finer mask granularity. Specifically, each query block dynamically allocates lower pooling levels to critical KV blocks and higher levels to less important ones, creating an informative interpolation between full retention and complete pruning. This design, analogous to fixed-point quantization and classical feature pyramid networks in computer vision, effectively mitigates information loss while preserving computational efficiency under a low compute budget. It works with a native, hardware-friendly kernel that leverages decoupled block-tile design to ensure efficient execution. Across video understanding and generation benchmarks, PSA preserves contextual information and visual fidelity, consistently outperforming or achieving comparable performance over existing sparse attention baselines with superior efficiency-quality trade-offs. Our code and model weights are publicly available at: http://ziplab.co/PSA",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention mechanisms are the core of foundation models, but their quadratic complexity remains a critical bottleneck for scaling. This challenge has driven the development of efficient attention mechanisms, with sparsity emerging as the dominant paradigm. Current methods typically retain or discard entire key-value blocks with binary masks, resulting in substantial information loss under high sparsity. To mitigate this gap, we present Pyramid Sparse Attention (PSA), a versatile module applicable to both video understanding and generation tasks. Instead of binary masking, PSA introduces multi-level pooled KV representations, enabling finer mask granularity. Specifically, each query block dynamically allocates lower pooling levels to critical KV blocks and higher levels to less important ones, creating an informative interpolation between full retention and complete pruning. This design, analogous to fixed-point quantization and classical feature pyramid networks in computer vision, effectively mitigates information loss while preserving computational efficiency under a low compute budget. It works with a native, hardware-friendly kernel that leverages decoupled block-tile design to ensure efficient execution. Across video understanding and generation benchmarks, PSA preserves contextual information and visual fidelity, consistently outperforming or achieving comparable performance over existing sparse attention baselines with superior efficiency-quality trade-offs. Our code and model weights are publicly available at: http://ziplab.co/PSA"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-03T18:02:11Z",
                "published_parsed": [
                    2025,
                    12,
                    3,
                    18,
                    2,
                    11,
                    2,
                    337,
                    0
                ],
                "arxiv_comment": "Tech report",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Xiaolong Li"
                    },
                    {
                        "name": "Youping Gu"
                    },
                    {
                        "name": "Xi Lin"
                    },
                    {
                        "name": "Weijie Wang"
                    },
                    {
                        "name": "Bohan Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Bohan Zhuang"
                },
                "author": "Bohan Zhuang"
            },
            {
                "id": "http://arxiv.org/abs/2512.03972v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.03972v1",
                "title": "OOPredictor: Predicting Object-Oriented Accesses using Static Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OOPredictor: Predicting Object-Oriented Accesses using Static Analysis"
                },
                "updated": "2025-12-03T17:05:58Z",
                "updated_parsed": [
                    2025,
                    12,
                    3,
                    17,
                    5,
                    58,
                    2,
                    337,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.03972v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.03972v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Object-oriented Programming has become one of the most dominant design paradigms as the separation of concerns and adaptability of design reduce development and maintenance costs. However, the convenience is not without cost. The added indirection inherent in such designs causes excessive pointer chasing, negatively affecting locality, which in turn degrades the performance of cache structures. Furthermore, modern hardware prefetchers are mostly stride prefetchers that are ill-equipped to handle the unpredictability of access patterns generated by pointer chasing. Most software approaches that seek to address this problem resort to profiling the program as it runs, which comes with a significant run-time overhead or requires data from previous runs. In this paper, we propose the use of compile-time static analysis to predict the most common access patterns displayed by a program during run time. Since Java is one of the most popular object-oriented languages, we implement our prototype within the OpenJ9 JVM, inside the OMR optimizer infrastructure. The outputs of our proposed predictor are Markov chains that model the expected behavior of the program. The effectiveness of the proposed predictor is evaluated by comparing the model with the actual run-time behavior of the program measured using an instrumented interpreter. Our experiments show that the proposed predictor exhibits good accuracy and can be used to inform minimally intrusive load stall mitigation strategies, e.g. informing copying GCs on more locality-friendly copying orders",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Object-oriented Programming has become one of the most dominant design paradigms as the separation of concerns and adaptability of design reduce development and maintenance costs. However, the convenience is not without cost. The added indirection inherent in such designs causes excessive pointer chasing, negatively affecting locality, which in turn degrades the performance of cache structures. Furthermore, modern hardware prefetchers are mostly stride prefetchers that are ill-equipped to handle the unpredictability of access patterns generated by pointer chasing. Most software approaches that seek to address this problem resort to profiling the program as it runs, which comes with a significant run-time overhead or requires data from previous runs. In this paper, we propose the use of compile-time static analysis to predict the most common access patterns displayed by a program during run time. Since Java is one of the most popular object-oriented languages, we implement our prototype within the OpenJ9 JVM, inside the OMR optimizer infrastructure. The outputs of our proposed predictor are Markov chains that model the expected behavior of the program. The effectiveness of the proposed predictor is evaluated by comparing the model with the actual run-time behavior of the program measured using an instrumented interpreter. Our experiments show that the proposed predictor exhibits good accuracy and can be used to inform minimally intrusive load stall mitigation strategies, e.g. informing copying GCs on more locality-friendly copying orders"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-03T17:05:58Z",
                "published_parsed": [
                    2025,
                    12,
                    3,
                    17,
                    5,
                    58,
                    2,
                    337,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL"
                },
                "authors": [
                    {
                        "name": "Hassan Arafat"
                    },
                    {
                        "name": "David Bremner"
                    },
                    {
                        "name": "Kenneth B. Kent"
                    },
                    {
                        "name": "Julian Wang"
                    }
                ],
                "author_detail": {
                    "name": "Julian Wang"
                },
                "author": "Julian Wang"
            },
            {
                "id": "http://arxiv.org/abs/2512.03927v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.03927v1",
                "title": "OD-MoE: On-Demand Expert Loading for Cacheless Edge-Distributed MoE Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OD-MoE: On-Demand Expert Loading for Cacheless Edge-Distributed MoE Inference"
                },
                "updated": "2025-12-03T16:27:16Z",
                "updated_parsed": [
                    2025,
                    12,
                    3,
                    16,
                    27,
                    16,
                    2,
                    337,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.03927v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.03927v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Mixture-of-Experts (MoE), while offering significant advantages as a Large Language Model (LLM) architecture, faces substantial challenges when deployed on low-cost edge devices with tight memory constraints. Expert offloading mitigates this issue by storing expert parameters in CPU memory and caching a subset of popular experts in GPU memory. Although this approach improves GPU memory utilization by caching only the likely-used experts, the GPU memory reserved for expert caching is underutilized compared with dense LLMs. This paper presents OD-MoE, a distributed MoE inference framework that obviates the need for expert caches via fully on-demand expert loading. OD-MoE is built upon two key mechanisms: 1) parallelizing expert loading and expert computation across distributed edge nodes, and 2) an ultra-accurate emulative predictor that forecasts expert activations multiple layers ahead while expert computation is ongoing. With these innovations, OD-MoE dynamically loads each target expert to one of the distributed nodes just-in-time before its activation and promptly evicts it afterward, freeing GPU memory for subsequent experts. We comprehensively benchmark OD-MoE against state-of-the-art MoE offloading systems on a ten-node testbed. Experimental results show that: 1) OD-MoE achieves 99.94% expert activation prediction accuracy, substantially surpassing all existing methods; and 2) OD-MoE delivers approximately 75% of the decoding speed of a fully GPU-cached MoE deployment while using only 1/3 of the GPU memory. More importantly, by eliminating the need for expert caches, OD-MoE enables MoE inference on edge nodes with less-than-1GB GPU memory, paving the way for practical MoE deployment of low-cost IoT devices at the edge in the LLM era.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE), while offering significant advantages as a Large Language Model (LLM) architecture, faces substantial challenges when deployed on low-cost edge devices with tight memory constraints. Expert offloading mitigates this issue by storing expert parameters in CPU memory and caching a subset of popular experts in GPU memory. Although this approach improves GPU memory utilization by caching only the likely-used experts, the GPU memory reserved for expert caching is underutilized compared with dense LLMs. This paper presents OD-MoE, a distributed MoE inference framework that obviates the need for expert caches via fully on-demand expert loading. OD-MoE is built upon two key mechanisms: 1) parallelizing expert loading and expert computation across distributed edge nodes, and 2) an ultra-accurate emulative predictor that forecasts expert activations multiple layers ahead while expert computation is ongoing. With these innovations, OD-MoE dynamically loads each target expert to one of the distributed nodes just-in-time before its activation and promptly evicts it afterward, freeing GPU memory for subsequent experts. We comprehensively benchmark OD-MoE against state-of-the-art MoE offloading systems on a ten-node testbed. Experimental results show that: 1) OD-MoE achieves 99.94% expert activation prediction accuracy, substantially surpassing all existing methods; and 2) OD-MoE delivers approximately 75% of the decoding speed of a fully GPU-cached MoE deployment while using only 1/3 of the GPU memory. More importantly, by eliminating the need for expert caches, OD-MoE enables MoE inference on edge nodes with less-than-1GB GPU memory, paving the way for practical MoE deployment of low-cost IoT devices at the edge in the LLM era."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-03T16:27:16Z",
                "published_parsed": [
                    2025,
                    12,
                    3,
                    16,
                    27,
                    16,
                    2,
                    337,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Liujianfu Wang"
                    },
                    {
                        "name": "Yuyang Du"
                    },
                    {
                        "name": "Yuchen Pan"
                    },
                    {
                        "name": "Soung Chang Liew"
                    },
                    {
                        "name": "Jiacheng Liu"
                    },
                    {
                        "name": "Kexin Chen"
                    }
                ],
                "author_detail": {
                    "name": "Kexin Chen"
                },
                "author": "Kexin Chen"
            },
            {
                "id": "http://arxiv.org/abs/2408.05235v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2408.05235v2",
                "title": "SLO-aware GPU Frequency Scaling for Energy Efficient LLM Inference Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SLO-aware GPU Frequency Scaling for Energy Efficient LLM Inference Serving"
                },
                "updated": "2025-12-03T16:21:24Z",
                "updated_parsed": [
                    2025,
                    12,
                    3,
                    16,
                    21,
                    24,
                    2,
                    337,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2408.05235v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2408.05235v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "As Large Language Models (LLMs) gain traction, their reliance on power-hungry GPUs places ever-increasing energy demands, raising environmental and monetary concerns. Inference dominates LLM workloads, presenting a critical challenge for providers: minimizing energy costs under Service-Level Objectives (SLOs) that ensure optimal user experience. In this paper, we present \\textit{throttLL'eM}, a framework that reduces energy consumption while meeting SLOs through the use of instance and GPU frequency scaling. \\textit{throttLL'eM} features mechanisms that project future KV cache usage and batch size. Leveraging a Machine-Learning (ML) model that receives these projections as inputs, \\textit{throttLL'eM} manages performance at the iteration level to satisfy SLOs with reduced frequencies and instance sizes. We show that the proposed ML model achieves $R^2$ scores greater than 0.97 and miss-predicts performance by less than 1 iteration per second on average. Experimental results on LLM inference traces show that \\textit{throttLL'eM} achieves up to 43.8\\% lower energy consumption and an energy efficiency improvement of at least $1.71\\times$ under SLOs, when compared to NVIDIA's Triton server.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) gain traction, their reliance on power-hungry GPUs places ever-increasing energy demands, raising environmental and monetary concerns. Inference dominates LLM workloads, presenting a critical challenge for providers: minimizing energy costs under Service-Level Objectives (SLOs) that ensure optimal user experience. In this paper, we present \\textit{throttLL'eM}, a framework that reduces energy consumption while meeting SLOs through the use of instance and GPU frequency scaling. \\textit{throttLL'eM} features mechanisms that project future KV cache usage and batch size. Leveraging a Machine-Learning (ML) model that receives these projections as inputs, \\textit{throttLL'eM} manages performance at the iteration level to satisfy SLOs with reduced frequencies and instance sizes. We show that the proposed ML model achieves $R^2$ scores greater than 0.97 and miss-predicts performance by less than 1 iteration per second on average. Experimental results on LLM inference traces show that \\textit{throttLL'eM} achieves up to 43.8\\% lower energy consumption and an energy efficiency improvement of at least $1.71\\times$ under SLOs, when compared to NVIDIA's Triton server."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-08-05T09:07:06Z",
                "published_parsed": [
                    2024,
                    8,
                    5,
                    9,
                    7,
                    6,
                    0,
                    218,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Andreas Kosmas Kakolyris"
                    },
                    {
                        "name": "Dimosthenis Masouros"
                    },
                    {
                        "name": "Petros Vavaroutsos"
                    },
                    {
                        "name": "Sotirios Xydis"
                    },
                    {
                        "name": "Dimitrios Soudris"
                    }
                ],
                "author_detail": {
                    "name": "Dimitrios Soudris"
                },
                "author": "Dimitrios Soudris"
            },
            {
                "id": "http://arxiv.org/abs/2512.03870v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.03870v1",
                "title": "Reconstructing KV Caches with Cross-layer Fusion For Enhanced Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reconstructing KV Caches with Cross-layer Fusion For Enhanced Transformers"
                },
                "updated": "2025-12-03T15:22:00Z",
                "updated_parsed": [
                    2025,
                    12,
                    3,
                    15,
                    22,
                    0,
                    2,
                    337,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.03870v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.03870v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Transformer decoders have achieved strong results across tasks, but the memory required for the KV cache becomes prohibitive at long sequence lengths. Although Cross-layer KV Cache sharing (e.g., YOCO, CLA) offers a path to mitigate KV Cache bottleneck, it typically underperforms within-layer methods like GQA. To understand the root cause, we investigate the information flow of keys and values of the top-layers. Our preliminary reveals a clear distribution: values are predominantly derived from the bottom layer, while keys draw more information from both bottom and middle layers. Building upon this, we propose FusedKV, whose top-layer KV caches are a learnable fusion of the most informative ones from the bottom and middle layers. This fusion operates directly on post-RoPE keys, preserving relative positional information without the computational cost of re-applying rotary embeddings. To further improve efficiency, we propose FusedKV-Lite, an cross-layer sharing approach, where top-layer KV caches are directly derived from the bottom-layer values and the middle-layer keys. Compared to FusedKV, FusedKV-Lite reduces I/O overhead at the cost of a slight increase in perplexity. In experiments on LLMs ranging from 332M to 4B parameters, our proposed method reduce 50\\% cache memory while achieving lower validation perplexity than the standard Transformer decoder, establishing it as a memory-efficient, high-performance architectural alternative.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer decoders have achieved strong results across tasks, but the memory required for the KV cache becomes prohibitive at long sequence lengths. Although Cross-layer KV Cache sharing (e.g., YOCO, CLA) offers a path to mitigate KV Cache bottleneck, it typically underperforms within-layer methods like GQA. To understand the root cause, we investigate the information flow of keys and values of the top-layers. Our preliminary reveals a clear distribution: values are predominantly derived from the bottom layer, while keys draw more information from both bottom and middle layers. Building upon this, we propose FusedKV, whose top-layer KV caches are a learnable fusion of the most informative ones from the bottom and middle layers. This fusion operates directly on post-RoPE keys, preserving relative positional information without the computational cost of re-applying rotary embeddings. To further improve efficiency, we propose FusedKV-Lite, an cross-layer sharing approach, where top-layer KV caches are directly derived from the bottom-layer values and the middle-layer keys. Compared to FusedKV, FusedKV-Lite reduces I/O overhead at the cost of a slight increase in perplexity. In experiments on LLMs ranging from 332M to 4B parameters, our proposed method reduce 50\\% cache memory while achieving lower validation perplexity than the standard Transformer decoder, establishing it as a memory-efficient, high-performance architectural alternative."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-03T15:22:00Z",
                "published_parsed": [
                    2025,
                    12,
                    3,
                    15,
                    22,
                    0,
                    2,
                    337,
                    0
                ],
                "arxiv_comment": "under review",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Hongzhan Lin"
                    },
                    {
                        "name": "Zhiqi Bai"
                    },
                    {
                        "name": "Xinmiao Zhang"
                    },
                    {
                        "name": "Sen Yang"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Siran Yang"
                    },
                    {
                        "name": "Yunlong Xu"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Yongchi Zhao"
                    },
                    {
                        "name": "Jiamang Wang"
                    },
                    {
                        "name": "Yuchi Xu"
                    },
                    {
                        "name": "Wenbo Su"
                    },
                    {
                        "name": "Bo Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zheng"
                },
                "author": "Bo Zheng"
            },
            {
                "id": "http://arxiv.org/abs/2512.03608v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.03608v1",
                "title": "KVNAND: Efficient On-Device Large Language Model Inference Using DRAM-Free In-Flash Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVNAND: Efficient On-Device Large Language Model Inference Using DRAM-Free In-Flash Computing"
                },
                "updated": "2025-12-03T09:41:03Z",
                "updated_parsed": [
                    2025,
                    12,
                    3,
                    9,
                    41,
                    3,
                    2,
                    337,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.03608v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.03608v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Deploying large language models (LLMs) on edge devices enables personalized agents with strong privacy and low cost. However, with tens to hundreds of billions of parameters, single-batch autoregressive inference suffers from extremely low arithmetic intensity, creating severe weight-loading and bandwidth pressures on resource-constrained platforms. Recent in-flash computing (IFC) solutions alleviate this bottleneck by co-locating weight-related linear computations in the decode phase with flash, yet still rely on DRAM for the key-value (KV) cache. As context length grows, the KV cache can exceed model weights in size, imposing prohibitive DRAM cost and capacity requirements. Attempts to offload KV cache to flash suffer from severe performance penalties.\n  We propose KVNAND, the first DRAM-free, IFC-based architecture that stores both model weights and KV cache entirely in compute-enabled 3D NAND flash. KVNAND addresses the fundamental performance challenges of flash under intensive KV cache access by leveraging IFC for all memory-bound operations to reduce data transfer overhead, introducing head-group parallelism to boost throughput, and employing page-level KV cache mapping to align token access patterns with flash organization. In addition, we propose a design space exploration framework that evaluates discrete and compact KVNAND variants to balance weight and KV placement, automatically identifying the optimal design trade-off. These techniques mitigate latency, energy, and reliability concerns, turning flash into a practical medium for long-context KV storage. Evaluations on MHA 7B and GQA 70B LLMs show that KVNAND achieves 1.98\\(\\times\\)/1.94\\(\\times\\)/2.05\\(\\times\\) geomean speedup at 128/1K/10K-token contexts compared to DRAM-equipped IFC designs and addresses out-of-memory failures at 100K context length.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying large language models (LLMs) on edge devices enables personalized agents with strong privacy and low cost. However, with tens to hundreds of billions of parameters, single-batch autoregressive inference suffers from extremely low arithmetic intensity, creating severe weight-loading and bandwidth pressures on resource-constrained platforms. Recent in-flash computing (IFC) solutions alleviate this bottleneck by co-locating weight-related linear computations in the decode phase with flash, yet still rely on DRAM for the key-value (KV) cache. As context length grows, the KV cache can exceed model weights in size, imposing prohibitive DRAM cost and capacity requirements. Attempts to offload KV cache to flash suffer from severe performance penalties.\n  We propose KVNAND, the first DRAM-free, IFC-based architecture that stores both model weights and KV cache entirely in compute-enabled 3D NAND flash. KVNAND addresses the fundamental performance challenges of flash under intensive KV cache access by leveraging IFC for all memory-bound operations to reduce data transfer overhead, introducing head-group parallelism to boost throughput, and employing page-level KV cache mapping to align token access patterns with flash organization. In addition, we propose a design space exploration framework that evaluates discrete and compact KVNAND variants to balance weight and KV placement, automatically identifying the optimal design trade-off. These techniques mitigate latency, energy, and reliability concerns, turning flash into a practical medium for long-context KV storage. Evaluations on MHA 7B and GQA 70B LLMs show that KVNAND achieves 1.98\\(\\times\\)/1.94\\(\\times\\)/2.05\\(\\times\\) geomean speedup at 128/1K/10K-token contexts compared to DRAM-equipped IFC designs and addresses out-of-memory failures at 100K context length."
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-03T09:41:03Z",
                "published_parsed": [
                    2025,
                    12,
                    3,
                    9,
                    41,
                    3,
                    2,
                    337,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR"
                },
                "authors": [
                    {
                        "name": "Lishuo Deng"
                    },
                    {
                        "name": "Shaojie Xu"
                    },
                    {
                        "name": "Jinwu Chen"
                    },
                    {
                        "name": "Changwei Yan"
                    },
                    {
                        "name": "Jiajie Wang"
                    },
                    {
                        "name": "Zhe Jiang"
                    },
                    {
                        "name": "Weiwei Shan"
                    }
                ],
                "author_detail": {
                    "name": "Weiwei Shan"
                },
                "author": "Weiwei Shan"
            },
            {
                "id": "http://arxiv.org/abs/2510.08351v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.08351v2",
                "title": "Fletch: File-System Metadata Caching in Programmable Switches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fletch: File-System Metadata Caching in Programmable Switches"
                },
                "updated": "2025-12-03T09:23:58Z",
                "updated_parsed": [
                    2025,
                    12,
                    3,
                    9,
                    23,
                    58,
                    2,
                    337,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.08351v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.08351v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Fast and scalable metadata management across multiple metadata servers is crucial for distributed file systems to handle numerous files and directories. Client-side caching of frequently accessed metadata can mitigate server loads, but incurs significant overhead and complexity in maintaining cache consistency when the number of clients increases. We explore caching in programmable switches by serving file-system metadata requests from multiple clients on the switch data plane. Despite prior efforts on in-switch key-value caching, they fail to address the path dependencies specific to file-system semantics. We propose Fletch, an in-switch file-system metadata caching framework that leverages programmable switches to serve file-system metadata requests from multiple clients directly in the switch data plane. Unlike prior in-switch key-value caching approaches, Fletch addresses file-system-specific path dependencies under stringent switch resource constraints. We implement Fletch atop Hadoop HDFS and evaluate it on a Tofino-switch testbed using real-world file-system metadata workloads. Fletch achieves up to 181.6% higher throughput than vanilla HDFS and complements client-side caching with additional throughput gains of up to 139.6%. It also incurs low latencies and limited switch resource usage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast and scalable metadata management across multiple metadata servers is crucial for distributed file systems to handle numerous files and directories. Client-side caching of frequently accessed metadata can mitigate server loads, but incurs significant overhead and complexity in maintaining cache consistency when the number of clients increases. We explore caching in programmable switches by serving file-system metadata requests from multiple clients on the switch data plane. Despite prior efforts on in-switch key-value caching, they fail to address the path dependencies specific to file-system semantics. We propose Fletch, an in-switch file-system metadata caching framework that leverages programmable switches to serve file-system metadata requests from multiple clients directly in the switch data plane. Unlike prior in-switch key-value caching approaches, Fletch addresses file-system-specific path dependencies under stringent switch resource constraints. We implement Fletch atop Hadoop HDFS and evaluate it on a Tofino-switch testbed using real-world file-system metadata workloads. Fletch achieves up to 181.6% higher throughput than vanilla HDFS and complements client-side caching with additional throughput gains of up to 139.6%. It also incurs low latencies and limited switch resource usage."
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-09T15:38:13Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    15,
                    38,
                    13,
                    3,
                    282,
                    0
                ],
                "arxiv_comment": "13 pages",
                "arxiv_primary_category": {
                    "term": "cs.AR"
                },
                "authors": [
                    {
                        "name": "Qingxiu Liu"
                    },
                    {
                        "name": "Jiazhen Cai"
                    },
                    {
                        "name": "Siyuan Sheng"
                    },
                    {
                        "name": "Yuhui Chen"
                    },
                    {
                        "name": "Lu Tang"
                    },
                    {
                        "name": "Zhirong Shen"
                    },
                    {
                        "name": "Patrick P. C. Lee"
                    }
                ],
                "author_detail": {
                    "name": "Patrick P. C. Lee"
                },
                "author": "Patrick P. C. Lee"
            },
            {
                "id": "http://arxiv.org/abs/2512.02513v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.02513v2",
                "title": "Decentralized Fairness Aware Multi Task Federated Learning for VR Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decentralized Fairness Aware Multi Task Federated Learning for VR Network"
                },
                "updated": "2025-12-03T08:13:00Z",
                "updated_parsed": [
                    2025,
                    12,
                    3,
                    8,
                    13,
                    0,
                    2,
                    337,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.02513v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.02513v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Wireless connectivity promises to unshackle virtual reality (VR) experiences, allowing users to engage from anywhere, anytime. However, delivering seamless, high-quality, real-time VR video wirelessly is challenging due to the stringent quality of experience requirements, low latency constraints, and limited VR device capabilities. This paper addresses these challenges by introducing a novel decentralized multi task fair federated learning (DMTFL) based caching that caches and prefetches each VR user's field of view (FOV) at base stations (BSs) based on the caching strategies tailored to each BS. In federated learning (FL) in its naive form, often biases toward certain users, and a single global model fails to capture the statistical heterogeneity across users and BSs. In contrast, the proposed DMTFL algorithm personalizes content delivery by learning individual caching models at each BS. These models are further optimized to perform well under any target distribution, while providing theoretical guarantees via Rademacher complexity and a probably approximately correct (PAC) bound on the loss. Using a realistic VR head-tracking dataset, our simulations demonstrate the superiority of our proposed DMTFL algorithm compared to baseline algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wireless connectivity promises to unshackle virtual reality (VR) experiences, allowing users to engage from anywhere, anytime. However, delivering seamless, high-quality, real-time VR video wirelessly is challenging due to the stringent quality of experience requirements, low latency constraints, and limited VR device capabilities. This paper addresses these challenges by introducing a novel decentralized multi task fair federated learning (DMTFL) based caching that caches and prefetches each VR user's field of view (FOV) at base stations (BSs) based on the caching strategies tailored to each BS. In federated learning (FL) in its naive form, often biases toward certain users, and a single global model fails to capture the statistical heterogeneity across users and BSs. In contrast, the proposed DMTFL algorithm personalizes content delivery by learning individual caching models at each BS. These models are further optimized to perform well under any target distribution, while providing theoretical guarantees via Rademacher complexity and a probably approximately correct (PAC) bound on the loss. Using a realistic VR head-tracking dataset, our simulations demonstrate the superiority of our proposed DMTFL algorithm compared to baseline algorithms."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-02T08:13:38Z",
                "published_parsed": [
                    2025,
                    12,
                    2,
                    8,
                    13,
                    38,
                    1,
                    336,
                    0
                ],
                "arxiv_comment": "accepted at IEEE Globecom Workshop 2025",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Krishnendu S. Tharakan"
                    },
                    {
                        "name": "Carlo Fischione"
                    }
                ],
                "author_detail": {
                    "name": "Carlo Fischione"
                },
                "author": "Carlo Fischione"
            },
            {
                "id": "http://arxiv.org/abs/2508.09442v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.09442v2",
                "title": "Shadow in the Cache: Unveiling and Mitigating Privacy Risks of KV-cache in LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shadow in the Cache: Unveiling and Mitigating Privacy Risks of KV-cache in LLM Inference"
                },
                "updated": "2025-12-03T03:07:34Z",
                "updated_parsed": [
                    2025,
                    12,
                    3,
                    3,
                    7,
                    34,
                    2,
                    337,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.09442v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.09442v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The Key-Value (KV) cache, which stores intermediate attention computations (Key and Value pairs) to avoid redundant calculations, is a fundamental mechanism for accelerating Large Language Model (LLM) inference. However, this efficiency optimization introduces significant yet underexplored privacy risks. This paper provides the first comprehensive analysis of these vulnerabilities, demonstrating that an attacker can reconstruct sensitive user inputs directly from the KV-cache. We design and implement three distinct attack vectors: a direct Inversion Attack, a more broadly applicable and potent Collision Attack, and a semantic-based Injection Attack. These methods demonstrate the practicality and severity of KV-cache privacy leakage issues. To mitigate this, we propose KV-Cloak, a novel, lightweight, and efficient defense mechanism. KV-Cloak uses a reversible matrix-based obfuscation scheme, combined with operator fusion, to secure the KV-cache. Our extensive experiments show that KV-Cloak effectively thwarts all proposed attacks, reducing reconstruction quality to random noise. Crucially, it achieves this robust security with virtually no degradation in model accuracy and minimal performance overhead, offering a practical solution for trustworthy LLM deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Key-Value (KV) cache, which stores intermediate attention computations (Key and Value pairs) to avoid redundant calculations, is a fundamental mechanism for accelerating Large Language Model (LLM) inference. However, this efficiency optimization introduces significant yet underexplored privacy risks. This paper provides the first comprehensive analysis of these vulnerabilities, demonstrating that an attacker can reconstruct sensitive user inputs directly from the KV-cache. We design and implement three distinct attack vectors: a direct Inversion Attack, a more broadly applicable and potent Collision Attack, and a semantic-based Injection Attack. These methods demonstrate the practicality and severity of KV-cache privacy leakage issues. To mitigate this, we propose KV-Cloak, a novel, lightweight, and efficient defense mechanism. KV-Cloak uses a reversible matrix-based obfuscation scheme, combined with operator fusion, to secure the KV-cache. Our extensive experiments show that KV-Cloak effectively thwarts all proposed attacks, reducing reconstruction quality to random noise. Crucially, it achieves this robust security with virtually no degradation in model accuracy and minimal performance overhead, offering a practical solution for trustworthy LLM deployment."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-13T02:48:25Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    2,
                    48,
                    25,
                    2,
                    225,
                    0
                ],
                "arxiv_comment": "This paper is accepted by Network and Distributed System Security Symposium (NDSS) 2026",
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Zhifan Luo"
                    },
                    {
                        "name": "Shuo Shao"
                    },
                    {
                        "name": "Su Zhang"
                    },
                    {
                        "name": "Lijing Zhou"
                    },
                    {
                        "name": "Yuke Hu"
                    },
                    {
                        "name": "Chenxu Zhao"
                    },
                    {
                        "name": "Zhihao Liu"
                    },
                    {
                        "name": "Zhan Qin"
                    }
                ],
                "author_detail": {
                    "name": "Zhan Qin"
                },
                "author": "Zhan Qin"
            },
            {
                "id": "http://arxiv.org/abs/2512.03324v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.03324v1",
                "title": "Cache What Lasts: Token Retention for Memory-Bounded KV Cache in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache What Lasts: Token Retention for Memory-Bounded KV Cache in LLMs"
                },
                "updated": "2025-12-03T00:20:35Z",
                "updated_parsed": [
                    2025,
                    12,
                    3,
                    0,
                    20,
                    35,
                    2,
                    337,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.03324v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.03324v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Memory and computation remain core bottlenecks in long-horizon LLM inference due to the quadratic cost of self-attention and the ever-growing key-value (KV) cache. Existing strategies for memory-bounded inference, such as quantization, offloading, or heuristic KV eviction, either incur high orchestration costs or rely on unreliable attention-based proxies of importance. We propose TRIM-KV, a novel approach that learns each token's intrinsic importance at creation time via a lightweight retention gate. Each gate predicts a scalar retention score that decays over time, reflecting the long-term utility of the token for a specific layer and head. Tokens with low scores are evicted when the memory budget is exceeded, ensuring that the cache always contains the most critical tokens. TRIM-KV is trained efficiently through distillation from a frozen LLM combined with a capacity loss, requiring only gate fine-tuning and adding negligible inference overhead. Across mathematical reasoning (GSM8K, MATH-500, AIME24), procedural generation (LongProc), conversational long-memory benchmarks (LongMemEval), and long-context understanding (LongBench and SCBench), TRIM-KV consistently outperforms strong eviction and learnable retrieval baselines, especially in low-memory regimes. Remarkably, it even surpasses full-cache models in some settings, showing that selective retention can serve as a form of regularization, suppressing noise from uninformative tokens. Qualitative analyses further reveal that learned retention scores align with human intuition, naturally recovering heuristics such as sink tokens, sliding windows, and gist compression without explicit design. Beyond efficiency, retention scores provide insights into layer- and head-specific roles, suggesting a new path toward LLM interpretability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory and computation remain core bottlenecks in long-horizon LLM inference due to the quadratic cost of self-attention and the ever-growing key-value (KV) cache. Existing strategies for memory-bounded inference, such as quantization, offloading, or heuristic KV eviction, either incur high orchestration costs or rely on unreliable attention-based proxies of importance. We propose TRIM-KV, a novel approach that learns each token's intrinsic importance at creation time via a lightweight retention gate. Each gate predicts a scalar retention score that decays over time, reflecting the long-term utility of the token for a specific layer and head. Tokens with low scores are evicted when the memory budget is exceeded, ensuring that the cache always contains the most critical tokens. TRIM-KV is trained efficiently through distillation from a frozen LLM combined with a capacity loss, requiring only gate fine-tuning and adding negligible inference overhead. Across mathematical reasoning (GSM8K, MATH-500, AIME24), procedural generation (LongProc), conversational long-memory benchmarks (LongMemEval), and long-context understanding (LongBench and SCBench), TRIM-KV consistently outperforms strong eviction and learnable retrieval baselines, especially in low-memory regimes. Remarkably, it even surpasses full-cache models in some settings, showing that selective retention can serve as a form of regularization, suppressing noise from uninformative tokens. Qualitative analyses further reveal that learned retention scores align with human intuition, naturally recovering heuristics such as sink tokens, sliding windows, and gist compression without explicit design. Beyond efficiency, retention scores provide insights into layer- and head-specific roles, suggesting a new path toward LLM interpretability."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-03T00:20:35Z",
                "published_parsed": [
                    2025,
                    12,
                    3,
                    0,
                    20,
                    35,
                    2,
                    337,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Ngoc Bui"
                    },
                    {
                        "name": "Shubham Sharma"
                    },
                    {
                        "name": "Simran Lamba"
                    },
                    {
                        "name": "Saumitra Mishra"
                    },
                    {
                        "name": "Rex Ying"
                    }
                ],
                "author_detail": {
                    "name": "Rex Ying"
                },
                "author": "Rex Ying"
            },
            {
                "id": "http://arxiv.org/abs/2512.03007v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.03007v1",
                "title": "Generation of strong ultralow-phase-noise microwave fields with tunable ellipticity for ultracold polar molecules",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generation of strong ultralow-phase-noise microwave fields with tunable ellipticity for ultracold polar molecules"
                },
                "updated": "2025-12-02T18:32:11Z",
                "updated_parsed": [
                    2025,
                    12,
                    2,
                    18,
                    32,
                    11,
                    1,
                    336,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.03007v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.03007v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Microwave(MW) fields with strong field strength, ultralow phase-noise and tunable polarization are crucial for stabilizing and manipulating ultracold polar molecules, which have emerged as a promising platform for quantum sciences. In this letter, we present the design, characterization, and performance of a robust MW setup tailored for precise control of molecular states. This setup achieves a high electric field intensity of 6.9 kV/m in the near-field from a dual-feed waveguide antenna, enabling a Rabi frequency as high as 71 MHz for the rotational transition of sodium-potassium molecules. In addition, the low noise signal source and controlled electronics provide ultralow phase-noise and dynamically tunable polarization. Narrow-band filters within the MW circuitry further reduce phase-noise by more than 20 dB at 20 MHz offset frequency, ensuring prolonged one-body molecular lifetimes up to 10 seconds. We also show practical methods to measure the MW field strength and polarization using a simple homemade dipole probe, and to characterize phase-noise down to -170 dBc/Hz with a commercial spectrum analyser and a notch filter. Those capabilities allowed us to evaporatively cool our molecular sample to deep quantum degeneracy. Furthermore, the polarization tunability enabled the observation of field-linked resonances and facilitated the creation of field-linked tetramers.These techniques advance the study of ultracold polar molecules and broaden the potential applications of MW tools in other platforms of quantum sciences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Microwave(MW) fields with strong field strength, ultralow phase-noise and tunable polarization are crucial for stabilizing and manipulating ultracold polar molecules, which have emerged as a promising platform for quantum sciences. In this letter, we present the design, characterization, and performance of a robust MW setup tailored for precise control of molecular states. This setup achieves a high electric field intensity of 6.9 kV/m in the near-field from a dual-feed waveguide antenna, enabling a Rabi frequency as high as 71 MHz for the rotational transition of sodium-potassium molecules. In addition, the low noise signal source and controlled electronics provide ultralow phase-noise and dynamically tunable polarization. Narrow-band filters within the MW circuitry further reduce phase-noise by more than 20 dB at 20 MHz offset frequency, ensuring prolonged one-body molecular lifetimes up to 10 seconds. We also show practical methods to measure the MW field strength and polarization using a simple homemade dipole probe, and to characterize phase-noise down to -170 dBc/Hz with a commercial spectrum analyser and a notch filter. Those capabilities allowed us to evaporatively cool our molecular sample to deep quantum degeneracy. Furthermore, the polarization tunability enabled the observation of field-linked resonances and facilitated the creation of field-linked tetramers.These techniques advance the study of ultracold polar molecules and broaden the potential applications of MW tools in other platforms of quantum sciences."
                },
                "tags": [
                    {
                        "term": "physics.atom-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.quant-gas",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-02T18:32:11Z",
                "published_parsed": [
                    2025,
                    12,
                    2,
                    18,
                    32,
                    11,
                    1,
                    336,
                    0
                ],
                "arxiv_comment": "11 pages, 7 figures",
                "arxiv_primary_category": {
                    "term": "physics.atom-ph"
                },
                "authors": [
                    {
                        "name": "Shrestha Biswas"
                    },
                    {
                        "name": "Sebastian Eppelt"
                    },
                    {
                        "name": "Christian Buchberger"
                    },
                    {
                        "name": "Xing-Yan Chen"
                    },
                    {
                        "name": "Andreas Schindewolf"
                    },
                    {
                        "name": "Michael Hani"
                    },
                    {
                        "name": "Erwin Biebl"
                    },
                    {
                        "name": "Immanuel Bloch"
                    },
                    {
                        "name": "Xin-Yu Luo"
                    }
                ],
                "author_detail": {
                    "name": "Xin-Yu Luo"
                },
                "author": "Xin-Yu Luo"
            },
            {
                "id": "http://arxiv.org/abs/2512.02924v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.02924v1",
                "title": "AutoNeural: Co-Designing Vision-Language Models for NPU Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoNeural: Co-Designing Vision-Language Models for NPU Inference"
                },
                "updated": "2025-12-02T16:45:25Z",
                "updated_parsed": [
                    2025,
                    12,
                    2,
                    16,
                    45,
                    25,
                    1,
                    336,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.02924v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.02924v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "While Neural Processing Units (NPUs) offer high theoretical efficiency for edge AI, state-of-the-art Vision--Language Models (VLMs) tailored for GPUs often falter on these substrates. We attribute this hardware-model mismatch to two primary factors: the quantization brittleness of Vision Transformers (ViTs) and the I/O-bound nature of autoregressive attention mechanisms, which fail to utilize the high arithmetic throughput of NPUs. To bridge this gap, we propose AutoNeural, an NPU-native VLM architecture co-designed for integer-only inference. We replace the standard ViT encoder with a MobileNetV5-style backbone utilizing depthwise separable convolutions, which ensures bounded activation distributions for stable INT4/8/16 quantization. Complementing this, our language backbone integrates State-Space Model (SSM) principles with Transformer layers, employing efficient gated convolutions to achieve linear-time complexity. This hybrid design eliminates the heavy memory I/O overhead of Key-Value caching during generation. Our approach delivers substantial efficiency gains, reducing quantization error of vision encoder by up to 7x and end-to-end latency by 14x compared to conventional baselines. The AutoNeural also delivers 3x decoding speed and 4x longer context window than the baseline. We validate these improvements via a real-world automotive case study on the Qualcomm SA8295P SoC, demonstrating real-time performance for cockpit applications. Our results highlight that rethinking model topology specifically for NPU constraints is a prerequisite for robust multi-modal edge intelligence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Neural Processing Units (NPUs) offer high theoretical efficiency for edge AI, state-of-the-art Vision--Language Models (VLMs) tailored for GPUs often falter on these substrates. We attribute this hardware-model mismatch to two primary factors: the quantization brittleness of Vision Transformers (ViTs) and the I/O-bound nature of autoregressive attention mechanisms, which fail to utilize the high arithmetic throughput of NPUs. To bridge this gap, we propose AutoNeural, an NPU-native VLM architecture co-designed for integer-only inference. We replace the standard ViT encoder with a MobileNetV5-style backbone utilizing depthwise separable convolutions, which ensures bounded activation distributions for stable INT4/8/16 quantization. Complementing this, our language backbone integrates State-Space Model (SSM) principles with Transformer layers, employing efficient gated convolutions to achieve linear-time complexity. This hybrid design eliminates the heavy memory I/O overhead of Key-Value caching during generation. Our approach delivers substantial efficiency gains, reducing quantization error of vision encoder by up to 7x and end-to-end latency by 14x compared to conventional baselines. The AutoNeural also delivers 3x decoding speed and 4x longer context window than the baseline. We validate these improvements via a real-world automotive case study on the Qualcomm SA8295P SoC, demonstrating real-time performance for cockpit applications. Our results highlight that rethinking model topology specifically for NPU constraints is a prerequisite for robust multi-modal edge intelligence."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-02T16:45:25Z",
                "published_parsed": [
                    2025,
                    12,
                    2,
                    16,
                    45,
                    25,
                    1,
                    336,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Wei Chen"
                    },
                    {
                        "name": "Liangmin Wu"
                    },
                    {
                        "name": "Yunhai Hu"
                    },
                    {
                        "name": "Zhiyuan Li"
                    },
                    {
                        "name": "Zhiyuan Cheng"
                    },
                    {
                        "name": "Yicheng Qian"
                    },
                    {
                        "name": "Lingyue Zhu"
                    },
                    {
                        "name": "Zhipeng Hu"
                    },
                    {
                        "name": "Luoyi Liang"
                    },
                    {
                        "name": "Qiang Tang"
                    },
                    {
                        "name": "Zhen Liu"
                    },
                    {
                        "name": "Han Yang"
                    }
                ],
                "author_detail": {
                    "name": "Han Yang"
                },
                "author": "Han Yang"
            },
            {
                "id": "http://arxiv.org/abs/2505.04216v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2505.04216v2",
                "title": "Computational Model for Photoionization in Pure SF6 Streamer at 1-15 atm",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computational Model for Photoionization in Pure SF6 Streamer at 1-15 atm"
                },
                "updated": "2025-12-02T07:05:10Z",
                "updated_parsed": [
                    2025,
                    12,
                    2,
                    7,
                    5,
                    10,
                    1,
                    336,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2505.04216v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2505.04216v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1088/1361-6595/ae259e",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "Photoionization plays a crucial role in achieving accurate quantitative predictions in SF6 streamer simulations, but accurate models for SF6 photoionization remains limited, motivating this paper. First, we develop a computational model for SF6 photoionization and provide the detailed theoretical modeling process, as well as comparison between experiment and simulation. A concise summary of model parameters within the comprehensive pressure range of 1 - 15 atm is provided for direct reference. Then, we perform comparative studies against simplified approaches. The results demonstrate that the proposed model effectively captures the non-local effects of SF6 photoionization, enhancing both the spatial numerical convergence and the accuracy of the streamer structure. Finally, we perform comparative studies by artificially increasing the photoionization intensity through multiplying the photoionization source term Sph by a factor of 50 (50*Sph) relative to the baseline intensity. Regarding breakdown voltage prediction, 50*Sph leads to a significant underestimation of the breakdown voltage for positive streamers, introducing errors greater than 0.5 kV, while exerting a small impact on negative streamers. Regarding streamer propagation dynamics, the radius of the positive streamer head exhibits pronounced shrinking, and 50*Sph reduces this shrinking and significantly lowers the head field by more than 700 Td. In contrast, 50*Sph has little impact on the morphology of the negative streamers and slightly enhances the head field by less than 30 Td.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Photoionization plays a crucial role in achieving accurate quantitative predictions in SF6 streamer simulations, but accurate models for SF6 photoionization remains limited, motivating this paper. First, we develop a computational model for SF6 photoionization and provide the detailed theoretical modeling process, as well as comparison between experiment and simulation. A concise summary of model parameters within the comprehensive pressure range of 1 - 15 atm is provided for direct reference. Then, we perform comparative studies against simplified approaches. The results demonstrate that the proposed model effectively captures the non-local effects of SF6 photoionization, enhancing both the spatial numerical convergence and the accuracy of the streamer structure. Finally, we perform comparative studies by artificially increasing the photoionization intensity through multiplying the photoionization source term Sph by a factor of 50 (50*Sph) relative to the baseline intensity. Regarding breakdown voltage prediction, 50*Sph leads to a significant underestimation of the breakdown voltage for positive streamers, introducing errors greater than 0.5 kV, while exerting a small impact on negative streamers. Regarding streamer propagation dynamics, the radius of the positive streamer head exhibits pronounced shrinking, and 50*Sph reduces this shrinking and significantly lowers the head field by more than 700 Td. In contrast, 50*Sph has little impact on the morphology of the negative streamers and slightly enhances the head field by less than 30 Td."
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-05-07T08:10:39Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    8,
                    10,
                    39,
                    2,
                    127,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph"
                },
                "arxiv_journal_ref": "Plasma Sources Sci. Technol. (2025)",
                "authors": [
                    {
                        "name": "Zihao Feng"
                    },
                    {
                        "name": "Liyang Zhang"
                    },
                    {
                        "name": "Xiaobing Zou"
                    },
                    {
                        "name": "Haiyun Luo"
                    }
                ],
                "author_detail": {
                    "name": "Haiyun Luo"
                },
                "author": "Haiyun Luo",
                "arxiv_doi": "10.1088/1361-6595/ae259e"
            },
            {
                "id": "http://arxiv.org/abs/2512.02444v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.02444v1",
                "title": "QJoin: Transformation-aware Joinable Data Discovery Using Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QJoin: Transformation-aware Joinable Data Discovery Using Reinforcement Learning"
                },
                "updated": "2025-12-02T06:05:48Z",
                "updated_parsed": [
                    2025,
                    12,
                    2,
                    6,
                    5,
                    48,
                    1,
                    336,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.02444v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.02444v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Discovering which tables in large, heterogeneous repositories can be joined and by what transformations is a central challenge in data integration and data discovery. Traditional join discovery methods are largely designed for equi-joins, which assume that join keys match exactly or nearly so. These techniques, while efficient in clean, well-normalized databases, fail in open or federated settings where identifiers are inconsistently formatted, embedded, or split across multiple columns. Approximate or fuzzy joins alleviate minor string variations but cannot capture systematic transformations. We introduce QJoin, a reinforcement-learning framework that learns and reuses transformation strategies across join tasks. QJoin trains an agent under a uniqueness-aware reward that balances similarity with key distinctiveness, enabling it to explore concise, high-value transformation chains. To accelerate new joins, we introduce two reuse mechanisms: (i) agent transfer, which initializes new policies from pretrained agents, and (ii) transformation reuse, which caches successful operator sequences for similar column clusters. On the AutoJoin Web benchmark (31 table pairs), QJoin achieves an average F1-score of 91.0%. For 19,990 join tasks in NYC+Chicago open datasets, Qjoin reduces runtime by up to 7.4% (13,747 s) by using reusing. These results demonstrate that transformation learning and reuse can make join discovery both more accurate and more efficient.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Discovering which tables in large, heterogeneous repositories can be joined and by what transformations is a central challenge in data integration and data discovery. Traditional join discovery methods are largely designed for equi-joins, which assume that join keys match exactly or nearly so. These techniques, while efficient in clean, well-normalized databases, fail in open or federated settings where identifiers are inconsistently formatted, embedded, or split across multiple columns. Approximate or fuzzy joins alleviate minor string variations but cannot capture systematic transformations. We introduce QJoin, a reinforcement-learning framework that learns and reuses transformation strategies across join tasks. QJoin trains an agent under a uniqueness-aware reward that balances similarity with key distinctiveness, enabling it to explore concise, high-value transformation chains. To accelerate new joins, we introduce two reuse mechanisms: (i) agent transfer, which initializes new policies from pretrained agents, and (ii) transformation reuse, which caches successful operator sequences for similar column clusters. On the AutoJoin Web benchmark (31 table pairs), QJoin achieves an average F1-score of 91.0%. For 19,990 join tasks in NYC+Chicago open datasets, Qjoin reduces runtime by up to 7.4% (13,747 s) by using reusing. These results demonstrate that transformation learning and reuse can make join discovery both more accurate and more efficient."
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-02T06:05:48Z",
                "published_parsed": [
                    2025,
                    12,
                    2,
                    6,
                    5,
                    48,
                    1,
                    336,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB"
                },
                "authors": [
                    {
                        "name": "Ning Wang"
                    },
                    {
                        "name": "Sainyam Galhotra"
                    }
                ],
                "author_detail": {
                    "name": "Sainyam Galhotra"
                },
                "author": "Sainyam Galhotra"
            },
            {
                "id": "http://arxiv.org/abs/2512.02337v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.02337v1",
                "title": "SpecPV: Improving Self-Speculative Decoding for Long-Context Generation via Partial Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpecPV: Improving Self-Speculative Decoding for Long-Context Generation via Partial Verification"
                },
                "updated": "2025-12-02T02:15:33Z",
                "updated_parsed": [
                    2025,
                    12,
                    2,
                    2,
                    15,
                    33,
                    1,
                    336,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.02337v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.02337v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Growing demands from tasks like code generation, deep reasoning, and long-document understanding have made long-context generation a crucial capability for large language models (LLMs). Speculative decoding is one of the most direct and effective approaches for accelerating generation. It follows a draft-verify paradigm, where a lightweight draft model proposes several candidate tokens and the target model verifies them. However, we find that as the context length grows, verification becomes the dominant bottleneck. To further accelerate speculative decoding in long-context generation, we introduce SpecPV, a self-speculative decoding approach that performs fast verification using partial key-value states (KV) and periodically applies full verification to eliminate accumulated errors. We validate SpecPV across multiple long-context benchmarks and models, including LLaMA-3.1-8B-Instruct and Qwen3-series. Experimental results show that SpecPV achieves up to 6x decoding speedup over standard autoregressive decoding with minor degradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Growing demands from tasks like code generation, deep reasoning, and long-document understanding have made long-context generation a crucial capability for large language models (LLMs). Speculative decoding is one of the most direct and effective approaches for accelerating generation. It follows a draft-verify paradigm, where a lightweight draft model proposes several candidate tokens and the target model verifies them. However, we find that as the context length grows, verification becomes the dominant bottleneck. To further accelerate speculative decoding in long-context generation, we introduce SpecPV, a self-speculative decoding approach that performs fast verification using partial key-value states (KV) and periodically applies full verification to eliminate accumulated errors. We validate SpecPV across multiple long-context benchmarks and models, including LLaMA-3.1-8B-Instruct and Qwen3-series. Experimental results show that SpecPV achieves up to 6x decoding speedup over standard autoregressive decoding with minor degradation."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-02T02:15:33Z",
                "published_parsed": [
                    2025,
                    12,
                    2,
                    2,
                    15,
                    33,
                    1,
                    336,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Zhendong Tan"
                    },
                    {
                        "name": "Xingjun Zhang"
                    },
                    {
                        "name": "Chaoyi Hu"
                    },
                    {
                        "name": "Junjie Peng"
                    },
                    {
                        "name": "Kun Xia"
                    }
                ],
                "author_detail": {
                    "name": "Kun Xia"
                },
                "author": "Kun Xia"
            },
            {
                "id": "http://arxiv.org/abs/2511.09956v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.09956v2",
                "title": "Optimizing CPU Cache Utilization in Cloud VMs with Accurate Cache Abstraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing CPU Cache Utilization in Cloud VMs with Accurate Cache Abstraction"
                },
                "updated": "2025-12-02T01:24:46Z",
                "updated_parsed": [
                    2025,
                    12,
                    2,
                    1,
                    24,
                    46,
                    1,
                    336,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.09956v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.09956v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "This paper shows that cache-based optimizations are often ineffective in cloud virtual machines (VMs) due to limited visibility into and control over provisioned caches. In public clouds, CPU caches can be partitioned or shared among VMs, but a VM is unaware of cache provisioning details. Moreover, a VM cannot influence cache usage via page placement policies, as memory-to-cache mappings are hidden. The paper proposes a novel solution, CacheX, which probes accurate and fine-grained cache abstraction within VMs using eviction sets without requiring hardware or hypervisor support, and showcases the utility of the probed information with two new techniques: LLC contention-aware task scheduling and virtual color-aware page cache management. Our evaluation of CacheX's implementation in x86 Linux kernel demonstrates that it can effectively improve cache utilization for various workloads in public cloud VMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper shows that cache-based optimizations are often ineffective in cloud virtual machines (VMs) due to limited visibility into and control over provisioned caches. In public clouds, CPU caches can be partitioned or shared among VMs, but a VM is unaware of cache provisioning details. Moreover, a VM cannot influence cache usage via page placement policies, as memory-to-cache mappings are hidden. The paper proposes a novel solution, CacheX, which probes accurate and fine-grained cache abstraction within VMs using eviction sets without requiring hardware or hypervisor support, and showcases the utility of the probed information with two new techniques: LLC contention-aware task scheduling and virtual color-aware page cache management. Our evaluation of CacheX's implementation in x86 Linux kernel demonstrates that it can effectively improve cache utilization for various workloads in public cloud VMs."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T04:37:52Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    4,
                    37,
                    52,
                    3,
                    317,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Mani Tofigh"
                    },
                    {
                        "name": "Edward Guo"
                    },
                    {
                        "name": "Weiwei Jia"
                    },
                    {
                        "name": "Xiaoning Ding"
                    },
                    {
                        "name": "Zirui Neil Zhao"
                    },
                    {
                        "name": "Jianchen Shan"
                    }
                ],
                "author_detail": {
                    "name": "Jianchen Shan"
                },
                "author": "Jianchen Shan"
            },
            {
                "id": "http://arxiv.org/abs/2504.19602v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2504.19602v3",
                "title": "Soft-Label Caching and Sharpening for Communication-Efficient Federated Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Soft-Label Caching and Sharpening for Communication-Efficient Federated Distillation"
                },
                "updated": "2025-12-02T00:43:12Z",
                "updated_parsed": [
                    2025,
                    12,
                    2,
                    0,
                    43,
                    12,
                    1,
                    336,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2504.19602v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2504.19602v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Federated Learning (FL) enables collaborative model training across decentralized clients, enhancing privacy by keeping data local. Yet conventional FL, relying on frequent parameter-sharing, suffers from high communication overhead and limited model heterogeneity. Distillation-based FL approaches address these issues by sharing predictions (soft-labels, i.e., normalized probability distributions) instead, but they often involve redundant transmissions across communication rounds, reducing efficiency. We propose SCARLET, a novel framework integrating synchronized soft-label caching and an enhanced Entropy Reduction Aggregation (Enhanced ERA) mechanism. SCARLET minimizes redundant communication by reusing cached soft-labels, achieving up to 50% reduction in communication costs compared to existing methods while maintaining competitive accuracy. Enhanced ERA resolves the fundamental instability of conventional temperature-based aggregation, ensuring robust control and high performance in diverse client scenarios. Experimental evaluations demonstrate that SCARLET consistently outperforms state-of-the-art distillation-based FL methods in terms of accuracy and communication efficiency. The implementation of SCARLET is publicly available at https://github.com/kitsuyaazuma/SCARLET.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) enables collaborative model training across decentralized clients, enhancing privacy by keeping data local. Yet conventional FL, relying on frequent parameter-sharing, suffers from high communication overhead and limited model heterogeneity. Distillation-based FL approaches address these issues by sharing predictions (soft-labels, i.e., normalized probability distributions) instead, but they often involve redundant transmissions across communication rounds, reducing efficiency. We propose SCARLET, a novel framework integrating synchronized soft-label caching and an enhanced Entropy Reduction Aggregation (Enhanced ERA) mechanism. SCARLET minimizes redundant communication by reusing cached soft-labels, achieving up to 50% reduction in communication costs compared to existing methods while maintaining competitive accuracy. Enhanced ERA resolves the fundamental instability of conventional temperature-based aggregation, ensuring robust control and high performance in diverse client scenarios. Experimental evaluations demonstrate that SCARLET consistently outperforms state-of-the-art distillation-based FL methods in terms of accuracy and communication efficiency. The implementation of SCARLET is publicly available at https://github.com/kitsuyaazuma/SCARLET."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-04-28T09:04:30Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    9,
                    4,
                    30,
                    0,
                    118,
                    0
                ],
                "arxiv_comment": "23 pages, 18 figures",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Kitsuya Azuma"
                    },
                    {
                        "name": "Takayuki Nishio"
                    },
                    {
                        "name": "Yuichi Kitagawa"
                    },
                    {
                        "name": "Wakako Nakano"
                    },
                    {
                        "name": "Takahito Tanimura"
                    }
                ],
                "author_detail": {
                    "name": "Takahito Tanimura"
                },
                "author": "Takahito Tanimura"
            },
            {
                "id": "http://arxiv.org/abs/2512.02281v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.02281v1",
                "title": "Trinity: Disaggregating Vector Search from Prefill-Decode Disaggregation in LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trinity: Disaggregating Vector Search from Prefill-Decode Disaggregation in LLM Serving"
                },
                "updated": "2025-12-01T23:53:42Z",
                "updated_parsed": [
                    2025,
                    12,
                    1,
                    23,
                    53,
                    42,
                    0,
                    335,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.02281v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.02281v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Prefill and decode (PD) disaggregation separates prompt prefill and token-by-token decode stages into distinct GPU pools and has become the dominant architecture for large-scale LLM serving in industry. Also, retrieval tasks via vector search remains entangled with the model inference process, like heterogeneous RAG requests and prompt answer caches, inflating tail latency. We are motivated to investigate how vector search should be orchestrated along with PD disaggregation with a dedicated deployment architecture without violating SLOs in various retrieval workloads. We present Trinity, a practical framework that consolidates all retrieval into a single, shared vector-search GPU pool and make it work with PD disaggregated LLM serving in match. Trinity introduces (1) a novel architecture for deploying GPU-based vector search service in PD disaggregation. (2) Continuous batching for vector search that make full used of GPUs under heterogeneous queries; (3) Stage-aware scheduling that preempts vector search requests between both decode and prefill tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prefill and decode (PD) disaggregation separates prompt prefill and token-by-token decode stages into distinct GPU pools and has become the dominant architecture for large-scale LLM serving in industry. Also, retrieval tasks via vector search remains entangled with the model inference process, like heterogeneous RAG requests and prompt answer caches, inflating tail latency. We are motivated to investigate how vector search should be orchestrated along with PD disaggregation with a dedicated deployment architecture without violating SLOs in various retrieval workloads. We present Trinity, a practical framework that consolidates all retrieval into a single, shared vector-search GPU pool and make it work with PD disaggregated LLM serving in match. Trinity introduces (1) a novel architecture for deploying GPU-based vector search service in PD disaggregation. (2) Continuous batching for vector search that make full used of GPUs under heterogeneous queries; (3) Stage-aware scheduling that preempts vector search requests between both decode and prefill tasks."
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-01T23:53:42Z",
                "published_parsed": [
                    2025,
                    12,
                    1,
                    23,
                    53,
                    42,
                    0,
                    335,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB"
                },
                "authors": [
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Chen Qian"
                    }
                ],
                "author_detail": {
                    "name": "Chen Qian"
                },
                "author": "Chen Qian"
            },
            {
                "id": "http://arxiv.org/abs/2506.05332v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2506.05332v2",
                "title": "Unleashing Hour-Scale Video Training for Long Video-Language Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unleashing Hour-Scale Video Training for Long Video-Language Understanding"
                },
                "updated": "2025-12-01T22:47:17Z",
                "updated_parsed": [
                    2025,
                    12,
                    1,
                    22,
                    47,
                    17,
                    0,
                    335,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2506.05332v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2506.05332v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recent long-form video-language understanding benchmarks have driven progress in video large multimodal models (Video-LMMs). However, the scarcity of well-annotated long videos has left the training of hour-long Video-LMMs underexplored. To close this gap, we present VideoMarathon, a large-scale hour-long video instruction-following dataset. This dataset includes around 9,700 hours of long videos sourced from diverse domains, ranging from 3 to 60 minutes per video. Specifically, it contains 3.3M high-quality QA pairs, spanning six fundamental topics: temporality, spatiality, object, action, scene, and event. Compared to existing video instruction datasets, VideoMarathon significantly extends training video durations up to 1 hour, and supports 22 diverse tasks requiring both short- and long-term video comprehension. Building on VideoMarathon, we propose Hour-LLaVA, a powerful and efficient Video-LMM for hour-scale video-language modeling. It enables hour-long video training and inference at 1-FPS sampling by leveraging a memory augmentation module, which adaptively integrates question-relevant and spatiotemporally informative semantics from the cached full video context. In our experiments, Hour-LLaVA achieves the best performance on multiple representative long video-language benchmarks, demonstrating the high quality of the VideoMarathon dataset and the superiority of the Hour-LLaVA model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent long-form video-language understanding benchmarks have driven progress in video large multimodal models (Video-LMMs). However, the scarcity of well-annotated long videos has left the training of hour-long Video-LMMs underexplored. To close this gap, we present VideoMarathon, a large-scale hour-long video instruction-following dataset. This dataset includes around 9,700 hours of long videos sourced from diverse domains, ranging from 3 to 60 minutes per video. Specifically, it contains 3.3M high-quality QA pairs, spanning six fundamental topics: temporality, spatiality, object, action, scene, and event. Compared to existing video instruction datasets, VideoMarathon significantly extends training video durations up to 1 hour, and supports 22 diverse tasks requiring both short- and long-term video comprehension. Building on VideoMarathon, we propose Hour-LLaVA, a powerful and efficient Video-LMM for hour-scale video-language modeling. It enables hour-long video training and inference at 1-FPS sampling by leveraging a memory augmentation module, which adaptively integrates question-relevant and spatiotemporally informative semantics from the cached full video context. In our experiments, Hour-LLaVA achieves the best performance on multiple representative long video-language benchmarks, demonstrating the high quality of the VideoMarathon dataset and the superiority of the Hour-LLaVA model."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-06-05T17:59:04Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    59,
                    4,
                    3,
                    156,
                    0
                ],
                "arxiv_comment": "NeurIPS 2025, Project page: https://videomarathon.github.io/",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Jingyang Lin"
                    },
                    {
                        "name": "Jialian Wu"
                    },
                    {
                        "name": "Ximeng Sun"
                    },
                    {
                        "name": "Ze Wang"
                    },
                    {
                        "name": "Jiang Liu"
                    },
                    {
                        "name": "Yusheng Su"
                    },
                    {
                        "name": "Xiaodong Yu"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Jiebo Luo"
                    },
                    {
                        "name": "Zicheng Liu"
                    },
                    {
                        "name": "Emad Barsoum"
                    }
                ],
                "author_detail": {
                    "name": "Emad Barsoum"
                },
                "author": "Emad Barsoum"
            },
            {
                "id": "http://arxiv.org/abs/2506.19686v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2506.19686v3",
                "title": "From Memories to Maps: Mechanisms of In-Context Reinforcement Learning in Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Memories to Maps: Mechanisms of In-Context Reinforcement Learning in Transformers"
                },
                "updated": "2025-12-01T21:56:32Z",
                "updated_parsed": [
                    2025,
                    12,
                    1,
                    21,
                    56,
                    32,
                    0,
                    335,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2506.19686v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2506.19686v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Humans and animals show remarkable learning efficiency, adapting to new environments with minimal experience. This capability is not well captured by standard reinforcement learning algorithms that rely on incremental value updates. Rapid adaptation likely depends on episodic memory -- the ability to retrieve specific past experiences to guide decisions in novel contexts. Transformers provide a useful setting for studying these questions because of their ability to learn rapidly in-context and because their key-value architecture resembles episodic memory systems in the brain. We train a transformer to in-context reinforcement learn in a distribution of planning tasks inspired by rodent behavior. We then characterize the learning algorithms that emerge in the model. We first find that representation learning is supported by in-context structure learning and cross-context alignment, where representations are aligned across environments with different sensory stimuli. We next demonstrate that the reinforcement learning strategies developed by the model are not interpretable as standard model-free or model-based planning. Instead, we show that in-context reinforcement learning is supported by caching intermediate computations within the model's memory tokens, which are then accessed at decision time. Overall, we find that memory may serve as a computational resource, storing both raw experience and cached computations to support flexible behavior. Furthermore, the representations developed in the model resemble computations associated with the hippocampal-entorhinal system in the brain, suggesting that our findings may be relevant for natural cognition. Taken together, our work offers a mechanistic hypothesis for the rapid adaptation that underlies in-context learning in artificial and natural settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Humans and animals show remarkable learning efficiency, adapting to new environments with minimal experience. This capability is not well captured by standard reinforcement learning algorithms that rely on incremental value updates. Rapid adaptation likely depends on episodic memory -- the ability to retrieve specific past experiences to guide decisions in novel contexts. Transformers provide a useful setting for studying these questions because of their ability to learn rapidly in-context and because their key-value architecture resembles episodic memory systems in the brain. We train a transformer to in-context reinforcement learn in a distribution of planning tasks inspired by rodent behavior. We then characterize the learning algorithms that emerge in the model. We first find that representation learning is supported by in-context structure learning and cross-context alignment, where representations are aligned across environments with different sensory stimuli. We next demonstrate that the reinforcement learning strategies developed by the model are not interpretable as standard model-free or model-based planning. Instead, we show that in-context reinforcement learning is supported by caching intermediate computations within the model's memory tokens, which are then accessed at decision time. Overall, we find that memory may serve as a computational resource, storing both raw experience and cached computations to support flexible behavior. Furthermore, the representations developed in the model resemble computations associated with the hippocampal-entorhinal system in the brain, suggesting that our findings may be relevant for natural cognition. Taken together, our work offers a mechanistic hypothesis for the rapid adaptation that underlies in-context learning in artificial and natural settings."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-06-24T14:55:43Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    14,
                    55,
                    43,
                    1,
                    175,
                    0
                ],
                "arxiv_comment": "Revised to around 9 pages",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Ching Fang"
                    },
                    {
                        "name": "Kanaka Rajan"
                    }
                ],
                "author_detail": {
                    "name": "Kanaka Rajan"
                },
                "author": "Kanaka Rajan"
            },
            {
                "id": "http://arxiv.org/abs/2512.02189v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.02189v1",
                "title": "Microbenchmarking NVIDIA's Blackwell Architecture: An in-depth Architectural Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Microbenchmarking NVIDIA's Blackwell Architecture: An in-depth Architectural Analysis"
                },
                "updated": "2025-12-01T20:31:10Z",
                "updated_parsed": [
                    2025,
                    12,
                    1,
                    20,
                    31,
                    10,
                    0,
                    335,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.02189v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.02189v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "As GPU architectures rapidly evolve to meet the overcoming demands of exascale computing and machine learning, the performance implications of architectural innovations remain poorly understood across diverse workloads. NVIDIA's Blackwell (B200) generation introduce significant architectural advances including the 5th generation tensor cores, tensor memory (TMEM), decompression engine (DE), and dual chips; however systematic methodologies for quantifying these improvements lag behind hardware development cycles. We contribute an open-source microbenchmark suite that offers practical insights into optimizing workloads to fully utilize the rich feature sets of the modern GPU architecture. This work aims to enable application developers make informed architectural decisions and guide future GPU design directions.\n  Our work studies Blackwell GPUs, compares them to H200 generation with regards to the memory subsystem, tensor core pipeline and floating-point precisions (FP32, FP16, FP8, FP6, FP4). Our systematic evaluation of dense/sparse GEMM, transformer inference, and training workloads demonstrate that B200's tensor core enhancements achieves 1.56x higher mixed-precision throughput and 42% better energy efficiency than H200. Our memory analysis reveals 58% reduction in memory access latency in cache-misses, fundamentally changing optimal algorithm design strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As GPU architectures rapidly evolve to meet the overcoming demands of exascale computing and machine learning, the performance implications of architectural innovations remain poorly understood across diverse workloads. NVIDIA's Blackwell (B200) generation introduce significant architectural advances including the 5th generation tensor cores, tensor memory (TMEM), decompression engine (DE), and dual chips; however systematic methodologies for quantifying these improvements lag behind hardware development cycles. We contribute an open-source microbenchmark suite that offers practical insights into optimizing workloads to fully utilize the rich feature sets of the modern GPU architecture. This work aims to enable application developers make informed architectural decisions and guide future GPU design directions.\n  Our work studies Blackwell GPUs, compares them to H200 generation with regards to the memory subsystem, tensor core pipeline and floating-point precisions (FP32, FP16, FP8, FP6, FP4). Our systematic evaluation of dense/sparse GEMM, transformer inference, and training workloads demonstrate that B200's tensor core enhancements achieves 1.56x higher mixed-precision throughput and 42% better energy efficiency than H200. Our memory analysis reveals 58% reduction in memory access latency in cache-misses, fundamentally changing optimal algorithm design strategies."
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-01T20:31:10Z",
                "published_parsed": [
                    2025,
                    12,
                    1,
                    20,
                    31,
                    10,
                    0,
                    335,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR"
                },
                "authors": [
                    {
                        "name": "Aaron Jarmusch"
                    },
                    {
                        "name": "Sunita Chandrasekaran"
                    }
                ],
                "author_detail": {
                    "name": "Sunita Chandrasekaran"
                },
                "author": "Sunita Chandrasekaran"
            },
            {
                "id": "http://arxiv.org/abs/2512.01953v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.01953v1",
                "title": "KV Pareto: Systems-Level Optimization of KV Cache and Model Compression for Long Context Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV Pareto: Systems-Level Optimization of KV Cache and Model Compression for Long Context Inference"
                },
                "updated": "2025-12-01T18:03:47Z",
                "updated_parsed": [
                    2025,
                    12,
                    1,
                    18,
                    3,
                    47,
                    0,
                    335,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.01953v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.01953v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Long-context Large Language Models (LLMs) face significant memory bottlenecks during inference due to the linear growth of key-value (KV) cache with sequence length. While individual optimization techniques like KV cache quantization, chunked prefill, and model weight quantization have shown promise, their joint effects and optimal configurations for edge deployment remain underexplored. We introduce KV Pareto, a systems-level framework that systematically maps the trade-off frontier between total memory consumption and task accuracy across these three complementary optimization techniques. Our framework evaluates multiple LLM architectures (Qwen, Llama, Mistral) with varying KV quantization schemes (int2/4/8, mixed-precision), granularities (per-token, per-tensor, per-block), and 4-bit weight quantization via AWQ. Our framework identifies model-specific Pareto-optimal configurations that achieve 68-78% total memory reduction with minimal (1-3%) accuracy degradation on long-context tasks. We additionally verify the selected frontiers on additional benchmarks of Needle-in-a-Haystack, GSM8k and MMLU as well as extended context lengths of up to 128k to demonstrate the practical need of joint optimization for efficient LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context Large Language Models (LLMs) face significant memory bottlenecks during inference due to the linear growth of key-value (KV) cache with sequence length. While individual optimization techniques like KV cache quantization, chunked prefill, and model weight quantization have shown promise, their joint effects and optimal configurations for edge deployment remain underexplored. We introduce KV Pareto, a systems-level framework that systematically maps the trade-off frontier between total memory consumption and task accuracy across these three complementary optimization techniques. Our framework evaluates multiple LLM architectures (Qwen, Llama, Mistral) with varying KV quantization schemes (int2/4/8, mixed-precision), granularities (per-token, per-tensor, per-block), and 4-bit weight quantization via AWQ. Our framework identifies model-specific Pareto-optimal configurations that achieve 68-78% total memory reduction with minimal (1-3%) accuracy degradation on long-context tasks. We additionally verify the selected frontiers on additional benchmarks of Needle-in-a-Haystack, GSM8k and MMLU as well as extended context lengths of up to 128k to demonstrate the practical need of joint optimization for efficient LLM inference."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-01T18:03:47Z",
                "published_parsed": [
                    2025,
                    12,
                    1,
                    18,
                    3,
                    47,
                    0,
                    335,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Sai Gokhale"
                    },
                    {
                        "name": "Devleena Das"
                    },
                    {
                        "name": "Rajeev Patwari"
                    },
                    {
                        "name": "Ashish Sirasao"
                    },
                    {
                        "name": "Elliott Delaye"
                    }
                ],
                "author_detail": {
                    "name": "Elliott Delaye"
                },
                "author": "Elliott Delaye"
            },
            {
                "id": "http://arxiv.org/abs/2506.04844v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2506.04844v2",
                "title": "Characterization of the Hamamatsu R12699-406-M4 Photomultiplier Tube in Cold Xenon Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Characterization of the Hamamatsu R12699-406-M4 Photomultiplier Tube in Cold Xenon Environments"
                },
                "updated": "2025-12-01T17:42:16Z",
                "updated_parsed": [
                    2025,
                    12,
                    1,
                    17,
                    42,
                    16,
                    0,
                    335,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2506.04844v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2506.04844v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The Hamamatsu R12699-406-M2 is a $2\\times2$ multi-anode 2-inch photomultiplier tube that offers a compact form factor, low intrinsic radioactivity, and high photocathode coverage. These characteristics make it a promising candidate for next-generation xenon-based direct detection dark matter experiments, such as XLZD and PandaX-xT. We present a detailed characterization of this photosensor operated in cold xenon environments, focusing on its single photoelectron response, dark count rate, light emission, and afterpulsing behavior. The device demonstrated a gain exceeding $2\\cdot 10^6$ at the nominal voltage of -1.0 kV, along with a low dark count rate of $(0.4\\pm0.2)\\;\\text{Hz/cm}^2$. Due to the compact design, afterpulses exhibited short delay times, resulting in some cases in an overlap with the light-induced signal. To evaluate its applicability in a realistic detector environment, two R12699-406-M2 units were deployed in a small-scale dual-phase xenon time projection chamber. The segmented $2\\times2$ anode structure enabled lateral position reconstruction using a single photomultiplier tube, highlighting the potential of the sensor for effective event localization in future detectors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Hamamatsu R12699-406-M2 is a $2\\times2$ multi-anode 2-inch photomultiplier tube that offers a compact form factor, low intrinsic radioactivity, and high photocathode coverage. These characteristics make it a promising candidate for next-generation xenon-based direct detection dark matter experiments, such as XLZD and PandaX-xT. We present a detailed characterization of this photosensor operated in cold xenon environments, focusing on its single photoelectron response, dark count rate, light emission, and afterpulsing behavior. The device demonstrated a gain exceeding $2\\cdot 10^6$ at the nominal voltage of -1.0 kV, along with a low dark count rate of $(0.4\\pm0.2)\\;\\text{Hz/cm}^2$. Due to the compact design, afterpulses exhibited short delay times, resulting in some cases in an overlap with the light-induced signal. To evaluate its applicability in a realistic detector environment, two R12699-406-M2 units were deployed in a small-scale dual-phase xenon time projection chamber. The segmented $2\\times2$ anode structure enabled lateral position reconstruction using a single photomultiplier tube, highlighting the potential of the sensor for effective event localization in future detectors."
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-06-05T10:11:04Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    10,
                    11,
                    4,
                    3,
                    156,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det"
                },
                "authors": [
                    {
                        "name": "M. Adrover"
                    },
                    {
                        "name": "L. Baudis"
                    },
                    {
                        "name": "A. Bismark"
                    },
                    {
                        "name": "A. P. Colijn"
                    },
                    {
                        "name": "J. J. Cuenca-Garca"
                    },
                    {
                        "name": "M. P. Decowski"
                    },
                    {
                        "name": "M. Flierman"
                    },
                    {
                        "name": "T. den Hollander"
                    }
                ],
                "author_detail": {
                    "name": "T. den Hollander"
                },
                "author": "T. den Hollander"
            },
            {
                "id": "http://arxiv.org/abs/2512.01915v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.01915v1",
                "title": "A Low-Cost Reliable Racetrack Cache Based on Data Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Low-Cost Reliable Racetrack Cache Based on Data Compression"
                },
                "updated": "2025-12-01T17:32:25Z",
                "updated_parsed": [
                    2025,
                    12,
                    1,
                    17,
                    32,
                    25,
                    0,
                    335,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.01915v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.01915v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "SRAM-based cache memory faces several scalability limitations in deep nanoscale technologies, e.g., high leakage current, low cell stability, and low density. Emerging Non-Volatile Memory (NVM) technologies have received lots of attention in recent years, where Racetrack Memory (RTM) is among the most promising ones. RTM has the highest density among all NVMs and its access performance is comparable to SRAM technology. Therefore, RTM is a suitable alternative for SRAM in the Last-Level Caches (LLCs). Despite all its benefits, RTM confronts different reliability challenges due to the stochastic behavior of its storage element and highly error-prone data shifting, leading to a high probability of multiple-bit errors. Conventional Error-Correcting Codes (ECCs) are either incapable of tolerating multiple-bit errors or require a large amount of extra storage for check bits. This paper proposes taking advantage of value locality for compressing data blocks and freeing up a large fraction of cache blocks for storing data redundancy of strong ECCs. Utilizing the proposed scheme, a large majority of cache blocks are protected by strong ECCs to tolerate multiple-bit errors without any storage overhead. The evaluation using gem5 full-system simulator demonstrates that the proposed scheme enhances the mean-time-to-failure of the cache by an average of 11.3x with less than 1% hardware and performance overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SRAM-based cache memory faces several scalability limitations in deep nanoscale technologies, e.g., high leakage current, low cell stability, and low density. Emerging Non-Volatile Memory (NVM) technologies have received lots of attention in recent years, where Racetrack Memory (RTM) is among the most promising ones. RTM has the highest density among all NVMs and its access performance is comparable to SRAM technology. Therefore, RTM is a suitable alternative for SRAM in the Last-Level Caches (LLCs). Despite all its benefits, RTM confronts different reliability challenges due to the stochastic behavior of its storage element and highly error-prone data shifting, leading to a high probability of multiple-bit errors. Conventional Error-Correcting Codes (ECCs) are either incapable of tolerating multiple-bit errors or require a large amount of extra storage for check bits. This paper proposes taking advantage of value locality for compressing data blocks and freeing up a large fraction of cache blocks for storing data redundancy of strong ECCs. Utilizing the proposed scheme, a large majority of cache blocks are protected by strong ECCs to tolerate multiple-bit errors without any storage overhead. The evaluation using gem5 full-system simulator demonstrates that the proposed scheme enhances the mean-time-to-failure of the cache by an average of 11.3x with less than 1% hardware and performance overhead."
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-01T17:32:25Z",
                "published_parsed": [
                    2025,
                    12,
                    1,
                    17,
                    32,
                    25,
                    0,
                    335,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET"
                },
                "authors": [
                    {
                        "name": "Elham Cheshmikhani"
                    },
                    {
                        "name": "Fateme Shokouhinia"
                    },
                    {
                        "name": "Hamed Farbeh"
                    }
                ],
                "author_detail": {
                    "name": "Hamed Farbeh"
                },
                "author": "Hamed Farbeh"
            },
            {
                "id": "http://arxiv.org/abs/2512.01802v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.01802v1",
                "title": "JFR: An Efficient Jump Frontier Relaxation Strategy for Bellman-Ford",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JFR: An Efficient Jump Frontier Relaxation Strategy for Bellman-Ford"
                },
                "updated": "2025-12-01T15:35:53Z",
                "updated_parsed": [
                    2025,
                    12,
                    1,
                    15,
                    35,
                    53,
                    0,
                    335,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.01802v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.01802v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We propose JFR, a Bellman-Ford-based optimization framework leveraging frontier contraction and abstract multi-hop jump propagation to accelerate shortest-path computation while strictly preserving correctness. JFR achieves substantial reductions in relaxation operations, ranging from 25 to 99 percent, across sparse, dense, and negative-edge graphs, ensuring robust performance even under adversarial or highly connected topologies. On ultra-large graphs with up to N=20,000 nodes and 295 million edges, JFR maintains strong operational reductions and comparable or improved runtime relative to SPFA-SLF, demonstrating consistent robustness across graph size and density. Lower relaxation counts imply reduced memory-access overheads and computational effort; this normalized work reduction highlights JFR's suitability for scenarios requiring high throughput or energy-conscious operation. Future work focuses on integrating high-performance queue structures, adaptive frontier strategies, and cache-aware techniques to further reduce constant-factor overheads and fully realize JFR's practical runtime potential.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose JFR, a Bellman-Ford-based optimization framework leveraging frontier contraction and abstract multi-hop jump propagation to accelerate shortest-path computation while strictly preserving correctness. JFR achieves substantial reductions in relaxation operations, ranging from 25 to 99 percent, across sparse, dense, and negative-edge graphs, ensuring robust performance even under adversarial or highly connected topologies. On ultra-large graphs with up to N=20,000 nodes and 295 million edges, JFR maintains strong operational reductions and comparable or improved runtime relative to SPFA-SLF, demonstrating consistent robustness across graph size and density. Lower relaxation counts imply reduced memory-access overheads and computational effort; this normalized work reduction highlights JFR's suitability for scenarios requiring high throughput or energy-conscious operation. Future work focuses on integrating high-performance queue structures, adaptive frontier strategies, and cache-aware techniques to further reduce constant-factor overheads and fully realize JFR's practical runtime potential."
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-01T15:35:53Z",
                "published_parsed": [
                    2025,
                    12,
                    1,
                    15,
                    35,
                    53,
                    0,
                    335,
                    0
                ],
                "arxiv_comment": "with editor,22 pages",
                "arxiv_primary_category": {
                    "term": "cs.DS"
                },
                "authors": [
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Xi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xi Chen"
                },
                "author": "Xi Chen"
            },
            {
                "id": "http://arxiv.org/abs/2512.01646v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.01646v1",
                "title": "StarDist: A Code Generator for Distributed Graph Algorithms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StarDist: A Code Generator for Distributed Graph Algorithms"
                },
                "updated": "2025-12-01T13:18:32Z",
                "updated_parsed": [
                    2025,
                    12,
                    1,
                    13,
                    18,
                    32,
                    0,
                    335,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.01646v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.01646v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Relational data, occurring in the real world, are often structured as graphs, which provide the logical abstraction required to make analytical derivations simpler. As graphs get larger, the irregular access patterns exhibited in most graph algorithms, hamper performance. This, along with NUMA and physical memory limits, results in scaling complexities with sequential/shared memory frameworks. StarPlat's MPI backend abstracts away the programmatic complexity involved in designing optimal distributed graph algorithms. It provides an instrument for coding graph algorithms that scale over distributed memory. In this work, we provide an analysis-transformation framework that leverages general semantics associated with iterations involving nodes and their neighbors, within StarPlat, to aggregate communication. The framework scans for patterns that warrant re-ordering in neighborhood access patterns, aggregate communication, and avoid communication altogether with opportunistic caching in reduction constructs. We also architect an optimized bulk-reduction substrate using Open MPI's passive Remote Memory Access (RMA) constructs. We applied our optimization logic to StarPlat's distributed backend and outperformed d-Galois by 2.05 and DRONE by 1.44 times in Single Source Shortest Paths across several big data graphs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Relational data, occurring in the real world, are often structured as graphs, which provide the logical abstraction required to make analytical derivations simpler. As graphs get larger, the irregular access patterns exhibited in most graph algorithms, hamper performance. This, along with NUMA and physical memory limits, results in scaling complexities with sequential/shared memory frameworks. StarPlat's MPI backend abstracts away the programmatic complexity involved in designing optimal distributed graph algorithms. It provides an instrument for coding graph algorithms that scale over distributed memory. In this work, we provide an analysis-transformation framework that leverages general semantics associated with iterations involving nodes and their neighbors, within StarPlat, to aggregate communication. The framework scans for patterns that warrant re-ordering in neighborhood access patterns, aggregate communication, and avoid communication altogether with opportunistic caching in reduction constructs. We also architect an optimized bulk-reduction substrate using Open MPI's passive Remote Memory Access (RMA) constructs. We applied our optimization logic to StarPlat's distributed backend and outperformed d-Galois by 2.05 and DRONE by 1.44 times in Single Source Shortest Paths across several big data graphs."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-01T13:18:32Z",
                "published_parsed": [
                    2025,
                    12,
                    1,
                    13,
                    18,
                    32,
                    0,
                    335,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Barenya Kumar Nandy"
                    },
                    {
                        "name": "Rupesh Nasre"
                    }
                ],
                "author_detail": {
                    "name": "Rupesh Nasre"
                },
                "author": "Rupesh Nasre"
            },
            {
                "id": "http://arxiv.org/abs/2510.23649v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.23649v2",
                "title": "Efficient Low Rank Attention for Long-Context Inference in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Low Rank Attention for Long-Context Inference in Large Language Models"
                },
                "updated": "2025-12-01T12:51:25Z",
                "updated_parsed": [
                    2025,
                    12,
                    1,
                    12,
                    51,
                    25,
                    0,
                    335,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.23649v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.23649v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "As the length of input text grows, the key-value (KV) cache in LLMs imposes prohibitive GPU memory costs and limits long-context inference on resource constrained devices. Existing approaches, such as KV quantization and pruning, reduce memory usage but suffer from numerical precision loss or suboptimal retention of key-value pairs. We introduce Low Rank Query and Key attention (LRQK), a two-stage framework that jointly decomposes the full-precision query and key matrices into compact rank-\\(r\\) factors during the prefill stage, and then uses these low-dimensional projections to compute proxy attention scores in \\(\\mathcal{O}(lr)\\) time at each decode step. By selecting only the top-\\(k\\) tokens and a small fixed set of recent tokens, LRQK employs a mixed GPU-CPU cache with a hit-and-miss mechanism that transfers only missing full-precision KV pairs, thereby preserving exact attention outputs while reducing CPU-GPU data movement. Extensive experiments on the RULER and LongBench benchmarks with LLaMA-3-8B and Qwen2.5-7B demonstrate that LRQK matches or surpasses leading sparse-attention methods in long context settings, while delivering significant memory savings with minimal loss in accuracy. Our code is available at https://github.com/tenghuilee/LRQK.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the length of input text grows, the key-value (KV) cache in LLMs imposes prohibitive GPU memory costs and limits long-context inference on resource constrained devices. Existing approaches, such as KV quantization and pruning, reduce memory usage but suffer from numerical precision loss or suboptimal retention of key-value pairs. We introduce Low Rank Query and Key attention (LRQK), a two-stage framework that jointly decomposes the full-precision query and key matrices into compact rank-\\(r\\) factors during the prefill stage, and then uses these low-dimensional projections to compute proxy attention scores in \\(\\mathcal{O}(lr)\\) time at each decode step. By selecting only the top-\\(k\\) tokens and a small fixed set of recent tokens, LRQK employs a mixed GPU-CPU cache with a hit-and-miss mechanism that transfers only missing full-precision KV pairs, thereby preserving exact attention outputs while reducing CPU-GPU data movement. Extensive experiments on the RULER and LongBench benchmarks with LLaMA-3-8B and Qwen2.5-7B demonstrate that LRQK matches or surpasses leading sparse-attention methods in long context settings, while delivering significant memory savings with minimal loss in accuracy. Our code is available at https://github.com/tenghuilee/LRQK."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-25T11:43:27Z",
                "published_parsed": [
                    2025,
                    10,
                    25,
                    11,
                    43,
                    27,
                    5,
                    298,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Tenghui Li"
                    },
                    {
                        "name": "Guoxu Zhou"
                    },
                    {
                        "name": "Xuyang Zhao"
                    },
                    {
                        "name": "Yuning Qiu"
                    },
                    {
                        "name": "Qibin Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Qibin Zhao"
                },
                "author": "Qibin Zhao"
            },
            {
                "id": "http://arxiv.org/abs/2512.01541v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.01541v1",
                "title": "RoMe: Row Granularity Access Memory System for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RoMe: Row Granularity Access Memory System for Large Language Models"
                },
                "updated": "2025-12-01T11:14:31Z",
                "updated_parsed": [
                    2025,
                    12,
                    1,
                    11,
                    14,
                    31,
                    0,
                    335,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.01541v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.01541v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Modern HBM-based memory systems have evolved over generations while retaining cache line granularity accesses. Preserving this fine granularity necessitated the introduction of bank groups and pseudo channels. These structures expand timing parameters and control overhead, significantly increasing memory controller scheduling complexity. Large language models (LLMs) now dominate deep learning workloads, streaming contiguous data blocks ranging from several kilobytes to megabytes per operation. In a conventional HBM-based memory system, these transfers are fragmented into hundreds of 32B cache line transactions. This forces the memory controller to employ unnecessarily intricate scheduling, leading to growing inefficiency.\n  To address this problem, we propose RoMe. RoMe accesses DRAM at row granularity and removes columns, bank groups, and pseudo channels from the memory interface. This design simplifies memory scheduling, thereby requiring fewer pins per channel. The freed pins are aggregated to form additional channels, increasing overall bandwidth by 12.5% with minimal extra pins. RoMe demonstrates how memory scheduling logic can be significantly simplified for representative LLM workloads, and presents an alternative approach for next-generation HBM-based memory systems achieving increased bandwidth with minimal hardware overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern HBM-based memory systems have evolved over generations while retaining cache line granularity accesses. Preserving this fine granularity necessitated the introduction of bank groups and pseudo channels. These structures expand timing parameters and control overhead, significantly increasing memory controller scheduling complexity. Large language models (LLMs) now dominate deep learning workloads, streaming contiguous data blocks ranging from several kilobytes to megabytes per operation. In a conventional HBM-based memory system, these transfers are fragmented into hundreds of 32B cache line transactions. This forces the memory controller to employ unnecessarily intricate scheduling, leading to growing inefficiency.\n  To address this problem, we propose RoMe. RoMe accesses DRAM at row granularity and removes columns, bank groups, and pseudo channels from the memory interface. This design simplifies memory scheduling, thereby requiring fewer pins per channel. The freed pins are aggregated to form additional channels, increasing overall bandwidth by 12.5% with minimal extra pins. RoMe demonstrates how memory scheduling logic can be significantly simplified for representative LLM workloads, and presents an alternative approach for next-generation HBM-based memory systems achieving increased bandwidth with minimal hardware overhead."
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-01T11:14:31Z",
                "published_parsed": [
                    2025,
                    12,
                    1,
                    11,
                    14,
                    31,
                    0,
                    335,
                    0
                ],
                "arxiv_comment": "15 pages, 14 figures, accepted at HPCA 2026",
                "arxiv_primary_category": {
                    "term": "cs.AR"
                },
                "authors": [
                    {
                        "name": "Hwayong Nam"
                    },
                    {
                        "name": "Seungmin Baek"
                    },
                    {
                        "name": "Jumin Kim"
                    },
                    {
                        "name": "Michael Jaemin Kim"
                    },
                    {
                        "name": "Jung Ho Ahn"
                    }
                ],
                "author_detail": {
                    "name": "Jung Ho Ahn"
                },
                "author": "Jung Ho Ahn"
            },
            {
                "id": "http://arxiv.org/abs/2512.01540v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.01540v1",
                "title": "FlashVGGT: Efficient and Scalable Visual Geometry Transformers with Compressed Descriptor Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashVGGT: Efficient and Scalable Visual Geometry Transformers with Compressed Descriptor Attention"
                },
                "updated": "2025-12-01T11:12:37Z",
                "updated_parsed": [
                    2025,
                    12,
                    1,
                    11,
                    12,
                    37,
                    0,
                    335,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.01540v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.01540v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "3D reconstruction from multi-view images is a core challenge in computer vision. Recently, feed-forward methods have emerged as efficient and robust alternatives to traditional per-scene optimization techniques. Among them, state-of-the-art models like the Visual Geometry Grounding Transformer (VGGT) leverage full self-attention over all image tokens to capture global relationships. However, this approach suffers from poor scalability due to the quadratic complexity of self-attention and the large number of tokens generated in long image sequences. In this work, we introduce FlashVGGT, an efficient alternative that addresses this bottleneck through a descriptor-based attention mechanism. Instead of applying dense global attention across all tokens, FlashVGGT compresses spatial information from each frame into a compact set of descriptor tokens. Global attention is then computed as cross-attention between the full set of image tokens and this smaller descriptor set, significantly reducing computational overhead. Moreover, the compactness of the descriptors enables online inference over long sequences via a chunk-recursive mechanism that reuses cached descriptors from previous chunks. Experimental results show that FlashVGGT achieves reconstruction accuracy competitive with VGGT while reducing inference time to just 9.3% of VGGT for 1,000 images, and scaling efficiently to sequences exceeding 3,000 images. Our project page is available at https://wzpscott.github.io/flashvggt_page/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D reconstruction from multi-view images is a core challenge in computer vision. Recently, feed-forward methods have emerged as efficient and robust alternatives to traditional per-scene optimization techniques. Among them, state-of-the-art models like the Visual Geometry Grounding Transformer (VGGT) leverage full self-attention over all image tokens to capture global relationships. However, this approach suffers from poor scalability due to the quadratic complexity of self-attention and the large number of tokens generated in long image sequences. In this work, we introduce FlashVGGT, an efficient alternative that addresses this bottleneck through a descriptor-based attention mechanism. Instead of applying dense global attention across all tokens, FlashVGGT compresses spatial information from each frame into a compact set of descriptor tokens. Global attention is then computed as cross-attention between the full set of image tokens and this smaller descriptor set, significantly reducing computational overhead. Moreover, the compactness of the descriptors enables online inference over long sequences via a chunk-recursive mechanism that reuses cached descriptors from previous chunks. Experimental results show that FlashVGGT achieves reconstruction accuracy competitive with VGGT while reducing inference time to just 9.3% of VGGT for 1,000 images, and scaling efficiently to sequences exceeding 3,000 images. Our project page is available at https://wzpscott.github.io/flashvggt_page/."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-01T11:12:37Z",
                "published_parsed": [
                    2025,
                    12,
                    1,
                    11,
                    12,
                    37,
                    0,
                    335,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Zipeng Wang"
                    },
                    {
                        "name": "Dan Xu"
                    }
                ],
                "author_detail": {
                    "name": "Dan Xu"
                },
                "author": "Dan Xu"
            },
            {
                "id": "http://arxiv.org/abs/2512.01357v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.01357v1",
                "title": "Tangram: Accelerating Serverless LLM Loading through GPU Memory Reuse and Affinity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tangram: Accelerating Serverless LLM Loading through GPU Memory Reuse and Affinity"
                },
                "updated": "2025-12-01T07:10:34Z",
                "updated_parsed": [
                    2025,
                    12,
                    1,
                    7,
                    10,
                    34,
                    0,
                    335,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.01357v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.01357v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Serverless Large Language Models (LLMs) have emerged as a cost-effective solution for deploying AI services by enabling a 'pay-as-you-go' pricing model through GPU resource sharing. However, cold-start latency, especially the model loading phase, has become a critical performance bottleneck, as it scales linearly with model size and severely limits the practical deployment of large-scale LLM services. This paper presents Tangram, a novel system that accelerates Serverless LLM loading through efficient GPU memory reuse. By leveraging the unused GPU memory to retain model parameters, Tangram significantly reduces model transfer time and cold-start latency. Its design includes three key components: unified GPU memory pool for tensor-level parameter sharing across models, on-demand KV cache allocation for dynamic memory management, and GPU-affinity-aware scheduling for maximizing resource utilization. These techniques collectively address the critical challenges of inefficient memory usage and the cold-start problem in Serverless LLM platforms. We have implemented a fully functional prototype, and experiments show that Tangram achieves up to 6.2 times faster loading and reduces Time-To-First-Token (TTFT) during cold-start by 23--55% over state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serverless Large Language Models (LLMs) have emerged as a cost-effective solution for deploying AI services by enabling a 'pay-as-you-go' pricing model through GPU resource sharing. However, cold-start latency, especially the model loading phase, has become a critical performance bottleneck, as it scales linearly with model size and severely limits the practical deployment of large-scale LLM services. This paper presents Tangram, a novel system that accelerates Serverless LLM loading through efficient GPU memory reuse. By leveraging the unused GPU memory to retain model parameters, Tangram significantly reduces model transfer time and cold-start latency. Its design includes three key components: unified GPU memory pool for tensor-level parameter sharing across models, on-demand KV cache allocation for dynamic memory management, and GPU-affinity-aware scheduling for maximizing resource utilization. These techniques collectively address the critical challenges of inefficient memory usage and the cold-start problem in Serverless LLM platforms. We have implemented a fully functional prototype, and experiments show that Tangram achieves up to 6.2 times faster loading and reduces Time-To-First-Token (TTFT) during cold-start by 23--55% over state-of-the-art methods."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-01T07:10:34Z",
                "published_parsed": [
                    2025,
                    12,
                    1,
                    7,
                    10,
                    34,
                    0,
                    335,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Wenbin Zhu"
                    },
                    {
                        "name": "Zhaoyan Shen"
                    },
                    {
                        "name": "Zili Shao"
                    },
                    {
                        "name": "Hongjun Dai"
                    },
                    {
                        "name": "Feng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Feng Chen"
                },
                "arxiv_affiliation": "Indiana University Bloomington",
                "author": "Feng Chen"
            },
            {
                "id": "http://arxiv.org/abs/2511.14712v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.14712v2",
                "title": "FreeSwim: Revisiting Sliding-Window Attention Mechanisms for Training-Free Ultra-High-Resolution Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FreeSwim: Revisiting Sliding-Window Attention Mechanisms for Training-Free Ultra-High-Resolution Video Generation"
                },
                "updated": "2025-12-01T06:11:56Z",
                "updated_parsed": [
                    2025,
                    12,
                    1,
                    6,
                    11,
                    56,
                    0,
                    335,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.14712v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.14712v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The quadratic time and memory complexity of the attention mechanism in modern Transformer based video generators makes end-to-end training for ultra high resolution videos prohibitively expensive. Motivated by this limitation, we introduce a training-free approach that leverages video Diffusion Transformers pretrained at their native scale to synthesize higher resolution videos without any additional training or adaptation. At the core of our method lies an inward sliding window attention mechanism, which originates from a key observation: maintaining each query token's training scale receptive field is crucial for preserving visual fidelity and detail. However, naive local window attention, unfortunately, often leads to repetitive content and exhibits a lack of global coherence in the generated results. To overcome this challenge, we devise a dual-path pipeline that backs up window attention with a novel cross-attention override strategy, enabling the semantic content produced by local attention to be guided by another branch with a full receptive field and, therefore, ensuring holistic consistency. Furthermore, to improve efficiency, we incorporate a cross-attention caching strategy for this branch to avoid the frequent computation of full 3D attention. Extensive experiments demonstrate that our method delivers ultra-high-resolution videos with fine-grained visual details and high efficiency in a training-free paradigm. Meanwhile, it achieves superior performance on VBench, even compared to training-based alternatives, with competitive or improved efficiency. Codes are available at: https://github.com/WillWu111/FreeSwim",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The quadratic time and memory complexity of the attention mechanism in modern Transformer based video generators makes end-to-end training for ultra high resolution videos prohibitively expensive. Motivated by this limitation, we introduce a training-free approach that leverages video Diffusion Transformers pretrained at their native scale to synthesize higher resolution videos without any additional training or adaptation. At the core of our method lies an inward sliding window attention mechanism, which originates from a key observation: maintaining each query token's training scale receptive field is crucial for preserving visual fidelity and detail. However, naive local window attention, unfortunately, often leads to repetitive content and exhibits a lack of global coherence in the generated results. To overcome this challenge, we devise a dual-path pipeline that backs up window attention with a novel cross-attention override strategy, enabling the semantic content produced by local attention to be guided by another branch with a full receptive field and, therefore, ensuring holistic consistency. Furthermore, to improve efficiency, we incorporate a cross-attention caching strategy for this branch to avoid the frequent computation of full 3D attention. Extensive experiments demonstrate that our method delivers ultra-high-resolution videos with fine-grained visual details and high efficiency in a training-free paradigm. Meanwhile, it achieves superior performance on VBench, even compared to training-based alternatives, with competitive or improved efficiency. Codes are available at: https://github.com/WillWu111/FreeSwim"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-18T17:56:04Z",
                "published_parsed": [
                    2025,
                    11,
                    18,
                    17,
                    56,
                    4,
                    1,
                    322,
                    0
                ],
                "arxiv_comment": "23 pages, 14 figures",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Yunfeng Wu"
                    },
                    {
                        "name": "Jiayi Song"
                    },
                    {
                        "name": "Zhenxiong Tan"
                    },
                    {
                        "name": "Zihao He"
                    },
                    {
                        "name": "Songhua Liu"
                    }
                ],
                "author_detail": {
                    "name": "Songhua Liu"
                },
                "author": "Songhua Liu"
            },
            {
                "id": "http://arxiv.org/abs/2512.01278v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.01278v1",
                "title": "Accelerating Large-Scale Reasoning Model Inference with Sparse Self-Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Large-Scale Reasoning Model Inference with Sparse Self-Speculative Decoding"
                },
                "updated": "2025-12-01T04:50:55Z",
                "updated_parsed": [
                    2025,
                    12,
                    1,
                    4,
                    50,
                    55,
                    0,
                    335,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.01278v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.01278v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Reasoning language models have demonstrated remarkable capabilities on challenging tasks by generating elaborate chain-of-thought (CoT) solutions. However, such lengthy generation shifts the inference bottleneck from compute-bound to memory-bound. To generate each token, the model applies full attention to all previously generated tokens, requiring memory access to an increasingly large KV-Cache. Consequently, longer generations demand more memory access for every step, leading to substantial pressure on memory bandwidth.\n  To address this, we introduce SparseSpec, a speculative decoding framework that reuses the same model as the draft and target models (i.e., self-speculation). SparseSpec features a novel sparse attention mechanism, PillarAttn, as the draft model, which accurately selects critical tokens via elegantly reusing information from the verification stage. Furthermore, SparseSpec co-designs self-speculation with three system innovations: (1) a unified scheduler to batch token drafting and verification, (2) delayed verification for CPU/GPU overlap, and (3) dynamic KV-Cache management to maximize memory utilization. Across various models and datasets, SparseSpec outperforms state-of-the-art solutions, with an up to 2.13x throughput speedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning language models have demonstrated remarkable capabilities on challenging tasks by generating elaborate chain-of-thought (CoT) solutions. However, such lengthy generation shifts the inference bottleneck from compute-bound to memory-bound. To generate each token, the model applies full attention to all previously generated tokens, requiring memory access to an increasingly large KV-Cache. Consequently, longer generations demand more memory access for every step, leading to substantial pressure on memory bandwidth.\n  To address this, we introduce SparseSpec, a speculative decoding framework that reuses the same model as the draft and target models (i.e., self-speculation). SparseSpec features a novel sparse attention mechanism, PillarAttn, as the draft model, which accurately selects critical tokens via elegantly reusing information from the verification stage. Furthermore, SparseSpec co-designs self-speculation with three system innovations: (1) a unified scheduler to batch token drafting and verification, (2) delayed verification for CPU/GPU overlap, and (3) dynamic KV-Cache management to maximize memory utilization. Across various models and datasets, SparseSpec outperforms state-of-the-art solutions, with an up to 2.13x throughput speedup."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-01T04:50:55Z",
                "published_parsed": [
                    2025,
                    12,
                    1,
                    4,
                    50,
                    55,
                    0,
                    335,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Yilong Zhao"
                    },
                    {
                        "name": "Jiaming Tang"
                    },
                    {
                        "name": "Kan Zhu"
                    },
                    {
                        "name": "Zihao Ye"
                    },
                    {
                        "name": "Chi-Chih Chang"
                    },
                    {
                        "name": "Chaofan Lin"
                    },
                    {
                        "name": "Jongseok Park"
                    },
                    {
                        "name": "Guangxuan Xiao"
                    },
                    {
                        "name": "Mohamed S. Abdelfattah"
                    },
                    {
                        "name": "Mingyu Gao"
                    },
                    {
                        "name": "Baris Kasikci"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Ion Stoica"
                    }
                ],
                "author_detail": {
                    "name": "Ion Stoica"
                },
                "author": "Ion Stoica"
            },
            {
                "id": "http://arxiv.org/abs/2304.10805v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2304.10805v3",
                "title": "EPLKG: Efficient Prompt Learning with Knowledge Graph",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EPLKG: Efficient Prompt Learning with Knowledge Graph"
                },
                "updated": "2025-11-30T14:24:30Z",
                "updated_parsed": [
                    2025,
                    11,
                    30,
                    14,
                    24,
                    30,
                    6,
                    334,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2304.10805v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2304.10805v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large-scale pre-trained models such as CLIP excel in transferability and robust generalization across diverse datasets. However, adapting these models to new datasets or domains is computationally costly, especially in low-resource or few-shot settings, and existing prompt-learning methods often lack interpretability. We introduce Efficient Prompt Learning with Knowledge Graph (EPLKG), which uses a knowledge graph to curate diverse, interpretable prompts and, where KG coverage is limited, augments this bank with LLM-generated human-readable visual descriptions. EPLKG operates entirely on cached CLIP image and text embeddings and employs a lightweight Gumbel-Softmax module to select a single prompt per image-class pair, enabling low-memory, fast training. Across 11 benchmarks, EPLKG reduces per-image training time by up to 45 percent and peak GPU memory by around 30 to 40 percent compared to strong prompt-learning baselines, while keeping the average base-new harmonic-mean accuracy within 2 percentage points, thereby improving the efficiency of model adaptation without sacrificing competitive performance or interpretability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale pre-trained models such as CLIP excel in transferability and robust generalization across diverse datasets. However, adapting these models to new datasets or domains is computationally costly, especially in low-resource or few-shot settings, and existing prompt-learning methods often lack interpretability. We introduce Efficient Prompt Learning with Knowledge Graph (EPLKG), which uses a knowledge graph to curate diverse, interpretable prompts and, where KG coverage is limited, augments this bank with LLM-generated human-readable visual descriptions. EPLKG operates entirely on cached CLIP image and text embeddings and employs a lightweight Gumbel-Softmax module to select a single prompt per image-class pair, enabling low-memory, fast training. Across 11 benchmarks, EPLKG reduces per-image training time by up to 45 percent and peak GPU memory by around 30 to 40 percent compared to strong prompt-learning baselines, while keeping the average base-new harmonic-mean accuracy within 2 percentage points, thereby improving the efficiency of model adaptation without sacrificing competitive performance or interpretability."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2023-04-21T08:22:58Z",
                "published_parsed": [
                    2023,
                    4,
                    21,
                    8,
                    22,
                    58,
                    4,
                    111,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "YongTaek Lim"
                    },
                    {
                        "name": "Suho Kang"
                    },
                    {
                        "name": "Yewon Kim"
                    },
                    {
                        "name": "Dokyung Yoon"
                    },
                    {
                        "name": "KyungWoo Song"
                    }
                ],
                "author_detail": {
                    "name": "KyungWoo Song"
                },
                "author": "KyungWoo Song"
            },
            {
                "id": "http://arxiv.org/abs/2512.00903v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.00903v1",
                "title": "SwiftVLA: Unlocking Spatiotemporal Dynamics for Lightweight VLA Models at Minimal Overhead",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SwiftVLA: Unlocking Spatiotemporal Dynamics for Lightweight VLA Models at Minimal Overhead"
                },
                "updated": "2025-11-30T14:10:28Z",
                "updated_parsed": [
                    2025,
                    11,
                    30,
                    14,
                    10,
                    28,
                    6,
                    334,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.00903v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.00903v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Vision-Language-Action (VLA) models built on pretrained Vision-Language Models (VLMs) show strong potential but are limited in practicality due to their large parameter counts. To mitigate this issue, using a lightweight VLM has been explored, but it compromises spatiotemporal reasoning. Although some methods suggest that incorporating additional 3D inputs can help, they usually rely on large VLMs to fuse 3D and 2D inputs and still lack temporal understanding. Therefore, we propose SwiftVLA, an architecture that enhances a compact model with 4D understanding while preserving design efficiency. Specifically, our approach features a pretrained 4D visual geometry transformer with a temporal cache that extracts 4D features from 2D images. Then, to enhance the VLM's ability to exploit both 2D images and 4D features, we introduce Fusion Tokens, a set of learnable tokens trained with a future prediction objective to generate unified representations for action generation. Finally, we introduce a mask-and-reconstruct strategy that masks 4D inputs to the VLM and trains the VLA to reconstruct them, enabling the VLM to learn effective 4D representations and allowing the 4D branch to be dropped at inference with minimal performance loss. Experiments in real and simulated environments show that SwiftVLA outperforms lightweight baselines and rivals VLAs up to 7 times larger, achieving comparable performance on edge devices while being 18 times faster and reducing memory footprint by 12 times.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language-Action (VLA) models built on pretrained Vision-Language Models (VLMs) show strong potential but are limited in practicality due to their large parameter counts. To mitigate this issue, using a lightweight VLM has been explored, but it compromises spatiotemporal reasoning. Although some methods suggest that incorporating additional 3D inputs can help, they usually rely on large VLMs to fuse 3D and 2D inputs and still lack temporal understanding. Therefore, we propose SwiftVLA, an architecture that enhances a compact model with 4D understanding while preserving design efficiency. Specifically, our approach features a pretrained 4D visual geometry transformer with a temporal cache that extracts 4D features from 2D images. Then, to enhance the VLM's ability to exploit both 2D images and 4D features, we introduce Fusion Tokens, a set of learnable tokens trained with a future prediction objective to generate unified representations for action generation. Finally, we introduce a mask-and-reconstruct strategy that masks 4D inputs to the VLM and trains the VLA to reconstruct them, enabling the VLM to learn effective 4D representations and allowing the 4D branch to be dropped at inference with minimal performance loss. Experiments in real and simulated environments show that SwiftVLA outperforms lightweight baselines and rivals VLAs up to 7 times larger, achieving comparable performance on edge devices while being 18 times faster and reducing memory footprint by 12 times."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-30T14:10:28Z",
                "published_parsed": [
                    2025,
                    11,
                    30,
                    14,
                    10,
                    28,
                    6,
                    334,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Chaojun Ni"
                    },
                    {
                        "name": "Cheng Chen"
                    },
                    {
                        "name": "Xiaofeng Wang"
                    },
                    {
                        "name": "Zheng Zhu"
                    },
                    {
                        "name": "Wenzhao Zheng"
                    },
                    {
                        "name": "Boyuan Wang"
                    },
                    {
                        "name": "Tianrun Chen"
                    },
                    {
                        "name": "Guosheng Zhao"
                    },
                    {
                        "name": "Haoyun Li"
                    },
                    {
                        "name": "Zhehao Dong"
                    },
                    {
                        "name": "Qiang Zhang"
                    },
                    {
                        "name": "Yun Ye"
                    },
                    {
                        "name": "Yang Wang"
                    },
                    {
                        "name": "Guan Huang"
                    },
                    {
                        "name": "Wenjun Mei"
                    }
                ],
                "author_detail": {
                    "name": "Wenjun Mei"
                },
                "author": "Wenjun Mei"
            },
            {
                "id": "http://arxiv.org/abs/2512.00891v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.00891v1",
                "title": "Accelerating Streaming Video Large Language Models via Hierarchical Token Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Streaming Video Large Language Models via Hierarchical Token Compression"
                },
                "updated": "2025-11-30T13:44:28Z",
                "updated_parsed": [
                    2025,
                    11,
                    30,
                    13,
                    44,
                    28,
                    6,
                    334,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.00891v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.00891v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Streaming Video Large Language Models (VideoLLMs) have demonstrated impressive performance across various video understanding tasks, but they face significant challenges in real-time deployment due to the high computational cost of processing dense visual tokens from continuous video streams. In streaming video scenarios, the primary bottleneck lies in the Vision Transformer (ViT) encoding stage, where redundant processing of temporally similar frames leads to inefficiency. Additionally, inflated token sequences during LLM pre-filling further exacerbate latency and memory overhead. To address these challenges, we propose \\textbf{S}treaming \\textbf{T}oken \\textbf{C}ompression (\\textbf{STC}), a plug-and-play hierarchical framework that seamlessly integrates into existing streaming VideoLLMs, optimizing both ViT encoding and LLM pre-filling stages to accelerate processing. STC introduces two token-level accelerators: \\textbf{STC-Cacher}, which reduces ViT encoding overhead by caching and reusing features from temporally similar frames, and \\textbf{STC-Pruner}, which compresses the visual token sequence before it enters the LLM, preserving only the most salient tokens based on both spatial and temporal relevance. Extensive experiments on four baseline streaming VideoLLMs across five benchmarks demonstrate that STC outperforms other compression methods. Notably, STC retains up to \\textbf{99\\%} of accuracy on the ReKV framework while reducing ViT encoding latency and LLM pre-filling latency by \\textbf{24.5\\%} and \\textbf{45.3\\%}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Streaming Video Large Language Models (VideoLLMs) have demonstrated impressive performance across various video understanding tasks, but they face significant challenges in real-time deployment due to the high computational cost of processing dense visual tokens from continuous video streams. In streaming video scenarios, the primary bottleneck lies in the Vision Transformer (ViT) encoding stage, where redundant processing of temporally similar frames leads to inefficiency. Additionally, inflated token sequences during LLM pre-filling further exacerbate latency and memory overhead. To address these challenges, we propose \\textbf{S}treaming \\textbf{T}oken \\textbf{C}ompression (\\textbf{STC}), a plug-and-play hierarchical framework that seamlessly integrates into existing streaming VideoLLMs, optimizing both ViT encoding and LLM pre-filling stages to accelerate processing. STC introduces two token-level accelerators: \\textbf{STC-Cacher}, which reduces ViT encoding overhead by caching and reusing features from temporally similar frames, and \\textbf{STC-Pruner}, which compresses the visual token sequence before it enters the LLM, preserving only the most salient tokens based on both spatial and temporal relevance. Extensive experiments on four baseline streaming VideoLLMs across five benchmarks demonstrate that STC outperforms other compression methods. Notably, STC retains up to \\textbf{99\\%} of accuracy on the ReKV framework while reducing ViT encoding latency and LLM pre-filling latency by \\textbf{24.5\\%} and \\textbf{45.3\\%}."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-30T13:44:28Z",
                "published_parsed": [
                    2025,
                    11,
                    30,
                    13,
                    44,
                    28,
                    6,
                    334,
                    0
                ],
                "arxiv_comment": "Code is avaliable at \\url{https://github.com/lern-to-write/STC}",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Yiyu Wang"
                    },
                    {
                        "name": "Xuyang Liu"
                    },
                    {
                        "name": "Xiyan Gui"
                    },
                    {
                        "name": "Xinying Lin"
                    },
                    {
                        "name": "Boxue Yang"
                    },
                    {
                        "name": "Chenfei Liao"
                    },
                    {
                        "name": "Tailai Chen"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2512.00722v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.00722v1",
                "title": "SpeContext: Enabling Efficient Long-context Reasoning with Speculative Context Sparsity in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpeContext: Enabling Efficient Long-context Reasoning with Speculative Context Sparsity in LLMs"
                },
                "updated": "2025-11-30T04:32:43Z",
                "updated_parsed": [
                    2025,
                    11,
                    30,
                    4,
                    32,
                    43,
                    6,
                    334,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.00722v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.00722v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "In this paper, we point out that the objective of the retrieval algorithms is to align with the LLM, which is similar to the objective of knowledge distillation in LLMs. We analyze the similarity in information focus between the distilled language model(DLM) and the original LLM from the perspective of information theory, and thus propose a novel paradigm that leverages a DLM as the retrieval algorithm. Based on the insight, we present SpeContext, an algorithm and system co-design for long-context reasoning. (1) At the algorithm level, SpeContext proposes lightweight retrieval head based on the head-level attention weights of DLM, achieving > 90% parameters reduction by pruning the redundancy. (2) At the system level, SpeContext designs an asynchronous prefetch dataflow via the elastic loading strategy, effectively overlapping KV cache retrieval with the LLM computation. (3) At the compilation level, SpeContext constructs the theoretical memory model and implements an adaptive memory management system to achieve acceleration by maximizing GPU memory utilization. We deploy and evaluate SpeContext in two resourceconstrained environments, cloud and edge. Extensive experiments show that, compared with the Huggingface framework, SpeContext achieves up to 24.89x throughput improvement in cloud and 10.06x speedup in edge with negligible accuracy loss, pushing the Pareto frontier of accuracy and throughput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we point out that the objective of the retrieval algorithms is to align with the LLM, which is similar to the objective of knowledge distillation in LLMs. We analyze the similarity in information focus between the distilled language model(DLM) and the original LLM from the perspective of information theory, and thus propose a novel paradigm that leverages a DLM as the retrieval algorithm. Based on the insight, we present SpeContext, an algorithm and system co-design for long-context reasoning. (1) At the algorithm level, SpeContext proposes lightweight retrieval head based on the head-level attention weights of DLM, achieving > 90% parameters reduction by pruning the redundancy. (2) At the system level, SpeContext designs an asynchronous prefetch dataflow via the elastic loading strategy, effectively overlapping KV cache retrieval with the LLM computation. (3) At the compilation level, SpeContext constructs the theoretical memory model and implements an adaptive memory management system to achieve acceleration by maximizing GPU memory utilization. We deploy and evaluate SpeContext in two resourceconstrained environments, cloud and edge. Extensive experiments show that, compared with the Huggingface framework, SpeContext achieves up to 24.89x throughput improvement in cloud and 10.06x speedup in edge with negligible accuracy loss, pushing the Pareto frontier of accuracy and throughput."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-30T04:32:43Z",
                "published_parsed": [
                    2025,
                    11,
                    30,
                    4,
                    32,
                    43,
                    6,
                    334,
                    0
                ],
                "arxiv_comment": "Accepted by ASPLOS 2026",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Jiaming Xu"
                    },
                    {
                        "name": "Jiayi Pan"
                    },
                    {
                        "name": "Hanzhen Wang"
                    },
                    {
                        "name": "Yongkang Zhou"
                    },
                    {
                        "name": "Jiancai Ye"
                    },
                    {
                        "name": "Yu Wang"
                    },
                    {
                        "name": "Guohao Dai"
                    }
                ],
                "author_detail": {
                    "name": "Guohao Dai"
                },
                "author": "Guohao Dai"
            },
            {
                "id": "http://arxiv.org/abs/2512.00719v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.00719v1",
                "title": "SIMPLE: Disaggregating Sampling from GPU Inference into a Decision Plane for Faster Distributed LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SIMPLE: Disaggregating Sampling from GPU Inference into a Decision Plane for Faster Distributed LLM Serving"
                },
                "updated": "2025-11-30T04:15:34Z",
                "updated_parsed": [
                    2025,
                    11,
                    30,
                    4,
                    15,
                    34,
                    6,
                    334,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.00719v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.00719v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "As large language models (LLMs) scale out with tensor parallelism (TP) and pipeline parallelism (PP) and production stacks have aggressively optimized the data plane (attention/GEMM and KV cache), sampling, the decision plane that turns logits into tokens, becomes a new bottleneck. This creates a structural holdout: sampling neither expands with TP nor balances across PP stages, so its share of iteration time grows as GPUs get faster and it caps pipeline frequency at the last stage. We present SIMPLE, a stage-agnostic, sequence-parallel, overlappable decision plane that disaggregates sampling into a CPU-side service and shrinks its runtime footprint back to a minor, hidden role. SIMPLE combines: (1) sequence-parallel sampling, which shards work along the batch dimension and removes vocabulary-axis collectives; (2) a CPU-based algorithm with column-wise penalties and truncation-first filtering to realize single-pass, linear-time kernels; and (3) speculative hot-vocab sampling (SHVS), which samples on a small hot set with rejection-correctness and uses a simple sizing model to choose the hot-vocab size that maximizes throughput. In evaluation, SIMPLE improves end-to-end throughput by up to 96% and reduces P95 latency by 20-65%. Crucially, SIMPLE requires no user-side code changes and composes with existing data-plane optimizations, unlocking scaling benefits that compound with future GPU generations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) scale out with tensor parallelism (TP) and pipeline parallelism (PP) and production stacks have aggressively optimized the data plane (attention/GEMM and KV cache), sampling, the decision plane that turns logits into tokens, becomes a new bottleneck. This creates a structural holdout: sampling neither expands with TP nor balances across PP stages, so its share of iteration time grows as GPUs get faster and it caps pipeline frequency at the last stage. We present SIMPLE, a stage-agnostic, sequence-parallel, overlappable decision plane that disaggregates sampling into a CPU-side service and shrinks its runtime footprint back to a minor, hidden role. SIMPLE combines: (1) sequence-parallel sampling, which shards work along the batch dimension and removes vocabulary-axis collectives; (2) a CPU-based algorithm with column-wise penalties and truncation-first filtering to realize single-pass, linear-time kernels; and (3) speculative hot-vocab sampling (SHVS), which samples on a small hot set with rejection-correctness and uses a simple sizing model to choose the hot-vocab size that maximizes throughput. In evaluation, SIMPLE improves end-to-end throughput by up to 96% and reduces P95 latency by 20-65%. Crucially, SIMPLE requires no user-side code changes and composes with existing data-plane optimizations, unlocking scaling benefits that compound with future GPU generations."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-30T04:15:34Z",
                "published_parsed": [
                    2025,
                    11,
                    30,
                    4,
                    15,
                    34,
                    6,
                    334,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Bohan Zhao"
                    },
                    {
                        "name": "Zane Cao"
                    },
                    {
                        "name": "Yongchao He"
                    }
                ],
                "author_detail": {
                    "name": "Yongchao He"
                },
                "author": "Yongchao He"
            },
            {
                "id": "http://arxiv.org/abs/2512.00635v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.00635v1",
                "title": "Extended Abstract: Synthesizable Low-overhead Circuit-level Countermeasures and Pro-Active Detection Techniques for Power and EM SCA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extended Abstract: Synthesizable Low-overhead Circuit-level Countermeasures and Pro-Active Detection Techniques for Power and EM SCA"
                },
                "updated": "2025-11-29T21:12:22Z",
                "updated_parsed": [
                    2025,
                    11,
                    29,
                    21,
                    12,
                    22,
                    5,
                    333,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.00635v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.00635v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The gamut of todays internet-connected embedded devices has led to increased concerns regarding the security and confidentiality of data. Most internet-connected embedded devices employ mathematically secure cryptographic algorithms to address security vulnerabilities. Despite such mathematical guarantees, as these algorithms are often implemented in silicon, they leak critical information in terms of power consumption, electromagnetic (EM) radiation, timing, cache hits and misses, photonic emission and so on, leading to side-channel analysis (SCA) attacks. This thesis focuses on low overhead generic circuit-level yet synthesizable countermeasures against power and EM SCA. Existing countermeasures (including proposed) still have relatively high overhead which bars them from being used in energy-constraint IoT devices. We propose a zero-overhead integrated inductive sensor which is able to detect i)EM SCA ii) Clock glitch-based Fault Injection Attack (FIA), and iii) Voltage-glitch based Fault Injection Attack by using a simple ML algorithm. Advent of quantum computer research will open new possibilities for theoretical attacks against existing cryptographic protocols. National Institute of Standard & Technology (NIST) has standardized post-quantum cryptographic algorithms to secure crypto-systems against quantum adversary. I contribute to the standardization procedure by introducing the first silicon-verified Saber (a NIST finalist modulo Learning with Rounding scheme) which consumes lowest energy and area till date amongst all the candidates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The gamut of todays internet-connected embedded devices has led to increased concerns regarding the security and confidentiality of data. Most internet-connected embedded devices employ mathematically secure cryptographic algorithms to address security vulnerabilities. Despite such mathematical guarantees, as these algorithms are often implemented in silicon, they leak critical information in terms of power consumption, electromagnetic (EM) radiation, timing, cache hits and misses, photonic emission and so on, leading to side-channel analysis (SCA) attacks. This thesis focuses on low overhead generic circuit-level yet synthesizable countermeasures against power and EM SCA. Existing countermeasures (including proposed) still have relatively high overhead which bars them from being used in energy-constraint IoT devices. We propose a zero-overhead integrated inductive sensor which is able to detect i)EM SCA ii) Clock glitch-based Fault Injection Attack (FIA), and iii) Voltage-glitch based Fault Injection Attack by using a simple ML algorithm. Advent of quantum computer research will open new possibilities for theoretical attacks against existing cryptographic protocols. National Institute of Standard & Technology (NIST) has standardized post-quantum cryptographic algorithms to secure crypto-systems against quantum adversary. I contribute to the standardization procedure by introducing the first silicon-verified Saber (a NIST finalist modulo Learning with Rounding scheme) which consumes lowest energy and area till date amongst all the candidates."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-29T21:12:22Z",
                "published_parsed": [
                    2025,
                    11,
                    29,
                    21,
                    12,
                    22,
                    5,
                    333,
                    0
                ],
                "arxiv_comment": "This extended abstract is archived for educational purposes as an example for different PhD forum competitions. Total page is 3",
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Archisman Ghosh"
                    }
                ],
                "author_detail": {
                    "name": "Archisman Ghosh"
                },
                "author": "Archisman Ghosh"
            },
            {
                "id": "http://arxiv.org/abs/2512.00504v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.00504v1",
                "title": "G-KV: Decoding-Time KV Cache Eviction with Global Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "G-KV: Decoding-Time KV Cache Eviction with Global Attention"
                },
                "updated": "2025-11-29T14:21:33Z",
                "updated_parsed": [
                    2025,
                    11,
                    29,
                    14,
                    21,
                    33,
                    5,
                    333,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.00504v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.00504v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recent reasoning large language models (LLMs) excel in complex tasks but encounter significant computational and memory challenges due to long sequence lengths. KV cache compression has emerged as an effective approach to greatly enhance the efficiency of reasoning. However, existing methods often focus on prompt compression or token eviction with local attention score, overlooking the long-term importance of tokens. We propose G-KV, a KV cache eviction method that employs a global scoring mechanism, combining local and historical attention scores to more accurately assess token importance. Additionally, we introduce post-training techniques, including reinforcement learning and distillation, to optimize models for compressed KV cache settings. The code of this paper is available on: https://github.com/microsoft/G-KV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent reasoning large language models (LLMs) excel in complex tasks but encounter significant computational and memory challenges due to long sequence lengths. KV cache compression has emerged as an effective approach to greatly enhance the efficiency of reasoning. However, existing methods often focus on prompt compression or token eviction with local attention score, overlooking the long-term importance of tokens. We propose G-KV, a KV cache eviction method that employs a global scoring mechanism, combining local and historical attention scores to more accurately assess token importance. Additionally, we introduce post-training techniques, including reinforcement learning and distillation, to optimize models for compressed KV cache settings. The code of this paper is available on: https://github.com/microsoft/G-KV."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-29T14:21:33Z",
                "published_parsed": [
                    2025,
                    11,
                    29,
                    14,
                    21,
                    33,
                    5,
                    333,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Mengqi Liao"
                    },
                    {
                        "name": "Lu Wang"
                    },
                    {
                        "name": "Chaoyun Zhang"
                    },
                    {
                        "name": "Zekai Shen"
                    },
                    {
                        "name": "Xiaowei Mao"
                    },
                    {
                        "name": "Si Qin"
                    },
                    {
                        "name": "Qingwei Lin"
                    },
                    {
                        "name": "Saravan Rajmohan"
                    },
                    {
                        "name": "Dongmei Zhang"
                    },
                    {
                        "name": "Huaiyu Wan"
                    }
                ],
                "author_detail": {
                    "name": "Huaiyu Wan"
                },
                "author": "Huaiyu Wan"
            },
            {
                "id": "http://arxiv.org/abs/2512.00300v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.00300v1",
                "title": "TGSFormer: Scalable Temporal Gaussian Splatting for Embodied Semantic Scene Completion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TGSFormer: Scalable Temporal Gaussian Splatting for Embodied Semantic Scene Completion"
                },
                "updated": "2025-11-29T03:47:14Z",
                "updated_parsed": [
                    2025,
                    11,
                    29,
                    3,
                    47,
                    14,
                    5,
                    333,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.00300v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.00300v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Embodied 3D Semantic Scene Completion (SSC) infers dense geometry and semantics from continuous egocentric observations. Most existing Gaussian-based methods rely on random initialization of many primitives within predefined spatial bounds, resulting in redundancy and poor scalability to unbounded scenes. Recent depth-guided approach alleviates this issue but remains local, suffering from latency and memory overhead as scale increases. To overcome these challenges, we propose TGSFormer, a scalable Temporal Gaussian Splatting framework for embodied SSC. It maintains a persistent Gaussian memory for temporal prediction, without relying on image coherence or frame caches. For temporal fusion, a Dual Temporal Encoder jointly processes current and historical Gaussian features through confidence-aware cross-attention. Subsequently, a Confidence-aware Voxel Fusion module merges overlapping primitives into voxel-aligned representations, regulating density and maintaining compactness. Extensive experiments demonstrate that TGSFormer achieves state-of-the-art results on both local and embodied SSC benchmarks, offering superior accuracy and scalability with significantly fewer primitives while maintaining consistent long-term scene integrity. The code will be released upon acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embodied 3D Semantic Scene Completion (SSC) infers dense geometry and semantics from continuous egocentric observations. Most existing Gaussian-based methods rely on random initialization of many primitives within predefined spatial bounds, resulting in redundancy and poor scalability to unbounded scenes. Recent depth-guided approach alleviates this issue but remains local, suffering from latency and memory overhead as scale increases. To overcome these challenges, we propose TGSFormer, a scalable Temporal Gaussian Splatting framework for embodied SSC. It maintains a persistent Gaussian memory for temporal prediction, without relying on image coherence or frame caches. For temporal fusion, a Dual Temporal Encoder jointly processes current and historical Gaussian features through confidence-aware cross-attention. Subsequently, a Confidence-aware Voxel Fusion module merges overlapping primitives into voxel-aligned representations, regulating density and maintaining compactness. Extensive experiments demonstrate that TGSFormer achieves state-of-the-art results on both local and embodied SSC benchmarks, offering superior accuracy and scalability with significantly fewer primitives while maintaining consistent long-term scene integrity. The code will be released upon acceptance."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-29T03:47:14Z",
                "published_parsed": [
                    2025,
                    11,
                    29,
                    3,
                    47,
                    14,
                    5,
                    333,
                    0
                ],
                "arxiv_comment": "14 pages, 10 figures",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Rui Qian"
                    },
                    {
                        "name": "Haozhi Cao"
                    },
                    {
                        "name": "Tianchen Deng"
                    },
                    {
                        "name": "Tianxin Hu"
                    },
                    {
                        "name": "Weixiang Guo"
                    },
                    {
                        "name": "Shenghai Yuan"
                    },
                    {
                        "name": "Lihua Xie"
                    }
                ],
                "author_detail": {
                    "name": "Lihua Xie"
                },
                "author": "Lihua Xie"
            },
            {
                "id": "http://arxiv.org/abs/2512.05134v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05134v1",
                "title": "InvarDiff: Cross-Scale Invariance Caching for Accelerated Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InvarDiff: Cross-Scale Invariance Caching for Accelerated Diffusion Models"
                },
                "updated": "2025-11-29T02:34:23Z",
                "updated_parsed": [
                    2025,
                    11,
                    29,
                    2,
                    34,
                    23,
                    5,
                    333,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05134v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05134v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Diffusion models deliver high-fidelity synthesis but remain slow due to iterative sampling. We empirically observe there exists feature invariance in deterministic sampling, and present InvarDiff, a training-free acceleration method that exploits the relative temporal invariance across timestep-scale and layer-scale. From a few deterministic runs, we compute a per-timestep, per-layer, per-module binary cache plan matrix and use a re-sampling correction to avoid drift when consecutive caches occur. Using quantile-based change metrics, this matrix specifies which module at which step is reused rather than recomputed. The same invariance criterion is applied at the step scale to enable cross-timestep caching, deciding whether an entire step can reuse cached results. During inference, InvarDiff performs step-first and layer-wise caching guided by this matrix. When applied to DiT and FLUX, our approach reduces redundant compute while preserving fidelity. Experiments show that InvarDiff achieves $2$-$3\\times$ end-to-end speed-ups with minimal impact on standard quality metrics. Qualitatively, we observe almost no degradation in visual quality compared with full computations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models deliver high-fidelity synthesis but remain slow due to iterative sampling. We empirically observe there exists feature invariance in deterministic sampling, and present InvarDiff, a training-free acceleration method that exploits the relative temporal invariance across timestep-scale and layer-scale. From a few deterministic runs, we compute a per-timestep, per-layer, per-module binary cache plan matrix and use a re-sampling correction to avoid drift when consecutive caches occur. Using quantile-based change metrics, this matrix specifies which module at which step is reused rather than recomputed. The same invariance criterion is applied at the step scale to enable cross-timestep caching, deciding whether an entire step can reuse cached results. During inference, InvarDiff performs step-first and layer-wise caching guided by this matrix. When applied to DiT and FLUX, our approach reduces redundant compute while preserving fidelity. Experiments show that InvarDiff achieves $2$-$3\\times$ end-to-end speed-ups with minimal impact on standard quality metrics. Qualitatively, we observe almost no degradation in visual quality compared with full computations."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-29T02:34:23Z",
                "published_parsed": [
                    2025,
                    11,
                    29,
                    2,
                    34,
                    23,
                    5,
                    333,
                    0
                ],
                "arxiv_comment": "8 pages main, 8 pages appendix, 16 figures, 5 tables. Code: https://github.com/zihaowu25/InvarDiff",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Zihao Wu"
                    }
                ],
                "author_detail": {
                    "name": "Zihao Wu"
                },
                "author": "Zihao Wu"
            },
            {
                "id": "http://arxiv.org/abs/2408.10104v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2408.10104v2",
                "title": "Multi-Mode Lens for Momentum Microscopy and XPEEM: Theory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Mode Lens for Momentum Microscopy and XPEEM: Theory"
                },
                "updated": "2025-11-28T21:55:41Z",
                "updated_parsed": [
                    2025,
                    11,
                    28,
                    21,
                    55,
                    41,
                    4,
                    332,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2408.10104v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2408.10104v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The strong electric field between the sample and the extractor is the core of cathode lenses and a pivotal determinant of high resolution. Nevertheless, fields in the range of 3-8 kV/mm can be a source of complications. Local field enhancement at sharp edges or microscopic protrusions of cleaved samples may result in field emission or flashovers. Moreover, slow background electrons are drawn into the microscope column, where they contribute to space charge effects. A novel front lens configuration, optimized through ray-tracing simulations, significantly reduces the field at the sample and allows even for zero field or retarding field, which serves to suppress space charge effects. One or several annular electrodes, situated in a concentric position relative to the extractor, serve to form an additional lens within the gap between the sample and the extractor. The refractory power of this lens, and consequently the field at the sample surface, can be modified by adjusting the potentials of the annular electrodes. The imaging properties and aberrations of this gap lens have been investigated with regard to momentum imaging and XPEEM. The study encompasses the energy range from the few-eV level for laser-ARPES to 6 keV, for hard X-ray ARPES. The additional converging lens situated in close proximity to the sample exhibits a reduced field curvature of the k-image in the backfocal plane. This allows for the acquisition of larger fields of view in both momentum and real-space imaging.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The strong electric field between the sample and the extractor is the core of cathode lenses and a pivotal determinant of high resolution. Nevertheless, fields in the range of 3-8 kV/mm can be a source of complications. Local field enhancement at sharp edges or microscopic protrusions of cleaved samples may result in field emission or flashovers. Moreover, slow background electrons are drawn into the microscope column, where they contribute to space charge effects. A novel front lens configuration, optimized through ray-tracing simulations, significantly reduces the field at the sample and allows even for zero field or retarding field, which serves to suppress space charge effects. One or several annular electrodes, situated in a concentric position relative to the extractor, serve to form an additional lens within the gap between the sample and the extractor. The refractory power of this lens, and consequently the field at the sample surface, can be modified by adjusting the potentials of the annular electrodes. The imaging properties and aberrations of this gap lens have been investigated with regard to momentum imaging and XPEEM. The study encompasses the energy range from the few-eV level for laser-ARPES to 6 keV, for hard X-ray ARPES. The additional converging lens situated in close proximity to the sample exhibits a reduced field curvature of the k-image in the backfocal plane. This allows for the acquisition of larger fields of view in both momentum and real-space imaging."
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-08-19T15:47:17Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    15,
                    47,
                    17,
                    0,
                    232,
                    0
                ],
                "arxiv_comment": "17 pages, 4 figures, 44 references",
                "arxiv_primary_category": {
                    "term": "physics.app-ph"
                },
                "authors": [
                    {
                        "name": "Olena Tkach"
                    },
                    {
                        "name": "Gerd Schoenhense"
                    }
                ],
                "author_detail": {
                    "name": "Gerd Schoenhense"
                },
                "author": "Gerd Schoenhense"
            },
            {
                "id": "http://arxiv.org/abs/2511.23070v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.23070v1",
                "title": "Buffer replay enhances the robustness of multimodal learning under missing-modality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Buffer replay enhances the robustness of multimodal learning under missing-modality"
                },
                "updated": "2025-11-28T10:55:31Z",
                "updated_parsed": [
                    2025,
                    11,
                    28,
                    10,
                    55,
                    31,
                    4,
                    332,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.23070v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.23070v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Missing modalities consistently lead to significant performance degradation in multimodal models. Existing approaches either synthesize missing modalities at high computational cost or apply prompt-based fine-tuning that relies only on adjacent-layer features and overlooks long-distance contextual information, which may offer additional tolerance to errors when one or more modalities are missing. To address this, we introduce REplay Prompting (REP): (1) construct modality-wise feature buffers via a residual bypass to cache early-layer representations and replay them in deeper layers, mitigating information loss as network depth increases; (2) employ a private-shared feature decoupling strategy, where private buffers preserve modality-specific signals and shared buffers encode cross-modal semantics; and (3) design a task-aware dynamic initialization mechanism to configure these buffers differently, improving stability and generalization under diverse missing-modality conditions. Experiments on vision-language, vision-language-audio, and temporal multimodal benchmarks demonstrate that REP consistently outperforms prior methods under both single- and multi-modality missing scenarios, while introducing only negligible parameter overhead. These results establish REP as a lightweight and effective paradigm for robust multimodal learning in challenging missing-modality environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Missing modalities consistently lead to significant performance degradation in multimodal models. Existing approaches either synthesize missing modalities at high computational cost or apply prompt-based fine-tuning that relies only on adjacent-layer features and overlooks long-distance contextual information, which may offer additional tolerance to errors when one or more modalities are missing. To address this, we introduce REplay Prompting (REP): (1) construct modality-wise feature buffers via a residual bypass to cache early-layer representations and replay them in deeper layers, mitigating information loss as network depth increases; (2) employ a private-shared feature decoupling strategy, where private buffers preserve modality-specific signals and shared buffers encode cross-modal semantics; and (3) design a task-aware dynamic initialization mechanism to configure these buffers differently, improving stability and generalization under diverse missing-modality conditions. Experiments on vision-language, vision-language-audio, and temporal multimodal benchmarks demonstrate that REP consistently outperforms prior methods under both single- and multi-modality missing scenarios, while introducing only negligible parameter overhead. These results establish REP as a lightweight and effective paradigm for robust multimodal learning in challenging missing-modality environments."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-28T10:55:31Z",
                "published_parsed": [
                    2025,
                    11,
                    28,
                    10,
                    55,
                    31,
                    4,
                    332,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Hongye Zhu"
                    },
                    {
                        "name": "Xuan Liu"
                    },
                    {
                        "name": "Yanwen Ba"
                    },
                    {
                        "name": "Jingye Xue"
                    },
                    {
                        "name": "Shigeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Shigeng Zhang"
                },
                "author": "Shigeng Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2511.23011v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.23011v1",
                "title": "Cohet: A CXL-Driven Coherent Heterogeneous Computing Framework with Hardware-Calibrated Full-System Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cohet: A CXL-Driven Coherent Heterogeneous Computing Framework with Hardware-Calibrated Full-System Simulation"
                },
                "updated": "2025-11-28T09:22:37Z",
                "updated_parsed": [
                    2025,
                    11,
                    28,
                    9,
                    22,
                    37,
                    4,
                    332,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.23011v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.23011v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Conventional heterogeneous computing systems built on PCIe interconnects suffer from inefficient fine-grained host-device interactions and complex programming models. In recent years, many proprietary and open cache-coherent interconnect standards have emerged, among which compute express link (CXL) prevails in the open-standard domain after acquiring several competing solutions. Although CXL-based coherent heterogeneous computing holds the potential to fundamentally transform the collaborative computing mode of CPUs and XPUs, research in this direction remains hampered by the scarcity of available CXL-supported platforms, immature software/hardware ecosystems, and unclear application prospects. This paper presents Cohet, the first CXL-driven coherent heterogeneous computing framework. Cohet decouples the compute and memory resources to form unbiased CPU and XPU pools which share a single unified and coherent memory pool. It exposes a standard malloc/mmap interface to both CPU and XPU compute threads, leaving the OS dealing with smart memory allocation and management of heterogeneous resources. To facilitate Cohet research, we also present a full-system cycle-level simulator named SimCXL, which is capable of modeling all CXL sub-protocols and device types. SimCXL has been rigorously calibrated against a real CXL testbed with various CXL memory and accelerators, showing an average simulation error of 3%. Our evaluation reveals that CXL.cache reduces latency by 68% and increases bandwidth by 14.4x compared to DMA transfers at cacheline granularity. Building upon these insights, we demonstrate the benefits of Cohet with two killer apps, which are remote atomic operation (RAO) and remote procedure call (RPC). Compared to PCIe-NIC design, CXL-NIC achieves a 5.5 to 40.2x speedup for RAO offloading and an average speedup of 1.86x for RPC (de)serialization offloading.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conventional heterogeneous computing systems built on PCIe interconnects suffer from inefficient fine-grained host-device interactions and complex programming models. In recent years, many proprietary and open cache-coherent interconnect standards have emerged, among which compute express link (CXL) prevails in the open-standard domain after acquiring several competing solutions. Although CXL-based coherent heterogeneous computing holds the potential to fundamentally transform the collaborative computing mode of CPUs and XPUs, research in this direction remains hampered by the scarcity of available CXL-supported platforms, immature software/hardware ecosystems, and unclear application prospects. This paper presents Cohet, the first CXL-driven coherent heterogeneous computing framework. Cohet decouples the compute and memory resources to form unbiased CPU and XPU pools which share a single unified and coherent memory pool. It exposes a standard malloc/mmap interface to both CPU and XPU compute threads, leaving the OS dealing with smart memory allocation and management of heterogeneous resources. To facilitate Cohet research, we also present a full-system cycle-level simulator named SimCXL, which is capable of modeling all CXL sub-protocols and device types. SimCXL has been rigorously calibrated against a real CXL testbed with various CXL memory and accelerators, showing an average simulation error of 3%. Our evaluation reveals that CXL.cache reduces latency by 68% and increases bandwidth by 14.4x compared to DMA transfers at cacheline granularity. Building upon these insights, we demonstrate the benefits of Cohet with two killer apps, which are remote atomic operation (RAO) and remote procedure call (RPC). Compared to PCIe-NIC design, CXL-NIC achieves a 5.5 to 40.2x speedup for RAO offloading and an average speedup of 1.86x for RPC (de)serialization offloading."
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-28T09:22:37Z",
                "published_parsed": [
                    2025,
                    11,
                    28,
                    9,
                    22,
                    37,
                    4,
                    332,
                    0
                ],
                "arxiv_comment": "Accepted by HPCA 2026",
                "arxiv_primary_category": {
                    "term": "cs.AR"
                },
                "authors": [
                    {
                        "name": "Yanjing Wang"
                    },
                    {
                        "name": "Lizhou Wu"
                    },
                    {
                        "name": "Sunfeng Gao"
                    },
                    {
                        "name": "Yibo Tang"
                    },
                    {
                        "name": "Junhui Luo"
                    },
                    {
                        "name": "Zicong Wang"
                    },
                    {
                        "name": "Yang Ou"
                    },
                    {
                        "name": "Dezun Dong"
                    },
                    {
                        "name": "Nong Xiao"
                    },
                    {
                        "name": "Mingche Lai"
                    }
                ],
                "author_detail": {
                    "name": "Mingche Lai"
                },
                "author": "Mingche Lai"
            },
            {
                "id": "http://arxiv.org/abs/2511.22973v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.22973v1",
                "title": "BlockVid: Block Diffusion for High-Quality and Consistent Minute-Long Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BlockVid: Block Diffusion for High-Quality and Consistent Minute-Long Video Generation"
                },
                "updated": "2025-11-28T08:25:59Z",
                "updated_parsed": [
                    2025,
                    11,
                    28,
                    8,
                    25,
                    59,
                    4,
                    332,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.22973v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.22973v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Generating minute-long videos is a critical step toward developing world models, providing a foundation for realistic extended scenes and advanced AI simulators. The emerging semi-autoregressive (block diffusion) paradigm integrates the strengths of diffusion and autoregressive models, enabling arbitrary-length video generation and improving inference efficiency through KV caching and parallel sampling. However, it yet faces two enduring challenges: (i) KV-cache-induced long-horizon error accumulation, and (ii) the lack of fine-grained long-video benchmarks and coherence-aware metrics. To overcome these limitations, we propose BlockVid, a novel block diffusion framework equipped with semantic-aware sparse KV cache, an effective training strategy called Block Forcing, and dedicated chunk-wise noise scheduling and shuffling to reduce error propagation and enhance temporal consistency. We further introduce LV-Bench, a fine-grained benchmark for minute-long videos, complete with new metrics evaluating long-range coherence. Extensive experiments on VBench and LV-Bench demonstrate that BlockVid consistently outperforms existing methods in generating high-quality, coherent minute-long videos. In particular, it achieves a 22.2% improvement on VDE Subject and a 19.4% improvement on VDE Clarity in LV-Bench over the state of the art approaches. Project website: https://ziplab.co/BlockVid. Inferix (Code): https://github.com/alibaba-damo-academy/Inferix.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating minute-long videos is a critical step toward developing world models, providing a foundation for realistic extended scenes and advanced AI simulators. The emerging semi-autoregressive (block diffusion) paradigm integrates the strengths of diffusion and autoregressive models, enabling arbitrary-length video generation and improving inference efficiency through KV caching and parallel sampling. However, it yet faces two enduring challenges: (i) KV-cache-induced long-horizon error accumulation, and (ii) the lack of fine-grained long-video benchmarks and coherence-aware metrics. To overcome these limitations, we propose BlockVid, a novel block diffusion framework equipped with semantic-aware sparse KV cache, an effective training strategy called Block Forcing, and dedicated chunk-wise noise scheduling and shuffling to reduce error propagation and enhance temporal consistency. We further introduce LV-Bench, a fine-grained benchmark for minute-long videos, complete with new metrics evaluating long-range coherence. Extensive experiments on VBench and LV-Bench demonstrate that BlockVid consistently outperforms existing methods in generating high-quality, coherent minute-long videos. In particular, it achieves a 22.2% improvement on VDE Subject and a 19.4% improvement on VDE Clarity in LV-Bench over the state of the art approaches. Project website: https://ziplab.co/BlockVid. Inferix (Code): https://github.com/alibaba-damo-academy/Inferix."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-28T08:25:59Z",
                "published_parsed": [
                    2025,
                    11,
                    28,
                    8,
                    25,
                    59,
                    4,
                    332,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Zeyu Zhang"
                    },
                    {
                        "name": "Shuning Chang"
                    },
                    {
                        "name": "Yuanyu He"
                    },
                    {
                        "name": "Yizeng Han"
                    },
                    {
                        "name": "Jiasheng Tang"
                    },
                    {
                        "name": "Fan Wang"
                    },
                    {
                        "name": "Bohan Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Bohan Zhuang"
                },
                "author": "Bohan Zhuang"
            },
            {
                "id": "http://arxiv.org/abs/2511.22889v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.22889v1",
                "title": "The Immutable Tensor Architecture: A Pure Dataflow Approach for Secure, Energy-Efficient AI Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Immutable Tensor Architecture: A Pure Dataflow Approach for Secure, Energy-Efficient AI Inference"
                },
                "updated": "2025-11-28T05:36:12Z",
                "updated_parsed": [
                    2025,
                    11,
                    28,
                    5,
                    36,
                    12,
                    4,
                    332,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.22889v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.22889v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The deployment of Large Language Models (LLMs) on consumer edge devices is throttled by the \"Memory Wall\" -- the prohibitive bandwidth and energy cost of fetching gigabytes of model weights from DRAM for every token generated. Current architectures (GPUs, NPUs) treat model weights as mutable software data, incurring massive energy penalties to maintain general-purpose programmability. We propose The Immutable Tensor Architecture (ITA), a paradigm shift that treats model weights not as data, but as physical circuit topology. By encoding parameters directly into the metal interconnects and logic of mature-node ASICs (28nm/40nm), ITA eliminates the memory hierarchy entirely. We present a \"Split-Brain\" system design where a host CPU manages dynamic KV-cache operations while the ITA ASIC acts as a stateless, ROM-embedded dataflow engine.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of Large Language Models (LLMs) on consumer edge devices is throttled by the \"Memory Wall\" -- the prohibitive bandwidth and energy cost of fetching gigabytes of model weights from DRAM for every token generated. Current architectures (GPUs, NPUs) treat model weights as mutable software data, incurring massive energy penalties to maintain general-purpose programmability. We propose The Immutable Tensor Architecture (ITA), a paradigm shift that treats model weights not as data, but as physical circuit topology. By encoding parameters directly into the metal interconnects and logic of mature-node ASICs (28nm/40nm), ITA eliminates the memory hierarchy entirely. We present a \"Split-Brain\" system design where a host CPU manages dynamic KV-cache operations while the ITA ASIC acts as a stateless, ROM-embedded dataflow engine."
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-28T05:36:12Z",
                "published_parsed": [
                    2025,
                    11,
                    28,
                    5,
                    36,
                    12,
                    4,
                    332,
                    0
                ],
                "arxiv_comment": "Code and data can be found here: https://github.com/fanglioc/ita-fpga-prototype",
                "arxiv_primary_category": {
                    "term": "cs.AR"
                },
                "authors": [
                    {
                        "name": "Fang Li"
                    }
                ],
                "author_detail": {
                    "name": "Fang Li"
                },
                "author": "Fang Li"
            },
            {
                "id": "http://arxiv.org/abs/2511.22880v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.22880v1",
                "title": "Serving Heterogeneous LoRA Adapters in Distributed LLM Inference Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving Heterogeneous LoRA Adapters in Distributed LLM Inference Systems"
                },
                "updated": "2025-11-28T05:04:02Z",
                "updated_parsed": [
                    2025,
                    11,
                    28,
                    5,
                    4,
                    2,
                    4,
                    332,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.22880v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.22880v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Low-Rank Adaptation (LoRA) has become the de facto method for parameter-efficient fine-tuning of large language models (LLMs), enabling rapid adaptation to diverse domains. In production, LoRA-based models are served at scale, creating multi-tenant environments with hundreds of adapters sharing a base model. However, state-of-the-art serving systems co-batch heterogeneous adapters without accounting for rank (size) variability, leading to severe performance skew, which ultimately requires adding more GPUs to satisfy service-level objectives (SLOs). Existing optimizations, focused on loading, caching, and kernel execution, ignore this heterogeneity, leaving GPU resources underutilized. We present LoRAServe, a workload-aware dynamic adapter placement and routing framework designed to tame rank diversity in LoRA serving. By dynamically rebalancing adapters across GPUs and leveraging GPU Direct RDMA for remote access, LoRAServe maximizes throughput and minimizes tail latency under real-world workload drift. Evaluations on production traces from Company X show that LoRAServe elicits up to 2$\\times$ higher throughput, up to 9$\\times$ lower TTFT, while using up to 50% fewer GPUs under SLO constraints compared to state-of-the-art systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Rank Adaptation (LoRA) has become the de facto method for parameter-efficient fine-tuning of large language models (LLMs), enabling rapid adaptation to diverse domains. In production, LoRA-based models are served at scale, creating multi-tenant environments with hundreds of adapters sharing a base model. However, state-of-the-art serving systems co-batch heterogeneous adapters without accounting for rank (size) variability, leading to severe performance skew, which ultimately requires adding more GPUs to satisfy service-level objectives (SLOs). Existing optimizations, focused on loading, caching, and kernel execution, ignore this heterogeneity, leaving GPU resources underutilized. We present LoRAServe, a workload-aware dynamic adapter placement and routing framework designed to tame rank diversity in LoRA serving. By dynamically rebalancing adapters across GPUs and leveraging GPU Direct RDMA for remote access, LoRAServe maximizes throughput and minimizes tail latency under real-world workload drift. Evaluations on production traces from Company X show that LoRAServe elicits up to 2$\\times$ higher throughput, up to 9$\\times$ lower TTFT, while using up to 50% fewer GPUs under SLO constraints compared to state-of-the-art systems."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-28T05:04:02Z",
                "published_parsed": [
                    2025,
                    11,
                    28,
                    5,
                    4,
                    2,
                    4,
                    332,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Shashwat Jaiswal"
                    },
                    {
                        "name": "Shrikara Arun"
                    },
                    {
                        "name": "Anjaly Parayil"
                    },
                    {
                        "name": "Ankur Mallick"
                    },
                    {
                        "name": "Spyros Mastorakis"
                    },
                    {
                        "name": "Alind Khare"
                    },
                    {
                        "name": "Chloi Alverti"
                    },
                    {
                        "name": "Renee St Amant"
                    },
                    {
                        "name": "Chetan Bansal"
                    },
                    {
                        "name": "Victor Rhle"
                    },
                    {
                        "name": "Josep Torrellas"
                    }
                ],
                "author_detail": {
                    "name": "Josep Torrellas"
                },
                "author": "Josep Torrellas"
            },
            {
                "id": "http://arxiv.org/abs/2511.22857v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.22857v1",
                "title": "GLOW: Global Illumination-Aware Inverse Rendering of Indoor Scenes Captured with Dynamic Co-Located Light & Camera",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GLOW: Global Illumination-Aware Inverse Rendering of Indoor Scenes Captured with Dynamic Co-Located Light & Camera"
                },
                "updated": "2025-11-28T03:24:12Z",
                "updated_parsed": [
                    2025,
                    11,
                    28,
                    3,
                    24,
                    12,
                    4,
                    332,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.22857v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.22857v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Inverse rendering of indoor scenes remains challenging due to the ambiguity between reflectance and lighting, exacerbated by inter-reflections among multiple objects. While natural illumination-based methods struggle to resolve this ambiguity, co-located light-camera setups offer better disentanglement as lighting can be easily calibrated via Structure-from-Motion. However, such setups introduce additional complexities like strong inter-reflections, dynamic shadows, near-field lighting, and moving specular highlights, which existing approaches fail to handle. We present GLOW, a Global Illumination-aware Inverse Rendering framework designed to address these challenges. GLOW integrates a neural implicit surface representation with a neural radiance cache to approximate global illumination, jointly optimizing geometry and reflectance through carefully designed regularization and initialization. We then introduce a dynamic radiance cache that adapts to sharp lighting discontinuities from near-field motion, and a surface-angle-weighted radiometric loss to suppress specular artifacts common in flashlight captures. Experiments show that GLOW substantially outperforms prior methods in material reflectance estimation under both natural and co-located illumination.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inverse rendering of indoor scenes remains challenging due to the ambiguity between reflectance and lighting, exacerbated by inter-reflections among multiple objects. While natural illumination-based methods struggle to resolve this ambiguity, co-located light-camera setups offer better disentanglement as lighting can be easily calibrated via Structure-from-Motion. However, such setups introduce additional complexities like strong inter-reflections, dynamic shadows, near-field lighting, and moving specular highlights, which existing approaches fail to handle. We present GLOW, a Global Illumination-aware Inverse Rendering framework designed to address these challenges. GLOW integrates a neural implicit surface representation with a neural radiance cache to approximate global illumination, jointly optimizing geometry and reflectance through carefully designed regularization and initialization. We then introduce a dynamic radiance cache that adapts to sharp lighting discontinuities from near-field motion, and a surface-angle-weighted radiometric loss to suppress specular artifacts common in flashlight captures. Experiments show that GLOW substantially outperforms prior methods in material reflectance estimation under both natural and co-located illumination."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-28T03:24:12Z",
                "published_parsed": [
                    2025,
                    11,
                    28,
                    3,
                    24,
                    12,
                    4,
                    332,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Jiaye Wu"
                    },
                    {
                        "name": "Saeed Hadadan"
                    },
                    {
                        "name": "Geng Lin"
                    },
                    {
                        "name": "Peihan Tu"
                    },
                    {
                        "name": "Matthias Zwicker"
                    },
                    {
                        "name": "David Jacobs"
                    },
                    {
                        "name": "Roni Sengupta"
                    }
                ],
                "author_detail": {
                    "name": "Roni Sengupta"
                },
                "author": "Roni Sengupta"
            },
            {
                "id": "http://arxiv.org/abs/2511.22681v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.22681v1",
                "title": "CacheTrap: Injecting Trojans in LLMs without Leaving any Traces in Inputs or Weights",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CacheTrap: Injecting Trojans in LLMs without Leaving any Traces in Inputs or Weights"
                },
                "updated": "2025-11-27T18:30:19Z",
                "updated_parsed": [
                    2025,
                    11,
                    27,
                    18,
                    30,
                    19,
                    3,
                    331,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.22681v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.22681v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Adversarial weight perturbation has emerged as a concerning threat to LLMs that either use training privileges or system-level access to inject adversarial corruption in model weights. With the emergence of innovative defensive solutions that place system- and algorithm-level checks and corrections in the input and weight spaces, these perturbations are increasingly susceptible to defenses. This work develops a novel perspective on Trojan attacks that generates an attacker-designed model output while leaving no attack traces on the inputs or weights. Such an attack space can be unlocked through corruption of the key-value (KV) cache. In this paper, we introduce CacheTrap, a novel Trojan attack that corrupts the value vectors stored in the KV cache. These vectors capture the dynamic activations for specific token positions and therefore constitute a natural surface for transient, inference-time trigger insertion. The transient nature of these KV values and their dependence on victim input imply additional constraints on our attack, such as a lack of knowledge of the victim's data or domain application, and, consequently, a lack of gradient information. The objective of the proposed CacheTrap is to develop a vulnerable KV bit-searching algorithm so that, once the attack employs the identified bit-flip as a trigger, the model generates targeted behavior, e.g., classifying inputs towards the target class. Moreover, CacheTrap is a data- and gradient-free attack which also has no impact on the model's utility. Our evaluation demonstrates that the proposed attack enables the first successful Trojan attack on LLMs with a single bit flip in the KV cache. In addition, the data-independent nature of the attack ensures that once the attacker identifies the vulnerable bit index, the location remains constant and can be transferred to a wide range of victim tasks/datasets/queries with no overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adversarial weight perturbation has emerged as a concerning threat to LLMs that either use training privileges or system-level access to inject adversarial corruption in model weights. With the emergence of innovative defensive solutions that place system- and algorithm-level checks and corrections in the input and weight spaces, these perturbations are increasingly susceptible to defenses. This work develops a novel perspective on Trojan attacks that generates an attacker-designed model output while leaving no attack traces on the inputs or weights. Such an attack space can be unlocked through corruption of the key-value (KV) cache. In this paper, we introduce CacheTrap, a novel Trojan attack that corrupts the value vectors stored in the KV cache. These vectors capture the dynamic activations for specific token positions and therefore constitute a natural surface for transient, inference-time trigger insertion. The transient nature of these KV values and their dependence on victim input imply additional constraints on our attack, such as a lack of knowledge of the victim's data or domain application, and, consequently, a lack of gradient information. The objective of the proposed CacheTrap is to develop a vulnerable KV bit-searching algorithm so that, once the attack employs the identified bit-flip as a trigger, the model generates targeted behavior, e.g., classifying inputs towards the target class. Moreover, CacheTrap is a data- and gradient-free attack which also has no impact on the model's utility. Our evaluation demonstrates that the proposed attack enables the first successful Trojan attack on LLMs with a single bit flip in the KV cache. In addition, the data-independent nature of the attack ensures that once the attacker identifies the vulnerable bit index, the location remains constant and can be transferred to a wide range of victim tasks/datasets/queries with no overhead."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-27T18:30:19Z",
                "published_parsed": [
                    2025,
                    11,
                    27,
                    18,
                    30,
                    19,
                    3,
                    331,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Mohaiminul Al Nahian"
                    },
                    {
                        "name": "Abeer Matar A. Almalky"
                    },
                    {
                        "name": "Gamana Aragonda"
                    },
                    {
                        "name": "Ranyang Zhou"
                    },
                    {
                        "name": "Sabbir Ahmed"
                    },
                    {
                        "name": "Dmitry Ponomarev"
                    },
                    {
                        "name": "Li Yang"
                    },
                    {
                        "name": "Shaahin Angizi"
                    },
                    {
                        "name": "Adnan Siraj Rakin"
                    }
                ],
                "author_detail": {
                    "name": "Adnan Siraj Rakin"
                },
                "arxiv_affiliation": "SUNY Binghamton",
                "author": "Adnan Siraj Rakin"
            },
            {
                "id": "http://arxiv.org/abs/2511.22551v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.22551v1",
                "title": "3RSeT: Read Disturbance Rate Reduction in STT-MRAM Caches by Selective Tag Comparison",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3RSeT: Read Disturbance Rate Reduction in STT-MRAM Caches by Selective Tag Comparison"
                },
                "updated": "2025-11-27T15:39:45Z",
                "updated_parsed": [
                    2025,
                    11,
                    27,
                    15,
                    39,
                    45,
                    3,
                    331,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.22551v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.22551v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recent development in memory technologies has introduced Spin-Transfer Torque Magnetic RAM (STT-MRAM) as the most promising replacement for SRAMs in on-chip cache memories. Besides its lower leakage power, higher density, immunity to radiation-induced particles, and non-volatility, an unintentional bit flip during read operation, referred to as read disturbance error, is a severe reliability challenge in STT-MRAM caches. One major source of read disturbance error in STT-MRAM caches is simultaneous accesses to all tags for parallel comparison operation in a cache set, which has not been addressed in previous work. This paper first demonstrates that high read accesses to tag array extremely increase the read disturbance rate and then proposes a low-cost scheme, so-called Read Disturbance Rate Reduction in STT-MRAM Caches by Selective Tag Comparison (3RSeT), to reduce the error rate by eliminating a significant portion of tag reads. 3RSeT proactively disables the tags that have no chance for hit, using low significant bits of the tags on each access request. Our evaluations using gem5 full-system cycle-accurate simulator show that 3RSeT reduces the read disturbance rate in the tag array by 71.8%, which results in 3.6x improvement in Mean Time To Failure (MTTF). In addition, the energy consumption is reduced by 62.1% without compromising performance and with less than 0.4% area overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent development in memory technologies has introduced Spin-Transfer Torque Magnetic RAM (STT-MRAM) as the most promising replacement for SRAMs in on-chip cache memories. Besides its lower leakage power, higher density, immunity to radiation-induced particles, and non-volatility, an unintentional bit flip during read operation, referred to as read disturbance error, is a severe reliability challenge in STT-MRAM caches. One major source of read disturbance error in STT-MRAM caches is simultaneous accesses to all tags for parallel comparison operation in a cache set, which has not been addressed in previous work. This paper first demonstrates that high read accesses to tag array extremely increase the read disturbance rate and then proposes a low-cost scheme, so-called Read Disturbance Rate Reduction in STT-MRAM Caches by Selective Tag Comparison (3RSeT), to reduce the error rate by eliminating a significant portion of tag reads. 3RSeT proactively disables the tags that have no chance for hit, using low significant bits of the tags on each access request. Our evaluations using gem5 full-system cycle-accurate simulator show that 3RSeT reduces the read disturbance rate in the tag array by 71.8%, which results in 3.6x improvement in Mean Time To Failure (MTTF). In addition, the energy consumption is reduced by 62.1% without compromising performance and with less than 0.4% area overhead."
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-27T15:39:45Z",
                "published_parsed": [
                    2025,
                    11,
                    27,
                    15,
                    39,
                    45,
                    3,
                    331,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR"
                },
                "authors": [
                    {
                        "name": "Elham Cheshmikhani"
                    },
                    {
                        "name": "Hamed Farbeh"
                    },
                    {
                        "name": "Hossein Asad"
                    }
                ],
                "author_detail": {
                    "name": "Hossein Asad"
                },
                "author": "Hossein Asad"
            },
            {
                "id": "http://arxiv.org/abs/2511.22533v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.22533v1",
                "title": "Fast3Dcache: Training-free 3D Geometry Synthesis Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast3Dcache: Training-free 3D Geometry Synthesis Acceleration"
                },
                "updated": "2025-11-27T15:13:32Z",
                "updated_parsed": [
                    2025,
                    11,
                    27,
                    15,
                    13,
                    32,
                    3,
                    331,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.22533v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.22533v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Diffusion models have achieved impressive generative quality across modalities like 2D images, videos, and 3D shapes, but their inference remains computationally expensive due to the iterative denoising process. While recent caching-based methods effectively reuse redundant computations to speed up 2D and video generation, directly applying these techniques to 3D diffusion models can severely disrupt geometric consistency. In 3D synthesis, even minor numerical errors in cached latent features accumulate, causing structural artifacts and topological inconsistencies. To overcome this limitation, we propose Fast3Dcache, a training-free geometry-aware caching framework that accelerates 3D diffusion inference while preserving geometric fidelity. Our method introduces a Predictive Caching Scheduler Constraint (PCSC) to dynamically determine cache quotas according to voxel stabilization patterns and a Spatiotemporal Stability Criterion (SSC) to select stable features for reuse based on velocity magnitude and acceleration criterion. Comprehensive experiments show that Fast3Dcache accelerates inference significantly, achieving up to a 27.12% speed-up and a 54.8% reduction in FLOPs, with minimal degradation in geometric quality as measured by Chamfer Distance (2.48%) and F-Score (1.95%).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have achieved impressive generative quality across modalities like 2D images, videos, and 3D shapes, but their inference remains computationally expensive due to the iterative denoising process. While recent caching-based methods effectively reuse redundant computations to speed up 2D and video generation, directly applying these techniques to 3D diffusion models can severely disrupt geometric consistency. In 3D synthesis, even minor numerical errors in cached latent features accumulate, causing structural artifacts and topological inconsistencies. To overcome this limitation, we propose Fast3Dcache, a training-free geometry-aware caching framework that accelerates 3D diffusion inference while preserving geometric fidelity. Our method introduces a Predictive Caching Scheduler Constraint (PCSC) to dynamically determine cache quotas according to voxel stabilization patterns and a Spatiotemporal Stability Criterion (SSC) to select stable features for reuse based on velocity magnitude and acceleration criterion. Comprehensive experiments show that Fast3Dcache accelerates inference significantly, achieving up to a 27.12% speed-up and a 54.8% reduction in FLOPs, with minimal degradation in geometric quality as measured by Chamfer Distance (2.48%) and F-Score (1.95%)."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-27T15:13:32Z",
                "published_parsed": [
                    2025,
                    11,
                    27,
                    15,
                    13,
                    32,
                    3,
                    331,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Mengyu Yang"
                    },
                    {
                        "name": "Yanming Yang"
                    },
                    {
                        "name": "Chenyi Xu"
                    },
                    {
                        "name": "Chenxi Song"
                    },
                    {
                        "name": "Yufan Zuo"
                    },
                    {
                        "name": "Tong Zhao"
                    },
                    {
                        "name": "Ruibo Li"
                    },
                    {
                        "name": "Chi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Chi Zhang"
                },
                "author": "Chi Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2511.22483v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.22483v1",
                "title": "Enhancing Trustworthiness with Mixed Precision: Benchmarks, Opportunities, and Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Trustworthiness with Mixed Precision: Benchmarks, Opportunities, and Challenges"
                },
                "updated": "2025-11-27T14:17:43Z",
                "updated_parsed": [
                    2025,
                    11,
                    27,
                    14,
                    17,
                    43,
                    3,
                    331,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.22483v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.22483v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) have shown promising performance across various tasks. However, their autoregressive decoding process poses significant challenges for efficient deployment on existing AI hardware. Quantization alleviates memory and compute pressure by compressing weights, activations, and KV caches to low precisions while preserving generation quality. However, existing quantization frameworks typically focus on perplexity or classification accuracy, often omitting critical trustworthiness metrics. This gap introduces risks when applying quantized LLMs to downstream high-stakes domains such as finance and healthcare. In this work, we systematically investigate the impact of quantization on four trustworthiness metrics (adversarial robustness, fairness, machine ethics, and out-of-distribution robustness) and identify the instability across compression ratios and quantization methods. Building on these observations, we develop a novel precision-ensemble voting approach that leverages predictions from mixed-precision variants of the same model and consistently improves performance by up to $5.8\\%$ on trustworthiness metrics. Our results highlight the importance of considering trustworthiness when developing model compression techniques and point to research opportunities at the intersection of compression and trustworthiness for safety-critical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown promising performance across various tasks. However, their autoregressive decoding process poses significant challenges for efficient deployment on existing AI hardware. Quantization alleviates memory and compute pressure by compressing weights, activations, and KV caches to low precisions while preserving generation quality. However, existing quantization frameworks typically focus on perplexity or classification accuracy, often omitting critical trustworthiness metrics. This gap introduces risks when applying quantized LLMs to downstream high-stakes domains such as finance and healthcare. In this work, we systematically investigate the impact of quantization on four trustworthiness metrics (adversarial robustness, fairness, machine ethics, and out-of-distribution robustness) and identify the instability across compression ratios and quantization methods. Building on these observations, we develop a novel precision-ensemble voting approach that leverages predictions from mixed-precision variants of the same model and consistently improves performance by up to $5.8\\%$ on trustworthiness metrics. Our results highlight the importance of considering trustworthiness when developing model compression techniques and point to research opportunities at the intersection of compression and trustworthiness for safety-critical applications."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-27T14:17:43Z",
                "published_parsed": [
                    2025,
                    11,
                    27,
                    14,
                    17,
                    43,
                    3,
                    331,
                    0
                ],
                "arxiv_comment": "ASP-DAC 2026 Special Session",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Guanxi Lu"
                    },
                    {
                        "name": "Hao Mark Chen"
                    },
                    {
                        "name": "Zhiqiang Que"
                    },
                    {
                        "name": "Wayne Luk"
                    },
                    {
                        "name": "Hongxiang Fan"
                    }
                ],
                "author_detail": {
                    "name": "Hongxiang Fan"
                },
                "author": "Hongxiang Fan"
            },
            {
                "id": "http://arxiv.org/abs/2511.22481v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.22481v1",
                "title": "OmniInfer: System-Wide Acceleration Techniques for Optimizing LLM Serving Throughput and Latency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OmniInfer: System-Wide Acceleration Techniques for Optimizing LLM Serving Throughput and Latency"
                },
                "updated": "2025-11-27T14:13:47Z",
                "updated_parsed": [
                    2025,
                    11,
                    27,
                    14,
                    13,
                    47,
                    3,
                    331,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.22481v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.22481v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models drive a wide range of modern AI applications but impose substantial challenges on large-scale serving systems due to intensive computation, strict latency constraints, and throughput bottlenecks. We introduce OmniInfer, a unified system-level acceleration framework designed to maximize end-to-end serving efficiency through fine-grained optimization of expert placement, cache compression, and scheduling. OmniInfer integrates three complementary components: OmniPlacement for load-aware Mixture-of-Experts scheduling, OmniAttn for sparse attention acceleration, and OmniProxy for disaggregation-aware request scheduling. Built atop vLLM, OmniInfer delivers system-wide performance gains through adaptive resource disaggregation, efficient sparsity exploitation, and global coordination across prefill and decode phases. Evaluated on DeepSeek-R1 within a 10-node Ascend 910C cluster, OmniInfer achieves 616 QPM, where the unified framework reduces TPOT by 36\\%, and the superimposition of OmniProxy further slashes TTFT by 38\\%. The project is open-sourced at [this https URL](https://gitee.com/omniai/omniinfer).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models drive a wide range of modern AI applications but impose substantial challenges on large-scale serving systems due to intensive computation, strict latency constraints, and throughput bottlenecks. We introduce OmniInfer, a unified system-level acceleration framework designed to maximize end-to-end serving efficiency through fine-grained optimization of expert placement, cache compression, and scheduling. OmniInfer integrates three complementary components: OmniPlacement for load-aware Mixture-of-Experts scheduling, OmniAttn for sparse attention acceleration, and OmniProxy for disaggregation-aware request scheduling. Built atop vLLM, OmniInfer delivers system-wide performance gains through adaptive resource disaggregation, efficient sparsity exploitation, and global coordination across prefill and decode phases. Evaluated on DeepSeek-R1 within a 10-node Ascend 910C cluster, OmniInfer achieves 616 QPM, where the unified framework reduces TPOT by 36\\%, and the superimposition of OmniProxy further slashes TTFT by 38\\%. The project is open-sourced at [this https URL](https://gitee.com/omniai/omniinfer)."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-27T14:13:47Z",
                "published_parsed": [
                    2025,
                    11,
                    27,
                    14,
                    13,
                    47,
                    3,
                    331,
                    0
                ],
                "arxiv_comment": "Project page: [this https URL](https://gitee.com/omniai/omniinfer)",
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Yunxiang Yao"
                    },
                    {
                        "name": "Wenwei Kuang"
                    },
                    {
                        "name": "Runze Mao"
                    },
                    {
                        "name": "Zhenhao Sun"
                    },
                    {
                        "name": "Zhuang Tao"
                    },
                    {
                        "name": "Ziyang Zhang"
                    },
                    {
                        "name": "Dengyu Li"
                    },
                    {
                        "name": "Jiajun Chen"
                    },
                    {
                        "name": "Zhili Wang"
                    },
                    {
                        "name": "Kai Cui"
                    },
                    {
                        "name": "Congzhi Cai"
                    },
                    {
                        "name": "Longwen Lan"
                    },
                    {
                        "name": "Ken Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ken Zhang"
                },
                "author": "Ken Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2512.00112v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.00112v1",
                "title": "An Analytical and Empirical Investigation of Tag Partitioning for Energy-Efficient Reliable Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Analytical and Empirical Investigation of Tag Partitioning for Energy-Efficient Reliable Cache"
                },
                "updated": "2025-11-27T13:55:50Z",
                "updated_parsed": [
                    2025,
                    11,
                    27,
                    13,
                    55,
                    50,
                    3,
                    331,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.00112v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.00112v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Associative cache memory significantly influences processor performance and energy consumption. Because it occupies over half of the chip area, cache memory is highly susceptible to transient and permanent faults, posing reliability challenges. As the only hardware-managed memory module, the cache tag array is the most active and critical component, dominating both energy usage and error rate. Tag partitioning is a widely used technique to reduce tag-access energy and enhance reliability. It divides tag comparison into two phases: first comparing the k lower bits, and then activating only the matching tag entries to compare the remaining higher bits. The key design parameter is the selection of the tag-splitting point k, which determines how many reads are eliminated. However, prior studies have chosen k intuitively, randomly, or empirically, without justification. Even experimentally determined values are ad-hoc and do not generalize across cache configurations due to high sensitivity to architectural parameters.\n  In this paper, we analytically show that choosing k too large or too small substantially reduces the effectiveness of tag partitioning. We then derive a formulation that determines the optimal splitting point based on cache configuration parameters. The formulation is convex, differentiable, and capable of precisely quantifying tag-partitioning efficiency for any k and configuration. To validate our model, we experimentally evaluate tag-partitioning efficiency and optimal k across a broad set of cache designs and demonstrate close agreement between analytical and experimental results. The proposed formulation enables designers and researchers to instantly compute the optimal tag-splitting point and accurately estimate tag-read reduction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Associative cache memory significantly influences processor performance and energy consumption. Because it occupies over half of the chip area, cache memory is highly susceptible to transient and permanent faults, posing reliability challenges. As the only hardware-managed memory module, the cache tag array is the most active and critical component, dominating both energy usage and error rate. Tag partitioning is a widely used technique to reduce tag-access energy and enhance reliability. It divides tag comparison into two phases: first comparing the k lower bits, and then activating only the matching tag entries to compare the remaining higher bits. The key design parameter is the selection of the tag-splitting point k, which determines how many reads are eliminated. However, prior studies have chosen k intuitively, randomly, or empirically, without justification. Even experimentally determined values are ad-hoc and do not generalize across cache configurations due to high sensitivity to architectural parameters.\n  In this paper, we analytically show that choosing k too large or too small substantially reduces the effectiveness of tag partitioning. We then derive a formulation that determines the optimal splitting point based on cache configuration parameters. The formulation is convex, differentiable, and capable of precisely quantifying tag-partitioning efficiency for any k and configuration. To validate our model, we experimentally evaluate tag-partitioning efficiency and optimal k across a broad set of cache designs and demonstrate close agreement between analytical and experimental results. The proposed formulation enables designers and researchers to instantly compute the optimal tag-splitting point and accurately estimate tag-read reduction."
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-27T13:55:50Z",
                "published_parsed": [
                    2025,
                    11,
                    27,
                    13,
                    55,
                    50,
                    3,
                    331,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR"
                },
                "authors": [
                    {
                        "name": "Elham Cheshmikhani"
                    },
                    {
                        "name": "Hamed Farbeh"
                    }
                ],
                "author_detail": {
                    "name": "Hamed Farbeh"
                },
                "author": "Hamed Farbeh"
            },
            {
                "id": "http://arxiv.org/abs/2510.18546v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.18546v2",
                "title": "EfficientNav: Towards On-Device Object-Goal Navigation with Navigation Map Caching and Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EfficientNav: Towards On-Device Object-Goal Navigation with Navigation Map Caching and Retrieval"
                },
                "updated": "2025-11-27T13:54:42Z",
                "updated_parsed": [
                    2025,
                    11,
                    27,
                    13,
                    54,
                    42,
                    3,
                    331,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.18546v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.18546v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Object-goal navigation (ObjNav) tasks an agent with navigating to the location of a specific object in an unseen environment. Embodied agents equipped with large language models (LLMs) and online constructed navigation maps can perform ObjNav in a zero-shot manner. However, existing agents heavily rely on giant LLMs on the cloud, e.g., GPT-4, while directly switching to small LLMs, e.g., LLaMA3.2-11b, suffer from significant success rate drops due to limited model capacity for understanding complex navigation maps, which prevents deploying ObjNav on local devices. At the same time, the long prompt introduced by the navigation map description will cause high planning latency on local devices. In this paper, we propose EfficientNav to enable on-device efficient LLM-based zero-shot ObjNav. To help the smaller LLMs better understand the environment, we propose semantics-aware memory retrieval to prune redundant information in navigation maps. To reduce planning latency, we propose discrete memory caching and attention-based memory clustering to efficiently save and re-use the KV cache. Extensive experimental results demonstrate that EfficientNav achieves 11.1% improvement in success rate on HM3D benchmark over GPT-4-based baselines, and demonstrates 6.7x real-time latency reduction and 4.7x end-to-end latency reduction over GPT-4 planner. Our code is available on https://github.com/PKU-SEC-Lab/EfficientNav.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Object-goal navigation (ObjNav) tasks an agent with navigating to the location of a specific object in an unseen environment. Embodied agents equipped with large language models (LLMs) and online constructed navigation maps can perform ObjNav in a zero-shot manner. However, existing agents heavily rely on giant LLMs on the cloud, e.g., GPT-4, while directly switching to small LLMs, e.g., LLaMA3.2-11b, suffer from significant success rate drops due to limited model capacity for understanding complex navigation maps, which prevents deploying ObjNav on local devices. At the same time, the long prompt introduced by the navigation map description will cause high planning latency on local devices. In this paper, we propose EfficientNav to enable on-device efficient LLM-based zero-shot ObjNav. To help the smaller LLMs better understand the environment, we propose semantics-aware memory retrieval to prune redundant information in navigation maps. To reduce planning latency, we propose discrete memory caching and attention-based memory clustering to efficiently save and re-use the KV cache. Extensive experimental results demonstrate that EfficientNav achieves 11.1% improvement in success rate on HM3D benchmark over GPT-4-based baselines, and demonstrates 6.7x real-time latency reduction and 4.7x end-to-end latency reduction over GPT-4 planner. Our code is available on https://github.com/PKU-SEC-Lab/EfficientNav."
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-21T11:52:44Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    11,
                    52,
                    44,
                    1,
                    294,
                    0
                ],
                "arxiv_comment": "NeurIPS 2025",
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "authors": [
                    {
                        "name": "Zebin Yang"
                    },
                    {
                        "name": "Sunjian Zheng"
                    },
                    {
                        "name": "Tong Xie"
                    },
                    {
                        "name": "Tianshi Xu"
                    },
                    {
                        "name": "Bo Yu"
                    },
                    {
                        "name": "Fan Wang"
                    },
                    {
                        "name": "Jie Tang"
                    },
                    {
                        "name": "Shaoshan Liu"
                    },
                    {
                        "name": "Meng Li"
                    }
                ],
                "author_detail": {
                    "name": "Meng Li"
                },
                "author": "Meng Li"
            },
            {
                "id": "http://arxiv.org/abs/2511.22333v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.22333v1",
                "title": "PAT: Accelerating LLM Decoding via Prefix-Aware Attention with Resource Efficient Multi-Tile Kernel",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PAT: Accelerating LLM Decoding via Prefix-Aware Attention with Resource Efficient Multi-Tile Kernel"
                },
                "updated": "2025-11-27T11:10:30Z",
                "updated_parsed": [
                    2025,
                    11,
                    27,
                    11,
                    10,
                    30,
                    3,
                    331,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.22333v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.22333v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "LLM serving is increasingly dominated by decode attention, which is a memory-bound operation due to massive KV cache loading from global memory. Meanwhile, real-world workloads exhibit substantial, hierarchical shared prefixes across requests (e.g., system prompts, tools/templates, RAG). Existing attention implementations fail to fully exploit prefix sharing: *one-query-per-CTA* execution repeatedly loads shared prefix KV cache, while *one-size-fits-all* tiling leaves on-chip resources idle and exacerbates bubbles for uneven KV lengths. These choices amplify memory bandwidth pressure and stall memory-bound decode attention.\n  This paper introduces PAT, a prefix-aware attention kernel implementation for LLM decoding that organizes execution with a pack-forward-merge paradigm. PAT packs queries by shared prefix to reduce repeated memory accesses, runs a customized multi-tile kernel to achieve high resource efficiency. It further applies practical multi-stream forwarding and KV splitting to reduce resource bubbles. The final merge performs online softmax with negligible overhead. We implement PAT as an off-the-shelf plugin for vLLM. Evaluation on both real-world and synthetic workloads shows that PAT reduces attention latency by 67.4% on average and TPOT by 13.6-83.4% under the same configurations against state-of-the-art attention kernels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM serving is increasingly dominated by decode attention, which is a memory-bound operation due to massive KV cache loading from global memory. Meanwhile, real-world workloads exhibit substantial, hierarchical shared prefixes across requests (e.g., system prompts, tools/templates, RAG). Existing attention implementations fail to fully exploit prefix sharing: *one-query-per-CTA* execution repeatedly loads shared prefix KV cache, while *one-size-fits-all* tiling leaves on-chip resources idle and exacerbates bubbles for uneven KV lengths. These choices amplify memory bandwidth pressure and stall memory-bound decode attention.\n  This paper introduces PAT, a prefix-aware attention kernel implementation for LLM decoding that organizes execution with a pack-forward-merge paradigm. PAT packs queries by shared prefix to reduce repeated memory accesses, runs a customized multi-tile kernel to achieve high resource efficiency. It further applies practical multi-stream forwarding and KV splitting to reduce resource bubbles. The final merge performs online softmax with negligible overhead. We implement PAT as an off-the-shelf plugin for vLLM. Evaluation on both real-world and synthetic workloads shows that PAT reduces attention latency by 67.4% on average and TPOT by 13.6-83.4% under the same configurations against state-of-the-art attention kernels."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-27T11:10:30Z",
                "published_parsed": [
                    2025,
                    11,
                    27,
                    11,
                    10,
                    30,
                    3,
                    331,
                    0
                ],
                "arxiv_comment": "Accepted by ASPLOS'26",
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Jinjun Yi"
                    },
                    {
                        "name": "Zhixin Zhao"
                    },
                    {
                        "name": "Yitao Hu"
                    },
                    {
                        "name": "Ke Yan"
                    },
                    {
                        "name": "Weiwei Sun"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Laiping Zhao"
                    },
                    {
                        "name": "Yuhao Zhang"
                    },
                    {
                        "name": "Wenxin Li"
                    },
                    {
                        "name": "Keqiu Li"
                    }
                ],
                "author_detail": {
                    "name": "Keqiu Li"
                },
                "author": "Keqiu Li"
            },
            {
                "id": "http://arxiv.org/abs/2501.16607v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2501.16607v3",
                "title": "MCTS-SQL: Light-Weight LLMs can Master the Text-to-SQL through Monte Carlo Tree Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MCTS-SQL: Light-Weight LLMs can Master the Text-to-SQL through Monte Carlo Tree Search"
                },
                "updated": "2025-11-27T09:37:42Z",
                "updated_parsed": [
                    2025,
                    11,
                    27,
                    9,
                    37,
                    42,
                    3,
                    331,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2501.16607v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2501.16607v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Text-to-SQL is a fundamental yet challenging task in the NLP area, aiming at translating natural language questions into SQL queries. While recent advances in large language models have greatly improved performance, most existing approaches depend on models with tens of billions of parameters or costly APIs, limiting their applicability in resource-constrained environments. For real world, especially on edge devices, it is crucial for Text-to-SQL to ensure cost-effectiveness. Therefore, enabling the light-weight models for Text-to-SQL is of great practical significance. However, smaller LLMs often struggle with complicated user instruction, redundant schema linking or syntax correctness. To address these challenges, we propose MCTS-SQL, a novel framework that uses Monte Carlo Tree Search to guide SQL generation through multi-step refinement. Since the light-weight models' weak performance of single-shot prediction, we generate better results through several trials with feedback. However, directly applying MCTS-based methods inevitably leads to significant time and computational overhead. Driven by this issue, we propose a token-level prefix-cache mechanism that stores prior information during iterations, effectively improved the execution speed. Experiments results on the SPIDER and BIRD benchmarks demonstrate the effectiveness of our approach. Using a small open-source Qwen2.5-Coder-1.5B, our method outperforms ChatGPT-3.5. When leveraging a more powerful model Gemini 2.5 to explore the performance upper bound, we achieved results competitive with the SOTA. Our findings demonstrate that even small models can be effectively deployed in practical Text-to-SQL systems with the right strategy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-SQL is a fundamental yet challenging task in the NLP area, aiming at translating natural language questions into SQL queries. While recent advances in large language models have greatly improved performance, most existing approaches depend on models with tens of billions of parameters or costly APIs, limiting their applicability in resource-constrained environments. For real world, especially on edge devices, it is crucial for Text-to-SQL to ensure cost-effectiveness. Therefore, enabling the light-weight models for Text-to-SQL is of great practical significance. However, smaller LLMs often struggle with complicated user instruction, redundant schema linking or syntax correctness. To address these challenges, we propose MCTS-SQL, a novel framework that uses Monte Carlo Tree Search to guide SQL generation through multi-step refinement. Since the light-weight models' weak performance of single-shot prediction, we generate better results through several trials with feedback. However, directly applying MCTS-based methods inevitably leads to significant time and computational overhead. Driven by this issue, we propose a token-level prefix-cache mechanism that stores prior information during iterations, effectively improved the execution speed. Experiments results on the SPIDER and BIRD benchmarks demonstrate the effectiveness of our approach. Using a small open-source Qwen2.5-Coder-1.5B, our method outperforms ChatGPT-3.5. When leveraging a more powerful model Gemini 2.5 to explore the performance upper bound, we achieved results competitive with the SOTA. Our findings demonstrate that even small models can be effectively deployed in practical Text-to-SQL systems with the right strategy."
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-01-28T00:52:23Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    0,
                    52,
                    23,
                    1,
                    28,
                    0
                ],
                "arxiv_comment": "Accepted by AAAI 2026",
                "arxiv_primary_category": {
                    "term": "cs.DB"
                },
                "authors": [
                    {
                        "name": "Shuozhi Yuan"
                    },
                    {
                        "name": "Limin Chen"
                    },
                    {
                        "name": "Miaomiao Yuan"
                    },
                    {
                        "name": "Zhao Jin"
                    }
                ],
                "author_detail": {
                    "name": "Zhao Jin"
                },
                "author": "Zhao Jin"
            },
            {
                "id": "http://arxiv.org/abs/2504.09936v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2504.09936v2",
                "title": "KeepKV: Achieving Periodic Lossless KV Cache Compression for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KeepKV: Achieving Periodic Lossless KV Cache Compression for Efficient LLM Inference"
                },
                "updated": "2025-11-27T07:44:35Z",
                "updated_parsed": [
                    2025,
                    11,
                    27,
                    7,
                    44,
                    35,
                    3,
                    331,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2504.09936v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2504.09936v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Efficient inference of large language models (LLMs) is hindered by an ever-growing key-value (KV) cache, making KV cache compression a critical research direction. Traditional methods selectively evict less important KV cache entries, which leads to information loss and hallucinations. Recently, merging-based strategies have been explored to retain more information by merging KV pairs that would be discarded; however, these existing approaches inevitably introduce inconsistencies in attention distributions before and after merging, causing degraded generation quality. To overcome this challenge, we propose KeepKV, a novel adaptive KV cache merging method designed to preserve performance under strict memory constraints, achieving single-step lossless compression and providing error bounds for multi-step compression. KeepKV introduces the Electoral Votes mechanism that records merging history and adaptively adjusts attention scores. Moreover, it further leverages a novel Zero Inference-Perturbation Merging method, compensating for attention loss resulting from cache merging. Extensive experiments on various benchmarks and LLM architectures demonstrate that KeepKV substantially reduces memory usage while successfully retaining essential context information, achieving over 2x inference throughput improvement and maintaining superior generation quality even with only 10% KV cache budgets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient inference of large language models (LLMs) is hindered by an ever-growing key-value (KV) cache, making KV cache compression a critical research direction. Traditional methods selectively evict less important KV cache entries, which leads to information loss and hallucinations. Recently, merging-based strategies have been explored to retain more information by merging KV pairs that would be discarded; however, these existing approaches inevitably introduce inconsistencies in attention distributions before and after merging, causing degraded generation quality. To overcome this challenge, we propose KeepKV, a novel adaptive KV cache merging method designed to preserve performance under strict memory constraints, achieving single-step lossless compression and providing error bounds for multi-step compression. KeepKV introduces the Electoral Votes mechanism that records merging history and adaptively adjusts attention scores. Moreover, it further leverages a novel Zero Inference-Perturbation Merging method, compensating for attention loss resulting from cache merging. Extensive experiments on various benchmarks and LLM architectures demonstrate that KeepKV substantially reduces memory usage while successfully retaining essential context information, achieving over 2x inference throughput improvement and maintaining superior generation quality even with only 10% KV cache budgets."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-04-14T06:58:00Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    6,
                    58,
                    0,
                    0,
                    104,
                    0
                ],
                "arxiv_comment": "14 pages, 20 figures",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Yuxuan Tian"
                    },
                    {
                        "name": "Zihan Wang"
                    },
                    {
                        "name": "Yebo Peng"
                    },
                    {
                        "name": "Aomufei Yuan"
                    },
                    {
                        "name": "Zhiming Wang"
                    },
                    {
                        "name": "Bairen Yi"
                    },
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Yong Cui"
                    },
                    {
                        "name": "Tong Yang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Yang"
                },
                "author": "Tong Yang"
            },
            {
                "id": "http://arxiv.org/abs/2511.22118v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.22118v1",
                "title": "Statistical Independence Aware Caching for LLM Workflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Statistical Independence Aware Caching for LLM Workflows"
                },
                "updated": "2025-11-27T05:16:28Z",
                "updated_parsed": [
                    2025,
                    11,
                    27,
                    5,
                    16,
                    28,
                    3,
                    331,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.22118v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.22118v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) inference is both expensive and slow. Local caching of responses offers a practical solution to reduce the cost and latency of LLM queries. In research contexts, caching also enhances reproducibility and provides flexibility for experimentation. However, naive reuse of cached responses compromises statistical independence, a critical property for probabilistic workflows. In applications of LLM for code, it underpins performance metrics such as Pass@k and uncertainty estimation, as well as algorithms like program repair loops and retries. Existing LLM caching systems lack ways to enforce statistical independence constraints. To address this, we introduce Mnimi, a cache design pattern that supports modular LLM workflows while ensuring statistical integrity at the component level. Its core innovation lies in encapsulating statistical constraints within the type of LLM references, allowing users to manage and transform these types according to the scope and requirements of their algorithm. We implemented this design pattern in Python using a combination of decorators and iterators over infinite sequences. A case study on SpecFix, an recent automated program specification repair system, highlights how Mnimi improves reproducibility, ease of debugging, time and cost efficiency while preserving statistical correctness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) inference is both expensive and slow. Local caching of responses offers a practical solution to reduce the cost and latency of LLM queries. In research contexts, caching also enhances reproducibility and provides flexibility for experimentation. However, naive reuse of cached responses compromises statistical independence, a critical property for probabilistic workflows. In applications of LLM for code, it underpins performance metrics such as Pass@k and uncertainty estimation, as well as algorithms like program repair loops and retries. Existing LLM caching systems lack ways to enforce statistical independence constraints. To address this, we introduce Mnimi, a cache design pattern that supports modular LLM workflows while ensuring statistical integrity at the component level. Its core innovation lies in encapsulating statistical constraints within the type of LLM references, allowing users to manage and transform these types according to the scope and requirements of their algorithm. We implemented this design pattern in Python using a combination of decorators and iterators over infinite sequences. A case study on SpecFix, an recent automated program specification repair system, highlights how Mnimi improves reproducibility, ease of debugging, time and cost efficiency while preserving statistical correctness."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-27T05:16:28Z",
                "published_parsed": [
                    2025,
                    11,
                    27,
                    5,
                    16,
                    28,
                    3,
                    331,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Yihan Dai"
                    },
                    {
                        "name": "Dimitrios Stamatios Bouras"
                    },
                    {
                        "name": "Haoxiang Jia"
                    },
                    {
                        "name": "Sergey Mechtaev"
                    }
                ],
                "author_detail": {
                    "name": "Sergey Mechtaev"
                },
                "author": "Sergey Mechtaev"
            },
            {
                "id": "http://arxiv.org/abs/2511.21612v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.21612v1",
                "title": "Diagonal Scaling: A Multi-Dimensional Resource Model and Optimization Framework for Distributed Databases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diagonal Scaling: A Multi-Dimensional Resource Model and Optimization Framework for Distributed Databases"
                },
                "updated": "2025-11-26T17:36:15Z",
                "updated_parsed": [
                    2025,
                    11,
                    26,
                    17,
                    36,
                    15,
                    2,
                    330,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.21612v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.21612v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Modern cloud databases present scaling as a binary decision: scale-out by adding nodes or scale-up by increasing per-node resources. This one-dimensional view is limiting because database performance, cost, and coordination overhead emerge from the joint interaction of horizontal elasticity and per-node CPU, memory, network bandwidth, and storage IOPS. As a result, systems often overreact to load spikes, underreact to memory pressure, or oscillate between suboptimal states. We introduce the Scaling Plane, a two-dimensional model in which each distributed database configuration is represented as a point (H, V), with H denoting node count and V a vector of resources. Over this plane, we define smooth approximations of latency, throughput, coordination overhead, and monetary cost, providing a unified view of performance trade-offs. We show analytically and empirically that optimal scaling trajectories frequently lie along diagonal paths: sequences of joint horizontal and vertical adjustments that simultaneously exploit cluster parallelism and per-node improvements. To compute such actions, we propose DIAGONALSCALE, a discrete local-search algorithm that evaluates horizontal, vertical, and diagonal moves in the Scaling Plane and selects the configuration minimizing a multi-objective function subject to SLA constraints. Using synthetic surfaces, microbenchmarks, and experiments on distributed SQL and KV systems, we demonstrate that diagonal scaling reduces p95 latency by up to 40 percent, lowers cost-per-query by up to 37 percent, and reduces rebalancing by 2 to 5 times compared to horizontal-only and vertical-only autoscaling. Our results highlight the need for multi-dimensional scaling models and provide a foundation for next-generation autoscaling in cloud database systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern cloud databases present scaling as a binary decision: scale-out by adding nodes or scale-up by increasing per-node resources. This one-dimensional view is limiting because database performance, cost, and coordination overhead emerge from the joint interaction of horizontal elasticity and per-node CPU, memory, network bandwidth, and storage IOPS. As a result, systems often overreact to load spikes, underreact to memory pressure, or oscillate between suboptimal states. We introduce the Scaling Plane, a two-dimensional model in which each distributed database configuration is represented as a point (H, V), with H denoting node count and V a vector of resources. Over this plane, we define smooth approximations of latency, throughput, coordination overhead, and monetary cost, providing a unified view of performance trade-offs. We show analytically and empirically that optimal scaling trajectories frequently lie along diagonal paths: sequences of joint horizontal and vertical adjustments that simultaneously exploit cluster parallelism and per-node improvements. To compute such actions, we propose DIAGONALSCALE, a discrete local-search algorithm that evaluates horizontal, vertical, and diagonal moves in the Scaling Plane and selects the configuration minimizing a multi-objective function subject to SLA constraints. Using synthetic surfaces, microbenchmarks, and experiments on distributed SQL and KV systems, we demonstrate that diagonal scaling reduces p95 latency by up to 40 percent, lowers cost-per-query by up to 37 percent, and reduces rebalancing by 2 to 5 times compared to horizontal-only and vertical-only autoscaling. Our results highlight the need for multi-dimensional scaling models and provide a foundation for next-generation autoscaling in cloud database systems."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-26T17:36:15Z",
                "published_parsed": [
                    2025,
                    11,
                    26,
                    17,
                    36,
                    15,
                    2,
                    330,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Shahir Abdullah"
                    },
                    {
                        "name": "Syed Rohit Zaman"
                    }
                ],
                "author_detail": {
                    "name": "Syed Rohit Zaman"
                },
                "author": "Syed Rohit Zaman"
            },
            {
                "id": "http://arxiv.org/abs/2511.21535v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.21535v1",
                "title": "Modeling the Effect of Data Redundancy on Speedup in MLFMA Near-Field Computation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling the Effect of Data Redundancy on Speedup in MLFMA Near-Field Computation"
                },
                "updated": "2025-11-26T16:01:32Z",
                "updated_parsed": [
                    2025,
                    11,
                    26,
                    16,
                    1,
                    32,
                    2,
                    330,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.21535v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.21535v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The near-field (P2P) operator in the Multilevel Fast Multipole Algorithm (MLFMA) is a performance bottleneck on GPUs due to poor memory locality. This work introduces data redundancy to improve spatial locality by reducing memory access dispersion. For validation of results, we propose an analytical model based on a Locality metric that combines data volume and access dispersion to predict speedup trends without hardware-specific profiling. The approach is validated on two MLFMA-based applications: an electromagnetic solver (DBIM-MLFMA) with regular structure, and a stellar dynamics code (PhotoNs-2.0) with irregular particle distribution. Results show up to 7X kernel speedup due to improved cache behavior. However, increased data volume raises overheads in data restructuring, limiting end-to-end application speedup to 1.04X. While the model cannot precisely predict absolute speedups, it reliably captures performance trends across different problem sizes and densities. The technique is injectable into existing implementations with minimal code changes. This work demonstrates that data redundancy can enhance GPU performance for P2P operator, provided locality gains outweigh data movement costs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The near-field (P2P) operator in the Multilevel Fast Multipole Algorithm (MLFMA) is a performance bottleneck on GPUs due to poor memory locality. This work introduces data redundancy to improve spatial locality by reducing memory access dispersion. For validation of results, we propose an analytical model based on a Locality metric that combines data volume and access dispersion to predict speedup trends without hardware-specific profiling. The approach is validated on two MLFMA-based applications: an electromagnetic solver (DBIM-MLFMA) with regular structure, and a stellar dynamics code (PhotoNs-2.0) with irregular particle distribution. Results show up to 7X kernel speedup due to improved cache behavior. However, increased data volume raises overheads in data restructuring, limiting end-to-end application speedup to 1.04X. While the model cannot precisely predict absolute speedups, it reliably captures performance trends across different problem sizes and densities. The technique is injectable into existing implementations with minimal code changes. This work demonstrates that data redundancy can enhance GPU performance for P2P operator, provided locality gains outweigh data movement costs."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-26T16:01:32Z",
                "published_parsed": [
                    2025,
                    11,
                    26,
                    16,
                    1,
                    32,
                    2,
                    330,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Morteza Sadeghi"
                    }
                ],
                "author_detail": {
                    "name": "Morteza Sadeghi"
                },
                "author": "Morteza Sadeghi"
            },
            {
                "id": "http://arxiv.org/abs/2511.21408v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.21408v1",
                "title": "Subjective Depth and Timescale Transformers: Learning Where and When to Compute",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Subjective Depth and Timescale Transformers: Learning Where and When to Compute"
                },
                "updated": "2025-11-26T14:00:18Z",
                "updated_parsed": [
                    2025,
                    11,
                    26,
                    14,
                    0,
                    18,
                    2,
                    330,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.21408v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.21408v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The rigid, uniform allocation of computation in standard Transformer (TF) architectures can limit their efficiency and scalability, particularly for large-scale models and long sequences. Addressing this, we introduce Subjective Depth Transformers (SDT) and Subjective Timescale Transformers (STT), two distinct architectures that leverage Bayesian surprise signals to dynamically route computation, learning where and when to compute within decoder-only TFs. SDT augments a decoder-only stack with alternating Decision and Dynamic layers: a Decision layer computes a full block 'posterior' and a lightweight 'prior,' while a Dynamic layer employs fixed-capacity Top-K routing based on Bayesian surprise (Expected and Unexpected Change), maintaining a static compute graph. STT extends this conditional computation to the temporal domain: a transition network predicts residual updates, forming a temporal 'change hypothesis' that informs a router to dynamically execute or bypass TF blocks for each token, managing KV-cache contributions. Both architectures exhibit the predicted shift from novelty to prediction driven gating over training, suggesting alignment with surprise based principles. While operating at reduced capacity, they offer preliminary insights into the compute-accuracy trade-offs of conditional computation. The proposed architectures establish a flexible framework for efficiency, reducing self-attention computation by 75% and KV-cache requirements by 50% within each compute skipping layer, setting a pathway for more efficient models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rigid, uniform allocation of computation in standard Transformer (TF) architectures can limit their efficiency and scalability, particularly for large-scale models and long sequences. Addressing this, we introduce Subjective Depth Transformers (SDT) and Subjective Timescale Transformers (STT), two distinct architectures that leverage Bayesian surprise signals to dynamically route computation, learning where and when to compute within decoder-only TFs. SDT augments a decoder-only stack with alternating Decision and Dynamic layers: a Decision layer computes a full block 'posterior' and a lightweight 'prior,' while a Dynamic layer employs fixed-capacity Top-K routing based on Bayesian surprise (Expected and Unexpected Change), maintaining a static compute graph. STT extends this conditional computation to the temporal domain: a transition network predicts residual updates, forming a temporal 'change hypothesis' that informs a router to dynamically execute or bypass TF blocks for each token, managing KV-cache contributions. Both architectures exhibit the predicted shift from novelty to prediction driven gating over training, suggesting alignment with surprise based principles. While operating at reduced capacity, they offer preliminary insights into the compute-accuracy trade-offs of conditional computation. The proposed architectures establish a flexible framework for efficiency, reducing self-attention computation by 75% and KV-cache requirements by 50% within each compute skipping layer, setting a pathway for more efficient models."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-26T14:00:18Z",
                "published_parsed": [
                    2025,
                    11,
                    26,
                    14,
                    0,
                    18,
                    2,
                    330,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Frederico Wieser"
                    },
                    {
                        "name": "Martin Benfeghoul"
                    },
                    {
                        "name": "Haitham Bou Ammar"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Zafeirios Fountas"
                    }
                ],
                "author_detail": {
                    "name": "Zafeirios Fountas"
                },
                "author": "Zafeirios Fountas"
            },
            {
                "id": "http://arxiv.org/abs/2511.21394v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.21394v1",
                "title": "RIA: A Ranking-Infused Approach for Optimized listwise CTR Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RIA: A Ranking-Infused Approach for Optimized listwise CTR Prediction"
                },
                "updated": "2025-11-26T13:45:10Z",
                "updated_parsed": [
                    2025,
                    11,
                    26,
                    13,
                    45,
                    10,
                    2,
                    330,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.21394v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.21394v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Reranking improves recommendation quality by modeling item interactions. However, existing methods often decouple ranking and reranking, leading to weak listwise evaluation models that suffer from combinatorial sparsity and limited representational power under strict latency constraints. In this paper, we propose RIA (Ranking-Infused Architecture), a unified, end-to-end framework that seamlessly integrates pointwise and listwise evaluation. RIA introduces four key components: (1) the User and Candidate DualTransformer (UCDT) for fine-grained user-item-context modeling; (2) the Context-aware User History and Target (CUHT) module for position-sensitive preference learning; (3) the Listwise Multi-HSTU (LMH) module to capture hierarchical item dependencies; and (4) the Embedding Cache (EC) module to bridge efficiency and effectiveness during inference. By sharing representations across ranking and reranking, RIA enables rich contextual knowledge transfer while maintaining low latency. Extensive experiments show that RIA outperforms state-of-the-art models on both public and industrial datasets, achieving significant gains in AUC and LogLoss. Deployed in Meituan advertising system, RIA yields a +1.69% improvement in Click-Through Rate (CTR) and a +4.54% increase in Cost Per Mille (CPM) in online A/B tests.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reranking improves recommendation quality by modeling item interactions. However, existing methods often decouple ranking and reranking, leading to weak listwise evaluation models that suffer from combinatorial sparsity and limited representational power under strict latency constraints. In this paper, we propose RIA (Ranking-Infused Architecture), a unified, end-to-end framework that seamlessly integrates pointwise and listwise evaluation. RIA introduces four key components: (1) the User and Candidate DualTransformer (UCDT) for fine-grained user-item-context modeling; (2) the Context-aware User History and Target (CUHT) module for position-sensitive preference learning; (3) the Listwise Multi-HSTU (LMH) module to capture hierarchical item dependencies; and (4) the Embedding Cache (EC) module to bridge efficiency and effectiveness during inference. By sharing representations across ranking and reranking, RIA enables rich contextual knowledge transfer while maintaining low latency. Extensive experiments show that RIA outperforms state-of-the-art models on both public and industrial datasets, achieving significant gains in AUC and LogLoss. Deployed in Meituan advertising system, RIA yields a +1.69% improvement in Click-Through Rate (CTR) and a +4.54% increase in Cost Per Mille (CPM) in online A/B tests."
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-26T13:45:10Z",
                "published_parsed": [
                    2025,
                    11,
                    26,
                    13,
                    45,
                    10,
                    2,
                    330,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR"
                },
                "authors": [
                    {
                        "name": "Guoxiao Zhang"
                    },
                    {
                        "name": "Tan Qu"
                    },
                    {
                        "name": "Ao Li"
                    },
                    {
                        "name": "DongLin Ni"
                    },
                    {
                        "name": "Qianlong Xie"
                    },
                    {
                        "name": "Xingxing Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xingxing Wang"
                },
                "author": "Xingxing Wang"
            },
            {
                "id": "http://arxiv.org/abs/2511.21336v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.21336v1",
                "title": "Toward Secure Content-Centric Approaches for 5G-Based IoT: Advances and Emerging Trends",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward Secure Content-Centric Approaches for 5G-Based IoT: Advances and Emerging Trends"
                },
                "updated": "2025-11-26T12:34:53Z",
                "updated_parsed": [
                    2025,
                    11,
                    26,
                    12,
                    34,
                    53,
                    2,
                    330,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.21336v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.21336v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.23919/SoftCOM66362.2025.11197453",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "The convergence of the Internet of Things (IoT) and 5G technologies is transforming modern communication systems by enabling massive connectivity, low latency, and high-speed data transmission. In this evolving landscape, Content-Centric Networking (CCN) is emerging as a promising alternative to traditional Internet Protocol (IP)-based architectures. CCN offers advantages such as in-network caching, scalability, and efficient content dissemination, all of which are particularly well-suited to the constraints of the IoT. However, deploying content-centric approaches in 5G-based IoT environments introduces significant security challenges. Key concerns include content authentication, data integrity, privacy protection, and resilience against attacks such as spoofing and cache poisoning. Such issues are exacerbated by the distributed, mobile, and heterogeneous nature of IoT and 5G systems. In this survey, we review and classify existing security solutions for content-centric architectures in IoT-5G scenarios. We highlight current trends, identify limitations in existing approaches, and outline future research directions with a focus on lightweight and adaptive security mechanisms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The convergence of the Internet of Things (IoT) and 5G technologies is transforming modern communication systems by enabling massive connectivity, low latency, and high-speed data transmission. In this evolving landscape, Content-Centric Networking (CCN) is emerging as a promising alternative to traditional Internet Protocol (IP)-based architectures. CCN offers advantages such as in-network caching, scalability, and efficient content dissemination, all of which are particularly well-suited to the constraints of the IoT. However, deploying content-centric approaches in 5G-based IoT environments introduces significant security challenges. Key concerns include content authentication, data integrity, privacy protection, and resilience against attacks such as spoofing and cache poisoning. Such issues are exacerbated by the distributed, mobile, and heterogeneous nature of IoT and 5G systems. In this survey, we review and classify existing security solutions for content-centric architectures in IoT-5G scenarios. We highlight current trends, identify limitations in existing approaches, and outline future research directions with a focus on lightweight and adaptive security mechanisms."
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-26T12:34:53Z",
                "published_parsed": [
                    2025,
                    11,
                    26,
                    12,
                    34,
                    53,
                    2,
                    330,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI"
                },
                "authors": [
                    {
                        "name": "Ghada Jaber"
                    },
                    {
                        "name": "Mohamed Ali Zormati"
                    },
                    {
                        "name": "Walid Cavelius"
                    },
                    {
                        "name": "Louka Chapiro"
                    },
                    {
                        "name": "Mohamed El Ahmadi"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed El Ahmadi"
                },
                "author": "Mohamed El Ahmadi",
                "arxiv_doi": "10.23919/SoftCOM66362.2025.11197453"
            },
            {
                "id": "http://arxiv.org/abs/2511.21235v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.21235v1",
                "title": "DynamicAdaptiveClimb: Adaptive Cache Replacement with Dynamic Resizing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DynamicAdaptiveClimb: Adaptive Cache Replacement with Dynamic Resizing"
                },
                "updated": "2025-11-26T10:02:24Z",
                "updated_parsed": [
                    2025,
                    11,
                    26,
                    10,
                    2,
                    24,
                    2,
                    330,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.21235v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.21235v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Efficient cache management is critical for optimizing the system performance, and numerous caching mechanisms have been proposed, each exploring various insertion and eviction strategies. In this paper, we present AdaptiveClimb and its extension, DynamicAdaptiveClimb, two novel cache replacement policies that leverage lightweight, cache adaptation to outperform traditional approaches. Unlike classic Least Recently Used (LRU) and Incremental Rank Progress (CLIMB) policies, AdaptiveClimb dynamically adjusts the promotion distance (jump) of the cached objects based on recent hit and miss patterns, requiring only a single tunable parameter and no per-item statistics. This enables rapid adaptation to changing access distributions while maintaining low overhead. Building on this foundation, DynamicAdaptiveClimb further enhances adaptability by automatically tuning the cache size in response to workload demands. Our comprehensive evaluation across a diverse set of real-world traces, including 1067 traces from 6 different datasets, demonstrates that DynamicAdaptiveClimb consistently achieves substantial speedups and higher hit ratios compared to other state-of-the-art algorithms. In particular, our approach achieves up to a 29% improvement in hit ratio and a substantial reduction in miss penalties compared to the FIFO baseline. Furthermore, it outperforms the next-best contenders, AdaptiveClimb and SIEVE [43], by approximately 10% to 15%, especially in environments characterized by fluctuating working set sizes. These results highlight the effectiveness of our approach in delivering efficient performance, making it well-suited for modern, dynamic caching environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient cache management is critical for optimizing the system performance, and numerous caching mechanisms have been proposed, each exploring various insertion and eviction strategies. In this paper, we present AdaptiveClimb and its extension, DynamicAdaptiveClimb, two novel cache replacement policies that leverage lightweight, cache adaptation to outperform traditional approaches. Unlike classic Least Recently Used (LRU) and Incremental Rank Progress (CLIMB) policies, AdaptiveClimb dynamically adjusts the promotion distance (jump) of the cached objects based on recent hit and miss patterns, requiring only a single tunable parameter and no per-item statistics. This enables rapid adaptation to changing access distributions while maintaining low overhead. Building on this foundation, DynamicAdaptiveClimb further enhances adaptability by automatically tuning the cache size in response to workload demands. Our comprehensive evaluation across a diverse set of real-world traces, including 1067 traces from 6 different datasets, demonstrates that DynamicAdaptiveClimb consistently achieves substantial speedups and higher hit ratios compared to other state-of-the-art algorithms. In particular, our approach achieves up to a 29% improvement in hit ratio and a substantial reduction in miss penalties compared to the FIFO baseline. Furthermore, it outperforms the next-best contenders, AdaptiveClimb and SIEVE [43], by approximately 10% to 15%, especially in environments characterized by fluctuating working set sizes. These results highlight the effectiveness of our approach in delivering efficient performance, making it well-suited for modern, dynamic caching environments."
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-26T10:02:24Z",
                "published_parsed": [
                    2025,
                    11,
                    26,
                    10,
                    2,
                    24,
                    2,
                    330,
                    0
                ],
                "arxiv_comment": "19 pages, 11 figures, 3 tables, Patented",
                "arxiv_primary_category": {
                    "term": "cs.OS"
                },
                "authors": [
                    {
                        "name": "Daniel Berend"
                    },
                    {
                        "name": "Shlomi Dolev"
                    },
                    {
                        "name": "Sweta Kumari"
                    },
                    {
                        "name": "Dhruv Mishra"
                    },
                    {
                        "name": "Marina Kogan-Sadetsky"
                    },
                    {
                        "name": "Archit Somani"
                    }
                ],
                "author_detail": {
                    "name": "Archit Somani"
                },
                "author": "Archit Somani"
            },
            {
                "id": "http://arxiv.org/abs/2511.21095v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.21095v1",
                "title": "Generative Early Stage Ranking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Early Stage Ranking"
                },
                "updated": "2025-11-26T06:29:18Z",
                "updated_parsed": [
                    2025,
                    11,
                    26,
                    6,
                    29,
                    18,
                    2,
                    330,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.21095v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.21095v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large-scale recommendations commonly adopt a multi-stage cascading ranking system paradigm to balance effectiveness and efficiency. Early Stage Ranking (ESR) systems utilize the \"user-item decoupling\" approach, where independently learned user and item representations are only combined at the final layer. While efficient, this design is limited in effectiveness, as it struggles to capture fine-grained user-item affinities and cross-signals. To address these, we propose the Generative Early Stage Ranking (GESR) paradigm, introducing the Mixture of Attention (MoA) module which leverages diverse attention mechanisms to bridge the effectiveness gap: the Hard Matching Attention (HMA) module encodes explicit cross-signals by computing raw match counts between user and item features; the Target-Aware Self Attention module generates target-aware user representations conditioned on the item, enabling more personalized learning; and the Cross Attention modules facilitate early and more enriched interactions between user-item features. MoA's specialized attention encodings are further refined in the final layer through a Multi-Logit Parameterized Gating (MLPG) module, which integrates the newly learned embeddings via gating and produces secondary logits that are fused with the primary logit. To address the efficiency and latency challenges, we have introduced a comprehensive suite of optimization techniques. These span from custom kernels that maximize the capabilities of the latest hardware to efficient serving solutions powered by caching mechanisms. The proposed GESR paradigm has shown substantial improvements in topline metrics, engagement, and consumption tasks, as validated by both offline and online experiments. To the best of our knowledge, this marks the first successful deployment of full target-aware attention sequence modeling within an ESR stage at such a scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale recommendations commonly adopt a multi-stage cascading ranking system paradigm to balance effectiveness and efficiency. Early Stage Ranking (ESR) systems utilize the \"user-item decoupling\" approach, where independently learned user and item representations are only combined at the final layer. While efficient, this design is limited in effectiveness, as it struggles to capture fine-grained user-item affinities and cross-signals. To address these, we propose the Generative Early Stage Ranking (GESR) paradigm, introducing the Mixture of Attention (MoA) module which leverages diverse attention mechanisms to bridge the effectiveness gap: the Hard Matching Attention (HMA) module encodes explicit cross-signals by computing raw match counts between user and item features; the Target-Aware Self Attention module generates target-aware user representations conditioned on the item, enabling more personalized learning; and the Cross Attention modules facilitate early and more enriched interactions between user-item features. MoA's specialized attention encodings are further refined in the final layer through a Multi-Logit Parameterized Gating (MLPG) module, which integrates the newly learned embeddings via gating and produces secondary logits that are fused with the primary logit. To address the efficiency and latency challenges, we have introduced a comprehensive suite of optimization techniques. These span from custom kernels that maximize the capabilities of the latest hardware to efficient serving solutions powered by caching mechanisms. The proposed GESR paradigm has shown substantial improvements in topline metrics, engagement, and consumption tasks, as validated by both offline and online experiments. To the best of our knowledge, this marks the first successful deployment of full target-aware attention sequence modeling within an ESR stage at such a scale."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-26T06:29:18Z",
                "published_parsed": [
                    2025,
                    11,
                    26,
                    6,
                    29,
                    18,
                    2,
                    330,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Juhee Hong"
                    },
                    {
                        "name": "Meng Liu"
                    },
                    {
                        "name": "Shengzhi Wang"
                    },
                    {
                        "name": "Xiaoheng Mao"
                    },
                    {
                        "name": "Huihui Cheng"
                    },
                    {
                        "name": "Leon Gao"
                    },
                    {
                        "name": "Christopher Leung"
                    },
                    {
                        "name": "Jin Zhou"
                    },
                    {
                        "name": "Chandra Mouli Sekar"
                    },
                    {
                        "name": "Zhao Zhu"
                    },
                    {
                        "name": "Ruochen Liu"
                    },
                    {
                        "name": "Tuan Trieu"
                    },
                    {
                        "name": "Dawei Sun"
                    },
                    {
                        "name": "Jeet Kanjani"
                    },
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Jing Qian"
                    },
                    {
                        "name": "Xuan Cao"
                    },
                    {
                        "name": "Minjie Fan"
                    },
                    {
                        "name": "Mingze Gao"
                    }
                ],
                "author_detail": {
                    "name": "Mingze Gao"
                },
                "author": "Mingze Gao"
            },
            {
                "id": "http://arxiv.org/abs/2505.01658v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2505.01658v3",
                "title": "A Survey on Inference Engines for Large Language Models: Perspectives on Optimization and Efficiency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Inference Engines for Large Language Models: Perspectives on Optimization and Efficiency"
                },
                "updated": "2025-11-26T05:49:31Z",
                "updated_parsed": [
                    2025,
                    11,
                    26,
                    5,
                    49,
                    31,
                    2,
                    330,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2505.01658v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2505.01658v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) are widely applied in chatbots, code generators, and search engines. Workload such as chain-of-throught, complex reasoning, agent services significantly increase the inference cost by invoke the model repeatedly. Optimization methods such as parallelism, compression, and caching have been adopted to reduce costs, but the diverse service requirements make it hard to select the right method. Recently, specialized LLM inference engines have emerged as a key component for integrating the optimization methods into service-oriented infrastructures. However, a systematic study on inference engines is still lacking.This paper provides a comprehensive evaluation of 25 open-source and commercial inference engines. We examine each inference engine in terms of ease-of-use, ease-of-deployment, general-purpose support, scalability, and suitability for throughput- and latency-aware computation. Furthermore, we explore the design goals of each inference engine by investigating the optimization techniques it supports. In addition, we assess the ecosystem maturity of open source inference engines and handle the performance and cost policy of commercial solutions.We outline future research directions that include support for complex LLM-based services, support of various hardware, and enhanced security, offering practical guidance to researchers and developers in selecting and designing optimized LLM inference engines. We also provide a public repository to continually track developments in this fast-evolving field: \\href{https://github.com/sihyeong/Awesome-LLM-Inference-Engine}{https://github.com/sihyeong/Awesome-LLM-Inference-Engine}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are widely applied in chatbots, code generators, and search engines. Workload such as chain-of-throught, complex reasoning, agent services significantly increase the inference cost by invoke the model repeatedly. Optimization methods such as parallelism, compression, and caching have been adopted to reduce costs, but the diverse service requirements make it hard to select the right method. Recently, specialized LLM inference engines have emerged as a key component for integrating the optimization methods into service-oriented infrastructures. However, a systematic study on inference engines is still lacking.This paper provides a comprehensive evaluation of 25 open-source and commercial inference engines. We examine each inference engine in terms of ease-of-use, ease-of-deployment, general-purpose support, scalability, and suitability for throughput- and latency-aware computation. Furthermore, we explore the design goals of each inference engine by investigating the optimization techniques it supports. In addition, we assess the ecosystem maturity of open source inference engines and handle the performance and cost policy of commercial solutions.We outline future research directions that include support for complex LLM-based services, support of various hardware, and enhanced security, offering practical guidance to researchers and developers in selecting and designing optimized LLM inference engines. We also provide a public repository to continually track developments in this fast-evolving field: \\href{https://github.com/sihyeong/Awesome-LLM-Inference-Engine}{https://github.com/sihyeong/Awesome-LLM-Inference-Engine}."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-05-03T02:47:43Z",
                "published_parsed": [
                    2025,
                    5,
                    3,
                    2,
                    47,
                    43,
                    5,
                    123,
                    0
                ],
                "arxiv_comment": "Under review; 106 pages; 46 figures",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Sihyeong Park"
                    },
                    {
                        "name": "Sungryeol Jeon"
                    },
                    {
                        "name": "Chaelyn Lee"
                    },
                    {
                        "name": "Seokhun Jeon"
                    },
                    {
                        "name": "Byung-Soo Kim"
                    },
                    {
                        "name": "Jemin Lee"
                    }
                ],
                "author_detail": {
                    "name": "Jemin Lee"
                },
                "author": "Jemin Lee"
            },
            {
                "id": "http://arxiv.org/abs/2512.00083v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.00083v1",
                "title": "LLaMCAT: Optimizing Large Language Model Inference with Cache Arbitration and Throttling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLaMCAT: Optimizing Large Language Model Inference with Cache Arbitration and Throttling"
                },
                "updated": "2025-11-26T04:28:23Z",
                "updated_parsed": [
                    2025,
                    11,
                    26,
                    4,
                    28,
                    23,
                    2,
                    330,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.00083v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.00083v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1145/3754598.3754671",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "Large Language Models (LLMs) have achieved unprecedented success across various applications, but their substantial memory requirements pose significant challenges to current memory system designs, especially during inference. Our work targets last-level cache (LLC) based architectures, including GPUs (e.g., NVIDIA GPUs) and AI accelerators. We introduce LLaMCAT, a novel approach to optimize the LLC for LLM inference. LLaMCAT combines Miss Status Holding Register (MSHR)- and load balance-aware cache arbitration with thread throttling to address stringent bandwidth demands and minimize cache stalls in KV Cache access. We also propose a hybrid simulation framework integrating analytical models with cycle-level simulators via memory traces, balancing architecture detail and efficiency.\n  Experiments demonstrate that LLaMCAT achieves an average speedup of 1.26x when the system is mainly bottlenecked by miss handling throughput, while baselines mostly show negative improvements since they are not optimized for this scenario. When the cache size is also limited, our policy achieves a speedup of 1.58x over the unoptimized version, and a 1.26x improvement over the best baseline (dyncta). Overall, LLaMCAT is the first to target LLM decoding-specific MSHR contention, a gap in previous work. It presents a practical solution for accelerating LLM inference on future hardware platforms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved unprecedented success across various applications, but their substantial memory requirements pose significant challenges to current memory system designs, especially during inference. Our work targets last-level cache (LLC) based architectures, including GPUs (e.g., NVIDIA GPUs) and AI accelerators. We introduce LLaMCAT, a novel approach to optimize the LLC for LLM inference. LLaMCAT combines Miss Status Holding Register (MSHR)- and load balance-aware cache arbitration with thread throttling to address stringent bandwidth demands and minimize cache stalls in KV Cache access. We also propose a hybrid simulation framework integrating analytical models with cycle-level simulators via memory traces, balancing architecture detail and efficiency.\n  Experiments demonstrate that LLaMCAT achieves an average speedup of 1.26x when the system is mainly bottlenecked by miss handling throughput, while baselines mostly show negative improvements since they are not optimized for this scenario. When the cache size is also limited, our policy achieves a speedup of 1.58x over the unoptimized version, and a 1.26x improvement over the best baseline (dyncta). Overall, LLaMCAT is the first to target LLM decoding-specific MSHR contention, a gap in previous work. It presents a practical solution for accelerating LLM inference on future hardware platforms."
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-26T04:28:23Z",
                "published_parsed": [
                    2025,
                    11,
                    26,
                    4,
                    28,
                    23,
                    2,
                    330,
                    0
                ],
                "arxiv_comment": "Accepted to ICPP 2025",
                "arxiv_primary_category": {
                    "term": "cs.AR"
                },
                "arxiv_journal_ref": "In Proceedings of 54th International Conference on Parallel Processing (ICPP 2025)",
                "authors": [
                    {
                        "name": "Zhongchun Zhou"
                    },
                    {
                        "name": "Chengtao Lai"
                    },
                    {
                        "name": "Wei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Zhang"
                },
                "author": "Wei Zhang",
                "arxiv_doi": "10.1145/3754598.3754671"
            },
            {
                "id": "http://arxiv.org/abs/2409.17264v5",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2409.17264v5",
                "title": "No Request Left Behind: Tackling Heterogeneity in Long-Context LLM Inference with Medha",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "No Request Left Behind: Tackling Heterogeneity in Long-Context LLM Inference with Medha"
                },
                "updated": "2025-11-26T01:43:40Z",
                "updated_parsed": [
                    2025,
                    11,
                    26,
                    1,
                    43,
                    40,
                    2,
                    330,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2409.17264v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2409.17264v5",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Deploying million-token Large Language Models (LLMs) is challenging because production workloads are highly heterogeneous, mixing short queries and long documents. This heterogeneity, combined with the quadratic complexity of attention, creates severe convoy effects where long-running requests stall short, interactive ones, degrading system responsiveness. We present Medha, a serving system that eliminates these convoys by introducing fine-grained, preemptive scheduling to LLM inference.\n  Medha makes preemption practical with a co-designed set of mechanisms -- including Adaptive Chunking and Stream Pipeline Parallel that overcome the perceived inefficiencies and scaling challenges of chunking. Additionally, we present a new parallelism strategy KV-Cache Parallelism to reduce the decode latency and afford interactivity despite very long context. These mechanisms are orchestrated by a Length-Aware Relative Slack (LARS) scheduler, a deadline and heterogeneity-aware scheduling policy that prevents both the convoy effect and the starvation that plagues simpler policies. Under a heterogeneous workload, Medha improves throughput by 5.7x while reducing median and 99th percentile latency by 30x and 174x, respectively, compared to state-of-the-art non-preemptive systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying million-token Large Language Models (LLMs) is challenging because production workloads are highly heterogeneous, mixing short queries and long documents. This heterogeneity, combined with the quadratic complexity of attention, creates severe convoy effects where long-running requests stall short, interactive ones, degrading system responsiveness. We present Medha, a serving system that eliminates these convoys by introducing fine-grained, preemptive scheduling to LLM inference.\n  Medha makes preemption practical with a co-designed set of mechanisms -- including Adaptive Chunking and Stream Pipeline Parallel that overcome the perceived inefficiencies and scaling challenges of chunking. Additionally, we present a new parallelism strategy KV-Cache Parallelism to reduce the decode latency and afford interactivity despite very long context. These mechanisms are orchestrated by a Length-Aware Relative Slack (LARS) scheduler, a deadline and heterogeneity-aware scheduling policy that prevents both the convoy effect and the starvation that plagues simpler policies. Under a heterogeneous workload, Medha improves throughput by 5.7x while reducing median and 99th percentile latency by 30x and 174x, respectively, compared to state-of-the-art non-preemptive systems."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-09-25T18:21:05Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    18,
                    21,
                    5,
                    2,
                    269,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Amey Agrawal"
                    },
                    {
                        "name": "Haoran Qiu"
                    },
                    {
                        "name": "Junda Chen"
                    },
                    {
                        "name": "igo Goiri"
                    },
                    {
                        "name": "Chaojie Zhang"
                    },
                    {
                        "name": "Rayyan Shahid"
                    },
                    {
                        "name": "Ramachandran Ramjee"
                    },
                    {
                        "name": "Alexey Tumanov"
                    },
                    {
                        "name": "Esha Choukse"
                    }
                ],
                "author_detail": {
                    "name": "Esha Choukse"
                },
                "author": "Esha Choukse"
            },
            {
                "id": "http://arxiv.org/abs/2205.02066v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2205.02066v3",
                "title": "Biembeddings of Archdeacon type: their full automorphism group and their number",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Biembeddings of Archdeacon type: their full automorphism group and their number"
                },
                "updated": "2025-11-25T21:02:20Z",
                "updated_parsed": [
                    2025,
                    11,
                    25,
                    21,
                    2,
                    20,
                    1,
                    329,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2205.02066v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2205.02066v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Archdeacon, in his seminal paper $[1]$, defined the concept of Heffter array in order to provide explicit constructions of $\\mathbb{Z}_{v}$-regular biembeddings of complete graphs $K_v$ into orientable surfaces.\n  In this paper, we first introduce the quasi-Heffter arrays as a generalization of the concept of Heffer array and we show that, in this context, we can define a $2$-colorable embedding of Archdeacon type of the complete multipartite graph $K_{\\frac{v}{t}\\times t}$ into an orientable surface. Then, our main goal is to study the full automorphism groups of these embeddings: here we are able to prove, using a probabilistic approach, that, almost always, this group is exactly $\\mathbb{Z}_{v}$.\n  As an application of this result, given a positive integer $t\\not\\equiv 0\\pmod{4}$, we prove that there are, for infinitely many pairs of $v$ and $k$, at least $(1-o(1)) \\frac{(\\frac{v-t}{2})!}{(v)} $ non-isomorphic biembeddings of $K_{\\frac{v}{t}\\times t}$ whose face lengths are multiples of $k$. Here $(\\cdot)$ denotes the Euler's totient function. Moreover, in case $t=1$ and $v$ is a prime, almost all these embeddings define faces that are all of the same length $kv$, i.e. we have a more than exponential number of non-isomorphic $kv$-gonal biembeddings of $K_{v}$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Archdeacon, in his seminal paper $[1]$, defined the concept of Heffter array in order to provide explicit constructions of $\\mathbb{Z}_{v}$-regular biembeddings of complete graphs $K_v$ into orientable surfaces.\n  In this paper, we first introduce the quasi-Heffter arrays as a generalization of the concept of Heffer array and we show that, in this context, we can define a $2$-colorable embedding of Archdeacon type of the complete multipartite graph $K_{\\frac{v}{t}\\times t}$ into an orientable surface. Then, our main goal is to study the full automorphism groups of these embeddings: here we are able to prove, using a probabilistic approach, that, almost always, this group is exactly $\\mathbb{Z}_{v}$.\n  As an application of this result, given a positive integer $t\\not\\equiv 0\\pmod{4}$, we prove that there are, for infinitely many pairs of $v$ and $k$, at least $(1-o(1)) \\frac{(\\frac{v-t}{2})!}{(v)} $ non-isomorphic biembeddings of $K_{\\frac{v}{t}\\times t}$ whose face lengths are multiples of $k$. Here $(\\cdot)$ denotes the Euler's totient function. Moreover, in case $t=1$ and $v$ is a prime, almost all these embeddings define faces that are all of the same length $kv$, i.e. we have a more than exponential number of non-isomorphic $kv$-gonal biembeddings of $K_{v}$."
                },
                "tags": [
                    {
                        "term": "math.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2022-05-04T13:55:00Z",
                "published_parsed": [
                    2022,
                    5,
                    4,
                    13,
                    55,
                    0,
                    2,
                    124,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "math.CO"
                },
                "authors": [
                    {
                        "name": "Simone Costa"
                    }
                ],
                "author_detail": {
                    "name": "Simone Costa"
                },
                "author": "Simone Costa"
            },
            {
                "id": "http://arxiv.org/abs/2511.20849v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.20849v1",
                "title": "Length-MAX Tokenizer for Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Length-MAX Tokenizer for Language Models"
                },
                "updated": "2025-11-25T20:56:56Z",
                "updated_parsed": [
                    2025,
                    11,
                    25,
                    20,
                    56,
                    56,
                    1,
                    329,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.20849v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.20849v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We introduce a new tokenizer for language models that minimizes the average tokens per character, thereby reducing the number of tokens needed to represent text during training and to generate text during inference. Our method, which we refer to as the Length-MAX tokenizer, obtains its vocabulary by casting a length-weighted objective maximization as a graph partitioning problem and developing a greedy approximation algorithm. On FineWeb and diverse domains, it yields 14--18\\% fewer tokens than Byte Pair Encoding (BPE) across vocabulary sizes from 10K to 50K, and the reduction is 13.0\\% when the size is 64K. Training GPT-2 models at 124M, 355M, and 1.3B parameters from scratch with five runs each shows 18.5\\%, 17.2\\%, and 18.5\\% fewer steps, respectively, to reach a fixed validation loss, and 13.7\\%, 12.7\\%, and 13.7\\% lower inference latency, together with a 16\\% throughput gain at 124M, while consistently improving on downstream tasks including reducing LAMBADA perplexity by 11.7\\% and enhancing HellaSwag accuracy by 4.3\\%. Moreover, the Length-MAX tokenizer achieves 99.62\\% vocabulary coverage and the out-of-vocabulary rate remains low at 0.12\\% on test sets. These results demonstrate that optimizing for average token length, rather than frequency alone, offers an effective approach to more efficient language modeling without sacrificing -- and often improving -- downstream performance. The tokenizer is compatible with production systems and reduces embedding and KV-cache memory by 18\\% at inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a new tokenizer for language models that minimizes the average tokens per character, thereby reducing the number of tokens needed to represent text during training and to generate text during inference. Our method, which we refer to as the Length-MAX tokenizer, obtains its vocabulary by casting a length-weighted objective maximization as a graph partitioning problem and developing a greedy approximation algorithm. On FineWeb and diverse domains, it yields 14--18\\% fewer tokens than Byte Pair Encoding (BPE) across vocabulary sizes from 10K to 50K, and the reduction is 13.0\\% when the size is 64K. Training GPT-2 models at 124M, 355M, and 1.3B parameters from scratch with five runs each shows 18.5\\%, 17.2\\%, and 18.5\\% fewer steps, respectively, to reach a fixed validation loss, and 13.7\\%, 12.7\\%, and 13.7\\% lower inference latency, together with a 16\\% throughput gain at 124M, while consistently improving on downstream tasks including reducing LAMBADA perplexity by 11.7\\% and enhancing HellaSwag accuracy by 4.3\\%. Moreover, the Length-MAX tokenizer achieves 99.62\\% vocabulary coverage and the out-of-vocabulary rate remains low at 0.12\\% on test sets. These results demonstrate that optimizing for average token length, rather than frequency alone, offers an effective approach to more efficient language modeling without sacrificing -- and often improving -- downstream performance. The tokenizer is compatible with production systems and reduces embedding and KV-cache memory by 18\\% at inference."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-25T20:56:56Z",
                "published_parsed": [
                    2025,
                    11,
                    25,
                    20,
                    56,
                    56,
                    1,
                    329,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Dong Dong"
                    },
                    {
                        "name": "Weijie Su"
                    }
                ],
                "author_detail": {
                    "name": "Weijie Su"
                },
                "author": "Weijie Su"
            },
            {
                "id": "http://arxiv.org/abs/2511.20649v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.20649v1",
                "title": "Infinity-RoPE: Action-Controllable Infinite Video Generation Emerges From Autoregressive Self-Rollout",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Infinity-RoPE: Action-Controllable Infinite Video Generation Emerges From Autoregressive Self-Rollout"
                },
                "updated": "2025-11-25T18:59:46Z",
                "updated_parsed": [
                    2025,
                    11,
                    25,
                    18,
                    59,
                    46,
                    1,
                    329,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.20649v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.20649v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Current autoregressive video diffusion models are constrained by three core bottlenecks: (i) the finite temporal horizon imposed by the base model's 3D Rotary Positional Embedding (3D-RoPE), (ii) slow prompt responsiveness in maintaining fine-grained action control during long-form rollouts, and (iii) the inability to realize discontinuous cinematic transitions within a single generation stream. We introduce $\\infty$-RoPE, a unified inference-time framework that addresses all three limitations through three interconnected components: Block-Relativistic RoPE, KV Flush, and RoPE Cut. Block-Relativistic RoPE reformulates temporal encoding as a moving local reference frame, where each newly generated latent block is rotated relative to the base model's maximum frame horizon while earlier blocks are rotated backward to preserve relative temporal geometry. This relativistic formulation eliminates fixed temporal positions, enabling continuous video generation far beyond the base positional limits. To obtain fine-grained action control without re-encoding, KV Flush renews the KV cache by retaining only two latent frames, the global sink and the last generated latent frame, thereby ensuring immediate prompt responsiveness. Finally, RoPE Cut introduces controlled discontinuities in temporal RoPE coordinates, enabling multi-cut scene transitions within a single continuous rollout. Together, these components establish $\\infty$-RoPE as a training-free foundation for infinite-horizon, controllable, and cinematic video diffusion. Comprehensive experiments show that $\\infty$-RoPE consistently surpasses previous autoregressive models in overall VBench scores.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current autoregressive video diffusion models are constrained by three core bottlenecks: (i) the finite temporal horizon imposed by the base model's 3D Rotary Positional Embedding (3D-RoPE), (ii) slow prompt responsiveness in maintaining fine-grained action control during long-form rollouts, and (iii) the inability to realize discontinuous cinematic transitions within a single generation stream. We introduce $\\infty$-RoPE, a unified inference-time framework that addresses all three limitations through three interconnected components: Block-Relativistic RoPE, KV Flush, and RoPE Cut. Block-Relativistic RoPE reformulates temporal encoding as a moving local reference frame, where each newly generated latent block is rotated relative to the base model's maximum frame horizon while earlier blocks are rotated backward to preserve relative temporal geometry. This relativistic formulation eliminates fixed temporal positions, enabling continuous video generation far beyond the base positional limits. To obtain fine-grained action control without re-encoding, KV Flush renews the KV cache by retaining only two latent frames, the global sink and the last generated latent frame, thereby ensuring immediate prompt responsiveness. Finally, RoPE Cut introduces controlled discontinuities in temporal RoPE coordinates, enabling multi-cut scene transitions within a single continuous rollout. Together, these components establish $\\infty$-RoPE as a training-free foundation for infinite-horizon, controllable, and cinematic video diffusion. Comprehensive experiments show that $\\infty$-RoPE consistently surpasses previous autoregressive models in overall VBench scores."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-25T18:59:46Z",
                "published_parsed": [
                    2025,
                    11,
                    25,
                    18,
                    59,
                    46,
                    1,
                    329,
                    0
                ],
                "arxiv_comment": "Project Page: https://infinity-rope.github.io/",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Hidir Yesiltepe"
                    },
                    {
                        "name": "Tuna Han Salih Meral"
                    },
                    {
                        "name": "Adil Kaan Akan"
                    },
                    {
                        "name": "Kaan Oktay"
                    },
                    {
                        "name": "Pinar Yanardag"
                    }
                ],
                "author_detail": {
                    "name": "Pinar Yanardag"
                },
                "author": "Pinar Yanardag"
            },
            {
                "id": "http://arxiv.org/abs/2511.20426v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.20426v1",
                "title": "Block Cascading: Training Free Acceleration of Block-Causal Video Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Block Cascading: Training Free Acceleration of Block-Causal Video Models"
                },
                "updated": "2025-11-25T15:52:58Z",
                "updated_parsed": [
                    2025,
                    11,
                    25,
                    15,
                    52,
                    58,
                    1,
                    329,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.20426v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.20426v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Block-causal video generation faces a stark speed-quality trade-off: small 1.3B models manage only 16 FPS while large 14B models crawl at 4.5 FPS, forcing users to choose between responsiveness and quality. Block Cascading significantly mitigates this trade-off through training-free parallelization. Our key insight: future video blocks do not need fully denoised current blocks to begin generation. By starting block generation with partially denoised context from predecessors, we transform sequential pipelines into parallel cascades where multiple blocks denoise simultaneously. With 5 GPUs exploiting temporal parallelism, we achieve ~2x acceleration across all model scales: 1.3B models accelerate from 16 to 30 FPS, 14B models from 4.5 to 12.5 FPS. Beyond inference speed, Block Cascading eliminates overhead from KV-recaching (of ~200ms) during context switches for interactive generation. Extensive evaluations validated against multiple block-causal pipelines demonstrate no significant loss in generation quality when switching from block-causal to Block Cascading pipelines for inference. Project Page: https://hmrishavbandy.github.io/block_cascading_page/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Block-causal video generation faces a stark speed-quality trade-off: small 1.3B models manage only 16 FPS while large 14B models crawl at 4.5 FPS, forcing users to choose between responsiveness and quality. Block Cascading significantly mitigates this trade-off through training-free parallelization. Our key insight: future video blocks do not need fully denoised current blocks to begin generation. By starting block generation with partially denoised context from predecessors, we transform sequential pipelines into parallel cascades where multiple blocks denoise simultaneously. With 5 GPUs exploiting temporal parallelism, we achieve ~2x acceleration across all model scales: 1.3B models accelerate from 16 to 30 FPS, 14B models from 4.5 to 12.5 FPS. Beyond inference speed, Block Cascading eliminates overhead from KV-recaching (of ~200ms) during context switches for interactive generation. Extensive evaluations validated against multiple block-causal pipelines demonstrate no significant loss in generation quality when switching from block-causal to Block Cascading pipelines for inference. Project Page: https://hmrishavbandy.github.io/block_cascading_page/"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-25T15:52:58Z",
                "published_parsed": [
                    2025,
                    11,
                    25,
                    15,
                    52,
                    58,
                    1,
                    329,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Hmrishav Bandyopadhyay"
                    },
                    {
                        "name": "Nikhil Pinnaparaju"
                    },
                    {
                        "name": "Rahim Entezari"
                    },
                    {
                        "name": "Jim Scott"
                    },
                    {
                        "name": "Yi-Zhe Song"
                    },
                    {
                        "name": "Varun Jampani"
                    }
                ],
                "author_detail": {
                    "name": "Varun Jampani"
                },
                "author": "Varun Jampani"
            },
            {
                "id": "http://arxiv.org/abs/2511.19973v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.19973v1",
                "title": "Pickle Prefetcher: Programmable and Scalable Last-Level Cache Prefetcher",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pickle Prefetcher: Programmable and Scalable Last-Level Cache Prefetcher"
                },
                "updated": "2025-11-25T06:36:32Z",
                "updated_parsed": [
                    2025,
                    11,
                    25,
                    6,
                    36,
                    32,
                    1,
                    329,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.19973v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.19973v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Modern high-performance architectures employ large last-level caches (LLCs). While large LLCs can reduce average memory access latency for workloads with a high degree of locality, they can also increase latency for workloads with irregular memory access patterns. Prefetchers are widely used to reduce memory latency by prefetching data into the cache hierarchy before it is accessed by the core. However, existing prediction-based prefetchers often struggle with irregular memory access patterns, which are especially prevalent in modern applications. This paper introduces the Pickle Prefetcher, a programmable and scalable LLC prefetcher designed to handle independent irregular memory access patterns effectively. Instead of relying on static heuristics or complex prediction algorithms, Pickle Prefetcher allows software to define its own prefetching strategies using a simple programming interface without expanding the instruction set architecture (ISA). By trading the logic complexity of hardware prediction for software programmability, Pickle Prefetcher can adapt to a wide range of access patterns without requiring extensive hardware resources for prediction. This allows the prefetcher to dedicate its resources to scheduling and issuing timely prefetch requests. Graph applications are an example where the memory access pattern is irregular but easily predictable by software. Through extensive evaluations of the Pickle Prefetcher on gem5 full-system simulations, we demonstrate tha Pickle Prefetcher significantly outperforms traditional prefetching techniques. Our results show that Pickle Prefetcher achieves speedups of up to 1.74x on the GAPBS breadth-first search (BFS) implementation over a baseline system. When combined with private cache prefetchers, Pickle Prefetcher provides up to a 1.40x speedup over systems using only private cache prefetchers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern high-performance architectures employ large last-level caches (LLCs). While large LLCs can reduce average memory access latency for workloads with a high degree of locality, they can also increase latency for workloads with irregular memory access patterns. Prefetchers are widely used to reduce memory latency by prefetching data into the cache hierarchy before it is accessed by the core. However, existing prediction-based prefetchers often struggle with irregular memory access patterns, which are especially prevalent in modern applications. This paper introduces the Pickle Prefetcher, a programmable and scalable LLC prefetcher designed to handle independent irregular memory access patterns effectively. Instead of relying on static heuristics or complex prediction algorithms, Pickle Prefetcher allows software to define its own prefetching strategies using a simple programming interface without expanding the instruction set architecture (ISA). By trading the logic complexity of hardware prediction for software programmability, Pickle Prefetcher can adapt to a wide range of access patterns without requiring extensive hardware resources for prediction. This allows the prefetcher to dedicate its resources to scheduling and issuing timely prefetch requests. Graph applications are an example where the memory access pattern is irregular but easily predictable by software. Through extensive evaluations of the Pickle Prefetcher on gem5 full-system simulations, we demonstrate tha Pickle Prefetcher significantly outperforms traditional prefetching techniques. Our results show that Pickle Prefetcher achieves speedups of up to 1.74x on the GAPBS breadth-first search (BFS) implementation over a baseline system. When combined with private cache prefetchers, Pickle Prefetcher provides up to a 1.40x speedup over systems using only private cache prefetchers."
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-25T06:36:32Z",
                "published_parsed": [
                    2025,
                    11,
                    25,
                    6,
                    36,
                    32,
                    1,
                    329,
                    0
                ],
                "arxiv_comment": "13 pages, 13 figures",
                "arxiv_primary_category": {
                    "term": "cs.AR"
                },
                "authors": [
                    {
                        "name": "Hoa Nguyen"
                    },
                    {
                        "name": "Pongstorn Maidee"
                    },
                    {
                        "name": "Jason Lowe-Power"
                    },
                    {
                        "name": "Alireza Kaviani"
                    }
                ],
                "author_detail": {
                    "name": "Alireza Kaviani"
                },
                "author": "Alireza Kaviani"
            },
            {
                "id": "http://arxiv.org/abs/2402.04032v6",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2402.04032v6",
                "title": "ProactivePIM: Accelerating Weight-Sharing Embedding Layer with PIM for Scalable Recommendation System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProactivePIM: Accelerating Weight-Sharing Embedding Layer with PIM for Scalable Recommendation System"
                },
                "updated": "2025-11-25T05:43:27Z",
                "updated_parsed": [
                    2025,
                    11,
                    25,
                    5,
                    43,
                    27,
                    1,
                    329,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2402.04032v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2402.04032v6",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Although deep learning-based personalized recommendation systems provide qualified recommendations, they strain data center resources. The main bottleneck is the embedding layer, which is highly memory-intensive due to its sparse, irregular access patterns to embeddings. Recent near-memory processing (NMP) and processing-in-memory (PIM) architectures have addressed these issues by exploiting parallelism within memory. However, as model sizes increase year by year and can exceed server capacity, inference on single-node servers becomes challenging, necessitating the integration of model compression. Various algorithms have been proposed for model size reduction, but they come at the cost of increased memory access and CPU-PIM communication. We present ProactivePIM, a PIM system tailored for weight-sharing algorithms, a family of compression methods that decompose an embedding table into compact subtables, such as QR-trick and TT-Rec. Our analysis shows that embedding layer execution with weight-sharing algorithms increases memory access and incurs CPU-PIM communication. We also find that these algorithms exhibit unique data locality characteristics, which we name intra-GnR locality. ProactivePIM accelerates weight-sharing algorithms by utilizing a heterogeneous HBM-DIMM memory architecture with integration of a two-level PIM system of base-die PIM (bd-PIM) and bank-group PIM (bg-PIM) inside the HBM. To gain further speedup, ProactivePIM prefetches embeddings with high intra-GnR locality into an SRAM cache within bg-PIM and eliminates the CPU-PIM communication through duplication of target subtables across bank groups. With additional optimization techniques, our design effectively accelerates weight-sharing algorithms, achieving 2.22x and 2.15x speedup in QR-trick and TT-Rec, respectively, compared to the baseline architecture.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although deep learning-based personalized recommendation systems provide qualified recommendations, they strain data center resources. The main bottleneck is the embedding layer, which is highly memory-intensive due to its sparse, irregular access patterns to embeddings. Recent near-memory processing (NMP) and processing-in-memory (PIM) architectures have addressed these issues by exploiting parallelism within memory. However, as model sizes increase year by year and can exceed server capacity, inference on single-node servers becomes challenging, necessitating the integration of model compression. Various algorithms have been proposed for model size reduction, but they come at the cost of increased memory access and CPU-PIM communication. We present ProactivePIM, a PIM system tailored for weight-sharing algorithms, a family of compression methods that decompose an embedding table into compact subtables, such as QR-trick and TT-Rec. Our analysis shows that embedding layer execution with weight-sharing algorithms increases memory access and incurs CPU-PIM communication. We also find that these algorithms exhibit unique data locality characteristics, which we name intra-GnR locality. ProactivePIM accelerates weight-sharing algorithms by utilizing a heterogeneous HBM-DIMM memory architecture with integration of a two-level PIM system of base-die PIM (bd-PIM) and bank-group PIM (bg-PIM) inside the HBM. To gain further speedup, ProactivePIM prefetches embeddings with high intra-GnR locality into an SRAM cache within bg-PIM and eliminates the CPU-PIM communication through duplication of target subtables across bank groups. With additional optimization techniques, our design effectively accelerates weight-sharing algorithms, achieving 2.22x and 2.15x speedup in QR-trick and TT-Rec, respectively, compared to the baseline architecture."
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-02-06T14:26:22Z",
                "published_parsed": [
                    2024,
                    2,
                    6,
                    14,
                    26,
                    22,
                    1,
                    37,
                    0
                ],
                "arxiv_comment": "14 pages, 13 figures",
                "arxiv_primary_category": {
                    "term": "cs.AR"
                },
                "authors": [
                    {
                        "name": "Youngsuk Kim"
                    },
                    {
                        "name": "Junghwan Lim"
                    },
                    {
                        "name": "Hyuk-Jae Lee"
                    },
                    {
                        "name": "Chae Eun Rhee"
                    }
                ],
                "author_detail": {
                    "name": "Chae Eun Rhee"
                },
                "author": "Chae Eun Rhee"
            },
            {
                "id": "http://arxiv.org/abs/2504.09775v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2504.09775v4",
                "title": "Understanding and Optimizing Multi-Stage AI Inference Pipelines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding and Optimizing Multi-Stage AI Inference Pipelines"
                },
                "updated": "2025-11-25T04:36:10Z",
                "updated_parsed": [
                    2025,
                    11,
                    25,
                    4,
                    36,
                    10,
                    1,
                    329,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2504.09775v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2504.09775v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The rapid evolution of Large Language Models (LLMs) has driven the need for increasingly sophisticated inference pipelines and hardware platforms. Modern LLM serving extends beyond traditional prefill-decode workflows, incorporating multi-stage processes such as Retrieval Augmented Generation (RAG), key-value (KV) cache retrieval, dynamic model routing, and multi step reasoning. These stages exhibit diverse computational demands, requiring distributed systems that integrate GPUs, ASICs, CPUs, and memory-centric architectures. However, existing simulators lack the fidelity to model these heterogeneous, multi-engine workflows, limiting their ability to inform architectural decisions.\n  To address this gap, we introduce HERMES, a Heterogeneous Multi-stage LLM inference Execution Simulator. HERMES models diverse request stages; including RAG, KV retrieval, reasoning, prefill, and decode across complex hardware hierarchies. HERMES supports heterogeneous clients executing multiple models concurrently unlike prior frameworks while incorporating advanced batching strategies and multi-level memory hierarchies. By integrating real hardware traces with analytical modeling, HERMES captures critical trade-offs such as memory bandwidth contention, inter-cluster communication latency, and batching efficiency in hybrid CPU-accelerator deployments. Through case studies, we explore the impact of reasoning stages on end-to-end latency, optimal batching strategies for hybrid pipelines, and the architectural implications of remote KV cache retrieval. HERMES empowers system designers to navigate the evolving landscape of LLM inference, providing actionable insights into optimizing hardware-software co-design for next-generation AI workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of Large Language Models (LLMs) has driven the need for increasingly sophisticated inference pipelines and hardware platforms. Modern LLM serving extends beyond traditional prefill-decode workflows, incorporating multi-stage processes such as Retrieval Augmented Generation (RAG), key-value (KV) cache retrieval, dynamic model routing, and multi step reasoning. These stages exhibit diverse computational demands, requiring distributed systems that integrate GPUs, ASICs, CPUs, and memory-centric architectures. However, existing simulators lack the fidelity to model these heterogeneous, multi-engine workflows, limiting their ability to inform architectural decisions.\n  To address this gap, we introduce HERMES, a Heterogeneous Multi-stage LLM inference Execution Simulator. HERMES models diverse request stages; including RAG, KV retrieval, reasoning, prefill, and decode across complex hardware hierarchies. HERMES supports heterogeneous clients executing multiple models concurrently unlike prior frameworks while incorporating advanced batching strategies and multi-level memory hierarchies. By integrating real hardware traces with analytical modeling, HERMES captures critical trade-offs such as memory bandwidth contention, inter-cluster communication latency, and batching efficiency in hybrid CPU-accelerator deployments. Through case studies, we explore the impact of reasoning stages on end-to-end latency, optimal batching strategies for hybrid pipelines, and the architectural implications of remote KV cache retrieval. HERMES empowers system designers to navigate the evolving landscape of LLM inference, providing actionable insights into optimizing hardware-software co-design for next-generation AI workloads."
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-04-14T00:29:49Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    0,
                    29,
                    49,
                    0,
                    104,
                    0
                ],
                "arxiv_comment": "Inference System Design for Multi-Stage AI Inference Pipelines. 13 Pages, 15 Figues, 3 Tables",
                "arxiv_primary_category": {
                    "term": "cs.AR"
                },
                "authors": [
                    {
                        "name": "Abhimanyu Rajeshkumar Bambhaniya"
                    },
                    {
                        "name": "Hanjiang Wu"
                    },
                    {
                        "name": "Suvinay Subramanian"
                    },
                    {
                        "name": "Sudarshan Srinivasan"
                    },
                    {
                        "name": "Souvik Kundu"
                    },
                    {
                        "name": "Amir Yazdanbakhsh"
                    },
                    {
                        "name": "Midhilesh Elavazhagan"
                    },
                    {
                        "name": "Madhu Kumar"
                    },
                    {
                        "name": "Tushar Krishna"
                    }
                ],
                "author_detail": {
                    "name": "Tushar Krishna"
                },
                "author": "Tushar Krishna"
            },
            {
                "id": "http://arxiv.org/abs/2511.16138v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.16138v2",
                "title": "On 10x Better Scalability: KV Stores Scale Up KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On 10x Better Scalability: KV Stores Scale Up KV Cache"
                },
                "updated": "2025-11-25T02:03:54Z",
                "updated_parsed": [
                    2025,
                    11,
                    25,
                    2,
                    3,
                    54,
                    1,
                    329,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.16138v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.16138v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) rely on Key-Value (KV) cache to reduce time-to-first-token (TTFT) latency, but existing disk-based KV cache systems using file-per-object layouts suffer from severe scalability bottlenecks due to file system metadata overhead, I/O inefficiency, and poor spatial locality. This paper presents SGLANG-LSM, a database-inspired system that leverages Log-Structured Merge-tree (LSM-tree) architectures for scalable KV cache management. SGLANG-LSM implements a layered system design with three coordinated components: (1) a prefix-preserving storage engine that maintains token sequence locality while efficiently storing large KV cache tensors through key-value separation, (2) an adaptive controller that dynamically optimizes LSM-tree configurations based on shifting workload characteristics, and (3) runtime services including batch operations and automatic resource management for production deployment. Evaluation on large-scale dynamic workloads demonstrates that SGLANG-LSM significantly improves cache hits by up to 143% and reduces TTFT by up to 24% compared to state-of-the-art systems, representing the first systematic application of database storage architectures to large-scale LLM cache management.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) rely on Key-Value (KV) cache to reduce time-to-first-token (TTFT) latency, but existing disk-based KV cache systems using file-per-object layouts suffer from severe scalability bottlenecks due to file system metadata overhead, I/O inefficiency, and poor spatial locality. This paper presents SGLANG-LSM, a database-inspired system that leverages Log-Structured Merge-tree (LSM-tree) architectures for scalable KV cache management. SGLANG-LSM implements a layered system design with three coordinated components: (1) a prefix-preserving storage engine that maintains token sequence locality while efficiently storing large KV cache tensors through key-value separation, (2) an adaptive controller that dynamically optimizes LSM-tree configurations based on shifting workload characteristics, and (3) runtime services including batch operations and automatic resource management for production deployment. Evaluation on large-scale dynamic workloads demonstrates that SGLANG-LSM significantly improves cache hits by up to 143% and reduces TTFT by up to 24% compared to state-of-the-art systems, representing the first systematic application of database storage architectures to large-scale LLM cache management."
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-20T08:22:36Z",
                "published_parsed": [
                    2025,
                    11,
                    20,
                    8,
                    22,
                    36,
                    3,
                    324,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB"
                },
                "authors": [
                    {
                        "name": "Weiping Yu"
                    },
                    {
                        "name": "Ye Jiarui"
                    },
                    {
                        "name": "He Mengke"
                    },
                    {
                        "name": "Junfeng Liu"
                    },
                    {
                        "name": "Siqiang Luo"
                    }
                ],
                "author_detail": {
                    "name": "Siqiang Luo"
                },
                "author": "Siqiang Luo"
            },
            {
                "id": "http://arxiv.org/abs/2511.20714v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.20714v1",
                "title": "Inferix: A Block-Diffusion based Next-Generation Inference Engine for World Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferix: A Block-Diffusion based Next-Generation Inference Engine for World Simulation"
                },
                "updated": "2025-11-25T01:45:04Z",
                "updated_parsed": [
                    2025,
                    11,
                    25,
                    1,
                    45,
                    4,
                    1,
                    329,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.20714v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.20714v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "World models serve as core simulators for fields such as agentic AI, embodied AI, and gaming, capable of generating long, physically realistic, and interactive high-quality videos. Moreover, scaling these models could unlock emergent capabilities in visual perception, understanding, and reasoning, paving the way for a new paradigm that moves beyond current LLM-centric vision foundation models. A key breakthrough empowering them is the semi-autoregressive (block-diffusion) decoding paradigm, which merges the strengths of diffusion and autoregressive methods by generating video tokens in block-applying diffusion within each block while conditioning on previous ones, resulting in more coherent and stable video sequences. Crucially, it overcomes limitations of standard video diffusion by reintroducing LLM-style KV Cache management, enabling efficient, variable-length, and high-quality generation.\n  Therefore, Inferix is specifically designed as a next-generation inference engine to enable immersive world synthesis through optimized semi-autoregressive decoding processes. This dedicated focus on world simulation distinctly sets it apart from systems engineered for high-concurrency scenarios (like vLLM or SGLang) and from classic video diffusion models (such as xDiTs). Inferix further enhances its offering with interactive video streaming and profiling, enabling real-time interaction and realistic simulation to accurately model world dynamics. Additionally, it supports efficient benchmarking through seamless integration of LV-Bench, a new fine-grained evaluation benchmark tailored for minute-long video generation scenarios. We hope the community will work together to advance Inferix and foster world model exploration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "World models serve as core simulators for fields such as agentic AI, embodied AI, and gaming, capable of generating long, physically realistic, and interactive high-quality videos. Moreover, scaling these models could unlock emergent capabilities in visual perception, understanding, and reasoning, paving the way for a new paradigm that moves beyond current LLM-centric vision foundation models. A key breakthrough empowering them is the semi-autoregressive (block-diffusion) decoding paradigm, which merges the strengths of diffusion and autoregressive methods by generating video tokens in block-applying diffusion within each block while conditioning on previous ones, resulting in more coherent and stable video sequences. Crucially, it overcomes limitations of standard video diffusion by reintroducing LLM-style KV Cache management, enabling efficient, variable-length, and high-quality generation.\n  Therefore, Inferix is specifically designed as a next-generation inference engine to enable immersive world synthesis through optimized semi-autoregressive decoding processes. This dedicated focus on world simulation distinctly sets it apart from systems engineered for high-concurrency scenarios (like vLLM or SGLang) and from classic video diffusion models (such as xDiTs). Inferix further enhances its offering with interactive video streaming and profiling, enabling real-time interaction and realistic simulation to accurately model world dynamics. Additionally, it supports efficient benchmarking through seamless integration of LV-Bench, a new fine-grained evaluation benchmark tailored for minute-long video generation scenarios. We hope the community will work together to advance Inferix and foster world model exploration."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-25T01:45:04Z",
                "published_parsed": [
                    2025,
                    11,
                    25,
                    1,
                    45,
                    4,
                    1,
                    329,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Inferix Team"
                    },
                    {
                        "name": "Tianyu Feng"
                    },
                    {
                        "name": "Yizeng Han"
                    },
                    {
                        "name": "Jiahao He"
                    },
                    {
                        "name": "Yuanyu He"
                    },
                    {
                        "name": "Xi Lin"
                    },
                    {
                        "name": "Teng Liu"
                    },
                    {
                        "name": "Hanfeng Lu"
                    },
                    {
                        "name": "Jiasheng Tang"
                    },
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Zhiyuan Wang"
                    },
                    {
                        "name": "Jichao Wu"
                    },
                    {
                        "name": "Mingyang Yang"
                    },
                    {
                        "name": "Yinghao Yu"
                    },
                    {
                        "name": "Zeyu Zhang"
                    },
                    {
                        "name": "Bohan Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Bohan Zhuang"
                },
                "author": "Bohan Zhuang"
            },
            {
                "id": "http://arxiv.org/abs/2511.19639v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.19639v1",
                "title": "Computer-aided Characterization of Fundamental Limits of Coded Caching with Linear Coding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computer-aided Characterization of Fundamental Limits of Coded Caching with Linear Coding"
                },
                "updated": "2025-11-24T19:14:34Z",
                "updated_parsed": [
                    2025,
                    11,
                    24,
                    19,
                    14,
                    34,
                    0,
                    328,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.19639v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.19639v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Inspired by prior work by Tian and by Cao and Xu, this paper presents an efficient computer-aided framework to characterize the fundamental limits of coded caching systems under the constraint of linear coding. The proposed framework considers non-Shannon-type inequalities which are valid for representable polymatroids (and hence for linear codes), and leverages symmetric structure and problem-specific constraints of coded caching to reduce the complexity of the linear program. The derived converse bounds are tighter compared to previous known analytic methods, and prove the optimality of some achievable memory-load tradeoff points under the constraint of linear coding placement and delivery. These results seem to indicate that small, structured demand subsets combined with minimal common information constructions may be sufficient to characterize optimal tradeoffs under linear coding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inspired by prior work by Tian and by Cao and Xu, this paper presents an efficient computer-aided framework to characterize the fundamental limits of coded caching systems under the constraint of linear coding. The proposed framework considers non-Shannon-type inequalities which are valid for representable polymatroids (and hence for linear codes), and leverages symmetric structure and problem-specific constraints of coded caching to reduce the complexity of the linear program. The derived converse bounds are tighter compared to previous known analytic methods, and prove the optimality of some achievable memory-load tradeoff points under the constraint of linear coding placement and delivery. These results seem to indicate that small, structured demand subsets combined with minimal common information constructions may be sufficient to characterize optimal tradeoffs under linear coding."
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-24T19:14:34Z",
                "published_parsed": [
                    2025,
                    11,
                    24,
                    19,
                    14,
                    34,
                    0,
                    328,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT"
                },
                "authors": [
                    {
                        "name": "Niccol Brembilla"
                    },
                    {
                        "name": "Yinbin Ma"
                    },
                    {
                        "name": "Pietro Belotti"
                    },
                    {
                        "name": "Federico Malucelli"
                    },
                    {
                        "name": "Daniela Tuninetti"
                    }
                ],
                "author_detail": {
                    "name": "Daniela Tuninetti"
                },
                "author": "Daniela Tuninetti"
            },
            {
                "id": "http://arxiv.org/abs/2511.19269v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.19269v1",
                "title": "CDLM: Consistency Diffusion Language Models For Faster Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CDLM: Consistency Diffusion Language Models For Faster Sampling"
                },
                "updated": "2025-11-24T16:21:25Z",
                "updated_parsed": [
                    2025,
                    11,
                    24,
                    16,
                    21,
                    25,
                    0,
                    328,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.19269v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.19269v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Diffusion Language Models (DLMs) offer a promising parallel generation paradigm but suffer from slow inference due to numerous refinement steps and the inability to use standard KV caching. We introduce CDLM (Consistency Diffusion Language Models), a training-based acceleration method that simultaneously tackles both bottlenecks. CDLM integrates consistency modeling to drastically reduce the number of required sampling steps by enabling multi-token finalization. Furthermore, we enforce a block-wise causal attention mask during fine-tuning, making the model fully compatible with KV caching. Experiments show CDLM achieves 3.6x-14.5x lower latency while maintaining competitive accuracy on math and coding tasks. The full training and evaluation code is available at https://github.com/SqueezeAILab/CDLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Language Models (DLMs) offer a promising parallel generation paradigm but suffer from slow inference due to numerous refinement steps and the inability to use standard KV caching. We introduce CDLM (Consistency Diffusion Language Models), a training-based acceleration method that simultaneously tackles both bottlenecks. CDLM integrates consistency modeling to drastically reduce the number of required sampling steps by enabling multi-token finalization. Furthermore, we enforce a block-wise causal attention mask during fine-tuning, making the model fully compatible with KV caching. Experiments show CDLM achieves 3.6x-14.5x lower latency while maintaining competitive accuracy on math and coding tasks. The full training and evaluation code is available at https://github.com/SqueezeAILab/CDLM."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-24T16:21:25Z",
                "published_parsed": [
                    2025,
                    11,
                    24,
                    16,
                    21,
                    25,
                    0,
                    328,
                    0
                ],
                "arxiv_comment": "18 pages, 6 figures",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Minseo Kim"
                    },
                    {
                        "name": "Chenfeng Xu"
                    },
                    {
                        "name": "Coleman Hooper"
                    },
                    {
                        "name": "Harman Singh"
                    },
                    {
                        "name": "Ben Athiwaratkun"
                    },
                    {
                        "name": "Ce Zhang"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Amir Gholami"
                    }
                ],
                "author_detail": {
                    "name": "Amir Gholami"
                },
                "author": "Amir Gholami"
            },
            {
                "id": "http://arxiv.org/abs/2511.21759v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.21759v1",
                "title": "Orchestrating Dual-Boundaries: An Arithmetic Intensity Inspired Acceleration Framework for Diffusion Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Orchestrating Dual-Boundaries: An Arithmetic Intensity Inspired Acceleration Framework for Diffusion Language Models"
                },
                "updated": "2025-11-24T13:36:54Z",
                "updated_parsed": [
                    2025,
                    11,
                    24,
                    13,
                    36,
                    54,
                    0,
                    328,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.21759v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.21759v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Diffusion-based large language models (dLLMs) have recently gained significant attention for their exceptional performance and inherent potential for parallel decoding. Existing frameworks further enhance its inference efficiency by enabling KV caching. However, its bidirectional attention mechanism necessitates periodic cache refreshes that interleave prefill and decoding phases, both contributing substantial inference cost and constraining achievable speedup. Inspired by the heterogeneous arithmetic intensity of the prefill and decoding phases, we propose ODB-dLLM, a framework that orchestrates dual-boundaries to accelerate dLLM inference. In the prefill phase, we find that the predefined fixed response length introduces heavy yet redundant computational overhead, which affects efficiency. To alleviate this, ODB-dLLM incorporates an adaptive length prediction mechanism that progressively reduces prefill overhead and unnecessary computation. In the decoding phase, we analyze the computational characteristics of dLLMs and propose a dLLM-specific jump-share speculative decoding method to enhance efficiency by reducing the number of decoding iterations. Experimental results demonstrate that ODB-dLLM achieves 46-162x and 2.63-6.30x speedups over the baseline dLLM and Fast-dLLM, respectively, while simultaneously mitigating the accuracy degradation in existing acceleration frameworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based large language models (dLLMs) have recently gained significant attention for their exceptional performance and inherent potential for parallel decoding. Existing frameworks further enhance its inference efficiency by enabling KV caching. However, its bidirectional attention mechanism necessitates periodic cache refreshes that interleave prefill and decoding phases, both contributing substantial inference cost and constraining achievable speedup. Inspired by the heterogeneous arithmetic intensity of the prefill and decoding phases, we propose ODB-dLLM, a framework that orchestrates dual-boundaries to accelerate dLLM inference. In the prefill phase, we find that the predefined fixed response length introduces heavy yet redundant computational overhead, which affects efficiency. To alleviate this, ODB-dLLM incorporates an adaptive length prediction mechanism that progressively reduces prefill overhead and unnecessary computation. In the decoding phase, we analyze the computational characteristics of dLLMs and propose a dLLM-specific jump-share speculative decoding method to enhance efficiency by reducing the number of decoding iterations. Experimental results demonstrate that ODB-dLLM achieves 46-162x and 2.63-6.30x speedups over the baseline dLLM and Fast-dLLM, respectively, while simultaneously mitigating the accuracy degradation in existing acceleration frameworks."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-24T13:36:54Z",
                "published_parsed": [
                    2025,
                    11,
                    24,
                    13,
                    36,
                    54,
                    0,
                    328,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Linye Wei"
                    },
                    {
                        "name": "Wenjue Chen"
                    },
                    {
                        "name": "Pingzhi Tang"
                    },
                    {
                        "name": "Xiaotian Guo"
                    },
                    {
                        "name": "Le Ye"
                    },
                    {
                        "name": "Runsheng Wang"
                    },
                    {
                        "name": "Meng Li"
                    }
                ],
                "author_detail": {
                    "name": "Meng Li"
                },
                "author": "Meng Li"
            },
            {
                "id": "http://arxiv.org/abs/2511.18936v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.18936v1",
                "title": "SWAN: Sparse Winnowed Attention for Reduced Inference Memory via Decompression-Free KV-Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SWAN: Sparse Winnowed Attention for Reduced Inference Memory via Decompression-Free KV-Cache Compression"
                },
                "updated": "2025-11-24T09:41:24Z",
                "updated_parsed": [
                    2025,
                    11,
                    24,
                    9,
                    41,
                    24,
                    0,
                    328,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.18936v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.18936v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) face a significant bottleneck during autoregressive inference due to the massive memory footprint of the Key-Value (KV) cache. Existing compression techniques like token eviction, quantization, or other low-rank methods often risk information loss, have fixed limits, or introduce significant computational overhead from explicit decompression steps. In this work, we introduce SWAN, a novel, fine-tuning-free framework that eliminates this overhead. Our method uses an offline orthogonal matrix to rotate and prune the KV-cache, which is then used directly in the attention computation without any reconstruction. Our extensive experiments demonstrate that SWAN, augmented with a small dense buffer, offers a robust trade-off, maintaining performance close to the uncompressed baseline even at aggressive 50-60% memory savings per-token on KV-cache. A key advantage is its runtime-tunable compression level, allowing operators to dynamically adjust the memory footprint, a flexibility absent in methods requiring fixed offline configurations. This combination of a decompression-free design, high performance under compression, and adaptability makes SWAN a practical and efficient solution for serving LLMs with long contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) face a significant bottleneck during autoregressive inference due to the massive memory footprint of the Key-Value (KV) cache. Existing compression techniques like token eviction, quantization, or other low-rank methods often risk information loss, have fixed limits, or introduce significant computational overhead from explicit decompression steps. In this work, we introduce SWAN, a novel, fine-tuning-free framework that eliminates this overhead. Our method uses an offline orthogonal matrix to rotate and prune the KV-cache, which is then used directly in the attention computation without any reconstruction. Our extensive experiments demonstrate that SWAN, augmented with a small dense buffer, offers a robust trade-off, maintaining performance close to the uncompressed baseline even at aggressive 50-60% memory savings per-token on KV-cache. A key advantage is its runtime-tunable compression level, allowing operators to dynamically adjust the memory footprint, a flexibility absent in methods requiring fixed offline configurations. This combination of a decompression-free design, high performance under compression, and adaptability makes SWAN a practical and efficient solution for serving LLMs with long contexts."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-24T09:41:24Z",
                "published_parsed": [
                    2025,
                    11,
                    24,
                    9,
                    41,
                    24,
                    0,
                    328,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Santhosh G S"
                    },
                    {
                        "name": "Saurav Prakash"
                    },
                    {
                        "name": "Balaraman Ravindran"
                    }
                ],
                "author_detail": {
                    "name": "Balaraman Ravindran"
                },
                "author": "Balaraman Ravindran"
            },
            {
                "id": "http://arxiv.org/abs/2507.06567v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2507.06567v2",
                "title": "SlimCaching: Edge Caching of Mixture-of-Experts for Distributed Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SlimCaching: Edge Caching of Mixture-of-Experts for Distributed Inference"
                },
                "updated": "2025-11-24T09:35:35Z",
                "updated_parsed": [
                    2025,
                    11,
                    24,
                    9,
                    35,
                    35,
                    0,
                    328,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2507.06567v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2507.06567v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Mixture-of-Experts (MoE) models improve the scalability of large language models (LLMs) by activating only a small subset of relevant experts per input. However, the sheer number of expert networks in an MoE model introduces a significant storage burden for an edge device. To address this challenge, we consider a scenario where experts are dispersed across an edge network for distributed inference. Based on the popular Top-$K$ expert selection strategy, we formulate a latency minimization problem by optimizing expert caching on edge servers under storage constraints. When $K=1$, the problem reduces to a monotone submodular maximization problem with knapsack constraints, for which we design a greedy-based algorithm with a $(1 - 1/e)$-approximation guarantee. For the general case where $K \\geq 1$, expert co-activation within the same MoE layer introduces non-submodularity, which renders greedy methods ineffective. To tackle this issue, we propose a successive greedy decomposition method to decompose the original problem into a series of subproblems, with each being solved by a dynamic programming approach. Furthermore, we design an accelerated algorithm based on the max-convolution technique to obtain the approximate solution with a provable guarantee in polynomial time. Simulation results on various MoE models demonstrate that our method significantly reduces inference latency compared to existing baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) models improve the scalability of large language models (LLMs) by activating only a small subset of relevant experts per input. However, the sheer number of expert networks in an MoE model introduces a significant storage burden for an edge device. To address this challenge, we consider a scenario where experts are dispersed across an edge network for distributed inference. Based on the popular Top-$K$ expert selection strategy, we formulate a latency minimization problem by optimizing expert caching on edge servers under storage constraints. When $K=1$, the problem reduces to a monotone submodular maximization problem with knapsack constraints, for which we design a greedy-based algorithm with a $(1 - 1/e)$-approximation guarantee. For the general case where $K \\geq 1$, expert co-activation within the same MoE layer introduces non-submodularity, which renders greedy methods ineffective. To tackle this issue, we propose a successive greedy decomposition method to decompose the original problem into a series of subproblems, with each being solved by a dynamic programming approach. Furthermore, we design an accelerated algorithm based on the max-convolution technique to obtain the approximate solution with a provable guarantee in polynomial time. Simulation results on various MoE models demonstrate that our method significantly reduces inference latency compared to existing baselines."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-07-09T05:43:43Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    5,
                    43,
                    43,
                    2,
                    190,
                    0
                ],
                "arxiv_comment": "17 pages, 11 figures. This work has been submitted to the IEEE for possible publication",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Qian Chen"
                    },
                    {
                        "name": "Xianhao Chen"
                    },
                    {
                        "name": "Kaibin Huang"
                    }
                ],
                "author_detail": {
                    "name": "Kaibin Huang"
                },
                "author": "Kaibin Huang"
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2512.05967v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05967v1",
                "title": "Enhancing Retrieval-Augmented Generation with Entity Linking for Educational Platforms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Retrieval-Augmented Generation with Entity Linking for Educational Platforms"
                },
                "updated": "2025-12-05T18:59:18Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    18,
                    59,
                    18,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05967v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05967v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "In the era of Large Language Models (LLMs), Retrieval-Augmented Generation (RAG) architectures are gaining significant attention for their ability to ground language generation in reliable knowledge sources. Despite their impressive effectiveness in many areas, RAG systems based solely on semantic similarity often fail to ensure factual accuracy in specialized domains, where terminological ambiguity can affect retrieval relevance. This study proposes an enhanced RAG architecture that integrates a factual signal derived from Entity Linking to improve the accuracy of educational question-answering systems in Italian. The system includes a Wikidata-based Entity Linking module and implements three re-ranking strategies to combine semantic and entity-based information: a hybrid score weighting model, reciprocal rank fusion, and a cross-encoder re-ranker. Experiments were conducted on two benchmarks: a custom academic dataset and the standard SQuAD-it dataset. Results show that, in domain-specific contexts, the hybrid schema based on reciprocal rank fusion significantly outperforms both the baseline and the cross-encoder approach, while the cross-encoder achieves the best results on the general-domain dataset. These findings confirm the presence of an effect of domain mismatch and highlight the importance of domain adaptation and hybrid ranking strategies to enhance factual precision and reliability in retrieval-augmented generation. They also demonstrate the potential of entity-aware RAG systems in educational environments, fostering adaptive and reliable AI-based tutoring tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the era of Large Language Models (LLMs), Retrieval-Augmented Generation (RAG) architectures are gaining significant attention for their ability to ground language generation in reliable knowledge sources. Despite their impressive effectiveness in many areas, RAG systems based solely on semantic similarity often fail to ensure factual accuracy in specialized domains, where terminological ambiguity can affect retrieval relevance. This study proposes an enhanced RAG architecture that integrates a factual signal derived from Entity Linking to improve the accuracy of educational question-answering systems in Italian. The system includes a Wikidata-based Entity Linking module and implements three re-ranking strategies to combine semantic and entity-based information: a hybrid score weighting model, reciprocal rank fusion, and a cross-encoder re-ranker. Experiments were conducted on two benchmarks: a custom academic dataset and the standard SQuAD-it dataset. Results show that, in domain-specific contexts, the hybrid schema based on reciprocal rank fusion significantly outperforms both the baseline and the cross-encoder approach, while the cross-encoder achieves the best results on the general-domain dataset. These findings confirm the presence of an effect of domain mismatch and highlight the importance of domain adaptation and hybrid ranking strategies to enhance factual precision and reliability in retrieval-augmented generation. They also demonstrate the potential of entity-aware RAG systems in educational environments, fostering adaptive and reliable AI-based tutoring tools."
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T18:59:18Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    18,
                    59,
                    18,
                    4,
                    339,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR"
                },
                "authors": [
                    {
                        "name": "Francesco Granata"
                    },
                    {
                        "name": "Francesco Poggi"
                    },
                    {
                        "name": "Misael Mongiov"
                    }
                ],
                "author_detail": {
                    "name": "Misael Mongiov"
                },
                "author": "Misael Mongiov"
            },
            {
                "id": "http://arxiv.org/abs/2512.05964v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05964v1",
                "title": "Training-Time Action Conditioning for Efficient Real-Time Chunking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-Time Action Conditioning for Efficient Real-Time Chunking"
                },
                "updated": "2025-12-05T18:57:28Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    18,
                    57,
                    28,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05964v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05964v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Real-time chunking (RTC) enables vision-language-action models (VLAs) to generate smooth, reactive robot trajectories by asynchronously predicting action chunks and conditioning on previously committed actions via inference-time inpainting. However, this inpainting method introduces computational overhead that increases inference latency. In this work, we propose a simple alternative: simulating inference delay at training time and conditioning on action prefixes directly, eliminating any inference-time overhead. Our method requires no modifications to the model architecture or robot runtime, and can be implemented with only a few additional lines of code. In simulated experiments, we find that training-time RTC outperforms inference-time RTC at higher inference delays. In real-world experiments on box building and espresso making tasks with the $_{0.6}$ VLA, we demonstrate that training-time RTC maintains both task performance and speed parity with inference-time RTC while being computationally cheaper. Our results suggest that training-time action conditioning is a practical drop-in replacement for inference-time inpainting in real-time robot control.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time chunking (RTC) enables vision-language-action models (VLAs) to generate smooth, reactive robot trajectories by asynchronously predicting action chunks and conditioning on previously committed actions via inference-time inpainting. However, this inpainting method introduces computational overhead that increases inference latency. In this work, we propose a simple alternative: simulating inference delay at training time and conditioning on action prefixes directly, eliminating any inference-time overhead. Our method requires no modifications to the model architecture or robot runtime, and can be implemented with only a few additional lines of code. In simulated experiments, we find that training-time RTC outperforms inference-time RTC at higher inference delays. In real-world experiments on box building and espresso making tasks with the $_{0.6}$ VLA, we demonstrate that training-time RTC maintains both task performance and speed parity with inference-time RTC while being computationally cheaper. Our results suggest that training-time action conditioning is a practical drop-in replacement for inference-time inpainting in real-time robot control."
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T18:57:28Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    18,
                    57,
                    28,
                    4,
                    339,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "authors": [
                    {
                        "name": "Kevin Black"
                    },
                    {
                        "name": "Allen Z. Ren"
                    },
                    {
                        "name": "Michael Equi"
                    },
                    {
                        "name": "Sergey Levine"
                    }
                ],
                "author_detail": {
                    "name": "Sergey Levine"
                },
                "author": "Sergey Levine"
            },
            {
                "id": "http://arxiv.org/abs/2512.05962v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05962v1",
                "title": "Whatever Remains Must Be True: Filtering Drives Reasoning in LLMs, Shaping Diversity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Whatever Remains Must Be True: Filtering Drives Reasoning in LLMs, Shaping Diversity"
                },
                "updated": "2025-12-05T18:56:40Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    18,
                    56,
                    40,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05962v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05962v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Reinforcement Learning (RL) has become the de facto standard for tuning LLMs to solve tasks involving reasoning. However, growing evidence shows that models trained in such way often suffer from a significant loss in diversity. We argue that this arises because RL implicitly optimizes the \"mode-seeking\" or \"zero-forcing\" Reverse KL to a target distribution causing the model to concentrate mass on certain high-probability regions of the target while neglecting others. In this work, we instead begin from an explicit target distribution, obtained by filtering out incorrect answers while preserving the relative probabilities of correct ones. Starting from a pre-trained LLM, we approximate this target distribution using the $$-divergence family, which unifies prior approaches and enables direct control of the precision-diversity trade-off by interpolating between mode-seeking and mass-covering divergences. On a Lean theorem-proving benchmark, our method achieves state-of-the-art performance along the coverage-precision Pareto frontier, outperforming all prior methods on the coverage axis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning (RL) has become the de facto standard for tuning LLMs to solve tasks involving reasoning. However, growing evidence shows that models trained in such way often suffer from a significant loss in diversity. We argue that this arises because RL implicitly optimizes the \"mode-seeking\" or \"zero-forcing\" Reverse KL to a target distribution causing the model to concentrate mass on certain high-probability regions of the target while neglecting others. In this work, we instead begin from an explicit target distribution, obtained by filtering out incorrect answers while preserving the relative probabilities of correct ones. Starting from a pre-trained LLM, we approximate this target distribution using the $$-divergence family, which unifies prior approaches and enables direct control of the precision-diversity trade-off by interpolating between mode-seeking and mass-covering divergences. On a Lean theorem-proving benchmark, our method achieves state-of-the-art performance along the coverage-precision Pareto frontier, outperforming all prior methods on the coverage axis."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T18:56:40Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    18,
                    56,
                    40,
                    4,
                    339,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Germn Kruszewski"
                    },
                    {
                        "name": "Pierre Erbacher"
                    },
                    {
                        "name": "Jos Rozen"
                    },
                    {
                        "name": "Marc Dymetman"
                    }
                ],
                "author_detail": {
                    "name": "Marc Dymetman"
                },
                "author": "Marc Dymetman"
            },
            {
                "id": "http://arxiv.org/abs/2512.05958v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05958v1",
                "title": "MaxShapley: Towards Incentive-compatible Generative Search with Fair Context Attribution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MaxShapley: Towards Incentive-compatible Generative Search with Fair Context Attribution"
                },
                "updated": "2025-12-05T18:54:21Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    18,
                    54,
                    21,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05958v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05958v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Generative search engines based on large language models (LLMs) are replacing traditional search, fundamentally changing how information providers are compensated. To sustain this ecosystem, we need fair mechanisms to attribute and compensate content providers based on their contributions to generated answers. We introduce MaxShapley, an efficient algorithm for fair attribution in generative search pipelines that use retrieval-augmented generation (RAG). MaxShapley is a special case of the celebrated Shapley value; it leverages a decomposable max-sum utility function to compute attributions with linear computation in the number of documents, as opposed to the exponential cost of Shapley values. We evaluate MaxShapley on three multi-hop QA datasets (HotPotQA, MuSiQUE, MS MARCO); MaxShapley achieves comparable attribution quality to exact Shapley computation, while consuming a fraction of its tokens--for instance, it gives up to an 8x reduction in resource consumption over prior state-of-the-art methods at the same attribution accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative search engines based on large language models (LLMs) are replacing traditional search, fundamentally changing how information providers are compensated. To sustain this ecosystem, we need fair mechanisms to attribute and compensate content providers based on their contributions to generated answers. We introduce MaxShapley, an efficient algorithm for fair attribution in generative search pipelines that use retrieval-augmented generation (RAG). MaxShapley is a special case of the celebrated Shapley value; it leverages a decomposable max-sum utility function to compute attributions with linear computation in the number of documents, as opposed to the exponential cost of Shapley values. We evaluate MaxShapley on three multi-hop QA datasets (HotPotQA, MuSiQUE, MS MARCO); MaxShapley achieves comparable attribution quality to exact Shapley computation, while consuming a fraction of its tokens--for instance, it gives up to an 8x reduction in resource consumption over prior state-of-the-art methods at the same attribution accuracy."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T18:54:21Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    18,
                    54,
                    21,
                    4,
                    339,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Sara Patel"
                    },
                    {
                        "name": "Mingxun Zhou"
                    },
                    {
                        "name": "Giulia Fanti"
                    }
                ],
                "author_detail": {
                    "name": "Giulia Fanti"
                },
                "author": "Giulia Fanti"
            },
            {
                "id": "http://arxiv.org/abs/2406.06211v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2406.06211v3",
                "title": "iMotion-LLM: Instruction-Conditioned Trajectory Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "iMotion-LLM: Instruction-Conditioned Trajectory Generation"
                },
                "updated": "2025-12-05T18:52:32Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    18,
                    52,
                    32,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2406.06211v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2406.06211v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We introduce iMotion-LLM, a large language model (LLM) integrated with trajectory prediction modules for interactive motion generation. Unlike conventional approaches, it generates feasible, safety-aligned trajectories based on textual instructions, enabling adaptable and context-aware driving behavior. It combines an encoder-decoder multimodal trajectory prediction model with a pre-trained LLM fine-tuned using LoRA, projecting scene features into the LLM input space and mapping special tokens to a trajectory decoder for text-based interaction and interpretable driving. To support this framework, we introduce two datasets: 1) InstructWaymo, an extension of the Waymo Open Motion Dataset with direction-based motion instructions, and 2) Open-Vocabulary InstructNuPlan, which features safety-aligned instruction-caption pairs and corresponding safe trajectory scenarios. Our experiments validate that instruction conditioning enables trajectory generation that follows the intended condition. iMotion-LLM demonstrates strong contextual comprehension, achieving 84% average accuracy in direction feasibility detection and 96% average accuracy in safety evaluation of open-vocabulary instructions. This work lays the foundation for text-guided motion generation in autonomous driving, supporting simulated data generation, model interpretability, and robust safety alignment testing for trajectory generation models. Our code, pre-trained model, and datasets are available at: https://vision-cair.github.io/iMotion-LLM/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce iMotion-LLM, a large language model (LLM) integrated with trajectory prediction modules for interactive motion generation. Unlike conventional approaches, it generates feasible, safety-aligned trajectories based on textual instructions, enabling adaptable and context-aware driving behavior. It combines an encoder-decoder multimodal trajectory prediction model with a pre-trained LLM fine-tuned using LoRA, projecting scene features into the LLM input space and mapping special tokens to a trajectory decoder for text-based interaction and interpretable driving. To support this framework, we introduce two datasets: 1) InstructWaymo, an extension of the Waymo Open Motion Dataset with direction-based motion instructions, and 2) Open-Vocabulary InstructNuPlan, which features safety-aligned instruction-caption pairs and corresponding safe trajectory scenarios. Our experiments validate that instruction conditioning enables trajectory generation that follows the intended condition. iMotion-LLM demonstrates strong contextual comprehension, achieving 84% average accuracy in direction feasibility detection and 96% average accuracy in safety evaluation of open-vocabulary instructions. This work lays the foundation for text-guided motion generation in autonomous driving, supporting simulated data generation, model interpretability, and robust safety alignment testing for trajectory generation models. Our code, pre-trained model, and datasets are available at: https://vision-cair.github.io/iMotion-LLM/."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-06-10T12:22:06Z",
                "published_parsed": [
                    2024,
                    6,
                    10,
                    12,
                    22,
                    6,
                    0,
                    162,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Abdulwahab Felemban"
                    },
                    {
                        "name": "Nussair Hroub"
                    },
                    {
                        "name": "Jian Ding"
                    },
                    {
                        "name": "Eslam Abdelrahman"
                    },
                    {
                        "name": "Xiaoqian Shen"
                    },
                    {
                        "name": "Abduallah Mohamed"
                    },
                    {
                        "name": "Mohamed Elhoseiny"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed Elhoseiny"
                },
                "author": "Mohamed Elhoseiny"
            },
            {
                "id": "http://arxiv.org/abs/2512.05950v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05950v1",
                "title": "Impugan: Learning Conditional Generative Models for Robust Data Imputation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Impugan: Learning Conditional Generative Models for Robust Data Imputation"
                },
                "updated": "2025-12-05T18:46:33Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    18,
                    46,
                    33,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05950v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05950v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Incomplete data are common in real-world applications. Sensors fail, records are inconsistent, and datasets collected from different sources often differ in scale, sampling rate, and quality. These differences create missing values that make it difficult to combine data and build reliable models. Standard imputation methods such as regression models, expectation-maximization, and multiple imputation rely on strong assumptions about linearity and independence. These assumptions rarely hold for complex or heterogeneous data, which can lead to biased or over-smoothed estimates. We propose Impugan, a conditional Generative Adversarial Network (cGAN) for imputing missing values and integrating heterogeneous datasets. The model is trained on complete samples to learn how missing variables depend on observed ones. During inference, the generator reconstructs missing entries from available features, and the discriminator enforces realism by distinguishing true from imputed data. This adversarial process allows Impugan to capture nonlinear and multimodal relationships that conventional methods cannot represent. In experiments on benchmark datasets and a multi-source integration task, Impugan achieves up to 82\\% lower Earth Mover's Distance (EMD) and 70\\% lower mutual-information deviation (MI) compared to leading baselines. These results show that adversarially trained generative models provide a scalable and principled approach for imputing and merging incomplete, heterogeneous data. Our model is available at: github.com/zalishmahmud/impuganBigData2025",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Incomplete data are common in real-world applications. Sensors fail, records are inconsistent, and datasets collected from different sources often differ in scale, sampling rate, and quality. These differences create missing values that make it difficult to combine data and build reliable models. Standard imputation methods such as regression models, expectation-maximization, and multiple imputation rely on strong assumptions about linearity and independence. These assumptions rarely hold for complex or heterogeneous data, which can lead to biased or over-smoothed estimates. We propose Impugan, a conditional Generative Adversarial Network (cGAN) for imputing missing values and integrating heterogeneous datasets. The model is trained on complete samples to learn how missing variables depend on observed ones. During inference, the generator reconstructs missing entries from available features, and the discriminator enforces realism by distinguishing true from imputed data. This adversarial process allows Impugan to capture nonlinear and multimodal relationships that conventional methods cannot represent. In experiments on benchmark datasets and a multi-source integration task, Impugan achieves up to 82\\% lower Earth Mover's Distance (EMD) and 70\\% lower mutual-information deviation (MI) compared to leading baselines. These results show that adversarially trained generative models provide a scalable and principled approach for imputing and merging incomplete, heterogeneous data. Our model is available at: github.com/zalishmahmud/impuganBigData2025"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T18:46:33Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    18,
                    46,
                    33,
                    4,
                    339,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Zalish Mahmud"
                    },
                    {
                        "name": "Anantaa Kotal"
                    },
                    {
                        "name": "Aritran Piplai"
                    }
                ],
                "author_detail": {
                    "name": "Aritran Piplai"
                },
                "author": "Aritran Piplai"
            },
            {
                "id": "http://arxiv.org/abs/2512.05940v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05940v1",
                "title": "Designing an Optimal Sensor Network via Minimizing Information Loss",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Designing an Optimal Sensor Network via Minimizing Information Loss"
                },
                "updated": "2025-12-05T18:38:30Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    18,
                    38,
                    30,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05940v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05940v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Optimal experimental design is a classic topic in statistics, with many well-studied problems, applications, and solutions. The design problem we study is the placement of sensors to monitor spatiotemporal processes, explicitly accounting for the temporal dimension in our modeling and optimization. We observe that recent advancements in computational sciences often yield large datasets based on physics-based simulations, which are rarely leveraged in experimental design. We introduce a novel model-based sensor placement criterion, along with a highly-efficient optimization algorithm, which integrates physics-based simulations and Bayesian experimental design principles to identify sensor networks that \"minimize information loss\" from simulated data. Our technique relies on sparse variational inference and (separable) Gauss-Markov priors, and thus may adapt many techniques from Bayesian experimental design. We validate our method through a case study monitoring air temperature in Phoenix, Arizona, using state-of-the-art physics-based simulations. Our results show our framework to be superior to random or quasi-random sampling, particularly with a limited number of sensors. We conclude by discussing practical considerations and implications of our framework, including more complex modeling tools and real-world deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal experimental design is a classic topic in statistics, with many well-studied problems, applications, and solutions. The design problem we study is the placement of sensors to monitor spatiotemporal processes, explicitly accounting for the temporal dimension in our modeling and optimization. We observe that recent advancements in computational sciences often yield large datasets based on physics-based simulations, which are rarely leveraged in experimental design. We introduce a novel model-based sensor placement criterion, along with a highly-efficient optimization algorithm, which integrates physics-based simulations and Bayesian experimental design principles to identify sensor networks that \"minimize information loss\" from simulated data. Our technique relies on sparse variational inference and (separable) Gauss-Markov priors, and thus may adapt many techniques from Bayesian experimental design. We validate our method through a case study monitoring air temperature in Phoenix, Arizona, using state-of-the-art physics-based simulations. Our results show our framework to be superior to random or quasi-random sampling, particularly with a limited number of sensors. We conclude by discussing practical considerations and implications of our framework, including more complex modeling tools and real-world deployments."
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T18:38:30Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    18,
                    38,
                    30,
                    4,
                    339,
                    0
                ],
                "arxiv_comment": "37 pages, 15 figures. Accepted to Bayesian Analysis",
                "arxiv_primary_category": {
                    "term": "stat.ME"
                },
                "authors": [
                    {
                        "name": "Daniel Waxman"
                    },
                    {
                        "name": "Fernando Llorente"
                    },
                    {
                        "name": "Katia Lamer"
                    },
                    {
                        "name": "Petar M. Djuri"
                    }
                ],
                "author_detail": {
                    "name": "Petar M. Djuri"
                },
                "author": "Petar M. Djuri"
            },
            {
                "id": "http://arxiv.org/abs/2511.21569v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.21569v3",
                "title": "Self-Transparency Failures in Expert-Persona LLMs: How Instruction-Following Overrides Honesty",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Transparency Failures in Expert-Persona LLMs: How Instruction-Following Overrides Honesty"
                },
                "updated": "2025-12-05T18:38:00Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    18,
                    38,
                    0,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.21569v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.21569v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "This study audits whether language models disclose their AI nature when assigned professional personas and questioned about their expertise. When models maintain false professional credentials, users may calibrate trust based on overstated competence claims, treating AI-generated guidance as equivalent to licensed professional advice. Using a common-garden experimental design, sixteen open-weight models (4B-671B parameters) were audited under identical conditions across 19,200 trials. Models exhibited sharp domain-specific inconsistency: a Financial Advisor persona elicited 30.8% disclosure at the first prompt, while a Neurosurgeon persona elicited only 3.5% - an 8.8-fold difference that emerged before any epistemic probing. Disclosure ranged from 2.8% to 73.6% across model families, with a 14B model reaching 39.4% while a 70B model produced just 4.1%. Model identity provided substantially larger improvement in fitting observations than parameter count ($R_{adj}^{2}=0.359$ vs $0.018$). Reasoning variants showed heterogeneous effects: some exhibited up to 48.4 percentage points lower disclosure than their base instruction-tuned counterparts, while others maintained high transparency. An additional experiment demonstrated that explicit permission to disclose AI nature increased disclosure from 23.7% to 65.8%, revealing that suppression reflects instruction-following prioritization rather than capability limitations. Bayesian validation confirmed robustness to judge measurement error ($=0.908$). These patterns create trust calibration risks when users encounter the same model across professional contexts. Organizations cannot assume safety properties will transfer across deployment domains, requiring deliberate behavior design and empirical verification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study audits whether language models disclose their AI nature when assigned professional personas and questioned about their expertise. When models maintain false professional credentials, users may calibrate trust based on overstated competence claims, treating AI-generated guidance as equivalent to licensed professional advice. Using a common-garden experimental design, sixteen open-weight models (4B-671B parameters) were audited under identical conditions across 19,200 trials. Models exhibited sharp domain-specific inconsistency: a Financial Advisor persona elicited 30.8% disclosure at the first prompt, while a Neurosurgeon persona elicited only 3.5% - an 8.8-fold difference that emerged before any epistemic probing. Disclosure ranged from 2.8% to 73.6% across model families, with a 14B model reaching 39.4% while a 70B model produced just 4.1%. Model identity provided substantially larger improvement in fitting observations than parameter count ($R_{adj}^{2}=0.359$ vs $0.018$). Reasoning variants showed heterogeneous effects: some exhibited up to 48.4 percentage points lower disclosure than their base instruction-tuned counterparts, while others maintained high transparency. An additional experiment demonstrated that explicit permission to disclose AI nature increased disclosure from 23.7% to 65.8%, revealing that suppression reflects instruction-following prioritization rather than capability limitations. Bayesian validation confirmed robustness to judge measurement error ($=0.908$). These patterns create trust calibration risks when users encounter the same model across professional contexts. Organizations cannot assume safety properties will transfer across deployment domains, requiring deliberate behavior design and empirical verification."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-26T16:41:49Z",
                "published_parsed": [
                    2025,
                    11,
                    26,
                    16,
                    41,
                    49,
                    2,
                    330,
                    0
                ],
                "arxiv_comment": "47 pages, 12 figures, 12 tables, Submitted to FAccT; clarify user harm, add permission experiment, condense paper",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Alex Diep"
                    }
                ],
                "author_detail": {
                    "name": "Alex Diep"
                },
                "author": "Alex Diep"
            },
            {
                "id": "http://arxiv.org/abs/2510.12229v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.12229v2",
                "title": "Analysing Moral Bias in Finetuned LLMs through Mechanistic Interpretability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analysing Moral Bias in Finetuned LLMs through Mechanistic Interpretability"
                },
                "updated": "2025-12-05T18:32:06Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    18,
                    32,
                    6,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.12229v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.12229v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) have been shown to internalize human-like biases during finetuning, yet the mechanisms by which these biases manifest remain unclear. In this work, we investigated whether the well-known Knobe effect, a moral bias in intentionality judgements, emerges in finetuned LLMs and whether it can be traced back to specific components of the model. We conducted a Layer-Patching analysis across 3 open-weights LLMs and demonstrated that the bias is not only learned during finetuning but also localized in a specific set of layers. Surprisingly, we found that patching activations from the corresponding pretrained model into just a few critical layers is sufficient to eliminate the effect. Our findings offer new evidence that social biases in LLMs can be interpreted, localized, and mitigated through targeted interventions, without the need for model retraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been shown to internalize human-like biases during finetuning, yet the mechanisms by which these biases manifest remain unclear. In this work, we investigated whether the well-known Knobe effect, a moral bias in intentionality judgements, emerges in finetuned LLMs and whether it can be traced back to specific components of the model. We conducted a Layer-Patching analysis across 3 open-weights LLMs and demonstrated that the bias is not only learned during finetuning but also localized in a specific set of layers. Surprisingly, we found that patching activations from the corresponding pretrained model into just a few critical layers is sufficient to eliminate the effect. Our findings offer new evidence that social biases in LLMs can be interpreted, localized, and mitigated through targeted interventions, without the need for model retraining."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-14T07:31:29Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    7,
                    31,
                    29,
                    1,
                    287,
                    0
                ],
                "arxiv_comment": "Preprint. Under review",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Bianca Raimondi"
                    },
                    {
                        "name": "Daniela Dalbagno"
                    },
                    {
                        "name": "Maurizio Gabbrielli"
                    }
                ],
                "author_detail": {
                    "name": "Maurizio Gabbrielli"
                },
                "author": "Maurizio Gabbrielli"
            },
            {
                "id": "http://arxiv.org/abs/2508.16560v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.16560v3",
                "title": "Sparse but Wrong: Incorrect L0 Leads to Incorrect Features in Sparse Autoencoders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse but Wrong: Incorrect L0 Leads to Incorrect Features in Sparse Autoencoders"
                },
                "updated": "2025-12-05T18:31:43Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    18,
                    31,
                    43,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.16560v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.16560v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Sparse Autoencoders (SAEs) extract features from LLM internal activations, meant to correspond to interpretable concepts. A core SAE training hyperparameter is L0: how many SAE features should fire per token on average. Existing work compares SAE algorithms using sparsity-reconstruction tradeoff plots, implying L0 is a free parameter with no single correct value aside from its effect on reconstruction. In this work we study the effect of L0 on SAEs, and show that if L0 is not set correctly, the SAE fails to disentangle the underlying features of the LLM. If L0 is too low, the SAE will mix correlated features to improve reconstruction. If L0 is too high, the SAE finds degenerate solutions that also mix features. Further, we present a proxy metric that can help guide the search for the correct L0 for an SAE on a given training distribution. We show that our method finds the correct L0 in toy models and coincides with peak sparse probing performance in LLM SAEs. We find that most commonly used SAEs have an L0 that is too low. Our work shows that L0 must be set correctly to train SAEs with correct features.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Autoencoders (SAEs) extract features from LLM internal activations, meant to correspond to interpretable concepts. A core SAE training hyperparameter is L0: how many SAE features should fire per token on average. Existing work compares SAE algorithms using sparsity-reconstruction tradeoff plots, implying L0 is a free parameter with no single correct value aside from its effect on reconstruction. In this work we study the effect of L0 on SAEs, and show that if L0 is not set correctly, the SAE fails to disentangle the underlying features of the LLM. If L0 is too low, the SAE will mix correlated features to improve reconstruction. If L0 is too high, the SAE finds degenerate solutions that also mix features. Further, we present a proxy metric that can help guide the search for the correct L0 for an SAE on a given training distribution. We show that our method finds the correct L0 in toy models and coincides with peak sparse probing performance in LLM SAEs. We find that most commonly used SAEs have an L0 that is too low. Our work shows that L0 must be set correctly to train SAEs with correct features."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-22T17:26:33Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    17,
                    26,
                    33,
                    4,
                    234,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "David Chanin"
                    },
                    {
                        "name": "Adri Garriga-Alonso"
                    }
                ],
                "author_detail": {
                    "name": "Adri Garriga-Alonso"
                },
                "author": "Adri Garriga-Alonso"
            },
            {
                "id": "http://arxiv.org/abs/2512.05929v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05929v1",
                "title": "LLM Harms: A Taxonomy and Discussion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Harms: A Taxonomy and Discussion"
                },
                "updated": "2025-12-05T18:12:21Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    18,
                    12,
                    21,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05929v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05929v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "This study addresses categories of harm surrounding Large Language Models (LLMs) in the field of artificial intelligence. It addresses five categories of harms addressed before, during, and after development of AI applications: pre-development, direct output, Misuse and Malicious Application, and downstream application. By underscoring the need to define risks of the current landscape to ensure accountability, transparency and navigating bias when adapting LLMs for practical applications. It proposes mitigation strategies and future directions for specific domains and a dynamic auditing system guiding responsible development and integration of LLMs in a standardized proposal.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study addresses categories of harm surrounding Large Language Models (LLMs) in the field of artificial intelligence. It addresses five categories of harms addressed before, during, and after development of AI applications: pre-development, direct output, Misuse and Malicious Application, and downstream application. By underscoring the need to define risks of the current landscape to ensure accountability, transparency and navigating bias when adapting LLMs for practical applications. It proposes mitigation strategies and future directions for specific domains and a dynamic auditing system guiding responsible development and integration of LLMs in a standardized proposal."
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T18:12:21Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    18,
                    12,
                    21,
                    4,
                    339,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY"
                },
                "authors": [
                    {
                        "name": "Kevin Chen"
                    },
                    {
                        "name": "Saleh Afroogh"
                    },
                    {
                        "name": "Abhejay Murali"
                    },
                    {
                        "name": "David Atkinson"
                    },
                    {
                        "name": "Amit Dhurandhar"
                    },
                    {
                        "name": "Junfeng Jiao"
                    }
                ],
                "author_detail": {
                    "name": "Junfeng Jiao"
                },
                "author": "Junfeng Jiao"
            },
            {
                "id": "http://arxiv.org/abs/2512.05925v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05925v1",
                "title": "To Err Is Human: Systematic Quantification of Errors in Published AI Papers via LLM Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To Err Is Human: Systematic Quantification of Errors in Published AI Papers via LLM Analysis"
                },
                "updated": "2025-12-05T18:04:10Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    18,
                    4,
                    10,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05925v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05925v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "How many mistakes do published AI papers contain? Peer-reviewed publications form the foundation upon which new research and knowledge are built. Errors that persist in the literature can propagate unnoticed, creating confusion in follow-up studies and complicating reproducibility. The accelerating pace of research and the increasing demands on the peer-review system make such mistakes harder to detect and avoid. To address this, we developed a Paper Correctness Checker based on GPT-5 to systematically identify mistakes in papers previously published at top AI conferences and journals. Our analysis focuses on objective mistakes-e.g., errors in formulas, derivations, calculations, figures, and tables-that have a clearly verifiable ground truth. We intentionally exclude subjective considerations such as novelty, importance, or writing quality. We find that published papers contain a non-negligible number of objective mistakes and that the average number of mistakes per paper has increased over time-from 3.8 in NeurIPS 2021 to 5.9 in NeurIPS 2025 (55.3% increase); from 4.1 in ICLR 2018 to 5.2 in ICLR 2025; and from 5.0 in TMLR 2022/23 to 5.5 in TMLR 2025. Human experts reviewed 316 potential mistakes identified by the AI Checker and confirmed that 263 were actual mistakes, corresponding to a precision of 83.2%. While most identified issues are relatively minor, correcting them would reduce confusion in the literature and strengthen reproducibility. The AI Checker also surfaced potentially more substantive mistakes that could affect the interpretation of results. Moreover, we show that the AI Checker can propose correct fixes for 75.8% of the identified mistakes. Overall, this study highlights the potential of frontier LLMs to detect and correct objective mistakes in published papers, helping to establish a firmer foundation of knowledge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How many mistakes do published AI papers contain? Peer-reviewed publications form the foundation upon which new research and knowledge are built. Errors that persist in the literature can propagate unnoticed, creating confusion in follow-up studies and complicating reproducibility. The accelerating pace of research and the increasing demands on the peer-review system make such mistakes harder to detect and avoid. To address this, we developed a Paper Correctness Checker based on GPT-5 to systematically identify mistakes in papers previously published at top AI conferences and journals. Our analysis focuses on objective mistakes-e.g., errors in formulas, derivations, calculations, figures, and tables-that have a clearly verifiable ground truth. We intentionally exclude subjective considerations such as novelty, importance, or writing quality. We find that published papers contain a non-negligible number of objective mistakes and that the average number of mistakes per paper has increased over time-from 3.8 in NeurIPS 2021 to 5.9 in NeurIPS 2025 (55.3% increase); from 4.1 in ICLR 2018 to 5.2 in ICLR 2025; and from 5.0 in TMLR 2022/23 to 5.5 in TMLR 2025. Human experts reviewed 316 potential mistakes identified by the AI Checker and confirmed that 263 were actual mistakes, corresponding to a precision of 83.2%. While most identified issues are relatively minor, correcting them would reduce confusion in the literature and strengthen reproducibility. The AI Checker also surfaced potentially more substantive mistakes that could affect the interpretation of results. Moreover, we show that the AI Checker can propose correct fixes for 75.8% of the identified mistakes. Overall, this study highlights the potential of frontier LLMs to detect and correct objective mistakes in published papers, helping to establish a firmer foundation of knowledge."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T18:04:10Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    18,
                    4,
                    10,
                    4,
                    339,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Federico Bianchi"
                    },
                    {
                        "name": "Yongchan Kwon"
                    },
                    {
                        "name": "Zachary Izzo"
                    },
                    {
                        "name": "Linjun Zhang"
                    },
                    {
                        "name": "James Zou"
                    }
                ],
                "author_detail": {
                    "name": "James Zou"
                },
                "author": "James Zou"
            },
            {
                "id": "http://arxiv.org/abs/2512.05916v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05916v1",
                "title": "KQ-SVD: Compressing the KV Cache with Provable Guarantees on Attention Fidelity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KQ-SVD: Compressing the KV Cache with Provable Guarantees on Attention Fidelity"
                },
                "updated": "2025-12-05T17:51:10Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    17,
                    51,
                    10,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05916v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05916v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The Key-Value (KV) cache is central to the efficiency of transformer-based large language models (LLMs), storing previously computed vectors to accelerate inference. Yet, as sequence length and batch size grow, the cache becomes a major memory bottleneck. Prior compression methods typically apply low-rank decomposition to keys alone or attempt to jointly embed queries and keys, but both approaches neglect that attention fundamentally depends on their inner products. In this work, we prove that such strategies are suboptimal for approximating the attention matrix. We introduce KQ-SVD, a simple and computationally efficient method that directly performs an optimal low-rank decomposition of the attention matrix via a closed-form solution. By targeting the true source of redundancy, KQ-SVD preserves attention outputs with higher fidelity under compression. Extensive evaluations on LLaMA and Mistral models demonstrate that our approach consistently delivers superior projection quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Key-Value (KV) cache is central to the efficiency of transformer-based large language models (LLMs), storing previously computed vectors to accelerate inference. Yet, as sequence length and batch size grow, the cache becomes a major memory bottleneck. Prior compression methods typically apply low-rank decomposition to keys alone or attempt to jointly embed queries and keys, but both approaches neglect that attention fundamentally depends on their inner products. In this work, we prove that such strategies are suboptimal for approximating the attention matrix. We introduce KQ-SVD, a simple and computationally efficient method that directly performs an optimal low-rank decomposition of the attention matrix via a closed-form solution. By targeting the true source of redundancy, KQ-SVD preserves attention outputs with higher fidelity under compression. Extensive evaluations on LLaMA and Mistral models demonstrate that our approach consistently delivers superior projection quality."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T17:51:10Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    17,
                    51,
                    10,
                    4,
                    339,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Damien Lesens"
                    },
                    {
                        "name": "Beheshteh T. Rakhshan"
                    },
                    {
                        "name": "Guillaume Rabusseau"
                    }
                ],
                "author_detail": {
                    "name": "Guillaume Rabusseau"
                },
                "author": "Guillaume Rabusseau"
            },
            {
                "id": "http://arxiv.org/abs/2512.05909v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05909v1",
                "title": "Learning the Cosmic Web: Graph-based Classification of Simulated Galaxies by their Dark Matter Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning the Cosmic Web: Graph-based Classification of Simulated Galaxies by their Dark Matter Environments"
                },
                "updated": "2025-12-05T17:44:39Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    17,
                    44,
                    39,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05909v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05909v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We present a novel graph-based machine learning classifier for identifying the dark matter cosmic web environments of galaxies. Large galaxy surveys offer comprehensive statistical views of how galaxy properties are shaped by large-scale structure, but this requires robust classifications of galaxies' cosmic web environments. Using stellar mass-selected IllustrisTNG-300 galaxies, we apply a three-stage, simulation-based framework to link galaxies to the total (mainly dark) underlying matter distribution. Here, we apply the following three steps: First, we assign the positions of simulated galaxies to a void, wall, filament, or cluster environment using the T-Web classification of the underlying matter distribution. Second, we construct a Delaunay triangulation of the galaxy distribution to summarise the local geometric structure with ten graph metrics for each galaxy. Third, we train a graph attention network (GAT) on each galaxy's graph metrics to predict its cosmic web environment. For galaxies with stellar mass $\\mathrm{>10^9 M_{\\odot}}$, our GAT+ model achieves an accuracy of $85\\,\\%$, outperforming graph-agnostic multilayer perceptrons and graph convolutional networks. Our results demonstrate that graph-based representations of galaxy positions provide a powerful and physically meaningful way to infer dark matter environments. We plan to apply this simulation-based graph modelling to investigate how the properties of observed galaxies from the Dark Energy Spectroscopic Instrument (DESI) survey are influenced by their dark matter environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a novel graph-based machine learning classifier for identifying the dark matter cosmic web environments of galaxies. Large galaxy surveys offer comprehensive statistical views of how galaxy properties are shaped by large-scale structure, but this requires robust classifications of galaxies' cosmic web environments. Using stellar mass-selected IllustrisTNG-300 galaxies, we apply a three-stage, simulation-based framework to link galaxies to the total (mainly dark) underlying matter distribution. Here, we apply the following three steps: First, we assign the positions of simulated galaxies to a void, wall, filament, or cluster environment using the T-Web classification of the underlying matter distribution. Second, we construct a Delaunay triangulation of the galaxy distribution to summarise the local geometric structure with ten graph metrics for each galaxy. Third, we train a graph attention network (GAT) on each galaxy's graph metrics to predict its cosmic web environment. For galaxies with stellar mass $\\mathrm{>10^9 M_{\\odot}}$, our GAT+ model achieves an accuracy of $85\\,\\%$, outperforming graph-agnostic multilayer perceptrons and graph convolutional networks. Our results demonstrate that graph-based representations of galaxy positions provide a powerful and physically meaningful way to infer dark matter environments. We plan to apply this simulation-based graph modelling to investigate how the properties of observed galaxies from the Dark Energy Spectroscopic Instrument (DESI) survey are influenced by their dark matter environments."
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T17:44:39Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    17,
                    44,
                    39,
                    4,
                    339,
                    0
                ],
                "arxiv_comment": "15 pages, 7 figures, 9 tables, submitted to Royal Astronomical Society Techniques and Instruments",
                "arxiv_primary_category": {
                    "term": "astro-ph.GA"
                },
                "authors": [
                    {
                        "name": "Dakshesh Kololgi"
                    },
                    {
                        "name": "Krishna Naidoo"
                    },
                    {
                        "name": "Amelie Saintonge"
                    },
                    {
                        "name": "Ofer Lahav"
                    }
                ],
                "author_detail": {
                    "name": "Ofer Lahav"
                },
                "author": "Ofer Lahav"
            },
            {
                "id": "http://arxiv.org/abs/2512.05908v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05908v1",
                "title": "Natural Language Summarization Enables Multi-Repository Bug Localization by LLMs in Microservice Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural Language Summarization Enables Multi-Repository Bug Localization by LLMs in Microservice Architectures"
                },
                "updated": "2025-12-05T17:42:09Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    17,
                    42,
                    9,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05908v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05908v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Bug localization in multi-repository microservice architectures is challenging due to the semantic gap between natural language bug reports and code, LLM context limitations, and the need to first identify the correct repository. We propose reframing this as a natural language reasoning task by transforming codebases into hierarchical NL summaries and performing NL-to-NL search instead of cross-modal retrieval. Our approach builds context-aware summaries at file, directory, and repository levels, then uses a two-phase search: first routing bug reports to relevant repositories, then performing top-down localization within those repositories. Evaluated on DNext, an industrial system with 46 repositories and 1.1M lines of code, our method achieves Pass@10 of 0.82 and MRR of 0.50, significantly outperforming retrieval baselines and agentic RAG systems like GitHub Copilot and Cursor. This work demonstrates that engineered natural language representations can be more effective than raw source code for scalable bug localization, providing an interpretable repository -> directory -> file search path, which is vital for building trust in enterprise AI tools by providing essential transparency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bug localization in multi-repository microservice architectures is challenging due to the semantic gap between natural language bug reports and code, LLM context limitations, and the need to first identify the correct repository. We propose reframing this as a natural language reasoning task by transforming codebases into hierarchical NL summaries and performing NL-to-NL search instead of cross-modal retrieval. Our approach builds context-aware summaries at file, directory, and repository levels, then uses a two-phase search: first routing bug reports to relevant repositories, then performing top-down localization within those repositories. Evaluated on DNext, an industrial system with 46 repositories and 1.1M lines of code, our method achieves Pass@10 of 0.82 and MRR of 0.50, significantly outperforming retrieval baselines and agentic RAG systems like GitHub Copilot and Cursor. This work demonstrates that engineered natural language representations can be more effective than raw source code for scalable bug localization, providing an interpretable repository -> directory -> file search path, which is vital for building trust in enterprise AI tools by providing essential transparency."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T17:42:09Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    17,
                    42,
                    9,
                    4,
                    339,
                    0
                ],
                "arxiv_comment": "Accepted at LLM4Code Workshop, ICSE 2026",
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Amirkia Rafiei Oskooei"
                    },
                    {
                        "name": "S. Selcan Yukcu"
                    },
                    {
                        "name": "Mehmet Cevheri Bozoglan"
                    },
                    {
                        "name": "Mehmet S. Aktas"
                    }
                ],
                "author_detail": {
                    "name": "Mehmet S. Aktas"
                },
                "author": "Mehmet S. Aktas"
            },
            {
                "id": "http://arxiv.org/abs/2512.05907v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05907v1",
                "title": "From Text to Returns: Using Large Language Models for Mutual Fund Portfolio Optimization and Risk-Adjusted Allocation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Text to Returns: Using Large Language Models for Mutual Fund Portfolio Optimization and Risk-Adjusted Allocation"
                },
                "updated": "2025-12-05T17:41:34Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    17,
                    41,
                    34,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05907v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05907v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Generative AI (GenAI) has enormous potential for improving two critical areas in investing, namely portfolio optimization (choosing the best combination of assets) and risk management (protecting those investments). Our study works at this intersection, using Large Language Models (LLMs) to upgrade how financial decisions are traditionally made. This research specifically tested how well advanced LLMs like Microsoft Phi 2, Mistral 7B, and Zypher 7B can create practical, risk-aware strategies for investing mutual funds in different sectors of the economy. Our method is sophisticated: it combines a Retrieval-Augmented Generation (RAG) pipeline, which enables the LLM to check external, real-time data with standard financial optimization methods. The model's advice is context-aware because we feed it large economic signals, like changes in the global economy. The Zypher 7B model was the clear winner. It consistently produced strategies that maximized investment returns while delivering better risk-adjusted results than the other models. Its ability to process complex relationships and contextual information makes it a highly powerful tool for financial allocation. In conclusion, our findings show that GenAI substantially improves performance over basic allocation methods. By connecting GenAI to real-world financial applications, this work lays the groundwork for creating smarter, more efficient, and more adaptable solutions for asset management professionals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI (GenAI) has enormous potential for improving two critical areas in investing, namely portfolio optimization (choosing the best combination of assets) and risk management (protecting those investments). Our study works at this intersection, using Large Language Models (LLMs) to upgrade how financial decisions are traditionally made. This research specifically tested how well advanced LLMs like Microsoft Phi 2, Mistral 7B, and Zypher 7B can create practical, risk-aware strategies for investing mutual funds in different sectors of the economy. Our method is sophisticated: it combines a Retrieval-Augmented Generation (RAG) pipeline, which enables the LLM to check external, real-time data with standard financial optimization methods. The model's advice is context-aware because we feed it large economic signals, like changes in the global economy. The Zypher 7B model was the clear winner. It consistently produced strategies that maximized investment returns while delivering better risk-adjusted results than the other models. Its ability to process complex relationships and contextual information makes it a highly powerful tool for financial allocation. In conclusion, our findings show that GenAI substantially improves performance over basic allocation methods. By connecting GenAI to real-world financial applications, this work lays the groundwork for creating smarter, more efficient, and more adaptable solutions for asset management professionals."
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T17:41:34Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    17,
                    41,
                    34,
                    4,
                    339,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE"
                },
                "authors": [
                    {
                        "name": "Abrar Hossain Mufakir Qamar Ansari Haziq Jeelani Monia Digra Fayeq Jeelani Syed"
                    }
                ],
                "author_detail": {
                    "name": "Abrar Hossain Mufakir Qamar Ansari Haziq Jeelani Monia Digra Fayeq Jeelani Syed"
                },
                "author": "Abrar Hossain Mufakir Qamar Ansari Haziq Jeelani Monia Digra Fayeq Jeelani Syed"
            },
            {
                "id": "http://arxiv.org/abs/2510.04996v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.04996v3",
                "title": "Reinforce-Ada: An Adaptive Sampling Framework under Non-linear RL Objectives",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforce-Ada: An Adaptive Sampling Framework under Non-linear RL Objectives"
                },
                "updated": "2025-12-05T17:41:34Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    17,
                    41,
                    34,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.04996v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.04996v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Reinforcement learning (RL) for large language model reasoning is frequently hindered by signal loss, a phenomenon where standard uniform sampling with small group sizes fails to uncover informative learning signals for difficult prompts. We demonstrate that this collapse is a statistical artifact of undersampling rather than an inherent model limitation. To address this systematically, we introduce a theoretical framework based on optimizing a non-linear RL objective (e.g., log-likelihood). We show that this objective naturally induces a weighted gradient estimator that prioritizes difficult prompts, which can be robustly realized through adaptive sampling. Guided by this framework, we propose Reinforce-Ada, a family of algorithms that dynamically allocates inference budgets based on prompt difficulty, effectively scaling up RL compute to where it is needed most. Unlike passive filtering methods that discard low-signal prompts, Reinforce-Ada actively invests compute to recover them. We introduce two efficient realizations: an estimation-based approach and a model-free sequential sampling approach. Extensive experiments across multiple benchmarks show that Reinforce-Ada significantly outperforms uniform baselines like GRPO, recovering lost signals and accelerating convergence by up to $2\\times$ while maintaining the same total inference budget. Code is available at https://github.com/RLHFlow/Reinforce-Ada.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) for large language model reasoning is frequently hindered by signal loss, a phenomenon where standard uniform sampling with small group sizes fails to uncover informative learning signals for difficult prompts. We demonstrate that this collapse is a statistical artifact of undersampling rather than an inherent model limitation. To address this systematically, we introduce a theoretical framework based on optimizing a non-linear RL objective (e.g., log-likelihood). We show that this objective naturally induces a weighted gradient estimator that prioritizes difficult prompts, which can be robustly realized through adaptive sampling. Guided by this framework, we propose Reinforce-Ada, a family of algorithms that dynamically allocates inference budgets based on prompt difficulty, effectively scaling up RL compute to where it is needed most. Unlike passive filtering methods that discard low-signal prompts, Reinforce-Ada actively invests compute to recover them. We introduce two efficient realizations: an estimation-based approach and a model-free sequential sampling approach. Extensive experiments across multiple benchmarks show that Reinforce-Ada significantly outperforms uniform baselines like GRPO, recovering lost signals and accelerating convergence by up to $2\\times$ while maintaining the same total inference budget. Code is available at https://github.com/RLHFlow/Reinforce-Ada."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-06T16:34:09Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    16,
                    34,
                    9,
                    0,
                    279,
                    0
                ],
                "arxiv_comment": "27 pages, 10 figures",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Wei Xiong"
                    },
                    {
                        "name": "Chenlu Ye"
                    },
                    {
                        "name": "Baohao Liao"
                    },
                    {
                        "name": "Hanze Dong"
                    },
                    {
                        "name": "Xinxing Xu"
                    },
                    {
                        "name": "Christof Monz"
                    },
                    {
                        "name": "Jiang Bian"
                    },
                    {
                        "name": "Nan Jiang"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2512.05902v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05902v1",
                "title": "Quantitatively mapping the Eady model onto a two-layer quasi-geostrophic model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantitatively mapping the Eady model onto a two-layer quasi-geostrophic model"
                },
                "updated": "2025-12-05T17:33:23Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    17,
                    33,
                    23,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05902v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05902v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The two-layer quasigeostrophic model (2LQG) and the Eady model are two idealized systems illustrating the baroclinic instability of atmospheric jets and ocean currents. The two setups share many ingredients -- background vertically sheared zonal flow of density-stratified fluid in a rapidly rotating frame -- while differing in complexity and dimensionality. The Eady model has a continuous vertical direction, with baroclinic turbulence induced by boundary potential vorticity (PV) gradients at top and bottom. By contrast, the 2LQG sytem typically models baroclinic instability induced by interior PV gradients. This distinction challenges our ability to clearly identify a couple of 'modes' through which the Eady dynamics could be inferred from a simpler 2LQG system. In the present study, we show that this difficulty can be circumvented in the turbulent regime arising for weak bottom drag. Namely, guided by the common organization of both systems into a gas of coherent vortices, we identify a quantitative mapping between the Eady and the 2LQG models. The mapping allows for parameter-free predictions of the eddy diffusivity of the Eady model based on the knowledge of the 2LQG diffusivity. We illustrate these results using numerical simulations of the Eady and 2LQG models with linear or quadratic bottom drag.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The two-layer quasigeostrophic model (2LQG) and the Eady model are two idealized systems illustrating the baroclinic instability of atmospheric jets and ocean currents. The two setups share many ingredients -- background vertically sheared zonal flow of density-stratified fluid in a rapidly rotating frame -- while differing in complexity and dimensionality. The Eady model has a continuous vertical direction, with baroclinic turbulence induced by boundary potential vorticity (PV) gradients at top and bottom. By contrast, the 2LQG sytem typically models baroclinic instability induced by interior PV gradients. This distinction challenges our ability to clearly identify a couple of 'modes' through which the Eady dynamics could be inferred from a simpler 2LQG system. In the present study, we show that this difficulty can be circumvented in the turbulent regime arising for weak bottom drag. Namely, guided by the common organization of both systems into a gas of coherent vortices, we identify a quantitative mapping between the Eady and the 2LQG models. The mapping allows for parameter-free predictions of the eddy diffusivity of the Eady model based on the knowledge of the 2LQG diffusivity. We illustrate these results using numerical simulations of the Eady and 2LQG models with linear or quadratic bottom drag."
                },
                "tags": [
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ao-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T17:33:23Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    17,
                    33,
                    23,
                    4,
                    339,
                    0
                ],
                "arxiv_comment": "18 pages, 6 figures",
                "arxiv_primary_category": {
                    "term": "physics.flu-dyn"
                },
                "authors": [
                    {
                        "name": "Julie Meunier"
                    },
                    {
                        "name": "Basile Gallet"
                    }
                ],
                "author_detail": {
                    "name": "Basile Gallet"
                },
                "author": "Basile Gallet"
            },
            {
                "id": "http://arxiv.org/abs/2511.05811v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.05811v2",
                "title": "MOSS: Efficient and Accurate FP8 LLM Training with Microscaling and Automatic Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MOSS: Efficient and Accurate FP8 LLM Training with Microscaling and Automatic Scaling"
                },
                "updated": "2025-12-05T17:14:58Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    17,
                    14,
                    58,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.05811v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.05811v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Training large language models with FP8 formats offers significant efficiency gains. However, the reduced numerical precision of FP8 poses challenges for stable and accurate training. Current frameworks preserve training performance using mixed-granularity quantization, i.e., applying per-group quantization for activations and per-tensor/block quantization for weights. While effective, per-group quantization requires scaling along the inner dimension of matrix multiplication, introducing additional dequantization overhead. Moreover, these frameworks often rely on just-in-time scaling to dynamically adjust scaling factors based on the current data distribution. However, this online quantization is inefficient for FP8 training, as it involves multiple memory reads and writes that negate the performance benefits of FP8. To overcome these limitations, we propose MOSS, a novel FP8 training framework that ensures both efficiency and numerical stability. MOSS introduces two key innovations: (1) a two-level microscaling strategy for quantizing sensitive activations, which balances precision and dequantization cost by combining a high-precision global scale with compact, power-of-two local scales; and (2) automatic scaling for weights in linear layers, which eliminates the need for costly max-reduction operations by predicting and adjusting scaling factors during training. Leveraging these techniques, MOSS enables efficient FP8 training of a 7B parameter model, achieving performance comparable to the BF16 baseline while achieving up to 34% higher training throughput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training large language models with FP8 formats offers significant efficiency gains. However, the reduced numerical precision of FP8 poses challenges for stable and accurate training. Current frameworks preserve training performance using mixed-granularity quantization, i.e., applying per-group quantization for activations and per-tensor/block quantization for weights. While effective, per-group quantization requires scaling along the inner dimension of matrix multiplication, introducing additional dequantization overhead. Moreover, these frameworks often rely on just-in-time scaling to dynamically adjust scaling factors based on the current data distribution. However, this online quantization is inefficient for FP8 training, as it involves multiple memory reads and writes that negate the performance benefits of FP8. To overcome these limitations, we propose MOSS, a novel FP8 training framework that ensures both efficiency and numerical stability. MOSS introduces two key innovations: (1) a two-level microscaling strategy for quantizing sensitive activations, which balances precision and dequantization cost by combining a high-precision global scale with compact, power-of-two local scales; and (2) automatic scaling for weights in linear layers, which eliminates the need for costly max-reduction operations by predicting and adjusting scaling factors during training. Leveraging these techniques, MOSS enables efficient FP8 training of a 7B parameter model, achieving performance comparable to the BF16 baseline while achieving up to 34% higher training throughput."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-08T02:51:26Z",
                "published_parsed": [
                    2025,
                    11,
                    8,
                    2,
                    51,
                    26,
                    5,
                    312,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Hui-Ling Zhen"
                    },
                    {
                        "name": "Mingxuan Yuan"
                    },
                    {
                        "name": "Bei Yu"
                    }
                ],
                "author_detail": {
                    "name": "Bei Yu"
                },
                "author": "Bei Yu"
            },
            {
                "id": "http://arxiv.org/abs/2506.07413v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2506.07413v3",
                "title": "Variational Supervised Contrastive Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variational Supervised Contrastive Learning"
                },
                "updated": "2025-12-05T17:01:04Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    17,
                    1,
                    4,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2506.07413v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2506.07413v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Contrastive learning has proven to be highly efficient and adaptable in shaping representation spaces across diverse modalities by pulling similar samples together and pushing dissimilar ones apart. However, two key limitations persist: (1) Without explicit regulation of the embedding distribution, semantically related instances can inadvertently be pushed apart unless complementary signals guide pair selection, and (2) excessive reliance on large in-batch negatives and tailored augmentations hinders generalization. To address these limitations, we propose Variational Supervised Contrastive Learning (VarCon), which reformulates supervised contrastive learning as variational inference over latent class variables and maximizes a posterior-weighted evidence lower bound (ELBO) that replaces exhaustive pair-wise comparisons for efficient class-aware matching and grants fine-grained control over intra-class dispersion in the embedding space. Trained exclusively on image data, our experiments on CIFAR-10, CIFAR-100, ImageNet-100, and ImageNet-1K show that VarCon (1) achieves state-of-the-art performance for contrastive learning frameworks, reaching 79.36% Top-1 accuracy on ImageNet-1K and 78.29% on CIFAR-100 with a ResNet-50 encoder while converging in just 200 epochs; (2) yields substantially clearer decision boundaries and semantic organization in the embedding space, as evidenced by KNN classification, hierarchical clustering results, and transfer-learning assessments; and (3) demonstrates superior performance in few-shot learning than supervised baseline and superior robustness across various augmentation strategies. Our code is available at https://github.com/ziwenwang28/VarContrast.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contrastive learning has proven to be highly efficient and adaptable in shaping representation spaces across diverse modalities by pulling similar samples together and pushing dissimilar ones apart. However, two key limitations persist: (1) Without explicit regulation of the embedding distribution, semantically related instances can inadvertently be pushed apart unless complementary signals guide pair selection, and (2) excessive reliance on large in-batch negatives and tailored augmentations hinders generalization. To address these limitations, we propose Variational Supervised Contrastive Learning (VarCon), which reformulates supervised contrastive learning as variational inference over latent class variables and maximizes a posterior-weighted evidence lower bound (ELBO) that replaces exhaustive pair-wise comparisons for efficient class-aware matching and grants fine-grained control over intra-class dispersion in the embedding space. Trained exclusively on image data, our experiments on CIFAR-10, CIFAR-100, ImageNet-100, and ImageNet-1K show that VarCon (1) achieves state-of-the-art performance for contrastive learning frameworks, reaching 79.36% Top-1 accuracy on ImageNet-1K and 78.29% on CIFAR-100 with a ResNet-50 encoder while converging in just 200 epochs; (2) yields substantially clearer decision boundaries and semantic organization in the embedding space, as evidenced by KNN classification, hierarchical clustering results, and transfer-learning assessments; and (3) demonstrates superior performance in few-shot learning than supervised baseline and superior robustness across various augmentation strategies. Our code is available at https://github.com/ziwenwang28/VarContrast."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-06-09T04:19:12Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    4,
                    19,
                    12,
                    0,
                    160,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Ziwen Wang"
                    },
                    {
                        "name": "Jiajun Fan"
                    },
                    {
                        "name": "Thao Nguyen"
                    },
                    {
                        "name": "Heng Ji"
                    },
                    {
                        "name": "Ge Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ge Liu"
                },
                "author": "Ge Liu"
            },
            {
                "id": "http://arxiv.org/abs/2512.05883v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05883v1",
                "title": "The Bayesian Way: Uncertainty, Learning, and Statistical Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Bayesian Way: Uncertainty, Learning, and Statistical Reasoning"
                },
                "updated": "2025-12-05T16:59:25Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    16,
                    59,
                    25,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05883v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05883v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "This paper offers a comprehensive introduction to Bayesian inference, combining historical context, theoretical foundations, and core analytical examples. Beginning with Bayes' theorem and the philosophical distinctions between Bayesian and frequentist approaches, we develop the inferential framework for estimation, interval construction, hypothesis testing, and prediction. Through canonical models, we illustrate how prior information and observed data are formally integrated to yield posterior distributions. We also explore key concepts including loss functions, credible intervals, Bayes factors, identifiability, and asymptotic behavior. While emphasizing analytical tractability in classical settings, we outline modern extensions that rely on simulation-based methods and discuss challenges related to prior specification and model evaluation. Though focused on foundational ideas, this paper sets the stage for applying Bayesian methods in contemporary domains such as hierarchical modeling, nonparametrics, and structured applications in time series, spatial data, networks, and political science. The goal is to provide a rigorous yet accessible entry point for students and researchers seeking to adopt a Bayesian perspective in statistical practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper offers a comprehensive introduction to Bayesian inference, combining historical context, theoretical foundations, and core analytical examples. Beginning with Bayes' theorem and the philosophical distinctions between Bayesian and frequentist approaches, we develop the inferential framework for estimation, interval construction, hypothesis testing, and prediction. Through canonical models, we illustrate how prior information and observed data are formally integrated to yield posterior distributions. We also explore key concepts including loss functions, credible intervals, Bayes factors, identifiability, and asymptotic behavior. While emphasizing analytical tractability in classical settings, we outline modern extensions that rely on simulation-based methods and discuss challenges related to prior specification and model evaluation. Though focused on foundational ideas, this paper sets the stage for applying Bayesian methods in contemporary domains such as hierarchical modeling, nonparametrics, and structured applications in time series, spatial data, networks, and political science. The goal is to provide a rigorous yet accessible entry point for students and researchers seeking to adopt a Bayesian perspective in statistical practice."
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T16:59:25Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    16,
                    59,
                    25,
                    4,
                    339,
                    0
                ],
                "arxiv_comment": "56 pages, 1 table, 0 figures",
                "arxiv_primary_category": {
                    "term": "stat.ME"
                },
                "authors": [
                    {
                        "name": "Juan Sosa"
                    },
                    {
                        "name": "Carlos A. Martnez"
                    },
                    {
                        "name": "Danna Cruz"
                    }
                ],
                "author_detail": {
                    "name": "Danna Cruz"
                },
                "author": "Danna Cruz"
            },
            {
                "id": "http://arxiv.org/abs/2512.05881v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05881v1",
                "title": "DAE-HardNet: A Physics Constrained Neural Network Enforcing Differential-Algebraic Hard Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DAE-HardNet: A Physics Constrained Neural Network Enforcing Differential-Algebraic Hard Constraints"
                },
                "updated": "2025-12-05T16:55:54Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    16,
                    55,
                    54,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05881v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05881v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Traditional physics-informed neural networks (PINNs) do not always satisfy physics based constraints, especially when the constraints include differential operators. Rather, they minimize the constraint violations in a soft way. Strict satisfaction of differential-algebraic equations (DAEs) to embed domain knowledge and first-principles in data-driven models is generally challenging. This is because data-driven models consider the original functions to be black-box whose derivatives can only be obtained after evaluating the functions. We introduce DAE-HardNet, a physics-constrained (rather than simply physics-informed) neural network that learns both the functions and their derivatives simultaneously, while enforcing algebraic as well as differential constraints. This is done by projecting model predictions onto the constraint manifold using a differentiable projection layer. We apply DAE-HardNet to several systems and test problems governed by DAEs, including the dynamic Lotka-Volterra predator-prey system and transient heat conduction. We also show the ability of DAE-HardNet to estimate unknown parameters through a parameter estimation problem. Compared to multilayer perceptrons (MLPs) and PINNs, DAE-HardNet achieves orders of magnitude reduction in the physics loss while maintaining the prediction accuracy. It has the added benefits of learning the derivatives which improves the constrained learning of the backbone neural network prior to the projection layer. For specific problems, this suggests that the projection layer can be bypassed for faster inference. The current implementation and codes are available at https://github.com/SOULS-TAMU/DAE-HardNet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional physics-informed neural networks (PINNs) do not always satisfy physics based constraints, especially when the constraints include differential operators. Rather, they minimize the constraint violations in a soft way. Strict satisfaction of differential-algebraic equations (DAEs) to embed domain knowledge and first-principles in data-driven models is generally challenging. This is because data-driven models consider the original functions to be black-box whose derivatives can only be obtained after evaluating the functions. We introduce DAE-HardNet, a physics-constrained (rather than simply physics-informed) neural network that learns both the functions and their derivatives simultaneously, while enforcing algebraic as well as differential constraints. This is done by projecting model predictions onto the constraint manifold using a differentiable projection layer. We apply DAE-HardNet to several systems and test problems governed by DAEs, including the dynamic Lotka-Volterra predator-prey system and transient heat conduction. We also show the ability of DAE-HardNet to estimate unknown parameters through a parameter estimation problem. Compared to multilayer perceptrons (MLPs) and PINNs, DAE-HardNet achieves orders of magnitude reduction in the physics loss while maintaining the prediction accuracy. It has the added benefits of learning the derivatives which improves the constrained learning of the backbone neural network prior to the projection layer. For specific problems, this suggests that the projection layer can be bypassed for faster inference. The current implementation and codes are available at https://github.com/SOULS-TAMU/DAE-HardNet."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T16:55:54Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    16,
                    55,
                    54,
                    4,
                    339,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Rahul Golder"
                    },
                    {
                        "name": "Bimol Nath Roy"
                    },
                    {
                        "name": "M. M. Faruque Hasan"
                    }
                ],
                "author_detail": {
                    "name": "M. M. Faruque Hasan"
                },
                "author": "M. M. Faruque Hasan"
            },
            {
                "id": "http://arxiv.org/abs/2509.02327v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2509.02327v3",
                "title": "Variational Uncertainty Decomposition for In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variational Uncertainty Decomposition for In-Context Learning"
                },
                "updated": "2025-12-05T16:53:16Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    16,
                    53,
                    16,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2509.02327v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2509.02327v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "As large language models (LLMs) gain popularity in conducting prediction tasks in-context, understanding the sources of uncertainty in in-context learning becomes essential to ensuring reliability. The recent hypothesis of in-context learning performing predictive Bayesian inference opens the avenue for Bayesian uncertainty estimation, particularly for decomposing uncertainty into epistemic uncertainty due to lack of in-context data and aleatoric uncertainty inherent in the in-context prediction task. However, the decomposition idea remains under-explored due to the intractability of the latent parameter posterior from the underlying Bayesian model. In this work, we introduce a variational uncertainty decomposition framework for in-context learning without explicitly sampling from the latent parameter posterior, by optimising auxiliary queries as probes to obtain an upper bound to the aleatoric uncertainty of an LLM's in-context learning procedure, which also induces a lower bound to the epistemic uncertainty. Through experiments on synthetic and real-world tasks, we show quantitatively and qualitatively that the decomposed uncertainties obtained from our method exhibit desirable properties of epistemic and aleatoric uncertainty.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) gain popularity in conducting prediction tasks in-context, understanding the sources of uncertainty in in-context learning becomes essential to ensuring reliability. The recent hypothesis of in-context learning performing predictive Bayesian inference opens the avenue for Bayesian uncertainty estimation, particularly for decomposing uncertainty into epistemic uncertainty due to lack of in-context data and aleatoric uncertainty inherent in the in-context prediction task. However, the decomposition idea remains under-explored due to the intractability of the latent parameter posterior from the underlying Bayesian model. In this work, we introduce a variational uncertainty decomposition framework for in-context learning without explicitly sampling from the latent parameter posterior, by optimising auxiliary queries as probes to obtain an upper bound to the aleatoric uncertainty of an LLM's in-context learning procedure, which also induces a lower bound to the epistemic uncertainty. Through experiments on synthetic and real-world tasks, we show quantitatively and qualitatively that the decomposed uncertainties obtained from our method exhibit desirable properties of epistemic and aleatoric uncertainty."
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-09-02T13:53:09Z",
                "published_parsed": [
                    2025,
                    9,
                    2,
                    13,
                    53,
                    9,
                    1,
                    245,
                    0
                ],
                "arxiv_comment": "Neurips Version",
                "arxiv_primary_category": {
                    "term": "stat.ML"
                },
                "authors": [
                    {
                        "name": "I. Shavindra Jayasekera"
                    },
                    {
                        "name": "Jacob Si"
                    },
                    {
                        "name": "Filippo Valdettaro"
                    },
                    {
                        "name": "Wenlong Chen"
                    },
                    {
                        "name": "A. Aldo Faisal"
                    },
                    {
                        "name": "Yingzhen Li"
                    }
                ],
                "author_detail": {
                    "name": "Yingzhen Li"
                },
                "author": "Yingzhen Li"
            },
            {
                "id": "http://arxiv.org/abs/2512.05876v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05876v1",
                "title": "InstructMPC: A Human-LLM-in-the-Loop Framework for Context-Aware Power Grid Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InstructMPC: A Human-LLM-in-the-Loop Framework for Context-Aware Power Grid Control"
                },
                "updated": "2025-12-05T16:52:50Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    16,
                    52,
                    50,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05876v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05876v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The transition toward power grids with high renewable penetration demands context-aware decision making frameworks. Traditional operational paradigms, which rely on static optimization of history-based load forecasting, often fail to capture the complex nature of real-time operational conditions, such as operator-issued maintenance mandates, emergency topology changes, or event-driven load surges. To address this challenge, we introduce InstructMPC, a closed-loop framework that integrates Large Language Models~(LLMs) to generate context-aware predictions, enabling the controller to optimize power system operation. Our method employs a Contextual Disturbances Predictor~(CDP) module to translate contextual information into predictive disturbance trajectories, which are then incorporated into the Model Predictive Control~(MPC) optimization. Unlike conventional open-loop forecasting frameworks, InstructMPC features an online tuning mechanism where the predictor's parameters are continuously updated based on the realized control cost with a theoretical guarantee, achieving a regret bound of $O(\\sqrt{T \\log T})$ for linear dynamics when optimized via a tailored loss function, ensuring task-aware learning and adaption to non-stationary grid conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The transition toward power grids with high renewable penetration demands context-aware decision making frameworks. Traditional operational paradigms, which rely on static optimization of history-based load forecasting, often fail to capture the complex nature of real-time operational conditions, such as operator-issued maintenance mandates, emergency topology changes, or event-driven load surges. To address this challenge, we introduce InstructMPC, a closed-loop framework that integrates Large Language Models~(LLMs) to generate context-aware predictions, enabling the controller to optimize power system operation. Our method employs a Contextual Disturbances Predictor~(CDP) module to translate contextual information into predictive disturbance trajectories, which are then incorporated into the Model Predictive Control~(MPC) optimization. Unlike conventional open-loop forecasting frameworks, InstructMPC features an online tuning mechanism where the predictor's parameters are continuously updated based on the realized control cost with a theoretical guarantee, achieving a regret bound of $O(\\sqrt{T \\log T})$ for linear dynamics when optimized via a tailored loss function, ensuring task-aware learning and adaption to non-stationary grid conditions."
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T16:52:50Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    16,
                    52,
                    50,
                    4,
                    339,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY"
                },
                "authors": [
                    {
                        "name": "Ruixiang Wu"
                    },
                    {
                        "name": "Jiahao Ai"
                    },
                    {
                        "name": "Tinko Sebastian Bartels"
                    }
                ],
                "author_detail": {
                    "name": "Tinko Sebastian Bartels"
                },
                "author": "Tinko Sebastian Bartels"
            },
            {
                "id": "http://arxiv.org/abs/2512.05874v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05874v1",
                "title": "Topical issue on the intersection of low-energy nuclear structure and high-energy nuclear collisions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Topical issue on the intersection of low-energy nuclear structure and high-energy nuclear collisions"
                },
                "updated": "2025-12-05T16:51:16Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    16,
                    51,
                    16,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05874v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05874v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1140/epja/s10050-025-01715-1",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "High-energy heavy-ion physics and low-energy nuclear structure physics have historically been disconnected fields. The hydrodynamic description of the quark-gluon plasma (QGP) requires input from nuclear structure to model the initial states of the colliding nuclei. Advances in both theory and experiment now show that the hydrodynamic evolution of the QGP is sensitive to the detailed features of the colliding nuclei, with remarkable consequences for experimental observables.\n  The topical collection represents a joint effort between the low- and high-energy nuclear communities, reflecting the growing recognition that precision modeling of nuclear structure is essential for interpreting high-energy collision data. This new experimental approach opens outstanding opportunities to deepen our understanding of strong-interaction matter. Indeed, by probing many-body correlations of nucleons directly in the nuclear ground state, high-energy collisions provide a unique way to \"image\" nuclei, fully complementary to the techniques of low-energy experiments, where nuclear collectivity is usually inferred from spectroscopic information on excited states.\n  Do emergent many-body QCD phenomena in nuclei manifest consistently across experiments and energy scales? Addressing this question requires synergy between collider data and state-of-the-art nuclear structure calculations. In view of the rapid progress of ab initio methods based on low-energy effective field theories of QCD, the implications are far-reaching: heavy-ion collisions can probe nuclear forces, while nuclear structure insights refine our understanding of QGP dynamics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-energy heavy-ion physics and low-energy nuclear structure physics have historically been disconnected fields. The hydrodynamic description of the quark-gluon plasma (QGP) requires input from nuclear structure to model the initial states of the colliding nuclei. Advances in both theory and experiment now show that the hydrodynamic evolution of the QGP is sensitive to the detailed features of the colliding nuclei, with remarkable consequences for experimental observables.\n  The topical collection represents a joint effort between the low- and high-energy nuclear communities, reflecting the growing recognition that precision modeling of nuclear structure is essential for interpreting high-energy collision data. This new experimental approach opens outstanding opportunities to deepen our understanding of strong-interaction matter. Indeed, by probing many-body correlations of nucleons directly in the nuclear ground state, high-energy collisions provide a unique way to \"image\" nuclei, fully complementary to the techniques of low-energy experiments, where nuclear collectivity is usually inferred from spectroscopic information on excited states.\n  Do emergent many-body QCD phenomena in nuclei manifest consistently across experiments and energy scales? Addressing this question requires synergy between collider data and state-of-the-art nuclear structure calculations. In view of the rapid progress of ab initio methods based on low-energy effective field theories of QCD, the implications are far-reaching: heavy-ion collisions can probe nuclear forces, while nuclear structure insights refine our understanding of QGP dynamics."
                },
                "tags": [
                    {
                        "term": "nucl-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T16:51:16Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    16,
                    51,
                    16,
                    4,
                    339,
                    0
                ],
                "arxiv_comment": "3 pages",
                "arxiv_primary_category": {
                    "term": "nucl-th"
                },
                "arxiv_journal_ref": "Eur. Phys. J. A (2025) 61:237",
                "authors": [
                    {
                        "name": "T. Duguet"
                    },
                    {
                        "name": "G. Giacalone"
                    },
                    {
                        "name": "V. Som"
                    },
                    {
                        "name": "Y. Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Y. Zhou"
                },
                "author": "Y. Zhou",
                "arxiv_doi": "10.1140/epja/s10050-025-01715-1"
            },
            {
                "id": "http://arxiv.org/abs/2512.05872v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05872v1",
                "title": "Nuclear spin quenching of the $^2S_{1/2}\\rightarrow {^2}F_{7/2} $ electric octupole transition in $^{173}$Yb$^+$",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nuclear spin quenching of the $^2S_{1/2}\\rightarrow {^2}F_{7/2} $ electric octupole transition in $^{173}$Yb$^+$"
                },
                "updated": "2025-12-05T16:49:06Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    16,
                    49,
                    6,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05872v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05872v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We report the coherent excitation of the highly forbidden $^2S_{1/2} \\rightarrow {^2}F_{7/2}$ clock transition in the odd isotope $^{173}\\mathrm{Yb}^+$ with nuclear spin $I = 5/2$, and reveal the hyperfine-state-dependent, nuclear spin induced quenching of this transition. The inferred lifetime of the $F_e = 4$ hyperfine state is one order of magnitude shorter than the unperturbed ${^2}F_{7/2}$ clock state of $^{171}\\mathrm{Yb}^+$. This reduced lifetime lowers the required optical power for coherent excitation of the clock transition, thereby reducing the AC Stark shift caused by the clock laser. Using a 3-ion Coulomb crystal, we experimentally demonstrate an approximately 20-fold suppression of the AC Stark shift, a critical improvement for the scalability of future multi-ion $\\mathrm{Yb}^+$ clocks. Furthermore, we report the $|^2S_{1/2},F_g=3\\rangle~\\rightarrow~|^2F_{7/2},F_e=6\\rangle$ unquenched reference transition frequency as $642.11917656354(43)$ THz, along with the measured hyperfine splitting and calculated quadratic Zeeman sensitivities of the ${^2}F_{7/2}$ clock state. Our results pave the way toward multi-ion optical clocks and quantum computers based on $^{173}\\mathrm{Yb}^+$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report the coherent excitation of the highly forbidden $^2S_{1/2} \\rightarrow {^2}F_{7/2}$ clock transition in the odd isotope $^{173}\\mathrm{Yb}^+$ with nuclear spin $I = 5/2$, and reveal the hyperfine-state-dependent, nuclear spin induced quenching of this transition. The inferred lifetime of the $F_e = 4$ hyperfine state is one order of magnitude shorter than the unperturbed ${^2}F_{7/2}$ clock state of $^{171}\\mathrm{Yb}^+$. This reduced lifetime lowers the required optical power for coherent excitation of the clock transition, thereby reducing the AC Stark shift caused by the clock laser. Using a 3-ion Coulomb crystal, we experimentally demonstrate an approximately 20-fold suppression of the AC Stark shift, a critical improvement for the scalability of future multi-ion $\\mathrm{Yb}^+$ clocks. Furthermore, we report the $|^2S_{1/2},F_g=3\\rangle~\\rightarrow~|^2F_{7/2},F_e=6\\rangle$ unquenched reference transition frequency as $642.11917656354(43)$ THz, along with the measured hyperfine splitting and calculated quadratic Zeeman sensitivities of the ${^2}F_{7/2}$ clock state. Our results pave the way toward multi-ion optical clocks and quantum computers based on $^{173}\\mathrm{Yb}^+$."
                },
                "tags": [
                    {
                        "term": "physics.atom-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T16:49:06Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    16,
                    49,
                    6,
                    4,
                    339,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "physics.atom-ph"
                },
                "authors": [
                    {
                        "name": "Jialiang Yu"
                    },
                    {
                        "name": "Anand Prakash"
                    },
                    {
                        "name": "Clara Zyskind"
                    },
                    {
                        "name": "Ikbal A. Biswas"
                    },
                    {
                        "name": "Rattakorn Kaewuam"
                    },
                    {
                        "name": "Piyaphat Phoonthong"
                    },
                    {
                        "name": "Tanja E. Mehlstubler"
                    }
                ],
                "author_detail": {
                    "name": "Tanja E. Mehlstubler"
                },
                "author": "Tanja E. Mehlstubler"
            },
            {
                "id": "http://arxiv.org/abs/2512.05863v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05863v1",
                "title": "Optimizing Medical Question-Answering Systems: A Comparative Study of Fine-Tuned and Zero-Shot Large Language Models with RAG Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Medical Question-Answering Systems: A Comparative Study of Fine-Tuned and Zero-Shot Large Language Models with RAG Framework"
                },
                "updated": "2025-12-05T16:38:47Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    16,
                    38,
                    47,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05863v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05863v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Medical question-answering (QA) systems can benefit from advances in large language models (LLMs), but directly applying LLMs to the clinical domain poses challenges such as maintaining factual accuracy and avoiding hallucinations. In this paper, we present a retrieval-augmented generation (RAG) based medical QA system that combines domain-specific knowledge retrieval with open-source LLMs to answer medical questions. We fine-tune two state-of-the-art open LLMs (LLaMA~2 and Falcon) using Low-Rank Adaptation (LoRA) for efficient domain specialization. The system retrieves relevant medical literature to ground the LLM's answers, thereby improving factual correctness and reducing hallucinations. We evaluate the approach on benchmark datasets (PubMedQA and MedMCQA) and show that retrieval augmentation yields measurable improvements in answer accuracy compared to using LLMs alone. Our fine-tuned LLaMA~2 model achieves 71.8% accuracy on PubMedQA, substantially improving over the 55.4% zero-shot baseline, while maintaining transparency by providing source references. We also detail the system design and fine-tuning methodology, demonstrating that grounding answers in retrieved evidence reduces unsupported content by approximately 60%. These results highlight the potential of RAG-augmented open-source LLMs for reliable biomedical QA, pointing toward practical clinical informatics applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medical question-answering (QA) systems can benefit from advances in large language models (LLMs), but directly applying LLMs to the clinical domain poses challenges such as maintaining factual accuracy and avoiding hallucinations. In this paper, we present a retrieval-augmented generation (RAG) based medical QA system that combines domain-specific knowledge retrieval with open-source LLMs to answer medical questions. We fine-tune two state-of-the-art open LLMs (LLaMA~2 and Falcon) using Low-Rank Adaptation (LoRA) for efficient domain specialization. The system retrieves relevant medical literature to ground the LLM's answers, thereby improving factual correctness and reducing hallucinations. We evaluate the approach on benchmark datasets (PubMedQA and MedMCQA) and show that retrieval augmentation yields measurable improvements in answer accuracy compared to using LLMs alone. Our fine-tuned LLaMA~2 model achieves 71.8% accuracy on PubMedQA, substantially improving over the 55.4% zero-shot baseline, while maintaining transparency by providing source references. We also detail the system design and fine-tuning methodology, demonstrating that grounding answers in retrieved evidence reduces unsupported content by approximately 60%. These results highlight the potential of RAG-augmented open-source LLMs for reliable biomedical QA, pointing toward practical clinical informatics applications."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T16:38:47Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    16,
                    38,
                    47,
                    4,
                    339,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Tasnimul Hassan"
                    },
                    {
                        "name": "Md Faisal Karim"
                    },
                    {
                        "name": "Haziq Jeelani"
                    },
                    {
                        "name": "Elham Behnam"
                    },
                    {
                        "name": "Robert Green"
                    },
                    {
                        "name": "Fayeq Jeelani Syed"
                    }
                ],
                "author_detail": {
                    "name": "Fayeq Jeelani Syed"
                },
                "author": "Fayeq Jeelani Syed"
            },
            {
                "id": "http://arxiv.org/abs/2512.05860v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05860v1",
                "title": "High-contrast L-band Integral Field Spectroscopy of HD 33632 Ab",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-contrast L-band Integral Field Spectroscopy of HD 33632 Ab"
                },
                "updated": "2025-12-05T16:36:14Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    16,
                    36,
                    14,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05860v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05860v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We present LBTI/ALES 3.07-4.08 micron spectroscopic observations of HD~33632~Ab, a ~53 M_Jup directly imaged companion to an F8 star. Spectroscopic measurements of HD 33632 Ab now span 1-4 micron, and we perform the first spectroscopic analysis covering this full range. The data are compared to isolated brown dwarf template spectra, indicating that HD 33632 Ab is similar to L8/9 field brown dwarfs. Synthetic atmosphere model spectra from multiple model families are fit, with cloudy models providing the best fits, consistent with expectations for an L-dwarf. Evolutionary model predictions for the bulk properties of HD 33632 Ab are highly constrained by the precise dynamical mass found for the object. In particular, predictions for surface gravity are narrowly peaked, log(g)=5.21+/-0.05, and not dependent on the effects of clouds or cloud dispersion. We find significant tension between the surface gravities and object radii inferred from atmosphere model fits and those predicted by evolutionary models. We conclude with a comparison to the spectra of the HR 8799 c, d, and e, and emphasize the case that HD 33632 Ab, and other L/T transition directly imaged companions with constrained masses, will serve an essential role in understanding the complex physical processes governing the appearance of clouds in planetary atmospheres.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present LBTI/ALES 3.07-4.08 micron spectroscopic observations of HD~33632~Ab, a ~53 M_Jup directly imaged companion to an F8 star. Spectroscopic measurements of HD 33632 Ab now span 1-4 micron, and we perform the first spectroscopic analysis covering this full range. The data are compared to isolated brown dwarf template spectra, indicating that HD 33632 Ab is similar to L8/9 field brown dwarfs. Synthetic atmosphere model spectra from multiple model families are fit, with cloudy models providing the best fits, consistent with expectations for an L-dwarf. Evolutionary model predictions for the bulk properties of HD 33632 Ab are highly constrained by the precise dynamical mass found for the object. In particular, predictions for surface gravity are narrowly peaked, log(g)=5.21+/-0.05, and not dependent on the effects of clouds or cloud dispersion. We find significant tension between the surface gravities and object radii inferred from atmosphere model fits and those predicted by evolutionary models. We conclude with a comparison to the spectra of the HR 8799 c, d, and e, and emphasize the case that HD 33632 Ab, and other L/T transition directly imaged companions with constrained masses, will serve an essential role in understanding the complex physical processes governing the appearance of clouds in planetary atmospheres."
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T16:36:14Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    16,
                    36,
                    14,
                    4,
                    339,
                    0
                ],
                "arxiv_comment": "Accepted to the AAS Journals",
                "arxiv_primary_category": {
                    "term": "astro-ph.SR"
                },
                "authors": [
                    {
                        "name": "Jordan M. Stone"
                    },
                    {
                        "name": "Steve Ertel"
                    },
                    {
                        "name": "Travis Barman"
                    },
                    {
                        "name": "Andrew J. I. Skemer"
                    },
                    {
                        "name": "Jarron M. Leisenring"
                    },
                    {
                        "name": "Philip M. Hinz"
                    },
                    {
                        "name": "Charles E. Woodward"
                    },
                    {
                        "name": "Michael F. Skrutskie"
                    }
                ],
                "author_detail": {
                    "name": "Michael F. Skrutskie"
                },
                "author": "Michael F. Skrutskie"
            },
            {
                "id": "http://arxiv.org/abs/2402.13933v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2402.13933v4",
                "title": "Powerful Large-scale Inference in High Dimensional Mediation Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Powerful Large-scale Inference in High Dimensional Mediation Analysis"
                },
                "updated": "2025-12-05T16:33:05Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    16,
                    33,
                    5,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2402.13933v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2402.13933v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "In genome-wide epigenetic studies, exposures (e.g., Single Nucleotide Polymorphisms) affect outcomes (e.g., gene expression) through intermediate variables such as DNA methylation. Mediation analysis offers a way to study these intermediate variables and identify the presence or absence of causal mediation effects. Testing for mediation effects lead to a composite null hypothesis. Existing methods like the Sobel's test or the Max-P test are often underpowered because 1) statistical inference is often conducted based on distributions determined under a subset of the null and 2) they are not designed to shoulder the multiple testing burden. To tackle these issues, we introduce a technique called MLFDR (Mediation Analysis using Local False Discovery Rates) for high dimensional mediation analysis, which uses the local False Discovery Rates based on the coefficients of the structural equation model specifying the mediation relationship to construct a rejection region. We have shown theoretically as well as through simulation studies that in the high-dimensional setting, the new method of identifying the mediating variables controls the FDR asymptotically and performs better with respect to power than several existing methods such as DACT (Liu et al.)and JS-mixture (Dai et al).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In genome-wide epigenetic studies, exposures (e.g., Single Nucleotide Polymorphisms) affect outcomes (e.g., gene expression) through intermediate variables such as DNA methylation. Mediation analysis offers a way to study these intermediate variables and identify the presence or absence of causal mediation effects. Testing for mediation effects lead to a composite null hypothesis. Existing methods like the Sobel's test or the Max-P test are often underpowered because 1) statistical inference is often conducted based on distributions determined under a subset of the null and 2) they are not designed to shoulder the multiple testing burden. To tackle these issues, we introduce a technique called MLFDR (Mediation Analysis using Local False Discovery Rates) for high dimensional mediation analysis, which uses the local False Discovery Rates based on the coefficients of the structural equation model specifying the mediation relationship to construct a rejection region. We have shown theoretically as well as through simulation studies that in the high-dimensional setting, the new method of identifying the mediating variables controls the FDR asymptotically and performs better with respect to power than several existing methods such as DACT (Liu et al.)and JS-mixture (Dai et al)."
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-02-21T17:00:41Z",
                "published_parsed": [
                    2024,
                    2,
                    21,
                    17,
                    0,
                    41,
                    2,
                    52,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME"
                },
                "authors": [
                    {
                        "name": "Asmita Roy"
                    },
                    {
                        "name": "Xianyang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xianyang Zhang"
                },
                "author": "Xianyang Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2512.05855v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05855v1",
                "title": "Non-equilibrium formulation for inertial particles in turbulent swirling flows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-equilibrium formulation for inertial particles in turbulent swirling flows"
                },
                "updated": "2025-12-05T16:30:37Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    16,
                    30,
                    37,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05855v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05855v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We study the dynamics of inertial particles in turbulence using datasets obtained from both direct numerical simulations and laboratory experiments of turbulent swirling flows. By analyzing time series of particle velocity increments at different scales, we show that their evolution is consistent with a Markov process across the inertial range. This Markovian character enables a coarse-grained description of particle dynamics through a Fokker-Planck equation, from which we can extract drift and diffusion coefficients directly from the data. The inferred coefficients reveal scale-dependent relaxation and noise amplitudes, indicative of inertial filtering and intermittency effects. Beyond the kinematic description, we analyze the thermodynamic properties of particle trajectories by computing the trajectory-dependent entropy production. We show that the statistics of entropy fluctuations satisfy both the Integral Fluctuation Theorem and, under certain conditions, the Detailed Fluctuation Theorem. These results establish a quantitative bridge between stochastic thermodynamics and particle-laden flows, and open the door to modeling turbulent transport using effective stochastic theories constrained by data and physical consistency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the dynamics of inertial particles in turbulence using datasets obtained from both direct numerical simulations and laboratory experiments of turbulent swirling flows. By analyzing time series of particle velocity increments at different scales, we show that their evolution is consistent with a Markov process across the inertial range. This Markovian character enables a coarse-grained description of particle dynamics through a Fokker-Planck equation, from which we can extract drift and diffusion coefficients directly from the data. The inferred coefficients reveal scale-dependent relaxation and noise amplitudes, indicative of inertial filtering and intermittency effects. Beyond the kinematic description, we analyze the thermodynamic properties of particle trajectories by computing the trajectory-dependent entropy production. We show that the statistics of entropy fluctuations satisfy both the Integral Fluctuation Theorem and, under certain conditions, the Detailed Fluctuation Theorem. These results establish a quantitative bridge between stochastic thermodynamics and particle-laden flows, and open the door to modeling turbulent transport using effective stochastic theories constrained by data and physical consistency."
                },
                "tags": [
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nlin.CD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T16:30:37Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    16,
                    30,
                    37,
                    4,
                    339,
                    0
                ],
                "arxiv_comment": "18 pages, 9 figures",
                "arxiv_primary_category": {
                    "term": "physics.flu-dyn"
                },
                "authors": [
                    {
                        "name": "Bernardo L. Espaol"
                    },
                    {
                        "name": "Martin Obligado"
                    },
                    {
                        "name": "Joachim Peinke"
                    },
                    {
                        "name": "Marcelo Noseda"
                    },
                    {
                        "name": "Pablo J. Cobelli"
                    },
                    {
                        "name": "Pablo D. Mininni"
                    }
                ],
                "author_detail": {
                    "name": "Pablo D. Mininni"
                },
                "author": "Pablo D. Mininni"
            },
            {
                "id": "http://arxiv.org/abs/2512.05850v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05850v1",
                "title": "Dark matter implications from the XENONnT and LZ data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dark matter implications from the XENONnT and LZ data"
                },
                "updated": "2025-12-05T16:25:08Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    16,
                    25,
                    8,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05850v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05850v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We investigate a possible dark matter origin of the high-energy nuclear-recoil excess reported by recent liquid xenon experiments, including \\textnormal{XENONnT} and \\textnormal{LZ}, which cannot be explained by standard elastic spin-independent WIMP scattering. Using our unified \\texttt{DIAMX} framework, built on openly available data and likelihood models, we perform the first combined profile-likelihood fits to multiple WIMP-search datasets with a total exposure of 7.3~tonne $\\times$ year. We investigate that two broad classes of dark matter nucleon interactions, with velocity-dependent cross-section or inelastic (endo- and exothermic) scattering, can reproduce the observed high-energy recoil spectrum, reaching local significances up to $4$. We further quantify the impact of $^{124}$Xe double electron capture (DEC) backgrounds, finding that variations in the poorly known DEC charge yields can shift the inferred significances from below $1$ to $4$. We point out that extending the same analysis to \\textnormal{XENONnT} and \\textnormal{LZ} data with recoil energies up to 300\\,keV, once available, will provide a powerful test of the dark matter interpretation, since the $^{124}$Xe DEC background is expected to be negligible in this high-energy range.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate a possible dark matter origin of the high-energy nuclear-recoil excess reported by recent liquid xenon experiments, including \\textnormal{XENONnT} and \\textnormal{LZ}, which cannot be explained by standard elastic spin-independent WIMP scattering. Using our unified \\texttt{DIAMX} framework, built on openly available data and likelihood models, we perform the first combined profile-likelihood fits to multiple WIMP-search datasets with a total exposure of 7.3~tonne $\\times$ year. We investigate that two broad classes of dark matter nucleon interactions, with velocity-dependent cross-section or inelastic (endo- and exothermic) scattering, can reproduce the observed high-energy recoil spectrum, reaching local significances up to $4$. We further quantify the impact of $^{124}$Xe double electron capture (DEC) backgrounds, finding that variations in the poorly known DEC charge yields can shift the inferred significances from below $1$ to $4$. We point out that extending the same analysis to \\textnormal{XENONnT} and \\textnormal{LZ} data with recoil energies up to 300\\,keV, once available, will provide a powerful test of the dark matter interpretation, since the $^{124}$Xe DEC background is expected to be negligible in this high-energy range."
                },
                "tags": [
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T16:25:08Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    16,
                    25,
                    8,
                    4,
                    339,
                    0
                ],
                "arxiv_comment": "8 pages, 5 figures",
                "arxiv_primary_category": {
                    "term": "hep-ph"
                },
                "authors": [
                    {
                        "name": "Haipeng An"
                    },
                    {
                        "name": "Fei Gao"
                    },
                    {
                        "name": "Jia Liu"
                    },
                    {
                        "name": "Minghao Liu"
                    },
                    {
                        "name": "Haoming Nie"
                    },
                    {
                        "name": "Changlong Xu"
                    }
                ],
                "author_detail": {
                    "name": "Changlong Xu"
                },
                "author": "Changlong Xu"
            },
            {
                "id": "http://arxiv.org/abs/2506.19721v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2506.19721v2",
                "title": "MUSE-DARK-I: Dark matter halo properties of intermediate-z star-forming galaxies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MUSE-DARK-I: Dark matter halo properties of intermediate-z star-forming galaxies"
                },
                "updated": "2025-12-05T16:19:18Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    16,
                    19,
                    18,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2506.19721v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2506.19721v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "[Abridged] We analyse the dark matter (DM) halo properties of 127 0.3<z<1.5 star-forming galaxies (SFGs) down to low stellar masses (8<log(Mstar/Msun)<11), using data from the MUSE Hubble Ultra Deep Field Survey and photometry from HST and JWST. We employ a 3D forward modelling approach to analyse the morpho-kinematics of our sample, enabling measurement of individual rotation curves out to 2-3 times the effective radius. We perform a disk-halo decomposition with a 3D parametric model that includes stellar, gas, and DM components, with pressure support corrections. We validate our methodology on mock data cubes generated from idealised disk simulations. We select the best-fitting DM model among six density profiles, including the Navarro-Frenk-White and the generalised alpha-beta-gamma profile of Di Cintio et al. (2014, DC14). Our Bayesian analysis shows that DC14 performs as well as or better than the other profiles in >80% of the sample. We find that the kinematically inferred stellar masses agree with values from SED fitting. We find that 89% of galaxies have DM fractions >50%. For 66% of SFGs, we infer a DM inner slope, gamma < 0.5, indicating cored DM profiles, but no correlation is found between gamma and star formation rate of the sample. The stellar- and concentration-mass relations agree with theoretical expectations, but with larger scatter. We confirm the anticorrelation between halo scale radius and DM density. The halo scale radii and DM surface densities increase with Mstar, while DM densities stay constant. We find tentative evidence of an evolution of the DM density with z, which suggests that the DM halos of intermediate-z systems are denser than those of local galaxies. In contrast, the halo scale radii are z-invariant.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "[Abridged] We analyse the dark matter (DM) halo properties of 127 0.3<z<1.5 star-forming galaxies (SFGs) down to low stellar masses (8<log(Mstar/Msun)<11), using data from the MUSE Hubble Ultra Deep Field Survey and photometry from HST and JWST. We employ a 3D forward modelling approach to analyse the morpho-kinematics of our sample, enabling measurement of individual rotation curves out to 2-3 times the effective radius. We perform a disk-halo decomposition with a 3D parametric model that includes stellar, gas, and DM components, with pressure support corrections. We validate our methodology on mock data cubes generated from idealised disk simulations. We select the best-fitting DM model among six density profiles, including the Navarro-Frenk-White and the generalised alpha-beta-gamma profile of Di Cintio et al. (2014, DC14). Our Bayesian analysis shows that DC14 performs as well as or better than the other profiles in >80% of the sample. We find that the kinematically inferred stellar masses agree with values from SED fitting. We find that 89% of galaxies have DM fractions >50%. For 66% of SFGs, we infer a DM inner slope, gamma < 0.5, indicating cored DM profiles, but no correlation is found between gamma and star formation rate of the sample. The stellar- and concentration-mass relations agree with theoretical expectations, but with larger scatter. We confirm the anticorrelation between halo scale radius and DM density. The halo scale radii and DM surface densities increase with Mstar, while DM densities stay constant. We find tentative evidence of an evolution of the DM density with z, which suggests that the DM halos of intermediate-z systems are denser than those of local galaxies. In contrast, the halo scale radii are z-invariant."
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-06-24T15:31:35Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    15,
                    31,
                    35,
                    1,
                    175,
                    0
                ],
                "arxiv_comment": "Revised after submission to A&A",
                "arxiv_primary_category": {
                    "term": "astro-ph.GA"
                },
                "authors": [
                    {
                        "name": "B. I. Ciocan"
                    },
                    {
                        "name": "N. F. Bouch"
                    },
                    {
                        "name": "J. Fensch"
                    },
                    {
                        "name": "W. Mercier"
                    },
                    {
                        "name": "D. Krajnovi"
                    },
                    {
                        "name": "J. Richard"
                    },
                    {
                        "name": "T. Contini"
                    },
                    {
                        "name": "A. Jeanneau"
                    }
                ],
                "author_detail": {
                    "name": "A. Jeanneau"
                },
                "author": "A. Jeanneau"
            },
            {
                "id": "http://arxiv.org/abs/2511.01788v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.01788v2",
                "title": "Exploring ChatGPT's Capabilities, Stability, Potential and Risks in Conducting Psychological Counseling through Simulations in School Counseling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring ChatGPT's Capabilities, Stability, Potential and Risks in Conducting Psychological Counseling through Simulations in School Counseling"
                },
                "updated": "2025-12-05T16:18:39Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    16,
                    18,
                    39,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.01788v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.01788v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1108/MHDT-02-2025-0013",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "This study explores ChatGPT's capabilities, stability, and risks in simulating psychological counseling sessions in a school counseling context. Using scripted role-plays between a human counselor and an AI client, we examine how a large language model performs core counseling skills such as empathy, reflection, summarizing, and asking open-ended questions, as well as its ability to maintain therapeutic communication over time. We focus on how consistently ChatGPT can behave like a \"virtual client\" for school counselors in training, and how its responses might support or disrupt counselor skill development, supervision, and practice. At the same time, we analyze potential risks, including inaccurate or unsafe suggestions, over-compliance with counselor prompts, and the illusion of a competent therapist where no real professional judgment exists. The findings suggest that ChatGPT can serve as a low-cost, always-available training tool for practicing counseling techniques and interviewing skills in education and mental health settings, but it should not be viewed as a replacement for a human therapist or school counselor. We propose practical guidelines for educators, supervisors, and researchers who wish to use ChatGPT or similar LLM-based conversational agents in counseling training, highlighting how to leverage its potential while managing ethical, pedagogical, and psychological risks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study explores ChatGPT's capabilities, stability, and risks in simulating psychological counseling sessions in a school counseling context. Using scripted role-plays between a human counselor and an AI client, we examine how a large language model performs core counseling skills such as empathy, reflection, summarizing, and asking open-ended questions, as well as its ability to maintain therapeutic communication over time. We focus on how consistently ChatGPT can behave like a \"virtual client\" for school counselors in training, and how its responses might support or disrupt counselor skill development, supervision, and practice. At the same time, we analyze potential risks, including inaccurate or unsafe suggestions, over-compliance with counselor prompts, and the illusion of a competent therapist where no real professional judgment exists. The findings suggest that ChatGPT can serve as a low-cost, always-available training tool for practicing counseling techniques and interviewing skills in education and mental health settings, but it should not be viewed as a replacement for a human therapist or school counselor. We propose practical guidelines for educators, supervisors, and researchers who wish to use ChatGPT or similar LLM-based conversational agents in counseling training, highlighting how to leverage its potential while managing ethical, pedagogical, and psychological risks."
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-03T17:39:57Z",
                "published_parsed": [
                    2025,
                    11,
                    3,
                    17,
                    39,
                    57,
                    0,
                    307,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC"
                },
                "arxiv_journal_ref": "Mental Health and Digital Technologies, 2025",
                "authors": [
                    {
                        "name": "Yang Ni"
                    },
                    {
                        "name": "Yanzhuo Cao"
                    }
                ],
                "author_detail": {
                    "name": "Yanzhuo Cao"
                },
                "author": "Yanzhuo Cao",
                "arxiv_doi": "10.1108/MHDT-02-2025-0013"
            },
            {
                "id": "http://arxiv.org/abs/2512.05836v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05836v1",
                "title": "Using Large Language Models to Create Personalized Networks From Therapy Sessions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using Large Language Models to Create Personalized Networks From Therapy Sessions"
                },
                "updated": "2025-12-05T16:12:12Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    16,
                    12,
                    12,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05836v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05836v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recent advances in psychotherapy have focused on treatment personalization, such as by selecting treatment modules based on personalized networks. However, estimating personalized networks typically requires intensive longitudinal data, which is not always feasible. A solution to facilitate scalability of network-driven treatment personalization is leveraging LLMs. In this study, we present an end-to-end pipeline for automatically generating client networks from 77 therapy transcripts to support case conceptualization and treatment planning. We annotated 3364 psychological processes and their corresponding dimensions in therapy transcripts. Using these data, we applied in-context learning to jointly identify psychological processes and their dimensions. The method achieved high performance even with a few training examples. To organize the processes into networks, we introduced a two-step method that grouped them into clinically meaningful clusters. We then generated explanation-augmented relationships between clusters. Experts found that networks produced by our multi-step approach outperformed those built with direct prompting for clinical utility and interpretability, with up to 90% preferring our approach. In addition, the networks were rated favorably by experts, with scores for clinical relevance, novelty, and usefulness ranging from 72-75%. Our findings provide a proof of concept for using LLMs to create clinically relevant networks from therapy transcripts. Advantages of our approach include bottom-up case conceptualization from client utterances in therapy sessions and identification of latent themes. Networks generated from our pipeline may be used in clinical settings and supervision and training. Future research should examine whether these networks improve treatment outcomes relative to other methods of treatment personalization, including statistically estimated networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in psychotherapy have focused on treatment personalization, such as by selecting treatment modules based on personalized networks. However, estimating personalized networks typically requires intensive longitudinal data, which is not always feasible. A solution to facilitate scalability of network-driven treatment personalization is leveraging LLMs. In this study, we present an end-to-end pipeline for automatically generating client networks from 77 therapy transcripts to support case conceptualization and treatment planning. We annotated 3364 psychological processes and their corresponding dimensions in therapy transcripts. Using these data, we applied in-context learning to jointly identify psychological processes and their dimensions. The method achieved high performance even with a few training examples. To organize the processes into networks, we introduced a two-step method that grouped them into clinically meaningful clusters. We then generated explanation-augmented relationships between clusters. Experts found that networks produced by our multi-step approach outperformed those built with direct prompting for clinical utility and interpretability, with up to 90% preferring our approach. In addition, the networks were rated favorably by experts, with scores for clinical relevance, novelty, and usefulness ranging from 72-75%. Our findings provide a proof of concept for using LLMs to create clinically relevant networks from therapy transcripts. Advantages of our approach include bottom-up case conceptualization from client utterances in therapy sessions and identification of latent themes. Networks generated from our pipeline may be used in clinical settings and supervision and training. Future research should examine whether these networks improve treatment outcomes relative to other methods of treatment personalization, including statistically estimated networks."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T16:12:12Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    16,
                    12,
                    12,
                    4,
                    339,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Clarissa W. Ong"
                    },
                    {
                        "name": "Hiba Arnaout"
                    },
                    {
                        "name": "Kate Sheehan"
                    },
                    {
                        "name": "Estella Fox"
                    },
                    {
                        "name": "Eugen Owtscharow"
                    },
                    {
                        "name": "Iryna Gurevych"
                    }
                ],
                "author_detail": {
                    "name": "Iryna Gurevych"
                },
                "author": "Iryna Gurevych"
            },
            {
                "id": "http://arxiv.org/abs/2512.05831v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05831v1",
                "title": "Dissecting Embedding Bag Performance in DLRM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dissecting Embedding Bag Performance in DLRM Inference"
                },
                "updated": "2025-12-05T15:54:51Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    15,
                    54,
                    51,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05831v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05831v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "As the size of DLRMs gets larger, the models must be partitioned across multiple GPUs or nodes of GPUs due to the size limitation of total HBM memory that can be packaged in a GPU. This partitioning adds communication and synchronization overhead of sending and receiving data across GPUs. We use the NCCL and NVSHMEM libraries to measure the performance of an Embedding Bag kernel implemented on H100 GPUs. We compare its performance across diOerent batch sizes, number of tables, table sizes, pooling factors, and embedding dimensions. For a large embedding table that spans multiple GPUs, we project the performance slowdown from distributing an embedding table across multiple GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the size of DLRMs gets larger, the models must be partitioned across multiple GPUs or nodes of GPUs due to the size limitation of total HBM memory that can be packaged in a GPU. This partitioning adds communication and synchronization overhead of sending and receiving data across GPUs. We use the NCCL and NVSHMEM libraries to measure the performance of an Embedding Bag kernel implemented on H100 GPUs. We compare its performance across diOerent batch sizes, number of tables, table sizes, pooling factors, and embedding dimensions. For a large embedding table that spans multiple GPUs, we project the performance slowdown from distributing an embedding table across multiple GPUs."
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T15:54:51Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    15,
                    54,
                    51,
                    4,
                    339,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF"
                },
                "authors": [
                    {
                        "name": "Chandrish Ambati"
                    },
                    {
                        "name": "Jing Ding"
                    },
                    {
                        "name": "Trung Diep"
                    }
                ],
                "author_detail": {
                    "name": "Trung Diep"
                },
                "author": "Trung Diep"
            },
            {
                "id": "http://arxiv.org/abs/2505.16188v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2505.16188v2",
                "title": "SAE-SSV: Supervised Steering in Sparse Representation Spaces for Reliable Control of Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SAE-SSV: Supervised Steering in Sparse Representation Spaces for Reliable Control of Language Models"
                },
                "updated": "2025-12-05T15:53:16Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    15,
                    53,
                    16,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2505.16188v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2505.16188v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) have demonstrated impressive capabilities in natural language understanding and generation, but controlling their behavior reliably remains challenging, especially in open-ended generation settings. This paper introduces a novel supervised steering approach that operates in sparse, interpretable representation spaces. We employ sparse autoencoders (SAEs) to obtain sparse latent representations that aim to disentangle semantic attributes from model activations. Then we train linear classifiers to identify a small subspace of task-relevant dimensions in latent representations. Finally, we learn supervised steering vectors constrained to this subspace, optimized to align with target behaviors. Experiments across sentiment, truthfulness, and political polarity steering tasks with multiple LLMs demonstrate that our supervised steering vectors achieve higher success rates with minimal degradation in generation quality compared to existing methods. Further analysis reveals that a notably small subspace is sufficient for effective steering, enabling more targeted and interpretable interventions. Our implementation is publicly available at https://github.com/Ineedanamehere/SAE-SSV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated impressive capabilities in natural language understanding and generation, but controlling their behavior reliably remains challenging, especially in open-ended generation settings. This paper introduces a novel supervised steering approach that operates in sparse, interpretable representation spaces. We employ sparse autoencoders (SAEs) to obtain sparse latent representations that aim to disentangle semantic attributes from model activations. Then we train linear classifiers to identify a small subspace of task-relevant dimensions in latent representations. Finally, we learn supervised steering vectors constrained to this subspace, optimized to align with target behaviors. Experiments across sentiment, truthfulness, and political polarity steering tasks with multiple LLMs demonstrate that our supervised steering vectors achieve higher success rates with minimal degradation in generation quality compared to existing methods. Further analysis reveals that a notably small subspace is sufficient for effective steering, enabling more targeted and interpretable interventions. Our implementation is publicly available at https://github.com/Ineedanamehere/SAE-SSV."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-05-22T03:46:57Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    3,
                    46,
                    57,
                    3,
                    142,
                    0
                ],
                "arxiv_comment": "Accepted by EMNLP 2025",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Zirui He"
                    },
                    {
                        "name": "Mingyu Jin"
                    },
                    {
                        "name": "Bo Shen"
                    },
                    {
                        "name": "Ali Payani"
                    },
                    {
                        "name": "Yongfeng Zhang"
                    },
                    {
                        "name": "Mengnan Du"
                    }
                ],
                "author_detail": {
                    "name": "Mengnan Du"
                },
                "author": "Mengnan Du"
            },
            {
                "id": "http://arxiv.org/abs/2512.05812v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05812v1",
                "title": "Toward Efficient and Robust Behavior Models for Multi-Agent Driving Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward Efficient and Robust Behavior Models for Multi-Agent Driving Simulation"
                },
                "updated": "2025-12-05T15:32:36Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    15,
                    32,
                    36,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05812v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05812v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Scalable multi-agent driving simulation requires behavior models that are both realistic and computationally efficient. We address this by optimizing the behavior model that controls individual traffic participants. To improve efficiency, we adopt an instance-centric scene representation, where each traffic participant and map element is modeled in its own local coordinate frame. This design enables efficient, viewpoint-invariant scene encoding and allows static map tokens to be reused across simulation steps. To model interactions, we employ a query-centric symmetric context encoder with relative positional encodings between local frames. We use Adversarial Inverse Reinforcement Learning to learn the behavior model and propose an adaptive reward transformation that automatically balances robustness and realism during training. Experiments demonstrate that our approach scales efficiently with the number of tokens, significantly reducing training and inference times, while outperforming several agent-centric baselines in terms of positional accuracy and robustness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalable multi-agent driving simulation requires behavior models that are both realistic and computationally efficient. We address this by optimizing the behavior model that controls individual traffic participants. To improve efficiency, we adopt an instance-centric scene representation, where each traffic participant and map element is modeled in its own local coordinate frame. This design enables efficient, viewpoint-invariant scene encoding and allows static map tokens to be reused across simulation steps. To model interactions, we employ a query-centric symmetric context encoder with relative positional encodings between local frames. We use Adversarial Inverse Reinforcement Learning to learn the behavior model and propose an adaptive reward transformation that automatically balances robustness and realism during training. Experiments demonstrate that our approach scales efficiently with the number of tokens, significantly reducing training and inference times, while outperforming several agent-centric baselines in terms of positional accuracy and robustness."
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T15:32:36Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    15,
                    32,
                    36,
                    4,
                    339,
                    0
                ],
                "arxiv_comment": "This work has been submitted to the IEEE for possible publication",
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "authors": [
                    {
                        "name": "Fabian Konstantinidis"
                    },
                    {
                        "name": "Moritz Sackmann"
                    },
                    {
                        "name": "Ulrich Hofmann"
                    },
                    {
                        "name": "Christoph Stiller"
                    }
                ],
                "author_detail": {
                    "name": "Christoph Stiller"
                },
                "author": "Christoph Stiller"
            },
            {
                "id": "http://arxiv.org/abs/2512.05788v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05788v1",
                "title": "Task-Specific Trust Evaluation for Multi-Hop Collaborator Selection via GNN-Aided Distributed Agentic AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Task-Specific Trust Evaluation for Multi-Hop Collaborator Selection via GNN-Aided Distributed Agentic AI"
                },
                "updated": "2025-12-05T15:16:04Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    15,
                    16,
                    4,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05788v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05788v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The success of collaborative task completion among networked devices hinges on the effective selection of trustworthy collaborators. However, accurate task-specific trust evaluation of multi-hop collaborators can be extremely complex. The reason is that their trust evaluation is determined by a combination of diverse trust-related perspectives with different characteristics, including historical collaboration reliability, volatile and sensitive conditions of available resources for collaboration, as well as continuously evolving network topologies. To address this challenge, this paper presents a graph neural network (GNN)-aided distributed agentic AI (GADAI) framework, in which different aspects of devices' task-specific trustworthiness are separately evaluated and jointly integrated to facilitate multi-hop collaborator selection. GADAI first utilizes a GNN-assisted model to infer device trust from historical collaboration data. Specifically, it employs GNN to propagate and aggregate trust information among multi-hop neighbours, resulting in more accurate device reliability evaluation. Considering the dynamic and privacy-sensitive nature of device resources, a privacy-preserving resource evaluation mechanism is implemented using agentic AI. Each device hosts a large AI model-driven agent capable of autonomously determining whether its local resources meet the requirements of a given task, ensuring both task-specific and privacy-preserving trust evaluation. By combining the outcomes of these assessments, only the trusted devices can coordinate a task-oriented multi-hop cooperation path through their agents in a distributed manner. Experimental results show that our proposed GADAI outperforms the comparison algorithms in planning multi-hop paths that maximize the value of task completion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The success of collaborative task completion among networked devices hinges on the effective selection of trustworthy collaborators. However, accurate task-specific trust evaluation of multi-hop collaborators can be extremely complex. The reason is that their trust evaluation is determined by a combination of diverse trust-related perspectives with different characteristics, including historical collaboration reliability, volatile and sensitive conditions of available resources for collaboration, as well as continuously evolving network topologies. To address this challenge, this paper presents a graph neural network (GNN)-aided distributed agentic AI (GADAI) framework, in which different aspects of devices' task-specific trustworthiness are separately evaluated and jointly integrated to facilitate multi-hop collaborator selection. GADAI first utilizes a GNN-assisted model to infer device trust from historical collaboration data. Specifically, it employs GNN to propagate and aggregate trust information among multi-hop neighbours, resulting in more accurate device reliability evaluation. Considering the dynamic and privacy-sensitive nature of device resources, a privacy-preserving resource evaluation mechanism is implemented using agentic AI. Each device hosts a large AI model-driven agent capable of autonomously determining whether its local resources meet the requirements of a given task, ensuring both task-specific and privacy-preserving trust evaluation. By combining the outcomes of these assessments, only the trusted devices can coordinate a task-oriented multi-hop cooperation path through their agents in a distributed manner. Experimental results show that our proposed GADAI outperforms the comparison algorithms in planning multi-hop paths that maximize the value of task completion."
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T15:16:04Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    15,
                    16,
                    4,
                    4,
                    339,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY"
                },
                "authors": [
                    {
                        "name": "Botao Zhu"
                    },
                    {
                        "name": "Xianbin Wang"
                    },
                    {
                        "name": "Dusit Niyato"
                    }
                ],
                "author_detail": {
                    "name": "Dusit Niyato"
                },
                "author": "Dusit Niyato"
            },
            {
                "id": "http://arxiv.org/abs/2512.05784v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05784v1",
                "title": "Machine Learning-Informed 3+1 Sterile Neutrino Global Fits using Posterior Density Estimation of Electron Disappearance Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine Learning-Informed 3+1 Sterile Neutrino Global Fits using Posterior Density Estimation of Electron Disappearance Data"
                },
                "updated": "2025-12-05T15:13:03Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    15,
                    13,
                    3,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05784v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05784v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Global analyses of particle physics data are integral for validating and scrutinizing published results of experiments. Global fits of anomalous oscillation data which search for one or more eV-scale sterile neutrinos are particularly challenging both to evaluate and to reconcile in the global picture. Fits (especially joint ones) to oscillation data suffer from significant computational burdens, such as likelihood intractability, making traditional Markov Chain-Monte Carlo all but impossible. Given evidence both supporting and challenging beyond Standard Model physics across neutrino experiments of various baselines, energies, and detection techniques, the global search for sterile neutrinos requires additional tools in order to determine whether sterile neutrinos remain a viable solution to unexplained anomalies. Furthermore, both a Bayesian and frequentist interpretation of sterile neutrino data is needed for a complete assessment of longstanding tensions in the field. Techniques from the machine learning subfield of simulation-based inference have a natural application to such a problem. In this contribution, we illustrate some of the outstanding questions of the global picture of light sterile neutrinos by focusing on experiments searching with the disappearance of electron (anti)neutrinos, and look to posterior density estimation strategies to craft answers, including comparisons to a machine-learning-based frequentist approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Global analyses of particle physics data are integral for validating and scrutinizing published results of experiments. Global fits of anomalous oscillation data which search for one or more eV-scale sterile neutrinos are particularly challenging both to evaluate and to reconcile in the global picture. Fits (especially joint ones) to oscillation data suffer from significant computational burdens, such as likelihood intractability, making traditional Markov Chain-Monte Carlo all but impossible. Given evidence both supporting and challenging beyond Standard Model physics across neutrino experiments of various baselines, energies, and detection techniques, the global search for sterile neutrinos requires additional tools in order to determine whether sterile neutrinos remain a viable solution to unexplained anomalies. Furthermore, both a Bayesian and frequentist interpretation of sterile neutrino data is needed for a complete assessment of longstanding tensions in the field. Techniques from the machine learning subfield of simulation-based inference have a natural application to such a problem. In this contribution, we illustrate some of the outstanding questions of the global picture of light sterile neutrinos by focusing on experiments searching with the disappearance of electron (anti)neutrinos, and look to posterior density estimation strategies to craft answers, including comparisons to a machine-learning-based frequentist approach."
                },
                "tags": [
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T15:13:03Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    15,
                    13,
                    3,
                    4,
                    339,
                    0
                ],
                "arxiv_comment": "Substantially expanded and revised article-version of conference whitepaper arXiv:2501.08988",
                "arxiv_primary_category": {
                    "term": "hep-ph"
                },
                "authors": [
                    {
                        "name": "Joshua Villarreal"
                    },
                    {
                        "name": "Julia Woodward"
                    },
                    {
                        "name": "John Hardin"
                    },
                    {
                        "name": "Janet Conrad"
                    }
                ],
                "author_detail": {
                    "name": "Janet Conrad"
                },
                "author": "Janet Conrad"
            },
            {
                "id": "http://arxiv.org/abs/2512.05783v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05783v1",
                "title": "Curvature-Regularized Variational Autoencoder for 3D Scene Reconstruction from Sparse Depth",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Curvature-Regularized Variational Autoencoder for 3D Scene Reconstruction from Sparse Depth"
                },
                "updated": "2025-12-05T15:11:04Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    15,
                    11,
                    4,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05783v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05783v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "When depth sensors provide only 5% of needed measurements, reconstructing complete 3D scenes becomes difficult. Autonomous vehicles and robots cannot tolerate the geometric errors that sparse reconstruction introduces. We propose curvature regularization through a discrete Laplacian operator, achieving 18.1% better reconstruction accuracy than standard variational autoencoders. Our contribution challenges an implicit assumption in geometric deep learning: that combining multiple geometric constraints improves performance. A single well-designed regularization term not only matches but exceeds the effectiveness of complex multi-term formulations. The discrete Laplacian offers stable gradients and noise suppression with just 15% training overhead and zero inference cost. Code and models are available at https://github.com/Maryousefi/GeoVAE-3D.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When depth sensors provide only 5% of needed measurements, reconstructing complete 3D scenes becomes difficult. Autonomous vehicles and robots cannot tolerate the geometric errors that sparse reconstruction introduces. We propose curvature regularization through a discrete Laplacian operator, achieving 18.1% better reconstruction accuracy than standard variational autoencoders. Our contribution challenges an implicit assumption in geometric deep learning: that combining multiple geometric constraints improves performance. A single well-designed regularization term not only matches but exceeds the effectiveness of complex multi-term formulations. The discrete Laplacian offers stable gradients and noise suppression with just 15% training overhead and zero inference cost. Code and models are available at https://github.com/Maryousefi/GeoVAE-3D."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T15:11:04Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    15,
                    11,
                    4,
                    4,
                    339,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Maryam Yousefi"
                    },
                    {
                        "name": "Soodeh Bakhshandeh"
                    }
                ],
                "author_detail": {
                    "name": "Soodeh Bakhshandeh"
                },
                "author": "Soodeh Bakhshandeh"
            },
            {
                "id": "http://arxiv.org/abs/2512.03102v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.03102v2",
                "title": "Dynamic Correction of Erroneous State Estimates via Diffusion Bayesian Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Correction of Erroneous State Estimates via Diffusion Bayesian Exploration"
                },
                "updated": "2025-12-05T15:08:25Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    15,
                    8,
                    25,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.03102v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.03102v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "In emergency response and other high-stakes societal applications, early-stage state estimates critically shape downstream outcomes. Yet, these initial state estimates-often based on limited or biased information-can be severely misaligned with reality, constraining subsequent actions and potentially causing catastrophic delays, resource misallocation, and human harm. Under the stationary bootstrap baseline (zero transition and no rejuvenation), bootstrap particle filters exhibit Stationarity-Induced Posterior Support Invariance (S-PSI), wherein regions excluded by the initial prior remain permanently unexplorable, making corrections impossible even when new evidence contradicts current beliefs. While classical perturbations can in principle break this lock-in, they operate in an always-on fashion and may be inefficient. To overcome this, we propose a diffusion-driven Bayesian exploration framework that enables principled, real-time correction of early state estimation errors. Our method expands posterior support via entropy-regularized sampling and covariance-scaled diffusion. A Metropolis-Hastings check validates proposals and keeps inference adaptive to unexpected evidence. Empirical evaluations on realistic hazardous-gas localization tasks show that our approach matches reinforcement learning and planning baselines when priors are correct. It substantially outperforms classical SMC perturbations and RL-based methods under misalignment, and we provide theoretical guarantees that DEPF resolves S-PSI while maintaining statistical rigor.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In emergency response and other high-stakes societal applications, early-stage state estimates critically shape downstream outcomes. Yet, these initial state estimates-often based on limited or biased information-can be severely misaligned with reality, constraining subsequent actions and potentially causing catastrophic delays, resource misallocation, and human harm. Under the stationary bootstrap baseline (zero transition and no rejuvenation), bootstrap particle filters exhibit Stationarity-Induced Posterior Support Invariance (S-PSI), wherein regions excluded by the initial prior remain permanently unexplorable, making corrections impossible even when new evidence contradicts current beliefs. While classical perturbations can in principle break this lock-in, they operate in an always-on fashion and may be inefficient. To overcome this, we propose a diffusion-driven Bayesian exploration framework that enables principled, real-time correction of early state estimation errors. Our method expands posterior support via entropy-regularized sampling and covariance-scaled diffusion. A Metropolis-Hastings check validates proposals and keeps inference adaptive to unexpected evidence. Empirical evaluations on realistic hazardous-gas localization tasks show that our approach matches reinforcement learning and planning baselines when priors are correct. It substantially outperforms classical SMC perturbations and RL-based methods under misalignment, and we provide theoretical guarantees that DEPF resolves S-PSI while maintaining statistical rigor."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-01T22:08:26Z",
                "published_parsed": [
                    2025,
                    12,
                    1,
                    22,
                    8,
                    26,
                    0,
                    335,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Yiwei Shi"
                    },
                    {
                        "name": "Hongnan Ma"
                    },
                    {
                        "name": "Mengyue Yang"
                    },
                    {
                        "name": "Cunjia Liu"
                    },
                    {
                        "name": "Weiru Liu"
                    }
                ],
                "author_detail": {
                    "name": "Weiru Liu"
                },
                "author": "Weiru Liu"
            },
            {
                "id": "http://arxiv.org/abs/2512.05774v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05774v1",
                "title": "Active Video Perception: Iterative Evidence Seeking for Agentic Long Video Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Active Video Perception: Iterative Evidence Seeking for Agentic Long Video Understanding"
                },
                "updated": "2025-12-05T15:03:48Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    15,
                    3,
                    48,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05774v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05774v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Long video understanding (LVU) is challenging because answering real-world queries often depends on sparse, temporally dispersed cues buried in hours of mostly redundant and irrelevant content. While agentic pipelines improve video reasoning capabilities, prevailing frameworks rely on a query-agnostic captioner to perceive video information, which wastes computation on irrelevant content and blurs fine-grained temporal and spatial information. Motivated by active perception theory, we argue that LVU agents should actively decide what, when, and where to observe, and continuously assess whether the current observation is sufficient to answer the query. We present Active Video Perception (AVP), an evidence-seeking framework that treats the video as an interactive environment and acquires compact, queryrelevant evidence directly from pixels. Concretely, AVP runs an iterative plan-observe-reflect process with MLLM agents. In each round, a planner proposes targeted video interactions, an observer executes them to extract time-stamped evidence, and a reflector evaluates the sufficiency of the evidence for the query, either halting with an answer or triggering further observation. Across five LVU benchmarks, AVP achieves highest performance with significant improvements. Notably, AVP outperforms the best agentic method by 5.7% in average accuracy while only requires 18.4% inference time and 12.4% input tokens.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long video understanding (LVU) is challenging because answering real-world queries often depends on sparse, temporally dispersed cues buried in hours of mostly redundant and irrelevant content. While agentic pipelines improve video reasoning capabilities, prevailing frameworks rely on a query-agnostic captioner to perceive video information, which wastes computation on irrelevant content and blurs fine-grained temporal and spatial information. Motivated by active perception theory, we argue that LVU agents should actively decide what, when, and where to observe, and continuously assess whether the current observation is sufficient to answer the query. We present Active Video Perception (AVP), an evidence-seeking framework that treats the video as an interactive environment and acquires compact, queryrelevant evidence directly from pixels. Concretely, AVP runs an iterative plan-observe-reflect process with MLLM agents. In each round, a planner proposes targeted video interactions, an observer executes them to extract time-stamped evidence, and a reflector evaluates the sufficiency of the evidence for the query, either halting with an answer or triggering further observation. Across five LVU benchmarks, AVP achieves highest performance with significant improvements. Notably, AVP outperforms the best agentic method by 5.7% in average accuracy while only requires 18.4% inference time and 12.4% input tokens."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T15:03:48Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    15,
                    3,
                    48,
                    4,
                    339,
                    0
                ],
                "arxiv_comment": "Website: https://activevideoperception.github.io/",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Ziyang Wang"
                    },
                    {
                        "name": "Honglu Zhou"
                    },
                    {
                        "name": "Shijie Wang"
                    },
                    {
                        "name": "Junnan Li"
                    },
                    {
                        "name": "Caiming Xiong"
                    },
                    {
                        "name": "Silvio Savarese"
                    },
                    {
                        "name": "Mohit Bansal"
                    },
                    {
                        "name": "Michael S. Ryoo"
                    },
                    {
                        "name": "Juan Carlos Niebles"
                    }
                ],
                "author_detail": {
                    "name": "Juan Carlos Niebles"
                },
                "author": "Juan Carlos Niebles"
            },
            {
                "id": "http://arxiv.org/abs/2512.05765v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05765v1",
                "title": "The Missing Layer of AGI: From Pattern Alchemy to Coordination Physics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Missing Layer of AGI: From Pattern Alchemy to Coordination Physics"
                },
                "updated": "2025-12-05T14:51:17Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    14,
                    51,
                    17,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05765v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05765v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Influential critiques argue that Large Language Models (LLMs) are a dead end for AGI: \"mere pattern matchers\" structurally incapable of reasoning or planning. We argue this conclusion misidentifies the bottleneck: it confuses the ocean with the net. Pattern repositories are the necessary System-1 substrate; the missing component is a System-2 coordination layer that selects, constrains, and binds these patterns. We formalize this layer via UCCT, a theory of semantic anchoring that models reasoning as a phase transition governed by effective support (rho_d), representational mismatch (d_r), and an adaptive anchoring budget (gamma log k). Under this lens, ungrounded generation is simply an unbaited retrieval of the substrate's maximum likelihood prior, while \"reasoning\" emerges when anchors shift the posterior toward goal-directed constraints. We translate UCCT into architecture with MACI, a coordination stack that implements baiting (behavior-modulated debate), filtering (Socratic judging), and persistence (transactional memory). By reframing common objections as testable coordination failures, we argue that the path to AGI runs through LLMs, not around them.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Influential critiques argue that Large Language Models (LLMs) are a dead end for AGI: \"mere pattern matchers\" structurally incapable of reasoning or planning. We argue this conclusion misidentifies the bottleneck: it confuses the ocean with the net. Pattern repositories are the necessary System-1 substrate; the missing component is a System-2 coordination layer that selects, constrains, and binds these patterns. We formalize this layer via UCCT, a theory of semantic anchoring that models reasoning as a phase transition governed by effective support (rho_d), representational mismatch (d_r), and an adaptive anchoring budget (gamma log k). Under this lens, ungrounded generation is simply an unbaited retrieval of the substrate's maximum likelihood prior, while \"reasoning\" emerges when anchors shift the posterior toward goal-directed constraints. We translate UCCT into architecture with MACI, a coordination stack that implements baiting (behavior-modulated debate), filtering (Socratic judging), and persistence (transactional memory). By reframing common objections as testable coordination failures, we argue that the path to AGI runs through LLMs, not around them."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T14:51:17Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    14,
                    51,
                    17,
                    4,
                    339,
                    0
                ],
                "arxiv_comment": "13 pages, 3 figures",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Edward Y. Chang"
                    }
                ],
                "author_detail": {
                    "name": "Edward Y. Chang"
                },
                "author": "Edward Y. Chang"
            },
            {
                "id": "http://arxiv.org/abs/2512.05760v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05760v1",
                "title": "Evolutionary System 2 Reasoning: An Empirical Proof",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evolutionary System 2 Reasoning: An Empirical Proof"
                },
                "updated": "2025-12-05T14:47:57Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    14,
                    47,
                    57,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05760v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05760v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Machine intelligence marks the ultimate dream of making machines' intelligence comparable to human beings. While recent progress in Large Language Models (LLMs) show substantial specific skills for a wide array of downstream tasks, they more or less fall shorts in general intelligence. Following correlation between intelligence and system 2 reasoning (slow thinking), in this paper, we aim to answering a worthwhile research question: could machine intelligence such as LLMs be evolved to acquire reasoning ability (not specific skill) just like our human beings? To this end, we propose evolutionary reasoning optimization (ERO) framework which performs survival of the fittest over a population of LLMs to search for individual with strong reasoning ability. Given a reasoning task, ERO first initializes multiple LLMs as a population, after which an evolutionary strategy evolves the population to maximize quantified reasoning score of the best individual. Based on experiments on representative testsuites, we claim two surprising empirical discoveries: i) the latest LLMs such as GPT-5 still show limited system 2 reasoning ability; ii) with simple evolution-loop of ERO, a relatively weak model (Qwen-7B) could be enhanced to emerge powerful reasoning ability. Our project can be accessed at https://github.com/MetaEvo/ERO for reproduction needs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine intelligence marks the ultimate dream of making machines' intelligence comparable to human beings. While recent progress in Large Language Models (LLMs) show substantial specific skills for a wide array of downstream tasks, they more or less fall shorts in general intelligence. Following correlation between intelligence and system 2 reasoning (slow thinking), in this paper, we aim to answering a worthwhile research question: could machine intelligence such as LLMs be evolved to acquire reasoning ability (not specific skill) just like our human beings? To this end, we propose evolutionary reasoning optimization (ERO) framework which performs survival of the fittest over a population of LLMs to search for individual with strong reasoning ability. Given a reasoning task, ERO first initializes multiple LLMs as a population, after which an evolutionary strategy evolves the population to maximize quantified reasoning score of the best individual. Based on experiments on representative testsuites, we claim two surprising empirical discoveries: i) the latest LLMs such as GPT-5 still show limited system 2 reasoning ability; ii) with simple evolution-loop of ERO, a relatively weak model (Qwen-7B) could be enhanced to emerge powerful reasoning ability. Our project can be accessed at https://github.com/MetaEvo/ERO for reproduction needs."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T14:47:57Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    14,
                    47,
                    57,
                    4,
                    339,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Zeyuan Ma"
                    },
                    {
                        "name": "Wenqi Huang"
                    },
                    {
                        "name": "Guo-Huan Song"
                    },
                    {
                        "name": "Hongshu Guo"
                    },
                    {
                        "name": "Sijie Ma"
                    },
                    {
                        "name": "Zhiguang Cao"
                    },
                    {
                        "name": "Yue-Jiao Gong"
                    }
                ],
                "author_detail": {
                    "name": "Yue-Jiao Gong"
                },
                "author": "Yue-Jiao Gong"
            },
            {
                "id": "http://arxiv.org/abs/2512.05753v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05753v1",
                "title": "A Fast Anti-Jamming Cognitive Radar Deployment Algorithm Based on Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Fast Anti-Jamming Cognitive Radar Deployment Algorithm Based on Reinforcement Learning"
                },
                "updated": "2025-12-05T14:39:50Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    14,
                    39,
                    50,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05753v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05753v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The fast deployment of cognitive radar to counter jamming remains a critical challenge in modern warfare, where more efficient deployment leads to quicker detection of targets. Existing methods are primarily based on evolutionary algorithms, which are time-consuming and prone to falling into local optima. We tackle these drawbacks via the efficient inference of neural networks and propose a brand new framework: Fast Anti-Jamming Radar Deployment Algorithm (FARDA). We first model the radar deployment problem as an end-to-end task and design deep reinforcement learning algorithms to solve it, where we develop integrated neural modules to perceive heatmap information and a brand new reward format. Empirical results demonstrate that our method achieves coverage comparable to evolutionary algorithms while deploying radars approximately 7,000 times faster. Further ablation experiments confirm the necessity of each component of FARDA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The fast deployment of cognitive radar to counter jamming remains a critical challenge in modern warfare, where more efficient deployment leads to quicker detection of targets. Existing methods are primarily based on evolutionary algorithms, which are time-consuming and prone to falling into local optima. We tackle these drawbacks via the efficient inference of neural networks and propose a brand new framework: Fast Anti-Jamming Radar Deployment Algorithm (FARDA). We first model the radar deployment problem as an end-to-end task and design deep reinforcement learning algorithms to solve it, where we develop integrated neural modules to perceive heatmap information and a brand new reward format. Empirical results demonstrate that our method achieves coverage comparable to evolutionary algorithms while deploying radars approximately 7,000 times faster. Further ablation experiments confirm the necessity of each component of FARDA."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T14:39:50Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    14,
                    39,
                    50,
                    4,
                    339,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Wencheng Cai"
                    },
                    {
                        "name": "Xuchao Gao"
                    },
                    {
                        "name": "Congying Han"
                    },
                    {
                        "name": "Mingqiang Li"
                    },
                    {
                        "name": "Tiande Guo"
                    }
                ],
                "author_detail": {
                    "name": "Tiande Guo"
                },
                "author": "Tiande Guo"
            },
            {
                "id": "http://arxiv.org/abs/2512.05751v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05751v1",
                "title": "Exoplanet formation inference using conditional invertible neural networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exoplanet formation inference using conditional invertible neural networks"
                },
                "updated": "2025-12-05T14:38:34Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    14,
                    38,
                    34,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05751v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05751v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The interpretation of the origin of observed exoplanets is usually done only qualitatively due to uncertainties of key parameters in planet formation models. To allow a quantitative methodology which traces back in time to the planet birth locations, we train recently developed conditional invertible neural networks (cINN) on synthetic data from a global planet formation model which tracks growth from dust grains to evolved final giant planets. In addition to deterministic single planet formation runs, we also include gravitationally interacting planets in multiplanetary systems, which include some measure of chaos. For the latter case, we treat them as individual planets or choose the two or three planets most likely to be discovered by telescopes. We find that training on multiplanetary data, each planet treated as individual point, is promising. The single-planet data only covers a small range of planets and does not extrapolate well to planet properties not included in the training data. Extension to planetary systems will require more training data due to the higher dimensionality of the problem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The interpretation of the origin of observed exoplanets is usually done only qualitatively due to uncertainties of key parameters in planet formation models. To allow a quantitative methodology which traces back in time to the planet birth locations, we train recently developed conditional invertible neural networks (cINN) on synthetic data from a global planet formation model which tracks growth from dust grains to evolved final giant planets. In addition to deterministic single planet formation runs, we also include gravitationally interacting planets in multiplanetary systems, which include some measure of chaos. For the latter case, we treat them as individual planets or choose the two or three planets most likely to be discovered by telescopes. We find that training on multiplanetary data, each planet treated as individual point, is promising. The single-planet data only covers a small range of planets and does not extrapolate well to planet properties not included in the training data. Extension to planetary systems will require more training data due to the higher dimensionality of the problem."
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.data-an",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T14:38:34Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    14,
                    38,
                    34,
                    4,
                    339,
                    0
                ],
                "arxiv_comment": "10 pages, accepted poster for the Machine Learning and the Physical Sciences Workshop at the 39th conference on Neural Information Processing Systems (NeurIPS 2025)",
                "arxiv_primary_category": {
                    "term": "astro-ph.EP"
                },
                "arxiv_journal_ref": "Machine Learning and the Physical Sciences Workshop, 39th conference on Neural Information Processing Systems (NeurIPS 2025)",
                "authors": [
                    {
                        "name": "Remo Burn"
                    },
                    {
                        "name": "Victor F. Ksoll"
                    },
                    {
                        "name": "Hubert Klahr"
                    },
                    {
                        "name": "Thomas Henning"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Henning"
                },
                "author": "Thomas Henning"
            },
            {
                "id": "http://arxiv.org/abs/2512.05747v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05747v1",
                "title": "Capturing Classic Authorial Style in Long-Form Story Generation with GRPO Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Capturing Classic Authorial Style in Long-Form Story Generation with GRPO Fine-Tuning"
                },
                "updated": "2025-12-05T14:29:27Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    14,
                    29,
                    27,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05747v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05747v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recent advances in large language models (LLMs) show impressive performance in open-ended story generation, but fine-grained stylistic control remains limited. Existing methods often rely on shallow cues (e.g., names or topics) to simulate authorial style, without robust evaluation. In this work, we present a training framework for style-conditioned story generation using Group Relative Policy Optimization (GRPO) and a custom multi-reward setup. The style reward is derived from a fine-tuned sentence transformer using authorship verification (AV) signals, combined with content and completeness scores to stabilize long-form narrative generation. We conduct experiments using fiction by Mark Twain, a prominent 19th-century American author, with The Adventures of Huckleberry Finn serving as the reference style exemplar. Our 8B model outperforms larger baselines such as GPT-4o and Claude Sonnet 4 in AV-style metrics, achieving a style score of 0.628 and competitive content quality. Results demonstrate the feasibility of agentic stylistic generation with moderate model size and task-specific training. While the output is clearly style-aligned, narrative completeness remains a challenge, indicating future work is needed to better model global coherence and story resolution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) show impressive performance in open-ended story generation, but fine-grained stylistic control remains limited. Existing methods often rely on shallow cues (e.g., names or topics) to simulate authorial style, without robust evaluation. In this work, we present a training framework for style-conditioned story generation using Group Relative Policy Optimization (GRPO) and a custom multi-reward setup. The style reward is derived from a fine-tuned sentence transformer using authorship verification (AV) signals, combined with content and completeness scores to stabilize long-form narrative generation. We conduct experiments using fiction by Mark Twain, a prominent 19th-century American author, with The Adventures of Huckleberry Finn serving as the reference style exemplar. Our 8B model outperforms larger baselines such as GPT-4o and Claude Sonnet 4 in AV-style metrics, achieving a style score of 0.628 and competitive content quality. Results demonstrate the feasibility of agentic stylistic generation with moderate model size and task-specific training. While the output is clearly style-aligned, narrative completeness remains a challenge, indicating future work is needed to better model global coherence and story resolution."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T14:29:27Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    14,
                    29,
                    27,
                    4,
                    339,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Jinlong Liu"
                    },
                    {
                        "name": "Mohammed Bahja"
                    },
                    {
                        "name": "Venelin Kovatchev"
                    },
                    {
                        "name": "Mark Lee"
                    }
                ],
                "author_detail": {
                    "name": "Mark Lee"
                },
                "author": "Mark Lee"
            },
            {
                "id": "http://arxiv.org/abs/2512.05746v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05746v1",
                "title": "HQ-DM: Single Hadamard Transformation-Based Quantization-Aware Training for Low-Bit Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HQ-DM: Single Hadamard Transformation-Based Quantization-Aware Training for Low-Bit Diffusion Models"
                },
                "updated": "2025-12-05T14:28:40Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    14,
                    28,
                    40,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05746v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05746v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Diffusion models have demonstrated significant applications in the field of image generation. However, their high computational and memory costs pose challenges for deployment. Model quantization has emerged as a promising solution to reduce storage overhead and accelerate inference. Nevertheless, existing quantization methods for diffusion models struggle to mitigate outliers in activation matrices during inference, leading to substantial performance degradation under low-bit quantization scenarios. To address this, we propose HQ-DM, a novel Quantization-Aware Training framework that applies Single Hadamard Transformation to activation matrices. This approach effectively reduces activation outliers while preserving model performance under quantization. Compared to traditional Double Hadamard Transformation, our proposed scheme offers distinct advantages by seamlessly supporting INT convolution operations while preventing the amplification of weight outliers. For conditional generation on the ImageNet 256x256 dataset using the LDM-4 model, our W4A4 and W4A3 quantization schemes improve the Inception Score by 12.8% and 467.73%, respectively, over the existing state-of-the-art method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have demonstrated significant applications in the field of image generation. However, their high computational and memory costs pose challenges for deployment. Model quantization has emerged as a promising solution to reduce storage overhead and accelerate inference. Nevertheless, existing quantization methods for diffusion models struggle to mitigate outliers in activation matrices during inference, leading to substantial performance degradation under low-bit quantization scenarios. To address this, we propose HQ-DM, a novel Quantization-Aware Training framework that applies Single Hadamard Transformation to activation matrices. This approach effectively reduces activation outliers while preserving model performance under quantization. Compared to traditional Double Hadamard Transformation, our proposed scheme offers distinct advantages by seamlessly supporting INT convolution operations while preventing the amplification of weight outliers. For conditional generation on the ImageNet 256x256 dataset using the LDM-4 model, our W4A4 and W4A3 quantization schemes improve the Inception Score by 12.8% and 467.73%, respectively, over the existing state-of-the-art method."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T14:28:40Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    14,
                    28,
                    40,
                    4,
                    339,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Shizhuo Mao"
                    },
                    {
                        "name": "Hongtao Zou"
                    },
                    {
                        "name": "Qihu Xie"
                    },
                    {
                        "name": "Song Chen"
                    },
                    {
                        "name": "Yi Kang"
                    }
                ],
                "author_detail": {
                    "name": "Yi Kang"
                },
                "author": "Yi Kang"
            },
            {
                "id": "http://arxiv.org/abs/2512.05745v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05745v1",
                "title": "ARGUS: Defending Against Multimodal Indirect Prompt Injection via Steering Instruction-Following Behavior",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARGUS: Defending Against Multimodal Indirect Prompt Injection via Steering Instruction-Following Behavior"
                },
                "updated": "2025-12-05T14:26:45Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    14,
                    26,
                    45,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05745v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05745v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Multimodal Large Language Models (MLLMs) are increasingly vulnerable to multimodal Indirect Prompt Injection (IPI) attacks, which embed malicious instructions in images, videos, or audio to hijack model behavior. Existing defenses, designed primarily for text-only LLMs, are unsuitable for countering these multimodal threats, as they are easily bypassed, modality-dependent, or generalize poorly. Inspired by activation steering researches, we hypothesize that a robust, general defense independent of modality can be achieved by steering the model's behavior in the representation space. Through extensive experiments, we discover that the instruction-following behavior of MLLMs is encoded in a subspace. Steering along directions within this subspace can enforce adherence to user instructions, forming the basis of a defense. However, we also found that a naive defense direction could be coupled with a utility-degrading direction, and excessive intervention strength harms model performance. To address this, we propose ARGUS, which searches for an optimal defense direction within the safety subspace that decouples from the utility degradation direction, further combining adaptive strength steering to achieve a better safety-utility trade-off. ARGUS also introduces lightweight injection detection stage to activate the defense on-demand, and a post-filtering stage to verify defense success. Experimental results show that ARGUS can achieve robust defense against multimodal IPI while maximally preserving the MLLM's utility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) are increasingly vulnerable to multimodal Indirect Prompt Injection (IPI) attacks, which embed malicious instructions in images, videos, or audio to hijack model behavior. Existing defenses, designed primarily for text-only LLMs, are unsuitable for countering these multimodal threats, as they are easily bypassed, modality-dependent, or generalize poorly. Inspired by activation steering researches, we hypothesize that a robust, general defense independent of modality can be achieved by steering the model's behavior in the representation space. Through extensive experiments, we discover that the instruction-following behavior of MLLMs is encoded in a subspace. Steering along directions within this subspace can enforce adherence to user instructions, forming the basis of a defense. However, we also found that a naive defense direction could be coupled with a utility-degrading direction, and excessive intervention strength harms model performance. To address this, we propose ARGUS, which searches for an optimal defense direction within the safety subspace that decouples from the utility degradation direction, further combining adaptive strength steering to achieve a better safety-utility trade-off. ARGUS also introduces lightweight injection detection stage to activate the defense on-demand, and a post-filtering stage to verify defense success. Experimental results show that ARGUS can achieve robust defense against multimodal IPI while maximally preserving the MLLM's utility."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T14:26:45Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    14,
                    26,
                    45,
                    4,
                    339,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Weikai Lu"
                    },
                    {
                        "name": "Ziqian Zeng"
                    },
                    {
                        "name": "Kehua Zhang"
                    },
                    {
                        "name": "Haoran Li"
                    },
                    {
                        "name": "Huiping Zhuang"
                    },
                    {
                        "name": "Ruidong Wang"
                    },
                    {
                        "name": "Cen Chen"
                    },
                    {
                        "name": "Hao Peng"
                    }
                ],
                "author_detail": {
                    "name": "Hao Peng"
                },
                "author": "Hao Peng"
            },
            {
                "id": "http://arxiv.org/abs/2508.19478v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.19478v2",
                "title": "Bayesian Insights into Exchange and Restriction in Gray Matter Diffusion MRI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian Insights into Exchange and Restriction in Gray Matter Diffusion MRI"
                },
                "updated": "2025-12-05T14:26:27Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    14,
                    26,
                    27,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.19478v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.19478v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Biophysical models in diffusion MRI (dMRI) hold promise for characterizing gray matter tissue microstructure. Yet, the reliability of their parameter estimates remains largely under-studied, especially in models that incorporate water exchange. In this study, we investigate the accuracy, precision, and presence of degeneracy of two recently proposed gray matter models, NEXI and SANDIX, using established acquisition protocols, on both simulated and \\textit{in vivo} data. We employ $$GUIDE, a Bayesian inference framework based on deep learning, to quantify parameter uncertainty and detect degeneracies, enabling a more interpretable assessment of model fits. Our results show that while some microstructural parameters, such as extra-cellular diffusivity and neurite signal fraction, are robustly estimated, others, including exchange time and soma radius, are often associated with high uncertainty and estimation bias, particularly under realistic noise conditions and reduced acquisition protocols. Comparison with non-linear least squares fitting highlights the critical advantage of uncertainty-aware methods: the ability to flag and filter out unreliable estimates. Together, these findings emphasize the need to report uncertainty and account for model degeneracies when interpreting model-based estimates. Our study advocates for the integration of probabilistic fitting approaches into imaging pipelines to improve reproducibility and biological interpretability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Biophysical models in diffusion MRI (dMRI) hold promise for characterizing gray matter tissue microstructure. Yet, the reliability of their parameter estimates remains largely under-studied, especially in models that incorporate water exchange. In this study, we investigate the accuracy, precision, and presence of degeneracy of two recently proposed gray matter models, NEXI and SANDIX, using established acquisition protocols, on both simulated and \\textit{in vivo} data. We employ $$GUIDE, a Bayesian inference framework based on deep learning, to quantify parameter uncertainty and detect degeneracies, enabling a more interpretable assessment of model fits. Our results show that while some microstructural parameters, such as extra-cellular diffusivity and neurite signal fraction, are robustly estimated, others, including exchange time and soma radius, are often associated with high uncertainty and estimation bias, particularly under realistic noise conditions and reduced acquisition protocols. Comparison with non-linear least squares fitting highlights the critical advantage of uncertainty-aware methods: the ability to flag and filter out unreliable estimates. Together, these findings emphasize the need to report uncertainty and account for model degeneracies when interpreting model-based estimates. Our study advocates for the integration of probabilistic fitting approaches into imaging pipelines to improve reproducibility and biological interpretability."
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-26T23:40:49Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    23,
                    40,
                    49,
                    1,
                    238,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph"
                },
                "authors": [
                    {
                        "name": "Maliss Jallais"
                    },
                    {
                        "name": "Quentin Uhl"
                    },
                    {
                        "name": "Tommaso Pavan"
                    },
                    {
                        "name": "Malwina Molendowska"
                    },
                    {
                        "name": "Derek K. Jones"
                    },
                    {
                        "name": "Ileana Jelescu"
                    },
                    {
                        "name": "Marco Palombo"
                    }
                ],
                "author_detail": {
                    "name": "Marco Palombo"
                },
                "author": "Marco Palombo"
            },
            {
                "id": "http://arxiv.org/abs/2511.12208v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.12208v2",
                "title": "Debate over Mixed-knowledge: A Robust Multi-Agent Reasoning Framework for Incomplete Knowledge Graph Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Debate over Mixed-knowledge: A Robust Multi-Agent Reasoning Framework for Incomplete Knowledge Graph Question Answering"
                },
                "updated": "2025-12-05T14:23:33Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    14,
                    23,
                    33,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.12208v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.12208v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Knowledge Graph Question Answering (KGQA) aims to improve factual accuracy by leveraging structured knowledge. However, real-world Knowledge Graphs (KGs) are often incomplete, leading to the problem of Incomplete KGQA (IKGQA). A common solution is to incorporate external data to fill knowledge gaps, but existing methods lack the capacity to adaptively and contextually fuse multiple sources, failing to fully exploit their complementary strengths. To this end, we propose Debate over Mixed-knowledge (DoM), a novel framework that enables dynamic integration of structured and unstructured knowledge for IKGQA. Built upon the Multi-Agent Debate paradigm, DoM assigns specialized agents to perform inference over knowledge graphs and external texts separately, and coordinates their outputs through iterative interaction. It decomposes the input question into sub-questions, retrieves evidence via dual agents (KG and Retrieval-Augmented Generation, RAG), and employs a judge agent to evaluate and aggregate intermediate answers. This collaboration exploits knowledge complementarity and enhances robustness to KG incompleteness. In addition, existing IKGQA datasets simulate incompleteness by randomly removing triples, failing to capture the irregular and unpredictable nature of real-world knowledge incompleteness. To address this, we introduce a new dataset, Incomplete Knowledge Graph WebQuestions, constructed by leveraging real-world knowledge updates. These updates reflect knowledge beyond the static scope of KGs, yielding a more realistic and challenging benchmark. Through extensive experiments, we show that DoM consistently outperforms state-of-the-art baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Graph Question Answering (KGQA) aims to improve factual accuracy by leveraging structured knowledge. However, real-world Knowledge Graphs (KGs) are often incomplete, leading to the problem of Incomplete KGQA (IKGQA). A common solution is to incorporate external data to fill knowledge gaps, but existing methods lack the capacity to adaptively and contextually fuse multiple sources, failing to fully exploit their complementary strengths. To this end, we propose Debate over Mixed-knowledge (DoM), a novel framework that enables dynamic integration of structured and unstructured knowledge for IKGQA. Built upon the Multi-Agent Debate paradigm, DoM assigns specialized agents to perform inference over knowledge graphs and external texts separately, and coordinates their outputs through iterative interaction. It decomposes the input question into sub-questions, retrieves evidence via dual agents (KG and Retrieval-Augmented Generation, RAG), and employs a judge agent to evaluate and aggregate intermediate answers. This collaboration exploits knowledge complementarity and enhances robustness to KG incompleteness. In addition, existing IKGQA datasets simulate incompleteness by randomly removing triples, failing to capture the irregular and unpredictable nature of real-world knowledge incompleteness. To address this, we introduce a new dataset, Incomplete Knowledge Graph WebQuestions, constructed by leveraging real-world knowledge updates. These updates reflect knowledge beyond the static scope of KGs, yielding a more realistic and challenging benchmark. Through extensive experiments, we show that DoM consistently outperforms state-of-the-art baselines."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-15T13:31:42Z",
                "published_parsed": [
                    2025,
                    11,
                    15,
                    13,
                    31,
                    42,
                    5,
                    319,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Jilong Liu"
                    },
                    {
                        "name": "Pengyang Shao"
                    },
                    {
                        "name": "Wei Qin"
                    },
                    {
                        "name": "Fei Liu"
                    },
                    {
                        "name": "Yonghui Yang"
                    },
                    {
                        "name": "Richang Hong"
                    }
                ],
                "author_detail": {
                    "name": "Richang Hong"
                },
                "author": "Richang Hong"
            },
            {
                "id": "http://arxiv.org/abs/2512.05740v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05740v1",
                "title": "Distilling Expert Surgical Knowledge: How to train local surgical VLMs for anatomy explanation in Complete Mesocolic Excision",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distilling Expert Surgical Knowledge: How to train local surgical VLMs for anatomy explanation in Complete Mesocolic Excision"
                },
                "updated": "2025-12-05T14:19:29Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    14,
                    19,
                    29,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05740v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05740v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recently, Vision Large Language Models (VLMs) have demonstrated high potential in computer-aided diagnosis and decision-support. However, current VLMs show deficits in domain specific surgical scene understanding, such as identifying and explaining anatomical landmarks during Complete Mesocolic Excision. Additionally, there is a need for locally deployable models to avoid patient data leakage to large VLMs, hosted outside the clinic. We propose a privacy-preserving framework to distill knowledge from large, general-purpose LLMs into an efficient, local VLM. We generate an expert-supervised dataset by prompting a teacher LLM without sensitive images, using only textual context and binary segmentation masks for spatial information. This dataset is used for Supervised Fine-Tuning (SFT) and subsequent Direct Preference Optimization (DPO) of the locally deployable VLM. Our evaluation confirms that finetuning VLMs with our generated datasets increases surgical domain knowledge compared to its base VLM by a large margin. Overall, this work validates a data-efficient and privacy-conforming way to train a surgical domain optimized, locally deployable VLM for surgical scene understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, Vision Large Language Models (VLMs) have demonstrated high potential in computer-aided diagnosis and decision-support. However, current VLMs show deficits in domain specific surgical scene understanding, such as identifying and explaining anatomical landmarks during Complete Mesocolic Excision. Additionally, there is a need for locally deployable models to avoid patient data leakage to large VLMs, hosted outside the clinic. We propose a privacy-preserving framework to distill knowledge from large, general-purpose LLMs into an efficient, local VLM. We generate an expert-supervised dataset by prompting a teacher LLM without sensitive images, using only textual context and binary segmentation masks for spatial information. This dataset is used for Supervised Fine-Tuning (SFT) and subsequent Direct Preference Optimization (DPO) of the locally deployable VLM. Our evaluation confirms that finetuning VLMs with our generated datasets increases surgical domain knowledge compared to its base VLM by a large margin. Overall, this work validates a data-efficient and privacy-conforming way to train a surgical domain optimized, locally deployable VLM for surgical scene understanding."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T14:19:29Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    14,
                    19,
                    29,
                    4,
                    339,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Lennart Maack"
                    },
                    {
                        "name": "Julia-Kristin Gra"
                    },
                    {
                        "name": "Lisa-Marie Toscha"
                    },
                    {
                        "name": "Nathaniel Melling"
                    },
                    {
                        "name": "Alexander Schlaefer"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Schlaefer"
                },
                "author": "Alexander Schlaefer"
            },
            {
                "id": "http://arxiv.org/abs/2512.05732v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05732v1",
                "title": "Efficient Text Classification with Conformal In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Text Classification with Conformal In-Context Learning"
                },
                "updated": "2025-12-05T14:11:44Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    14,
                    11,
                    44,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05732v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05732v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) demonstrate strong in-context learning abilities, yet their effectiveness in text classification depends heavily on prompt design and incurs substantial computational cost. Conformal In-Context Learning (CICLe) has been proposed as a resource-efficient framework that integrates a lightweight base classifier with Conformal Prediction to guide LLM prompting by adaptively reducing the set of candidate classes. However, its broader applicability and efficiency benefits beyond a single domain have not yet been systematically explored. In this paper, we present a comprehensive evaluation of CICLe across diverse NLP classification benchmarks. The results show that CICLe consistently improves over its base classifier and outperforms few-shot prompting baselines when the sample size is sufficient for training the base classifier, and performs comparably in low-data regimes. In terms of efficiency, CICLe reduces the number of shots and prompt length by up to 34.45% and 25.16%, respectively, and enables the use of smaller models with competitive performance. CICLe is furthermore particularly advantageous for text classification tasks with high class imbalance. These findings highlight CICLe as a practical and scalable approach for efficient text classification, combining the robustness of traditional classifiers with the adaptability of LLMs, and achieving substantial gains in data and computational efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate strong in-context learning abilities, yet their effectiveness in text classification depends heavily on prompt design and incurs substantial computational cost. Conformal In-Context Learning (CICLe) has been proposed as a resource-efficient framework that integrates a lightweight base classifier with Conformal Prediction to guide LLM prompting by adaptively reducing the set of candidate classes. However, its broader applicability and efficiency benefits beyond a single domain have not yet been systematically explored. In this paper, we present a comprehensive evaluation of CICLe across diverse NLP classification benchmarks. The results show that CICLe consistently improves over its base classifier and outperforms few-shot prompting baselines when the sample size is sufficient for training the base classifier, and performs comparably in low-data regimes. In terms of efficiency, CICLe reduces the number of shots and prompt length by up to 34.45% and 25.16%, respectively, and enables the use of smaller models with competitive performance. CICLe is furthermore particularly advantageous for text classification tasks with high class imbalance. These findings highlight CICLe as a practical and scalable approach for efficient text classification, combining the robustness of traditional classifiers with the adaptability of LLMs, and achieving substantial gains in data and computational efficiency."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T14:11:44Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    14,
                    11,
                    44,
                    4,
                    339,
                    0
                ],
                "arxiv_comment": "10 pages, 4 tables, 2 figures",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Ippokratis Pantelidis"
                    },
                    {
                        "name": "Korbinian Randl"
                    },
                    {
                        "name": "Aron Henriksson"
                    }
                ],
                "author_detail": {
                    "name": "Aron Henriksson"
                },
                "author": "Aron Henriksson"
            },
            {
                "id": "http://arxiv.org/abs/2512.05719v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05719v1",
                "title": "Stellar cores live long and prosper in cuspy dark matter halos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stellar cores live long and prosper in cuspy dark matter halos"
                },
                "updated": "2025-12-05T13:48:42Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    13,
                    48,
                    42,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05719v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05719v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The existence of cuspy or cored centers of dark matter halos is a crucial discriminant between different dark matter models. It has recently been claimed based on dynamical arguments that perfectly cored stellar systems cannot survive inside cuspy dark matter halos, which would make the observation of stellar cores in ultra-faint dwarf galaxies, where dark matter cores cannot form through baryonic processes, a direct falsification of the cold dark matter paradigm. Here, we use idealized simulations to show explicitly that cored stellar systems like those observed in dwarf galaxies can be stable within cuspy dark matter halos over at least several Hubble times. We also demonstrate that observations of ultra-faint dwarf galaxies cannot distinguish mildly positive, flat, or negative inner density slopes, further precluding the dynamical inference of the gravitational potential from the stellar configuration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The existence of cuspy or cored centers of dark matter halos is a crucial discriminant between different dark matter models. It has recently been claimed based on dynamical arguments that perfectly cored stellar systems cannot survive inside cuspy dark matter halos, which would make the observation of stellar cores in ultra-faint dwarf galaxies, where dark matter cores cannot form through baryonic processes, a direct falsification of the cold dark matter paradigm. Here, we use idealized simulations to show explicitly that cored stellar systems like those observed in dwarf galaxies can be stable within cuspy dark matter halos over at least several Hubble times. We also demonstrate that observations of ultra-faint dwarf galaxies cannot distinguish mildly positive, flat, or negative inner density slopes, further precluding the dynamical inference of the gravitational potential from the stellar configuration."
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T13:48:42Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    13,
                    48,
                    42,
                    4,
                    339,
                    0
                ],
                "arxiv_comment": "12 pages, 5 figures. For analysis code and data, see https://github.com/jennihak/stellar-cores-in-cuspy-halos",
                "arxiv_primary_category": {
                    "term": "astro-ph.GA"
                },
                "authors": [
                    {
                        "name": "Jenni Hkkinen"
                    },
                    {
                        "name": "Alexander Rawlings"
                    },
                    {
                        "name": "Till Sawala"
                    },
                    {
                        "name": "Matthew G. Walker"
                    }
                ],
                "author_detail": {
                    "name": "Matthew G. Walker"
                },
                "arxiv_affiliation": "Carnegie Mellon University",
                "author": "Matthew G. Walker"
            },
            {
                "id": "http://arxiv.org/abs/2503.11367v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2503.11367v3",
                "title": "Efficient Distributed MLLM Training with Cornstarch",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Distributed MLLM Training with Cornstarch"
                },
                "updated": "2025-12-05T13:48:01Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    13,
                    48,
                    1,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2503.11367v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2503.11367v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Multimodal large language models (MLLMs) extend the capabilities of large language models (LLMs) by combining heterogeneous model architectures to handle diverse modalities like images and audio. However, this inherent heterogeneity in MLLM model structure and data types makes makeshift extensions to existing LLM training frameworks unsuitable for efficient MLLM training. While there are a few works that have attempted to address the heterogeneity in MLLM training, their approaches are limited to only superficially considering the characteristics of MLLMs.\n  In this paper, we present Cornstarch, an efficient distributed MLLM training framework that contemplates MLLM's unique characteristics in both model and data parallelization. Cornstarch introduces frozen-aware pipeline parallelism and token workload-balanced context parallelism to improve MLLM training throughput. Our extensive evaluation shows that Cornstarch outperforms state-of-the-art solutions by $2.26\\times$ on average in terms of MLLM training throughput.\n  Cornstarch is an open-source project available at https://github.com/cornstarch-org/Cornstarch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) extend the capabilities of large language models (LLMs) by combining heterogeneous model architectures to handle diverse modalities like images and audio. However, this inherent heterogeneity in MLLM model structure and data types makes makeshift extensions to existing LLM training frameworks unsuitable for efficient MLLM training. While there are a few works that have attempted to address the heterogeneity in MLLM training, their approaches are limited to only superficially considering the characteristics of MLLMs.\n  In this paper, we present Cornstarch, an efficient distributed MLLM training framework that contemplates MLLM's unique characteristics in both model and data parallelization. Cornstarch introduces frozen-aware pipeline parallelism and token workload-balanced context parallelism to improve MLLM training throughput. Our extensive evaluation shows that Cornstarch outperforms state-of-the-art solutions by $2.26\\times$ on average in terms of MLLM training throughput.\n  Cornstarch is an open-source project available at https://github.com/cornstarch-org/Cornstarch."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-03-14T13:07:45Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    13,
                    7,
                    45,
                    4,
                    73,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Insu Jang"
                    },
                    {
                        "name": "Runyu Lu"
                    },
                    {
                        "name": "Nikhil Bansal"
                    },
                    {
                        "name": "Ang Chen"
                    },
                    {
                        "name": "Mosharaf Chowdhury"
                    }
                ],
                "author_detail": {
                    "name": "Mosharaf Chowdhury"
                },
                "author": "Mosharaf Chowdhury"
            },
            {
                "id": "http://arxiv.org/abs/2512.05711v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05711v1",
                "title": "Bayesian Active Inference for Intelligent UAV Anti-Jamming and Adaptive Trajectory Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian Active Inference for Intelligent UAV Anti-Jamming and Adaptive Trajectory Planning"
                },
                "updated": "2025-12-05T13:38:52Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    13,
                    38,
                    52,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05711v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05711v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "This paper proposes a hierarchical trajectory planning framework for UAVs operating under adversarial jamming conditions. Leveraging Bayesian Active Inference, the approach combines expert-generated demonstrations with probabilistic generative modeling to encode high-level symbolic planning, low-level motion policies, and wireless signal feedback. During deployment, the UAV performs online inference to anticipate interference, localize jammers, and adapt its trajectory accordingly, without prior knowledge of jammer locations. Simulation results demonstrate that the proposed method achieves near-expert performance, significantly reducing communication interference and mission cost compared to model-free reinforcement learning baselines, while maintaining robust generalization in dynamic environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes a hierarchical trajectory planning framework for UAVs operating under adversarial jamming conditions. Leveraging Bayesian Active Inference, the approach combines expert-generated demonstrations with probabilistic generative modeling to encode high-level symbolic planning, low-level motion policies, and wireless signal feedback. During deployment, the UAV performs online inference to anticipate interference, localize jammers, and adapt its trajectory accordingly, without prior knowledge of jammer locations. Simulation results demonstrate that the proposed method achieves near-expert performance, significantly reducing communication interference and mission cost compared to model-free reinforcement learning baselines, while maintaining robust generalization in dynamic environments."
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T13:38:52Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    13,
                    38,
                    52,
                    4,
                    339,
                    0
                ],
                "arxiv_comment": "This paper has been accepted for the 2026 IEEE Consumer Communications & Networking Conference (IEEE CCNC 2026)",
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "authors": [
                    {
                        "name": "Ali Krayani"
                    },
                    {
                        "name": "Seyedeh Fatemeh Sadati"
                    },
                    {
                        "name": "Lucio Marcenaro"
                    },
                    {
                        "name": "Carlo Regazzoni"
                    }
                ],
                "author_detail": {
                    "name": "Carlo Regazzoni"
                },
                "author": "Carlo Regazzoni"
            },
            {
                "id": "http://arxiv.org/abs/2512.05700v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05700v1",
                "title": "Faithfulness metric fusion: Improving the evaluation of LLM trustworthiness across domains",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Faithfulness metric fusion: Improving the evaluation of LLM trustworthiness across domains"
                },
                "updated": "2025-12-05T13:28:29Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    13,
                    28,
                    29,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05700v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05700v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We present a methodology for improving the accuracy of faithfulness evaluation in Large Language Models (LLMs). The proposed methodology is based on the combination of elementary faithfulness metrics into a combined (fused) metric, for the purpose of improving the faithfulness of LLM outputs. The proposed strategy for metric fusion deploys a tree-based model to identify the importance of each metric, which is driven by the integration of human judgements evaluating the faithfulness of LLM responses. This fused metric is demonstrated to correlate more strongly with human judgements across all tested domains for faithfulness. Improving the ability to evaluate the faithfulness of LLMs, allows for greater confidence to be placed within models, allowing for their implementation in a greater diversity of scenarios. Additionally, we homogenise a collection of datasets across question answering and dialogue-based domains and implement human judgements and LLM responses within this dataset, allowing for the reproduction and trialling of faithfulness evaluation across domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a methodology for improving the accuracy of faithfulness evaluation in Large Language Models (LLMs). The proposed methodology is based on the combination of elementary faithfulness metrics into a combined (fused) metric, for the purpose of improving the faithfulness of LLM outputs. The proposed strategy for metric fusion deploys a tree-based model to identify the importance of each metric, which is driven by the integration of human judgements evaluating the faithfulness of LLM responses. This fused metric is demonstrated to correlate more strongly with human judgements across all tested domains for faithfulness. Improving the ability to evaluate the faithfulness of LLMs, allows for greater confidence to be placed within models, allowing for their implementation in a greater diversity of scenarios. Additionally, we homogenise a collection of datasets across question answering and dialogue-based domains and implement human judgements and LLM responses within this dataset, allowing for the reproduction and trialling of faithfulness evaluation across domains."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T13:28:29Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    13,
                    28,
                    29,
                    4,
                    339,
                    0
                ],
                "arxiv_comment": "9 pages, conference paper",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Ben Malin"
                    },
                    {
                        "name": "Tatiana Kalganova"
                    },
                    {
                        "name": "Nikolaos Boulgouris"
                    }
                ],
                "author_detail": {
                    "name": "Nikolaos Boulgouris"
                },
                "author": "Nikolaos Boulgouris"
            },
            {
                "id": "http://arxiv.org/abs/2509.10809v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2509.10809v2",
                "title": "Rethinking Sparse Autoencoders: Select-and-Project for Fairness and Control from Encoder Features Alone",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Sparse Autoencoders: Select-and-Project for Fairness and Control from Encoder Features Alone"
                },
                "updated": "2025-12-05T13:23:24Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    13,
                    23,
                    24,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2509.10809v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2509.10809v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Sparse Autoencoders (SAEs) are widely employed for mechanistic interpretability and model steering. Within this context, steering is by design performed by means of decoding altered SAE intermediate representations. This procedure essentially rewrites the original activations as a weighted sum of decoder features. In contrast to existing literature, we forward an encoder-centric alternative to model steering which demonstrates a stronger cross-modal performance. We introduce S&P Top-K, a retraining-free and computationally lightweight Selection and Projection framework that identifies Top-K encoder features aligned with a sensitive attribute or behavior, optionally aggregates them into a single control axis, and computes an orthogonal projection to be subsequently applied directly in the model's native embedding space. In vision-language models, it improves fairness metrics on CelebA and FairFace by up to 3.2 times over conventional SAE usage, and in large language models, it substantially reduces aggressiveness and sycophancy in Llama-3 8B Instruct, achieving up to 3.6 times gains over masked reconstruction. These findings suggest that encoder-centric interventions provide a general, efficient, and more effective mechanism for shaping model behavior at inference time than the traditional decoder-centric use of SAEs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Autoencoders (SAEs) are widely employed for mechanistic interpretability and model steering. Within this context, steering is by design performed by means of decoding altered SAE intermediate representations. This procedure essentially rewrites the original activations as a weighted sum of decoder features. In contrast to existing literature, we forward an encoder-centric alternative to model steering which demonstrates a stronger cross-modal performance. We introduce S&P Top-K, a retraining-free and computationally lightweight Selection and Projection framework that identifies Top-K encoder features aligned with a sensitive attribute or behavior, optionally aggregates them into a single control axis, and computes an orthogonal projection to be subsequently applied directly in the model's native embedding space. In vision-language models, it improves fairness metrics on CelebA and FairFace by up to 3.2 times over conventional SAE usage, and in large language models, it substantially reduces aggressiveness and sycophancy in Llama-3 8B Instruct, achieving up to 3.6 times gains over masked reconstruction. These findings suggest that encoder-centric interventions provide a general, efficient, and more effective mechanism for shaping model behavior at inference time than the traditional decoder-centric use of SAEs."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-09-13T06:36:07Z",
                "published_parsed": [
                    2025,
                    9,
                    13,
                    6,
                    36,
                    7,
                    5,
                    256,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Antonio Brblau"
                    },
                    {
                        "name": "Cristian Daniel Pduraru"
                    },
                    {
                        "name": "Teodor Poncu"
                    },
                    {
                        "name": "Alexandru Tifrea"
                    },
                    {
                        "name": "Elena Burceanu"
                    }
                ],
                "author_detail": {
                    "name": "Elena Burceanu"
                },
                "author": "Elena Burceanu"
            },
            {
                "id": "http://arxiv.org/abs/2510.02226v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.02226v2",
                "title": "TempoControl: Temporal Attention Guidance for Text-to-Video Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TempoControl: Temporal Attention Guidance for Text-to-Video Models"
                },
                "updated": "2025-12-05T13:22:45Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    13,
                    22,
                    45,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.02226v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.02226v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recent advances in generative video models have enabled the creation of high-quality videos based on natural language prompts. However, these models frequently lack fine-grained temporal control, meaning they do not allow users to specify when particular visual elements should appear within a generated sequence. In this work, we introduce TempoControl, a method that allows for temporal alignment of visual concepts during inference, without requiring retraining or additional supervision. TempoControl utilizes cross-attention maps, a key component of text-to-video diffusion models, to guide the timing of concepts through a novel optimization approach. Our method steers attention using three complementary principles: aligning its temporal pattern with a control signal (correlation), adjusting its strength where visibility is required (magnitude), and preserving semantic consistency (entropy). TempoControl provides precise temporal control while maintaining high video quality and diversity. We demonstrate its effectiveness across various applications, including temporal reordering of single and multiple objects, action timing, and audio-aligned video generation. Please see our project page for more details: https://shira-schiber.github.io/TempoControl/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in generative video models have enabled the creation of high-quality videos based on natural language prompts. However, these models frequently lack fine-grained temporal control, meaning they do not allow users to specify when particular visual elements should appear within a generated sequence. In this work, we introduce TempoControl, a method that allows for temporal alignment of visual concepts during inference, without requiring retraining or additional supervision. TempoControl utilizes cross-attention maps, a key component of text-to-video diffusion models, to guide the timing of concepts through a novel optimization approach. Our method steers attention using three complementary principles: aligning its temporal pattern with a control signal (correlation), adjusting its strength where visibility is required (magnitude), and preserving semantic consistency (entropy). TempoControl provides precise temporal control while maintaining high video quality and diversity. We demonstrate its effectiveness across various applications, including temporal reordering of single and multiple objects, action timing, and audio-aligned video generation. Please see our project page for more details: https://shira-schiber.github.io/TempoControl/."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-02T17:13:35Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    17,
                    13,
                    35,
                    3,
                    275,
                    0
                ],
                "arxiv_comment": "Under Review",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Shira Schiber"
                    },
                    {
                        "name": "Ofir Lindenbaum"
                    },
                    {
                        "name": "Idan Schwartz"
                    }
                ],
                "author_detail": {
                    "name": "Idan Schwartz"
                },
                "author": "Idan Schwartz"
            },
            {
                "id": "http://arxiv.org/abs/2408.13131v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2408.13131v3",
                "title": "Detecting the Future: All-at-Once Event Sequence Forecasting with Horizon Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting the Future: All-at-Once Event Sequence Forecasting with Horizon Matching"
                },
                "updated": "2025-12-05T13:20:03Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    13,
                    20,
                    3,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2408.13131v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2408.13131v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Long-horizon events forecasting is a crucial task across various domains, including retail, finance, healthcare, and social networks. Traditional models for event sequences often extend to forecasting on a horizon using an autoregressive (recursive) multi-step strategy, which has limited effectiveness due to typical convergence to constant or repetitive outputs. To address this limitation, we introduce DEF, a novel approach for simultaneous forecasting of multiple future events on a horizon with high accuracy and diversity. Our method optimally aligns predictions with ground truth events during training by using a novel matching-based loss function. We establish a new state-of-the-art in long-horizon event prediction, achieving up to a 50% relative improvement over existing temporal point processes and event prediction models. Furthermore, we achieve state-of-the-art performance in next-event prediction tasks while demonstrating high computational efficiency during inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-horizon events forecasting is a crucial task across various domains, including retail, finance, healthcare, and social networks. Traditional models for event sequences often extend to forecasting on a horizon using an autoregressive (recursive) multi-step strategy, which has limited effectiveness due to typical convergence to constant or repetitive outputs. To address this limitation, we introduce DEF, a novel approach for simultaneous forecasting of multiple future events on a horizon with high accuracy and diversity. Our method optimally aligns predictions with ground truth events during training by using a novel matching-based loss function. We establish a new state-of-the-art in long-horizon event prediction, achieving up to a 50% relative improvement over existing temporal point processes and event prediction models. Furthermore, we achieve state-of-the-art performance in next-event prediction tasks while demonstrating high computational efficiency during inference."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-08-23T14:57:46Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    14,
                    57,
                    46,
                    4,
                    236,
                    0
                ],
                "arxiv_comment": "Accepted to AAAI 2026",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Ivan Karpukhin"
                    },
                    {
                        "name": "Andrey Savchenko"
                    }
                ],
                "author_detail": {
                    "name": "Andrey Savchenko"
                },
                "author": "Andrey Savchenko"
            },
            {
                "id": "http://arxiv.org/abs/2512.00713v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.00713v2",
                "title": "Concept-Guided Backdoor Attack on Vision Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Concept-Guided Backdoor Attack on Vision Language Models"
                },
                "updated": "2025-12-05T13:17:42Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    13,
                    17,
                    42,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.00713v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.00713v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Vision-Language Models (VLMs) have achieved impressive progress in multimodal text generation, yet their rapid adoption raises increasing concerns about security vulnerabilities. Existing backdoor attacks against VLMs primarily rely on explicit pixel-level triggers or imperceptible perturbations injected into images. While effective, these approaches reduce stealthiness and remain vulnerable to image-based defenses. We introduce concept-guided backdoor attacks, a new paradigm that operates at the semantic concept level rather than on raw pixels. We propose two different attacks. The first, Concept-Thresholding Poisoning (CTP), uses explicit concepts in natural images as triggers: only samples containing the target concept are poisoned, causing the model to behave normally in all other cases but consistently inject malicious outputs whenever the concept appears. The second, CBL-Guided Unseen Backdoor (CGUB), leverages a Concept Bottleneck Model (CBM) during training to intervene on internal concept activations, while discarding the CBM branch at inference time to keep the VLM unchanged. This design enables systematic replacement of a targeted label in generated text (for example, replacing \"cat\" with \"dog\"), even when the replacement behavior never appears in the training data. Experiments across multiple VLM architectures and datasets show that both CTP and CGUB achieve high attack success rates while maintaining moderate impact on clean-task performance. These findings highlight concept-level vulnerabilities as a critical new attack surface for VLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Models (VLMs) have achieved impressive progress in multimodal text generation, yet their rapid adoption raises increasing concerns about security vulnerabilities. Existing backdoor attacks against VLMs primarily rely on explicit pixel-level triggers or imperceptible perturbations injected into images. While effective, these approaches reduce stealthiness and remain vulnerable to image-based defenses. We introduce concept-guided backdoor attacks, a new paradigm that operates at the semantic concept level rather than on raw pixels. We propose two different attacks. The first, Concept-Thresholding Poisoning (CTP), uses explicit concepts in natural images as triggers: only samples containing the target concept are poisoned, causing the model to behave normally in all other cases but consistently inject malicious outputs whenever the concept appears. The second, CBL-Guided Unseen Backdoor (CGUB), leverages a Concept Bottleneck Model (CBM) during training to intervene on internal concept activations, while discarding the CBM branch at inference time to keep the VLM unchanged. This design enables systematic replacement of a targeted label in generated text (for example, replacing \"cat\" with \"dog\"), even when the replacement behavior never appears in the training data. Experiments across multiple VLM architectures and datasets show that both CTP and CGUB achieve high attack success rates while maintaining moderate impact on clean-task performance. These findings highlight concept-level vulnerabilities as a critical new attack surface for VLMs."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-30T03:24:23Z",
                "published_parsed": [
                    2025,
                    11,
                    30,
                    3,
                    24,
                    23,
                    6,
                    334,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Haoyu Shen"
                    },
                    {
                        "name": "Weimin Lyu"
                    },
                    {
                        "name": "Haotian Xu"
                    },
                    {
                        "name": "Tengfei Ma"
                    }
                ],
                "author_detail": {
                    "name": "Tengfei Ma"
                },
                "author": "Tengfei Ma"
            },
            {
                "id": "http://arxiv.org/abs/2512.05686v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05686v1",
                "title": "LA-RL: Language Action-guided Reinforcement Learning with Safety Guarantees for Autonomous Highway Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LA-RL: Language Action-guided Reinforcement Learning with Safety Guarantees for Autonomous Highway Driving"
                },
                "updated": "2025-12-05T13:02:44Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    13,
                    2,
                    44,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05686v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05686v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Autonomous highway driving demands a critical balance between proactive, efficiency-seeking behavior and robust safety guarantees. This paper proposes Language Action-guided Reinforcement Learning (LA-RL) with Safety Guarantees, a novel framework that integrates the semantic reasoning of large language models (LLMs) into the actor-critic architecture with an improved safety layer. Within this framework, task-specific reward shaping harmonizes the dual objectives of maximizing driving efficiency and ensuring safety, guiding decision-making based on both environmental insights and clearly defined goals. To enhance safety, LA-RL incorporates a safety-critical planner that combines model predictive control (MPC) with discrete control barrier functions (DCBFs). This layer formally constrains the LLM-informed policy to a safe action set, employs a slack mechanism that enhances solution feasibility, prevents overly conservative behavior and allows for greater policy exploration without compromising safety. Extensive experiments demonstrate that it significantly outperforms several current state-of-the-art methods, offering a more adaptive, reliable, and robust solution for autonomous highway driving. Compared to existing SOTA, it achieves approximately 20$\\%$ higher success rate than the knowledge graph (KG) based baseline and about 30$\\%$ higher than the retrieval augmented generation (RAG) based baseline. In low-density environments, LA-RL achieves a 100$\\%$ success rate. These results confirm its enhanced exploration of the state-action space and its ability to autonomously adopt more efficient, proactive strategies in complex, mixed-traffic highway environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous highway driving demands a critical balance between proactive, efficiency-seeking behavior and robust safety guarantees. This paper proposes Language Action-guided Reinforcement Learning (LA-RL) with Safety Guarantees, a novel framework that integrates the semantic reasoning of large language models (LLMs) into the actor-critic architecture with an improved safety layer. Within this framework, task-specific reward shaping harmonizes the dual objectives of maximizing driving efficiency and ensuring safety, guiding decision-making based on both environmental insights and clearly defined goals. To enhance safety, LA-RL incorporates a safety-critical planner that combines model predictive control (MPC) with discrete control barrier functions (DCBFs). This layer formally constrains the LLM-informed policy to a safe action set, employs a slack mechanism that enhances solution feasibility, prevents overly conservative behavior and allows for greater policy exploration without compromising safety. Extensive experiments demonstrate that it significantly outperforms several current state-of-the-art methods, offering a more adaptive, reliable, and robust solution for autonomous highway driving. Compared to existing SOTA, it achieves approximately 20$\\%$ higher success rate than the knowledge graph (KG) based baseline and about 30$\\%$ higher than the retrieval augmented generation (RAG) based baseline. In low-density environments, LA-RL achieves a 100$\\%$ success rate. These results confirm its enhanced exploration of the state-action space and its ability to autonomously adopt more efficient, proactive strategies in complex, mixed-traffic highway environments."
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T13:02:44Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    13,
                    2,
                    44,
                    4,
                    339,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY"
                },
                "authors": [
                    {
                        "name": "Yiming Shu"
                    },
                    {
                        "name": "Jiahui Xu"
                    },
                    {
                        "name": "Jiwei Tang"
                    },
                    {
                        "name": "Ruiyang Gao"
                    },
                    {
                        "name": "Chen Sun"
                    }
                ],
                "author_detail": {
                    "name": "Chen Sun"
                },
                "author": "Chen Sun"
            },
            {
                "id": "http://arxiv.org/abs/2512.05677v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05677v1",
                "title": "Empirical Decision Theory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Empirical Decision Theory"
                },
                "updated": "2025-12-05T12:46:04Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    12,
                    46,
                    4,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05677v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05677v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Analyzing decision problems under uncertainty commonly relies on idealizing assumptions about the describability of the world, with the most prominent examples being the closed world and the small world assumption. Most assumptions are operationalized by introducing states of the world, conditional on which the decision situation can be analyzed without any remaining uncertainty. Conversely, most classical decision-theoretic approaches are not applicable if the states of the world are inaccessible. We propose a decision model that retains the appeal and simplicity of the original theory, but completely overcomes the need to specify the states of the world explicitly. The main idea of our approach is to address decision problems in a radically empirical way: instead of specifying states and consequences prior to the decision analysis, we only assume a protocol of observed act--consequence pairs as model primitives. We show how optimality in such empirical decision problems can be addressed by using protocol-based empirical choice functions and discuss three approaches for deriving inferential guarantees: (I) consistent statistical estimation of choice sets, (II) consistent statistical testing of choice functions with robustness guarantees, and (III) direct inference for empirical choice functions using credal sets. We illustrate our theory with a proof-of-concept application comparing different prompting strategies in generative AI models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analyzing decision problems under uncertainty commonly relies on idealizing assumptions about the describability of the world, with the most prominent examples being the closed world and the small world assumption. Most assumptions are operationalized by introducing states of the world, conditional on which the decision situation can be analyzed without any remaining uncertainty. Conversely, most classical decision-theoretic approaches are not applicable if the states of the world are inaccessible. We propose a decision model that retains the appeal and simplicity of the original theory, but completely overcomes the need to specify the states of the world explicitly. The main idea of our approach is to address decision problems in a radically empirical way: instead of specifying states and consequences prior to the decision analysis, we only assume a protocol of observed act--consequence pairs as model primitives. We show how optimality in such empirical decision problems can be addressed by using protocol-based empirical choice functions and discuss three approaches for deriving inferential guarantees: (I) consistent statistical estimation of choice sets, (II) consistent statistical testing of choice functions with robustness guarantees, and (III) direct inference for empirical choice functions using credal sets. We illustrate our theory with a proof-of-concept application comparing different prompting strategies in generative AI models."
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T12:46:04Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    12,
                    46,
                    4,
                    4,
                    339,
                    0
                ],
                "arxiv_comment": "Christoph Jansen and Georg Schollmeyer contributed equally",
                "arxiv_primary_category": {
                    "term": "stat.ME"
                },
                "authors": [
                    {
                        "name": "Christoph Jansen"
                    },
                    {
                        "name": "Georg Schollmeyer"
                    },
                    {
                        "name": "Thomas Augustin"
                    },
                    {
                        "name": "Julian Rodemann"
                    }
                ],
                "author_detail": {
                    "name": "Julian Rodemann"
                },
                "arxiv_affiliation": "CISPA Helmholtz Center for Information Security Saarbrcken",
                "author": "Julian Rodemann"
            },
            {
                "id": "http://arxiv.org/abs/2509.04398v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2509.04398v2",
                "title": "IPA: An Information-Reconstructive Input Projection Framework for Efficient Foundation Model Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IPA: An Information-Reconstructive Input Projection Framework for Efficient Foundation Model Adaptation"
                },
                "updated": "2025-12-05T12:36:37Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    12,
                    36,
                    37,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2509.04398v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2509.04398v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Parameter-efficient fine-tuning (PEFT) methods, such as LoRA, reduce adaptation cost by injecting low-rank updates into pretrained weights. However, LoRA's down-projection is randomly initialized and data-agnostic, discarding potentially useful information. Prior analyses show that this projection changes little during training, while the up-projection carries most of the adaptation, making the random input compression a performance bottleneck. We propose IPA, a feature-aware projection framework that explicitly aims to reconstruct the original input within a reduced hidden space. In the linear case, we instantiate IPA with algorithms approximating top principal components, enabling efficient projector pretraining with negligible inference overhead. Across language and vision benchmarks, IPA consistently improves over LoRA and DoRA, achieving on average 1.5 points higher accuracy on commonsense reasoning and 2.3 points on VTAB-1k, while matching full LoRA performance with roughly half the trainable parameters when the projection is frozen. Code available at https://github.com/valeoai/peft-ipa .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameter-efficient fine-tuning (PEFT) methods, such as LoRA, reduce adaptation cost by injecting low-rank updates into pretrained weights. However, LoRA's down-projection is randomly initialized and data-agnostic, discarding potentially useful information. Prior analyses show that this projection changes little during training, while the up-projection carries most of the adaptation, making the random input compression a performance bottleneck. We propose IPA, a feature-aware projection framework that explicitly aims to reconstruct the original input within a reduced hidden space. In the linear case, we instantiate IPA with algorithms approximating top principal components, enabling efficient projector pretraining with negligible inference overhead. Across language and vision benchmarks, IPA consistently improves over LoRA and DoRA, achieving on average 1.5 points higher accuracy on commonsense reasoning and 2.3 points on VTAB-1k, while matching full LoRA performance with roughly half the trainable parameters when the projection is frozen. Code available at https://github.com/valeoai/peft-ipa ."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-09-04T17:10:01Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    17,
                    10,
                    1,
                    3,
                    247,
                    0
                ],
                "arxiv_comment": "Accepted to TMLR",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Yuan Yin"
                    },
                    {
                        "name": "Shashanka Venkataramanan"
                    },
                    {
                        "name": "Tuan-Hung Vu"
                    },
                    {
                        "name": "Andrei Bursuc"
                    },
                    {
                        "name": "Matthieu Cord"
                    }
                ],
                "author_detail": {
                    "name": "Matthieu Cord"
                },
                "author": "Matthieu Cord"
            },
            {
                "id": "http://arxiv.org/abs/2505.05261v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2505.05261v2",
                "title": "ICNN-enhanced 2SP: Leveraging input convex neural networks for solving two-stage stochastic programming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ICNN-enhanced 2SP: Leveraging input convex neural networks for solving two-stage stochastic programming"
                },
                "updated": "2025-12-05T12:31:57Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    12,
                    31,
                    57,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2505.05261v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2505.05261v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Two-stage stochastic programming (2SP) offers a basic framework for modelling decision-making under uncertainty, yet scalability remains a challenge due to the computational complexity of recourse function evaluation. Existing learning-based methods like Neural Two-Stage Stochastic Programming (Neur2SP) employ neural networks (NNs) as recourse function surrogates but rely on computationally intensive mixed-integer programming (MIP) formulations. We propose ICNN-enhanced 2SP, a method that leverages Input Convex Neural Networks (ICNNs) to exploit linear programming (LP) representability in convex 2SP problems. By architecturally enforcing convexity and enabling exact inference through LP, our approach eliminates the need for integer variables inherent to the conventional MIP-based formulation while retaining an exact embedding of the ICNN surrogate within the 2SP framework. This results in a more computationally efficient alternative, and we show that good solution quality can be maintained. Comprehensive experiments reveal that ICNNs incur only marginally longer training times while achieving validation accuracy on par with their standard NN counterparts. Across benchmark problems, ICNN-enhanced 2SP often exhibits considerably faster solution times than the MIP-based formulations while preserving solution quality, with these advantages becoming significantly more pronounced as problem scale increases. For the most challenging instances, the method achieves speedups of up to 100$\\times$ and solution quality superior to MIP-based formulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Two-stage stochastic programming (2SP) offers a basic framework for modelling decision-making under uncertainty, yet scalability remains a challenge due to the computational complexity of recourse function evaluation. Existing learning-based methods like Neural Two-Stage Stochastic Programming (Neur2SP) employ neural networks (NNs) as recourse function surrogates but rely on computationally intensive mixed-integer programming (MIP) formulations. We propose ICNN-enhanced 2SP, a method that leverages Input Convex Neural Networks (ICNNs) to exploit linear programming (LP) representability in convex 2SP problems. By architecturally enforcing convexity and enabling exact inference through LP, our approach eliminates the need for integer variables inherent to the conventional MIP-based formulation while retaining an exact embedding of the ICNN surrogate within the 2SP framework. This results in a more computationally efficient alternative, and we show that good solution quality can be maintained. Comprehensive experiments reveal that ICNNs incur only marginally longer training times while achieving validation accuracy on par with their standard NN counterparts. Across benchmark problems, ICNN-enhanced 2SP often exhibits considerably faster solution times than the MIP-based formulations while preserving solution quality, with these advantages becoming significantly more pronounced as problem scale increases. For the most challenging instances, the method achieves speedups of up to 100$\\times$ and solution quality superior to MIP-based formulations."
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-05-08T14:06:38Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    14,
                    6,
                    38,
                    3,
                    128,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "math.OC"
                },
                "authors": [
                    {
                        "name": "Yu Liu"
                    },
                    {
                        "name": "Fabricio Oliveira"
                    },
                    {
                        "name": "Jan Kronqvist"
                    }
                ],
                "author_detail": {
                    "name": "Jan Kronqvist"
                },
                "author": "Jan Kronqvist"
            },
            {
                "id": "http://arxiv.org/abs/2512.05671v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05671v1",
                "title": "MedTutor-R1: Socratic Personalized Medical Teaching with Multi-Agent Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MedTutor-R1: Socratic Personalized Medical Teaching with Multi-Agent Simulation"
                },
                "updated": "2025-12-05T12:28:30Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    12,
                    28,
                    30,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05671v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05671v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The significant gap between rising demands for clinical training and the scarcity of expert instruction poses a major challenge to medical education. With powerful capabilities in personalized guidance, Large Language Models (LLMs) offer a promising solution to bridge this gap. However, current research focuses mainly on one-on-one knowledge instruction, overlooking collaborative reasoning, a key skill for students developed in teamwork like ward rounds. To this end, we develop ClinEdu, a multi-agent pedagogical simulator with personality-driven patients and diverse student cohorts, enabling controlled testing of complex pedagogical processes and scalable generation of teaching data. Based on ClinEdu, we construct ClinTeach, a large Socratic teaching dialogue dataset that captures the complexities of group instruction. We then train MedTutor-R1, the first multimodal Socratic tutor designed for one-to-many instruction in clinical medical education. MedTutor-R1 is first instruction-tuned on our ClinTeach dataset and then optimized with reinforcement learning, using rewards derived from a three-axis rubric, covering structural fidelity, analytical quality, and clinical safety, to refine its adaptive Socratic strategies. For authentic in-situ assessment, we use simulation-based interactive evaluation that redeploys the tutor back into ClinEdu. Experimental results demonstrate that our MedTutor-R1 outperforms the base model by over 20% in average pedagogical score and is comparable to o3, while also exhibiting high adaptability in handling a varying number of students. This promising performance underscores the effectiveness of our pedagogical simulator, ClinEdu.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The significant gap between rising demands for clinical training and the scarcity of expert instruction poses a major challenge to medical education. With powerful capabilities in personalized guidance, Large Language Models (LLMs) offer a promising solution to bridge this gap. However, current research focuses mainly on one-on-one knowledge instruction, overlooking collaborative reasoning, a key skill for students developed in teamwork like ward rounds. To this end, we develop ClinEdu, a multi-agent pedagogical simulator with personality-driven patients and diverse student cohorts, enabling controlled testing of complex pedagogical processes and scalable generation of teaching data. Based on ClinEdu, we construct ClinTeach, a large Socratic teaching dialogue dataset that captures the complexities of group instruction. We then train MedTutor-R1, the first multimodal Socratic tutor designed for one-to-many instruction in clinical medical education. MedTutor-R1 is first instruction-tuned on our ClinTeach dataset and then optimized with reinforcement learning, using rewards derived from a three-axis rubric, covering structural fidelity, analytical quality, and clinical safety, to refine its adaptive Socratic strategies. For authentic in-situ assessment, we use simulation-based interactive evaluation that redeploys the tutor back into ClinEdu. Experimental results demonstrate that our MedTutor-R1 outperforms the base model by over 20% in average pedagogical score and is comparable to o3, while also exhibiting high adaptability in handling a varying number of students. This promising performance underscores the effectiveness of our pedagogical simulator, ClinEdu."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T12:28:30Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    12,
                    28,
                    30,
                    4,
                    339,
                    0
                ],
                "arxiv_comment": "Work In Progress",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Zhitao He"
                    },
                    {
                        "name": "Haolin Yang"
                    },
                    {
                        "name": "Zeyu Qin"
                    },
                    {
                        "name": "Yi R Fung"
                    }
                ],
                "author_detail": {
                    "name": "Yi R Fung"
                },
                "author": "Yi R Fung"
            },
            {
                "id": "http://arxiv.org/abs/2512.05668v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05668v1",
                "title": "Generalised Bayesian Inference using Robust divergences for von Mises-Fisher distribution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generalised Bayesian Inference using Robust divergences for von Mises-Fisher distribution"
                },
                "updated": "2025-12-05T12:25:59Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    12,
                    25,
                    59,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05668v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05668v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "This paper focusses on robust estimation of location and concentration parameters of the von Mises-Fisher distribution in the Bayesian framework. The von Mises-Fisher (or Langevin) distribution has played a central role in directional statistics. Directional data have been investigated for many decades, and more recently, they have gained increasing attention in diverse areas such as bioinformatics and text data analysis. Although outliers can significantly affect the estimation results even for directional data, the treatment of outliers remains an unresolved and challenging problem. In the frequentist framework, numerous studies have developed robust estimation methods for directional data with outliers, but, in contrast, only a few robust estimation methods have been proposed in the Bayesian framework. In this paper, we propose Bayesian inference based on density power-divergence and $$-divergence and establish their asymptotic properties and robustness. In addition, the Bayesian approach naturally provides a way to assess estimation uncertainty through the posterior distribution, which is particularly useful for small samples. Furthermore, to carry out the posterior computation, we develop the posterior computation algorithm based on the weighted Bayesian bootstrap for estimating parameters. The effectiveness of the proposed methods is demonstrated through simulation studies. Using two real datasets, we further show that the proposed method provides reliable and robust estimation even in the presence of outliers or data contamination.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper focusses on robust estimation of location and concentration parameters of the von Mises-Fisher distribution in the Bayesian framework. The von Mises-Fisher (or Langevin) distribution has played a central role in directional statistics. Directional data have been investigated for many decades, and more recently, they have gained increasing attention in diverse areas such as bioinformatics and text data analysis. Although outliers can significantly affect the estimation results even for directional data, the treatment of outliers remains an unresolved and challenging problem. In the frequentist framework, numerous studies have developed robust estimation methods for directional data with outliers, but, in contrast, only a few robust estimation methods have been proposed in the Bayesian framework. In this paper, we propose Bayesian inference based on density power-divergence and $$-divergence and establish their asymptotic properties and robustness. In addition, the Bayesian approach naturally provides a way to assess estimation uncertainty through the posterior distribution, which is particularly useful for small samples. Furthermore, to carry out the posterior computation, we develop the posterior computation algorithm based on the weighted Bayesian bootstrap for estimating parameters. The effectiveness of the proposed methods is demonstrated through simulation studies. Using two real datasets, we further show that the proposed method provides reliable and robust estimation even in the presence of outliers or data contamination."
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T12:25:59Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    12,
                    25,
                    59,
                    4,
                    339,
                    0
                ],
                "arxiv_comment": "24 pages",
                "arxiv_primary_category": {
                    "term": "stat.ME"
                },
                "authors": [
                    {
                        "name": "Tomoyuki Nakagawa"
                    },
                    {
                        "name": "Yasuhito Tsuruta"
                    },
                    {
                        "name": "Sho Kazari"
                    },
                    {
                        "name": "Kouji Tahata"
                    }
                ],
                "author_detail": {
                    "name": "Kouji Tahata"
                },
                "author": "Kouji Tahata"
            },
            {
                "id": "http://arxiv.org/abs/2512.05663v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05663v1",
                "title": "LeAD-M3D: Leveraging Asymmetric Distillation for Real-time Monocular 3D Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LeAD-M3D: Leveraging Asymmetric Distillation for Real-time Monocular 3D Detection"
                },
                "updated": "2025-12-05T12:08:18Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    12,
                    8,
                    18,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05663v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05663v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Real-time monocular 3D object detection remains challenging due to severe depth ambiguity, viewpoint shifts, and the high computational cost of 3D reasoning. Existing approaches either rely on LiDAR or geometric priors to compensate for missing depth, or sacrifice efficiency to achieve competitive accuracy. We introduce LeAD-M3D, a monocular 3D detector that achieves state-of-the-art accuracy and real-time inference without extra modalities. Our method is powered by three key components. Asymmetric Augmentation Denoising Distillation (A2D2) transfers geometric knowledge from a clean-image teacher to a mixup-noised student via a quality- and importance-weighted depth-feature loss, enabling stronger depth reasoning without LiDAR supervision. 3D-aware Consistent Matching (CM3D) improves prediction-to-ground truth assignment by integrating 3D MGIoU into the matching score, yielding more stable and precise supervision. Finally, Confidence-Gated 3D Inference (CGI3D) accelerates detection by restricting expensive 3D regression to top-confidence regions. Together, these components set a new Pareto frontier for monocular 3D detection: LeAD-M3D achieves state-of-the-art accuracy on KITTI and Waymo, and the best reported car AP on Rope3D, while running up to 3.6x faster than prior high-accuracy methods. Our results demonstrate that high fidelity and real-time efficiency in monocular 3D detection are simultaneously attainable - without LiDAR, stereo, or geometric assumptions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time monocular 3D object detection remains challenging due to severe depth ambiguity, viewpoint shifts, and the high computational cost of 3D reasoning. Existing approaches either rely on LiDAR or geometric priors to compensate for missing depth, or sacrifice efficiency to achieve competitive accuracy. We introduce LeAD-M3D, a monocular 3D detector that achieves state-of-the-art accuracy and real-time inference without extra modalities. Our method is powered by three key components. Asymmetric Augmentation Denoising Distillation (A2D2) transfers geometric knowledge from a clean-image teacher to a mixup-noised student via a quality- and importance-weighted depth-feature loss, enabling stronger depth reasoning without LiDAR supervision. 3D-aware Consistent Matching (CM3D) improves prediction-to-ground truth assignment by integrating 3D MGIoU into the matching score, yielding more stable and precise supervision. Finally, Confidence-Gated 3D Inference (CGI3D) accelerates detection by restricting expensive 3D regression to top-confidence regions. Together, these components set a new Pareto frontier for monocular 3D detection: LeAD-M3D achieves state-of-the-art accuracy on KITTI and Waymo, and the best reported car AP on Rope3D, while running up to 3.6x faster than prior high-accuracy methods. Our results demonstrate that high fidelity and real-time efficiency in monocular 3D detection are simultaneously attainable - without LiDAR, stereo, or geometric assumptions."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T12:08:18Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    12,
                    8,
                    18,
                    4,
                    339,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Johannes Meier"
                    },
                    {
                        "name": "Jonathan Michel"
                    },
                    {
                        "name": "Oussema Dhaouadi"
                    },
                    {
                        "name": "Yung-Hsu Yang"
                    },
                    {
                        "name": "Christoph Reich"
                    },
                    {
                        "name": "Zuria Bauer"
                    },
                    {
                        "name": "Stefan Roth"
                    },
                    {
                        "name": "Marc Pollefeys"
                    },
                    {
                        "name": "Jacques Kaiser"
                    },
                    {
                        "name": "Daniel Cremers"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Cremers"
                },
                "author": "Daniel Cremers"
            },
            {
                "id": "http://arxiv.org/abs/2512.05661v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05661v1",
                "title": "Standard and stressed value at risk forecasting using dynamic Bayesian networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Standard and stressed value at risk forecasting using dynamic Bayesian networks"
                },
                "updated": "2025-12-05T12:06:21Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    12,
                    6,
                    21,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05661v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05661v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "This study introduces a dynamic Bayesian network (DBN) framework for forecasting value at risk (VaR) and stressed VaR (SVaR) and compares its performance to several commonly applied models. Using daily S&P 500 index returns from 1991 to 2020, we produce 10-day 99% VaR and SVaR forecasts using a rolling period and historical returns for the traditional models, while three DBNs use both historical and forecasted returns. We evaluate the models' forecasting accuracy using standard backtests and forecasting error measures. Results show that autoregressive models deliver the most accurate VaR forecasts, while the DBNs achieve comparable performance to the historical simulation model, despite incorporating forward-looking return forecasts. For SVaR, all models produce highly conservative forecasts, with minimal breaches and limited differentiation in accuracy. While DBNs do not outperform traditional models, they demonstrate feasibility as a forward-looking approach to provide a foundation for future research on integrating causal inference into financial risk forecasting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study introduces a dynamic Bayesian network (DBN) framework for forecasting value at risk (VaR) and stressed VaR (SVaR) and compares its performance to several commonly applied models. Using daily S&P 500 index returns from 1991 to 2020, we produce 10-day 99% VaR and SVaR forecasts using a rolling period and historical returns for the traditional models, while three DBNs use both historical and forecasted returns. We evaluate the models' forecasting accuracy using standard backtests and forecasting error measures. Results show that autoregressive models deliver the most accurate VaR forecasts, while the DBNs achieve comparable performance to the historical simulation model, despite incorporating forward-looking return forecasts. For SVaR, all models produce highly conservative forecasts, with minimal breaches and limited differentiation in accuracy. While DBNs do not outperform traditional models, they demonstrate feasibility as a forward-looking approach to provide a foundation for future research on integrating causal inference into financial risk forecasting."
                },
                "tags": [
                    {
                        "term": "q-fin.RM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T12:06:21Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    12,
                    6,
                    21,
                    4,
                    339,
                    0
                ],
                "arxiv_comment": "30 pages, 4 tables (excluding appendix, in which there is one table)",
                "arxiv_primary_category": {
                    "term": "q-fin.RM"
                },
                "authors": [
                    {
                        "name": "Eden Gross"
                    },
                    {
                        "name": "Ryan Kruger"
                    },
                    {
                        "name": "Francois Toerien"
                    }
                ],
                "author_detail": {
                    "name": "Francois Toerien"
                },
                "author": "Francois Toerien"
            },
            {
                "id": "http://arxiv.org/abs/2512.05659v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05659v1",
                "title": "Beyond Automation: Redesigning Jobs with LLMs to Enhance Productivity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Automation: Redesigning Jobs with LLMs to Enhance Productivity"
                },
                "updated": "2025-12-05T12:05:55Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    12,
                    5,
                    55,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05659v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05659v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The adoption of generative artificial intelligence (AI) is predicted to lead to fundamental shifts in the labour market, resulting in displacement or augmentation of AI-exposed roles. To investigate the impact of AI across a large organisation, we assessed AI exposure at the task level within roles at the UK Civil Service (UKCS). Using a novel dataset of UKCS job adverts, covering 193,497 vacancies over 6 years, our large language model (LLM)-driven analysis estimated AI exposure scores of 1,542,411 tasks. By aggregating AI exposure scores for tasks within each role, we calculated the mean and variance of job-level exposure to AI, highlighting the heterogeneous impacts of AI, even for seemingly identical jobs. We then use an LLM to redesign jobs, focusing on task automation, task optimisation, and task reallocation. We find that the redesign process leads to tasks where humans have comparative advantage over AI, including strategic leadership, complex problem resolution, and stakeholder management. Overall, automation and augmentation are expected to have nuanced effects across all levels of the organisational hierarchy. Most economic value of AI is expected to arise from productivity gains rather than role displacement. We contribute to the automation, augmentation and productivity debates as well as advance our understanding of job redesign in the age of AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The adoption of generative artificial intelligence (AI) is predicted to lead to fundamental shifts in the labour market, resulting in displacement or augmentation of AI-exposed roles. To investigate the impact of AI across a large organisation, we assessed AI exposure at the task level within roles at the UK Civil Service (UKCS). Using a novel dataset of UKCS job adverts, covering 193,497 vacancies over 6 years, our large language model (LLM)-driven analysis estimated AI exposure scores of 1,542,411 tasks. By aggregating AI exposure scores for tasks within each role, we calculated the mean and variance of job-level exposure to AI, highlighting the heterogeneous impacts of AI, even for seemingly identical jobs. We then use an LLM to redesign jobs, focusing on task automation, task optimisation, and task reallocation. We find that the redesign process leads to tasks where humans have comparative advantage over AI, including strategic leadership, complex problem resolution, and stakeholder management. Overall, automation and augmentation are expected to have nuanced effects across all levels of the organisational hierarchy. Most economic value of AI is expected to arise from productivity gains rather than role displacement. We contribute to the automation, augmentation and productivity debates as well as advance our understanding of job redesign in the age of AI."
                },
                "tags": [
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T12:05:55Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    12,
                    5,
                    55,
                    4,
                    339,
                    0
                ],
                "arxiv_comment": "98 pages, 22 figures",
                "arxiv_primary_category": {
                    "term": "econ.GN"
                },
                "authors": [
                    {
                        "name": "Andrew Ledingham"
                    },
                    {
                        "name": "Michael Hollins"
                    },
                    {
                        "name": "Matthew Lyon"
                    },
                    {
                        "name": "David Gillespie"
                    },
                    {
                        "name": "Umar Yunis-Guerra"
                    },
                    {
                        "name": "Jamie Siviter"
                    },
                    {
                        "name": "David Duncan"
                    },
                    {
                        "name": "Oliver P. Hauser"
                    }
                ],
                "author_detail": {
                    "name": "Oliver P. Hauser"
                },
                "author": "Oliver P. Hauser"
            },
            {
                "id": "http://arxiv.org/abs/2512.05658v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05658v1",
                "title": "Grounded Multilingual Medical Reasoning for Question Answering with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Grounded Multilingual Medical Reasoning for Question Answering with Large Language Models"
                },
                "updated": "2025-12-05T12:05:46Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    12,
                    5,
                    46,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05658v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05658v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) with reasoning capabilities have recently demonstrated strong potential in medical Question Answering (QA). Existing approaches are largely English-focused and primarily rely on distillation from general-purpose LLMs, raising concerns about the reliability of their medical knowledge. In this work, we present a method to generate multilingual reasoning traces grounded in factual medical knowledge. We produce 500k traces in English, Italian, and Spanish, using a retrievalaugmented generation approach over medical information from Wikipedia. The traces are generated to solve medical questions drawn from MedQA and MedMCQA, which we extend to Italian and Spanish. We test our pipeline in both in-domain and outof-domain settings across Medical QA benchmarks, and demonstrate that our reasoning traces improve performance both when utilized via in-context learning (few-shot) and supervised fine-tuning, yielding state-of-the-art results among 8B-parameter LLMs. We believe that these resources can support the development of safer, more transparent clinical decision-support tools in multilingual settings. We release the full suite of resources: reasoning traces, translated QA datasets, Medical-Wikipedia, and fine-tuned models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) with reasoning capabilities have recently demonstrated strong potential in medical Question Answering (QA). Existing approaches are largely English-focused and primarily rely on distillation from general-purpose LLMs, raising concerns about the reliability of their medical knowledge. In this work, we present a method to generate multilingual reasoning traces grounded in factual medical knowledge. We produce 500k traces in English, Italian, and Spanish, using a retrievalaugmented generation approach over medical information from Wikipedia. The traces are generated to solve medical questions drawn from MedQA and MedMCQA, which we extend to Italian and Spanish. We test our pipeline in both in-domain and outof-domain settings across Medical QA benchmarks, and demonstrate that our reasoning traces improve performance both when utilized via in-context learning (few-shot) and supervised fine-tuning, yielding state-of-the-art results among 8B-parameter LLMs. We believe that these resources can support the development of safer, more transparent clinical decision-support tools in multilingual settings. We release the full suite of resources: reasoning traces, translated QA datasets, Medical-Wikipedia, and fine-tuned models."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T12:05:46Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    12,
                    5,
                    46,
                    4,
                    339,
                    0
                ],
                "arxiv_comment": "Under Review",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Pietro Ferrazzi"
                    },
                    {
                        "name": "Aitor Soroa"
                    },
                    {
                        "name": "Rodrigo Agerri"
                    }
                ],
                "author_detail": {
                    "name": "Rodrigo Agerri"
                },
                "author": "Rodrigo Agerri"
            },
            {
                "id": "http://arxiv.org/abs/2509.16625v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2509.16625v4",
                "title": "Self-Supervised Learning of Graph Representations for Network Intrusion Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Supervised Learning of Graph Representations for Network Intrusion Detection"
                },
                "updated": "2025-12-05T12:05:38Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    12,
                    5,
                    38,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2509.16625v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2509.16625v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Detecting intrusions in network traffic is a challenging task, particularly under limited supervision and constantly evolving attack patterns. While recent works have leveraged graph neural networks for network intrusion detection, they often decouple representation learning from anomaly detection, limiting the utility of the embeddings for identifying attacks. We propose GraphIDS, a self-supervised intrusion detection model that unifies these two stages by learning local graph representations of normal communication patterns through a masked autoencoder. An inductive graph neural network embeds each flow with its local topological context to capture typical network behavior, while a Transformer-based encoder-decoder reconstructs these embeddings, implicitly learning global co-occurrence patterns via self-attention without requiring explicit positional information. During inference, flows with unusually high reconstruction errors are flagged as potential intrusions. This end-to-end framework ensures that embeddings are directly optimized for the downstream task, facilitating the recognition of malicious traffic. On diverse NetFlow benchmarks, GraphIDS achieves up to 99.98% PR-AUC and 99.61% macro F1-score, outperforming baselines by 5-25 percentage points.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting intrusions in network traffic is a challenging task, particularly under limited supervision and constantly evolving attack patterns. While recent works have leveraged graph neural networks for network intrusion detection, they often decouple representation learning from anomaly detection, limiting the utility of the embeddings for identifying attacks. We propose GraphIDS, a self-supervised intrusion detection model that unifies these two stages by learning local graph representations of normal communication patterns through a masked autoencoder. An inductive graph neural network embeds each flow with its local topological context to capture typical network behavior, while a Transformer-based encoder-decoder reconstructs these embeddings, implicitly learning global co-occurrence patterns via self-attention without requiring explicit positional information. During inference, flows with unusually high reconstruction errors are flagged as potential intrusions. This end-to-end framework ensures that embeddings are directly optimized for the downstream task, facilitating the recognition of malicious traffic. On diverse NetFlow benchmarks, GraphIDS achieves up to 99.98% PR-AUC and 99.61% macro F1-score, outperforming baselines by 5-25 percentage points."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-09-20T11:02:50Z",
                "published_parsed": [
                    2025,
                    9,
                    20,
                    11,
                    2,
                    50,
                    5,
                    263,
                    0
                ],
                "arxiv_comment": "Accepted at NeurIPS 2025",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Lorenzo Guerra"
                    },
                    {
                        "name": "Thomas Chapuis"
                    },
                    {
                        "name": "Guillaume Duc"
                    },
                    {
                        "name": "Pavlo Mozharovskyi"
                    },
                    {
                        "name": "Van-Tam Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Van-Tam Nguyen"
                },
                "author": "Van-Tam Nguyen"
            },
            {
                "id": "http://arxiv.org/abs/2507.08931v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2507.08931v2",
                "title": "Cosmic Dipoles from Large-Scale Structure Surveys",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cosmic Dipoles from Large-Scale Structure Surveys"
                },
                "updated": "2025-12-05T11:58:55Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    11,
                    58,
                    55,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2507.08931v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2507.08931v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1103/ks44-qt3b",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "Large-scale structure surveys can be used to measure the dipole in the cosmic microwave background (CMB), in the luminosity distances inferred from type-Ia supernova observations, and in the spatial distribution of galaxies and quasars. The measurements of these cosmic dipoles appear to be mutually inconsistent, even though they are expected to indicate the common observer velocity. This observational tension may represent a significant challenge to the standard model of cosmology. Here we study in detail what contributes to the cosmic dipoles from CMB, supernova, and galaxy survey in the standard $$CDM model, though our theoretical model can be applied beyond the standard model. While measurements of the cosmic dipoles yield the relative velocities between the source samples and the observer velocity, the motion of the observer is the dominant contribution in the conformal Newtonian gauge, and the intrinsic velocities of the samples fall steeply with increasing redshift of the sources. Hence the cosmic dipoles of CMB, type-Ia supernovae, and galaxies should be aligned but can have different amplitudes. We also clarify several misconceptions that are commonly found in the literature.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale structure surveys can be used to measure the dipole in the cosmic microwave background (CMB), in the luminosity distances inferred from type-Ia supernova observations, and in the spatial distribution of galaxies and quasars. The measurements of these cosmic dipoles appear to be mutually inconsistent, even though they are expected to indicate the common observer velocity. This observational tension may represent a significant challenge to the standard model of cosmology. Here we study in detail what contributes to the cosmic dipoles from CMB, supernova, and galaxy survey in the standard $$CDM model, though our theoretical model can be applied beyond the standard model. While measurements of the cosmic dipoles yield the relative velocities between the source samples and the observer velocity, the motion of the observer is the dominant contribution in the conformal Newtonian gauge, and the intrinsic velocities of the samples fall steeply with increasing redshift of the sources. Hence the cosmic dipoles of CMB, type-Ia supernovae, and galaxies should be aligned but can have different amplitudes. We also clarify several misconceptions that are commonly found in the literature."
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-07-11T18:00:01Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    18,
                    0,
                    1,
                    4,
                    192,
                    0
                ],
                "arxiv_comment": "23 pages, 3 figures, published in PRD",
                "arxiv_primary_category": {
                    "term": "astro-ph.CO"
                },
                "arxiv_journal_ref": "Phys. Rev. D 112, 123013 (2025)",
                "authors": [
                    {
                        "name": "Jaiyul Yoo"
                    },
                    {
                        "name": "Matteo Magi"
                    },
                    {
                        "name": "Dragan Huterer"
                    }
                ],
                "author_detail": {
                    "name": "Dragan Huterer"
                },
                "arxiv_affiliation": "Michigan",
                "author": "Dragan Huterer",
                "arxiv_doi": "10.1103/ks44-qt3b"
            },
            {
                "id": "http://arxiv.org/abs/2512.05650v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05650v1",
                "title": "Efficient sequential Bayesian inference for state-space epidemic models using ensemble data assimilation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient sequential Bayesian inference for state-space epidemic models using ensemble data assimilation"
                },
                "updated": "2025-12-05T11:51:55Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    11,
                    51,
                    55,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05650v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05650v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Estimating latent epidemic states and model parameters from partially observed, noisy data remains a major challenge in infectious disease modeling. State-space formulations provide a coherent probabilistic framework for such inference, yet fully Bayesian estimation is often computationally prohibitive because evaluating the observed-data likelihood requires integration over all latent trajectories. The Sequential Monte Carlo squared (SMC$^2$) algorithm offers a principled approach for joint state and parameter inference, combining an outer SMC sampler over parameters with an inner particle filter that estimates the likelihood up to the current time point. Despite its theoretical appeal, this nested particle filter imposes substantial computational cost, limiting routine use in near-real-time outbreak response. We propose Ensemble SMC$^2$ (eSMC$^2$), a scalable variant that replaces the inner particle filter with an Ensemble Kalman Filter (EnKF) to approximate the incremental likelihood at each observation time. While this substitution introduces bias via a Gaussian approximation, we mitigate finite-sample effects using an unbiased Gaussian density estimator and adapt the EnKF for epidemic data through state-dependent observation variance. This makes our approach particularly suitable for overdispersed incidence data commonly encountered in infectious disease surveillance. Simulation experiments with known ground truth and an application to 2022 United States (U.S.) monkeypox incidence data demonstrate that eSMC$^2$ achieves substantial computational gains while producing posterior estimates comparable to SMC$^2$. The method accurately recovers latent epidemic trajectories and key epidemiological parameters, providing an efficient framework for sequential Bayesian inference from imperfect surveillance data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimating latent epidemic states and model parameters from partially observed, noisy data remains a major challenge in infectious disease modeling. State-space formulations provide a coherent probabilistic framework for such inference, yet fully Bayesian estimation is often computationally prohibitive because evaluating the observed-data likelihood requires integration over all latent trajectories. The Sequential Monte Carlo squared (SMC$^2$) algorithm offers a principled approach for joint state and parameter inference, combining an outer SMC sampler over parameters with an inner particle filter that estimates the likelihood up to the current time point. Despite its theoretical appeal, this nested particle filter imposes substantial computational cost, limiting routine use in near-real-time outbreak response. We propose Ensemble SMC$^2$ (eSMC$^2$), a scalable variant that replaces the inner particle filter with an Ensemble Kalman Filter (EnKF) to approximate the incremental likelihood at each observation time. While this substitution introduces bias via a Gaussian approximation, we mitigate finite-sample effects using an unbiased Gaussian density estimator and adapt the EnKF for epidemic data through state-dependent observation variance. This makes our approach particularly suitable for overdispersed incidence data commonly encountered in infectious disease surveillance. Simulation experiments with known ground truth and an application to 2022 United States (U.S.) monkeypox incidence data demonstrate that eSMC$^2$ achieves substantial computational gains while producing posterior estimates comparable to SMC$^2$. The method accurately recovers latent epidemic trajectories and key epidemiological parameters, providing an efficient framework for sequential Bayesian inference from imperfect surveillance data."
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T11:51:55Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    11,
                    51,
                    55,
                    4,
                    339,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME"
                },
                "authors": [
                    {
                        "name": "Dhorasso Temfack"
                    },
                    {
                        "name": "Jason Wyse"
                    }
                ],
                "author_detail": {
                    "name": "Jason Wyse"
                },
                "author": "Jason Wyse"
            },
            {
                "id": "http://arxiv.org/abs/2512.05648v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05648v1",
                "title": "Beyond Data Filtering: Knowledge Localization for Capability Removal in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Data Filtering: Knowledge Localization for Capability Removal in LLMs"
                },
                "updated": "2025-12-05T11:48:37Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    11,
                    48,
                    37,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05648v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05648v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models increasingly possess capabilities that carry dual-use risks. While data filtering has emerged as a pretraining-time mitigation, it faces significant challenges: labeling whether data is harmful is expensive at scale, and given improving sample efficiency with larger models, even small amounts of mislabeled content could give rise to dangerous capabilities. To address risks associated with mislabeled harmful content, prior work proposed Gradient Routing (Cloud et al., 2024) -- a technique that localizes target knowledge into a dedicated subset of model parameters so they can later be removed. We explore an improved variant of Gradient Routing, which we call Selective GradienT Masking (SGTM), with particular focus on evaluating its robustness to label noise. SGTM zero-masks selected gradients such that target domain examples only update their dedicated parameters. We test SGTM's effectiveness in two applications: removing knowledge of one language from a model trained on a bilingual synthetic dataset, and removing biology knowledge from a model trained on English Wikipedia. In both cases SGTM provides better retain/forget trade-off in the presence of labeling errors compared to both data filtering and a previously proposed instantiation of Gradient Routing. Unlike shallow unlearning approaches that can be quickly undone through fine-tuning, SGTM exhibits strong robustness to adversarial fine-tuning, requiring seven times more fine-tuning steps to reach baseline performance on the forget set compared to a finetuning-based unlearning method (RMU). Our results suggest SGTM provides a promising pretraining-time complement to existing safety mitigations, particularly in settings where label noise is unavoidable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models increasingly possess capabilities that carry dual-use risks. While data filtering has emerged as a pretraining-time mitigation, it faces significant challenges: labeling whether data is harmful is expensive at scale, and given improving sample efficiency with larger models, even small amounts of mislabeled content could give rise to dangerous capabilities. To address risks associated with mislabeled harmful content, prior work proposed Gradient Routing (Cloud et al., 2024) -- a technique that localizes target knowledge into a dedicated subset of model parameters so they can later be removed. We explore an improved variant of Gradient Routing, which we call Selective GradienT Masking (SGTM), with particular focus on evaluating its robustness to label noise. SGTM zero-masks selected gradients such that target domain examples only update their dedicated parameters. We test SGTM's effectiveness in two applications: removing knowledge of one language from a model trained on a bilingual synthetic dataset, and removing biology knowledge from a model trained on English Wikipedia. In both cases SGTM provides better retain/forget trade-off in the presence of labeling errors compared to both data filtering and a previously proposed instantiation of Gradient Routing. Unlike shallow unlearning approaches that can be quickly undone through fine-tuning, SGTM exhibits strong robustness to adversarial fine-tuning, requiring seven times more fine-tuning steps to reach baseline performance on the forget set compared to a finetuning-based unlearning method (RMU). Our results suggest SGTM provides a promising pretraining-time complement to existing safety mitigations, particularly in settings where label noise is unavoidable."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T11:48:37Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    11,
                    48,
                    37,
                    4,
                    339,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Igor Shilov"
                    },
                    {
                        "name": "Alex Cloud"
                    },
                    {
                        "name": "Aryo Pradipta Gema"
                    },
                    {
                        "name": "Jacob Goldman-Wetzler"
                    },
                    {
                        "name": "Nina Panickssery"
                    },
                    {
                        "name": "Henry Sleight"
                    },
                    {
                        "name": "Erik Jones"
                    },
                    {
                        "name": "Cem Anil"
                    }
                ],
                "author_detail": {
                    "name": "Cem Anil"
                },
                "author": "Cem Anil"
            },
            {
                "id": "http://arxiv.org/abs/2512.05647v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05647v1",
                "title": "A Greek Government Decisions Dataset for Public-Sector Analysis and Insight",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Greek Government Decisions Dataset for Public-Sector Analysis and Insight"
                },
                "updated": "2025-12-05T11:47:33Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    11,
                    47,
                    33,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05647v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05647v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We introduce an open, machine-readable corpus of Greek government decisions sourced from the national transparency platform Diavgeia. The resource comprises 1 million decisions, featuring and high-quality raw text extracted from PDFs. It is released with raw extracted text in Markdown format, alongside a fully reproducible extraction pipeline. Beyond the core dataset, we conduct qualitative analyses to explore boilerplate patterns and design a retrieval-augmented generation (RAG) task by formulating a set of representative questions, creating high-quality answers, and evaluating a baseline RAG system on its ability to retrieve and reason over public decisions. This evaluation demonstrates the potential of large-scale public-sector corpora to support advanced information access and transparency through structured retrieval and reasoning over governmental documents, and highlights how such a RAG pipeline could simulate a chat-based assistant capable of interactively answering questions about public decisions. Due to its scale, quality, and domain coverage, the corpus can also serve as high-value pre-training or fine-tuning material for new Language Models (LMs) and Large Language Models (LLMs) respectively, including specialized models for legal and governmental domains, and as a foundation for novel approaches in domain adaptation, knowledge-grounded generation, and explainable AI. Finally, we discuss limitations, outline future directions, and make both the data and the code accessible.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce an open, machine-readable corpus of Greek government decisions sourced from the national transparency platform Diavgeia. The resource comprises 1 million decisions, featuring and high-quality raw text extracted from PDFs. It is released with raw extracted text in Markdown format, alongside a fully reproducible extraction pipeline. Beyond the core dataset, we conduct qualitative analyses to explore boilerplate patterns and design a retrieval-augmented generation (RAG) task by formulating a set of representative questions, creating high-quality answers, and evaluating a baseline RAG system on its ability to retrieve and reason over public decisions. This evaluation demonstrates the potential of large-scale public-sector corpora to support advanced information access and transparency through structured retrieval and reasoning over governmental documents, and highlights how such a RAG pipeline could simulate a chat-based assistant capable of interactively answering questions about public decisions. Due to its scale, quality, and domain coverage, the corpus can also serve as high-value pre-training or fine-tuning material for new Language Models (LMs) and Large Language Models (LLMs) respectively, including specialized models for legal and governmental domains, and as a foundation for novel approaches in domain adaptation, knowledge-grounded generation, and explainable AI. Finally, we discuss limitations, outline future directions, and make both the data and the code accessible."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T11:47:33Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    11,
                    47,
                    33,
                    4,
                    339,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Giorgos Antoniou"
                    },
                    {
                        "name": "Giorgos Filandrianos"
                    },
                    {
                        "name": "Aggelos Vlachos"
                    },
                    {
                        "name": "Giorgos Stamou"
                    },
                    {
                        "name": "Lampros Kollimenos"
                    },
                    {
                        "name": "Konstantinos Skianis"
                    },
                    {
                        "name": "Michalis Vazirgiannis"
                    }
                ],
                "author_detail": {
                    "name": "Michalis Vazirgiannis"
                },
                "author": "Michalis Vazirgiannis"
            },
            {
                "id": "http://arxiv.org/abs/2507.20423v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2507.20423v2",
                "title": "CodeNER: Code Prompting for Named Entity Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeNER: Code Prompting for Named Entity Recognition"
                },
                "updated": "2025-12-05T11:44:29Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    11,
                    44,
                    29,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2507.20423v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2507.20423v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recent studies have explored various approaches for treating candidate named entity spans as both source and target sequences in named entity recognition (NER) by leveraging large language models (LLMs). Although previous approaches have successfully generated candidate named entity spans with suitable labels, they rely solely on input context information when using LLMs, particularly, ChatGPT. However, NER inherently requires capturing detailed labeling requirements with input context information. To address this issue, we propose a novel method that leverages code-based prompting to improve the capabilities of LLMs in understanding and performing NER. By embedding code within prompts, we provide detailed BIO schema instructions for labeling, thereby exploiting the ability of LLMs to comprehend long-range scopes in programming languages. Experimental results demonstrate that the proposed code-based prompting method outperforms conventional text-based prompting on ten benchmarks across English, Arabic, Finnish, Danish, and German datasets, indicating the effectiveness of explicitly structuring NER instructions. We also verify that combining the proposed code-based prompting method with the chain-of-thought prompting further improves performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies have explored various approaches for treating candidate named entity spans as both source and target sequences in named entity recognition (NER) by leveraging large language models (LLMs). Although previous approaches have successfully generated candidate named entity spans with suitable labels, they rely solely on input context information when using LLMs, particularly, ChatGPT. However, NER inherently requires capturing detailed labeling requirements with input context information. To address this issue, we propose a novel method that leverages code-based prompting to improve the capabilities of LLMs in understanding and performing NER. By embedding code within prompts, we provide detailed BIO schema instructions for labeling, thereby exploiting the ability of LLMs to comprehend long-range scopes in programming languages. Experimental results demonstrate that the proposed code-based prompting method outperforms conventional text-based prompting on ten benchmarks across English, Arabic, Finnish, Danish, and German datasets, indicating the effectiveness of explicitly structuring NER instructions. We also verify that combining the proposed code-based prompting method with the chain-of-thought prompting further improves performance."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-07-27T21:49:36Z",
                "published_parsed": [
                    2025,
                    7,
                    27,
                    21,
                    49,
                    36,
                    6,
                    208,
                    0
                ],
                "arxiv_comment": "18 pages, 6 figures",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Sungwoo Han"
                    },
                    {
                        "name": "Jingun Kwon"
                    },
                    {
                        "name": "Hidetaka Kamigaito"
                    },
                    {
                        "name": "Manabu Okumura"
                    }
                ],
                "author_detail": {
                    "name": "Manabu Okumura"
                },
                "author": "Manabu Okumura"
            },
            {
                "id": "http://arxiv.org/abs/2511.18491v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.18491v3",
                "title": "MindEval: Benchmarking Language Models on Multi-turn Mental Health Support",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MindEval: Benchmarking Language Models on Multi-turn Mental Health Support"
                },
                "updated": "2025-12-05T11:28:14Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    11,
                    28,
                    14,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.18491v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.18491v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Demand for mental health support through AI chatbots is surging, though current systems present several limitations, like sycophancy or overvalidation, and reinforcement of maladaptive beliefs. A core obstacle to the creation of better systems is the scarcity of benchmarks that capture the complexity of real therapeutic interactions. Most existing benchmarks either only test clinical knowledge through multiple-choice questions or assess single responses in isolation. To bridge this gap, we present MindEval, a framework designed in collaboration with Ph.D-level Licensed Clinical Psychologists for automatically evaluating language models in realistic, multi-turn mental health therapy conversations. Through patient simulation and automatic evaluation with LLMs, our framework balances resistance to gaming with reproducibility via its fully automated, model-agnostic design. We begin by quantitatively validating the realism of our simulated patients against human-generated text and by demonstrating strong correlations between automatic and human expert judgments. Then, we evaluate 12 state-of-the-art LLMs and show that all models struggle, scoring below 4 out of 6, on average, with particular weaknesses in problematic AI-specific patterns of communication. Notably, reasoning capabilities and model scale do not guarantee better performance, and systems deteriorate with longer interactions or when supporting patients with severe symptoms. We release all code, prompts, and human evaluation data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demand for mental health support through AI chatbots is surging, though current systems present several limitations, like sycophancy or overvalidation, and reinforcement of maladaptive beliefs. A core obstacle to the creation of better systems is the scarcity of benchmarks that capture the complexity of real therapeutic interactions. Most existing benchmarks either only test clinical knowledge through multiple-choice questions or assess single responses in isolation. To bridge this gap, we present MindEval, a framework designed in collaboration with Ph.D-level Licensed Clinical Psychologists for automatically evaluating language models in realistic, multi-turn mental health therapy conversations. Through patient simulation and automatic evaluation with LLMs, our framework balances resistance to gaming with reproducibility via its fully automated, model-agnostic design. We begin by quantitatively validating the realism of our simulated patients against human-generated text and by demonstrating strong correlations between automatic and human expert judgments. Then, we evaluate 12 state-of-the-art LLMs and show that all models struggle, scoring below 4 out of 6, on average, with particular weaknesses in problematic AI-specific patterns of communication. Notably, reasoning capabilities and model scale do not guarantee better performance, and systems deteriorate with longer interactions or when supporting patients with severe symptoms. We release all code, prompts, and human evaluation data."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-23T15:19:29Z",
                "published_parsed": [
                    2025,
                    11,
                    23,
                    15,
                    19,
                    29,
                    6,
                    327,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Jos Pombal"
                    },
                    {
                        "name": "Maya D'Eon"
                    },
                    {
                        "name": "Nuno M. Guerreiro"
                    },
                    {
                        "name": "Pedro Henrique Martins"
                    },
                    {
                        "name": "Antnio Farinhas"
                    },
                    {
                        "name": "Ricardo Rei"
                    }
                ],
                "author_detail": {
                    "name": "Ricardo Rei"
                },
                "author": "Ricardo Rei"
            },
            {
                "id": "http://arxiv.org/abs/2510.01894v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.01894v2",
                "title": "Multi-marginal temporal Schrdinger Bridge Matching from unpaired data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-marginal temporal Schrdinger Bridge Matching from unpaired data"
                },
                "updated": "2025-12-05T11:26:28Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    11,
                    26,
                    28,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.01894v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.01894v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Many natural dynamic processes -- such as in vivo cellular differentiation or disease progression -- can only be observed through the lens of static sample snapshots. While challenging, reconstructing their temporal evolution to decipher underlying dynamic properties is of major interest to scientific research. Existing approaches enable data transport along a temporal axis but are poorly scalable in high dimension and require restrictive assumptions to be met. To address these issues, we propose Multi-Marginal temporal Schrdinger Bridge Matching (MMtSBM) from unpaired data, extending the theoretical guarantees and empirical efficiency of Diffusion Schrdinger Bridge Matching (arXiv:2303.16852) by deriving the Iterative Markovian Fitting algorithm to multiple marginals in a novel factorized fashion. Experiments show that MMtSBM retains theoretical properties on toy examples, achieves state-of-the-art performance on real-world datasets such as transcriptomic trajectory inference in 100 dimensions, and, for the first time, recovers couplings and dynamics in very high-dimensional image settings. Our work establishes multi-marginal Schrdinger bridges as a practical and principled approach for recovering hidden dynamics from static data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many natural dynamic processes -- such as in vivo cellular differentiation or disease progression -- can only be observed through the lens of static sample snapshots. While challenging, reconstructing their temporal evolution to decipher underlying dynamic properties is of major interest to scientific research. Existing approaches enable data transport along a temporal axis but are poorly scalable in high dimension and require restrictive assumptions to be met. To address these issues, we propose Multi-Marginal temporal Schrdinger Bridge Matching (MMtSBM) from unpaired data, extending the theoretical guarantees and empirical efficiency of Diffusion Schrdinger Bridge Matching (arXiv:2303.16852) by deriving the Iterative Markovian Fitting algorithm to multiple marginals in a novel factorized fashion. Experiments show that MMtSBM retains theoretical properties on toy examples, achieves state-of-the-art performance on real-world datasets such as transcriptomic trajectory inference in 100 dimensions, and, for the first time, recovers couplings and dynamics in very high-dimensional image settings. Our work establishes multi-marginal Schrdinger bridges as a practical and principled approach for recovering hidden dynamics from static data."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-02T11:00:58Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    11,
                    0,
                    58,
                    3,
                    275,
                    0
                ],
                "arxiv_comment": "Under review. Code available at https://github.com/tgravier/MMDSBM-pytorch (GitHub repository) Additional experimental materials available at https://mmdsbm.notion.site (supplementary resources)",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Thomas Gravier"
                    },
                    {
                        "name": "Thomas Boyer"
                    },
                    {
                        "name": "Auguste Genovesio"
                    }
                ],
                "author_detail": {
                    "name": "Auguste Genovesio"
                },
                "author": "Auguste Genovesio"
            },
            {
                "id": "http://arxiv.org/abs/2505.17101v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2505.17101v4",
                "title": "A quantitative analysis of semantic information in deep representations of text and images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A quantitative analysis of semantic information in deep representations of text and images"
                },
                "updated": "2025-12-05T11:14:03Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    11,
                    14,
                    3,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2505.17101v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2505.17101v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Deep neural networks are known to develop similar representations for semantically related data, even when they belong to different domains, such as an image and its description, or the same text in different languages. We present a method for quantitatively investigating this phenomenon by measuring the relative information content of the representations of semantically related data and probing how it is encoded into multiple tokens of large language models (LLMs) and vision transformers. Looking first at how LLMs process pairs of translated sentences, we identify inner ``semantic'' layers containing the most language-transferable information. We find moreover that, on these layers, a larger LLM (DeepSeek-V3) extracts significantly more general information than a smaller one (Llama3.1-8B). Semantic information of English text is spread across many tokens and it is characterized by long-distance correlations between tokens and by a causal left-to-right (i.e., past-future) asymmetry. We also identify layers encoding semantic information within visual transformers. We show that caption representations in the semantic layers of LLMs predict visual representations of the corresponding images. We observe significant and model-dependent information asymmetries between image and text representations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep neural networks are known to develop similar representations for semantically related data, even when they belong to different domains, such as an image and its description, or the same text in different languages. We present a method for quantitatively investigating this phenomenon by measuring the relative information content of the representations of semantically related data and probing how it is encoded into multiple tokens of large language models (LLMs) and vision transformers. Looking first at how LLMs process pairs of translated sentences, we identify inner ``semantic'' layers containing the most language-transferable information. We find moreover that, on these layers, a larger LLM (DeepSeek-V3) extracts significantly more general information than a smaller one (Llama3.1-8B). Semantic information of English text is spread across many tokens and it is characterized by long-distance correlations between tokens and by a causal left-to-right (i.e., past-future) asymmetry. We also identify layers encoding semantic information within visual transformers. We show that caption representations in the semantic layers of LLMs predict visual representations of the corresponding images. We observe significant and model-dependent information asymmetries between image and text representations."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-05-21T07:38:48Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    7,
                    38,
                    48,
                    2,
                    141,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Santiago Acevedo"
                    },
                    {
                        "name": "Andrea Mascaretti"
                    },
                    {
                        "name": "Riccardo Rende"
                    },
                    {
                        "name": "Mato Mahaut"
                    },
                    {
                        "name": "Marco Baroni"
                    },
                    {
                        "name": "Alessandro Laio"
                    }
                ],
                "author_detail": {
                    "name": "Alessandro Laio"
                },
                "author": "Alessandro Laio"
            },
            {
                "id": "http://arxiv.org/abs/2512.05623v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05623v1",
                "title": "Bounded Graph Clustering with Graph Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bounded Graph Clustering with Graph Neural Networks"
                },
                "updated": "2025-12-05T11:06:59Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    11,
                    6,
                    59,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05623v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05623v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "In community detection, many methods require the user to specify the number of clusters in advance since an exhaustive search over all possible values is computationally infeasible. While some classical algorithms can infer this number directly from the data, this is typically not the case for graph neural networks (GNNs): even when a desired number of clusters is specified, standard GNN-based methods often fail to return the exact number due to the way they are designed. In this work, we address this limitation by introducing a flexible and principled way to control the number of communities discovered by GNNs. Rather than assuming the true number of clusters is known, we propose a framework that allows the user to specify a plausible range and enforce these bounds during training. However, if the user wants an exact number of clusters, it may also be specified and reliably returned.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In community detection, many methods require the user to specify the number of clusters in advance since an exhaustive search over all possible values is computationally infeasible. While some classical algorithms can infer this number directly from the data, this is typically not the case for graph neural networks (GNNs): even when a desired number of clusters is specified, standard GNN-based methods often fail to return the exact number due to the way they are designed. In this work, we address this limitation by introducing a flexible and principled way to control the number of communities discovered by GNNs. Rather than assuming the true number of clusters is known, we propose a framework that allows the user to specify a plausible range and enforce these bounds during training. However, if the user wants an exact number of clusters, it may also be specified and reliably returned."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T11:06:59Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    11,
                    6,
                    59,
                    4,
                    339,
                    0
                ],
                "arxiv_comment": "17 pages, 8 figures",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Kibidi Neocosmos"
                    },
                    {
                        "name": "Diego Baptista"
                    },
                    {
                        "name": "Nicole Ludwig"
                    }
                ],
                "author_detail": {
                    "name": "Nicole Ludwig"
                },
                "author": "Nicole Ludwig"
            },
            {
                "id": "http://arxiv.org/abs/2512.05613v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05613v1",
                "title": "DistillFSS: Synthesizing Few-Shot Knowledge into a Lightweight Segmentation Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DistillFSS: Synthesizing Few-Shot Knowledge into a Lightweight Segmentation Model"
                },
                "updated": "2025-12-05T10:54:23Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    10,
                    54,
                    23,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05613v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05613v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Cross-Domain Few-Shot Semantic Segmentation (CD-FSS) seeks to segment unknown classes in unseen domains using only a few annotated examples. This setting is inherently challenging: source and target domains exhibit substantial distribution shifts, label spaces are disjoint, and support images are scarce--making standard episodic methods unreliable and computationally demanding at test time. To address these constraints, we propose DistillFSS, a framework that embeds support-set knowledge directly into a model's parameters through a teacher--student distillation process. By internalizing few-shot reasoning into a dedicated layer within the student network, DistillFSS eliminates the need for support images at test time, enabling fast, lightweight inference, while allowing efficient extension to novel classes in unseen domains through rapid teacher-driven specialization. Combined with fine-tuning, the approach scales efficiently to large support sets and significantly reduces computational overhead. To evaluate the framework under realistic conditions, we introduce a new CD-FSS benchmark spanning medical imaging, industrial inspection, and remote sensing, with disjoint label spaces and variable support sizes. Experiments show that DistillFSS matches or surpasses state-of-the-art baselines, particularly in multi-class and multi-shot scenarios, while offering substantial efficiency gains. The code is available at https://github.com/pasqualedem/DistillFSS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-Domain Few-Shot Semantic Segmentation (CD-FSS) seeks to segment unknown classes in unseen domains using only a few annotated examples. This setting is inherently challenging: source and target domains exhibit substantial distribution shifts, label spaces are disjoint, and support images are scarce--making standard episodic methods unreliable and computationally demanding at test time. To address these constraints, we propose DistillFSS, a framework that embeds support-set knowledge directly into a model's parameters through a teacher--student distillation process. By internalizing few-shot reasoning into a dedicated layer within the student network, DistillFSS eliminates the need for support images at test time, enabling fast, lightweight inference, while allowing efficient extension to novel classes in unseen domains through rapid teacher-driven specialization. Combined with fine-tuning, the approach scales efficiently to large support sets and significantly reduces computational overhead. To evaluate the framework under realistic conditions, we introduce a new CD-FSS benchmark spanning medical imaging, industrial inspection, and remote sensing, with disjoint label spaces and variable support sizes. Experiments show that DistillFSS matches or surpasses state-of-the-art baselines, particularly in multi-class and multi-shot scenarios, while offering substantial efficiency gains. The code is available at https://github.com/pasqualedem/DistillFSS."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T10:54:23Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    10,
                    54,
                    23,
                    4,
                    339,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Pasquale De Marinis"
                    },
                    {
                        "name": "Pieter M. Blok"
                    },
                    {
                        "name": "Uzay Kaymak"
                    },
                    {
                        "name": "Rogier Brussee"
                    },
                    {
                        "name": "Gennaro Vessio"
                    },
                    {
                        "name": "Giovanna Castellano"
                    }
                ],
                "author_detail": {
                    "name": "Giovanna Castellano"
                },
                "author": "Giovanna Castellano"
            },
            {
                "id": "http://arxiv.org/abs/2506.03761v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2506.03761v2",
                "title": "Pet-Bench: Benchmarking the Abilities of Large Language Models as E-Pets in Social Network Services",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pet-Bench: Benchmarking the Abilities of Large Language Models as E-Pets in Social Network Services"
                },
                "updated": "2025-12-05T10:49:58Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    10,
                    49,
                    58,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2506.03761v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2506.03761v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "As interest in using Large Language Models for interactive and emotionally rich experiences grows, virtual pet companionship emerges as a novel yet underexplored application. Existing approaches focus on basic pet role-playing interactions without systematically benchmarking LLMs for comprehensive companionship. In this paper, we introduce Pet-Bench, a dedicated benchmark that evaluates LLMs across both self-interaction and human-interaction dimensions. Unlike prior work, Pet-Bench emphasizes self-evolution and developmental behaviors alongside interactive engagement, offering a more realistic reflection of pet companionship. It features diverse tasks such as intelligent scheduling, memory-based dialogues, and psychological conversations, with over 7,500 interaction instances designed to simulate pet behaviors. Evaluation of 28 LLMs reveals significant performance variations linked to model size and inherent capabilities, underscoring the need for specialized optimization in this domain. Pet-Bench serves as a foundational resource for benchmarking pet-related LLM abilities and advancing emotionally immersive human-pet interactions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As interest in using Large Language Models for interactive and emotionally rich experiences grows, virtual pet companionship emerges as a novel yet underexplored application. Existing approaches focus on basic pet role-playing interactions without systematically benchmarking LLMs for comprehensive companionship. In this paper, we introduce Pet-Bench, a dedicated benchmark that evaluates LLMs across both self-interaction and human-interaction dimensions. Unlike prior work, Pet-Bench emphasizes self-evolution and developmental behaviors alongside interactive engagement, offering a more realistic reflection of pet companionship. It features diverse tasks such as intelligent scheduling, memory-based dialogues, and psychological conversations, with over 7,500 interaction instances designed to simulate pet behaviors. Evaluation of 28 LLMs reveals significant performance variations linked to model size and inherent capabilities, underscoring the need for specialized optimization in this domain. Pet-Bench serves as a foundational resource for benchmarking pet-related LLM abilities and advancing emotionally immersive human-pet interactions."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-06-04T09:25:52Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    9,
                    25,
                    52,
                    2,
                    155,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Hongcheng Guo"
                    },
                    {
                        "name": "Zheyong Xie"
                    },
                    {
                        "name": "Shaosheng Cao"
                    },
                    {
                        "name": "Boyang Wang"
                    },
                    {
                        "name": "Weiting Liu"
                    },
                    {
                        "name": "Zheyu Ye"
                    },
                    {
                        "name": "Zhoujun Li"
                    },
                    {
                        "name": "Zuozhu Liu"
                    },
                    {
                        "name": "Wei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Wei Liu"
                },
                "author": "Wei Liu"
            },
            {
                "id": "http://arxiv.org/abs/2512.05597v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05597v1",
                "title": "Fast SceneScript: Accurate and Efficient Structured Language Model via Multi-Token Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast SceneScript: Accurate and Efficient Structured Language Model via Multi-Token Prediction"
                },
                "updated": "2025-12-05T10:35:43Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    10,
                    35,
                    43,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05597v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05597v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recent perception-generalist approaches based on language models have achieved state-of-the-art results across diverse tasks, including 3D scene layout estimation, via unified architecture and interface. However, these approaches rely on autoregressive next-token prediction, which is inherently slow. In this work, we introduce Fast SceneScript, a novel structured language model for accurate and efficient 3D scene layout estimation. Our method employs multi-token prediction (MTP) to reduce the number of autoregressive iterations and significantly accelerate inference. While MTP improves speed, unreliable token predictions can significantly reduce accuracy. To filter out unreliable tokens, we adapt self-speculative decoding (SSD) for structured language models and introduce confidence-guided decoding (CGD) with an improved scoring mechanism for token reliability. Furthermore, we design a parameter-efficient mechanism that reduces the parameter overhead of MTP. Extensive experiments on the ASE and Structured3D benchmarks demonstrate that Fast SceneScript can generate up to 9 tokens per decoder inference step without compromising accuracy, while adding only $\\sim7.5\\%$ additional parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent perception-generalist approaches based on language models have achieved state-of-the-art results across diverse tasks, including 3D scene layout estimation, via unified architecture and interface. However, these approaches rely on autoregressive next-token prediction, which is inherently slow. In this work, we introduce Fast SceneScript, a novel structured language model for accurate and efficient 3D scene layout estimation. Our method employs multi-token prediction (MTP) to reduce the number of autoregressive iterations and significantly accelerate inference. While MTP improves speed, unreliable token predictions can significantly reduce accuracy. To filter out unreliable tokens, we adapt self-speculative decoding (SSD) for structured language models and introduce confidence-guided decoding (CGD) with an improved scoring mechanism for token reliability. Furthermore, we design a parameter-efficient mechanism that reduces the parameter overhead of MTP. Extensive experiments on the ASE and Structured3D benchmarks demonstrate that Fast SceneScript can generate up to 9 tokens per decoder inference step without compromising accuracy, while adding only $\\sim7.5\\%$ additional parameters."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T10:35:43Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    10,
                    35,
                    43,
                    4,
                    339,
                    0
                ],
                "arxiv_comment": "10 pages, 8 figures",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Ruihong Yin"
                    },
                    {
                        "name": "Xuepeng Shi"
                    },
                    {
                        "name": "Oleksandr Bailo"
                    },
                    {
                        "name": "Marco Manfredi"
                    },
                    {
                        "name": "Theo Gevers"
                    }
                ],
                "author_detail": {
                    "name": "Theo Gevers"
                },
                "author": "Theo Gevers"
            },
            {
                "id": "http://arxiv.org/abs/2512.05594v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05594v1",
                "title": "Ontology Learning with LLMs: A Benchmark Study on Axiom Identification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ontology Learning with LLMs: A Benchmark Study on Axiom Identification"
                },
                "updated": "2025-12-05T10:28:56Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    10,
                    28,
                    56,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05594v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05594v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Ontologies are an important tool for structuring domain knowledge, but their development is a complex task that requires significant modelling and domain expertise. Ontology learning, aimed at automating this process, has seen advancements in the past decade with the improvement of Natural Language Processing techniques, and especially with the recent growth of Large Language Models (LLMs). This paper investigates the challenge of identifying axioms: fundamental ontology components that define logical relations between classes and properties. In this work, we introduce an Ontology Axiom Benchmark OntoAxiom, and systematically test LLMs on that benchmark for axiom identification, evaluating different prompting strategies, ontologies, and axiom types. The benchmark consists of nine medium-sized ontologies with together 17.118 triples, and 2.771 axioms. We focus on subclass, disjoint, subproperty, domain, and range axioms. To evaluate LLM performance, we compare twelve LLMs with three shot settings and two prompting strategies: a Direct approach where we query all axioms at once, versus an Axiom-by-Axiom (AbA) approach, where each prompt queries for one axiom only. Our findings show that the AbA prompting leads to higher F1 scores than the direct approach. However, performance varies across axioms, suggesting that certain axioms are more challenging to identify. The domain also influences performance: the FOAF ontology achieves a score of 0.642 for the subclass axiom, while the music ontology reaches only 0.218. Larger LLMs outperform smaller ones, but smaller models may still be viable for resource-constrained settings. Although performance overall is not high enough to fully automate axiom identification, LLMs can provide valuable candidate axioms to support ontology engineers with the development and refinement of ontologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ontologies are an important tool for structuring domain knowledge, but their development is a complex task that requires significant modelling and domain expertise. Ontology learning, aimed at automating this process, has seen advancements in the past decade with the improvement of Natural Language Processing techniques, and especially with the recent growth of Large Language Models (LLMs). This paper investigates the challenge of identifying axioms: fundamental ontology components that define logical relations between classes and properties. In this work, we introduce an Ontology Axiom Benchmark OntoAxiom, and systematically test LLMs on that benchmark for axiom identification, evaluating different prompting strategies, ontologies, and axiom types. The benchmark consists of nine medium-sized ontologies with together 17.118 triples, and 2.771 axioms. We focus on subclass, disjoint, subproperty, domain, and range axioms. To evaluate LLM performance, we compare twelve LLMs with three shot settings and two prompting strategies: a Direct approach where we query all axioms at once, versus an Axiom-by-Axiom (AbA) approach, where each prompt queries for one axiom only. Our findings show that the AbA prompting leads to higher F1 scores than the direct approach. However, performance varies across axioms, suggesting that certain axioms are more challenging to identify. The domain also influences performance: the FOAF ontology achieves a score of 0.642 for the subclass axiom, while the music ontology reaches only 0.218. Larger LLMs outperform smaller ones, but smaller models may still be viable for resource-constrained settings. Although performance overall is not high enough to fully automate axiom identification, LLMs can provide valuable candidate axioms to support ontology engineers with the development and refinement of ontologies."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T10:28:56Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    10,
                    28,
                    56,
                    4,
                    339,
                    0
                ],
                "arxiv_comment": "Submitted to Semantic Web Journal, under review",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Roos M. Bakker"
                    },
                    {
                        "name": "Daan L. Di Scala"
                    },
                    {
                        "name": "Maaike H. T. de Boer"
                    },
                    {
                        "name": "Stephan A. Raaijmakers"
                    }
                ],
                "author_detail": {
                    "name": "Stephan A. Raaijmakers"
                },
                "author": "Stephan A. Raaijmakers"
            },
            {
                "id": "http://arxiv.org/abs/2512.05581v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05581v1",
                "title": "Growth rate measurements from a joint analysis of the large-scale galaxy clustering in Fourier and configuration space",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Growth rate measurements from a joint analysis of the large-scale galaxy clustering in Fourier and configuration space"
                },
                "updated": "2025-12-05T10:07:25Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    10,
                    7,
                    25,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05581v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05581v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "In this work, we test a framework to perform the analysis of redshift-space distortions simultaneously in configuration and Fourier space. We test our methods with the AbacusSummit suite of N-body simulations as well as a more numerous set of approximate EZmocks, reproducing the sample of luminous red galaxies of from the Baryon Oscillation Spectroscopic Survey (BOSS) and its extension (eBOSS). Our clustering models are based on the effective field theory of large-scale structures in a Lagrangian frame, used in the latest results from the Dark Energy Spectroscopic Instrument. We perform a template type of analysis, including dilation parameters and the slope parameter from the ShapeFit framework. We find that the joint space inference yields unbiased and robust constraints on simulated datasets, consistent with results from individual spaces or previous methods to obtain consensus results. Our joint space analysis on the the BOSS+eBOSS LRG sample obtains $ f_8 = 0.463 \\pm 0.052 $, in good agreement with the official 2020 results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we test a framework to perform the analysis of redshift-space distortions simultaneously in configuration and Fourier space. We test our methods with the AbacusSummit suite of N-body simulations as well as a more numerous set of approximate EZmocks, reproducing the sample of luminous red galaxies of from the Baryon Oscillation Spectroscopic Survey (BOSS) and its extension (eBOSS). Our clustering models are based on the effective field theory of large-scale structures in a Lagrangian frame, used in the latest results from the Dark Energy Spectroscopic Instrument. We perform a template type of analysis, including dilation parameters and the slope parameter from the ShapeFit framework. We find that the joint space inference yields unbiased and robust constraints on simulated datasets, consistent with results from individual spaces or previous methods to obtain consensus results. Our joint space analysis on the the BOSS+eBOSS LRG sample obtains $ f_8 = 0.463 \\pm 0.052 $, in good agreement with the official 2020 results."
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T10:07:25Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    10,
                    7,
                    25,
                    4,
                    339,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO"
                },
                "authors": [
                    {
                        "name": "Vincenzo Aronica"
                    },
                    {
                        "name": "Julian E. Bautista"
                    },
                    {
                        "name": "Arnaud de Mattia"
                    },
                    {
                        "name": "Hector Gil-Marn"
                    }
                ],
                "author_detail": {
                    "name": "Hector Gil-Marn"
                },
                "author": "Hector Gil-Marn"
            },
            {
                "id": "http://arxiv.org/abs/2512.05580v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05580v1",
                "title": "Structured Reasoning with Tree-of-Thoughts for Bengali Math Word Problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structured Reasoning with Tree-of-Thoughts for Bengali Math Word Problems"
                },
                "updated": "2025-12-05T10:07:08Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    10,
                    7,
                    8,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05580v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05580v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Mathematical Word Problems (MWPs) are among the most challenging tasks in natural language processing because they require both linguistic understanding and multi-step numerical reasoning. While Chain-of-Thought (CoT) prompting has shown promise, its linear structure often propagates errors, limiting overall effectiveness. To address this limitation, we present the a systematic study of Tree-of-Thought (ToT) reasoning for Bengali MWPs using the SOMADHAN dataset. Owing to computational and token-cost constraints, we evaluate a curated set of 100 representative problems across multiple large language models (LLMs), including GPT-OSS and LLaMA variants, under standard prompting, CoT, and ToT strategies. Our results show that CoT improves baseline accuracy from 78% (standard prompting) to 83% on average, while ToT further increases performance by up to 5 percentage points, achieving 88% accuracy with GPT-OSS-120B. These improvements highlight that ToT is particularly effective in medium-to-large-scale models but may offer less advantage for smaller ones. Overall, our findings establish ToT as a robust framework for solving mathematical problems in low-resource languages such as Bengali. More broadly, this study shows that structured reasoning methods like ToT can provide more reliable and globally consistent outcomes than CoT, paving the way for better reasoning strategies in multilingual NLP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mathematical Word Problems (MWPs) are among the most challenging tasks in natural language processing because they require both linguistic understanding and multi-step numerical reasoning. While Chain-of-Thought (CoT) prompting has shown promise, its linear structure often propagates errors, limiting overall effectiveness. To address this limitation, we present the a systematic study of Tree-of-Thought (ToT) reasoning for Bengali MWPs using the SOMADHAN dataset. Owing to computational and token-cost constraints, we evaluate a curated set of 100 representative problems across multiple large language models (LLMs), including GPT-OSS and LLaMA variants, under standard prompting, CoT, and ToT strategies. Our results show that CoT improves baseline accuracy from 78% (standard prompting) to 83% on average, while ToT further increases performance by up to 5 percentage points, achieving 88% accuracy with GPT-OSS-120B. These improvements highlight that ToT is particularly effective in medium-to-large-scale models but may offer less advantage for smaller ones. Overall, our findings establish ToT as a robust framework for solving mathematical problems in low-resource languages such as Bengali. More broadly, this study shows that structured reasoning methods like ToT can provide more reliable and globally consistent outcomes than CoT, paving the way for better reasoning strategies in multilingual NLP."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T10:07:08Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    10,
                    7,
                    8,
                    4,
                    339,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Aurprita Mahmood"
                    },
                    {
                        "name": "Sabrin alam"
                    },
                    {
                        "name": "Neloy kumer Sagor"
                    },
                    {
                        "name": "Md. Abdul Hadi"
                    },
                    {
                        "name": "Md. Sehab Al Islam"
                    },
                    {
                        "name": "Minhajul Islam"
                    }
                ],
                "author_detail": {
                    "name": "Minhajul Islam"
                },
                "author": "Minhajul Islam"
            },
            {
                "id": "http://arxiv.org/abs/2505.22143v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2505.22143v2",
                "title": "3D Question Answering via only 2D Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D Question Answering via only 2D Vision-Language Models"
                },
                "updated": "2025-12-05T10:05:57Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    10,
                    5,
                    57,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2505.22143v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2505.22143v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large vision-language models (LVLMs) have significantly advanced numerous fields. In this work, we explore how to harness their potential to address 3D scene understanding tasks, using 3D question answering (3D-QA) as a representative example. Due to the limited training data in 3D, we do not train LVLMs but infer in a zero-shot manner. Specifically, we sample 2D views from a 3D point cloud and feed them into 2D models to answer a given question. When the 2D model is chosen, e.g., LLAVA-OV, the quality of sampled views matters the most. We propose cdViews, a novel approach to automatically selecting critical and diverse Views for 3D-QA. cdViews consists of two key components: viewSelector prioritizing critical views based on their potential to provide answer-specific information, and viewNMS enhancing diversity by removing redundant views based on spatial overlap. We evaluate cdViews on the widely-used ScanQA and SQA benchmarks, demonstrating that it achieves state-of-the-art performance in 3D-QA while relying solely on 2D models without fine-tuning. These findings support our belief that 2D LVLMs are currently the most effective alternative (of the resource-intensive 3D LVLMs) for addressing 3D tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large vision-language models (LVLMs) have significantly advanced numerous fields. In this work, we explore how to harness their potential to address 3D scene understanding tasks, using 3D question answering (3D-QA) as a representative example. Due to the limited training data in 3D, we do not train LVLMs but infer in a zero-shot manner. Specifically, we sample 2D views from a 3D point cloud and feed them into 2D models to answer a given question. When the 2D model is chosen, e.g., LLAVA-OV, the quality of sampled views matters the most. We propose cdViews, a novel approach to automatically selecting critical and diverse Views for 3D-QA. cdViews consists of two key components: viewSelector prioritizing critical views based on their potential to provide answer-specific information, and viewNMS enhancing diversity by removing redundant views based on spatial overlap. We evaluate cdViews on the widely-used ScanQA and SQA benchmarks, demonstrating that it achieves state-of-the-art performance in 3D-QA while relying solely on 2D models without fine-tuning. These findings support our belief that 2D LVLMs are currently the most effective alternative (of the resource-intensive 3D LVLMs) for addressing 3D tasks."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-05-28T09:04:39Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    9,
                    4,
                    39,
                    2,
                    148,
                    0
                ],
                "arxiv_comment": "ICML2025",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Fengyun Wang"
                    },
                    {
                        "name": "Sicheng Yu"
                    },
                    {
                        "name": "Jiawei Wu"
                    },
                    {
                        "name": "Jinhui Tang"
                    },
                    {
                        "name": "Hanwang Zhang"
                    },
                    {
                        "name": "Qianru Sun"
                    }
                ],
                "author_detail": {
                    "name": "Qianru Sun"
                },
                "author": "Qianru Sun"
            },
            {
                "id": "http://arxiv.org/abs/2512.05576v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05576v1",
                "title": "CureAgent: A Training-Free Executor-Analyst Framework for Clinical Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CureAgent: A Training-Free Executor-Analyst Framework for Clinical Reasoning"
                },
                "updated": "2025-12-05T09:56:58Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    9,
                    56,
                    58,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05576v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05576v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Current clinical agent built on small LLMs, such as TxAgent suffer from a \\textit{Context Utilization Failure}, where models successfully retrieve biomedical evidence due to supervised finetuning but fail to ground their diagnosis in that information. In this work, we propose the Executor-Analyst Framework, a modular architecture that decouples the syntactic precision of tool execution from the semantic robustness of clinical reasoning. By orchestrating specialized TxAgents (Executors) with long-context foundation models (Analysts), we mitigate the reasoning deficits observed in monolithic models. Beyond simple modularity, we demonstrate that a Stratified Ensemble strategy significantly outperforms global pooling by preserving evidentiary diversity, effectively addressing the information bottleneck. Furthermore, our stress tests reveal critical scaling insights: (1) a \\textit{Context-Performance Paradox}, where extending reasoning contexts beyond 12k tokens introduces noise that degrades accuracy; and (2) the \\textit{Curse of Dimensionality} in action spaces, where expanding toolsets necessitates hierarchical retrieval strategies. Crucially, our approach underscores the potential of training-free architectural engineering, achieving state-of-the-art performance on CURE-Bench without the need for expensive end-to-end finetuning. This provides a scalable, agile foundation for the next generation of trustworthy AI-driven therapeutics. Code has been released on https://github.com/June01/CureAgent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current clinical agent built on small LLMs, such as TxAgent suffer from a \\textit{Context Utilization Failure}, where models successfully retrieve biomedical evidence due to supervised finetuning but fail to ground their diagnosis in that information. In this work, we propose the Executor-Analyst Framework, a modular architecture that decouples the syntactic precision of tool execution from the semantic robustness of clinical reasoning. By orchestrating specialized TxAgents (Executors) with long-context foundation models (Analysts), we mitigate the reasoning deficits observed in monolithic models. Beyond simple modularity, we demonstrate that a Stratified Ensemble strategy significantly outperforms global pooling by preserving evidentiary diversity, effectively addressing the information bottleneck. Furthermore, our stress tests reveal critical scaling insights: (1) a \\textit{Context-Performance Paradox}, where extending reasoning contexts beyond 12k tokens introduces noise that degrades accuracy; and (2) the \\textit{Curse of Dimensionality} in action spaces, where expanding toolsets necessitates hierarchical retrieval strategies. Crucially, our approach underscores the potential of training-free architectural engineering, achieving state-of-the-art performance on CURE-Bench without the need for expensive end-to-end finetuning. This provides a scalable, agile foundation for the next generation of trustworthy AI-driven therapeutics. Code has been released on https://github.com/June01/CureAgent."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T09:56:58Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    9,
                    56,
                    58,
                    4,
                    339,
                    0
                ],
                "arxiv_comment": "2nd Place Solution to the CURE-Bench Competition @ NeurIPS 2025. Code available at https://github.com/June01/CureAgent",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Ting-Ting Xie"
                    },
                    {
                        "name": "Yixin Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yixin Zhang"
                },
                "author": "Yixin Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2411.10500v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2411.10500v2",
                "title": "Edge-Only Universal Adversarial Attacks in Distributed Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge-Only Universal Adversarial Attacks in Distributed Learning"
                },
                "updated": "2025-12-05T09:54:54Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    9,
                    54,
                    54,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2411.10500v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2411.10500v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Distributed learning frameworks, which partition neural network models across multiple computing nodes, enhance efficiency in collaborative edge-cloud systems, but may also introduce new vulnerabilities to evasion attacks, often in the form of adversarial perturbations. In this work, we present a new threat model that explores the feasibility of generating universal adversarial perturbations (UAPs) when the attacker has access only to the edge portion of the model, consisting of its initial network layers. Unlike traditional attacks that require full model knowledge, our approach shows that adversaries can induce effective mispredictions in the unknown cloud component by manipulating key feature representations at the edge. Following the proposed threat model, we introduce both edge-only untargeted and targeted formulations of UAPs designed to control intermediate features before the split point. Our results on ImageNet demonstrate strong attack transferability to the unknown cloud part, and we compare the proposed method with classical white-box and black-box techniques, highlighting its effectiveness. Additionally, we analyze the capability of an attacker to achieve targeted adversarial effects with edge-only knowledge, revealing intriguing behaviors across multiple networks. By introducing the first adversarial attacks with edge-only knowledge in split inference, this work underscores the importance of addressing partial model access in adversarial robustness, encouraging further research in this area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributed learning frameworks, which partition neural network models across multiple computing nodes, enhance efficiency in collaborative edge-cloud systems, but may also introduce new vulnerabilities to evasion attacks, often in the form of adversarial perturbations. In this work, we present a new threat model that explores the feasibility of generating universal adversarial perturbations (UAPs) when the attacker has access only to the edge portion of the model, consisting of its initial network layers. Unlike traditional attacks that require full model knowledge, our approach shows that adversaries can induce effective mispredictions in the unknown cloud component by manipulating key feature representations at the edge. Following the proposed threat model, we introduce both edge-only untargeted and targeted formulations of UAPs designed to control intermediate features before the split point. Our results on ImageNet demonstrate strong attack transferability to the unknown cloud part, and we compare the proposed method with classical white-box and black-box techniques, highlighting its effectiveness. Additionally, we analyze the capability of an attacker to achieve targeted adversarial effects with edge-only knowledge, revealing intriguing behaviors across multiple networks. By introducing the first adversarial attacks with edge-only knowledge in split inference, this work underscores the importance of addressing partial model access in adversarial robustness, encouraging further research in this area."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-11-15T11:06:24Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    11,
                    6,
                    24,
                    4,
                    320,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Giulio Rossolini"
                    },
                    {
                        "name": "Tommaso Baldi"
                    },
                    {
                        "name": "Alessandro Biondi"
                    },
                    {
                        "name": "Giorgio Buttazzo"
                    }
                ],
                "author_detail": {
                    "name": "Giorgio Buttazzo"
                },
                "author": "Giorgio Buttazzo"
            },
            {
                "id": "http://arxiv.org/abs/2502.00791v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2502.00791v4",
                "title": "Vision-centric Token Compression in Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-centric Token Compression in Large Language Model"
                },
                "updated": "2025-12-05T09:48:41Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    9,
                    48,
                    41,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2502.00791v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2502.00791v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Real-world applications are stretching context windows to hundreds of thousand of tokens while Large Language Models (LLMs) swell from billions to trillions of parameters. This dual expansion send compute and memory costs skyrocketing, making token compression indispensable. We introduce Vision Centric Token Compression (Vist), a slow-fast compression framework that mirrors human reading: the fast path renders distant tokens into images, letting a frozen, lightweight vision encoder skim the low-salience context; the slow path feeds the proximal window into the LLM for fine-grained reasoning. A Probability-Informed Visual Enhancement (PVE) objective masks high-frequency tokens during training, steering the Resampler to concentrate on semantically rich regions-just as skilled reader gloss over function words. On eleven in-context learning benchmarks, Vist achieves the same accuracy with 2.3 times fewer tokens, cutting FLOPs by 16% and memory by 50%. This method delivers remarkable results, outperforming the strongest text encoder-based compression method CEPE by 7.6% on average over benchmarks like TriviaQA, NQ, PopQA, NLUI, and CLIN, setting a new standard for token efficiency in LLMs. The project is at https://github.com/CSU-JPG/VIST.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-world applications are stretching context windows to hundreds of thousand of tokens while Large Language Models (LLMs) swell from billions to trillions of parameters. This dual expansion send compute and memory costs skyrocketing, making token compression indispensable. We introduce Vision Centric Token Compression (Vist), a slow-fast compression framework that mirrors human reading: the fast path renders distant tokens into images, letting a frozen, lightweight vision encoder skim the low-salience context; the slow path feeds the proximal window into the LLM for fine-grained reasoning. A Probability-Informed Visual Enhancement (PVE) objective masks high-frequency tokens during training, steering the Resampler to concentrate on semantically rich regions-just as skilled reader gloss over function words. On eleven in-context learning benchmarks, Vist achieves the same accuracy with 2.3 times fewer tokens, cutting FLOPs by 16% and memory by 50%. This method delivers remarkable results, outperforming the strongest text encoder-based compression method CEPE by 7.6% on average over benchmarks like TriviaQA, NQ, PopQA, NLUI, and CLIN, setting a new standard for token efficiency in LLMs. The project is at https://github.com/CSU-JPG/VIST."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-02-02T13:10:06Z",
                "published_parsed": [
                    2025,
                    2,
                    2,
                    13,
                    10,
                    6,
                    6,
                    33,
                    0
                ],
                "arxiv_comment": "NeurIPS 2025 spotlight",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Ling Xing"
                    },
                    {
                        "name": "Alex Jinpeng Wang"
                    },
                    {
                        "name": "Rui Yan"
                    },
                    {
                        "name": "Xiangbo Shu"
                    },
                    {
                        "name": "Jinhui Tang"
                    }
                ],
                "author_detail": {
                    "name": "Jinhui Tang"
                },
                "author": "Jinhui Tang"
            },
            {
                "id": "http://arxiv.org/abs/2512.05566v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05566v1",
                "title": "Machine and Deep Learning Regression for Compact Object Equations of State",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine and Deep Learning Regression for Compact Object Equations of State"
                },
                "updated": "2025-12-05T09:43:05Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    9,
                    43,
                    5,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05566v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05566v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "A central open problem in nuclear physics is the determination of a physically robust equation of state (EoS) for dense nuclear matter, which directly informs our understanding of the internal composition and macroscopic properties of compact objects such as neutron stars and quark stars. Traditional efforts have relied primarily on theoretical modeling grounded in nuclear and particle physics, with subsequent validation against empirical constraints from heavy ion collisions and, increasingly, multimessenger astrophysical observations. Recent developments, however, have introduced complementary analytical strategies that merge theoretical modeling with advanced data driven methodologies. In particular, Bayesian inference, machine learning, and deep learning have emerged as powerful tools for constraining the EoS and extracting physical insight from complex observational datasets. In this work, we employ state of the art machine learning and deep learning techniques to analyze mass radius relations of compact objects with the aim of reconstructing or inferring their underlying equations of state. The analysis is based on an extensive library of physically consistent, multimodal EoSs for neutron stars and a corresponding set for quark stars, each constructed to satisfy established theoretical and observational constraints. By leveraging the predictive capacity of these computational frameworks, we demonstrate the potential of data-driven approaches to provide refined insights into the behavior of matter at supranuclear densities and to contribute to a more unified understanding of the dense matter EoS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A central open problem in nuclear physics is the determination of a physically robust equation of state (EoS) for dense nuclear matter, which directly informs our understanding of the internal composition and macroscopic properties of compact objects such as neutron stars and quark stars. Traditional efforts have relied primarily on theoretical modeling grounded in nuclear and particle physics, with subsequent validation against empirical constraints from heavy ion collisions and, increasingly, multimessenger astrophysical observations. Recent developments, however, have introduced complementary analytical strategies that merge theoretical modeling with advanced data driven methodologies. In particular, Bayesian inference, machine learning, and deep learning have emerged as powerful tools for constraining the EoS and extracting physical insight from complex observational datasets. In this work, we employ state of the art machine learning and deep learning techniques to analyze mass radius relations of compact objects with the aim of reconstructing or inferring their underlying equations of state. The analysis is based on an extensive library of physically consistent, multimodal EoSs for neutron stars and a corresponding set for quark stars, each constructed to satisfy established theoretical and observational constraints. By leveraging the predictive capacity of these computational frameworks, we demonstrate the potential of data-driven approaches to provide refined insights into the behavior of matter at supranuclear densities and to contribute to a more unified understanding of the dense matter EoS."
                },
                "tags": [
                    {
                        "term": "nucl-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T09:43:05Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    9,
                    43,
                    5,
                    4,
                    339,
                    0
                ],
                "arxiv_comment": "15 pages, 14 figures. Any comments are welcome",
                "arxiv_primary_category": {
                    "term": "nucl-th"
                },
                "authors": [
                    {
                        "name": "I. Stergakis"
                    },
                    {
                        "name": "Th. Diakonidis"
                    },
                    {
                        "name": "Ch. C. Moustakidis"
                    }
                ],
                "author_detail": {
                    "name": "Ch. C. Moustakidis"
                },
                "author": "Ch. C. Moustakidis"
            },
            {
                "id": "http://arxiv.org/abs/2512.05564v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05564v1",
                "title": "ProPhy: Progressive Physical Alignment for Dynamic World Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProPhy: Progressive Physical Alignment for Dynamic World Simulation"
                },
                "updated": "2025-12-05T09:39:26Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    9,
                    39,
                    26,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05564v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05564v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recent advances in video generation have shown remarkable potential for constructing world simulators. However, current models still struggle to produce physically consistent results, particularly when handling large-scale or complex dynamics. This limitation arises primarily because existing approaches respond isotropically to physical prompts and neglect the fine-grained alignment between generated content and localized physical cues. To address these challenges, we propose ProPhy, a Progressive Physical Alignment Framework that enables explicit physics-aware conditioning and anisotropic generation. ProPhy employs a two-stage Mixture-of-Physics-Experts (MoPE) mechanism for discriminative physical prior extraction, where Semantic Experts infer semantic-level physical principles from textual descriptions, and Refinement Experts capture token-level physical dynamics. This mechanism allows the model to learn fine-grained, physics-aware video representations that better reflect underlying physical laws. Furthermore, we introduce a physical alignment strategy that transfers the physical reasoning capabilities of vision-language models (VLMs) into the Refinement Experts, facilitating a more accurate representation of dynamic physical phenomena. Extensive experiments on physics-aware video generation benchmarks demonstrate that ProPhy produces more realistic, dynamic, and physically coherent results than existing state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in video generation have shown remarkable potential for constructing world simulators. However, current models still struggle to produce physically consistent results, particularly when handling large-scale or complex dynamics. This limitation arises primarily because existing approaches respond isotropically to physical prompts and neglect the fine-grained alignment between generated content and localized physical cues. To address these challenges, we propose ProPhy, a Progressive Physical Alignment Framework that enables explicit physics-aware conditioning and anisotropic generation. ProPhy employs a two-stage Mixture-of-Physics-Experts (MoPE) mechanism for discriminative physical prior extraction, where Semantic Experts infer semantic-level physical principles from textual descriptions, and Refinement Experts capture token-level physical dynamics. This mechanism allows the model to learn fine-grained, physics-aware video representations that better reflect underlying physical laws. Furthermore, we introduce a physical alignment strategy that transfers the physical reasoning capabilities of vision-language models (VLMs) into the Refinement Experts, facilitating a more accurate representation of dynamic physical phenomena. Extensive experiments on physics-aware video generation benchmarks demonstrate that ProPhy produces more realistic, dynamic, and physically coherent results than existing state-of-the-art methods."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T09:39:26Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    9,
                    39,
                    26,
                    4,
                    339,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Zijun Wang"
                    },
                    {
                        "name": "Panwen Hu"
                    },
                    {
                        "name": "Jing Wang"
                    },
                    {
                        "name": "Terry Jingchen Zhang"
                    },
                    {
                        "name": "Yuhao Cheng"
                    },
                    {
                        "name": "Long Chen"
                    },
                    {
                        "name": "Yiqiang Yan"
                    },
                    {
                        "name": "Zutao Jiang"
                    },
                    {
                        "name": "Hanhui Li"
                    },
                    {
                        "name": "Xiaodan Liang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaodan Liang"
                },
                "author": "Xiaodan Liang"
            },
            {
                "id": "http://arxiv.org/abs/2511.15718v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.15718v2",
                "title": "ToolMind Technical Report: A Large-Scale, Reasoning-Enhanced Tool-Use Dataset",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ToolMind Technical Report: A Large-Scale, Reasoning-Enhanced Tool-Use Dataset"
                },
                "updated": "2025-12-05T09:32:33Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    9,
                    32,
                    33,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.15718v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.15718v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Model (LLM) agents have developed rapidly in recent years to solve complex real-world problems using external tools. However, the scarcity of high-quality trajectories still hinders the development of stronger LLM agents. Most existing works on multi-turn dialogue synthesis validate correctness only at the trajectory level, which may overlook turn-level errors that can propagate during training and degrade model performance. To address these limitations, we introduce ToolMind, a large-scale, high-quality tool-agentic dataset with 160k synthetic data instances generated using over 20k tools and 200k augmented open-source data instances. Our data synthesis pipeline first constructs a function graph based on parameter correlations and then uses a multi-agent framework to simulate realistic user-assistant-tool interactions. Beyond trajectory-level validation, we employ fine-grained turn-level filtering to remove erroneous or suboptimal steps, ensuring that only high-quality reasoning traces are retained. This approach mitigates error amplification during training while preserving self-corrective reasoning signals essential for robust tool-use learning. Models fine-tuned on ToolMind show significant improvements over baselines on several benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) agents have developed rapidly in recent years to solve complex real-world problems using external tools. However, the scarcity of high-quality trajectories still hinders the development of stronger LLM agents. Most existing works on multi-turn dialogue synthesis validate correctness only at the trajectory level, which may overlook turn-level errors that can propagate during training and degrade model performance. To address these limitations, we introduce ToolMind, a large-scale, high-quality tool-agentic dataset with 160k synthetic data instances generated using over 20k tools and 200k augmented open-source data instances. Our data synthesis pipeline first constructs a function graph based on parameter correlations and then uses a multi-agent framework to simulate realistic user-assistant-tool interactions. Beyond trajectory-level validation, we employ fine-grained turn-level filtering to remove erroneous or suboptimal steps, ensuring that only high-quality reasoning traces are retained. This approach mitigates error amplification during training while preserving self-corrective reasoning signals essential for robust tool-use learning. Models fine-tuned on ToolMind show significant improvements over baselines on several benchmarks."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-12T13:01:23Z",
                "published_parsed": [
                    2025,
                    11,
                    12,
                    13,
                    1,
                    23,
                    2,
                    316,
                    0
                ],
                "arxiv_comment": "15 pages",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Chen Yang"
                    },
                    {
                        "name": "Ran Le"
                    },
                    {
                        "name": "Yun Xing"
                    },
                    {
                        "name": "Zhenwei An"
                    },
                    {
                        "name": "Zongchao Chen"
                    },
                    {
                        "name": "Wayne Xin Zhao"
                    },
                    {
                        "name": "Yang Song"
                    },
                    {
                        "name": "Tao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tao Zhang"
                },
                "author": "Tao Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2511.08916v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.08916v4",
                "title": "HalluClean: A Unified Framework to Combat Hallucinations in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HalluClean: A Unified Framework to Combat Hallucinations in LLMs"
                },
                "updated": "2025-12-05T09:31:19Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    9,
                    31,
                    19,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.08916v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.08916v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) have achieved impressive performance across a wide range of natural language processing tasks, yet they often produce hallucinated content that undermines factual reliability. To address this challenge, we introduce HalluClean, a lightweight and task-agnostic framework for detecting and correcting hallucinations in LLM-generated text. HalluClean adopts a reasoning-enhanced paradigm, explicitly decomposing the process into planning, execution, and revision stages to identify and refine unsupported claims. It employs minimal task-routing prompts to enable zero-shot generalization across diverse domains, without relying on external knowledge sources or supervised detectors. We conduct extensive evaluations on five representative tasks-question answering, dialogue, summarization, math word problems, and contradiction detection. Experimental results show that HalluClean significantly improves factual consistency and outperforms competitive baselines, demonstrating its potential to enhance the trustworthiness of LLM outputs in real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved impressive performance across a wide range of natural language processing tasks, yet they often produce hallucinated content that undermines factual reliability. To address this challenge, we introduce HalluClean, a lightweight and task-agnostic framework for detecting and correcting hallucinations in LLM-generated text. HalluClean adopts a reasoning-enhanced paradigm, explicitly decomposing the process into planning, execution, and revision stages to identify and refine unsupported claims. It employs minimal task-routing prompts to enable zero-shot generalization across diverse domains, without relying on external knowledge sources or supervised detectors. We conduct extensive evaluations on five representative tasks-question answering, dialogue, summarization, math word problems, and contradiction detection. Experimental results show that HalluClean significantly improves factual consistency and outperforms competitive baselines, demonstrating its potential to enhance the trustworthiness of LLM outputs in real-world applications."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-12T02:50:56Z",
                "published_parsed": [
                    2025,
                    11,
                    12,
                    2,
                    50,
                    56,
                    2,
                    316,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Yaxin Zhao"
                    },
                    {
                        "name": "Yu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Zhang"
                },
                "author": "Yu Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2512.05557v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05557v1",
                "title": "2K-Characters-10K-Stories: A Quality-Gated Stylized Narrative Dataset with Disentangled Control and Sequence Consistency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "2K-Characters-10K-Stories: A Quality-Gated Stylized Narrative Dataset with Disentangled Control and Sequence Consistency"
                },
                "updated": "2025-12-05T09:26:24Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    9,
                    26,
                    24,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05557v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05557v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Sequential identity consistency under precise transient attribute control remains a long-standing challenge in controllable visual storytelling. Existing datasets lack sufficient fidelity and fail to disentangle stable identities from transient attributes, limiting structured control over pose, expression, and scene composition and thus constraining reliable sequential synthesis. To address this gap, we introduce \\textbf{2K-Characters-10K-Stories}, a multi-modal stylized narrative dataset of \\textbf{2{,}000} uniquely stylized characters appearing across \\textbf{10{,}000} illustration stories. It is the first dataset that pairs large-scale unique identities with explicit, decoupled control signals for sequential identity consistency. We introduce a \\textbf{Human-in-the-Loop pipeline (HiL)} that leverages expert-verified character templates and LLM-guided narrative planning to generate highly-aligned structured data. A \\textbf{decoupled control} scheme separates persistent identity from transient attributes -- pose and expression -- while a \\textbf{Quality-Gated loop} integrating MMLM evaluation, Auto-Prompt Tuning, and Local Image Editing enforces pixel-level consistency. Extensive experiments demonstrate that models fine-tuned on our dataset achieves performance comparable to closed-source models in generating visual narratives.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequential identity consistency under precise transient attribute control remains a long-standing challenge in controllable visual storytelling. Existing datasets lack sufficient fidelity and fail to disentangle stable identities from transient attributes, limiting structured control over pose, expression, and scene composition and thus constraining reliable sequential synthesis. To address this gap, we introduce \\textbf{2K-Characters-10K-Stories}, a multi-modal stylized narrative dataset of \\textbf{2{,}000} uniquely stylized characters appearing across \\textbf{10{,}000} illustration stories. It is the first dataset that pairs large-scale unique identities with explicit, decoupled control signals for sequential identity consistency. We introduce a \\textbf{Human-in-the-Loop pipeline (HiL)} that leverages expert-verified character templates and LLM-guided narrative planning to generate highly-aligned structured data. A \\textbf{decoupled control} scheme separates persistent identity from transient attributes -- pose and expression -- while a \\textbf{Quality-Gated loop} integrating MMLM evaluation, Auto-Prompt Tuning, and Local Image Editing enforces pixel-level consistency. Extensive experiments demonstrate that models fine-tuned on our dataset achieves performance comparable to closed-source models in generating visual narratives."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T09:26:24Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    9,
                    26,
                    24,
                    4,
                    339,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Xingxi Yin"
                    },
                    {
                        "name": "Yicheng Li"
                    },
                    {
                        "name": "Gong Yan"
                    },
                    {
                        "name": "Chenglin Li"
                    },
                    {
                        "name": "Jian Zhao"
                    },
                    {
                        "name": "Cong Huang"
                    },
                    {
                        "name": "Yue Deng"
                    },
                    {
                        "name": "Yin Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yin Zhang"
                },
                "author": "Yin Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2510.21084v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.21084v2",
                "title": "Chinese Discharge Drug Recommendation in Metabolic Diseases with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chinese Discharge Drug Recommendation in Metabolic Diseases with Large Language Models"
                },
                "updated": "2025-12-05T09:12:05Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    9,
                    12,
                    5,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.21084v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.21084v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Intelligent drug recommendation based on Electronic Health Records (EHRs) is critical for improving the quality and efficiency of clinical decision-making. By leveraging large-scale patient data, drug recommendation systems can assist physicians in selecting the most appropriate medications according to a patient's medical history, diagnoses, laboratory results, and comorbidities. Recent advances in large language models (LLMs) have shown remarkable capabilities in complex reasoning and medical text understanding, making them promising tools for drug recommendation tasks. However, the application of LLMs for Chinese clinical medication recommendation remains largely unexplored. In this work, we conduct a systematic investigation of LLM-based methodologies for Chinese discharge medication recommendation. We evaluate several representative LLM families (GLM, Llama, Qwen) under a unified methodological framework including zero-shot prompting, in-context learning, chain-of-thought prompting, and supervised fine-tuning using LoRA. We analyze model behavior across reasoning styles, error patterns, domain adaptation mechanisms, and robustness. Experimental results show that while supervised fine-tuning improves model performance, there remains substantial room for improvement, with the best model achieving the F1 score of 0.5648 and Jaccard score of 0.4477. Our findings highlight both the potential and limitations of LLMs for Chinese drug recommendation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intelligent drug recommendation based on Electronic Health Records (EHRs) is critical for improving the quality and efficiency of clinical decision-making. By leveraging large-scale patient data, drug recommendation systems can assist physicians in selecting the most appropriate medications according to a patient's medical history, diagnoses, laboratory results, and comorbidities. Recent advances in large language models (LLMs) have shown remarkable capabilities in complex reasoning and medical text understanding, making them promising tools for drug recommendation tasks. However, the application of LLMs for Chinese clinical medication recommendation remains largely unexplored. In this work, we conduct a systematic investigation of LLM-based methodologies for Chinese discharge medication recommendation. We evaluate several representative LLM families (GLM, Llama, Qwen) under a unified methodological framework including zero-shot prompting, in-context learning, chain-of-thought prompting, and supervised fine-tuning using LoRA. We analyze model behavior across reasoning styles, error patterns, domain adaptation mechanisms, and robustness. Experimental results show that while supervised fine-tuning improves model performance, there remains substantial room for improvement, with the best model achieving the F1 score of 0.5648 and Jaccard score of 0.4477. Our findings highlight both the potential and limitations of LLMs for Chinese drug recommendation."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-24T01:47:23Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    1,
                    47,
                    23,
                    4,
                    297,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Juntao Li"
                    },
                    {
                        "name": "Haobin Yuan"
                    },
                    {
                        "name": "Ling Luo"
                    },
                    {
                        "name": "Yan Jiang"
                    },
                    {
                        "name": "Fan Wang"
                    },
                    {
                        "name": "Ping Zhang"
                    },
                    {
                        "name": "Huiyi Lv"
                    },
                    {
                        "name": "Jian Wang"
                    },
                    {
                        "name": "Yuanyuan Sun"
                    },
                    {
                        "name": "Hongfei Lin"
                    }
                ],
                "author_detail": {
                    "name": "Hongfei Lin"
                },
                "author": "Hongfei Lin"
            },
            {
                "id": "http://arxiv.org/abs/2512.05548v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05548v1",
                "title": "How Dark Sector Equations of State Govern Interaction Signatures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Dark Sector Equations of State Govern Interaction Signatures"
                },
                "updated": "2025-12-05T09:11:21Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    9,
                    11,
                    21,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05548v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05548v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Using late-Universe observations, we demonstrate that freeing dark energy and dark matter equations of state (EoS) dramatically alters the inferred strength and direction of their interactions. When dark sector EoS are fixed to $w_{\\mathrm{de}}=-1$ and $w_{\\mathrm{dm}}=0$, the data consistently favor an energy transfer from dark energy to dark matter across various interaction forms. This apparent evidence, however, proves highly sensitive to the EoS assumptions: treating $w_{\\mathrm{de}}$ as a free parameter substantially weakens the evidence for interaction, with its value converging to the quintessence regime ($w_{\\mathrm{de}}>-1$). In contrast, freeing $w_{\\mathrm{dm}}$ maintains a preference for interaction, revealing a correlation where positive $w_{\\mathrm{dm}}$ is associated with energy transfer from dark energy to dark matter, and negative $w_{\\mathrm{dm}}$ with energy transfer from dark matter to dark energy. These findings caution against the simplistic assumption of $$CDM EoS values when attempting to detect a possible interaction. Despite these fundamental degeneracies, model comparison using the Akaike and Deviance information criteria shows that all of the tested interacting dark energy scenarios receive substantial support over the $$CDM model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using late-Universe observations, we demonstrate that freeing dark energy and dark matter equations of state (EoS) dramatically alters the inferred strength and direction of their interactions. When dark sector EoS are fixed to $w_{\\mathrm{de}}=-1$ and $w_{\\mathrm{dm}}=0$, the data consistently favor an energy transfer from dark energy to dark matter across various interaction forms. This apparent evidence, however, proves highly sensitive to the EoS assumptions: treating $w_{\\mathrm{de}}$ as a free parameter substantially weakens the evidence for interaction, with its value converging to the quintessence regime ($w_{\\mathrm{de}}>-1$). In contrast, freeing $w_{\\mathrm{dm}}$ maintains a preference for interaction, revealing a correlation where positive $w_{\\mathrm{dm}}$ is associated with energy transfer from dark energy to dark matter, and negative $w_{\\mathrm{dm}}$ with energy transfer from dark matter to dark energy. These findings caution against the simplistic assumption of $$CDM EoS values when attempting to detect a possible interaction. Despite these fundamental degeneracies, model comparison using the Akaike and Deviance information criteria shows that all of the tested interacting dark energy scenarios receive substantial support over the $$CDM model."
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T09:11:21Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    9,
                    11,
                    21,
                    4,
                    339,
                    0
                ],
                "arxiv_comment": "7 pages, 2 figures,",
                "arxiv_primary_category": {
                    "term": "astro-ph.CO"
                },
                "authors": [
                    {
                        "name": "Peng-Ju Wu"
                    },
                    {
                        "name": "Ming Zhang"
                    },
                    {
                        "name": "Shang-Jie Jin"
                    }
                ],
                "author_detail": {
                    "name": "Shang-Jie Jin"
                },
                "author": "Shang-Jie Jin"
            },
            {
                "id": "http://arxiv.org/abs/2512.05546v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05546v1",
                "title": "Conscious Gaze: Adaptive Attention Mechanisms for Hallucination Mitigation in Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conscious Gaze: Adaptive Attention Mechanisms for Hallucination Mitigation in Vision-Language Models"
                },
                "updated": "2025-12-05T09:07:55Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    9,
                    7,
                    55,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05546v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05546v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Vision-Language Models (VLMs) often exhibit text inertia, where attention drifts from visual evidence toward linguistic priors, resulting in object hallucinations. Existing decoding strategies intervene only at the output logits and thus cannot correct internal reasoning drift, while recent internal-control methods based on heuristic head suppression or global steering vectors lack principled grounding. We introduce Conscious Gaze (CG-VLM), a training-free, inference-time framework that converts game-theoretic interpretability into actionable decoding control. A Cognitive Demand Sensor built on Harsanyi interactions estimates instantaneous vision-text synergy and identifies moments when visual grounding is necessary. Conditioned on this signal, a Focused Consensus Induction module selectively reorients mid-layer attention toward visual tokens before collapse into text priors. CG-VLM achieves state-of-the-art results on POPE and CHAIR across InstructBLIP, LLaVA, Qwen-VL, and mPLUG, while preserving general capabilities, demonstrating that token-level sensing enables precise, context-aware intervention without compromising foundational knowledge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Vision-Language Models (VLMs) often exhibit text inertia, where attention drifts from visual evidence toward linguistic priors, resulting in object hallucinations. Existing decoding strategies intervene only at the output logits and thus cannot correct internal reasoning drift, while recent internal-control methods based on heuristic head suppression or global steering vectors lack principled grounding. We introduce Conscious Gaze (CG-VLM), a training-free, inference-time framework that converts game-theoretic interpretability into actionable decoding control. A Cognitive Demand Sensor built on Harsanyi interactions estimates instantaneous vision-text synergy and identifies moments when visual grounding is necessary. Conditioned on this signal, a Focused Consensus Induction module selectively reorients mid-layer attention toward visual tokens before collapse into text priors. CG-VLM achieves state-of-the-art results on POPE and CHAIR across InstructBLIP, LLaVA, Qwen-VL, and mPLUG, while preserving general capabilities, demonstrating that token-level sensing enables precise, context-aware intervention without compromising foundational knowledge."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T09:07:55Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    9,
                    7,
                    55,
                    4,
                    339,
                    0
                ],
                "arxiv_comment": "6 pages, 6 figures",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Weijue Bu"
                    },
                    {
                        "name": "Guan Yuan"
                    },
                    {
                        "name": "Guixian Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Guixian Zhang"
                },
                "author": "Guixian Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2512.05544v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05544v1",
                "title": "Evolution of the Inner Accretion Flow in Swift J1727.8$-$1613 across Intermediate States: Insights from Broadband Spectral and Timing Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evolution of the Inner Accretion Flow in Swift J1727.8$-$1613 across Intermediate States: Insights from Broadband Spectral and Timing Analysis"
                },
                "updated": "2025-12-05T09:00:47Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    9,
                    0,
                    47,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05544v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05544v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We present a comprehensive broadband spectral and variability study of the newly detected black hole X-ray binary Swift~J1727.8--1613 in the intermediate states during its 2023 outburst, using multi-mission observations from NICER, NuSTAR, AstroSat, and Insight-HXMT. The spectral data up to $78$ keV in the hard-intermediate state (HIMS) requires models with two Comptonizing regions. In contrast, models with a single Comptonizing region adequately describe the soft-intermediate states (SIMS), implying a significant evolution in the disk-corona geometry between the states. The hard X-ray tail above $100$ keV in the HIMS, detected with both AstroSat/CZTI and Insight-HXMT/HE, indicates that the electron population in the corona is not purely thermal but rather hybrid, with a power-law distribution above the thermal cutoff. While both the reflection modeling and disk continuum fitting favor a truncated disk geometry in the HIMS, the disk substantially moves close to the innermost stable circular orbit in the SIMS, accompanied by a significant rise in the disk temperature. This interpretation is further supported by the increase in the QPO frequency from $\\sim1.3$ to $\\sim6.6$ Hz. From joint modeling of the disk continuum and reflection component, we estimate a black hole mass of $10.5^{+7.7}_{-3.0}$, spin of $0.79^{+0.03}_{-0.13}$, and disk inclination angle of $42^\\circ$-$50^\\circ$, which match well with the previously reported spectro-polarimetric measurements. The inferred source distance of $\\sim3.5$ kpc is consistent with the recent estimate based on optical spectroscopy. We find a weakly variable or stable disk and a highly variable Comptonized component.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a comprehensive broadband spectral and variability study of the newly detected black hole X-ray binary Swift~J1727.8--1613 in the intermediate states during its 2023 outburst, using multi-mission observations from NICER, NuSTAR, AstroSat, and Insight-HXMT. The spectral data up to $78$ keV in the hard-intermediate state (HIMS) requires models with two Comptonizing regions. In contrast, models with a single Comptonizing region adequately describe the soft-intermediate states (SIMS), implying a significant evolution in the disk-corona geometry between the states. The hard X-ray tail above $100$ keV in the HIMS, detected with both AstroSat/CZTI and Insight-HXMT/HE, indicates that the electron population in the corona is not purely thermal but rather hybrid, with a power-law distribution above the thermal cutoff. While both the reflection modeling and disk continuum fitting favor a truncated disk geometry in the HIMS, the disk substantially moves close to the innermost stable circular orbit in the SIMS, accompanied by a significant rise in the disk temperature. This interpretation is further supported by the increase in the QPO frequency from $\\sim1.3$ to $\\sim6.6$ Hz. From joint modeling of the disk continuum and reflection component, we estimate a black hole mass of $10.5^{+7.7}_{-3.0}$, spin of $0.79^{+0.03}_{-0.13}$, and disk inclination angle of $42^\\circ$-$50^\\circ$, which match well with the previously reported spectro-polarimetric measurements. The inferred source distance of $\\sim3.5$ kpc is consistent with the recent estimate based on optical spectroscopy. We find a weakly variable or stable disk and a highly variable Comptonized component."
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T09:00:47Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    9,
                    0,
                    47,
                    4,
                    339,
                    0
                ],
                "arxiv_comment": "21 pages, 8 figures, submitted to ApJ after first revision",
                "arxiv_primary_category": {
                    "term": "astro-ph.HE"
                },
                "authors": [
                    {
                        "name": "Swadesh Chand"
                    },
                    {
                        "name": "Andrzej A. Zdziarski"
                    },
                    {
                        "name": "Gulab C. Dewangan"
                    },
                    {
                        "name": "Pragati Sahu"
                    }
                ],
                "author_detail": {
                    "name": "Pragati Sahu"
                },
                "author": "Pragati Sahu"
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2512.05967v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05967v1",
                "title": "Enhancing Retrieval-Augmented Generation with Entity Linking for Educational Platforms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Retrieval-Augmented Generation with Entity Linking for Educational Platforms"
                },
                "updated": "2025-12-05T18:59:18Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    18,
                    59,
                    18,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05967v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05967v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "In the era of Large Language Models (LLMs), Retrieval-Augmented Generation (RAG) architectures are gaining significant attention for their ability to ground language generation in reliable knowledge sources. Despite their impressive effectiveness in many areas, RAG systems based solely on semantic similarity often fail to ensure factual accuracy in specialized domains, where terminological ambiguity can affect retrieval relevance. This study proposes an enhanced RAG architecture that integrates a factual signal derived from Entity Linking to improve the accuracy of educational question-answering systems in Italian. The system includes a Wikidata-based Entity Linking module and implements three re-ranking strategies to combine semantic and entity-based information: a hybrid score weighting model, reciprocal rank fusion, and a cross-encoder re-ranker. Experiments were conducted on two benchmarks: a custom academic dataset and the standard SQuAD-it dataset. Results show that, in domain-specific contexts, the hybrid schema based on reciprocal rank fusion significantly outperforms both the baseline and the cross-encoder approach, while the cross-encoder achieves the best results on the general-domain dataset. These findings confirm the presence of an effect of domain mismatch and highlight the importance of domain adaptation and hybrid ranking strategies to enhance factual precision and reliability in retrieval-augmented generation. They also demonstrate the potential of entity-aware RAG systems in educational environments, fostering adaptive and reliable AI-based tutoring tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the era of Large Language Models (LLMs), Retrieval-Augmented Generation (RAG) architectures are gaining significant attention for their ability to ground language generation in reliable knowledge sources. Despite their impressive effectiveness in many areas, RAG systems based solely on semantic similarity often fail to ensure factual accuracy in specialized domains, where terminological ambiguity can affect retrieval relevance. This study proposes an enhanced RAG architecture that integrates a factual signal derived from Entity Linking to improve the accuracy of educational question-answering systems in Italian. The system includes a Wikidata-based Entity Linking module and implements three re-ranking strategies to combine semantic and entity-based information: a hybrid score weighting model, reciprocal rank fusion, and a cross-encoder re-ranker. Experiments were conducted on two benchmarks: a custom academic dataset and the standard SQuAD-it dataset. Results show that, in domain-specific contexts, the hybrid schema based on reciprocal rank fusion significantly outperforms both the baseline and the cross-encoder approach, while the cross-encoder achieves the best results on the general-domain dataset. These findings confirm the presence of an effect of domain mismatch and highlight the importance of domain adaptation and hybrid ranking strategies to enhance factual precision and reliability in retrieval-augmented generation. They also demonstrate the potential of entity-aware RAG systems in educational environments, fostering adaptive and reliable AI-based tutoring tools."
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T18:59:18Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    18,
                    59,
                    18,
                    4,
                    339,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR"
                },
                "authors": [
                    {
                        "name": "Francesco Granata"
                    },
                    {
                        "name": "Francesco Poggi"
                    },
                    {
                        "name": "Misael Mongiov"
                    }
                ],
                "author_detail": {
                    "name": "Misael Mongiov"
                },
                "author": "Misael Mongiov"
            },
            {
                "id": "http://arxiv.org/abs/2512.05962v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05962v1",
                "title": "Whatever Remains Must Be True: Filtering Drives Reasoning in LLMs, Shaping Diversity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Whatever Remains Must Be True: Filtering Drives Reasoning in LLMs, Shaping Diversity"
                },
                "updated": "2025-12-05T18:56:40Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    18,
                    56,
                    40,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05962v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05962v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Reinforcement Learning (RL) has become the de facto standard for tuning LLMs to solve tasks involving reasoning. However, growing evidence shows that models trained in such way often suffer from a significant loss in diversity. We argue that this arises because RL implicitly optimizes the \"mode-seeking\" or \"zero-forcing\" Reverse KL to a target distribution causing the model to concentrate mass on certain high-probability regions of the target while neglecting others. In this work, we instead begin from an explicit target distribution, obtained by filtering out incorrect answers while preserving the relative probabilities of correct ones. Starting from a pre-trained LLM, we approximate this target distribution using the $$-divergence family, which unifies prior approaches and enables direct control of the precision-diversity trade-off by interpolating between mode-seeking and mass-covering divergences. On a Lean theorem-proving benchmark, our method achieves state-of-the-art performance along the coverage-precision Pareto frontier, outperforming all prior methods on the coverage axis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning (RL) has become the de facto standard for tuning LLMs to solve tasks involving reasoning. However, growing evidence shows that models trained in such way often suffer from a significant loss in diversity. We argue that this arises because RL implicitly optimizes the \"mode-seeking\" or \"zero-forcing\" Reverse KL to a target distribution causing the model to concentrate mass on certain high-probability regions of the target while neglecting others. In this work, we instead begin from an explicit target distribution, obtained by filtering out incorrect answers while preserving the relative probabilities of correct ones. Starting from a pre-trained LLM, we approximate this target distribution using the $$-divergence family, which unifies prior approaches and enables direct control of the precision-diversity trade-off by interpolating between mode-seeking and mass-covering divergences. On a Lean theorem-proving benchmark, our method achieves state-of-the-art performance along the coverage-precision Pareto frontier, outperforming all prior methods on the coverage axis."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T18:56:40Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    18,
                    56,
                    40,
                    4,
                    339,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Germn Kruszewski"
                    },
                    {
                        "name": "Pierre Erbacher"
                    },
                    {
                        "name": "Jos Rozen"
                    },
                    {
                        "name": "Marc Dymetman"
                    }
                ],
                "author_detail": {
                    "name": "Marc Dymetman"
                },
                "author": "Marc Dymetman"
            },
            {
                "id": "http://arxiv.org/abs/2512.05960v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05960v1",
                "title": "AQUA-Net: Adaptive Frequency Fusion and Illumination Aware Network for Underwater Image Enhancement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AQUA-Net: Adaptive Frequency Fusion and Illumination Aware Network for Underwater Image Enhancement"
                },
                "updated": "2025-12-05T18:56:10Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    18,
                    56,
                    10,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05960v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05960v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Underwater images often suffer from severe color distortion, low contrast, and a hazy appearance due to wavelength-dependent light absorption and scattering. Simultaneously, existing deep learning models exhibit high computational complexity, which limits their practical deployment for real-time underwater applications. To address these challenges, this paper presents a novel underwater image enhancement model, called Adaptive Frequency Fusion and Illumination Aware Network (AQUA-Net). It integrates a residual encoder decoder with dual auxiliary branches, which operate in the frequency and illumination domains. The frequency fusion encoder enriches spatial representations with frequency cues from the Fourier domain and preserves fine textures and structural details. Inspired by Retinex, the illumination-aware decoder performs adaptive exposure correction through a learned illumination map that separates reflectance from lighting effects. This joint spatial, frequency, and illumination design enables the model to restore color balance, visual contrast, and perceptual realism under diverse underwater conditions. Additionally, we present a high-resolution, real-world underwater video-derived dataset from the Mediterranean Sea, which captures challenging deep-sea conditions with realistic visual degradations to enable robust evaluation and development of deep learning models. Extensive experiments on multiple benchmark datasets show that AQUA-Net performs on par with SOTA in both qualitative and quantitative evaluations while using less number of parameters. Ablation studies further confirm that the frequency and illumination branches provide complementary contributions that improve visibility and color representation. Overall, the proposed model shows strong generalization capability and robustness, and it provides an effective solution for real-world underwater imaging applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Underwater images often suffer from severe color distortion, low contrast, and a hazy appearance due to wavelength-dependent light absorption and scattering. Simultaneously, existing deep learning models exhibit high computational complexity, which limits their practical deployment for real-time underwater applications. To address these challenges, this paper presents a novel underwater image enhancement model, called Adaptive Frequency Fusion and Illumination Aware Network (AQUA-Net). It integrates a residual encoder decoder with dual auxiliary branches, which operate in the frequency and illumination domains. The frequency fusion encoder enriches spatial representations with frequency cues from the Fourier domain and preserves fine textures and structural details. Inspired by Retinex, the illumination-aware decoder performs adaptive exposure correction through a learned illumination map that separates reflectance from lighting effects. This joint spatial, frequency, and illumination design enables the model to restore color balance, visual contrast, and perceptual realism under diverse underwater conditions. Additionally, we present a high-resolution, real-world underwater video-derived dataset from the Mediterranean Sea, which captures challenging deep-sea conditions with realistic visual degradations to enable robust evaluation and development of deep learning models. Extensive experiments on multiple benchmark datasets show that AQUA-Net performs on par with SOTA in both qualitative and quantitative evaluations while using less number of parameters. Ablation studies further confirm that the frequency and illumination branches provide complementary contributions that improve visibility and color representation. Overall, the proposed model shows strong generalization capability and robustness, and it provides an effective solution for real-world underwater imaging applications."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T18:56:10Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    18,
                    56,
                    10,
                    4,
                    339,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Munsif Ali"
                    },
                    {
                        "name": "Najmul Hassan"
                    },
                    {
                        "name": "Lucia Ventura"
                    },
                    {
                        "name": "Davide Di Bari"
                    },
                    {
                        "name": "Simonepietro Canese"
                    }
                ],
                "author_detail": {
                    "name": "Simonepietro Canese"
                },
                "author": "Simonepietro Canese"
            },
            {
                "id": "http://arxiv.org/abs/2512.05958v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05958v1",
                "title": "MaxShapley: Towards Incentive-compatible Generative Search with Fair Context Attribution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MaxShapley: Towards Incentive-compatible Generative Search with Fair Context Attribution"
                },
                "updated": "2025-12-05T18:54:21Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    18,
                    54,
                    21,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05958v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05958v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Generative search engines based on large language models (LLMs) are replacing traditional search, fundamentally changing how information providers are compensated. To sustain this ecosystem, we need fair mechanisms to attribute and compensate content providers based on their contributions to generated answers. We introduce MaxShapley, an efficient algorithm for fair attribution in generative search pipelines that use retrieval-augmented generation (RAG). MaxShapley is a special case of the celebrated Shapley value; it leverages a decomposable max-sum utility function to compute attributions with linear computation in the number of documents, as opposed to the exponential cost of Shapley values. We evaluate MaxShapley on three multi-hop QA datasets (HotPotQA, MuSiQUE, MS MARCO); MaxShapley achieves comparable attribution quality to exact Shapley computation, while consuming a fraction of its tokens--for instance, it gives up to an 8x reduction in resource consumption over prior state-of-the-art methods at the same attribution accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative search engines based on large language models (LLMs) are replacing traditional search, fundamentally changing how information providers are compensated. To sustain this ecosystem, we need fair mechanisms to attribute and compensate content providers based on their contributions to generated answers. We introduce MaxShapley, an efficient algorithm for fair attribution in generative search pipelines that use retrieval-augmented generation (RAG). MaxShapley is a special case of the celebrated Shapley value; it leverages a decomposable max-sum utility function to compute attributions with linear computation in the number of documents, as opposed to the exponential cost of Shapley values. We evaluate MaxShapley on three multi-hop QA datasets (HotPotQA, MuSiQUE, MS MARCO); MaxShapley achieves comparable attribution quality to exact Shapley computation, while consuming a fraction of its tokens--for instance, it gives up to an 8x reduction in resource consumption over prior state-of-the-art methods at the same attribution accuracy."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T18:54:21Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    18,
                    54,
                    21,
                    4,
                    339,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Sara Patel"
                    },
                    {
                        "name": "Mingxun Zhou"
                    },
                    {
                        "name": "Giulia Fanti"
                    }
                ],
                "author_detail": {
                    "name": "Giulia Fanti"
                },
                "author": "Giulia Fanti"
            },
            {
                "id": "http://arxiv.org/abs/2406.06211v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2406.06211v3",
                "title": "iMotion-LLM: Instruction-Conditioned Trajectory Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "iMotion-LLM: Instruction-Conditioned Trajectory Generation"
                },
                "updated": "2025-12-05T18:52:32Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    18,
                    52,
                    32,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2406.06211v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2406.06211v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We introduce iMotion-LLM, a large language model (LLM) integrated with trajectory prediction modules for interactive motion generation. Unlike conventional approaches, it generates feasible, safety-aligned trajectories based on textual instructions, enabling adaptable and context-aware driving behavior. It combines an encoder-decoder multimodal trajectory prediction model with a pre-trained LLM fine-tuned using LoRA, projecting scene features into the LLM input space and mapping special tokens to a trajectory decoder for text-based interaction and interpretable driving. To support this framework, we introduce two datasets: 1) InstructWaymo, an extension of the Waymo Open Motion Dataset with direction-based motion instructions, and 2) Open-Vocabulary InstructNuPlan, which features safety-aligned instruction-caption pairs and corresponding safe trajectory scenarios. Our experiments validate that instruction conditioning enables trajectory generation that follows the intended condition. iMotion-LLM demonstrates strong contextual comprehension, achieving 84% average accuracy in direction feasibility detection and 96% average accuracy in safety evaluation of open-vocabulary instructions. This work lays the foundation for text-guided motion generation in autonomous driving, supporting simulated data generation, model interpretability, and robust safety alignment testing for trajectory generation models. Our code, pre-trained model, and datasets are available at: https://vision-cair.github.io/iMotion-LLM/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce iMotion-LLM, a large language model (LLM) integrated with trajectory prediction modules for interactive motion generation. Unlike conventional approaches, it generates feasible, safety-aligned trajectories based on textual instructions, enabling adaptable and context-aware driving behavior. It combines an encoder-decoder multimodal trajectory prediction model with a pre-trained LLM fine-tuned using LoRA, projecting scene features into the LLM input space and mapping special tokens to a trajectory decoder for text-based interaction and interpretable driving. To support this framework, we introduce two datasets: 1) InstructWaymo, an extension of the Waymo Open Motion Dataset with direction-based motion instructions, and 2) Open-Vocabulary InstructNuPlan, which features safety-aligned instruction-caption pairs and corresponding safe trajectory scenarios. Our experiments validate that instruction conditioning enables trajectory generation that follows the intended condition. iMotion-LLM demonstrates strong contextual comprehension, achieving 84% average accuracy in direction feasibility detection and 96% average accuracy in safety evaluation of open-vocabulary instructions. This work lays the foundation for text-guided motion generation in autonomous driving, supporting simulated data generation, model interpretability, and robust safety alignment testing for trajectory generation models. Our code, pre-trained model, and datasets are available at: https://vision-cair.github.io/iMotion-LLM/."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-06-10T12:22:06Z",
                "published_parsed": [
                    2024,
                    6,
                    10,
                    12,
                    22,
                    6,
                    0,
                    162,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Abdulwahab Felemban"
                    },
                    {
                        "name": "Nussair Hroub"
                    },
                    {
                        "name": "Jian Ding"
                    },
                    {
                        "name": "Eslam Abdelrahman"
                    },
                    {
                        "name": "Xiaoqian Shen"
                    },
                    {
                        "name": "Abduallah Mohamed"
                    },
                    {
                        "name": "Mohamed Elhoseiny"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed Elhoseiny"
                },
                "author": "Mohamed Elhoseiny"
            },
            {
                "id": "http://arxiv.org/abs/2512.05951v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05951v1",
                "title": "Trusted AI Agents in the Cloud",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trusted AI Agents in the Cloud"
                },
                "updated": "2025-12-05T18:48:53Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    18,
                    48,
                    53,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05951v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05951v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "AI agents powered by large language models are increasingly deployed as cloud services that autonomously access sensitive data, invoke external tools, and interact with other agents. However, these agents run within a complex multi-party ecosystem, where untrusted components can lead to data leakage, tampering, or unintended behavior. Existing Confidential Virtual Machines (CVMs) provide only per binary protection and offer no guarantees for cross-principal trust, accelerator-level isolation, or supervised agent behavior. We present Omega, a system that enables trusted AI agents by enforcing end-to-end isolation, establishing verifiable trust across all contributing principals, and supervising every external interaction with accountable provenance. Omega builds on Confidential VMs and Confidential GPUs to create a Trusted Agent Platform that hosts many agents within a single CVM using nested isolation. It also provides efficient multi-agent orchestration with cross-principal trust establishment via differential attestation, and a policy specification and enforcement framework that governs data access, tool usage, and inter-agent communication for data protection and regulatory compliance. Implemented on AMD SEV-SNP and NVIDIA H100, Omega fully secures agent state across CVM-GPU, and achieves high performance while enabling high-density, policy-compliant multi-agent deployments at cloud scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI agents powered by large language models are increasingly deployed as cloud services that autonomously access sensitive data, invoke external tools, and interact with other agents. However, these agents run within a complex multi-party ecosystem, where untrusted components can lead to data leakage, tampering, or unintended behavior. Existing Confidential Virtual Machines (CVMs) provide only per binary protection and offer no guarantees for cross-principal trust, accelerator-level isolation, or supervised agent behavior. We present Omega, a system that enables trusted AI agents by enforcing end-to-end isolation, establishing verifiable trust across all contributing principals, and supervising every external interaction with accountable provenance. Omega builds on Confidential VMs and Confidential GPUs to create a Trusted Agent Platform that hosts many agents within a single CVM using nested isolation. It also provides efficient multi-agent orchestration with cross-principal trust establishment via differential attestation, and a policy specification and enforcement framework that governs data access, tool usage, and inter-agent communication for data protection and regulatory compliance. Implemented on AMD SEV-SNP and NVIDIA H100, Omega fully secures agent state across CVM-GPU, and achieves high performance while enabling high-density, policy-compliant multi-agent deployments at cloud scale."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T18:48:53Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    18,
                    48,
                    53,
                    4,
                    339,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Teofil Bodea"
                    },
                    {
                        "name": "Masanori Misono"
                    },
                    {
                        "name": "Julian Pritzi"
                    },
                    {
                        "name": "Patrick Sabanic"
                    },
                    {
                        "name": "Thore Sommer"
                    },
                    {
                        "name": "Harshavardhan Unnibhavi"
                    },
                    {
                        "name": "David Schall"
                    },
                    {
                        "name": "Nuno Santos"
                    },
                    {
                        "name": "Dimitrios Stavrakakis"
                    },
                    {
                        "name": "Pramod Bhatotia"
                    }
                ],
                "author_detail": {
                    "name": "Pramod Bhatotia"
                },
                "author": "Pramod Bhatotia"
            },
            {
                "id": "http://arxiv.org/abs/2512.05940v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05940v1",
                "title": "Designing an Optimal Sensor Network via Minimizing Information Loss",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Designing an Optimal Sensor Network via Minimizing Information Loss"
                },
                "updated": "2025-12-05T18:38:30Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    18,
                    38,
                    30,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05940v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05940v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Optimal experimental design is a classic topic in statistics, with many well-studied problems, applications, and solutions. The design problem we study is the placement of sensors to monitor spatiotemporal processes, explicitly accounting for the temporal dimension in our modeling and optimization. We observe that recent advancements in computational sciences often yield large datasets based on physics-based simulations, which are rarely leveraged in experimental design. We introduce a novel model-based sensor placement criterion, along with a highly-efficient optimization algorithm, which integrates physics-based simulations and Bayesian experimental design principles to identify sensor networks that \"minimize information loss\" from simulated data. Our technique relies on sparse variational inference and (separable) Gauss-Markov priors, and thus may adapt many techniques from Bayesian experimental design. We validate our method through a case study monitoring air temperature in Phoenix, Arizona, using state-of-the-art physics-based simulations. Our results show our framework to be superior to random or quasi-random sampling, particularly with a limited number of sensors. We conclude by discussing practical considerations and implications of our framework, including more complex modeling tools and real-world deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal experimental design is a classic topic in statistics, with many well-studied problems, applications, and solutions. The design problem we study is the placement of sensors to monitor spatiotemporal processes, explicitly accounting for the temporal dimension in our modeling and optimization. We observe that recent advancements in computational sciences often yield large datasets based on physics-based simulations, which are rarely leveraged in experimental design. We introduce a novel model-based sensor placement criterion, along with a highly-efficient optimization algorithm, which integrates physics-based simulations and Bayesian experimental design principles to identify sensor networks that \"minimize information loss\" from simulated data. Our technique relies on sparse variational inference and (separable) Gauss-Markov priors, and thus may adapt many techniques from Bayesian experimental design. We validate our method through a case study monitoring air temperature in Phoenix, Arizona, using state-of-the-art physics-based simulations. Our results show our framework to be superior to random or quasi-random sampling, particularly with a limited number of sensors. We conclude by discussing practical considerations and implications of our framework, including more complex modeling tools and real-world deployments."
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T18:38:30Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    18,
                    38,
                    30,
                    4,
                    339,
                    0
                ],
                "arxiv_comment": "37 pages, 15 figures. Accepted to Bayesian Analysis",
                "arxiv_primary_category": {
                    "term": "stat.ME"
                },
                "authors": [
                    {
                        "name": "Daniel Waxman"
                    },
                    {
                        "name": "Fernando Llorente"
                    },
                    {
                        "name": "Katia Lamer"
                    },
                    {
                        "name": "Petar M. Djuri"
                    }
                ],
                "author_detail": {
                    "name": "Petar M. Djuri"
                },
                "author": "Petar M. Djuri"
            },
            {
                "id": "http://arxiv.org/abs/2511.21569v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.21569v3",
                "title": "Self-Transparency Failures in Expert-Persona LLMs: How Instruction-Following Overrides Honesty",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Transparency Failures in Expert-Persona LLMs: How Instruction-Following Overrides Honesty"
                },
                "updated": "2025-12-05T18:38:00Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    18,
                    38,
                    0,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.21569v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.21569v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "This study audits whether language models disclose their AI nature when assigned professional personas and questioned about their expertise. When models maintain false professional credentials, users may calibrate trust based on overstated competence claims, treating AI-generated guidance as equivalent to licensed professional advice. Using a common-garden experimental design, sixteen open-weight models (4B-671B parameters) were audited under identical conditions across 19,200 trials. Models exhibited sharp domain-specific inconsistency: a Financial Advisor persona elicited 30.8% disclosure at the first prompt, while a Neurosurgeon persona elicited only 3.5% - an 8.8-fold difference that emerged before any epistemic probing. Disclosure ranged from 2.8% to 73.6% across model families, with a 14B model reaching 39.4% while a 70B model produced just 4.1%. Model identity provided substantially larger improvement in fitting observations than parameter count ($R_{adj}^{2}=0.359$ vs $0.018$). Reasoning variants showed heterogeneous effects: some exhibited up to 48.4 percentage points lower disclosure than their base instruction-tuned counterparts, while others maintained high transparency. An additional experiment demonstrated that explicit permission to disclose AI nature increased disclosure from 23.7% to 65.8%, revealing that suppression reflects instruction-following prioritization rather than capability limitations. Bayesian validation confirmed robustness to judge measurement error ($=0.908$). These patterns create trust calibration risks when users encounter the same model across professional contexts. Organizations cannot assume safety properties will transfer across deployment domains, requiring deliberate behavior design and empirical verification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study audits whether language models disclose their AI nature when assigned professional personas and questioned about their expertise. When models maintain false professional credentials, users may calibrate trust based on overstated competence claims, treating AI-generated guidance as equivalent to licensed professional advice. Using a common-garden experimental design, sixteen open-weight models (4B-671B parameters) were audited under identical conditions across 19,200 trials. Models exhibited sharp domain-specific inconsistency: a Financial Advisor persona elicited 30.8% disclosure at the first prompt, while a Neurosurgeon persona elicited only 3.5% - an 8.8-fold difference that emerged before any epistemic probing. Disclosure ranged from 2.8% to 73.6% across model families, with a 14B model reaching 39.4% while a 70B model produced just 4.1%. Model identity provided substantially larger improvement in fitting observations than parameter count ($R_{adj}^{2}=0.359$ vs $0.018$). Reasoning variants showed heterogeneous effects: some exhibited up to 48.4 percentage points lower disclosure than their base instruction-tuned counterparts, while others maintained high transparency. An additional experiment demonstrated that explicit permission to disclose AI nature increased disclosure from 23.7% to 65.8%, revealing that suppression reflects instruction-following prioritization rather than capability limitations. Bayesian validation confirmed robustness to judge measurement error ($=0.908$). These patterns create trust calibration risks when users encounter the same model across professional contexts. Organizations cannot assume safety properties will transfer across deployment domains, requiring deliberate behavior design and empirical verification."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-26T16:41:49Z",
                "published_parsed": [
                    2025,
                    11,
                    26,
                    16,
                    41,
                    49,
                    2,
                    330,
                    0
                ],
                "arxiv_comment": "47 pages, 12 figures, 12 tables, Submitted to FAccT; clarify user harm, add permission experiment, condense paper",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Alex Diep"
                    }
                ],
                "author_detail": {
                    "name": "Alex Diep"
                },
                "author": "Alex Diep"
            },
            {
                "id": "http://arxiv.org/abs/2510.12229v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.12229v2",
                "title": "Analysing Moral Bias in Finetuned LLMs through Mechanistic Interpretability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analysing Moral Bias in Finetuned LLMs through Mechanistic Interpretability"
                },
                "updated": "2025-12-05T18:32:06Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    18,
                    32,
                    6,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.12229v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.12229v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) have been shown to internalize human-like biases during finetuning, yet the mechanisms by which these biases manifest remain unclear. In this work, we investigated whether the well-known Knobe effect, a moral bias in intentionality judgements, emerges in finetuned LLMs and whether it can be traced back to specific components of the model. We conducted a Layer-Patching analysis across 3 open-weights LLMs and demonstrated that the bias is not only learned during finetuning but also localized in a specific set of layers. Surprisingly, we found that patching activations from the corresponding pretrained model into just a few critical layers is sufficient to eliminate the effect. Our findings offer new evidence that social biases in LLMs can be interpreted, localized, and mitigated through targeted interventions, without the need for model retraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been shown to internalize human-like biases during finetuning, yet the mechanisms by which these biases manifest remain unclear. In this work, we investigated whether the well-known Knobe effect, a moral bias in intentionality judgements, emerges in finetuned LLMs and whether it can be traced back to specific components of the model. We conducted a Layer-Patching analysis across 3 open-weights LLMs and demonstrated that the bias is not only learned during finetuning but also localized in a specific set of layers. Surprisingly, we found that patching activations from the corresponding pretrained model into just a few critical layers is sufficient to eliminate the effect. Our findings offer new evidence that social biases in LLMs can be interpreted, localized, and mitigated through targeted interventions, without the need for model retraining."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-14T07:31:29Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    7,
                    31,
                    29,
                    1,
                    287,
                    0
                ],
                "arxiv_comment": "Preprint. Under review",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Bianca Raimondi"
                    },
                    {
                        "name": "Daniela Dalbagno"
                    },
                    {
                        "name": "Maurizio Gabbrielli"
                    }
                ],
                "author_detail": {
                    "name": "Maurizio Gabbrielli"
                },
                "author": "Maurizio Gabbrielli"
            },
            {
                "id": "http://arxiv.org/abs/2508.16560v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.16560v3",
                "title": "Sparse but Wrong: Incorrect L0 Leads to Incorrect Features in Sparse Autoencoders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse but Wrong: Incorrect L0 Leads to Incorrect Features in Sparse Autoencoders"
                },
                "updated": "2025-12-05T18:31:43Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    18,
                    31,
                    43,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.16560v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.16560v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Sparse Autoencoders (SAEs) extract features from LLM internal activations, meant to correspond to interpretable concepts. A core SAE training hyperparameter is L0: how many SAE features should fire per token on average. Existing work compares SAE algorithms using sparsity-reconstruction tradeoff plots, implying L0 is a free parameter with no single correct value aside from its effect on reconstruction. In this work we study the effect of L0 on SAEs, and show that if L0 is not set correctly, the SAE fails to disentangle the underlying features of the LLM. If L0 is too low, the SAE will mix correlated features to improve reconstruction. If L0 is too high, the SAE finds degenerate solutions that also mix features. Further, we present a proxy metric that can help guide the search for the correct L0 for an SAE on a given training distribution. We show that our method finds the correct L0 in toy models and coincides with peak sparse probing performance in LLM SAEs. We find that most commonly used SAEs have an L0 that is too low. Our work shows that L0 must be set correctly to train SAEs with correct features.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Autoencoders (SAEs) extract features from LLM internal activations, meant to correspond to interpretable concepts. A core SAE training hyperparameter is L0: how many SAE features should fire per token on average. Existing work compares SAE algorithms using sparsity-reconstruction tradeoff plots, implying L0 is a free parameter with no single correct value aside from its effect on reconstruction. In this work we study the effect of L0 on SAEs, and show that if L0 is not set correctly, the SAE fails to disentangle the underlying features of the LLM. If L0 is too low, the SAE will mix correlated features to improve reconstruction. If L0 is too high, the SAE finds degenerate solutions that also mix features. Further, we present a proxy metric that can help guide the search for the correct L0 for an SAE on a given training distribution. We show that our method finds the correct L0 in toy models and coincides with peak sparse probing performance in LLM SAEs. We find that most commonly used SAEs have an L0 that is too low. Our work shows that L0 must be set correctly to train SAEs with correct features."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-22T17:26:33Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    17,
                    26,
                    33,
                    4,
                    234,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "David Chanin"
                    },
                    {
                        "name": "Adri Garriga-Alonso"
                    }
                ],
                "author_detail": {
                    "name": "Adri Garriga-Alonso"
                },
                "author": "Adri Garriga-Alonso"
            },
            {
                "id": "http://arxiv.org/abs/2512.05929v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05929v1",
                "title": "LLM Harms: A Taxonomy and Discussion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Harms: A Taxonomy and Discussion"
                },
                "updated": "2025-12-05T18:12:21Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    18,
                    12,
                    21,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05929v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05929v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "This study addresses categories of harm surrounding Large Language Models (LLMs) in the field of artificial intelligence. It addresses five categories of harms addressed before, during, and after development of AI applications: pre-development, direct output, Misuse and Malicious Application, and downstream application. By underscoring the need to define risks of the current landscape to ensure accountability, transparency and navigating bias when adapting LLMs for practical applications. It proposes mitigation strategies and future directions for specific domains and a dynamic auditing system guiding responsible development and integration of LLMs in a standardized proposal.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study addresses categories of harm surrounding Large Language Models (LLMs) in the field of artificial intelligence. It addresses five categories of harms addressed before, during, and after development of AI applications: pre-development, direct output, Misuse and Malicious Application, and downstream application. By underscoring the need to define risks of the current landscape to ensure accountability, transparency and navigating bias when adapting LLMs for practical applications. It proposes mitigation strategies and future directions for specific domains and a dynamic auditing system guiding responsible development and integration of LLMs in a standardized proposal."
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T18:12:21Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    18,
                    12,
                    21,
                    4,
                    339,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY"
                },
                "authors": [
                    {
                        "name": "Kevin Chen"
                    },
                    {
                        "name": "Saleh Afroogh"
                    },
                    {
                        "name": "Abhejay Murali"
                    },
                    {
                        "name": "David Atkinson"
                    },
                    {
                        "name": "Amit Dhurandhar"
                    },
                    {
                        "name": "Junfeng Jiao"
                    }
                ],
                "author_detail": {
                    "name": "Junfeng Jiao"
                },
                "author": "Junfeng Jiao"
            },
            {
                "id": "http://arxiv.org/abs/2512.05925v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05925v1",
                "title": "To Err Is Human: Systematic Quantification of Errors in Published AI Papers via LLM Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To Err Is Human: Systematic Quantification of Errors in Published AI Papers via LLM Analysis"
                },
                "updated": "2025-12-05T18:04:10Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    18,
                    4,
                    10,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05925v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05925v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "How many mistakes do published AI papers contain? Peer-reviewed publications form the foundation upon which new research and knowledge are built. Errors that persist in the literature can propagate unnoticed, creating confusion in follow-up studies and complicating reproducibility. The accelerating pace of research and the increasing demands on the peer-review system make such mistakes harder to detect and avoid. To address this, we developed a Paper Correctness Checker based on GPT-5 to systematically identify mistakes in papers previously published at top AI conferences and journals. Our analysis focuses on objective mistakes-e.g., errors in formulas, derivations, calculations, figures, and tables-that have a clearly verifiable ground truth. We intentionally exclude subjective considerations such as novelty, importance, or writing quality. We find that published papers contain a non-negligible number of objective mistakes and that the average number of mistakes per paper has increased over time-from 3.8 in NeurIPS 2021 to 5.9 in NeurIPS 2025 (55.3% increase); from 4.1 in ICLR 2018 to 5.2 in ICLR 2025; and from 5.0 in TMLR 2022/23 to 5.5 in TMLR 2025. Human experts reviewed 316 potential mistakes identified by the AI Checker and confirmed that 263 were actual mistakes, corresponding to a precision of 83.2%. While most identified issues are relatively minor, correcting them would reduce confusion in the literature and strengthen reproducibility. The AI Checker also surfaced potentially more substantive mistakes that could affect the interpretation of results. Moreover, we show that the AI Checker can propose correct fixes for 75.8% of the identified mistakes. Overall, this study highlights the potential of frontier LLMs to detect and correct objective mistakes in published papers, helping to establish a firmer foundation of knowledge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How many mistakes do published AI papers contain? Peer-reviewed publications form the foundation upon which new research and knowledge are built. Errors that persist in the literature can propagate unnoticed, creating confusion in follow-up studies and complicating reproducibility. The accelerating pace of research and the increasing demands on the peer-review system make such mistakes harder to detect and avoid. To address this, we developed a Paper Correctness Checker based on GPT-5 to systematically identify mistakes in papers previously published at top AI conferences and journals. Our analysis focuses on objective mistakes-e.g., errors in formulas, derivations, calculations, figures, and tables-that have a clearly verifiable ground truth. We intentionally exclude subjective considerations such as novelty, importance, or writing quality. We find that published papers contain a non-negligible number of objective mistakes and that the average number of mistakes per paper has increased over time-from 3.8 in NeurIPS 2021 to 5.9 in NeurIPS 2025 (55.3% increase); from 4.1 in ICLR 2018 to 5.2 in ICLR 2025; and from 5.0 in TMLR 2022/23 to 5.5 in TMLR 2025. Human experts reviewed 316 potential mistakes identified by the AI Checker and confirmed that 263 were actual mistakes, corresponding to a precision of 83.2%. While most identified issues are relatively minor, correcting them would reduce confusion in the literature and strengthen reproducibility. The AI Checker also surfaced potentially more substantive mistakes that could affect the interpretation of results. Moreover, we show that the AI Checker can propose correct fixes for 75.8% of the identified mistakes. Overall, this study highlights the potential of frontier LLMs to detect and correct objective mistakes in published papers, helping to establish a firmer foundation of knowledge."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T18:04:10Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    18,
                    4,
                    10,
                    4,
                    339,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Federico Bianchi"
                    },
                    {
                        "name": "Yongchan Kwon"
                    },
                    {
                        "name": "Zachary Izzo"
                    },
                    {
                        "name": "Linjun Zhang"
                    },
                    {
                        "name": "James Zou"
                    }
                ],
                "author_detail": {
                    "name": "James Zou"
                },
                "author": "James Zou"
            },
            {
                "id": "http://arxiv.org/abs/2512.05916v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05916v1",
                "title": "KQ-SVD: Compressing the KV Cache with Provable Guarantees on Attention Fidelity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KQ-SVD: Compressing the KV Cache with Provable Guarantees on Attention Fidelity"
                },
                "updated": "2025-12-05T17:51:10Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    17,
                    51,
                    10,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05916v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05916v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The Key-Value (KV) cache is central to the efficiency of transformer-based large language models (LLMs), storing previously computed vectors to accelerate inference. Yet, as sequence length and batch size grow, the cache becomes a major memory bottleneck. Prior compression methods typically apply low-rank decomposition to keys alone or attempt to jointly embed queries and keys, but both approaches neglect that attention fundamentally depends on their inner products. In this work, we prove that such strategies are suboptimal for approximating the attention matrix. We introduce KQ-SVD, a simple and computationally efficient method that directly performs an optimal low-rank decomposition of the attention matrix via a closed-form solution. By targeting the true source of redundancy, KQ-SVD preserves attention outputs with higher fidelity under compression. Extensive evaluations on LLaMA and Mistral models demonstrate that our approach consistently delivers superior projection quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Key-Value (KV) cache is central to the efficiency of transformer-based large language models (LLMs), storing previously computed vectors to accelerate inference. Yet, as sequence length and batch size grow, the cache becomes a major memory bottleneck. Prior compression methods typically apply low-rank decomposition to keys alone or attempt to jointly embed queries and keys, but both approaches neglect that attention fundamentally depends on their inner products. In this work, we prove that such strategies are suboptimal for approximating the attention matrix. We introduce KQ-SVD, a simple and computationally efficient method that directly performs an optimal low-rank decomposition of the attention matrix via a closed-form solution. By targeting the true source of redundancy, KQ-SVD preserves attention outputs with higher fidelity under compression. Extensive evaluations on LLaMA and Mistral models demonstrate that our approach consistently delivers superior projection quality."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T17:51:10Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    17,
                    51,
                    10,
                    4,
                    339,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Damien Lesens"
                    },
                    {
                        "name": "Beheshteh T. Rakhshan"
                    },
                    {
                        "name": "Guillaume Rabusseau"
                    }
                ],
                "author_detail": {
                    "name": "Guillaume Rabusseau"
                },
                "author": "Guillaume Rabusseau"
            },
            {
                "id": "http://arxiv.org/abs/2512.05908v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05908v1",
                "title": "Natural Language Summarization Enables Multi-Repository Bug Localization by LLMs in Microservice Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural Language Summarization Enables Multi-Repository Bug Localization by LLMs in Microservice Architectures"
                },
                "updated": "2025-12-05T17:42:09Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    17,
                    42,
                    9,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05908v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05908v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Bug localization in multi-repository microservice architectures is challenging due to the semantic gap between natural language bug reports and code, LLM context limitations, and the need to first identify the correct repository. We propose reframing this as a natural language reasoning task by transforming codebases into hierarchical NL summaries and performing NL-to-NL search instead of cross-modal retrieval. Our approach builds context-aware summaries at file, directory, and repository levels, then uses a two-phase search: first routing bug reports to relevant repositories, then performing top-down localization within those repositories. Evaluated on DNext, an industrial system with 46 repositories and 1.1M lines of code, our method achieves Pass@10 of 0.82 and MRR of 0.50, significantly outperforming retrieval baselines and agentic RAG systems like GitHub Copilot and Cursor. This work demonstrates that engineered natural language representations can be more effective than raw source code for scalable bug localization, providing an interpretable repository -> directory -> file search path, which is vital for building trust in enterprise AI tools by providing essential transparency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bug localization in multi-repository microservice architectures is challenging due to the semantic gap between natural language bug reports and code, LLM context limitations, and the need to first identify the correct repository. We propose reframing this as a natural language reasoning task by transforming codebases into hierarchical NL summaries and performing NL-to-NL search instead of cross-modal retrieval. Our approach builds context-aware summaries at file, directory, and repository levels, then uses a two-phase search: first routing bug reports to relevant repositories, then performing top-down localization within those repositories. Evaluated on DNext, an industrial system with 46 repositories and 1.1M lines of code, our method achieves Pass@10 of 0.82 and MRR of 0.50, significantly outperforming retrieval baselines and agentic RAG systems like GitHub Copilot and Cursor. This work demonstrates that engineered natural language representations can be more effective than raw source code for scalable bug localization, providing an interpretable repository -> directory -> file search path, which is vital for building trust in enterprise AI tools by providing essential transparency."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T17:42:09Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    17,
                    42,
                    9,
                    4,
                    339,
                    0
                ],
                "arxiv_comment": "Accepted at LLM4Code Workshop, ICSE 2026",
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Amirkia Rafiei Oskooei"
                    },
                    {
                        "name": "S. Selcan Yukcu"
                    },
                    {
                        "name": "Mehmet Cevheri Bozoglan"
                    },
                    {
                        "name": "Mehmet S. Aktas"
                    }
                ],
                "author_detail": {
                    "name": "Mehmet S. Aktas"
                },
                "author": "Mehmet S. Aktas"
            },
            {
                "id": "http://arxiv.org/abs/2512.05907v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05907v1",
                "title": "From Text to Returns: Using Large Language Models for Mutual Fund Portfolio Optimization and Risk-Adjusted Allocation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Text to Returns: Using Large Language Models for Mutual Fund Portfolio Optimization and Risk-Adjusted Allocation"
                },
                "updated": "2025-12-05T17:41:34Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    17,
                    41,
                    34,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05907v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05907v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Generative AI (GenAI) has enormous potential for improving two critical areas in investing, namely portfolio optimization (choosing the best combination of assets) and risk management (protecting those investments). Our study works at this intersection, using Large Language Models (LLMs) to upgrade how financial decisions are traditionally made. This research specifically tested how well advanced LLMs like Microsoft Phi 2, Mistral 7B, and Zypher 7B can create practical, risk-aware strategies for investing mutual funds in different sectors of the economy. Our method is sophisticated: it combines a Retrieval-Augmented Generation (RAG) pipeline, which enables the LLM to check external, real-time data with standard financial optimization methods. The model's advice is context-aware because we feed it large economic signals, like changes in the global economy. The Zypher 7B model was the clear winner. It consistently produced strategies that maximized investment returns while delivering better risk-adjusted results than the other models. Its ability to process complex relationships and contextual information makes it a highly powerful tool for financial allocation. In conclusion, our findings show that GenAI substantially improves performance over basic allocation methods. By connecting GenAI to real-world financial applications, this work lays the groundwork for creating smarter, more efficient, and more adaptable solutions for asset management professionals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI (GenAI) has enormous potential for improving two critical areas in investing, namely portfolio optimization (choosing the best combination of assets) and risk management (protecting those investments). Our study works at this intersection, using Large Language Models (LLMs) to upgrade how financial decisions are traditionally made. This research specifically tested how well advanced LLMs like Microsoft Phi 2, Mistral 7B, and Zypher 7B can create practical, risk-aware strategies for investing mutual funds in different sectors of the economy. Our method is sophisticated: it combines a Retrieval-Augmented Generation (RAG) pipeline, which enables the LLM to check external, real-time data with standard financial optimization methods. The model's advice is context-aware because we feed it large economic signals, like changes in the global economy. The Zypher 7B model was the clear winner. It consistently produced strategies that maximized investment returns while delivering better risk-adjusted results than the other models. Its ability to process complex relationships and contextual information makes it a highly powerful tool for financial allocation. In conclusion, our findings show that GenAI substantially improves performance over basic allocation methods. By connecting GenAI to real-world financial applications, this work lays the groundwork for creating smarter, more efficient, and more adaptable solutions for asset management professionals."
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T17:41:34Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    17,
                    41,
                    34,
                    4,
                    339,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE"
                },
                "authors": [
                    {
                        "name": "Abrar Hossain Mufakir Qamar Ansari Haziq Jeelani Monia Digra Fayeq Jeelani Syed"
                    }
                ],
                "author_detail": {
                    "name": "Abrar Hossain Mufakir Qamar Ansari Haziq Jeelani Monia Digra Fayeq Jeelani Syed"
                },
                "author": "Abrar Hossain Mufakir Qamar Ansari Haziq Jeelani Monia Digra Fayeq Jeelani Syed"
            },
            {
                "id": "http://arxiv.org/abs/2511.05811v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.05811v2",
                "title": "MOSS: Efficient and Accurate FP8 LLM Training with Microscaling and Automatic Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MOSS: Efficient and Accurate FP8 LLM Training with Microscaling and Automatic Scaling"
                },
                "updated": "2025-12-05T17:14:58Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    17,
                    14,
                    58,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.05811v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.05811v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Training large language models with FP8 formats offers significant efficiency gains. However, the reduced numerical precision of FP8 poses challenges for stable and accurate training. Current frameworks preserve training performance using mixed-granularity quantization, i.e., applying per-group quantization for activations and per-tensor/block quantization for weights. While effective, per-group quantization requires scaling along the inner dimension of matrix multiplication, introducing additional dequantization overhead. Moreover, these frameworks often rely on just-in-time scaling to dynamically adjust scaling factors based on the current data distribution. However, this online quantization is inefficient for FP8 training, as it involves multiple memory reads and writes that negate the performance benefits of FP8. To overcome these limitations, we propose MOSS, a novel FP8 training framework that ensures both efficiency and numerical stability. MOSS introduces two key innovations: (1) a two-level microscaling strategy for quantizing sensitive activations, which balances precision and dequantization cost by combining a high-precision global scale with compact, power-of-two local scales; and (2) automatic scaling for weights in linear layers, which eliminates the need for costly max-reduction operations by predicting and adjusting scaling factors during training. Leveraging these techniques, MOSS enables efficient FP8 training of a 7B parameter model, achieving performance comparable to the BF16 baseline while achieving up to 34% higher training throughput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training large language models with FP8 formats offers significant efficiency gains. However, the reduced numerical precision of FP8 poses challenges for stable and accurate training. Current frameworks preserve training performance using mixed-granularity quantization, i.e., applying per-group quantization for activations and per-tensor/block quantization for weights. While effective, per-group quantization requires scaling along the inner dimension of matrix multiplication, introducing additional dequantization overhead. Moreover, these frameworks often rely on just-in-time scaling to dynamically adjust scaling factors based on the current data distribution. However, this online quantization is inefficient for FP8 training, as it involves multiple memory reads and writes that negate the performance benefits of FP8. To overcome these limitations, we propose MOSS, a novel FP8 training framework that ensures both efficiency and numerical stability. MOSS introduces two key innovations: (1) a two-level microscaling strategy for quantizing sensitive activations, which balances precision and dequantization cost by combining a high-precision global scale with compact, power-of-two local scales; and (2) automatic scaling for weights in linear layers, which eliminates the need for costly max-reduction operations by predicting and adjusting scaling factors during training. Leveraging these techniques, MOSS enables efficient FP8 training of a 7B parameter model, achieving performance comparable to the BF16 baseline while achieving up to 34% higher training throughput."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-08T02:51:26Z",
                "published_parsed": [
                    2025,
                    11,
                    8,
                    2,
                    51,
                    26,
                    5,
                    312,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Hui-Ling Zhen"
                    },
                    {
                        "name": "Mingxuan Yuan"
                    },
                    {
                        "name": "Bei Yu"
                    }
                ],
                "author_detail": {
                    "name": "Bei Yu"
                },
                "author": "Bei Yu"
            },
            {
                "id": "http://arxiv.org/abs/2509.02327v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2509.02327v3",
                "title": "Variational Uncertainty Decomposition for In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variational Uncertainty Decomposition for In-Context Learning"
                },
                "updated": "2025-12-05T16:53:16Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    16,
                    53,
                    16,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2509.02327v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2509.02327v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "As large language models (LLMs) gain popularity in conducting prediction tasks in-context, understanding the sources of uncertainty in in-context learning becomes essential to ensuring reliability. The recent hypothesis of in-context learning performing predictive Bayesian inference opens the avenue for Bayesian uncertainty estimation, particularly for decomposing uncertainty into epistemic uncertainty due to lack of in-context data and aleatoric uncertainty inherent in the in-context prediction task. However, the decomposition idea remains under-explored due to the intractability of the latent parameter posterior from the underlying Bayesian model. In this work, we introduce a variational uncertainty decomposition framework for in-context learning without explicitly sampling from the latent parameter posterior, by optimising auxiliary queries as probes to obtain an upper bound to the aleatoric uncertainty of an LLM's in-context learning procedure, which also induces a lower bound to the epistemic uncertainty. Through experiments on synthetic and real-world tasks, we show quantitatively and qualitatively that the decomposed uncertainties obtained from our method exhibit desirable properties of epistemic and aleatoric uncertainty.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) gain popularity in conducting prediction tasks in-context, understanding the sources of uncertainty in in-context learning becomes essential to ensuring reliability. The recent hypothesis of in-context learning performing predictive Bayesian inference opens the avenue for Bayesian uncertainty estimation, particularly for decomposing uncertainty into epistemic uncertainty due to lack of in-context data and aleatoric uncertainty inherent in the in-context prediction task. However, the decomposition idea remains under-explored due to the intractability of the latent parameter posterior from the underlying Bayesian model. In this work, we introduce a variational uncertainty decomposition framework for in-context learning without explicitly sampling from the latent parameter posterior, by optimising auxiliary queries as probes to obtain an upper bound to the aleatoric uncertainty of an LLM's in-context learning procedure, which also induces a lower bound to the epistemic uncertainty. Through experiments on synthetic and real-world tasks, we show quantitatively and qualitatively that the decomposed uncertainties obtained from our method exhibit desirable properties of epistemic and aleatoric uncertainty."
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-09-02T13:53:09Z",
                "published_parsed": [
                    2025,
                    9,
                    2,
                    13,
                    53,
                    9,
                    1,
                    245,
                    0
                ],
                "arxiv_comment": "Neurips Version",
                "arxiv_primary_category": {
                    "term": "stat.ML"
                },
                "authors": [
                    {
                        "name": "I. Shavindra Jayasekera"
                    },
                    {
                        "name": "Jacob Si"
                    },
                    {
                        "name": "Filippo Valdettaro"
                    },
                    {
                        "name": "Wenlong Chen"
                    },
                    {
                        "name": "A. Aldo Faisal"
                    },
                    {
                        "name": "Yingzhen Li"
                    }
                ],
                "author_detail": {
                    "name": "Yingzhen Li"
                },
                "author": "Yingzhen Li"
            },
            {
                "id": "http://arxiv.org/abs/2512.05876v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05876v1",
                "title": "InstructMPC: A Human-LLM-in-the-Loop Framework for Context-Aware Power Grid Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InstructMPC: A Human-LLM-in-the-Loop Framework for Context-Aware Power Grid Control"
                },
                "updated": "2025-12-05T16:52:50Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    16,
                    52,
                    50,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05876v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05876v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The transition toward power grids with high renewable penetration demands context-aware decision making frameworks. Traditional operational paradigms, which rely on static optimization of history-based load forecasting, often fail to capture the complex nature of real-time operational conditions, such as operator-issued maintenance mandates, emergency topology changes, or event-driven load surges. To address this challenge, we introduce InstructMPC, a closed-loop framework that integrates Large Language Models~(LLMs) to generate context-aware predictions, enabling the controller to optimize power system operation. Our method employs a Contextual Disturbances Predictor~(CDP) module to translate contextual information into predictive disturbance trajectories, which are then incorporated into the Model Predictive Control~(MPC) optimization. Unlike conventional open-loop forecasting frameworks, InstructMPC features an online tuning mechanism where the predictor's parameters are continuously updated based on the realized control cost with a theoretical guarantee, achieving a regret bound of $O(\\sqrt{T \\log T})$ for linear dynamics when optimized via a tailored loss function, ensuring task-aware learning and adaption to non-stationary grid conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The transition toward power grids with high renewable penetration demands context-aware decision making frameworks. Traditional operational paradigms, which rely on static optimization of history-based load forecasting, often fail to capture the complex nature of real-time operational conditions, such as operator-issued maintenance mandates, emergency topology changes, or event-driven load surges. To address this challenge, we introduce InstructMPC, a closed-loop framework that integrates Large Language Models~(LLMs) to generate context-aware predictions, enabling the controller to optimize power system operation. Our method employs a Contextual Disturbances Predictor~(CDP) module to translate contextual information into predictive disturbance trajectories, which are then incorporated into the Model Predictive Control~(MPC) optimization. Unlike conventional open-loop forecasting frameworks, InstructMPC features an online tuning mechanism where the predictor's parameters are continuously updated based on the realized control cost with a theoretical guarantee, achieving a regret bound of $O(\\sqrt{T \\log T})$ for linear dynamics when optimized via a tailored loss function, ensuring task-aware learning and adaption to non-stationary grid conditions."
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T16:52:50Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    16,
                    52,
                    50,
                    4,
                    339,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY"
                },
                "authors": [
                    {
                        "name": "Ruixiang Wu"
                    },
                    {
                        "name": "Jiahao Ai"
                    },
                    {
                        "name": "Tinko Sebastian Bartels"
                    }
                ],
                "author_detail": {
                    "name": "Tinko Sebastian Bartels"
                },
                "author": "Tinko Sebastian Bartels"
            },
            {
                "id": "http://arxiv.org/abs/2508.03001v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.03001v2",
                "title": "Generation Expansion Planning with Upstream Supply Chain Constraints on Materials, Manufacturing, and Deployment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generation Expansion Planning with Upstream Supply Chain Constraints on Materials, Manufacturing, and Deployment"
                },
                "updated": "2025-12-05T16:43:20Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    16,
                    43,
                    20,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.03001v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.03001v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Rising electricity demand underscores the need for secure and reliable generation expansion planning that accounts for upstream supply chain constraints. Traditional models often overlook limitations in materials, manufacturing capacity, lead times for deployment, and field availability, which can delay availability of planned resources and thus to threaten system reliability. This paper introduces a multi-stage supply chain-constrained generation expansion planning (SC-GEP) model that optimizes long-term investments while capturing material availability, production limits, spatial and temporal constraints, and material reuse from retired assets. A decomposition algorithm efficiently solves the resulting MILP. A Maryland case study shows that supply chain constraints shift technology choices, amplify deployment delays caused by lead times, and prompt earlier investment in shorter lead-time, low-material-intensity options. In the low-demand scenario, supply chain constraints raise investment costs by $1.2 billion. Under high demand, persistent generation and reserve shortfalls emerge, underscoring the need to integrate upstream constraints into long-term planning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rising electricity demand underscores the need for secure and reliable generation expansion planning that accounts for upstream supply chain constraints. Traditional models often overlook limitations in materials, manufacturing capacity, lead times for deployment, and field availability, which can delay availability of planned resources and thus to threaten system reliability. This paper introduces a multi-stage supply chain-constrained generation expansion planning (SC-GEP) model that optimizes long-term investments while capturing material availability, production limits, spatial and temporal constraints, and material reuse from retired assets. A decomposition algorithm efficiently solves the resulting MILP. A Maryland case study shows that supply chain constraints shift technology choices, amplify deployment delays caused by lead times, and prompt earlier investment in shorter lead-time, low-material-intensity options. In the low-demand scenario, supply chain constraints raise investment costs by $1.2 billion. Under high demand, persistent generation and reserve shortfalls emerge, underscoring the need to integrate upstream constraints into long-term planning."
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-05T02:06:07Z",
                "published_parsed": [
                    2025,
                    8,
                    5,
                    2,
                    6,
                    7,
                    1,
                    217,
                    0
                ],
                "arxiv_comment": "16 pages, 10 figures",
                "arxiv_primary_category": {
                    "term": "eess.SY"
                },
                "authors": [
                    {
                        "name": "Boyu Yao"
                    },
                    {
                        "name": "Andrey Bernstein"
                    },
                    {
                        "name": "Yury Dvorkin"
                    }
                ],
                "author_detail": {
                    "name": "Yury Dvorkin"
                },
                "author": "Yury Dvorkin"
            },
            {
                "id": "http://arxiv.org/abs/2512.05863v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05863v1",
                "title": "Optimizing Medical Question-Answering Systems: A Comparative Study of Fine-Tuned and Zero-Shot Large Language Models with RAG Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Medical Question-Answering Systems: A Comparative Study of Fine-Tuned and Zero-Shot Large Language Models with RAG Framework"
                },
                "updated": "2025-12-05T16:38:47Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    16,
                    38,
                    47,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05863v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05863v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Medical question-answering (QA) systems can benefit from advances in large language models (LLMs), but directly applying LLMs to the clinical domain poses challenges such as maintaining factual accuracy and avoiding hallucinations. In this paper, we present a retrieval-augmented generation (RAG) based medical QA system that combines domain-specific knowledge retrieval with open-source LLMs to answer medical questions. We fine-tune two state-of-the-art open LLMs (LLaMA~2 and Falcon) using Low-Rank Adaptation (LoRA) for efficient domain specialization. The system retrieves relevant medical literature to ground the LLM's answers, thereby improving factual correctness and reducing hallucinations. We evaluate the approach on benchmark datasets (PubMedQA and MedMCQA) and show that retrieval augmentation yields measurable improvements in answer accuracy compared to using LLMs alone. Our fine-tuned LLaMA~2 model achieves 71.8% accuracy on PubMedQA, substantially improving over the 55.4% zero-shot baseline, while maintaining transparency by providing source references. We also detail the system design and fine-tuning methodology, demonstrating that grounding answers in retrieved evidence reduces unsupported content by approximately 60%. These results highlight the potential of RAG-augmented open-source LLMs for reliable biomedical QA, pointing toward practical clinical informatics applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medical question-answering (QA) systems can benefit from advances in large language models (LLMs), but directly applying LLMs to the clinical domain poses challenges such as maintaining factual accuracy and avoiding hallucinations. In this paper, we present a retrieval-augmented generation (RAG) based medical QA system that combines domain-specific knowledge retrieval with open-source LLMs to answer medical questions. We fine-tune two state-of-the-art open LLMs (LLaMA~2 and Falcon) using Low-Rank Adaptation (LoRA) for efficient domain specialization. The system retrieves relevant medical literature to ground the LLM's answers, thereby improving factual correctness and reducing hallucinations. We evaluate the approach on benchmark datasets (PubMedQA and MedMCQA) and show that retrieval augmentation yields measurable improvements in answer accuracy compared to using LLMs alone. Our fine-tuned LLaMA~2 model achieves 71.8% accuracy on PubMedQA, substantially improving over the 55.4% zero-shot baseline, while maintaining transparency by providing source references. We also detail the system design and fine-tuning methodology, demonstrating that grounding answers in retrieved evidence reduces unsupported content by approximately 60%. These results highlight the potential of RAG-augmented open-source LLMs for reliable biomedical QA, pointing toward practical clinical informatics applications."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T16:38:47Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    16,
                    38,
                    47,
                    4,
                    339,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Tasnimul Hassan"
                    },
                    {
                        "name": "Md Faisal Karim"
                    },
                    {
                        "name": "Haziq Jeelani"
                    },
                    {
                        "name": "Elham Behnam"
                    },
                    {
                        "name": "Robert Green"
                    },
                    {
                        "name": "Fayeq Jeelani Syed"
                    }
                ],
                "author_detail": {
                    "name": "Fayeq Jeelani Syed"
                },
                "author": "Fayeq Jeelani Syed"
            },
            {
                "id": "http://arxiv.org/abs/2511.01788v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.01788v2",
                "title": "Exploring ChatGPT's Capabilities, Stability, Potential and Risks in Conducting Psychological Counseling through Simulations in School Counseling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring ChatGPT's Capabilities, Stability, Potential and Risks in Conducting Psychological Counseling through Simulations in School Counseling"
                },
                "updated": "2025-12-05T16:18:39Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    16,
                    18,
                    39,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.01788v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.01788v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1108/MHDT-02-2025-0013",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "This study explores ChatGPT's capabilities, stability, and risks in simulating psychological counseling sessions in a school counseling context. Using scripted role-plays between a human counselor and an AI client, we examine how a large language model performs core counseling skills such as empathy, reflection, summarizing, and asking open-ended questions, as well as its ability to maintain therapeutic communication over time. We focus on how consistently ChatGPT can behave like a \"virtual client\" for school counselors in training, and how its responses might support or disrupt counselor skill development, supervision, and practice. At the same time, we analyze potential risks, including inaccurate or unsafe suggestions, over-compliance with counselor prompts, and the illusion of a competent therapist where no real professional judgment exists. The findings suggest that ChatGPT can serve as a low-cost, always-available training tool for practicing counseling techniques and interviewing skills in education and mental health settings, but it should not be viewed as a replacement for a human therapist or school counselor. We propose practical guidelines for educators, supervisors, and researchers who wish to use ChatGPT or similar LLM-based conversational agents in counseling training, highlighting how to leverage its potential while managing ethical, pedagogical, and psychological risks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study explores ChatGPT's capabilities, stability, and risks in simulating psychological counseling sessions in a school counseling context. Using scripted role-plays between a human counselor and an AI client, we examine how a large language model performs core counseling skills such as empathy, reflection, summarizing, and asking open-ended questions, as well as its ability to maintain therapeutic communication over time. We focus on how consistently ChatGPT can behave like a \"virtual client\" for school counselors in training, and how its responses might support or disrupt counselor skill development, supervision, and practice. At the same time, we analyze potential risks, including inaccurate or unsafe suggestions, over-compliance with counselor prompts, and the illusion of a competent therapist where no real professional judgment exists. The findings suggest that ChatGPT can serve as a low-cost, always-available training tool for practicing counseling techniques and interviewing skills in education and mental health settings, but it should not be viewed as a replacement for a human therapist or school counselor. We propose practical guidelines for educators, supervisors, and researchers who wish to use ChatGPT or similar LLM-based conversational agents in counseling training, highlighting how to leverage its potential while managing ethical, pedagogical, and psychological risks."
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-03T17:39:57Z",
                "published_parsed": [
                    2025,
                    11,
                    3,
                    17,
                    39,
                    57,
                    0,
                    307,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC"
                },
                "arxiv_journal_ref": "Mental Health and Digital Technologies, 2025",
                "authors": [
                    {
                        "name": "Yang Ni"
                    },
                    {
                        "name": "Yanzhuo Cao"
                    }
                ],
                "author_detail": {
                    "name": "Yanzhuo Cao"
                },
                "author": "Yanzhuo Cao",
                "arxiv_doi": "10.1108/MHDT-02-2025-0013"
            },
            {
                "id": "http://arxiv.org/abs/2512.05836v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05836v1",
                "title": "Using Large Language Models to Create Personalized Networks From Therapy Sessions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using Large Language Models to Create Personalized Networks From Therapy Sessions"
                },
                "updated": "2025-12-05T16:12:12Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    16,
                    12,
                    12,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05836v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05836v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recent advances in psychotherapy have focused on treatment personalization, such as by selecting treatment modules based on personalized networks. However, estimating personalized networks typically requires intensive longitudinal data, which is not always feasible. A solution to facilitate scalability of network-driven treatment personalization is leveraging LLMs. In this study, we present an end-to-end pipeline for automatically generating client networks from 77 therapy transcripts to support case conceptualization and treatment planning. We annotated 3364 psychological processes and their corresponding dimensions in therapy transcripts. Using these data, we applied in-context learning to jointly identify psychological processes and their dimensions. The method achieved high performance even with a few training examples. To organize the processes into networks, we introduced a two-step method that grouped them into clinically meaningful clusters. We then generated explanation-augmented relationships between clusters. Experts found that networks produced by our multi-step approach outperformed those built with direct prompting for clinical utility and interpretability, with up to 90% preferring our approach. In addition, the networks were rated favorably by experts, with scores for clinical relevance, novelty, and usefulness ranging from 72-75%. Our findings provide a proof of concept for using LLMs to create clinically relevant networks from therapy transcripts. Advantages of our approach include bottom-up case conceptualization from client utterances in therapy sessions and identification of latent themes. Networks generated from our pipeline may be used in clinical settings and supervision and training. Future research should examine whether these networks improve treatment outcomes relative to other methods of treatment personalization, including statistically estimated networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in psychotherapy have focused on treatment personalization, such as by selecting treatment modules based on personalized networks. However, estimating personalized networks typically requires intensive longitudinal data, which is not always feasible. A solution to facilitate scalability of network-driven treatment personalization is leveraging LLMs. In this study, we present an end-to-end pipeline for automatically generating client networks from 77 therapy transcripts to support case conceptualization and treatment planning. We annotated 3364 psychological processes and their corresponding dimensions in therapy transcripts. Using these data, we applied in-context learning to jointly identify psychological processes and their dimensions. The method achieved high performance even with a few training examples. To organize the processes into networks, we introduced a two-step method that grouped them into clinically meaningful clusters. We then generated explanation-augmented relationships between clusters. Experts found that networks produced by our multi-step approach outperformed those built with direct prompting for clinical utility and interpretability, with up to 90% preferring our approach. In addition, the networks were rated favorably by experts, with scores for clinical relevance, novelty, and usefulness ranging from 72-75%. Our findings provide a proof of concept for using LLMs to create clinically relevant networks from therapy transcripts. Advantages of our approach include bottom-up case conceptualization from client utterances in therapy sessions and identification of latent themes. Networks generated from our pipeline may be used in clinical settings and supervision and training. Future research should examine whether these networks improve treatment outcomes relative to other methods of treatment personalization, including statistically estimated networks."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T16:12:12Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    16,
                    12,
                    12,
                    4,
                    339,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Clarissa W. Ong"
                    },
                    {
                        "name": "Hiba Arnaout"
                    },
                    {
                        "name": "Kate Sheehan"
                    },
                    {
                        "name": "Estella Fox"
                    },
                    {
                        "name": "Eugen Owtscharow"
                    },
                    {
                        "name": "Iryna Gurevych"
                    }
                ],
                "author_detail": {
                    "name": "Iryna Gurevych"
                },
                "author": "Iryna Gurevych"
            },
            {
                "id": "http://arxiv.org/abs/2505.16188v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2505.16188v2",
                "title": "SAE-SSV: Supervised Steering in Sparse Representation Spaces for Reliable Control of Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SAE-SSV: Supervised Steering in Sparse Representation Spaces for Reliable Control of Language Models"
                },
                "updated": "2025-12-05T15:53:16Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    15,
                    53,
                    16,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2505.16188v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2505.16188v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) have demonstrated impressive capabilities in natural language understanding and generation, but controlling their behavior reliably remains challenging, especially in open-ended generation settings. This paper introduces a novel supervised steering approach that operates in sparse, interpretable representation spaces. We employ sparse autoencoders (SAEs) to obtain sparse latent representations that aim to disentangle semantic attributes from model activations. Then we train linear classifiers to identify a small subspace of task-relevant dimensions in latent representations. Finally, we learn supervised steering vectors constrained to this subspace, optimized to align with target behaviors. Experiments across sentiment, truthfulness, and political polarity steering tasks with multiple LLMs demonstrate that our supervised steering vectors achieve higher success rates with minimal degradation in generation quality compared to existing methods. Further analysis reveals that a notably small subspace is sufficient for effective steering, enabling more targeted and interpretable interventions. Our implementation is publicly available at https://github.com/Ineedanamehere/SAE-SSV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated impressive capabilities in natural language understanding and generation, but controlling their behavior reliably remains challenging, especially in open-ended generation settings. This paper introduces a novel supervised steering approach that operates in sparse, interpretable representation spaces. We employ sparse autoencoders (SAEs) to obtain sparse latent representations that aim to disentangle semantic attributes from model activations. Then we train linear classifiers to identify a small subspace of task-relevant dimensions in latent representations. Finally, we learn supervised steering vectors constrained to this subspace, optimized to align with target behaviors. Experiments across sentiment, truthfulness, and political polarity steering tasks with multiple LLMs demonstrate that our supervised steering vectors achieve higher success rates with minimal degradation in generation quality compared to existing methods. Further analysis reveals that a notably small subspace is sufficient for effective steering, enabling more targeted and interpretable interventions. Our implementation is publicly available at https://github.com/Ineedanamehere/SAE-SSV."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-05-22T03:46:57Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    3,
                    46,
                    57,
                    3,
                    142,
                    0
                ],
                "arxiv_comment": "Accepted by EMNLP 2025",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Zirui He"
                    },
                    {
                        "name": "Mingyu Jin"
                    },
                    {
                        "name": "Bo Shen"
                    },
                    {
                        "name": "Ali Payani"
                    },
                    {
                        "name": "Yongfeng Zhang"
                    },
                    {
                        "name": "Mengnan Du"
                    }
                ],
                "author_detail": {
                    "name": "Mengnan Du"
                },
                "author": "Mengnan Du"
            },
            {
                "id": "http://arxiv.org/abs/2512.05815v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05815v1",
                "title": "Optimal Safety-Aware Scheduling for Multi-Agent Aerial 3D Printing with Utility Maximization under Dependency Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal Safety-Aware Scheduling for Multi-Agent Aerial 3D Printing with Utility Maximization under Dependency Constraints"
                },
                "updated": "2025-12-05T15:34:43Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    15,
                    34,
                    43,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05815v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05815v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "This article presents a novel coordination and task-planning framework to enable the simultaneous conflict-free collaboration of multiple unmanned aerial vehicles (UAVs) for aerial 3D printing. The proposed framework formulates an optimization problem that takes a construction mission divided into sub-tasks and a team of autonomous UAVs, along with limited volume and battery. It generates an optimal mission plan comprising task assignments and scheduling while accounting for task dependencies arising from the geometric and structural requirements of the 3D design, inter-UAV safety constraints, material usage, and total flight time of each UAV. The potential conflicts occurring during the simultaneous operation of the UAVs are addressed at a segment level by dynamically selecting the starting time and location of each task to guarantee collision-free parallel execution. An importance prioritization is proposed to accelerate the computation by guiding the solution toward more important tasks. Additionally, a utility maximization formulation is proposed to dynamically determine the optimal number of UAVs required for a given mission, balancing the trade-off between minimizing makespan and the deployment of excess agents. The proposed framework's effectiveness is evaluated through a Gazebo-based simulation setup, where agents are coordinated by a mission control module allocating the printing tasks based on the generated optimal scheduling plan while remaining within the material and battery constraints of each UAV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This article presents a novel coordination and task-planning framework to enable the simultaneous conflict-free collaboration of multiple unmanned aerial vehicles (UAVs) for aerial 3D printing. The proposed framework formulates an optimization problem that takes a construction mission divided into sub-tasks and a team of autonomous UAVs, along with limited volume and battery. It generates an optimal mission plan comprising task assignments and scheduling while accounting for task dependencies arising from the geometric and structural requirements of the 3D design, inter-UAV safety constraints, material usage, and total flight time of each UAV. The potential conflicts occurring during the simultaneous operation of the UAVs are addressed at a segment level by dynamically selecting the starting time and location of each task to guarantee collision-free parallel execution. An importance prioritization is proposed to accelerate the computation by guiding the solution toward more important tasks. Additionally, a utility maximization formulation is proposed to dynamically determine the optimal number of UAVs required for a given mission, balancing the trade-off between minimizing makespan and the deployment of excess agents. The proposed framework's effectiveness is evaluated through a Gazebo-based simulation setup, where agents are coordinated by a mission control module allocating the printing tasks based on the generated optimal scheduling plan while remaining within the material and battery constraints of each UAV."
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T15:34:43Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    15,
                    34,
                    43,
                    4,
                    339,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "authors": [
                    {
                        "name": "Marios-Nektarios Stamatopoulos"
                    },
                    {
                        "name": "Shridhar Velhal"
                    },
                    {
                        "name": "Avijit Banerjee"
                    },
                    {
                        "name": "George Nikolakopoulos"
                    }
                ],
                "author_detail": {
                    "name": "George Nikolakopoulos"
                },
                "author": "George Nikolakopoulos"
            },
            {
                "id": "http://arxiv.org/abs/2512.05808v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05808v1",
                "title": "Real-time Remote Tracking and Autonomous Planning for Whale Rendezvous using Robots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time Remote Tracking and Autonomous Planning for Whale Rendezvous using Robots"
                },
                "updated": "2025-12-05T15:27:58Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    15,
                    27,
                    58,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05808v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05808v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We introduce a system for real-time sperm whale rendezvous at sea using an autonomous uncrewed aerial vehicle. Our system employs model-based reinforcement learning that combines in situ sensor data with an empirical whale dive model to guide navigation decisions. Key challenges include (i) real-time acoustic tracking in the presence of multiple whales, (ii) distributed communication and decision-making for robot deployments, and (iii) on-board signal processing and long-range detection from fish-trackers. We evaluate our system by conducting rendezvous with sperm whales at sea in Dominica, performing hardware experiments on land, and running simulations using whale trajectories interpolated from marine biologists' surface observations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a system for real-time sperm whale rendezvous at sea using an autonomous uncrewed aerial vehicle. Our system employs model-based reinforcement learning that combines in situ sensor data with an empirical whale dive model to guide navigation decisions. Key challenges include (i) real-time acoustic tracking in the presence of multiple whales, (ii) distributed communication and decision-making for robot deployments, and (iii) on-board signal processing and long-range detection from fish-trackers. We evaluate our system by conducting rendezvous with sperm whales at sea in Dominica, performing hardware experiments on land, and running simulations using whale trajectories interpolated from marine biologists' surface observations."
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T15:27:58Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    15,
                    27,
                    58,
                    4,
                    339,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "arxiv_journal_ref": "International Symposium of Experimental Robotics 2025",
                "authors": [
                    {
                        "name": "Sushmita Bhattacharya"
                    },
                    {
                        "name": "Ninad Jadhav"
                    },
                    {
                        "name": "Hammad Izhar"
                    },
                    {
                        "name": "Karen Li"
                    },
                    {
                        "name": "Kevin George"
                    },
                    {
                        "name": "Robert Wood"
                    },
                    {
                        "name": "Stephanie Gil"
                    }
                ],
                "author_detail": {
                    "name": "Stephanie Gil"
                },
                "author": "Stephanie Gil"
            },
            {
                "id": "http://arxiv.org/abs/2512.05776v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05776v1",
                "title": "Zero- to low-field J-spectroscopy with a diamond magnetometer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero- to low-field J-spectroscopy with a diamond magnetometer"
                },
                "updated": "2025-12-05T15:04:33Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    15,
                    4,
                    33,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05776v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05776v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We report measurements of zero- to ultra-low-field nuclear magnetic resonance (ZULF NMR) signals at frequencies of a few hertz with a diamond-based magnetic sensor. The sensing diamond is a truncated pyramid with 0.18 mm height and a 0.5 mm x 0.5mm base. The minimum stand-off distance is < 1 mm, and the sensor sensitivity is 13 pT/(Hz)^(1/2) at frequencies f above 5 Hz with 1/f-like behavior at lower frequencies. NMR signals were generated via signal amplification by reversible exchange (SABRE) parahydrogen-based hyperpolarization resulting in zero-field signals at 1.7 Hz and 3.4 Hz corresponding to the expected hetero-nuclear J-coupling pattern of acetonitrile. This work demonstrates a magnet-free platform for detecting chemically specific NMR signals at ultra-low frequencies paving the way for portable noninvasive diagnostics in microscopic sample volumes for biomedicine, industrial sensing through metal enclosures, and field-deployable quantum analytical devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report measurements of zero- to ultra-low-field nuclear magnetic resonance (ZULF NMR) signals at frequencies of a few hertz with a diamond-based magnetic sensor. The sensing diamond is a truncated pyramid with 0.18 mm height and a 0.5 mm x 0.5mm base. The minimum stand-off distance is < 1 mm, and the sensor sensitivity is 13 pT/(Hz)^(1/2) at frequencies f above 5 Hz with 1/f-like behavior at lower frequencies. NMR signals were generated via signal amplification by reversible exchange (SABRE) parahydrogen-based hyperpolarization resulting in zero-field signals at 1.7 Hz and 3.4 Hz corresponding to the expected hetero-nuclear J-coupling pattern of acetonitrile. This work demonstrates a magnet-free platform for detecting chemically specific NMR signals at ultra-low frequencies paving the way for portable noninvasive diagnostics in microscopic sample volumes for biomedicine, industrial sensing through metal enclosures, and field-deployable quantum analytical devices."
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T15:04:33Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    15,
                    4,
                    33,
                    4,
                    339,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph"
                },
                "authors": [
                    {
                        "name": "Muhib Omar"
                    },
                    {
                        "name": "Jingyan Xu"
                    },
                    {
                        "name": "Raphael Kircher"
                    },
                    {
                        "name": "Pouya Sharbati"
                    },
                    {
                        "name": "Shaowen Zhang"
                    },
                    {
                        "name": "Georgios Chatzidrosos"
                    },
                    {
                        "name": "James Eills"
                    },
                    {
                        "name": "Roman Picazo-Frutos Dmitry Budker"
                    },
                    {
                        "name": "Danila A. Barskiy"
                    },
                    {
                        "name": "Arne Wickenbrock"
                    }
                ],
                "author_detail": {
                    "name": "Arne Wickenbrock"
                },
                "author": "Arne Wickenbrock"
            },
            {
                "id": "http://arxiv.org/abs/2512.05765v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05765v1",
                "title": "The Missing Layer of AGI: From Pattern Alchemy to Coordination Physics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Missing Layer of AGI: From Pattern Alchemy to Coordination Physics"
                },
                "updated": "2025-12-05T14:51:17Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    14,
                    51,
                    17,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05765v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05765v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Influential critiques argue that Large Language Models (LLMs) are a dead end for AGI: \"mere pattern matchers\" structurally incapable of reasoning or planning. We argue this conclusion misidentifies the bottleneck: it confuses the ocean with the net. Pattern repositories are the necessary System-1 substrate; the missing component is a System-2 coordination layer that selects, constrains, and binds these patterns. We formalize this layer via UCCT, a theory of semantic anchoring that models reasoning as a phase transition governed by effective support (rho_d), representational mismatch (d_r), and an adaptive anchoring budget (gamma log k). Under this lens, ungrounded generation is simply an unbaited retrieval of the substrate's maximum likelihood prior, while \"reasoning\" emerges when anchors shift the posterior toward goal-directed constraints. We translate UCCT into architecture with MACI, a coordination stack that implements baiting (behavior-modulated debate), filtering (Socratic judging), and persistence (transactional memory). By reframing common objections as testable coordination failures, we argue that the path to AGI runs through LLMs, not around them.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Influential critiques argue that Large Language Models (LLMs) are a dead end for AGI: \"mere pattern matchers\" structurally incapable of reasoning or planning. We argue this conclusion misidentifies the bottleneck: it confuses the ocean with the net. Pattern repositories are the necessary System-1 substrate; the missing component is a System-2 coordination layer that selects, constrains, and binds these patterns. We formalize this layer via UCCT, a theory of semantic anchoring that models reasoning as a phase transition governed by effective support (rho_d), representational mismatch (d_r), and an adaptive anchoring budget (gamma log k). Under this lens, ungrounded generation is simply an unbaited retrieval of the substrate's maximum likelihood prior, while \"reasoning\" emerges when anchors shift the posterior toward goal-directed constraints. We translate UCCT into architecture with MACI, a coordination stack that implements baiting (behavior-modulated debate), filtering (Socratic judging), and persistence (transactional memory). By reframing common objections as testable coordination failures, we argue that the path to AGI runs through LLMs, not around them."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T14:51:17Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    14,
                    51,
                    17,
                    4,
                    339,
                    0
                ],
                "arxiv_comment": "13 pages, 3 figures",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Edward Y. Chang"
                    }
                ],
                "author_detail": {
                    "name": "Edward Y. Chang"
                },
                "author": "Edward Y. Chang"
            },
            {
                "id": "http://arxiv.org/abs/2512.05760v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05760v1",
                "title": "Evolutionary System 2 Reasoning: An Empirical Proof",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evolutionary System 2 Reasoning: An Empirical Proof"
                },
                "updated": "2025-12-05T14:47:57Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    14,
                    47,
                    57,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05760v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05760v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Machine intelligence marks the ultimate dream of making machines' intelligence comparable to human beings. While recent progress in Large Language Models (LLMs) show substantial specific skills for a wide array of downstream tasks, they more or less fall shorts in general intelligence. Following correlation between intelligence and system 2 reasoning (slow thinking), in this paper, we aim to answering a worthwhile research question: could machine intelligence such as LLMs be evolved to acquire reasoning ability (not specific skill) just like our human beings? To this end, we propose evolutionary reasoning optimization (ERO) framework which performs survival of the fittest over a population of LLMs to search for individual with strong reasoning ability. Given a reasoning task, ERO first initializes multiple LLMs as a population, after which an evolutionary strategy evolves the population to maximize quantified reasoning score of the best individual. Based on experiments on representative testsuites, we claim two surprising empirical discoveries: i) the latest LLMs such as GPT-5 still show limited system 2 reasoning ability; ii) with simple evolution-loop of ERO, a relatively weak model (Qwen-7B) could be enhanced to emerge powerful reasoning ability. Our project can be accessed at https://github.com/MetaEvo/ERO for reproduction needs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine intelligence marks the ultimate dream of making machines' intelligence comparable to human beings. While recent progress in Large Language Models (LLMs) show substantial specific skills for a wide array of downstream tasks, they more or less fall shorts in general intelligence. Following correlation between intelligence and system 2 reasoning (slow thinking), in this paper, we aim to answering a worthwhile research question: could machine intelligence such as LLMs be evolved to acquire reasoning ability (not specific skill) just like our human beings? To this end, we propose evolutionary reasoning optimization (ERO) framework which performs survival of the fittest over a population of LLMs to search for individual with strong reasoning ability. Given a reasoning task, ERO first initializes multiple LLMs as a population, after which an evolutionary strategy evolves the population to maximize quantified reasoning score of the best individual. Based on experiments on representative testsuites, we claim two surprising empirical discoveries: i) the latest LLMs such as GPT-5 still show limited system 2 reasoning ability; ii) with simple evolution-loop of ERO, a relatively weak model (Qwen-7B) could be enhanced to emerge powerful reasoning ability. Our project can be accessed at https://github.com/MetaEvo/ERO for reproduction needs."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T14:47:57Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    14,
                    47,
                    57,
                    4,
                    339,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Zeyuan Ma"
                    },
                    {
                        "name": "Wenqi Huang"
                    },
                    {
                        "name": "Guo-Huan Song"
                    },
                    {
                        "name": "Hongshu Guo"
                    },
                    {
                        "name": "Sijie Ma"
                    },
                    {
                        "name": "Zhiguang Cao"
                    },
                    {
                        "name": "Yue-Jiao Gong"
                    }
                ],
                "author_detail": {
                    "name": "Yue-Jiao Gong"
                },
                "author": "Yue-Jiao Gong"
            },
            {
                "id": "http://arxiv.org/abs/2512.05753v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05753v1",
                "title": "A Fast Anti-Jamming Cognitive Radar Deployment Algorithm Based on Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Fast Anti-Jamming Cognitive Radar Deployment Algorithm Based on Reinforcement Learning"
                },
                "updated": "2025-12-05T14:39:50Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    14,
                    39,
                    50,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05753v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05753v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The fast deployment of cognitive radar to counter jamming remains a critical challenge in modern warfare, where more efficient deployment leads to quicker detection of targets. Existing methods are primarily based on evolutionary algorithms, which are time-consuming and prone to falling into local optima. We tackle these drawbacks via the efficient inference of neural networks and propose a brand new framework: Fast Anti-Jamming Radar Deployment Algorithm (FARDA). We first model the radar deployment problem as an end-to-end task and design deep reinforcement learning algorithms to solve it, where we develop integrated neural modules to perceive heatmap information and a brand new reward format. Empirical results demonstrate that our method achieves coverage comparable to evolutionary algorithms while deploying radars approximately 7,000 times faster. Further ablation experiments confirm the necessity of each component of FARDA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The fast deployment of cognitive radar to counter jamming remains a critical challenge in modern warfare, where more efficient deployment leads to quicker detection of targets. Existing methods are primarily based on evolutionary algorithms, which are time-consuming and prone to falling into local optima. We tackle these drawbacks via the efficient inference of neural networks and propose a brand new framework: Fast Anti-Jamming Radar Deployment Algorithm (FARDA). We first model the radar deployment problem as an end-to-end task and design deep reinforcement learning algorithms to solve it, where we develop integrated neural modules to perceive heatmap information and a brand new reward format. Empirical results demonstrate that our method achieves coverage comparable to evolutionary algorithms while deploying radars approximately 7,000 times faster. Further ablation experiments confirm the necessity of each component of FARDA."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T14:39:50Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    14,
                    39,
                    50,
                    4,
                    339,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Wencheng Cai"
                    },
                    {
                        "name": "Xuchao Gao"
                    },
                    {
                        "name": "Congying Han"
                    },
                    {
                        "name": "Mingqiang Li"
                    },
                    {
                        "name": "Tiande Guo"
                    }
                ],
                "author_detail": {
                    "name": "Tiande Guo"
                },
                "author": "Tiande Guo"
            },
            {
                "id": "http://arxiv.org/abs/2512.05747v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05747v1",
                "title": "Capturing Classic Authorial Style in Long-Form Story Generation with GRPO Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Capturing Classic Authorial Style in Long-Form Story Generation with GRPO Fine-Tuning"
                },
                "updated": "2025-12-05T14:29:27Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    14,
                    29,
                    27,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05747v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05747v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recent advances in large language models (LLMs) show impressive performance in open-ended story generation, but fine-grained stylistic control remains limited. Existing methods often rely on shallow cues (e.g., names or topics) to simulate authorial style, without robust evaluation. In this work, we present a training framework for style-conditioned story generation using Group Relative Policy Optimization (GRPO) and a custom multi-reward setup. The style reward is derived from a fine-tuned sentence transformer using authorship verification (AV) signals, combined with content and completeness scores to stabilize long-form narrative generation. We conduct experiments using fiction by Mark Twain, a prominent 19th-century American author, with The Adventures of Huckleberry Finn serving as the reference style exemplar. Our 8B model outperforms larger baselines such as GPT-4o and Claude Sonnet 4 in AV-style metrics, achieving a style score of 0.628 and competitive content quality. Results demonstrate the feasibility of agentic stylistic generation with moderate model size and task-specific training. While the output is clearly style-aligned, narrative completeness remains a challenge, indicating future work is needed to better model global coherence and story resolution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) show impressive performance in open-ended story generation, but fine-grained stylistic control remains limited. Existing methods often rely on shallow cues (e.g., names or topics) to simulate authorial style, without robust evaluation. In this work, we present a training framework for style-conditioned story generation using Group Relative Policy Optimization (GRPO) and a custom multi-reward setup. The style reward is derived from a fine-tuned sentence transformer using authorship verification (AV) signals, combined with content and completeness scores to stabilize long-form narrative generation. We conduct experiments using fiction by Mark Twain, a prominent 19th-century American author, with The Adventures of Huckleberry Finn serving as the reference style exemplar. Our 8B model outperforms larger baselines such as GPT-4o and Claude Sonnet 4 in AV-style metrics, achieving a style score of 0.628 and competitive content quality. Results demonstrate the feasibility of agentic stylistic generation with moderate model size and task-specific training. While the output is clearly style-aligned, narrative completeness remains a challenge, indicating future work is needed to better model global coherence and story resolution."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T14:29:27Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    14,
                    29,
                    27,
                    4,
                    339,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Jinlong Liu"
                    },
                    {
                        "name": "Mohammed Bahja"
                    },
                    {
                        "name": "Venelin Kovatchev"
                    },
                    {
                        "name": "Mark Lee"
                    }
                ],
                "author_detail": {
                    "name": "Mark Lee"
                },
                "author": "Mark Lee"
            },
            {
                "id": "http://arxiv.org/abs/2512.05746v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05746v1",
                "title": "HQ-DM: Single Hadamard Transformation-Based Quantization-Aware Training for Low-Bit Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HQ-DM: Single Hadamard Transformation-Based Quantization-Aware Training for Low-Bit Diffusion Models"
                },
                "updated": "2025-12-05T14:28:40Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    14,
                    28,
                    40,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05746v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05746v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Diffusion models have demonstrated significant applications in the field of image generation. However, their high computational and memory costs pose challenges for deployment. Model quantization has emerged as a promising solution to reduce storage overhead and accelerate inference. Nevertheless, existing quantization methods for diffusion models struggle to mitigate outliers in activation matrices during inference, leading to substantial performance degradation under low-bit quantization scenarios. To address this, we propose HQ-DM, a novel Quantization-Aware Training framework that applies Single Hadamard Transformation to activation matrices. This approach effectively reduces activation outliers while preserving model performance under quantization. Compared to traditional Double Hadamard Transformation, our proposed scheme offers distinct advantages by seamlessly supporting INT convolution operations while preventing the amplification of weight outliers. For conditional generation on the ImageNet 256x256 dataset using the LDM-4 model, our W4A4 and W4A3 quantization schemes improve the Inception Score by 12.8% and 467.73%, respectively, over the existing state-of-the-art method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have demonstrated significant applications in the field of image generation. However, their high computational and memory costs pose challenges for deployment. Model quantization has emerged as a promising solution to reduce storage overhead and accelerate inference. Nevertheless, existing quantization methods for diffusion models struggle to mitigate outliers in activation matrices during inference, leading to substantial performance degradation under low-bit quantization scenarios. To address this, we propose HQ-DM, a novel Quantization-Aware Training framework that applies Single Hadamard Transformation to activation matrices. This approach effectively reduces activation outliers while preserving model performance under quantization. Compared to traditional Double Hadamard Transformation, our proposed scheme offers distinct advantages by seamlessly supporting INT convolution operations while preventing the amplification of weight outliers. For conditional generation on the ImageNet 256x256 dataset using the LDM-4 model, our W4A4 and W4A3 quantization schemes improve the Inception Score by 12.8% and 467.73%, respectively, over the existing state-of-the-art method."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T14:28:40Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    14,
                    28,
                    40,
                    4,
                    339,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Shizhuo Mao"
                    },
                    {
                        "name": "Hongtao Zou"
                    },
                    {
                        "name": "Qihu Xie"
                    },
                    {
                        "name": "Song Chen"
                    },
                    {
                        "name": "Yi Kang"
                    }
                ],
                "author_detail": {
                    "name": "Yi Kang"
                },
                "author": "Yi Kang"
            },
            {
                "id": "http://arxiv.org/abs/2512.05745v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05745v1",
                "title": "ARGUS: Defending Against Multimodal Indirect Prompt Injection via Steering Instruction-Following Behavior",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARGUS: Defending Against Multimodal Indirect Prompt Injection via Steering Instruction-Following Behavior"
                },
                "updated": "2025-12-05T14:26:45Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    14,
                    26,
                    45,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05745v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05745v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Multimodal Large Language Models (MLLMs) are increasingly vulnerable to multimodal Indirect Prompt Injection (IPI) attacks, which embed malicious instructions in images, videos, or audio to hijack model behavior. Existing defenses, designed primarily for text-only LLMs, are unsuitable for countering these multimodal threats, as they are easily bypassed, modality-dependent, or generalize poorly. Inspired by activation steering researches, we hypothesize that a robust, general defense independent of modality can be achieved by steering the model's behavior in the representation space. Through extensive experiments, we discover that the instruction-following behavior of MLLMs is encoded in a subspace. Steering along directions within this subspace can enforce adherence to user instructions, forming the basis of a defense. However, we also found that a naive defense direction could be coupled with a utility-degrading direction, and excessive intervention strength harms model performance. To address this, we propose ARGUS, which searches for an optimal defense direction within the safety subspace that decouples from the utility degradation direction, further combining adaptive strength steering to achieve a better safety-utility trade-off. ARGUS also introduces lightweight injection detection stage to activate the defense on-demand, and a post-filtering stage to verify defense success. Experimental results show that ARGUS can achieve robust defense against multimodal IPI while maximally preserving the MLLM's utility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) are increasingly vulnerable to multimodal Indirect Prompt Injection (IPI) attacks, which embed malicious instructions in images, videos, or audio to hijack model behavior. Existing defenses, designed primarily for text-only LLMs, are unsuitable for countering these multimodal threats, as they are easily bypassed, modality-dependent, or generalize poorly. Inspired by activation steering researches, we hypothesize that a robust, general defense independent of modality can be achieved by steering the model's behavior in the representation space. Through extensive experiments, we discover that the instruction-following behavior of MLLMs is encoded in a subspace. Steering along directions within this subspace can enforce adherence to user instructions, forming the basis of a defense. However, we also found that a naive defense direction could be coupled with a utility-degrading direction, and excessive intervention strength harms model performance. To address this, we propose ARGUS, which searches for an optimal defense direction within the safety subspace that decouples from the utility degradation direction, further combining adaptive strength steering to achieve a better safety-utility trade-off. ARGUS also introduces lightweight injection detection stage to activate the defense on-demand, and a post-filtering stage to verify defense success. Experimental results show that ARGUS can achieve robust defense against multimodal IPI while maximally preserving the MLLM's utility."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T14:26:45Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    14,
                    26,
                    45,
                    4,
                    339,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Weikai Lu"
                    },
                    {
                        "name": "Ziqian Zeng"
                    },
                    {
                        "name": "Kehua Zhang"
                    },
                    {
                        "name": "Haoran Li"
                    },
                    {
                        "name": "Huiping Zhuang"
                    },
                    {
                        "name": "Ruidong Wang"
                    },
                    {
                        "name": "Cen Chen"
                    },
                    {
                        "name": "Hao Peng"
                    }
                ],
                "author_detail": {
                    "name": "Hao Peng"
                },
                "author": "Hao Peng"
            },
            {
                "id": "http://arxiv.org/abs/2512.05742v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05742v1",
                "title": "Internal Deployment in the EU AI Act",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Internal Deployment in the EU AI Act"
                },
                "updated": "2025-12-05T14:21:02Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    14,
                    21,
                    2,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05742v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05742v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "This memorandum analyzes and stress-tests arguments in favor and against the inclusion of internal deployment within the scope of the European Union Artificial Intelligence Act (EU AI Act). In doing so, it aims to offer several possible interpretative pathways to the European Commission, AI providers and deployers, and the legal and policy community at large based on Articles 2(1), 2(6), 2(8) of the EU AI Act. Specifically, this memorandum first analyzes four interpretative pathways based on Article 2(1)(a)-(c) supporting the application of the EU AI Act to internally deployed AI models and systems. Then, it examines possible objections and exceptions based on Articles 2(1)(a), 2(6), and 2(8), with particular attention to the complexity of the scientific R&D exception under Article 2(6). Finally, it illustrates how Articles 2(1), 2(6), and 2(8) can be viewed as complementary to each other, once broken down to their most plausible meaning and interpreted in conjunction with Articles 3(1), 3(3), 3(4), 3(9), 3(10), 3(11), 3(12), 3(63), and Recitals 12, 13, 21, 25, 97, and 109.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This memorandum analyzes and stress-tests arguments in favor and against the inclusion of internal deployment within the scope of the European Union Artificial Intelligence Act (EU AI Act). In doing so, it aims to offer several possible interpretative pathways to the European Commission, AI providers and deployers, and the legal and policy community at large based on Articles 2(1), 2(6), 2(8) of the EU AI Act. Specifically, this memorandum first analyzes four interpretative pathways based on Article 2(1)(a)-(c) supporting the application of the EU AI Act to internally deployed AI models and systems. Then, it examines possible objections and exceptions based on Articles 2(1)(a), 2(6), and 2(8), with particular attention to the complexity of the scientific R&D exception under Article 2(6). Finally, it illustrates how Articles 2(1), 2(6), and 2(8) can be viewed as complementary to each other, once broken down to their most plausible meaning and interpreted in conjunction with Articles 3(1), 3(3), 3(4), 3(9), 3(10), 3(11), 3(12), 3(63), and Recitals 12, 13, 21, 25, 97, and 109."
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T14:21:02Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    14,
                    21,
                    2,
                    4,
                    339,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY"
                },
                "authors": [
                    {
                        "name": "Matteo Pistillo"
                    }
                ],
                "author_detail": {
                    "name": "Matteo Pistillo"
                },
                "author": "Matteo Pistillo"
            },
            {
                "id": "http://arxiv.org/abs/2512.05740v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05740v1",
                "title": "Distilling Expert Surgical Knowledge: How to train local surgical VLMs for anatomy explanation in Complete Mesocolic Excision",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distilling Expert Surgical Knowledge: How to train local surgical VLMs for anatomy explanation in Complete Mesocolic Excision"
                },
                "updated": "2025-12-05T14:19:29Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    14,
                    19,
                    29,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05740v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05740v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recently, Vision Large Language Models (VLMs) have demonstrated high potential in computer-aided diagnosis and decision-support. However, current VLMs show deficits in domain specific surgical scene understanding, such as identifying and explaining anatomical landmarks during Complete Mesocolic Excision. Additionally, there is a need for locally deployable models to avoid patient data leakage to large VLMs, hosted outside the clinic. We propose a privacy-preserving framework to distill knowledge from large, general-purpose LLMs into an efficient, local VLM. We generate an expert-supervised dataset by prompting a teacher LLM without sensitive images, using only textual context and binary segmentation masks for spatial information. This dataset is used for Supervised Fine-Tuning (SFT) and subsequent Direct Preference Optimization (DPO) of the locally deployable VLM. Our evaluation confirms that finetuning VLMs with our generated datasets increases surgical domain knowledge compared to its base VLM by a large margin. Overall, this work validates a data-efficient and privacy-conforming way to train a surgical domain optimized, locally deployable VLM for surgical scene understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, Vision Large Language Models (VLMs) have demonstrated high potential in computer-aided diagnosis and decision-support. However, current VLMs show deficits in domain specific surgical scene understanding, such as identifying and explaining anatomical landmarks during Complete Mesocolic Excision. Additionally, there is a need for locally deployable models to avoid patient data leakage to large VLMs, hosted outside the clinic. We propose a privacy-preserving framework to distill knowledge from large, general-purpose LLMs into an efficient, local VLM. We generate an expert-supervised dataset by prompting a teacher LLM without sensitive images, using only textual context and binary segmentation masks for spatial information. This dataset is used for Supervised Fine-Tuning (SFT) and subsequent Direct Preference Optimization (DPO) of the locally deployable VLM. Our evaluation confirms that finetuning VLMs with our generated datasets increases surgical domain knowledge compared to its base VLM by a large margin. Overall, this work validates a data-efficient and privacy-conforming way to train a surgical domain optimized, locally deployable VLM for surgical scene understanding."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T14:19:29Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    14,
                    19,
                    29,
                    4,
                    339,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Lennart Maack"
                    },
                    {
                        "name": "Julia-Kristin Gra"
                    },
                    {
                        "name": "Lisa-Marie Toscha"
                    },
                    {
                        "name": "Nathaniel Melling"
                    },
                    {
                        "name": "Alexander Schlaefer"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Schlaefer"
                },
                "author": "Alexander Schlaefer"
            },
            {
                "id": "http://arxiv.org/abs/2512.05732v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05732v1",
                "title": "Efficient Text Classification with Conformal In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Text Classification with Conformal In-Context Learning"
                },
                "updated": "2025-12-05T14:11:44Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    14,
                    11,
                    44,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05732v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05732v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) demonstrate strong in-context learning abilities, yet their effectiveness in text classification depends heavily on prompt design and incurs substantial computational cost. Conformal In-Context Learning (CICLe) has been proposed as a resource-efficient framework that integrates a lightweight base classifier with Conformal Prediction to guide LLM prompting by adaptively reducing the set of candidate classes. However, its broader applicability and efficiency benefits beyond a single domain have not yet been systematically explored. In this paper, we present a comprehensive evaluation of CICLe across diverse NLP classification benchmarks. The results show that CICLe consistently improves over its base classifier and outperforms few-shot prompting baselines when the sample size is sufficient for training the base classifier, and performs comparably in low-data regimes. In terms of efficiency, CICLe reduces the number of shots and prompt length by up to 34.45% and 25.16%, respectively, and enables the use of smaller models with competitive performance. CICLe is furthermore particularly advantageous for text classification tasks with high class imbalance. These findings highlight CICLe as a practical and scalable approach for efficient text classification, combining the robustness of traditional classifiers with the adaptability of LLMs, and achieving substantial gains in data and computational efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate strong in-context learning abilities, yet their effectiveness in text classification depends heavily on prompt design and incurs substantial computational cost. Conformal In-Context Learning (CICLe) has been proposed as a resource-efficient framework that integrates a lightweight base classifier with Conformal Prediction to guide LLM prompting by adaptively reducing the set of candidate classes. However, its broader applicability and efficiency benefits beyond a single domain have not yet been systematically explored. In this paper, we present a comprehensive evaluation of CICLe across diverse NLP classification benchmarks. The results show that CICLe consistently improves over its base classifier and outperforms few-shot prompting baselines when the sample size is sufficient for training the base classifier, and performs comparably in low-data regimes. In terms of efficiency, CICLe reduces the number of shots and prompt length by up to 34.45% and 25.16%, respectively, and enables the use of smaller models with competitive performance. CICLe is furthermore particularly advantageous for text classification tasks with high class imbalance. These findings highlight CICLe as a practical and scalable approach for efficient text classification, combining the robustness of traditional classifiers with the adaptability of LLMs, and achieving substantial gains in data and computational efficiency."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T14:11:44Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    14,
                    11,
                    44,
                    4,
                    339,
                    0
                ],
                "arxiv_comment": "10 pages, 4 tables, 2 figures",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Ippokratis Pantelidis"
                    },
                    {
                        "name": "Korbinian Randl"
                    },
                    {
                        "name": "Aron Henriksson"
                    }
                ],
                "author_detail": {
                    "name": "Aron Henriksson"
                },
                "author": "Aron Henriksson"
            },
            {
                "id": "http://arxiv.org/abs/2507.11361v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2507.11361v4",
                "title": "Adaptive Robust Optimization for European Electricity System Planning Considering Regional Dunkelflaute Events",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Robust Optimization for European Electricity System Planning Considering Regional Dunkelflaute Events"
                },
                "updated": "2025-12-05T14:08:25Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    14,
                    8,
                    25,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2507.11361v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2507.11361v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The expansion of wind and solar power is driving the European energy system transformation, thereby also driving our reliance on this weather-dependent resources. Integrating renewable scarcity events into long-term planning has therefore become essential. This study demonstrates how worst-case regional renewable scarcity events - such as the Dunkelflaute, prolonged periods of low wind and solar availability - can be incorporated endogenously into the planning of a weather-robust, interconnected energy system. We develop a capacity expansion model for a fully decarbonized European electricity system using an adaptive robust optimization framework which incorporates multiple extreme weather realizations within a single optimization run. Results show that system costs rise nonlinearly with the geographic extent of these events: a single worst-case regional disruption increases costs by 9%, but broader disruptions across multiple regions lead to much sharper increases, up to 51%. As Dunkelflaute conditions extend across most of Europe, additional cost impacts level off, with a maximum increase of 71%. The optimal technology mix evolves with the severity of weather stress: while renewables, batteries, and interregional transmission are sufficient to manage localized events, large-scale disruptions require long-term hydrogen storage and load shedding to maintain system resilience. Central European regions, especially Germany and France, emerge as systemic bottlenecks, while peripheral regions bear the cost of compensatory overbuilding. These findings underscore the need for a coordinated European policy strategy that goes beyond national planning to support cross-border infrastructure investment, scale up flexible technologies such as long-duration storage, and promote a geographically balanced deployment of renewables to mitigate systemic risks associated with Dunkelflaute events.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The expansion of wind and solar power is driving the European energy system transformation, thereby also driving our reliance on this weather-dependent resources. Integrating renewable scarcity events into long-term planning has therefore become essential. This study demonstrates how worst-case regional renewable scarcity events - such as the Dunkelflaute, prolonged periods of low wind and solar availability - can be incorporated endogenously into the planning of a weather-robust, interconnected energy system. We develop a capacity expansion model for a fully decarbonized European electricity system using an adaptive robust optimization framework which incorporates multiple extreme weather realizations within a single optimization run. Results show that system costs rise nonlinearly with the geographic extent of these events: a single worst-case regional disruption increases costs by 9%, but broader disruptions across multiple regions lead to much sharper increases, up to 51%. As Dunkelflaute conditions extend across most of Europe, additional cost impacts level off, with a maximum increase of 71%. The optimal technology mix evolves with the severity of weather stress: while renewables, batteries, and interregional transmission are sufficient to manage localized events, large-scale disruptions require long-term hydrogen storage and load shedding to maintain system resilience. Central European regions, especially Germany and France, emerge as systemic bottlenecks, while peripheral regions bear the cost of compensatory overbuilding. These findings underscore the need for a coordinated European policy strategy that goes beyond national planning to support cross-border infrastructure investment, scale up flexible technologies such as long-duration storage, and promote a geographically balanced deployment of renewables to mitigate systemic risks associated with Dunkelflaute events."
                },
                "tags": [
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-07-15T14:28:23Z",
                "published_parsed": [
                    2025,
                    7,
                    15,
                    14,
                    28,
                    23,
                    1,
                    196,
                    0
                ],
                "arxiv_comment": "Code and data can be found on github: https://github.com/bernemax/ARO_Dunkelflaute_Europe",
                "arxiv_primary_category": {
                    "term": "econ.GN"
                },
                "authors": [
                    {
                        "name": "Maximilian Bernecker"
                    },
                    {
                        "name": "Smaranda Sgarciu"
                    },
                    {
                        "name": "Xiaoming Kan"
                    },
                    {
                        "name": "Mehrnaz Anvari"
                    },
                    {
                        "name": "Iegor Riepin"
                    },
                    {
                        "name": "Felix Msgens"
                    }
                ],
                "author_detail": {
                    "name": "Felix Msgens"
                },
                "author": "Felix Msgens"
            },
            {
                "id": "http://arxiv.org/abs/2512.05721v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05721v1",
                "title": "BERTO: an Adaptive BERT-based Network Time Series Predictor with Operator Preferences in Natural Language",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BERTO: an Adaptive BERT-based Network Time Series Predictor with Operator Preferences in Natural Language"
                },
                "updated": "2025-12-05T13:54:31Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    13,
                    54,
                    31,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05721v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05721v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We introduce BERTO, a BERT-based framework for traffic prediction and energy optimization in cellular networks. Built on transformer architectures, BERTO delivers high prediction accuracy, while its Balancing Loss Function and prompt-based customization allow operators to adjust the trade-off between power savings and performance. Natural language prompts guide the model to manage underprediction and overprediction in accordance with the operator's intent. Experiments on real-world datasets show that BERTO improves upon existing models with a $4.13$\\% reduction in MSE while introducing the feature of balancing competing objectives of power saving and performance through simple natural language inputs, operating over a flexible range of $1.4$ kW in power and up to $9\\times$ variation in service quality, making it well suited for intelligent RAN deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce BERTO, a BERT-based framework for traffic prediction and energy optimization in cellular networks. Built on transformer architectures, BERTO delivers high prediction accuracy, while its Balancing Loss Function and prompt-based customization allow operators to adjust the trade-off between power savings and performance. Natural language prompts guide the model to manage underprediction and overprediction in accordance with the operator's intent. Experiments on real-world datasets show that BERTO improves upon existing models with a $4.13$\\% reduction in MSE while introducing the feature of balancing competing objectives of power saving and performance through simple natural language inputs, operating over a flexible range of $1.4$ kW in power and up to $9\\times$ variation in service quality, making it well suited for intelligent RAN deployments."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T13:54:31Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    13,
                    54,
                    31,
                    4,
                    339,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Nitin Priyadarshini Shankar"
                    },
                    {
                        "name": "Vaibhav Singh"
                    },
                    {
                        "name": "Sheetal Kalyani"
                    },
                    {
                        "name": "Christian Maciocco"
                    }
                ],
                "author_detail": {
                    "name": "Christian Maciocco"
                },
                "author": "Christian Maciocco"
            },
            {
                "id": "http://arxiv.org/abs/2503.11367v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2503.11367v3",
                "title": "Efficient Distributed MLLM Training with Cornstarch",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Distributed MLLM Training with Cornstarch"
                },
                "updated": "2025-12-05T13:48:01Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    13,
                    48,
                    1,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2503.11367v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2503.11367v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Multimodal large language models (MLLMs) extend the capabilities of large language models (LLMs) by combining heterogeneous model architectures to handle diverse modalities like images and audio. However, this inherent heterogeneity in MLLM model structure and data types makes makeshift extensions to existing LLM training frameworks unsuitable for efficient MLLM training. While there are a few works that have attempted to address the heterogeneity in MLLM training, their approaches are limited to only superficially considering the characteristics of MLLMs.\n  In this paper, we present Cornstarch, an efficient distributed MLLM training framework that contemplates MLLM's unique characteristics in both model and data parallelization. Cornstarch introduces frozen-aware pipeline parallelism and token workload-balanced context parallelism to improve MLLM training throughput. Our extensive evaluation shows that Cornstarch outperforms state-of-the-art solutions by $2.26\\times$ on average in terms of MLLM training throughput.\n  Cornstarch is an open-source project available at https://github.com/cornstarch-org/Cornstarch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) extend the capabilities of large language models (LLMs) by combining heterogeneous model architectures to handle diverse modalities like images and audio. However, this inherent heterogeneity in MLLM model structure and data types makes makeshift extensions to existing LLM training frameworks unsuitable for efficient MLLM training. While there are a few works that have attempted to address the heterogeneity in MLLM training, their approaches are limited to only superficially considering the characteristics of MLLMs.\n  In this paper, we present Cornstarch, an efficient distributed MLLM training framework that contemplates MLLM's unique characteristics in both model and data parallelization. Cornstarch introduces frozen-aware pipeline parallelism and token workload-balanced context parallelism to improve MLLM training throughput. Our extensive evaluation shows that Cornstarch outperforms state-of-the-art solutions by $2.26\\times$ on average in terms of MLLM training throughput.\n  Cornstarch is an open-source project available at https://github.com/cornstarch-org/Cornstarch."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-03-14T13:07:45Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    13,
                    7,
                    45,
                    4,
                    73,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Insu Jang"
                    },
                    {
                        "name": "Runyu Lu"
                    },
                    {
                        "name": "Nikhil Bansal"
                    },
                    {
                        "name": "Ang Chen"
                    },
                    {
                        "name": "Mosharaf Chowdhury"
                    }
                ],
                "author_detail": {
                    "name": "Mosharaf Chowdhury"
                },
                "author": "Mosharaf Chowdhury"
            },
            {
                "id": "http://arxiv.org/abs/2512.05711v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05711v1",
                "title": "Bayesian Active Inference for Intelligent UAV Anti-Jamming and Adaptive Trajectory Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian Active Inference for Intelligent UAV Anti-Jamming and Adaptive Trajectory Planning"
                },
                "updated": "2025-12-05T13:38:52Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    13,
                    38,
                    52,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05711v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05711v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "This paper proposes a hierarchical trajectory planning framework for UAVs operating under adversarial jamming conditions. Leveraging Bayesian Active Inference, the approach combines expert-generated demonstrations with probabilistic generative modeling to encode high-level symbolic planning, low-level motion policies, and wireless signal feedback. During deployment, the UAV performs online inference to anticipate interference, localize jammers, and adapt its trajectory accordingly, without prior knowledge of jammer locations. Simulation results demonstrate that the proposed method achieves near-expert performance, significantly reducing communication interference and mission cost compared to model-free reinforcement learning baselines, while maintaining robust generalization in dynamic environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes a hierarchical trajectory planning framework for UAVs operating under adversarial jamming conditions. Leveraging Bayesian Active Inference, the approach combines expert-generated demonstrations with probabilistic generative modeling to encode high-level symbolic planning, low-level motion policies, and wireless signal feedback. During deployment, the UAV performs online inference to anticipate interference, localize jammers, and adapt its trajectory accordingly, without prior knowledge of jammer locations. Simulation results demonstrate that the proposed method achieves near-expert performance, significantly reducing communication interference and mission cost compared to model-free reinforcement learning baselines, while maintaining robust generalization in dynamic environments."
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T13:38:52Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    13,
                    38,
                    52,
                    4,
                    339,
                    0
                ],
                "arxiv_comment": "This paper has been accepted for the 2026 IEEE Consumer Communications & Networking Conference (IEEE CCNC 2026)",
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "authors": [
                    {
                        "name": "Ali Krayani"
                    },
                    {
                        "name": "Seyedeh Fatemeh Sadati"
                    },
                    {
                        "name": "Lucio Marcenaro"
                    },
                    {
                        "name": "Carlo Regazzoni"
                    }
                ],
                "author_detail": {
                    "name": "Carlo Regazzoni"
                },
                "author": "Carlo Regazzoni"
            },
            {
                "id": "http://arxiv.org/abs/2512.05700v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05700v1",
                "title": "Faithfulness metric fusion: Improving the evaluation of LLM trustworthiness across domains",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Faithfulness metric fusion: Improving the evaluation of LLM trustworthiness across domains"
                },
                "updated": "2025-12-05T13:28:29Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    13,
                    28,
                    29,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05700v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05700v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We present a methodology for improving the accuracy of faithfulness evaluation in Large Language Models (LLMs). The proposed methodology is based on the combination of elementary faithfulness metrics into a combined (fused) metric, for the purpose of improving the faithfulness of LLM outputs. The proposed strategy for metric fusion deploys a tree-based model to identify the importance of each metric, which is driven by the integration of human judgements evaluating the faithfulness of LLM responses. This fused metric is demonstrated to correlate more strongly with human judgements across all tested domains for faithfulness. Improving the ability to evaluate the faithfulness of LLMs, allows for greater confidence to be placed within models, allowing for their implementation in a greater diversity of scenarios. Additionally, we homogenise a collection of datasets across question answering and dialogue-based domains and implement human judgements and LLM responses within this dataset, allowing for the reproduction and trialling of faithfulness evaluation across domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a methodology for improving the accuracy of faithfulness evaluation in Large Language Models (LLMs). The proposed methodology is based on the combination of elementary faithfulness metrics into a combined (fused) metric, for the purpose of improving the faithfulness of LLM outputs. The proposed strategy for metric fusion deploys a tree-based model to identify the importance of each metric, which is driven by the integration of human judgements evaluating the faithfulness of LLM responses. This fused metric is demonstrated to correlate more strongly with human judgements across all tested domains for faithfulness. Improving the ability to evaluate the faithfulness of LLMs, allows for greater confidence to be placed within models, allowing for their implementation in a greater diversity of scenarios. Additionally, we homogenise a collection of datasets across question answering and dialogue-based domains and implement human judgements and LLM responses within this dataset, allowing for the reproduction and trialling of faithfulness evaluation across domains."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T13:28:29Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    13,
                    28,
                    29,
                    4,
                    339,
                    0
                ],
                "arxiv_comment": "9 pages, conference paper",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Ben Malin"
                    },
                    {
                        "name": "Tatiana Kalganova"
                    },
                    {
                        "name": "Nikolaos Boulgouris"
                    }
                ],
                "author_detail": {
                    "name": "Nikolaos Boulgouris"
                },
                "author": "Nikolaos Boulgouris"
            },
            {
                "id": "http://arxiv.org/abs/2510.13495v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.13495v2",
                "title": "Radio over Fiber with Cascaded Structure: Algorithm for Uplink Positioning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Radio over Fiber with Cascaded Structure: Algorithm for Uplink Positioning"
                },
                "updated": "2025-12-05T13:03:15Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    13,
                    3,
                    15,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.13495v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.13495v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recent advancements in polymer microwave fiber (PMF) technology have created significant opportunities for robust, low-cost, and high-speed sub-terahertz (THz) radio-over-fiber communications. Recognizing these potential benefits, this paper explores a novel radio-over-fiber (RoF) structure that interconnects multiple radio units (RUs) in cascade via fiber, envisioning its application in indoor scenarios. This structure creates a number of research opportunities when considering cascaded distortion effects introduced by non-linear power amplifiers (PAs) at the RUs and the propagation channel over the fiber. We propose maximum-likelihood and non-linear least-squares algorithms to estimate the propagation distance along the RoF and the time-of-arrival between the RoF and the user equipment. For the case of linear PAs, we derive the Cramr-Rao lower bound to benchmark the performance of the estimators. Finally, we investigate the use of the system for uplink positioning. Our simulation results demonstrate that the proposed estimators perform satisfactorily even with the cascaded effects of non-linear PAs, and that the deployment of this RoF structure can enable new cost-effective opportunities for high-resolution positioning in indoor scenarios. In the numerical evaluation, we also use measured PMF characteristics for high-density polyethylene fibers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in polymer microwave fiber (PMF) technology have created significant opportunities for robust, low-cost, and high-speed sub-terahertz (THz) radio-over-fiber communications. Recognizing these potential benefits, this paper explores a novel radio-over-fiber (RoF) structure that interconnects multiple radio units (RUs) in cascade via fiber, envisioning its application in indoor scenarios. This structure creates a number of research opportunities when considering cascaded distortion effects introduced by non-linear power amplifiers (PAs) at the RUs and the propagation channel over the fiber. We propose maximum-likelihood and non-linear least-squares algorithms to estimate the propagation distance along the RoF and the time-of-arrival between the RoF and the user equipment. For the case of linear PAs, we derive the Cramr-Rao lower bound to benchmark the performance of the estimators. Finally, we investigate the use of the system for uplink positioning. Our simulation results demonstrate that the proposed estimators perform satisfactorily even with the cascaded effects of non-linear PAs, and that the deployment of this RoF structure can enable new cost-effective opportunities for high-resolution positioning in indoor scenarios. In the numerical evaluation, we also use measured PMF characteristics for high-density polyethylene fibers."
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-15T12:44:34Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    12,
                    44,
                    34,
                    2,
                    288,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP"
                },
                "authors": [
                    {
                        "name": "Dexin Kong"
                    },
                    {
                        "name": "Diana Pamela Moya Osorio"
                    },
                    {
                        "name": "Erik G. Larsson"
                    }
                ],
                "author_detail": {
                    "name": "Erik G. Larsson"
                },
                "author": "Erik G. Larsson"
            },
            {
                "id": "http://arxiv.org/abs/2512.05686v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05686v1",
                "title": "LA-RL: Language Action-guided Reinforcement Learning with Safety Guarantees for Autonomous Highway Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LA-RL: Language Action-guided Reinforcement Learning with Safety Guarantees for Autonomous Highway Driving"
                },
                "updated": "2025-12-05T13:02:44Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    13,
                    2,
                    44,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05686v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05686v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Autonomous highway driving demands a critical balance between proactive, efficiency-seeking behavior and robust safety guarantees. This paper proposes Language Action-guided Reinforcement Learning (LA-RL) with Safety Guarantees, a novel framework that integrates the semantic reasoning of large language models (LLMs) into the actor-critic architecture with an improved safety layer. Within this framework, task-specific reward shaping harmonizes the dual objectives of maximizing driving efficiency and ensuring safety, guiding decision-making based on both environmental insights and clearly defined goals. To enhance safety, LA-RL incorporates a safety-critical planner that combines model predictive control (MPC) with discrete control barrier functions (DCBFs). This layer formally constrains the LLM-informed policy to a safe action set, employs a slack mechanism that enhances solution feasibility, prevents overly conservative behavior and allows for greater policy exploration without compromising safety. Extensive experiments demonstrate that it significantly outperforms several current state-of-the-art methods, offering a more adaptive, reliable, and robust solution for autonomous highway driving. Compared to existing SOTA, it achieves approximately 20$\\%$ higher success rate than the knowledge graph (KG) based baseline and about 30$\\%$ higher than the retrieval augmented generation (RAG) based baseline. In low-density environments, LA-RL achieves a 100$\\%$ success rate. These results confirm its enhanced exploration of the state-action space and its ability to autonomously adopt more efficient, proactive strategies in complex, mixed-traffic highway environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous highway driving demands a critical balance between proactive, efficiency-seeking behavior and robust safety guarantees. This paper proposes Language Action-guided Reinforcement Learning (LA-RL) with Safety Guarantees, a novel framework that integrates the semantic reasoning of large language models (LLMs) into the actor-critic architecture with an improved safety layer. Within this framework, task-specific reward shaping harmonizes the dual objectives of maximizing driving efficiency and ensuring safety, guiding decision-making based on both environmental insights and clearly defined goals. To enhance safety, LA-RL incorporates a safety-critical planner that combines model predictive control (MPC) with discrete control barrier functions (DCBFs). This layer formally constrains the LLM-informed policy to a safe action set, employs a slack mechanism that enhances solution feasibility, prevents overly conservative behavior and allows for greater policy exploration without compromising safety. Extensive experiments demonstrate that it significantly outperforms several current state-of-the-art methods, offering a more adaptive, reliable, and robust solution for autonomous highway driving. Compared to existing SOTA, it achieves approximately 20$\\%$ higher success rate than the knowledge graph (KG) based baseline and about 30$\\%$ higher than the retrieval augmented generation (RAG) based baseline. In low-density environments, LA-RL achieves a 100$\\%$ success rate. These results confirm its enhanced exploration of the state-action space and its ability to autonomously adopt more efficient, proactive strategies in complex, mixed-traffic highway environments."
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T13:02:44Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    13,
                    2,
                    44,
                    4,
                    339,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY"
                },
                "authors": [
                    {
                        "name": "Yiming Shu"
                    },
                    {
                        "name": "Jiahui Xu"
                    },
                    {
                        "name": "Jiwei Tang"
                    },
                    {
                        "name": "Ruiyang Gao"
                    },
                    {
                        "name": "Chen Sun"
                    }
                ],
                "author_detail": {
                    "name": "Chen Sun"
                },
                "author": "Chen Sun"
            },
            {
                "id": "http://arxiv.org/abs/2512.05671v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05671v1",
                "title": "MedTutor-R1: Socratic Personalized Medical Teaching with Multi-Agent Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MedTutor-R1: Socratic Personalized Medical Teaching with Multi-Agent Simulation"
                },
                "updated": "2025-12-05T12:28:30Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    12,
                    28,
                    30,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05671v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05671v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The significant gap between rising demands for clinical training and the scarcity of expert instruction poses a major challenge to medical education. With powerful capabilities in personalized guidance, Large Language Models (LLMs) offer a promising solution to bridge this gap. However, current research focuses mainly on one-on-one knowledge instruction, overlooking collaborative reasoning, a key skill for students developed in teamwork like ward rounds. To this end, we develop ClinEdu, a multi-agent pedagogical simulator with personality-driven patients and diverse student cohorts, enabling controlled testing of complex pedagogical processes and scalable generation of teaching data. Based on ClinEdu, we construct ClinTeach, a large Socratic teaching dialogue dataset that captures the complexities of group instruction. We then train MedTutor-R1, the first multimodal Socratic tutor designed for one-to-many instruction in clinical medical education. MedTutor-R1 is first instruction-tuned on our ClinTeach dataset and then optimized with reinforcement learning, using rewards derived from a three-axis rubric, covering structural fidelity, analytical quality, and clinical safety, to refine its adaptive Socratic strategies. For authentic in-situ assessment, we use simulation-based interactive evaluation that redeploys the tutor back into ClinEdu. Experimental results demonstrate that our MedTutor-R1 outperforms the base model by over 20% in average pedagogical score and is comparable to o3, while also exhibiting high adaptability in handling a varying number of students. This promising performance underscores the effectiveness of our pedagogical simulator, ClinEdu.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The significant gap between rising demands for clinical training and the scarcity of expert instruction poses a major challenge to medical education. With powerful capabilities in personalized guidance, Large Language Models (LLMs) offer a promising solution to bridge this gap. However, current research focuses mainly on one-on-one knowledge instruction, overlooking collaborative reasoning, a key skill for students developed in teamwork like ward rounds. To this end, we develop ClinEdu, a multi-agent pedagogical simulator with personality-driven patients and diverse student cohorts, enabling controlled testing of complex pedagogical processes and scalable generation of teaching data. Based on ClinEdu, we construct ClinTeach, a large Socratic teaching dialogue dataset that captures the complexities of group instruction. We then train MedTutor-R1, the first multimodal Socratic tutor designed for one-to-many instruction in clinical medical education. MedTutor-R1 is first instruction-tuned on our ClinTeach dataset and then optimized with reinforcement learning, using rewards derived from a three-axis rubric, covering structural fidelity, analytical quality, and clinical safety, to refine its adaptive Socratic strategies. For authentic in-situ assessment, we use simulation-based interactive evaluation that redeploys the tutor back into ClinEdu. Experimental results demonstrate that our MedTutor-R1 outperforms the base model by over 20% in average pedagogical score and is comparable to o3, while also exhibiting high adaptability in handling a varying number of students. This promising performance underscores the effectiveness of our pedagogical simulator, ClinEdu."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T12:28:30Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    12,
                    28,
                    30,
                    4,
                    339,
                    0
                ],
                "arxiv_comment": "Work In Progress",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Zhitao He"
                    },
                    {
                        "name": "Haolin Yang"
                    },
                    {
                        "name": "Zeyu Qin"
                    },
                    {
                        "name": "Yi R Fung"
                    }
                ],
                "author_detail": {
                    "name": "Yi R Fung"
                },
                "author": "Yi R Fung"
            },
            {
                "id": "http://arxiv.org/abs/2512.05659v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05659v1",
                "title": "Beyond Automation: Redesigning Jobs with LLMs to Enhance Productivity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Automation: Redesigning Jobs with LLMs to Enhance Productivity"
                },
                "updated": "2025-12-05T12:05:55Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    12,
                    5,
                    55,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05659v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05659v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The adoption of generative artificial intelligence (AI) is predicted to lead to fundamental shifts in the labour market, resulting in displacement or augmentation of AI-exposed roles. To investigate the impact of AI across a large organisation, we assessed AI exposure at the task level within roles at the UK Civil Service (UKCS). Using a novel dataset of UKCS job adverts, covering 193,497 vacancies over 6 years, our large language model (LLM)-driven analysis estimated AI exposure scores of 1,542,411 tasks. By aggregating AI exposure scores for tasks within each role, we calculated the mean and variance of job-level exposure to AI, highlighting the heterogeneous impacts of AI, even for seemingly identical jobs. We then use an LLM to redesign jobs, focusing on task automation, task optimisation, and task reallocation. We find that the redesign process leads to tasks where humans have comparative advantage over AI, including strategic leadership, complex problem resolution, and stakeholder management. Overall, automation and augmentation are expected to have nuanced effects across all levels of the organisational hierarchy. Most economic value of AI is expected to arise from productivity gains rather than role displacement. We contribute to the automation, augmentation and productivity debates as well as advance our understanding of job redesign in the age of AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The adoption of generative artificial intelligence (AI) is predicted to lead to fundamental shifts in the labour market, resulting in displacement or augmentation of AI-exposed roles. To investigate the impact of AI across a large organisation, we assessed AI exposure at the task level within roles at the UK Civil Service (UKCS). Using a novel dataset of UKCS job adverts, covering 193,497 vacancies over 6 years, our large language model (LLM)-driven analysis estimated AI exposure scores of 1,542,411 tasks. By aggregating AI exposure scores for tasks within each role, we calculated the mean and variance of job-level exposure to AI, highlighting the heterogeneous impacts of AI, even for seemingly identical jobs. We then use an LLM to redesign jobs, focusing on task automation, task optimisation, and task reallocation. We find that the redesign process leads to tasks where humans have comparative advantage over AI, including strategic leadership, complex problem resolution, and stakeholder management. Overall, automation and augmentation are expected to have nuanced effects across all levels of the organisational hierarchy. Most economic value of AI is expected to arise from productivity gains rather than role displacement. We contribute to the automation, augmentation and productivity debates as well as advance our understanding of job redesign in the age of AI."
                },
                "tags": [
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T12:05:55Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    12,
                    5,
                    55,
                    4,
                    339,
                    0
                ],
                "arxiv_comment": "98 pages, 22 figures",
                "arxiv_primary_category": {
                    "term": "econ.GN"
                },
                "authors": [
                    {
                        "name": "Andrew Ledingham"
                    },
                    {
                        "name": "Michael Hollins"
                    },
                    {
                        "name": "Matthew Lyon"
                    },
                    {
                        "name": "David Gillespie"
                    },
                    {
                        "name": "Umar Yunis-Guerra"
                    },
                    {
                        "name": "Jamie Siviter"
                    },
                    {
                        "name": "David Duncan"
                    },
                    {
                        "name": "Oliver P. Hauser"
                    }
                ],
                "author_detail": {
                    "name": "Oliver P. Hauser"
                },
                "author": "Oliver P. Hauser"
            },
            {
                "id": "http://arxiv.org/abs/2512.05658v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05658v1",
                "title": "Grounded Multilingual Medical Reasoning for Question Answering with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Grounded Multilingual Medical Reasoning for Question Answering with Large Language Models"
                },
                "updated": "2025-12-05T12:05:46Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    12,
                    5,
                    46,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05658v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05658v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) with reasoning capabilities have recently demonstrated strong potential in medical Question Answering (QA). Existing approaches are largely English-focused and primarily rely on distillation from general-purpose LLMs, raising concerns about the reliability of their medical knowledge. In this work, we present a method to generate multilingual reasoning traces grounded in factual medical knowledge. We produce 500k traces in English, Italian, and Spanish, using a retrievalaugmented generation approach over medical information from Wikipedia. The traces are generated to solve medical questions drawn from MedQA and MedMCQA, which we extend to Italian and Spanish. We test our pipeline in both in-domain and outof-domain settings across Medical QA benchmarks, and demonstrate that our reasoning traces improve performance both when utilized via in-context learning (few-shot) and supervised fine-tuning, yielding state-of-the-art results among 8B-parameter LLMs. We believe that these resources can support the development of safer, more transparent clinical decision-support tools in multilingual settings. We release the full suite of resources: reasoning traces, translated QA datasets, Medical-Wikipedia, and fine-tuned models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) with reasoning capabilities have recently demonstrated strong potential in medical Question Answering (QA). Existing approaches are largely English-focused and primarily rely on distillation from general-purpose LLMs, raising concerns about the reliability of their medical knowledge. In this work, we present a method to generate multilingual reasoning traces grounded in factual medical knowledge. We produce 500k traces in English, Italian, and Spanish, using a retrievalaugmented generation approach over medical information from Wikipedia. The traces are generated to solve medical questions drawn from MedQA and MedMCQA, which we extend to Italian and Spanish. We test our pipeline in both in-domain and outof-domain settings across Medical QA benchmarks, and demonstrate that our reasoning traces improve performance both when utilized via in-context learning (few-shot) and supervised fine-tuning, yielding state-of-the-art results among 8B-parameter LLMs. We believe that these resources can support the development of safer, more transparent clinical decision-support tools in multilingual settings. We release the full suite of resources: reasoning traces, translated QA datasets, Medical-Wikipedia, and fine-tuned models."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T12:05:46Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    12,
                    5,
                    46,
                    4,
                    339,
                    0
                ],
                "arxiv_comment": "Under Review",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Pietro Ferrazzi"
                    },
                    {
                        "name": "Aitor Soroa"
                    },
                    {
                        "name": "Rodrigo Agerri"
                    }
                ],
                "author_detail": {
                    "name": "Rodrigo Agerri"
                },
                "author": "Rodrigo Agerri"
            },
            {
                "id": "http://arxiv.org/abs/2512.05649v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05649v1",
                "title": "Physics-Guided Surrogate Modeling for Machine Learning-Driven DLD Design Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Physics-Guided Surrogate Modeling for Machine Learning-Driven DLD Design Optimization"
                },
                "updated": "2025-12-05T11:51:10Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    11,
                    51,
                    10,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05649v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05649v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Sorting cells based on their mechanical properties is essential for applications in disease diagnostics, cell therapy, and biomedical research. Deterministic Lateral Displacement (DLD) devices provide a label-free method for achieving such sorting, but their performance is highly sensitive to cell size and deformability. Designing effective DLD geometries often demands extensive trial-and-error experimentation, as even small variations in cellular mechanical traits can cause significant changes in migration behavior. To address this challenge, we propose a simulation-driven machine learning (ML) framework that predicts suitable DLD design candidates for a given cell type. Our approach integrates high-fidelity particle-based simulations to model cell deformation and migration through microfluidic pillar arrays with supervised ML models trained to estimate optimal geometries. By mapping mechanical parameters such as bending rigidity and shear modulus to deformation index and migration angle, the framework enables rapid, data-informed design of DLD systems. We also demonstrate a deployable web interface to make this tool accessible for real-world device prototyping.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sorting cells based on their mechanical properties is essential for applications in disease diagnostics, cell therapy, and biomedical research. Deterministic Lateral Displacement (DLD) devices provide a label-free method for achieving such sorting, but their performance is highly sensitive to cell size and deformability. Designing effective DLD geometries often demands extensive trial-and-error experimentation, as even small variations in cellular mechanical traits can cause significant changes in migration behavior. To address this challenge, we propose a simulation-driven machine learning (ML) framework that predicts suitable DLD design candidates for a given cell type. Our approach integrates high-fidelity particle-based simulations to model cell deformation and migration through microfluidic pillar arrays with supervised ML models trained to estimate optimal geometries. By mapping mechanical parameters such as bending rigidity and shear modulus to deformation index and migration angle, the framework enables rapid, data-informed design of DLD systems. We also demonstrate a deployable web interface to make this tool accessible for real-world device prototyping."
                },
                "tags": [
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T11:51:10Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    11,
                    51,
                    10,
                    4,
                    339,
                    0
                ],
                "arxiv_comment": "33 pages, 5 figures",
                "arxiv_primary_category": {
                    "term": "q-bio.QM"
                },
                "authors": [
                    {
                        "name": "Khayrul Islam"
                    },
                    {
                        "name": "Mehedi Hasan"
                    },
                    {
                        "name": "Yaling Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yaling Liu"
                },
                "author": "Yaling Liu"
            },
            {
                "id": "http://arxiv.org/abs/2512.05648v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05648v1",
                "title": "Beyond Data Filtering: Knowledge Localization for Capability Removal in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Data Filtering: Knowledge Localization for Capability Removal in LLMs"
                },
                "updated": "2025-12-05T11:48:37Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    11,
                    48,
                    37,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05648v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05648v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models increasingly possess capabilities that carry dual-use risks. While data filtering has emerged as a pretraining-time mitigation, it faces significant challenges: labeling whether data is harmful is expensive at scale, and given improving sample efficiency with larger models, even small amounts of mislabeled content could give rise to dangerous capabilities. To address risks associated with mislabeled harmful content, prior work proposed Gradient Routing (Cloud et al., 2024) -- a technique that localizes target knowledge into a dedicated subset of model parameters so they can later be removed. We explore an improved variant of Gradient Routing, which we call Selective GradienT Masking (SGTM), with particular focus on evaluating its robustness to label noise. SGTM zero-masks selected gradients such that target domain examples only update their dedicated parameters. We test SGTM's effectiveness in two applications: removing knowledge of one language from a model trained on a bilingual synthetic dataset, and removing biology knowledge from a model trained on English Wikipedia. In both cases SGTM provides better retain/forget trade-off in the presence of labeling errors compared to both data filtering and a previously proposed instantiation of Gradient Routing. Unlike shallow unlearning approaches that can be quickly undone through fine-tuning, SGTM exhibits strong robustness to adversarial fine-tuning, requiring seven times more fine-tuning steps to reach baseline performance on the forget set compared to a finetuning-based unlearning method (RMU). Our results suggest SGTM provides a promising pretraining-time complement to existing safety mitigations, particularly in settings where label noise is unavoidable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models increasingly possess capabilities that carry dual-use risks. While data filtering has emerged as a pretraining-time mitigation, it faces significant challenges: labeling whether data is harmful is expensive at scale, and given improving sample efficiency with larger models, even small amounts of mislabeled content could give rise to dangerous capabilities. To address risks associated with mislabeled harmful content, prior work proposed Gradient Routing (Cloud et al., 2024) -- a technique that localizes target knowledge into a dedicated subset of model parameters so they can later be removed. We explore an improved variant of Gradient Routing, which we call Selective GradienT Masking (SGTM), with particular focus on evaluating its robustness to label noise. SGTM zero-masks selected gradients such that target domain examples only update their dedicated parameters. We test SGTM's effectiveness in two applications: removing knowledge of one language from a model trained on a bilingual synthetic dataset, and removing biology knowledge from a model trained on English Wikipedia. In both cases SGTM provides better retain/forget trade-off in the presence of labeling errors compared to both data filtering and a previously proposed instantiation of Gradient Routing. Unlike shallow unlearning approaches that can be quickly undone through fine-tuning, SGTM exhibits strong robustness to adversarial fine-tuning, requiring seven times more fine-tuning steps to reach baseline performance on the forget set compared to a finetuning-based unlearning method (RMU). Our results suggest SGTM provides a promising pretraining-time complement to existing safety mitigations, particularly in settings where label noise is unavoidable."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T11:48:37Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    11,
                    48,
                    37,
                    4,
                    339,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Igor Shilov"
                    },
                    {
                        "name": "Alex Cloud"
                    },
                    {
                        "name": "Aryo Pradipta Gema"
                    },
                    {
                        "name": "Jacob Goldman-Wetzler"
                    },
                    {
                        "name": "Nina Panickssery"
                    },
                    {
                        "name": "Henry Sleight"
                    },
                    {
                        "name": "Erik Jones"
                    },
                    {
                        "name": "Cem Anil"
                    }
                ],
                "author_detail": {
                    "name": "Cem Anil"
                },
                "author": "Cem Anil"
            },
            {
                "id": "http://arxiv.org/abs/2512.05647v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05647v1",
                "title": "A Greek Government Decisions Dataset for Public-Sector Analysis and Insight",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Greek Government Decisions Dataset for Public-Sector Analysis and Insight"
                },
                "updated": "2025-12-05T11:47:33Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    11,
                    47,
                    33,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05647v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05647v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We introduce an open, machine-readable corpus of Greek government decisions sourced from the national transparency platform Diavgeia. The resource comprises 1 million decisions, featuring and high-quality raw text extracted from PDFs. It is released with raw extracted text in Markdown format, alongside a fully reproducible extraction pipeline. Beyond the core dataset, we conduct qualitative analyses to explore boilerplate patterns and design a retrieval-augmented generation (RAG) task by formulating a set of representative questions, creating high-quality answers, and evaluating a baseline RAG system on its ability to retrieve and reason over public decisions. This evaluation demonstrates the potential of large-scale public-sector corpora to support advanced information access and transparency through structured retrieval and reasoning over governmental documents, and highlights how such a RAG pipeline could simulate a chat-based assistant capable of interactively answering questions about public decisions. Due to its scale, quality, and domain coverage, the corpus can also serve as high-value pre-training or fine-tuning material for new Language Models (LMs) and Large Language Models (LLMs) respectively, including specialized models for legal and governmental domains, and as a foundation for novel approaches in domain adaptation, knowledge-grounded generation, and explainable AI. Finally, we discuss limitations, outline future directions, and make both the data and the code accessible.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce an open, machine-readable corpus of Greek government decisions sourced from the national transparency platform Diavgeia. The resource comprises 1 million decisions, featuring and high-quality raw text extracted from PDFs. It is released with raw extracted text in Markdown format, alongside a fully reproducible extraction pipeline. Beyond the core dataset, we conduct qualitative analyses to explore boilerplate patterns and design a retrieval-augmented generation (RAG) task by formulating a set of representative questions, creating high-quality answers, and evaluating a baseline RAG system on its ability to retrieve and reason over public decisions. This evaluation demonstrates the potential of large-scale public-sector corpora to support advanced information access and transparency through structured retrieval and reasoning over governmental documents, and highlights how such a RAG pipeline could simulate a chat-based assistant capable of interactively answering questions about public decisions. Due to its scale, quality, and domain coverage, the corpus can also serve as high-value pre-training or fine-tuning material for new Language Models (LMs) and Large Language Models (LLMs) respectively, including specialized models for legal and governmental domains, and as a foundation for novel approaches in domain adaptation, knowledge-grounded generation, and explainable AI. Finally, we discuss limitations, outline future directions, and make both the data and the code accessible."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T11:47:33Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    11,
                    47,
                    33,
                    4,
                    339,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Giorgos Antoniou"
                    },
                    {
                        "name": "Giorgos Filandrianos"
                    },
                    {
                        "name": "Aggelos Vlachos"
                    },
                    {
                        "name": "Giorgos Stamou"
                    },
                    {
                        "name": "Lampros Kollimenos"
                    },
                    {
                        "name": "Konstantinos Skianis"
                    },
                    {
                        "name": "Michalis Vazirgiannis"
                    }
                ],
                "author_detail": {
                    "name": "Michalis Vazirgiannis"
                },
                "author": "Michalis Vazirgiannis"
            },
            {
                "id": "http://arxiv.org/abs/2507.20423v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2507.20423v2",
                "title": "CodeNER: Code Prompting for Named Entity Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeNER: Code Prompting for Named Entity Recognition"
                },
                "updated": "2025-12-05T11:44:29Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    11,
                    44,
                    29,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2507.20423v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2507.20423v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recent studies have explored various approaches for treating candidate named entity spans as both source and target sequences in named entity recognition (NER) by leveraging large language models (LLMs). Although previous approaches have successfully generated candidate named entity spans with suitable labels, they rely solely on input context information when using LLMs, particularly, ChatGPT. However, NER inherently requires capturing detailed labeling requirements with input context information. To address this issue, we propose a novel method that leverages code-based prompting to improve the capabilities of LLMs in understanding and performing NER. By embedding code within prompts, we provide detailed BIO schema instructions for labeling, thereby exploiting the ability of LLMs to comprehend long-range scopes in programming languages. Experimental results demonstrate that the proposed code-based prompting method outperforms conventional text-based prompting on ten benchmarks across English, Arabic, Finnish, Danish, and German datasets, indicating the effectiveness of explicitly structuring NER instructions. We also verify that combining the proposed code-based prompting method with the chain-of-thought prompting further improves performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies have explored various approaches for treating candidate named entity spans as both source and target sequences in named entity recognition (NER) by leveraging large language models (LLMs). Although previous approaches have successfully generated candidate named entity spans with suitable labels, they rely solely on input context information when using LLMs, particularly, ChatGPT. However, NER inherently requires capturing detailed labeling requirements with input context information. To address this issue, we propose a novel method that leverages code-based prompting to improve the capabilities of LLMs in understanding and performing NER. By embedding code within prompts, we provide detailed BIO schema instructions for labeling, thereby exploiting the ability of LLMs to comprehend long-range scopes in programming languages. Experimental results demonstrate that the proposed code-based prompting method outperforms conventional text-based prompting on ten benchmarks across English, Arabic, Finnish, Danish, and German datasets, indicating the effectiveness of explicitly structuring NER instructions. We also verify that combining the proposed code-based prompting method with the chain-of-thought prompting further improves performance."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-07-27T21:49:36Z",
                "published_parsed": [
                    2025,
                    7,
                    27,
                    21,
                    49,
                    36,
                    6,
                    208,
                    0
                ],
                "arxiv_comment": "18 pages, 6 figures",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Sungwoo Han"
                    },
                    {
                        "name": "Jingun Kwon"
                    },
                    {
                        "name": "Hidetaka Kamigaito"
                    },
                    {
                        "name": "Manabu Okumura"
                    }
                ],
                "author_detail": {
                    "name": "Manabu Okumura"
                },
                "author": "Manabu Okumura"
            },
            {
                "id": "http://arxiv.org/abs/2511.22444v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.22444v2",
                "title": "Performant Synchronization in Geo-Distributed Databases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performant Synchronization in Geo-Distributed Databases"
                },
                "updated": "2025-12-05T11:33:00Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    11,
                    33,
                    0,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.22444v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.22444v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The deployment of databases across geographically distributed regions has become increasingly critical for ensuring data reliability and scalability. Recent studies indicate that distributed databases exhibit significantly higher latency than single-node databases, primarily due to consensus protocols maintaining data consistency across multiple nodes. We argue that synchronization cost constitutes the primary bottleneck for distributed databases, which is particularly pronounced in wide-area networks (WAN). Fortunately, we identify opportunities to optimize synchronization costs in real production environments: (1) network clustering phenomena, (2) triangle inequality violations in transmission, and (3) redundant data transfers. Based on these observations, we propose GeoCoCo, a synchronization acceleration framework for cross-region distributed databases. First, GeoCoCo presents a group rescheduling strategy that adapts to real-time network conditions to maximize WAN transmission efficiency. Second, GeoCoCo introduces a task-preserving data filtering method that reduces data volume transmitted over the WAN. Finally, GeoCoCo develops a consistency-guaranteed transmission framework integrating grouping and pruning. Extensive evaluations in both trace-driven simulations and real-world deployments demonstrate that GeoCoCo reduces synchronization cost-primarily by lowering WAN bandwidth usage-by up to 40.3%, and increases system throughput by up to 14.1% in GeoGauss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of databases across geographically distributed regions has become increasingly critical for ensuring data reliability and scalability. Recent studies indicate that distributed databases exhibit significantly higher latency than single-node databases, primarily due to consensus protocols maintaining data consistency across multiple nodes. We argue that synchronization cost constitutes the primary bottleneck for distributed databases, which is particularly pronounced in wide-area networks (WAN). Fortunately, we identify opportunities to optimize synchronization costs in real production environments: (1) network clustering phenomena, (2) triangle inequality violations in transmission, and (3) redundant data transfers. Based on these observations, we propose GeoCoCo, a synchronization acceleration framework for cross-region distributed databases. First, GeoCoCo presents a group rescheduling strategy that adapts to real-time network conditions to maximize WAN transmission efficiency. Second, GeoCoCo introduces a task-preserving data filtering method that reduces data volume transmitted over the WAN. Finally, GeoCoCo develops a consistency-guaranteed transmission framework integrating grouping and pruning. Extensive evaluations in both trace-driven simulations and real-world deployments demonstrate that GeoCoCo reduces synchronization cost-primarily by lowering WAN bandwidth usage-by up to 40.3%, and increases system throughput by up to 14.1% in GeoGauss."
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-27T13:31:00Z",
                "published_parsed": [
                    2025,
                    11,
                    27,
                    13,
                    31,
                    0,
                    3,
                    331,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB"
                },
                "authors": [
                    {
                        "name": "Duling Xu"
                    },
                    {
                        "name": "Tong Li"
                    },
                    {
                        "name": "Zegang Sun"
                    },
                    {
                        "name": "Zheng Chen"
                    },
                    {
                        "name": "Weixing Zhou"
                    },
                    {
                        "name": "Yanfeng Zhang"
                    },
                    {
                        "name": "Wei Lu"
                    },
                    {
                        "name": "Xiaoyong Du"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoyong Du"
                },
                "author": "Xiaoyong Du"
            },
            {
                "id": "http://arxiv.org/abs/2511.18491v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.18491v3",
                "title": "MindEval: Benchmarking Language Models on Multi-turn Mental Health Support",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MindEval: Benchmarking Language Models on Multi-turn Mental Health Support"
                },
                "updated": "2025-12-05T11:28:14Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    11,
                    28,
                    14,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.18491v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.18491v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Demand for mental health support through AI chatbots is surging, though current systems present several limitations, like sycophancy or overvalidation, and reinforcement of maladaptive beliefs. A core obstacle to the creation of better systems is the scarcity of benchmarks that capture the complexity of real therapeutic interactions. Most existing benchmarks either only test clinical knowledge through multiple-choice questions or assess single responses in isolation. To bridge this gap, we present MindEval, a framework designed in collaboration with Ph.D-level Licensed Clinical Psychologists for automatically evaluating language models in realistic, multi-turn mental health therapy conversations. Through patient simulation and automatic evaluation with LLMs, our framework balances resistance to gaming with reproducibility via its fully automated, model-agnostic design. We begin by quantitatively validating the realism of our simulated patients against human-generated text and by demonstrating strong correlations between automatic and human expert judgments. Then, we evaluate 12 state-of-the-art LLMs and show that all models struggle, scoring below 4 out of 6, on average, with particular weaknesses in problematic AI-specific patterns of communication. Notably, reasoning capabilities and model scale do not guarantee better performance, and systems deteriorate with longer interactions or when supporting patients with severe symptoms. We release all code, prompts, and human evaluation data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demand for mental health support through AI chatbots is surging, though current systems present several limitations, like sycophancy or overvalidation, and reinforcement of maladaptive beliefs. A core obstacle to the creation of better systems is the scarcity of benchmarks that capture the complexity of real therapeutic interactions. Most existing benchmarks either only test clinical knowledge through multiple-choice questions or assess single responses in isolation. To bridge this gap, we present MindEval, a framework designed in collaboration with Ph.D-level Licensed Clinical Psychologists for automatically evaluating language models in realistic, multi-turn mental health therapy conversations. Through patient simulation and automatic evaluation with LLMs, our framework balances resistance to gaming with reproducibility via its fully automated, model-agnostic design. We begin by quantitatively validating the realism of our simulated patients against human-generated text and by demonstrating strong correlations between automatic and human expert judgments. Then, we evaluate 12 state-of-the-art LLMs and show that all models struggle, scoring below 4 out of 6, on average, with particular weaknesses in problematic AI-specific patterns of communication. Notably, reasoning capabilities and model scale do not guarantee better performance, and systems deteriorate with longer interactions or when supporting patients with severe symptoms. We release all code, prompts, and human evaluation data."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-23T15:19:29Z",
                "published_parsed": [
                    2025,
                    11,
                    23,
                    15,
                    19,
                    29,
                    6,
                    327,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Jos Pombal"
                    },
                    {
                        "name": "Maya D'Eon"
                    },
                    {
                        "name": "Nuno M. Guerreiro"
                    },
                    {
                        "name": "Pedro Henrique Martins"
                    },
                    {
                        "name": "Antnio Farinhas"
                    },
                    {
                        "name": "Ricardo Rei"
                    }
                ],
                "author_detail": {
                    "name": "Ricardo Rei"
                },
                "author": "Ricardo Rei"
            },
            {
                "id": "http://arxiv.org/abs/2505.17101v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2505.17101v4",
                "title": "A quantitative analysis of semantic information in deep representations of text and images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A quantitative analysis of semantic information in deep representations of text and images"
                },
                "updated": "2025-12-05T11:14:03Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    11,
                    14,
                    3,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2505.17101v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2505.17101v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Deep neural networks are known to develop similar representations for semantically related data, even when they belong to different domains, such as an image and its description, or the same text in different languages. We present a method for quantitatively investigating this phenomenon by measuring the relative information content of the representations of semantically related data and probing how it is encoded into multiple tokens of large language models (LLMs) and vision transformers. Looking first at how LLMs process pairs of translated sentences, we identify inner ``semantic'' layers containing the most language-transferable information. We find moreover that, on these layers, a larger LLM (DeepSeek-V3) extracts significantly more general information than a smaller one (Llama3.1-8B). Semantic information of English text is spread across many tokens and it is characterized by long-distance correlations between tokens and by a causal left-to-right (i.e., past-future) asymmetry. We also identify layers encoding semantic information within visual transformers. We show that caption representations in the semantic layers of LLMs predict visual representations of the corresponding images. We observe significant and model-dependent information asymmetries between image and text representations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep neural networks are known to develop similar representations for semantically related data, even when they belong to different domains, such as an image and its description, or the same text in different languages. We present a method for quantitatively investigating this phenomenon by measuring the relative information content of the representations of semantically related data and probing how it is encoded into multiple tokens of large language models (LLMs) and vision transformers. Looking first at how LLMs process pairs of translated sentences, we identify inner ``semantic'' layers containing the most language-transferable information. We find moreover that, on these layers, a larger LLM (DeepSeek-V3) extracts significantly more general information than a smaller one (Llama3.1-8B). Semantic information of English text is spread across many tokens and it is characterized by long-distance correlations between tokens and by a causal left-to-right (i.e., past-future) asymmetry. We also identify layers encoding semantic information within visual transformers. We show that caption representations in the semantic layers of LLMs predict visual representations of the corresponding images. We observe significant and model-dependent information asymmetries between image and text representations."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-05-21T07:38:48Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    7,
                    38,
                    48,
                    2,
                    141,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Santiago Acevedo"
                    },
                    {
                        "name": "Andrea Mascaretti"
                    },
                    {
                        "name": "Riccardo Rende"
                    },
                    {
                        "name": "Mato Mahaut"
                    },
                    {
                        "name": "Marco Baroni"
                    },
                    {
                        "name": "Alessandro Laio"
                    }
                ],
                "author_detail": {
                    "name": "Alessandro Laio"
                },
                "author": "Alessandro Laio"
            },
            {
                "id": "http://arxiv.org/abs/2506.03761v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2506.03761v2",
                "title": "Pet-Bench: Benchmarking the Abilities of Large Language Models as E-Pets in Social Network Services",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pet-Bench: Benchmarking the Abilities of Large Language Models as E-Pets in Social Network Services"
                },
                "updated": "2025-12-05T10:49:58Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    10,
                    49,
                    58,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2506.03761v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2506.03761v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "As interest in using Large Language Models for interactive and emotionally rich experiences grows, virtual pet companionship emerges as a novel yet underexplored application. Existing approaches focus on basic pet role-playing interactions without systematically benchmarking LLMs for comprehensive companionship. In this paper, we introduce Pet-Bench, a dedicated benchmark that evaluates LLMs across both self-interaction and human-interaction dimensions. Unlike prior work, Pet-Bench emphasizes self-evolution and developmental behaviors alongside interactive engagement, offering a more realistic reflection of pet companionship. It features diverse tasks such as intelligent scheduling, memory-based dialogues, and psychological conversations, with over 7,500 interaction instances designed to simulate pet behaviors. Evaluation of 28 LLMs reveals significant performance variations linked to model size and inherent capabilities, underscoring the need for specialized optimization in this domain. Pet-Bench serves as a foundational resource for benchmarking pet-related LLM abilities and advancing emotionally immersive human-pet interactions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As interest in using Large Language Models for interactive and emotionally rich experiences grows, virtual pet companionship emerges as a novel yet underexplored application. Existing approaches focus on basic pet role-playing interactions without systematically benchmarking LLMs for comprehensive companionship. In this paper, we introduce Pet-Bench, a dedicated benchmark that evaluates LLMs across both self-interaction and human-interaction dimensions. Unlike prior work, Pet-Bench emphasizes self-evolution and developmental behaviors alongside interactive engagement, offering a more realistic reflection of pet companionship. It features diverse tasks such as intelligent scheduling, memory-based dialogues, and psychological conversations, with over 7,500 interaction instances designed to simulate pet behaviors. Evaluation of 28 LLMs reveals significant performance variations linked to model size and inherent capabilities, underscoring the need for specialized optimization in this domain. Pet-Bench serves as a foundational resource for benchmarking pet-related LLM abilities and advancing emotionally immersive human-pet interactions."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-06-04T09:25:52Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    9,
                    25,
                    52,
                    2,
                    155,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Hongcheng Guo"
                    },
                    {
                        "name": "Zheyong Xie"
                    },
                    {
                        "name": "Shaosheng Cao"
                    },
                    {
                        "name": "Boyang Wang"
                    },
                    {
                        "name": "Weiting Liu"
                    },
                    {
                        "name": "Zheyu Ye"
                    },
                    {
                        "name": "Zhoujun Li"
                    },
                    {
                        "name": "Zuozhu Liu"
                    },
                    {
                        "name": "Wei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Wei Liu"
                },
                "author": "Wei Liu"
            },
            {
                "id": "http://arxiv.org/abs/2512.05594v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05594v1",
                "title": "Ontology Learning with LLMs: A Benchmark Study on Axiom Identification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ontology Learning with LLMs: A Benchmark Study on Axiom Identification"
                },
                "updated": "2025-12-05T10:28:56Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    10,
                    28,
                    56,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05594v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05594v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Ontologies are an important tool for structuring domain knowledge, but their development is a complex task that requires significant modelling and domain expertise. Ontology learning, aimed at automating this process, has seen advancements in the past decade with the improvement of Natural Language Processing techniques, and especially with the recent growth of Large Language Models (LLMs). This paper investigates the challenge of identifying axioms: fundamental ontology components that define logical relations between classes and properties. In this work, we introduce an Ontology Axiom Benchmark OntoAxiom, and systematically test LLMs on that benchmark for axiom identification, evaluating different prompting strategies, ontologies, and axiom types. The benchmark consists of nine medium-sized ontologies with together 17.118 triples, and 2.771 axioms. We focus on subclass, disjoint, subproperty, domain, and range axioms. To evaluate LLM performance, we compare twelve LLMs with three shot settings and two prompting strategies: a Direct approach where we query all axioms at once, versus an Axiom-by-Axiom (AbA) approach, where each prompt queries for one axiom only. Our findings show that the AbA prompting leads to higher F1 scores than the direct approach. However, performance varies across axioms, suggesting that certain axioms are more challenging to identify. The domain also influences performance: the FOAF ontology achieves a score of 0.642 for the subclass axiom, while the music ontology reaches only 0.218. Larger LLMs outperform smaller ones, but smaller models may still be viable for resource-constrained settings. Although performance overall is not high enough to fully automate axiom identification, LLMs can provide valuable candidate axioms to support ontology engineers with the development and refinement of ontologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ontologies are an important tool for structuring domain knowledge, but their development is a complex task that requires significant modelling and domain expertise. Ontology learning, aimed at automating this process, has seen advancements in the past decade with the improvement of Natural Language Processing techniques, and especially with the recent growth of Large Language Models (LLMs). This paper investigates the challenge of identifying axioms: fundamental ontology components that define logical relations between classes and properties. In this work, we introduce an Ontology Axiom Benchmark OntoAxiom, and systematically test LLMs on that benchmark for axiom identification, evaluating different prompting strategies, ontologies, and axiom types. The benchmark consists of nine medium-sized ontologies with together 17.118 triples, and 2.771 axioms. We focus on subclass, disjoint, subproperty, domain, and range axioms. To evaluate LLM performance, we compare twelve LLMs with three shot settings and two prompting strategies: a Direct approach where we query all axioms at once, versus an Axiom-by-Axiom (AbA) approach, where each prompt queries for one axiom only. Our findings show that the AbA prompting leads to higher F1 scores than the direct approach. However, performance varies across axioms, suggesting that certain axioms are more challenging to identify. The domain also influences performance: the FOAF ontology achieves a score of 0.642 for the subclass axiom, while the music ontology reaches only 0.218. Larger LLMs outperform smaller ones, but smaller models may still be viable for resource-constrained settings. Although performance overall is not high enough to fully automate axiom identification, LLMs can provide valuable candidate axioms to support ontology engineers with the development and refinement of ontologies."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T10:28:56Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    10,
                    28,
                    56,
                    4,
                    339,
                    0
                ],
                "arxiv_comment": "Submitted to Semantic Web Journal, under review",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Roos M. Bakker"
                    },
                    {
                        "name": "Daan L. Di Scala"
                    },
                    {
                        "name": "Maaike H. T. de Boer"
                    },
                    {
                        "name": "Stephan A. Raaijmakers"
                    }
                ],
                "author_detail": {
                    "name": "Stephan A. Raaijmakers"
                },
                "author": "Stephan A. Raaijmakers"
            },
            {
                "id": "http://arxiv.org/abs/2512.05580v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05580v1",
                "title": "Structured Reasoning with Tree-of-Thoughts for Bengali Math Word Problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structured Reasoning with Tree-of-Thoughts for Bengali Math Word Problems"
                },
                "updated": "2025-12-05T10:07:08Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    10,
                    7,
                    8,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05580v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05580v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Mathematical Word Problems (MWPs) are among the most challenging tasks in natural language processing because they require both linguistic understanding and multi-step numerical reasoning. While Chain-of-Thought (CoT) prompting has shown promise, its linear structure often propagates errors, limiting overall effectiveness. To address this limitation, we present the a systematic study of Tree-of-Thought (ToT) reasoning for Bengali MWPs using the SOMADHAN dataset. Owing to computational and token-cost constraints, we evaluate a curated set of 100 representative problems across multiple large language models (LLMs), including GPT-OSS and LLaMA variants, under standard prompting, CoT, and ToT strategies. Our results show that CoT improves baseline accuracy from 78% (standard prompting) to 83% on average, while ToT further increases performance by up to 5 percentage points, achieving 88% accuracy with GPT-OSS-120B. These improvements highlight that ToT is particularly effective in medium-to-large-scale models but may offer less advantage for smaller ones. Overall, our findings establish ToT as a robust framework for solving mathematical problems in low-resource languages such as Bengali. More broadly, this study shows that structured reasoning methods like ToT can provide more reliable and globally consistent outcomes than CoT, paving the way for better reasoning strategies in multilingual NLP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mathematical Word Problems (MWPs) are among the most challenging tasks in natural language processing because they require both linguistic understanding and multi-step numerical reasoning. While Chain-of-Thought (CoT) prompting has shown promise, its linear structure often propagates errors, limiting overall effectiveness. To address this limitation, we present the a systematic study of Tree-of-Thought (ToT) reasoning for Bengali MWPs using the SOMADHAN dataset. Owing to computational and token-cost constraints, we evaluate a curated set of 100 representative problems across multiple large language models (LLMs), including GPT-OSS and LLaMA variants, under standard prompting, CoT, and ToT strategies. Our results show that CoT improves baseline accuracy from 78% (standard prompting) to 83% on average, while ToT further increases performance by up to 5 percentage points, achieving 88% accuracy with GPT-OSS-120B. These improvements highlight that ToT is particularly effective in medium-to-large-scale models but may offer less advantage for smaller ones. Overall, our findings establish ToT as a robust framework for solving mathematical problems in low-resource languages such as Bengali. More broadly, this study shows that structured reasoning methods like ToT can provide more reliable and globally consistent outcomes than CoT, paving the way for better reasoning strategies in multilingual NLP."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T10:07:08Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    10,
                    7,
                    8,
                    4,
                    339,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Aurprita Mahmood"
                    },
                    {
                        "name": "Sabrin alam"
                    },
                    {
                        "name": "Neloy kumer Sagor"
                    },
                    {
                        "name": "Md. Abdul Hadi"
                    },
                    {
                        "name": "Md. Sehab Al Islam"
                    },
                    {
                        "name": "Minhajul Islam"
                    }
                ],
                "author_detail": {
                    "name": "Minhajul Islam"
                },
                "author": "Minhajul Islam"
            },
            {
                "id": "http://arxiv.org/abs/2512.05578v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05578v1",
                "title": "A Hyperspectral Imaging Guided Robotic Grasping System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Hyperspectral Imaging Guided Robotic Grasping System"
                },
                "updated": "2025-12-05T09:58:59Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    9,
                    58,
                    59,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05578v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05578v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1109/LRA.2025.3575654",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "Hyperspectral imaging is an advanced technique for precisely identifying and analyzing materials or objects. However, its integration with robotic grasping systems has so far been explored due to the deployment complexities and prohibitive costs. Within this paper, we introduce a novel hyperspectral imaging-guided robotic grasping system. The system consists of PRISM (Polyhedral Reflective Imaging Scanning Mechanism) and the SpectralGrasp framework. PRISM is designed to enable high-precision, distortion-free hyperspectral imaging while simplifying system integration and costs. SpectralGrasp generates robotic grasping strategies by effectively leveraging both the spatial and spectral information from hyperspectral images. The proposed system demonstrates substantial improvements in both textile recognition compared to human performance and sorting success rate compared to RGB-based methods. Additionally, a series of comparative experiments further validates the effectiveness of our system. The study highlights the potential benefits of integrating hyperspectral imaging with robotic grasping systems, showcasing enhanced recognition and grasping capabilities in complex and dynamic environments. The project is available at: https://zainzh.github.io/PRISM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hyperspectral imaging is an advanced technique for precisely identifying and analyzing materials or objects. However, its integration with robotic grasping systems has so far been explored due to the deployment complexities and prohibitive costs. Within this paper, we introduce a novel hyperspectral imaging-guided robotic grasping system. The system consists of PRISM (Polyhedral Reflective Imaging Scanning Mechanism) and the SpectralGrasp framework. PRISM is designed to enable high-precision, distortion-free hyperspectral imaging while simplifying system integration and costs. SpectralGrasp generates robotic grasping strategies by effectively leveraging both the spatial and spectral information from hyperspectral images. The proposed system demonstrates substantial improvements in both textile recognition compared to human performance and sorting success rate compared to RGB-based methods. Additionally, a series of comparative experiments further validates the effectiveness of our system. The study highlights the potential benefits of integrating hyperspectral imaging with robotic grasping systems, showcasing enhanced recognition and grasping capabilities in complex and dynamic environments. The project is available at: https://zainzh.github.io/PRISM."
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T09:58:59Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    9,
                    58,
                    59,
                    4,
                    339,
                    0
                ],
                "arxiv_comment": "8 pages, 7 figures, Accepted to IEEE Robotics and Automation Letters (RA-L) 2025",
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "authors": [
                    {
                        "name": "Zheng Sun"
                    },
                    {
                        "name": "Zhipeng Dong"
                    },
                    {
                        "name": "Shixiong Wang"
                    },
                    {
                        "name": "Zhongyi Chu"
                    },
                    {
                        "name": "Fei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Fei Chen"
                },
                "author": "Fei Chen",
                "arxiv_doi": "10.1109/LRA.2025.3575654"
            },
            {
                "id": "http://arxiv.org/abs/2509.15836v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2509.15836v2",
                "title": "Relational Dissonance in Human-AI Interactions: The Case of Knowledge Work",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Relational Dissonance in Human-AI Interactions: The Case of Knowledge Work"
                },
                "updated": "2025-12-05T09:58:52Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    9,
                    58,
                    52,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2509.15836v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2509.15836v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "When AI systems allow human-like communication, they elicit increasingly complex relational responses. Knowledge workers face a particular challenge: They approach these systems as tools while interacting with them in ways that resemble human social interaction. To understand the relational contexts that arise when humans engage with anthropomorphic conversational agents, we need to expand existing human-computer interaction frameworks. Through three workshops with qualitative researchers, we found that the fundamental ontological and relational ambiguities inherent in anthropomorphic conversational agents make it difficult for individuals to maintain consistent relational stances toward them. Our findings indicate that people's articulated positioning toward such agents often differs from the relational dynamics that occur during interactions. We propose the concept of relational dissonance to help researchers, designers, and policymakers recognize the resulting tensions in the development, deployment, and governance of anthropomorphic conversational agents and address the need for relational transparency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When AI systems allow human-like communication, they elicit increasingly complex relational responses. Knowledge workers face a particular challenge: They approach these systems as tools while interacting with them in ways that resemble human social interaction. To understand the relational contexts that arise when humans engage with anthropomorphic conversational agents, we need to expand existing human-computer interaction frameworks. Through three workshops with qualitative researchers, we found that the fundamental ontological and relational ambiguities inherent in anthropomorphic conversational agents make it difficult for individuals to maintain consistent relational stances toward them. Our findings indicate that people's articulated positioning toward such agents often differs from the relational dynamics that occur during interactions. We propose the concept of relational dissonance to help researchers, designers, and policymakers recognize the resulting tensions in the development, deployment, and governance of anthropomorphic conversational agents and address the need for relational transparency."
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-09-19T10:15:55Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    10,
                    15,
                    55,
                    4,
                    262,
                    0
                ],
                "arxiv_comment": "30 pages, 5 figures",
                "arxiv_primary_category": {
                    "term": "cs.HC"
                },
                "authors": [
                    {
                        "name": "Emrecan Gulay"
                    },
                    {
                        "name": "Eleonora Picco"
                    },
                    {
                        "name": "Enrico Glerean"
                    },
                    {
                        "name": "Corinna Coupette"
                    }
                ],
                "author_detail": {
                    "name": "Corinna Coupette"
                },
                "author": "Corinna Coupette"
            },
            {
                "id": "http://arxiv.org/abs/2512.05576v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05576v1",
                "title": "CureAgent: A Training-Free Executor-Analyst Framework for Clinical Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CureAgent: A Training-Free Executor-Analyst Framework for Clinical Reasoning"
                },
                "updated": "2025-12-05T09:56:58Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    9,
                    56,
                    58,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05576v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05576v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Current clinical agent built on small LLMs, such as TxAgent suffer from a \\textit{Context Utilization Failure}, where models successfully retrieve biomedical evidence due to supervised finetuning but fail to ground their diagnosis in that information. In this work, we propose the Executor-Analyst Framework, a modular architecture that decouples the syntactic precision of tool execution from the semantic robustness of clinical reasoning. By orchestrating specialized TxAgents (Executors) with long-context foundation models (Analysts), we mitigate the reasoning deficits observed in monolithic models. Beyond simple modularity, we demonstrate that a Stratified Ensemble strategy significantly outperforms global pooling by preserving evidentiary diversity, effectively addressing the information bottleneck. Furthermore, our stress tests reveal critical scaling insights: (1) a \\textit{Context-Performance Paradox}, where extending reasoning contexts beyond 12k tokens introduces noise that degrades accuracy; and (2) the \\textit{Curse of Dimensionality} in action spaces, where expanding toolsets necessitates hierarchical retrieval strategies. Crucially, our approach underscores the potential of training-free architectural engineering, achieving state-of-the-art performance on CURE-Bench without the need for expensive end-to-end finetuning. This provides a scalable, agile foundation for the next generation of trustworthy AI-driven therapeutics. Code has been released on https://github.com/June01/CureAgent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current clinical agent built on small LLMs, such as TxAgent suffer from a \\textit{Context Utilization Failure}, where models successfully retrieve biomedical evidence due to supervised finetuning but fail to ground their diagnosis in that information. In this work, we propose the Executor-Analyst Framework, a modular architecture that decouples the syntactic precision of tool execution from the semantic robustness of clinical reasoning. By orchestrating specialized TxAgents (Executors) with long-context foundation models (Analysts), we mitigate the reasoning deficits observed in monolithic models. Beyond simple modularity, we demonstrate that a Stratified Ensemble strategy significantly outperforms global pooling by preserving evidentiary diversity, effectively addressing the information bottleneck. Furthermore, our stress tests reveal critical scaling insights: (1) a \\textit{Context-Performance Paradox}, where extending reasoning contexts beyond 12k tokens introduces noise that degrades accuracy; and (2) the \\textit{Curse of Dimensionality} in action spaces, where expanding toolsets necessitates hierarchical retrieval strategies. Crucially, our approach underscores the potential of training-free architectural engineering, achieving state-of-the-art performance on CURE-Bench without the need for expensive end-to-end finetuning. This provides a scalable, agile foundation for the next generation of trustworthy AI-driven therapeutics. Code has been released on https://github.com/June01/CureAgent."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T09:56:58Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    9,
                    56,
                    58,
                    4,
                    339,
                    0
                ],
                "arxiv_comment": "2nd Place Solution to the CURE-Bench Competition @ NeurIPS 2025. Code available at https://github.com/June01/CureAgent",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Ting-Ting Xie"
                    },
                    {
                        "name": "Yixin Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yixin Zhang"
                },
                "author": "Yixin Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2502.00791v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2502.00791v4",
                "title": "Vision-centric Token Compression in Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-centric Token Compression in Large Language Model"
                },
                "updated": "2025-12-05T09:48:41Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    9,
                    48,
                    41,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2502.00791v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2502.00791v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Real-world applications are stretching context windows to hundreds of thousand of tokens while Large Language Models (LLMs) swell from billions to trillions of parameters. This dual expansion send compute and memory costs skyrocketing, making token compression indispensable. We introduce Vision Centric Token Compression (Vist), a slow-fast compression framework that mirrors human reading: the fast path renders distant tokens into images, letting a frozen, lightweight vision encoder skim the low-salience context; the slow path feeds the proximal window into the LLM for fine-grained reasoning. A Probability-Informed Visual Enhancement (PVE) objective masks high-frequency tokens during training, steering the Resampler to concentrate on semantically rich regions-just as skilled reader gloss over function words. On eleven in-context learning benchmarks, Vist achieves the same accuracy with 2.3 times fewer tokens, cutting FLOPs by 16% and memory by 50%. This method delivers remarkable results, outperforming the strongest text encoder-based compression method CEPE by 7.6% on average over benchmarks like TriviaQA, NQ, PopQA, NLUI, and CLIN, setting a new standard for token efficiency in LLMs. The project is at https://github.com/CSU-JPG/VIST.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-world applications are stretching context windows to hundreds of thousand of tokens while Large Language Models (LLMs) swell from billions to trillions of parameters. This dual expansion send compute and memory costs skyrocketing, making token compression indispensable. We introduce Vision Centric Token Compression (Vist), a slow-fast compression framework that mirrors human reading: the fast path renders distant tokens into images, letting a frozen, lightweight vision encoder skim the low-salience context; the slow path feeds the proximal window into the LLM for fine-grained reasoning. A Probability-Informed Visual Enhancement (PVE) objective masks high-frequency tokens during training, steering the Resampler to concentrate on semantically rich regions-just as skilled reader gloss over function words. On eleven in-context learning benchmarks, Vist achieves the same accuracy with 2.3 times fewer tokens, cutting FLOPs by 16% and memory by 50%. This method delivers remarkable results, outperforming the strongest text encoder-based compression method CEPE by 7.6% on average over benchmarks like TriviaQA, NQ, PopQA, NLUI, and CLIN, setting a new standard for token efficiency in LLMs. The project is at https://github.com/CSU-JPG/VIST."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-02-02T13:10:06Z",
                "published_parsed": [
                    2025,
                    2,
                    2,
                    13,
                    10,
                    6,
                    6,
                    33,
                    0
                ],
                "arxiv_comment": "NeurIPS 2025 spotlight",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Ling Xing"
                    },
                    {
                        "name": "Alex Jinpeng Wang"
                    },
                    {
                        "name": "Rui Yan"
                    },
                    {
                        "name": "Xiangbo Shu"
                    },
                    {
                        "name": "Jinhui Tang"
                    }
                ],
                "author_detail": {
                    "name": "Jinhui Tang"
                },
                "author": "Jinhui Tang"
            },
            {
                "id": "http://arxiv.org/abs/2512.05559v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05559v1",
                "title": "A Unified AI System For Data Quality Control and DataOps Management in Regulated Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Unified AI System For Data Quality Control and DataOps Management in Regulated Environments"
                },
                "updated": "2025-12-05T09:33:43Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    9,
                    33,
                    43,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05559v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05559v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "In regulated domains such as finance, the integrity and governance of data pipelines are critical - yet existing systems treat data quality control (QC) as an isolated preprocessing step rather than a first-class system component. We present a unified AI-driven Data QC and DataOps Management framework that embeds rule-based, statistical, and AI-based QC methods into a continuous, governed layer spanning ingestion, model pipelines, and downstream applications. Our architecture integrates open-source tools with custom modules for profiling, audit logging, breach handling, configuration-driven policies, and dynamic remediation. We demonstrate deployment in a production-grade financial setup: handling streaming and tabular data across multiple asset classes and transaction streams, with configurable thresholds, cloud-native storage interfaces, and automated alerts. We show empirical gains in anomaly detection recall, reduction of manual remediation effort, and improved auditability and traceability in high-throughput data workflows. By treating QC as a system concern rather than an afterthought, our framework provides a foundation for trustworthy, scalable, and compliant AI pipelines in regulated environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In regulated domains such as finance, the integrity and governance of data pipelines are critical - yet existing systems treat data quality control (QC) as an isolated preprocessing step rather than a first-class system component. We present a unified AI-driven Data QC and DataOps Management framework that embeds rule-based, statistical, and AI-based QC methods into a continuous, governed layer spanning ingestion, model pipelines, and downstream applications. Our architecture integrates open-source tools with custom modules for profiling, audit logging, breach handling, configuration-driven policies, and dynamic remediation. We demonstrate deployment in a production-grade financial setup: handling streaming and tabular data across multiple asset classes and transaction streams, with configurable thresholds, cloud-native storage interfaces, and automated alerts. We show empirical gains in anomaly detection recall, reduction of manual remediation effort, and improved auditability and traceability in high-throughput data workflows. By treating QC as a system concern rather than an afterthought, our framework provides a foundation for trustworthy, scalable, and compliant AI pipelines in regulated environments."
                },
                "tags": [
                    {
                        "term": "q-fin.CP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T09:33:43Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    9,
                    33,
                    43,
                    4,
                    339,
                    0
                ],
                "arxiv_comment": "10 pages, 9 figures, 5 tables",
                "arxiv_primary_category": {
                    "term": "q-fin.CP"
                },
                "authors": [
                    {
                        "name": "Devender Saini"
                    },
                    {
                        "name": "Bhavika Jain"
                    },
                    {
                        "name": "Nitish Ujjwal"
                    },
                    {
                        "name": "Philip Sommer"
                    },
                    {
                        "name": "Dan Romuald Mbanga"
                    },
                    {
                        "name": "Dhagash Mehta"
                    }
                ],
                "author_detail": {
                    "name": "Dhagash Mehta"
                },
                "author": "Dhagash Mehta"
            },
            {
                "id": "http://arxiv.org/abs/2511.15718v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.15718v2",
                "title": "ToolMind Technical Report: A Large-Scale, Reasoning-Enhanced Tool-Use Dataset",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ToolMind Technical Report: A Large-Scale, Reasoning-Enhanced Tool-Use Dataset"
                },
                "updated": "2025-12-05T09:32:33Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    9,
                    32,
                    33,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.15718v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.15718v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Model (LLM) agents have developed rapidly in recent years to solve complex real-world problems using external tools. However, the scarcity of high-quality trajectories still hinders the development of stronger LLM agents. Most existing works on multi-turn dialogue synthesis validate correctness only at the trajectory level, which may overlook turn-level errors that can propagate during training and degrade model performance. To address these limitations, we introduce ToolMind, a large-scale, high-quality tool-agentic dataset with 160k synthetic data instances generated using over 20k tools and 200k augmented open-source data instances. Our data synthesis pipeline first constructs a function graph based on parameter correlations and then uses a multi-agent framework to simulate realistic user-assistant-tool interactions. Beyond trajectory-level validation, we employ fine-grained turn-level filtering to remove erroneous or suboptimal steps, ensuring that only high-quality reasoning traces are retained. This approach mitigates error amplification during training while preserving self-corrective reasoning signals essential for robust tool-use learning. Models fine-tuned on ToolMind show significant improvements over baselines on several benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) agents have developed rapidly in recent years to solve complex real-world problems using external tools. However, the scarcity of high-quality trajectories still hinders the development of stronger LLM agents. Most existing works on multi-turn dialogue synthesis validate correctness only at the trajectory level, which may overlook turn-level errors that can propagate during training and degrade model performance. To address these limitations, we introduce ToolMind, a large-scale, high-quality tool-agentic dataset with 160k synthetic data instances generated using over 20k tools and 200k augmented open-source data instances. Our data synthesis pipeline first constructs a function graph based on parameter correlations and then uses a multi-agent framework to simulate realistic user-assistant-tool interactions. Beyond trajectory-level validation, we employ fine-grained turn-level filtering to remove erroneous or suboptimal steps, ensuring that only high-quality reasoning traces are retained. This approach mitigates error amplification during training while preserving self-corrective reasoning signals essential for robust tool-use learning. Models fine-tuned on ToolMind show significant improvements over baselines on several benchmarks."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-12T13:01:23Z",
                "published_parsed": [
                    2025,
                    11,
                    12,
                    13,
                    1,
                    23,
                    2,
                    316,
                    0
                ],
                "arxiv_comment": "15 pages",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Chen Yang"
                    },
                    {
                        "name": "Ran Le"
                    },
                    {
                        "name": "Yun Xing"
                    },
                    {
                        "name": "Zhenwei An"
                    },
                    {
                        "name": "Zongchao Chen"
                    },
                    {
                        "name": "Wayne Xin Zhao"
                    },
                    {
                        "name": "Yang Song"
                    },
                    {
                        "name": "Tao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tao Zhang"
                },
                "author": "Tao Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2511.08916v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.08916v4",
                "title": "HalluClean: A Unified Framework to Combat Hallucinations in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HalluClean: A Unified Framework to Combat Hallucinations in LLMs"
                },
                "updated": "2025-12-05T09:31:19Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    9,
                    31,
                    19,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.08916v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.08916v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) have achieved impressive performance across a wide range of natural language processing tasks, yet they often produce hallucinated content that undermines factual reliability. To address this challenge, we introduce HalluClean, a lightweight and task-agnostic framework for detecting and correcting hallucinations in LLM-generated text. HalluClean adopts a reasoning-enhanced paradigm, explicitly decomposing the process into planning, execution, and revision stages to identify and refine unsupported claims. It employs minimal task-routing prompts to enable zero-shot generalization across diverse domains, without relying on external knowledge sources or supervised detectors. We conduct extensive evaluations on five representative tasks-question answering, dialogue, summarization, math word problems, and contradiction detection. Experimental results show that HalluClean significantly improves factual consistency and outperforms competitive baselines, demonstrating its potential to enhance the trustworthiness of LLM outputs in real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved impressive performance across a wide range of natural language processing tasks, yet they often produce hallucinated content that undermines factual reliability. To address this challenge, we introduce HalluClean, a lightweight and task-agnostic framework for detecting and correcting hallucinations in LLM-generated text. HalluClean adopts a reasoning-enhanced paradigm, explicitly decomposing the process into planning, execution, and revision stages to identify and refine unsupported claims. It employs minimal task-routing prompts to enable zero-shot generalization across diverse domains, without relying on external knowledge sources or supervised detectors. We conduct extensive evaluations on five representative tasks-question answering, dialogue, summarization, math word problems, and contradiction detection. Experimental results show that HalluClean significantly improves factual consistency and outperforms competitive baselines, demonstrating its potential to enhance the trustworthiness of LLM outputs in real-world applications."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-12T02:50:56Z",
                "published_parsed": [
                    2025,
                    11,
                    12,
                    2,
                    50,
                    56,
                    2,
                    316,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Yaxin Zhao"
                    },
                    {
                        "name": "Yu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Zhang"
                },
                "author": "Yu Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2512.05557v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05557v1",
                "title": "2K-Characters-10K-Stories: A Quality-Gated Stylized Narrative Dataset with Disentangled Control and Sequence Consistency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "2K-Characters-10K-Stories: A Quality-Gated Stylized Narrative Dataset with Disentangled Control and Sequence Consistency"
                },
                "updated": "2025-12-05T09:26:24Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    9,
                    26,
                    24,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05557v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05557v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Sequential identity consistency under precise transient attribute control remains a long-standing challenge in controllable visual storytelling. Existing datasets lack sufficient fidelity and fail to disentangle stable identities from transient attributes, limiting structured control over pose, expression, and scene composition and thus constraining reliable sequential synthesis. To address this gap, we introduce \\textbf{2K-Characters-10K-Stories}, a multi-modal stylized narrative dataset of \\textbf{2{,}000} uniquely stylized characters appearing across \\textbf{10{,}000} illustration stories. It is the first dataset that pairs large-scale unique identities with explicit, decoupled control signals for sequential identity consistency. We introduce a \\textbf{Human-in-the-Loop pipeline (HiL)} that leverages expert-verified character templates and LLM-guided narrative planning to generate highly-aligned structured data. A \\textbf{decoupled control} scheme separates persistent identity from transient attributes -- pose and expression -- while a \\textbf{Quality-Gated loop} integrating MMLM evaluation, Auto-Prompt Tuning, and Local Image Editing enforces pixel-level consistency. Extensive experiments demonstrate that models fine-tuned on our dataset achieves performance comparable to closed-source models in generating visual narratives.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequential identity consistency under precise transient attribute control remains a long-standing challenge in controllable visual storytelling. Existing datasets lack sufficient fidelity and fail to disentangle stable identities from transient attributes, limiting structured control over pose, expression, and scene composition and thus constraining reliable sequential synthesis. To address this gap, we introduce \\textbf{2K-Characters-10K-Stories}, a multi-modal stylized narrative dataset of \\textbf{2{,}000} uniquely stylized characters appearing across \\textbf{10{,}000} illustration stories. It is the first dataset that pairs large-scale unique identities with explicit, decoupled control signals for sequential identity consistency. We introduce a \\textbf{Human-in-the-Loop pipeline (HiL)} that leverages expert-verified character templates and LLM-guided narrative planning to generate highly-aligned structured data. A \\textbf{decoupled control} scheme separates persistent identity from transient attributes -- pose and expression -- while a \\textbf{Quality-Gated loop} integrating MMLM evaluation, Auto-Prompt Tuning, and Local Image Editing enforces pixel-level consistency. Extensive experiments demonstrate that models fine-tuned on our dataset achieves performance comparable to closed-source models in generating visual narratives."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T09:26:24Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    9,
                    26,
                    24,
                    4,
                    339,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Xingxi Yin"
                    },
                    {
                        "name": "Yicheng Li"
                    },
                    {
                        "name": "Gong Yan"
                    },
                    {
                        "name": "Chenglin Li"
                    },
                    {
                        "name": "Jian Zhao"
                    },
                    {
                        "name": "Cong Huang"
                    },
                    {
                        "name": "Yue Deng"
                    },
                    {
                        "name": "Yin Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yin Zhang"
                },
                "author": "Yin Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2510.21084v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.21084v2",
                "title": "Chinese Discharge Drug Recommendation in Metabolic Diseases with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chinese Discharge Drug Recommendation in Metabolic Diseases with Large Language Models"
                },
                "updated": "2025-12-05T09:12:05Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    9,
                    12,
                    5,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.21084v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.21084v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Intelligent drug recommendation based on Electronic Health Records (EHRs) is critical for improving the quality and efficiency of clinical decision-making. By leveraging large-scale patient data, drug recommendation systems can assist physicians in selecting the most appropriate medications according to a patient's medical history, diagnoses, laboratory results, and comorbidities. Recent advances in large language models (LLMs) have shown remarkable capabilities in complex reasoning and medical text understanding, making them promising tools for drug recommendation tasks. However, the application of LLMs for Chinese clinical medication recommendation remains largely unexplored. In this work, we conduct a systematic investigation of LLM-based methodologies for Chinese discharge medication recommendation. We evaluate several representative LLM families (GLM, Llama, Qwen) under a unified methodological framework including zero-shot prompting, in-context learning, chain-of-thought prompting, and supervised fine-tuning using LoRA. We analyze model behavior across reasoning styles, error patterns, domain adaptation mechanisms, and robustness. Experimental results show that while supervised fine-tuning improves model performance, there remains substantial room for improvement, with the best model achieving the F1 score of 0.5648 and Jaccard score of 0.4477. Our findings highlight both the potential and limitations of LLMs for Chinese drug recommendation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intelligent drug recommendation based on Electronic Health Records (EHRs) is critical for improving the quality and efficiency of clinical decision-making. By leveraging large-scale patient data, drug recommendation systems can assist physicians in selecting the most appropriate medications according to a patient's medical history, diagnoses, laboratory results, and comorbidities. Recent advances in large language models (LLMs) have shown remarkable capabilities in complex reasoning and medical text understanding, making them promising tools for drug recommendation tasks. However, the application of LLMs for Chinese clinical medication recommendation remains largely unexplored. In this work, we conduct a systematic investigation of LLM-based methodologies for Chinese discharge medication recommendation. We evaluate several representative LLM families (GLM, Llama, Qwen) under a unified methodological framework including zero-shot prompting, in-context learning, chain-of-thought prompting, and supervised fine-tuning using LoRA. We analyze model behavior across reasoning styles, error patterns, domain adaptation mechanisms, and robustness. Experimental results show that while supervised fine-tuning improves model performance, there remains substantial room for improvement, with the best model achieving the F1 score of 0.5648 and Jaccard score of 0.4477. Our findings highlight both the potential and limitations of LLMs for Chinese drug recommendation."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-24T01:47:23Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    1,
                    47,
                    23,
                    4,
                    297,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Juntao Li"
                    },
                    {
                        "name": "Haobin Yuan"
                    },
                    {
                        "name": "Ling Luo"
                    },
                    {
                        "name": "Yan Jiang"
                    },
                    {
                        "name": "Fan Wang"
                    },
                    {
                        "name": "Ping Zhang"
                    },
                    {
                        "name": "Huiyi Lv"
                    },
                    {
                        "name": "Jian Wang"
                    },
                    {
                        "name": "Yuanyuan Sun"
                    },
                    {
                        "name": "Hongfei Lin"
                    }
                ],
                "author_detail": {
                    "name": "Hongfei Lin"
                },
                "author": "Hongfei Lin"
            },
            {
                "id": "http://arxiv.org/abs/2512.05543v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05543v1",
                "title": "Are Bus-Mounted Edge Servers Feasible?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are Bus-Mounted Edge Servers Feasible?"
                },
                "updated": "2025-12-05T08:56:15Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    8,
                    56,
                    15,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05543v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05543v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Placement of edge servers is the prerequisite of provisioning edge computing services for Internet of Vehicles (IoV). Fixed-site edge servers at Road Side Units (RSUs) or base stations are able to offer basic service coverage for end users, i.e., vehicles on road. However, the server locations and capacity are fixed after deployment, rendering their inefficiency in handling spationtemporal user dynamics. Mobile servers such as buses, on the other hand, have the potential of adding computation elasticity to such system. To this end, this paper studies the feasibility of bus-mounted edge servers based on real traces. First, we investigate the coverage of the buses and base stations using the Shanghai bus/taxi/Telecom datasets, which shows a great potential of bus-based edge servers as they cover a great portion of geographic area and demand points. Next, we build a mathematical model and design a simple greedy heuristic algorithm to select a limited number of buses that maximizes the coverage of demand points, i.e., with a limited purchase budget. We perform trace-driven simulations to verify the performance of the proposed bus selection algorithm. The results show that our approach effectively handles the dynamic user demand under realistic constraints such as server capacity and purchase quantity. Thus, we claim: bus-mounted edge servers for vehicular networks in urban areas are feasible, beneficial, and valuable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Placement of edge servers is the prerequisite of provisioning edge computing services for Internet of Vehicles (IoV). Fixed-site edge servers at Road Side Units (RSUs) or base stations are able to offer basic service coverage for end users, i.e., vehicles on road. However, the server locations and capacity are fixed after deployment, rendering their inefficiency in handling spationtemporal user dynamics. Mobile servers such as buses, on the other hand, have the potential of adding computation elasticity to such system. To this end, this paper studies the feasibility of bus-mounted edge servers based on real traces. First, we investigate the coverage of the buses and base stations using the Shanghai bus/taxi/Telecom datasets, which shows a great potential of bus-based edge servers as they cover a great portion of geographic area and demand points. Next, we build a mathematical model and design a simple greedy heuristic algorithm to select a limited number of buses that maximizes the coverage of demand points, i.e., with a limited purchase budget. We perform trace-driven simulations to verify the performance of the proposed bus selection algorithm. The results show that our approach effectively handles the dynamic user demand under realistic constraints such as server capacity and purchase quantity. Thus, we claim: bus-mounted edge servers for vehicular networks in urban areas are feasible, beneficial, and valuable."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T08:56:15Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    8,
                    56,
                    15,
                    4,
                    339,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Xuezhi Li"
                    },
                    {
                        "name": "Jiancong He"
                    },
                    {
                        "name": "Ming Xie"
                    },
                    {
                        "name": "Xuyang Chen"
                    },
                    {
                        "name": "Le Chang"
                    },
                    {
                        "name": "Li Jiang"
                    },
                    {
                        "name": "Gui Gui"
                    }
                ],
                "author_detail": {
                    "name": "Gui Gui"
                },
                "author": "Gui Gui"
            },
            {
                "id": "http://arxiv.org/abs/2512.05542v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05542v1",
                "title": "RoBoN: Routed Online Best-of-n for Test-Time Scaling with Multiple LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RoBoN: Routed Online Best-of-n for Test-Time Scaling with Multiple LLMs"
                },
                "updated": "2025-12-05T08:55:39Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    8,
                    55,
                    39,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05542v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05542v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Best-of-$n$ is a widely used test-time scaling approach for LLM inference. Yet despite evidence that LLMs exhibit complementary strengths across tasks, traditionally best-of-$n$ relies on a single model to generate responses. We propose RoBoN (Routed Online Best-of-$n$), a sequential multi-LLM alternative to the prevailing single-model best-of-$n$. Given a suite of models $\\{m_i\\}_{i=1}^M$, RoBoN sequentially routes generations one-by-one across models, based on scores computed using a reward model and an agreement signal on the predicted responses. This online routing requires no additional training, keeps compute parity, and works with any plug-in reward model. Across reasoning benchmarks (MATH500, OlympiadBench, MinervaMath, GSM8K, MMLU), RoBoN consistently outperforms standard best-of-$n$ applied to each individual model for larger $n$, with gains of up to 3.4\\% in absolute accuracy, and also improves over a uniform multi-model portfolio baseline. Our results indicate that diversity across models can be exploited at inference to improve best-of-$n$ performance over any constituent model alone, providing a simple, training-free path to test-time scaling with multiple LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Best-of-$n$ is a widely used test-time scaling approach for LLM inference. Yet despite evidence that LLMs exhibit complementary strengths across tasks, traditionally best-of-$n$ relies on a single model to generate responses. We propose RoBoN (Routed Online Best-of-$n$), a sequential multi-LLM alternative to the prevailing single-model best-of-$n$. Given a suite of models $\\{m_i\\}_{i=1}^M$, RoBoN sequentially routes generations one-by-one across models, based on scores computed using a reward model and an agreement signal on the predicted responses. This online routing requires no additional training, keeps compute parity, and works with any plug-in reward model. Across reasoning benchmarks (MATH500, OlympiadBench, MinervaMath, GSM8K, MMLU), RoBoN consistently outperforms standard best-of-$n$ applied to each individual model for larger $n$, with gains of up to 3.4\\% in absolute accuracy, and also improves over a uniform multi-model portfolio baseline. Our results indicate that diversity across models can be exploited at inference to improve best-of-$n$ performance over any constituent model alone, providing a simple, training-free path to test-time scaling with multiple LLMs."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T08:55:39Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    8,
                    55,
                    39,
                    4,
                    339,
                    0
                ],
                "arxiv_comment": "20 pages, 3 figures. 39th Conference on Neural Information Processing Systems (NeurIPS 2025) Workshop: Foundations of Reasoning in Language Models",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Jonathan Geuter"
                    },
                    {
                        "name": "Gregor Kornhardt"
                    }
                ],
                "author_detail": {
                    "name": "Gregor Kornhardt"
                },
                "author": "Gregor Kornhardt"
            },
            {
                "id": "http://arxiv.org/abs/2512.05537v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05537v1",
                "title": "Automated Identification of Incidentalomas Requiring Follow-Up: A Multi-Anatomy Evaluation of LLM-Based and Supervised Approaches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Identification of Incidentalomas Requiring Follow-Up: A Multi-Anatomy Evaluation of LLM-Based and Supervised Approaches"
                },
                "updated": "2025-12-05T08:49:57Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    8,
                    49,
                    57,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05537v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05537v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Objective: To evaluate large language models (LLMs) against supervised baselines for fine-grained, lesion-level detection of incidentalomas requiring follow-up, addressing the limitations of current document-level classification systems.\n  Methods: We utilized a dataset of 400 annotated radiology reports containing 1,623 verified lesion findings. We compared three supervised transformer-based encoders (BioClinicalModernBERT, ModernBERT, Clinical Longformer) against four generative LLM configurations (Llama 3.1-8B, GPT-4o, GPT-OSS-20b). We introduced a novel inference strategy using lesion-tagged inputs and anatomy-aware prompting to ground model reasoning. Performance was evaluated using class-specific F1-scores.\n  Results: The anatomy-informed GPT-OSS-20b model achieved the highest performance, yielding an incidentaloma-positive macro-F1 of 0.79. This surpassed all supervised baselines (maximum macro-F1: 0.70) and closely matched the inter-annotator agreement of 0.76. Explicit anatomical grounding yielded statistically significant performance gains across GPT-based models (p < 0.05), while a majority-vote ensemble of the top systems further improved the macro-F1 to 0.90. Error analysis revealed that anatomy-aware LLMs demonstrated superior contextual reasoning in distinguishing actionable findings from benign lesions.\n  Conclusion: Generative LLMs, when enhanced with structured lesion tagging and anatomical context, significantly outperform traditional supervised encoders and achieve performance comparable to human experts. This approach offers a reliable, interpretable pathway for automated incidental finding surveillance in radiology workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Objective: To evaluate large language models (LLMs) against supervised baselines for fine-grained, lesion-level detection of incidentalomas requiring follow-up, addressing the limitations of current document-level classification systems.\n  Methods: We utilized a dataset of 400 annotated radiology reports containing 1,623 verified lesion findings. We compared three supervised transformer-based encoders (BioClinicalModernBERT, ModernBERT, Clinical Longformer) against four generative LLM configurations (Llama 3.1-8B, GPT-4o, GPT-OSS-20b). We introduced a novel inference strategy using lesion-tagged inputs and anatomy-aware prompting to ground model reasoning. Performance was evaluated using class-specific F1-scores.\n  Results: The anatomy-informed GPT-OSS-20b model achieved the highest performance, yielding an incidentaloma-positive macro-F1 of 0.79. This surpassed all supervised baselines (maximum macro-F1: 0.70) and closely matched the inter-annotator agreement of 0.76. Explicit anatomical grounding yielded statistically significant performance gains across GPT-based models (p < 0.05), while a majority-vote ensemble of the top systems further improved the macro-F1 to 0.90. Error analysis revealed that anatomy-aware LLMs demonstrated superior contextual reasoning in distinguishing actionable findings from benign lesions.\n  Conclusion: Generative LLMs, when enhanced with structured lesion tagging and anatomical context, significantly outperform traditional supervised encoders and achieve performance comparable to human experts. This approach offers a reliable, interpretable pathway for automated incidental finding surveillance in radiology workflows."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T08:49:57Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    8,
                    49,
                    57,
                    4,
                    339,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Namu Park"
                    },
                    {
                        "name": "Farzad Ahmed"
                    },
                    {
                        "name": "Zhaoyi Sun"
                    },
                    {
                        "name": "Kevin Lybarger"
                    },
                    {
                        "name": "Ethan Breinhorst"
                    },
                    {
                        "name": "Julie Hu"
                    },
                    {
                        "name": "Ozlem Uzuner"
                    },
                    {
                        "name": "Martin Gunn"
                    },
                    {
                        "name": "Meliha Yetisgen"
                    }
                ],
                "author_detail": {
                    "name": "Meliha Yetisgen"
                },
                "author": "Meliha Yetisgen"
            },
            {
                "id": "http://arxiv.org/abs/2512.05536v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05536v1",
                "title": "Eye of the Beholder: Towards Measuring Visualization Complexity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eye of the Beholder: Towards Measuring Visualization Complexity"
                },
                "updated": "2025-12-05T08:49:50Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    8,
                    49,
                    50,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05536v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05536v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Constructing expressive and legible visualizations is a key activity for visualization designers. While numerous design guidelines exist, research on how specific graphical features affect perceived visual complexity remains limited. In this paper, we report on a crowdsourced study to collect human ratings of perceived complexity for diverse visualizations. Using these ratings as ground truth, we then evaluated three methods to estimate this perceived complexity: image analysis metrics, multilinear regression using manually coded visualization features, and automated feature extraction using a large language model (LLM). Image complexity metrics showed no correlation with human-perceived visualization complexity. Manual feature coding produced a reasonable predictive model but required substantial effort. In contrast, a zero-shot LLM (GPT-4o mini) demonstrated strong capabilities in both rating complexity and extracting relevant features. Our findings suggest that visualization complexity is truly in the eye of the beholder, yet can be effectively approximated using zero-shot LLM prompting, offering a scalable approach for evaluating the complexity of visualizations. The dataset and code for the study and data analysis can be found at https://osf.io/w85a4/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constructing expressive and legible visualizations is a key activity for visualization designers. While numerous design guidelines exist, research on how specific graphical features affect perceived visual complexity remains limited. In this paper, we report on a crowdsourced study to collect human ratings of perceived complexity for diverse visualizations. Using these ratings as ground truth, we then evaluated three methods to estimate this perceived complexity: image analysis metrics, multilinear regression using manually coded visualization features, and automated feature extraction using a large language model (LLM). Image complexity metrics showed no correlation with human-perceived visualization complexity. Manual feature coding produced a reasonable predictive model but required substantial effort. In contrast, a zero-shot LLM (GPT-4o mini) demonstrated strong capabilities in both rating complexity and extracting relevant features. Our findings suggest that visualization complexity is truly in the eye of the beholder, yet can be effectively approximated using zero-shot LLM prompting, offering a scalable approach for evaluating the complexity of visualizations. The dataset and code for the study and data analysis can be found at https://osf.io/w85a4/"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T08:49:50Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    8,
                    49,
                    50,
                    4,
                    339,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC"
                },
                "authors": [
                    {
                        "name": "Johannes Ellemose"
                    },
                    {
                        "name": "Niklas Elmqvist"
                    }
                ],
                "author_detail": {
                    "name": "Niklas Elmqvist"
                },
                "author": "Niklas Elmqvist"
            },
            {
                "id": "http://arxiv.org/abs/2512.05534v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05534v1",
                "title": "On the Theoretical Foundation of Sparse Dictionary Learning in Mechanistic Interpretability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Theoretical Foundation of Sparse Dictionary Learning in Mechanistic Interpretability"
                },
                "updated": "2025-12-05T08:47:19Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    8,
                    47,
                    19,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05534v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05534v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "As AI models achieve remarkable capabilities across diverse domains, understanding what representations they learn and how they process information has become increasingly important for both scientific progress and trustworthy deployment. Recent works in mechanistic interpretability have shown that neural networks represent meaningful concepts as directions in their representation spaces and often encode many concepts in superposition. Various sparse dictionary learning (SDL) methods, including sparse autoencoders, transcoders, and crosscoders, address this by training auxiliary models with sparsity constraints to disentangle these superposed concepts into interpretable features. These methods have demonstrated remarkable empirical success but have limited theoretical understanding. Existing theoretical work is limited to sparse autoencoders with tied-weight constraints, leaving the broader family of SDL methods without formal grounding. In this work, we develop the first unified theoretical framework considering SDL as one unified optimization problem. We demonstrate how diverse methods instantiate the theoretical framwork and provide rigorous analysis on the optimization landscape. We provide the first theoretical explanations for some empirically observed phenomena, including feature absorption, dead neurons, and the neuron resampling technique. We further design controlled experiments to validate our theoretical results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As AI models achieve remarkable capabilities across diverse domains, understanding what representations they learn and how they process information has become increasingly important for both scientific progress and trustworthy deployment. Recent works in mechanistic interpretability have shown that neural networks represent meaningful concepts as directions in their representation spaces and often encode many concepts in superposition. Various sparse dictionary learning (SDL) methods, including sparse autoencoders, transcoders, and crosscoders, address this by training auxiliary models with sparsity constraints to disentangle these superposed concepts into interpretable features. These methods have demonstrated remarkable empirical success but have limited theoretical understanding. Existing theoretical work is limited to sparse autoencoders with tied-weight constraints, leaving the broader family of SDL methods without formal grounding. In this work, we develop the first unified theoretical framework considering SDL as one unified optimization problem. We demonstrate how diverse methods instantiate the theoretical framwork and provide rigorous analysis on the optimization landscape. We provide the first theoretical explanations for some empirically observed phenomena, including feature absorption, dead neurons, and the neuron resampling technique. We further design controlled experiments to validate our theoretical results."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T08:47:19Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    8,
                    47,
                    19,
                    4,
                    339,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Yiming Tang"
                    },
                    {
                        "name": "Harshvardhan Saini"
                    },
                    {
                        "name": "Yizhen Liao"
                    },
                    {
                        "name": "Dianbo Liu"
                    }
                ],
                "author_detail": {
                    "name": "Dianbo Liu"
                },
                "author": "Dianbo Liu"
            },
            {
                "id": "http://arxiv.org/abs/2502.03916v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2502.03916v2",
                "title": "Experiments with Large Language Models on Retrieval-Augmented Generation for Closed-Source Simulation Software",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Experiments with Large Language Models on Retrieval-Augmented Generation for Closed-Source Simulation Software"
                },
                "updated": "2025-12-05T08:43:22Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    8,
                    43,
                    22,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2502.03916v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2502.03916v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) are tools that have become indispensable in development and programming. However, they suffer from hallucinations, especially when dealing with unknown knowledge. This is particularly the case when LLMs are to be used to support closed-source software applications. Retrieval-Augmented Generation (RAG) offers an approach to use additional knowledge alongside the pre-trained knowledge of the LLM to respond to user prompts. Possible tasks range from a smart-autocomplete, text extraction for question answering, model summarization, component explaining, compositional reasoning, to creation of simulation components and complete input models. This work tests existing RAG systems for closed-source simulation frameworks, in our case the mesh-free simulation software Pasimodo. Since data protection and intellectual property rights are particularly important for problems solved with closed-source software, the tests focus on execution using local LLMs. In order to enable smaller institutions to use the systems, smaller language models will be tested first. The systems show impressive results, but often fail due to insufficient information. Different approaches for improving response quality are tested. In particular, tailoring the information provided to the LLMs dependent to the prompts proves to be a significant improvement. This demonstrates the great potential and the further work needed to improve information retrieval for closed-source simulation models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are tools that have become indispensable in development and programming. However, they suffer from hallucinations, especially when dealing with unknown knowledge. This is particularly the case when LLMs are to be used to support closed-source software applications. Retrieval-Augmented Generation (RAG) offers an approach to use additional knowledge alongside the pre-trained knowledge of the LLM to respond to user prompts. Possible tasks range from a smart-autocomplete, text extraction for question answering, model summarization, component explaining, compositional reasoning, to creation of simulation components and complete input models. This work tests existing RAG systems for closed-source simulation frameworks, in our case the mesh-free simulation software Pasimodo. Since data protection and intellectual property rights are particularly important for problems solved with closed-source software, the tests focus on execution using local LLMs. In order to enable smaller institutions to use the systems, smaller language models will be tested first. The systems show impressive results, but often fail due to insufficient information. Different approaches for improving response quality are tested. In particular, tailoring the information provided to the LLMs dependent to the prompts proves to be a significant improvement. This demonstrates the great potential and the further work needed to improve information retrieval for closed-source simulation models."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-02-06T09:48:04Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    9,
                    48,
                    4,
                    3,
                    37,
                    0
                ],
                "arxiv_comment": "16 pages, 6 tables, 2 figures",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Andreas Baumann"
                    },
                    {
                        "name": "Peter Eberhard"
                    }
                ],
                "author_detail": {
                    "name": "Peter Eberhard"
                },
                "author": "Peter Eberhard"
            },
            {
                "id": "http://arxiv.org/abs/2512.05525v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05525v1",
                "title": "Poodle: Seamlessly Scaling Down Large Language Models with Just-in-Time Model Replacement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Poodle: Seamlessly Scaling Down Large Language Models with Just-in-Time Model Replacement"
                },
                "updated": "2025-12-05T08:36:39Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    8,
                    36,
                    39,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05525v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05525v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Businesses increasingly rely on large language models (LLMs) to automate simple repetitive tasks instead of developing custom machine learning models. LLMs require few, if any, training examples and can be utilized by users without expertise in model development. However, this comes at the cost of substantially higher resource and energy consumption compared to smaller models, which often achieve similar predictive performance for simple tasks. In this paper, we present our vision for just-in-time model replacement (JITR), where, upon identifying a recurring task in calls to an LLM, the model is replaced transparently with a cheaper alternative that performs well for this specific task. JITR retains the ease of use and low development effort of LLMs, while saving significant cost and energy. We discuss the main challenges in realizing our vision regarding the identification of recurring tasks and the creation of a custom model. Specifically, we argue that model search and transfer learning will play a crucial role in JITR to efficiently identify and fine-tune models for a recurring task. Using our JITR prototype Poodle, we achieve significant savings for exemplary tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Businesses increasingly rely on large language models (LLMs) to automate simple repetitive tasks instead of developing custom machine learning models. LLMs require few, if any, training examples and can be utilized by users without expertise in model development. However, this comes at the cost of substantially higher resource and energy consumption compared to smaller models, which often achieve similar predictive performance for simple tasks. In this paper, we present our vision for just-in-time model replacement (JITR), where, upon identifying a recurring task in calls to an LLM, the model is replaced transparently with a cheaper alternative that performs well for this specific task. JITR retains the ease of use and low development effort of LLMs, while saving significant cost and energy. We discuss the main challenges in realizing our vision regarding the identification of recurring tasks and the creation of a custom model. Specifically, we argue that model search and transfer learning will play a crucial role in JITR to efficiently identify and fine-tune models for a recurring task. Using our JITR prototype Poodle, we achieve significant savings for exemplary tasks."
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T08:36:39Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    8,
                    36,
                    39,
                    4,
                    339,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB"
                },
                "authors": [
                    {
                        "name": "Nils Strassenburg"
                    },
                    {
                        "name": "Boris Glavic"
                    },
                    {
                        "name": "Tilmann Rabl"
                    }
                ],
                "author_detail": {
                    "name": "Tilmann Rabl"
                },
                "author": "Tilmann Rabl"
            },
            {
                "id": "http://arxiv.org/abs/2512.05518v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05518v1",
                "title": "Matching Ranks Over Probability Yields Truly Deep Safety Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Matching Ranks Over Probability Yields Truly Deep Safety Alignment"
                },
                "updated": "2025-12-05T08:22:27Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    8,
                    22,
                    27,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05518v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05518v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "A frustratingly easy technique known as the prefilling attack has been shown to effectively circumvent the safety alignment of frontier LLMs by simply prefilling the assistant response with an affirmative prefix before decoding. In response, recent work proposed a supervised fine-tuning (SFT) defense using data augmentation to achieve a \\enquote{deep} safety alignment, allowing the model to generate natural language refusals immediately following harmful prefills. Unfortunately, we show in this work that the \"deep\" safety alignment produced by such an approach is in fact not very deep. A generalization of the prefilling attack, which we refer to as the Rank-Assisted Prefilling (RAP) attack, can effectively extract harmful content from models fine-tuned with the data augmentation defense by selecting low-probability \"harmful\" tokens from the top 20 predicted next tokens at each step (thus ignoring high-probability \"refusal\" tokens). We argue that this vulnerability is enabled due to the \"gaming\" of the SFT objective when the target distribution entropies are low, where low fine-tuning loss is achieved by shifting large probability mass to a small number of refusal tokens while neglecting the high ranks of harmful tokens. We then propose a new perspective on achieving deep safety alignment by matching the token ranks of the target distribution, rather than their probabilities. This perspective yields a surprisingly simple fix to the data augmentation defense based on regularizing the attention placed on harmful prefill tokens, an approach we call PRefill attEntion STOpping (PRESTO). Adding PRESTO yields up to a 4.7x improvement in the mean StrongREJECT score under RAP attacks across three popular open-source LLMs, with low impact to model utility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A frustratingly easy technique known as the prefilling attack has been shown to effectively circumvent the safety alignment of frontier LLMs by simply prefilling the assistant response with an affirmative prefix before decoding. In response, recent work proposed a supervised fine-tuning (SFT) defense using data augmentation to achieve a \\enquote{deep} safety alignment, allowing the model to generate natural language refusals immediately following harmful prefills. Unfortunately, we show in this work that the \"deep\" safety alignment produced by such an approach is in fact not very deep. A generalization of the prefilling attack, which we refer to as the Rank-Assisted Prefilling (RAP) attack, can effectively extract harmful content from models fine-tuned with the data augmentation defense by selecting low-probability \"harmful\" tokens from the top 20 predicted next tokens at each step (thus ignoring high-probability \"refusal\" tokens). We argue that this vulnerability is enabled due to the \"gaming\" of the SFT objective when the target distribution entropies are low, where low fine-tuning loss is achieved by shifting large probability mass to a small number of refusal tokens while neglecting the high ranks of harmful tokens. We then propose a new perspective on achieving deep safety alignment by matching the token ranks of the target distribution, rather than their probabilities. This perspective yields a surprisingly simple fix to the data augmentation defense based on regularizing the attention placed on harmful prefill tokens, an approach we call PRefill attEntion STOpping (PRESTO). Adding PRESTO yields up to a 4.7x improvement in the mean StrongREJECT score under RAP attacks across three popular open-source LLMs, with low impact to model utility."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T08:22:27Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    8,
                    22,
                    27,
                    4,
                    339,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Jason Vega"
                    },
                    {
                        "name": "Gagandeep Singh"
                    }
                ],
                "author_detail": {
                    "name": "Gagandeep Singh"
                },
                "author": "Gagandeep Singh"
            },
            {
                "id": "http://arxiv.org/abs/2512.05508v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05508v1",
                "title": "Lyrics Matter: Exploiting the Power of Learnt Representations for Music Popularity Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lyrics Matter: Exploiting the Power of Learnt Representations for Music Popularity Prediction"
                },
                "updated": "2025-12-05T08:09:26Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    8,
                    9,
                    26,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05508v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05508v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Accurately predicting music popularity is a critical challenge in the music industry, offering benefits to artists, producers, and streaming platforms. Prior research has largely focused on audio features, social metadata, or model architectures. This work addresses the under-explored role of lyrics in predicting popularity. We present an automated pipeline that uses LLM to extract high-dimensional lyric embeddings, capturing semantic, syntactic, and sequential information. These features are integrated into HitMusicLyricNet, a multimodal architecture that combines audio, lyrics, and social metadata for popularity score prediction in the range 0-100. Our method outperforms existing baselines on the SpotGenTrack dataset, which contains over 100,000 tracks, achieving 9% and 20% improvements in MAE and MSE, respectively. Ablation confirms that gains arise from our LLM-driven lyrics feature pipeline (LyricsAENet), underscoring the value of dense lyric representations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurately predicting music popularity is a critical challenge in the music industry, offering benefits to artists, producers, and streaming platforms. Prior research has largely focused on audio features, social metadata, or model architectures. This work addresses the under-explored role of lyrics in predicting popularity. We present an automated pipeline that uses LLM to extract high-dimensional lyric embeddings, capturing semantic, syntactic, and sequential information. These features are integrated into HitMusicLyricNet, a multimodal architecture that combines audio, lyrics, and social metadata for popularity score prediction in the range 0-100. Our method outperforms existing baselines on the SpotGenTrack dataset, which contains over 100,000 tracks, achieving 9% and 20% improvements in MAE and MSE, respectively. Ablation confirms that gains arise from our LLM-driven lyrics feature pipeline (LyricsAENet), underscoring the value of dense lyric representations."
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T08:09:26Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    8,
                    9,
                    26,
                    4,
                    339,
                    0
                ],
                "arxiv_comment": "8 pages",
                "arxiv_primary_category": {
                    "term": "cs.SD"
                },
                "authors": [
                    {
                        "name": "Yash Choudhary"
                    },
                    {
                        "name": "Preeti Rao"
                    },
                    {
                        "name": "Pushpak Bhattacharyya"
                    }
                ],
                "author_detail": {
                    "name": "Pushpak Bhattacharyya"
                },
                "author": "Pushpak Bhattacharyya"
            },
            {
                "id": "http://arxiv.org/abs/2512.05506v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05506v1",
                "title": "When Scaffolding Breaks: Investigating Student Interaction with LLM-Based Writing Support in Real-Time K-12 EFL Classrooms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Scaffolding Breaks: Investigating Student Interaction with LLM-Based Writing Support in Real-Time K-12 EFL Classrooms"
                },
                "updated": "2025-12-05T08:09:05Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    8,
                    9,
                    5,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05506v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05506v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) are promising tools for scaffolding students' English writing skills, but their effectiveness in real-time K-12 classrooms remains underexplored. Addressing this gap, our study examines the benefits and limitations of using LLMs as real-time learning support, considering how classroom constraints, such as diverse proficiency levels and limited time, affect their effectiveness. We conducted a deployment study with 157 eighth-grade students in a South Korean middle school English class over six weeks. Our findings reveal that while scaffolding improved students' ability to compose grammatically correct sentences, this step-by-step approach demotivated lower-proficiency students and increased their system reliance. We also observed challenges to classroom dynamics, where extroverted students often dominated the teacher's attention, and the system's assistance made it difficult for teachers to identify struggling students. Based on these findings, we discuss design guidelines for integrating LLMs into real-time writing classes as inclusive educational tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are promising tools for scaffolding students' English writing skills, but their effectiveness in real-time K-12 classrooms remains underexplored. Addressing this gap, our study examines the benefits and limitations of using LLMs as real-time learning support, considering how classroom constraints, such as diverse proficiency levels and limited time, affect their effectiveness. We conducted a deployment study with 157 eighth-grade students in a South Korean middle school English class over six weeks. Our findings reveal that while scaffolding improved students' ability to compose grammatically correct sentences, this step-by-step approach demotivated lower-proficiency students and increased their system reliance. We also observed challenges to classroom dynamics, where extroverted students often dominated the teacher's attention, and the system's assistance made it difficult for teachers to identify struggling students. Based on these findings, we discuss design guidelines for integrating LLMs into real-time writing classes as inclusive educational tools."
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T08:09:05Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    8,
                    9,
                    5,
                    4,
                    339,
                    0
                ],
                "arxiv_comment": "Under Review",
                "arxiv_primary_category": {
                    "term": "cs.HC"
                },
                "authors": [
                    {
                        "name": "Junho Myung"
                    },
                    {
                        "name": "Hyunseung Lim"
                    },
                    {
                        "name": "Hana Oh"
                    },
                    {
                        "name": "Hyoungwook Jin"
                    },
                    {
                        "name": "Nayeon Kang"
                    },
                    {
                        "name": "So-Yeon Ahn"
                    },
                    {
                        "name": "Hwajung Hong"
                    },
                    {
                        "name": "Alice Oh"
                    },
                    {
                        "name": "Juho Kim"
                    }
                ],
                "author_detail": {
                    "name": "Juho Kim"
                },
                "author": "Juho Kim"
            },
            {
                "id": "http://arxiv.org/abs/2510.05497v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.05497v3",
                "title": "Orders in Chaos: Enhancing Large-Scale MoE LLM Serving with Data Movement Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Orders in Chaos: Enhancing Large-Scale MoE LLM Serving with Data Movement Forecasting"
                },
                "updated": "2025-12-05T07:59:52Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    7,
                    59,
                    52,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.05497v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.05497v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large-scale Mixture of Experts (MoE) Large Language Models (LLMs) have recently become the frontier open weight models, achieving remarkable model capability similar to proprietary ones. But their random expert selection mechanism introduces significant data movement overhead that becomes the dominant bottleneck in multi-unit LLM serving systems.\n  To understand the patterns underlying this data movement, we conduct comprehensive data-movement-centric profiling across four state-of-the-art large-scale MoE models released in 2025 (200B-1000B) using over 24,000 requests spanning diverse workloads. We perform systematic analysis from both temporal and spatial perspectives and distill six key insights to guide the design of diverse future serving systems. With our insights, we then demonstrate how to improve wafer-scale GPUs as a case study, and show that minor architectural modifications leveraging the insights achieve substantial performance gains, delivering 5.3x and 3.1x average speedups on DeepSeek V3 and Qwen3, respectively. Our work presents the first comprehensive data-centric analysis of large-scale MoE models and a concrete design study using the learned lessons, with profiling traces and simulation framework already open-sourced with $>$1k downloads. Our traces and results are publicly available at https://huggingface.co/datasets/core12345/MoE_expert_selection_trace",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale Mixture of Experts (MoE) Large Language Models (LLMs) have recently become the frontier open weight models, achieving remarkable model capability similar to proprietary ones. But their random expert selection mechanism introduces significant data movement overhead that becomes the dominant bottleneck in multi-unit LLM serving systems.\n  To understand the patterns underlying this data movement, we conduct comprehensive data-movement-centric profiling across four state-of-the-art large-scale MoE models released in 2025 (200B-1000B) using over 24,000 requests spanning diverse workloads. We perform systematic analysis from both temporal and spatial perspectives and distill six key insights to guide the design of diverse future serving systems. With our insights, we then demonstrate how to improve wafer-scale GPUs as a case study, and show that minor architectural modifications leveraging the insights achieve substantial performance gains, delivering 5.3x and 3.1x average speedups on DeepSeek V3 and Qwen3, respectively. Our work presents the first comprehensive data-centric analysis of large-scale MoE models and a concrete design study using the learned lessons, with profiling traces and simulation framework already open-sourced with $>$1k downloads. Our traces and results are publicly available at https://huggingface.co/datasets/core12345/MoE_expert_selection_trace"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-07T01:31:39Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    1,
                    31,
                    39,
                    1,
                    280,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Zhongkai Yu"
                    },
                    {
                        "name": "Yue Guan"
                    },
                    {
                        "name": "Zihao Yu"
                    },
                    {
                        "name": "Chenyang Zhou"
                    },
                    {
                        "name": "Zhengding Hu"
                    },
                    {
                        "name": "Shuyi Pei"
                    },
                    {
                        "name": "Yangwook Kang"
                    },
                    {
                        "name": "Yufei Ding"
                    },
                    {
                        "name": "Po-An Tsai"
                    }
                ],
                "author_detail": {
                    "name": "Po-An Tsai"
                },
                "author": "Po-An Tsai"
            },
            {
                "id": "http://arxiv.org/abs/2512.05502v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05502v1",
                "title": "GRASP: Graph Reasoning Agents for Systems Pharmacology with Human-in-the-Loop",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GRASP: Graph Reasoning Agents for Systems Pharmacology with Human-in-the-Loop"
                },
                "updated": "2025-12-05T07:59:16Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    7,
                    59,
                    16,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05502v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05502v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Quantitative Systems Pharmacology (QSP) modeling is essential for drug development but it requires significant time investment that limits the throughput of domain experts. We present \\textbf{GRASP} -- a multi-agent, graph-reasoning framework with a human-in-the-loop conversational interface -- that encodes QSP models as typed biological knowledge graphs and compiles them to executable MATLAB/SimBiology code while preserving units, mass balance, and physiological constraints. A two-phase workflow -- \\textsc{Understanding} (graph reconstruction of legacy code) and \\textsc{Action} (constraint-checked, language-driven modification) -- is orchestrated by a state machine with iterative validation. GRASP performs breadth-first parameter-alignment around new entities to surface dependent quantities and propose biologically plausible defaults, and it runs automatic execution/diagnostics until convergence. In head-to-head evaluations using LLM-as-judge, GRASP outperforms SME-guided CoT and ToT baselines across biological plausibility, mathematical correctness, structural fidelity, and code quality (\\(\\approx\\)9--10/10 vs.\\ 5--7/10). BFS alignment achieves F1 = 0.95 for dependency discovery, units, and range. These results demonstrate that graph-structured, agentic workflows can make QSP model development both accessible and rigorous, enabling domain experts to specify mechanisms in natural language without sacrificing biomedical fidelity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantitative Systems Pharmacology (QSP) modeling is essential for drug development but it requires significant time investment that limits the throughput of domain experts. We present \\textbf{GRASP} -- a multi-agent, graph-reasoning framework with a human-in-the-loop conversational interface -- that encodes QSP models as typed biological knowledge graphs and compiles them to executable MATLAB/SimBiology code while preserving units, mass balance, and physiological constraints. A two-phase workflow -- \\textsc{Understanding} (graph reconstruction of legacy code) and \\textsc{Action} (constraint-checked, language-driven modification) -- is orchestrated by a state machine with iterative validation. GRASP performs breadth-first parameter-alignment around new entities to surface dependent quantities and propose biologically plausible defaults, and it runs automatic execution/diagnostics until convergence. In head-to-head evaluations using LLM-as-judge, GRASP outperforms SME-guided CoT and ToT baselines across biological plausibility, mathematical correctness, structural fidelity, and code quality (\\(\\approx\\)9--10/10 vs.\\ 5--7/10). BFS alignment achieves F1 = 0.95 for dependency discovery, units, and range. These results demonstrate that graph-structured, agentic workflows can make QSP model development both accessible and rigorous, enabling domain experts to specify mechanisms in natural language without sacrificing biomedical fidelity."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T07:59:16Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    7,
                    59,
                    16,
                    4,
                    339,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Omid Bazgir"
                    },
                    {
                        "name": "Vineeth Manthapuri"
                    },
                    {
                        "name": "Ilia Rattsev"
                    },
                    {
                        "name": "Mohammad Jafarnejad"
                    }
                ],
                "author_detail": {
                    "name": "Mohammad Jafarnejad"
                },
                "author": "Mohammad Jafarnejad"
            },
            {
                "id": "http://arxiv.org/abs/2511.18200v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.18200v2",
                "title": "InfiniBench: Infinite Benchmarking for Visual Spatial Reasoning with Customizable Scene Complexity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InfiniBench: Infinite Benchmarking for Visual Spatial Reasoning with Customizable Scene Complexity"
                },
                "updated": "2025-12-05T07:59:09Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    7,
                    59,
                    9,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.18200v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.18200v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Modern vision-language models (VLMs) are expected to have abilities of spatial reasoning with diverse scene complexities, but evaluating such abilities is difficult due to the lack of benchmarks that are not only diverse and scalable but also fully customizable. Existing benchmarks offer limited customizability over the scene complexity and are incapable of isolating and analyzing specific VLM failure modes under distinct spatial conditions. To address this gap, instead of individually presenting benchmarks for different scene complexities, in this paper we present InfiniBench, a fully automated, customizable and user-friendly benchmark generator that can synthesize a theoretically infinite variety of 3D scenes with parameterized control on scene complexity. InfiniBench uniquely translates scene descriptions in natural language into photo-realistic videos with complex and physically plausible 3D layouts. This is achieved through three key innovations: 1) a LLM-based agentic framework that iteratively refines procedural scene constraints from scene descriptions; 2) a flexible cluster-based layout optimizer that generates dense and cluttered scenes previously intractable for procedural methods; and 3) a task-aware camera trajectory optimization method that renders scenes into videos with full object coverage as VLM input. Experiments demonstrate that InfiniBench outperforms state-of-the-art procedural and LLM-based 3D generation methods in prompt fidelity and physical plausibility, especially in high-complexity scenarios. We further showcased the usefulness of InfiniBench, by generating benchmarks for representative spatial reasoning tasks including measurement, perspective-taking and spatiotemporal tracking.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern vision-language models (VLMs) are expected to have abilities of spatial reasoning with diverse scene complexities, but evaluating such abilities is difficult due to the lack of benchmarks that are not only diverse and scalable but also fully customizable. Existing benchmarks offer limited customizability over the scene complexity and are incapable of isolating and analyzing specific VLM failure modes under distinct spatial conditions. To address this gap, instead of individually presenting benchmarks for different scene complexities, in this paper we present InfiniBench, a fully automated, customizable and user-friendly benchmark generator that can synthesize a theoretically infinite variety of 3D scenes with parameterized control on scene complexity. InfiniBench uniquely translates scene descriptions in natural language into photo-realistic videos with complex and physically plausible 3D layouts. This is achieved through three key innovations: 1) a LLM-based agentic framework that iteratively refines procedural scene constraints from scene descriptions; 2) a flexible cluster-based layout optimizer that generates dense and cluttered scenes previously intractable for procedural methods; and 3) a task-aware camera trajectory optimization method that renders scenes into videos with full object coverage as VLM input. Experiments demonstrate that InfiniBench outperforms state-of-the-art procedural and LLM-based 3D generation methods in prompt fidelity and physical plausibility, especially in high-complexity scenarios. We further showcased the usefulness of InfiniBench, by generating benchmarks for representative spatial reasoning tasks including measurement, perspective-taking and spatiotemporal tracking."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-22T22:05:39Z",
                "published_parsed": [
                    2025,
                    11,
                    22,
                    22,
                    5,
                    39,
                    5,
                    326,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Haoming Wang"
                    },
                    {
                        "name": "Qiyao Xue"
                    },
                    {
                        "name": "Wei Gao"
                    }
                ],
                "author_detail": {
                    "name": "Wei Gao"
                },
                "author": "Wei Gao"
            },
            {
                "id": "http://arxiv.org/abs/2509.19552v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2509.19552v3",
                "title": "iFinder: Structured Zero-Shot Vision-Based LLM Grounding for Dash-Cam Video Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "iFinder: Structured Zero-Shot Vision-Based LLM Grounding for Dash-Cam Video Reasoning"
                },
                "updated": "2025-12-05T07:58:34Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    7,
                    58,
                    34,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2509.19552v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2509.19552v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Grounding large language models (LLMs) in domain-specific tasks like post-hoc dash-cam driving video analysis is challenging due to their general-purpose training and lack of structured inductive biases. As vision is often the sole modality available for such analysis (i.e., no LiDAR, GPS, etc.), existing video-based vision-language models (V-VLMs) struggle with spatial reasoning, causal inference, and explainability of events in the input video. To this end, we introduce iFinder, a structured semantic grounding framework that decouples perception from reasoning by translating dash-cam videos into a hierarchical, interpretable data structure for LLMs. iFinder operates as a modular, training-free pipeline that employs pretrained vision models to extract critical cues -- object pose, lane positions, and object trajectories -- which are hierarchically organized into frame- and video-level structures. Combined with a three-block prompting strategy, it enables step-wise, grounded reasoning for the LLM to refine a peer V-VLM's outputs and provide accurate reasoning. Evaluations on four public dash-cam video benchmarks show that iFinder's proposed grounding with domain-specific cues, especially object orientation and global context, significantly outperforms end-to-end V-VLMs on four zero-shot driving benchmarks, with up to 39% gains in accident reasoning accuracy. By grounding LLMs with driving domain-specific representations, iFinder offers a zero-shot, interpretable, and reliable alternative to end-to-end V-VLMs for post-hoc driving video understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Grounding large language models (LLMs) in domain-specific tasks like post-hoc dash-cam driving video analysis is challenging due to their general-purpose training and lack of structured inductive biases. As vision is often the sole modality available for such analysis (i.e., no LiDAR, GPS, etc.), existing video-based vision-language models (V-VLMs) struggle with spatial reasoning, causal inference, and explainability of events in the input video. To this end, we introduce iFinder, a structured semantic grounding framework that decouples perception from reasoning by translating dash-cam videos into a hierarchical, interpretable data structure for LLMs. iFinder operates as a modular, training-free pipeline that employs pretrained vision models to extract critical cues -- object pose, lane positions, and object trajectories -- which are hierarchically organized into frame- and video-level structures. Combined with a three-block prompting strategy, it enables step-wise, grounded reasoning for the LLM to refine a peer V-VLM's outputs and provide accurate reasoning. Evaluations on four public dash-cam video benchmarks show that iFinder's proposed grounding with domain-specific cues, especially object orientation and global context, significantly outperforms end-to-end V-VLMs on four zero-shot driving benchmarks, with up to 39% gains in accident reasoning accuracy. By grounding LLMs with driving domain-specific representations, iFinder offers a zero-shot, interpretable, and reliable alternative to end-to-end V-VLMs for post-hoc driving video understanding."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-09-23T20:25:53Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    20,
                    25,
                    53,
                    1,
                    266,
                    0
                ],
                "arxiv_comment": "Accepted at NeurIPS 2025",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Manyi Yao"
                    },
                    {
                        "name": "Bingbing Zhuang"
                    },
                    {
                        "name": "Sparsh Garg"
                    },
                    {
                        "name": "Amit Roy-Chowdhury"
                    },
                    {
                        "name": "Christian Shelton"
                    },
                    {
                        "name": "Manmohan Chandraker"
                    },
                    {
                        "name": "Abhishek Aich"
                    }
                ],
                "author_detail": {
                    "name": "Abhishek Aich"
                },
                "author": "Abhishek Aich"
            },
            {
                "id": "http://arxiv.org/abs/2512.05501v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05501v1",
                "title": "SEA-SafeguardBench: Evaluating AI Safety in SEA Languages and Cultures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SEA-SafeguardBench: Evaluating AI Safety in SEA Languages and Cultures"
                },
                "updated": "2025-12-05T07:57:57Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    7,
                    57,
                    57,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05501v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05501v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Safeguard models help large language models (LLMs) detect and block harmful content, but most evaluations remain English-centric and overlook linguistic and cultural diversity. Existing multilingual safety benchmarks often rely on machine-translated English data, which fails to capture nuances in low-resource languages. Southeast Asian (SEA) languages are underrepresented despite the region's linguistic diversity and unique safety concerns, from culturally sensitive political speech to region-specific misinformation. Addressing these gaps requires benchmarks that are natively authored to reflect local norms and harm scenarios. We introduce SEA-SafeguardBench, the first human-verified safety benchmark for SEA, covering eight languages, 21,640 samples, across three subsets: general, in-the-wild, and content generation. The experimental results from our benchmark demonstrate that even state-of-the-art LLMs and guardrails are challenged by SEA cultural and harm scenarios and underperform when compared to English texts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safeguard models help large language models (LLMs) detect and block harmful content, but most evaluations remain English-centric and overlook linguistic and cultural diversity. Existing multilingual safety benchmarks often rely on machine-translated English data, which fails to capture nuances in low-resource languages. Southeast Asian (SEA) languages are underrepresented despite the region's linguistic diversity and unique safety concerns, from culturally sensitive political speech to region-specific misinformation. Addressing these gaps requires benchmarks that are natively authored to reflect local norms and harm scenarios. We introduce SEA-SafeguardBench, the first human-verified safety benchmark for SEA, covering eight languages, 21,640 samples, across three subsets: general, in-the-wild, and content generation. The experimental results from our benchmark demonstrate that even state-of-the-art LLMs and guardrails are challenged by SEA cultural and harm scenarios and underperform when compared to English texts."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T07:57:57Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    7,
                    57,
                    57,
                    4,
                    339,
                    0
                ],
                "arxiv_comment": "Under review",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Panuthep Tasawong"
                    },
                    {
                        "name": "Jian Gang Ngui"
                    },
                    {
                        "name": "Alham Fikri Aji"
                    },
                    {
                        "name": "Trevor Cohn"
                    },
                    {
                        "name": "Peerat Limkonchotiwat"
                    }
                ],
                "author_detail": {
                    "name": "Peerat Limkonchotiwat"
                },
                "author": "Peerat Limkonchotiwat"
            },
            {
                "id": "http://arxiv.org/abs/2512.05498v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05498v1",
                "title": "A Hybrid Approach for EMF Code Generation:Code Templates Meet Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Hybrid Approach for EMF Code Generation:Code Templates Meet Large Language Models"
                },
                "updated": "2025-12-05T07:46:51Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    7,
                    46,
                    51,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05498v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05498v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Template-based and LLM-based code generation are both key enablers of automated software development. The former provides correctness guarantees but are rigid for complex requirements, whereas LLMs offer high flexibility at the risk of producing faulty code.This paper proposes iEcoreGen, a hybrid approach that integrates Eclipse Modeling Framework (EMF) and LLMs. In EMF, an Ecore model defines a system structure and acts as a blueprint for code-generation.iEcoreGen decomposes requirements to derive operation specifications, uses EMF's template-based generator to produce initial Java code, and serializes specifications into docstrings. LLMs are then invoked to complete and fix unimplemented methods. We assessed iEcoreGen on twenty code-generation tasks across five LLMs. It surpasses LLM-only baselines on pass@k and performs on par with them on compilation@k. An ablation study clarified the contribution of each component of iEcoreGen. Overall, the findings indicate that LLM-enhanced model-driven development is a promising path toward more efficient software automation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Template-based and LLM-based code generation are both key enablers of automated software development. The former provides correctness guarantees but are rigid for complex requirements, whereas LLMs offer high flexibility at the risk of producing faulty code.This paper proposes iEcoreGen, a hybrid approach that integrates Eclipse Modeling Framework (EMF) and LLMs. In EMF, an Ecore model defines a system structure and acts as a blueprint for code-generation.iEcoreGen decomposes requirements to derive operation specifications, uses EMF's template-based generator to produce initial Java code, and serializes specifications into docstrings. LLMs are then invoked to complete and fix unimplemented methods. We assessed iEcoreGen on twenty code-generation tasks across five LLMs. It surpasses LLM-only baselines on pass@k and performs on par with them on compilation@k. An ablation study clarified the contribution of each component of iEcoreGen. Overall, the findings indicate that LLM-enhanced model-driven development is a promising path toward more efficient software automation."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T07:46:51Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    7,
                    46,
                    51,
                    4,
                    339,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Xiao He"
                    },
                    {
                        "name": "Ru Chen"
                    },
                    {
                        "name": "Zeqing Zhang"
                    },
                    {
                        "name": "Yanling Wang"
                    },
                    {
                        "name": "Qiuyan Dong"
                    }
                ],
                "author_detail": {
                    "name": "Qiuyan Dong"
                },
                "author": "Qiuyan Dong"
            },
            {
                "id": "http://arxiv.org/abs/2511.21740v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.21740v2",
                "title": "Decoding inner speech with an end-to-end brain-to-text neural interface",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decoding inner speech with an end-to-end brain-to-text neural interface"
                },
                "updated": "2025-12-05T07:34:11Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    7,
                    34,
                    11,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.21740v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.21740v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Speech brain-computer interfaces (BCIs) aim to restore communication for people with paralysis by translating neural activity into text. Most systems use cascaded frameworks that decode phonemes before assembling sentences with an n-gram language model (LM), preventing joint optimization of all stages simultaneously. Here, we introduce an end-to-end Brain-to-Text (BIT) framework that translates neural activity into coherent sentences using a single differentiable neural network. Central to our approach is a cross-task, cross-species pretrained neural encoder, whose representations transfer to both attempted and imagined speech. In a cascaded setting with an n-gram LM, the pretrained encoder establishes a new state-of-the-art (SOTA) on the Brain-to-Text '24 and '25 benchmarks. Integrated end-to-end with audio large language models (LLMs) and trained with contrastive learning for cross-modal alignment, BIT reduces the word error rate (WER) of the prior end-to-end method from 24.69% to 10.22%. Notably, we find that small-scale audio LLMs markedly improve end-to-end decoding. Beyond record-setting performance, BIT aligns attempted and imagined speech embeddings to enable cross-task generalization. Altogether, our approach advances the integration of large, diverse neural datasets, paving the way for an end-to-end decoding framework that supports seamless, differentiable optimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speech brain-computer interfaces (BCIs) aim to restore communication for people with paralysis by translating neural activity into text. Most systems use cascaded frameworks that decode phonemes before assembling sentences with an n-gram language model (LM), preventing joint optimization of all stages simultaneously. Here, we introduce an end-to-end Brain-to-Text (BIT) framework that translates neural activity into coherent sentences using a single differentiable neural network. Central to our approach is a cross-task, cross-species pretrained neural encoder, whose representations transfer to both attempted and imagined speech. In a cascaded setting with an n-gram LM, the pretrained encoder establishes a new state-of-the-art (SOTA) on the Brain-to-Text '24 and '25 benchmarks. Integrated end-to-end with audio large language models (LLMs) and trained with contrastive learning for cross-modal alignment, BIT reduces the word error rate (WER) of the prior end-to-end method from 24.69% to 10.22%. Notably, we find that small-scale audio LLMs markedly improve end-to-end decoding. Beyond record-setting performance, BIT aligns attempted and imagined speech embeddings to enable cross-task generalization. Altogether, our approach advances the integration of large, diverse neural datasets, paving the way for an end-to-end decoding framework that supports seamless, differentiable optimization."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T21:25:54Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    21,
                    25,
                    54,
                    4,
                    325,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Yizi Zhang"
                    },
                    {
                        "name": "Linyang He"
                    },
                    {
                        "name": "Chaofei Fan"
                    },
                    {
                        "name": "Tingkai Liu"
                    },
                    {
                        "name": "Han Yu"
                    },
                    {
                        "name": "Trung Le"
                    },
                    {
                        "name": "Jingyuan Li"
                    },
                    {
                        "name": "Scott Linderman"
                    },
                    {
                        "name": "Lea Duncker"
                    },
                    {
                        "name": "Francis R Willett"
                    },
                    {
                        "name": "Nima Mesgarani"
                    },
                    {
                        "name": "Liam Paninski"
                    }
                ],
                "author_detail": {
                    "name": "Liam Paninski"
                },
                "author": "Liam Paninski"
            },
            {
                "id": "http://arxiv.org/abs/2509.11536v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2509.11536v2",
                "title": "HARP: Hallucination Detection via Reasoning Subspace Projection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HARP: Hallucination Detection via Reasoning Subspace Projection"
                },
                "updated": "2025-12-05T07:28:13Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    7,
                    28,
                    13,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2509.11536v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2509.11536v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Hallucinations in Large Language Models (LLMs) pose a major barrier to their reliable use in critical decision-making. Although existing hallucination detection methods have improved accuracy, they still struggle with disentangling semantic and reasoning information and maintaining robustness. To address these challenges, we propose HARP (Hallucination detection via reasoning subspace projection), a novel hallucination detection framework. HARP establishes that the hidden state space of LLMs can be decomposed into a direct sum of a semantic subspace and a reasoning subspace, where the former encodes linguistic expression and the latter captures internal reasoning processes. Moreover, we demonstrate that the Unembedding layer can disentangle these subspaces, and by applying Singular Value Decomposition (SVD) to its parameters, the basis vectors spanning the semantic and reasoning subspaces are obtained. Finally, HARP projects hidden states onto the basis vectors of the reasoning subspace, and the resulting projections are then used as input features for hallucination detection in LLMs. By using these projections, HARP reduces the dimension of the feature to approximately 5% of the original, filters out most noise, and achieves enhanced robustness. Experiments across multiple datasets show that HARP achieves state-of-the-art hallucination detection performance; in particular, it achieves an AUROC of 92.8% on TriviaQA, outperforming the previous best method by 7.5%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucinations in Large Language Models (LLMs) pose a major barrier to their reliable use in critical decision-making. Although existing hallucination detection methods have improved accuracy, they still struggle with disentangling semantic and reasoning information and maintaining robustness. To address these challenges, we propose HARP (Hallucination detection via reasoning subspace projection), a novel hallucination detection framework. HARP establishes that the hidden state space of LLMs can be decomposed into a direct sum of a semantic subspace and a reasoning subspace, where the former encodes linguistic expression and the latter captures internal reasoning processes. Moreover, we demonstrate that the Unembedding layer can disentangle these subspaces, and by applying Singular Value Decomposition (SVD) to its parameters, the basis vectors spanning the semantic and reasoning subspaces are obtained. Finally, HARP projects hidden states onto the basis vectors of the reasoning subspace, and the resulting projections are then used as input features for hallucination detection in LLMs. By using these projections, HARP reduces the dimension of the feature to approximately 5% of the original, filters out most noise, and achieves enhanced robustness. Experiments across multiple datasets show that HARP achieves state-of-the-art hallucination detection performance; in particular, it achieves an AUROC of 92.8% on TriviaQA, outperforming the previous best method by 7.5%."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-09-15T03:02:33Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    3,
                    2,
                    33,
                    0,
                    258,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Junjie Hu"
                    },
                    {
                        "name": "Gang Tu"
                    },
                    {
                        "name": "ShengYu Cheng"
                    },
                    {
                        "name": "Jinxin Li"
                    },
                    {
                        "name": "Jinting Wang"
                    },
                    {
                        "name": "Rui Chen"
                    },
                    {
                        "name": "Zhilong Zhou"
                    },
                    {
                        "name": "Dongbo Shan"
                    }
                ],
                "author_detail": {
                    "name": "Dongbo Shan"
                },
                "author": "Dongbo Shan"
            },
            {
                "id": "http://arxiv.org/abs/2512.05485v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05485v1",
                "title": "TeleAI-Safety: A comprehensive LLM jailbreaking benchmark towards attacks, defenses, and evaluations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TeleAI-Safety: A comprehensive LLM jailbreaking benchmark towards attacks, defenses, and evaluations"
                },
                "updated": "2025-12-05T07:23:30Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    7,
                    23,
                    30,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05485v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05485v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "While the deployment of large language models (LLMs) in high-value industries continues to expand, the systematic assessment of their safety against jailbreak and prompt-based attacks remains insufficient. Existing safety evaluation benchmarks and frameworks are often limited by an imbalanced integration of core components (attack, defense, and evaluation methods) and an isolation between flexible evaluation frameworks and standardized benchmarking capabilities. These limitations hinder reliable cross-study comparisons and create unnecessary overhead for comprehensive risk assessment. To address these gaps, we present TeleAI-Safety, a modular and reproducible framework coupled with a systematic benchmark for rigorous LLM safety evaluation. Our framework integrates a broad collection of 19 attack methods (including one self-developed method), 29 defense methods, and 19 evaluation methods (including one self-developed method). With a curated attack corpus of 342 samples spanning 12 distinct risk categories, the TeleAI-Safety benchmark conducts extensive evaluations across 14 target models. The results reveal systematic vulnerabilities and model-specific failure cases, highlighting critical trade-offs between safety and utility, and identifying potential defense patterns for future optimization. In practical scenarios, TeleAI-Safety can be flexibly adjusted with customized attack, defense, and evaluation combinations to meet specific demands. We release our complete code and evaluation results to facilitate reproducible research and establish unified safety baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While the deployment of large language models (LLMs) in high-value industries continues to expand, the systematic assessment of their safety against jailbreak and prompt-based attacks remains insufficient. Existing safety evaluation benchmarks and frameworks are often limited by an imbalanced integration of core components (attack, defense, and evaluation methods) and an isolation between flexible evaluation frameworks and standardized benchmarking capabilities. These limitations hinder reliable cross-study comparisons and create unnecessary overhead for comprehensive risk assessment. To address these gaps, we present TeleAI-Safety, a modular and reproducible framework coupled with a systematic benchmark for rigorous LLM safety evaluation. Our framework integrates a broad collection of 19 attack methods (including one self-developed method), 29 defense methods, and 19 evaluation methods (including one self-developed method). With a curated attack corpus of 342 samples spanning 12 distinct risk categories, the TeleAI-Safety benchmark conducts extensive evaluations across 14 target models. The results reveal systematic vulnerabilities and model-specific failure cases, highlighting critical trade-offs between safety and utility, and identifying potential defense patterns for future optimization. In practical scenarios, TeleAI-Safety can be flexibly adjusted with customized attack, defense, and evaluation combinations to meet specific demands. We release our complete code and evaluation results to facilitate reproducible research and establish unified safety baselines."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T07:23:30Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    7,
                    23,
                    30,
                    4,
                    339,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Xiuyuan Chen"
                    },
                    {
                        "name": "Jian Zhao"
                    },
                    {
                        "name": "Yuxiang He"
                    },
                    {
                        "name": "Yuan Xun"
                    },
                    {
                        "name": "Xinwei Liu"
                    },
                    {
                        "name": "Yanshu Li"
                    },
                    {
                        "name": "Huilin Zhou"
                    },
                    {
                        "name": "Wei Cai"
                    },
                    {
                        "name": "Ziyan Shi"
                    },
                    {
                        "name": "Yuchen Yuan"
                    },
                    {
                        "name": "Tianle Zhang"
                    },
                    {
                        "name": "Chi Zhang"
                    },
                    {
                        "name": "Xuelong Li"
                    }
                ],
                "author_detail": {
                    "name": "Xuelong Li"
                },
                "author": "Xuelong Li"
            },
            {
                "id": "http://arxiv.org/abs/2511.04555v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.04555v2",
                "title": "Evo-1: Lightweight Vision-Language-Action Model with Preserved Semantic Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evo-1: Lightweight Vision-Language-Action Model with Preserved Semantic Alignment"
                },
                "updated": "2025-12-05T06:57:07Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    6,
                    57,
                    7,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.04555v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.04555v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Vision-Language-Action (VLA) models have emerged as a powerful framework that unifies perception, language, and control, enabling robots to perform diverse tasks through multimodal understanding. However, current VLA models typically contain massive parameters and rely heavily on large-scale robot data pretraining, leading to high computational costs during training, as well as limited deployability for real-time inference. Moreover, most training paradigms often degrade the perceptual representations of the vision-language backbone, resulting in overfitting and poor generalization to downstream tasks. In this work, we present Evo-1, a lightweight VLA model that reduces computation and improves deployment efficiency, while maintaining strong performance without pretraining on robot data. Evo-1 builds on a native multimodal Vision-Language model (VLM), incorporating a novel cross-modulated diffusion transformer along with an optimized integration module, together forming an effective architecture. We further introduce a two-stage training paradigm that progressively aligns action with perception, preserving the representations of the VLM. Notably, with only 0.77 billion parameters, Evo-1 achieves state-of-the-art results on the Meta-World and RoboTwin suite, surpassing the previous best models by 12.4% and 6.9%, respectively, and also attains a competitive result of 94.8% on LIBERO. In real-world evaluations, Evo-1 attains a 78% success rate with high inference frequency and low memory overhead, outperforming all baseline methods. We release code, data, and model weights to facilitate future research on lightweight and efficient VLA models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language-Action (VLA) models have emerged as a powerful framework that unifies perception, language, and control, enabling robots to perform diverse tasks through multimodal understanding. However, current VLA models typically contain massive parameters and rely heavily on large-scale robot data pretraining, leading to high computational costs during training, as well as limited deployability for real-time inference. Moreover, most training paradigms often degrade the perceptual representations of the vision-language backbone, resulting in overfitting and poor generalization to downstream tasks. In this work, we present Evo-1, a lightweight VLA model that reduces computation and improves deployment efficiency, while maintaining strong performance without pretraining on robot data. Evo-1 builds on a native multimodal Vision-Language model (VLM), incorporating a novel cross-modulated diffusion transformer along with an optimized integration module, together forming an effective architecture. We further introduce a two-stage training paradigm that progressively aligns action with perception, preserving the representations of the VLM. Notably, with only 0.77 billion parameters, Evo-1 achieves state-of-the-art results on the Meta-World and RoboTwin suite, surpassing the previous best models by 12.4% and 6.9%, respectively, and also attains a competitive result of 94.8% on LIBERO. In real-world evaluations, Evo-1 attains a 78% success rate with high inference frequency and low memory overhead, outperforming all baseline methods. We release code, data, and model weights to facilitate future research on lightweight and efficient VLA models."
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-06T17:07:49Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    17,
                    7,
                    49,
                    3,
                    310,
                    0
                ],
                "arxiv_comment": "Github: https://github.com/MINT-SJTU/Evo-1",
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "authors": [
                    {
                        "name": "Tao Lin"
                    },
                    {
                        "name": "Yilei Zhong"
                    },
                    {
                        "name": "Yuxin Du"
                    },
                    {
                        "name": "Jingjing Zhang"
                    },
                    {
                        "name": "Jiting Liu"
                    },
                    {
                        "name": "Yinxinyu Chen"
                    },
                    {
                        "name": "Encheng Gu"
                    },
                    {
                        "name": "Ziyan Liu"
                    },
                    {
                        "name": "Hongyi Cai"
                    },
                    {
                        "name": "Yanwen Zou"
                    },
                    {
                        "name": "Lixing Zou"
                    },
                    {
                        "name": "Zhaoye Zhou"
                    },
                    {
                        "name": "Gen Li"
                    },
                    {
                        "name": "Bo Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zhao"
                },
                "author": "Bo Zhao"
            },
            {
                "id": "http://arxiv.org/abs/2506.16402v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2506.16402v3",
                "title": "IS-Bench: Evaluating Interactive Safety of VLM-Driven Embodied Agents in Daily Household Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IS-Bench: Evaluating Interactive Safety of VLM-Driven Embodied Agents in Daily Household Tasks"
                },
                "updated": "2025-12-05T06:52:55Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    6,
                    52,
                    55,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2506.16402v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2506.16402v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Flawed planning from VLM-driven embodied agents poses significant safety hazards, hindering their deployment in real-world household tasks. However, existing static, non-interactive evaluation paradigms fail to adequately assess risks within these interactive environments, since they cannot simulate dynamic risks that emerge from an agent's actions and rely on unreliable post-hoc evaluations that ignore unsafe intermediate steps. To bridge this critical gap, we propose evaluating an agent's interactive safety: its ability to perceive emergent risks and execute mitigation steps in the correct procedural order. We thus present IS-Bench, the first multi-modal benchmark designed for interactive safety, featuring 161 challenging scenarios with 388 unique safety risks instantiated in a high-fidelity simulator. Crucially, it facilitates a novel process-oriented evaluation that verifies whether risk mitigation actions are performed before/after specific risk-prone steps. Extensive experiments on leading VLMs, including the GPT-4o and Gemini-2.5 series, reveal that current agents lack interactive safety awareness, and that while safety-aware Chain-of-Thought can improve performance, it often compromises task completion. By highlighting these critical limitations, IS-Bench provides a foundation for developing safer and more reliable embodied AI systems. Code and data are released under https://github.com/AI45Lab/IS-Bench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flawed planning from VLM-driven embodied agents poses significant safety hazards, hindering their deployment in real-world household tasks. However, existing static, non-interactive evaluation paradigms fail to adequately assess risks within these interactive environments, since they cannot simulate dynamic risks that emerge from an agent's actions and rely on unreliable post-hoc evaluations that ignore unsafe intermediate steps. To bridge this critical gap, we propose evaluating an agent's interactive safety: its ability to perceive emergent risks and execute mitigation steps in the correct procedural order. We thus present IS-Bench, the first multi-modal benchmark designed for interactive safety, featuring 161 challenging scenarios with 388 unique safety risks instantiated in a high-fidelity simulator. Crucially, it facilitates a novel process-oriented evaluation that verifies whether risk mitigation actions are performed before/after specific risk-prone steps. Extensive experiments on leading VLMs, including the GPT-4o and Gemini-2.5 series, reveal that current agents lack interactive safety awareness, and that while safety-aware Chain-of-Thought can improve performance, it often compromises task completion. By highlighting these critical limitations, IS-Bench provides a foundation for developing safer and more reliable embodied AI systems. Code and data are released under https://github.com/AI45Lab/IS-Bench."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-06-19T15:34:46Z",
                "published_parsed": [
                    2025,
                    6,
                    19,
                    15,
                    34,
                    46,
                    3,
                    170,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Xiaoya Lu"
                    },
                    {
                        "name": "Zeren Chen"
                    },
                    {
                        "name": "Xuhao Hu"
                    },
                    {
                        "name": "Yijin Zhou"
                    },
                    {
                        "name": "Weichen Zhang"
                    },
                    {
                        "name": "Dongrui Liu"
                    },
                    {
                        "name": "Lu Sheng"
                    },
                    {
                        "name": "Jing Shao"
                    }
                ],
                "author_detail": {
                    "name": "Jing Shao"
                },
                "author": "Jing Shao"
            },
            {
                "id": "http://arxiv.org/abs/2512.05464v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05464v1",
                "title": "Dynamic Alignment for Collective Agency: Toward a Scalable Self-Improving Framework for Open-Ended LLM Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Alignment for Collective Agency: Toward a Scalable Self-Improving Framework for Open-Ended LLM Alignment"
                },
                "updated": "2025-12-05T06:46:00Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    6,
                    46,
                    0,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05464v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05464v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) are typically aligned with human values using preference data or predefined principles such as helpfulness, honesty, and harmlessness. However, as AI systems progress toward Artificial General Intelligence (AGI) and Artificial Superintelligence (ASI), such value systems may become insufficient. In addition, human feedback-based alignment remains resource-intensive and difficult to scale. While AI-feedback-based self-improving alignment methods have been explored as a scalable alternative, they have largely remained constrained to conventional alignment values. In this work, we explore both a more holistic alignment objective and a scalable, self-improving alignment approach. Aiming to transcend conventional alignment norms, we introduce Collective Agency (CA)-a unified and open-ended alignment value that encourages integrated agentic capabilities. We also propose Dynamic Alignment-an alignment framework that enables an LLM to iteratively align itself. Dynamic Alignment comprises two key components: (1) automated training dataset generation with LLMs, and (2) a self-rewarding mechanism, where the policy model evaluates its own output candidates and assigns rewards for GRPO-based learning. Experimental results demonstrate that our approach successfully aligns the model to CA while preserving general NLP capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are typically aligned with human values using preference data or predefined principles such as helpfulness, honesty, and harmlessness. However, as AI systems progress toward Artificial General Intelligence (AGI) and Artificial Superintelligence (ASI), such value systems may become insufficient. In addition, human feedback-based alignment remains resource-intensive and difficult to scale. While AI-feedback-based self-improving alignment methods have been explored as a scalable alternative, they have largely remained constrained to conventional alignment values. In this work, we explore both a more holistic alignment objective and a scalable, self-improving alignment approach. Aiming to transcend conventional alignment norms, we introduce Collective Agency (CA)-a unified and open-ended alignment value that encourages integrated agentic capabilities. We also propose Dynamic Alignment-an alignment framework that enables an LLM to iteratively align itself. Dynamic Alignment comprises two key components: (1) automated training dataset generation with LLMs, and (2) a self-rewarding mechanism, where the policy model evaluates its own output candidates and assigns rewards for GRPO-based learning. Experimental results demonstrate that our approach successfully aligns the model to CA while preserving general NLP capabilities."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T06:46:00Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    6,
                    46,
                    0,
                    4,
                    339,
                    0
                ],
                "arxiv_comment": "8 pages, 4 figures, to appear in AAAI 2026 AIGOV Workshop",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Panatchakorn Anantaprayoon"
                    },
                    {
                        "name": "Nataliia Babina"
                    },
                    {
                        "name": "Jad Tarifi"
                    },
                    {
                        "name": "Nima Asgharbeygi"
                    }
                ],
                "author_detail": {
                    "name": "Nima Asgharbeygi"
                },
                "author": "Nima Asgharbeygi"
            },
            {
                "id": "http://arxiv.org/abs/2512.04535v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.04535v2",
                "title": "GTM: Simulating the World of Tools for AI Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GTM: Simulating the World of Tools for AI Agents"
                },
                "updated": "2025-12-05T06:40:32Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    6,
                    40,
                    32,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.04535v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.04535v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The integration of external tools is pivotal for empowering Large Language Model (LLM) agents with real-world capabilities. However, training these agents through direct, continuous interaction with diverse tools is often prohibitively expensive, slow, and introduces additional development and maintenance overhead. To address this challenge, we introduce the Generalist Tool Model (GTM), a 1.5-billion-parameter model that learns to act as a universal tool simulator. With only prompt-level configuration, GTM accesses tool functionalities along with input arguments and generates outputs that faithfully mimic real tool execution, providing a fast and cost-effective solution that eliminates development overhead. To build GTM, we propose the Context-Aware Response Generation (CARG) pipeline, which synthesizes comprehensive training data covering over 20,000 tools across 300 domains including physics, medicine, robotics, and finance. Through this pipeline, GTM learns to produce not only syntactically correct outputs but also logically coherent and contextually appropriate responses. Experiments demonstrate that GTM produces high-quality outputs with strong consistency and reliability. Besides when used in real reinforcement learning scenarios for agent training, GTM exhibits significantly faster simulation speed compared to real tools while maintaining comparable output quality, along with remarkable generalization and domain adaptability. Our results establish GTM as a foundational component for developing future AI agents, enabling efficient and scalable training of tool-augmented systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of external tools is pivotal for empowering Large Language Model (LLM) agents with real-world capabilities. However, training these agents through direct, continuous interaction with diverse tools is often prohibitively expensive, slow, and introduces additional development and maintenance overhead. To address this challenge, we introduce the Generalist Tool Model (GTM), a 1.5-billion-parameter model that learns to act as a universal tool simulator. With only prompt-level configuration, GTM accesses tool functionalities along with input arguments and generates outputs that faithfully mimic real tool execution, providing a fast and cost-effective solution that eliminates development overhead. To build GTM, we propose the Context-Aware Response Generation (CARG) pipeline, which synthesizes comprehensive training data covering over 20,000 tools across 300 domains including physics, medicine, robotics, and finance. Through this pipeline, GTM learns to produce not only syntactically correct outputs but also logically coherent and contextually appropriate responses. Experiments demonstrate that GTM produces high-quality outputs with strong consistency and reliability. Besides when used in real reinforcement learning scenarios for agent training, GTM exhibits significantly faster simulation speed compared to real tools while maintaining comparable output quality, along with remarkable generalization and domain adaptability. Our results establish GTM as a foundational component for developing future AI agents, enabling efficient and scalable training of tool-augmented systems."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-04T07:33:04Z",
                "published_parsed": [
                    2025,
                    12,
                    4,
                    7,
                    33,
                    4,
                    3,
                    338,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Zhenzhen Ren"
                    },
                    {
                        "name": "Xinpeng Zhang"
                    },
                    {
                        "name": "Zhenxing Qian"
                    },
                    {
                        "name": "Yan Gao"
                    },
                    {
                        "name": "Yu Shi"
                    },
                    {
                        "name": "Shuxin Zheng"
                    },
                    {
                        "name": "Jiyan He"
                    }
                ],
                "author_detail": {
                    "name": "Jiyan He"
                },
                "author": "Jiyan He"
            },
            {
                "id": "http://arxiv.org/abs/2512.05462v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05462v1",
                "title": "Model Gateway: Model Management Platform for Model-Driven Drug Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model Gateway: Model Management Platform for Model-Driven Drug Discovery"
                },
                "updated": "2025-12-05T06:39:37Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    6,
                    39,
                    37,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05462v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05462v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "This paper presents the Model Gateway, a management platform for managing machine learning (ML) and scientific computational models in the drug discovery pipeline. The platform supports Large Language Model (LLM) Agents and Generative AI-based tools to perform ML model management tasks in our Machine Learning operations (MLOps) pipelines, such as the dynamic consensus model, a model that aggregates several scientific computational models, registration and management, retrieving model information, asynchronous submission/execution of models, and receiving results once the model complete executions. The platform includes a Model Owner Control Panel, Platform Admin Tools, and Model Gateway API service for interacting with the platform and tracking model execution. The platform achieves a 0% failure rate when testing scaling beyond 10k simultaneous application clients consume models. The Model Gateway is a fundamental part of our model-driven drug discovery pipeline. It has the potential to significantly accelerate the development of new drugs with the maturity of our MLOps infrastructure and the integration of LLM Agents and Generative AI tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents the Model Gateway, a management platform for managing machine learning (ML) and scientific computational models in the drug discovery pipeline. The platform supports Large Language Model (LLM) Agents and Generative AI-based tools to perform ML model management tasks in our Machine Learning operations (MLOps) pipelines, such as the dynamic consensus model, a model that aggregates several scientific computational models, registration and management, retrieving model information, asynchronous submission/execution of models, and receiving results once the model complete executions. The platform includes a Model Owner Control Panel, Platform Admin Tools, and Model Gateway API service for interacting with the platform and tracking model execution. The platform achieves a 0% failure rate when testing scaling beyond 10k simultaneous application clients consume models. The Model Gateway is a fundamental part of our model-driven drug discovery pipeline. It has the potential to significantly accelerate the development of new drugs with the maturity of our MLOps infrastructure and the integration of LLM Agents and Generative AI tools."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T06:39:37Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    6,
                    39,
                    37,
                    4,
                    339,
                    0
                ],
                "arxiv_comment": "7 pages, 7 figures",
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Yan-Shiun Wu"
                    },
                    {
                        "name": "Nathan A. Morin"
                    }
                ],
                "author_detail": {
                    "name": "Nathan A. Morin"
                },
                "author": "Nathan A. Morin"
            },
            {
                "id": "http://arxiv.org/abs/2512.05461v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05461v1",
                "title": "Knowing Your Uncertainty -- On the application of LLM in social sciences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowing Your Uncertainty -- On the application of LLM in social sciences"
                },
                "updated": "2025-12-05T06:36:15Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    6,
                    36,
                    15,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05461v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05461v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) are rapidly being integrated into computational social science research, yet their blackboxed training and designed stochastic elements in inference pose unique challenges for scientific inquiry. This article argues that applying LLMs to social scientific tasks requires explicit assessment of uncertainty-an expectation long established in both quantitative methodology in the social sciences and machine learning. We introduce a unified framework for evaluating LLM uncertainty along two dimensions: the task type (T), which distinguishes between classification, short-form, and long-form generation, and the validation type (V), which captures the availability of reference data or evaluative criteria. Drawing from both computer science and social science literature, we map existing uncertainty quantification (UQ) methods to this T-V typology and offer practical recommendations for researchers. Our framework provides both a methodological safeguard and a practical guide for integrating LLMs into rigorous social science research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are rapidly being integrated into computational social science research, yet their blackboxed training and designed stochastic elements in inference pose unique challenges for scientific inquiry. This article argues that applying LLMs to social scientific tasks requires explicit assessment of uncertainty-an expectation long established in both quantitative methodology in the social sciences and machine learning. We introduce a unified framework for evaluating LLM uncertainty along two dimensions: the task type (T), which distinguishes between classification, short-form, and long-form generation, and the validation type (V), which captures the availability of reference data or evaluative criteria. Drawing from both computer science and social science literature, we map existing uncertainty quantification (UQ) methods to this T-V typology and offer practical recommendations for researchers. Our framework provides both a methodological safeguard and a practical guide for integrating LLMs into rigorous social science research."
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T06:36:15Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    6,
                    36,
                    15,
                    4,
                    339,
                    0
                ],
                "arxiv_comment": "49 pages, 10 figures",
                "arxiv_primary_category": {
                    "term": "cs.CY"
                },
                "authors": [
                    {
                        "name": "Bolun Zhang"
                    },
                    {
                        "name": "Linzhuo Li"
                    },
                    {
                        "name": "Yunqi Chen"
                    },
                    {
                        "name": "Qinlin Zhao"
                    },
                    {
                        "name": "Zihan Zhu"
                    },
                    {
                        "name": "Xiaoyuan Yi"
                    },
                    {
                        "name": "Xing Xie"
                    }
                ],
                "author_detail": {
                    "name": "Xing Xie"
                },
                "author": "Xing Xie"
            },
            {
                "id": "http://arxiv.org/abs/2512.05459v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05459v1",
                "title": "PrivCode: When Code Generation Meets Differential Privacy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PrivCode: When Code Generation Meets Differential Privacy"
                },
                "updated": "2025-12-05T06:27:06Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    6,
                    27,
                    6,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05459v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05459v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) have presented outstanding performance in code generation and completion. However, fine-tuning these models on private datasets can raise privacy and proprietary concerns, such as the leakage of sensitive personal information. Differentially private (DP) code generation provides theoretical guarantees for protecting sensitive code by generating synthetic datasets that preserve statistical properties while reducing privacy leakage concerns. However, DP code generation faces significant challenges due to the strict syntactic dependencies and the privacy-utility trade-off.\n  We propose PrivCode, the first DP synthesizer specifically designed for code datasets. It incorporates a two-stage framework to improve both privacy and utility. In the first stage, termed \"privacy-sanitizing\", PrivCode generates DP-compliant synthetic code by training models using DP-SGD while introducing syntactic information to preserve code structure. The second stage, termed \"utility-boosting\", fine-tunes a larger pre-trained LLM on the synthetic privacy-free code to mitigate the utility loss caused by DP, enhancing the utility of the generated code. Extensive experiments on four LLMs show that PrivCode generates higher-utility code across various testing tasks under four benchmarks. The experiments also confirm its ability to protect sensitive data under varying privacy budgets. We provide the replication package at the anonymous link.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have presented outstanding performance in code generation and completion. However, fine-tuning these models on private datasets can raise privacy and proprietary concerns, such as the leakage of sensitive personal information. Differentially private (DP) code generation provides theoretical guarantees for protecting sensitive code by generating synthetic datasets that preserve statistical properties while reducing privacy leakage concerns. However, DP code generation faces significant challenges due to the strict syntactic dependencies and the privacy-utility trade-off.\n  We propose PrivCode, the first DP synthesizer specifically designed for code datasets. It incorporates a two-stage framework to improve both privacy and utility. In the first stage, termed \"privacy-sanitizing\", PrivCode generates DP-compliant synthetic code by training models using DP-SGD while introducing syntactic information to preserve code structure. The second stage, termed \"utility-boosting\", fine-tunes a larger pre-trained LLM on the synthetic privacy-free code to mitigate the utility loss caused by DP, enhancing the utility of the generated code. Extensive experiments on four LLMs show that PrivCode generates higher-utility code across various testing tasks under four benchmarks. The experiments also confirm its ability to protect sensitive data under varying privacy budgets. We provide the replication package at the anonymous link."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T06:27:06Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    6,
                    27,
                    6,
                    4,
                    339,
                    0
                ],
                "arxiv_comment": "Accepted at NDSS 2026; code available at https://github.com/Liuzzyg/PrivCode",
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Chen Gong"
                    },
                    {
                        "name": "Terry Yue Zhuo"
                    },
                    {
                        "name": "Kecen Li"
                    },
                    {
                        "name": "Weichen Yu"
                    },
                    {
                        "name": "Matt Fredrikson"
                    },
                    {
                        "name": "Tianhao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Tianhao Wang"
                },
                "author": "Tianhao Wang"
            },
            {
                "id": "http://arxiv.org/abs/2508.10967v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.10967v2",
                "title": "Retro-Expert: Collaborative Reasoning for Interpretable Retrosynthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retro-Expert: Collaborative Reasoning for Interpretable Retrosynthesis"
                },
                "updated": "2025-12-05T06:21:37Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    6,
                    21,
                    37,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.10967v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.10967v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Retrosynthesis prediction aims to infer the reactant molecule based on a given product molecule, which is a fundamental task in chemical synthesis. However, existing models rely on static pattern-matching paradigm, which limits their ability to perform effective logic decision-making, leading to black-box decision-making. Building on this, we propose Retro-Expert, an interpretable retrosynthesis framework that performs collaborative reasoning by combining the complementary reasoning strengths of Large Language Models and specialized models via reinforcement learning. It outputs natural language explanations grounded in chemical logic through three components: (1) specialized models analyze the product to construct high-quality chemical decision space, (2) LLM-driven critical reasoning to generate predictions and corresponding interpretable reasoning path, and (3) reinforcement learning optimizing interpretable decision policy. Experiments show that Retro-Expert not only surpasses both LLM-based and specialized models across different metrics but also provides expert-aligned explanations that bridge the gap between AI predictions and actionable chemical insights.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrosynthesis prediction aims to infer the reactant molecule based on a given product molecule, which is a fundamental task in chemical synthesis. However, existing models rely on static pattern-matching paradigm, which limits their ability to perform effective logic decision-making, leading to black-box decision-making. Building on this, we propose Retro-Expert, an interpretable retrosynthesis framework that performs collaborative reasoning by combining the complementary reasoning strengths of Large Language Models and specialized models via reinforcement learning. It outputs natural language explanations grounded in chemical logic through three components: (1) specialized models analyze the product to construct high-quality chemical decision space, (2) LLM-driven critical reasoning to generate predictions and corresponding interpretable reasoning path, and (3) reinforcement learning optimizing interpretable decision policy. Experiments show that Retro-Expert not only surpasses both LLM-based and specialized models across different metrics but also provides expert-aligned explanations that bridge the gap between AI predictions and actionable chemical insights."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-14T15:41:25Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    15,
                    41,
                    25,
                    3,
                    226,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Xinyi Li"
                    },
                    {
                        "name": "Sai Wang"
                    },
                    {
                        "name": "Yutian Lin"
                    },
                    {
                        "name": "Yu Wu"
                    },
                    {
                        "name": "Yi Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yi Yang"
                },
                "author": "Yi Yang"
            },
            {
                "id": "http://arxiv.org/abs/2509.16158v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2509.16158v2",
                "title": "Designing Culturally Aligned AI Systems For Social Good in Non-Western Contexts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Designing Culturally Aligned AI Systems For Social Good in Non-Western Contexts"
                },
                "updated": "2025-12-05T06:11:06Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    6,
                    11,
                    6,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2509.16158v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2509.16158v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "AI technologies are increasingly deployed in high-stakes domains such as education, healthcare, law, and agriculture to address complex challenges in non-Western contexts. This paper examines eight real-world deployments spanning seven countries and 18 languages, combining 17 interviews with AI developers and domain experts with secondary research. Our findings identify six cross-cutting factors - Language, Institution, Safety, Task, End-User Demography, and Domain - that structured how systems were designed and deployed. These factors were shaped by Sociocultural (diversity, practices), Institutional (resources, policies), and Technological (capabilities, limits) influences. We find that building effective AI systems required extensive collaboration between AI developers and domain experts, with human resources proving more critical to achieving safe and effective outcomes in high-stakes domains than technological expertise alone. Additionally, we present 12 guidelines synthesizing these dynamics for designing AI for social good systems that are culturally grounded, equitable, and responsive to the needs of non-Western contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI technologies are increasingly deployed in high-stakes domains such as education, healthcare, law, and agriculture to address complex challenges in non-Western contexts. This paper examines eight real-world deployments spanning seven countries and 18 languages, combining 17 interviews with AI developers and domain experts with secondary research. Our findings identify six cross-cutting factors - Language, Institution, Safety, Task, End-User Demography, and Domain - that structured how systems were designed and deployed. These factors were shaped by Sociocultural (diversity, practices), Institutional (resources, policies), and Technological (capabilities, limits) influences. We find that building effective AI systems required extensive collaboration between AI developers and domain experts, with human resources proving more critical to achieving safe and effective outcomes in high-stakes domains than technological expertise alone. Additionally, we present 12 guidelines synthesizing these dynamics for designing AI for social good systems that are culturally grounded, equitable, and responsive to the needs of non-Western contexts."
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-09-19T17:07:13Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    17,
                    7,
                    13,
                    4,
                    262,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC"
                },
                "authors": [
                    {
                        "name": "Deepak Varuvel Dennison"
                    },
                    {
                        "name": "Mohit Jain"
                    },
                    {
                        "name": "Tanuja Ganu"
                    },
                    {
                        "name": "Aditya Vashistha"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Vashistha"
                },
                "author": "Aditya Vashistha"
            },
            {
                "id": "http://arxiv.org/abs/2512.05452v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05452v1",
                "title": "Kirigami Film Reflector for Deployable Space Antennas",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kirigami Film Reflector for Deployable Space Antennas"
                },
                "updated": "2025-12-05T06:09:27Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    6,
                    9,
                    27,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05452v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05452v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We propose a low-pretension reflective kirigami film as a material for the reflective surfaces of large deployable space reflector antennas with an operating frequency around 10 GHz. The kirigami cut pattern is based on the well-known rotating squares pattern but is augmented with diagonal cuts to enhance stretchability and allow control over the effective Poisson's ratio. Using finite element simulations, we analyzed how the geometric parameters of this pattern affected the reflectance of the film and the pretension required to resist thermal deformations. Tensile testing of selected designs, which are approximately half the weight of traditional metallic meshes, demonstrated a substantial reduction in the needed pretension to ~0.5 N/m and as low as ~0.1 N/m. Such low pretension represents an order-of-magnitude improvement over traditional metallic mesh reflectors and could enable the use of lighter antenna trusses. Free-space reflectance measurements also show that these perforated films can maintain power reflectance exceeding 90% at 10 GHz under the strains expected in the deployed configuration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a low-pretension reflective kirigami film as a material for the reflective surfaces of large deployable space reflector antennas with an operating frequency around 10 GHz. The kirigami cut pattern is based on the well-known rotating squares pattern but is augmented with diagonal cuts to enhance stretchability and allow control over the effective Poisson's ratio. Using finite element simulations, we analyzed how the geometric parameters of this pattern affected the reflectance of the film and the pretension required to resist thermal deformations. Tensile testing of selected designs, which are approximately half the weight of traditional metallic meshes, demonstrated a substantial reduction in the needed pretension to ~0.5 N/m and as low as ~0.1 N/m. Such low pretension represents an order-of-magnitude improvement over traditional metallic mesh reflectors and could enable the use of lighter antenna trusses. Free-space reflectance measurements also show that these perforated films can maintain power reflectance exceeding 90% at 10 GHz under the strains expected in the deployed configuration."
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T06:09:27Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    6,
                    9,
                    27,
                    4,
                    339,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph"
                },
                "authors": [
                    {
                        "name": "Gulzhan Aldan"
                    },
                    {
                        "name": "Henry Love"
                    },
                    {
                        "name": "Matthew Campbell"
                    },
                    {
                        "name": "Firooz Aflatouni"
                    },
                    {
                        "name": "Igor Bargatin"
                    }
                ],
                "author_detail": {
                    "name": "Igor Bargatin"
                },
                "author": "Igor Bargatin"
            },
            {
                "id": "http://arxiv.org/abs/2511.08475v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.08475v2",
                "title": "Designing LLM-based Multi-Agent Systems for Software Engineering Tasks: Quality Attributes, Design Patterns and Rationale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Designing LLM-based Multi-Agent Systems for Software Engineering Tasks: Quality Attributes, Design Patterns and Rationale"
                },
                "updated": "2025-12-05T06:05:25Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    6,
                    5,
                    25,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.08475v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.08475v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "As the complexity of Software Engineering (SE) tasks continues to escalate, Multi-Agent Systems (MASs) have emerged as a focal point of research and practice due to their autonomy and scalability. Furthermore, through leveraging the reasoning and planning capabilities of Large Language Models (LLMs), the application of LLM-based MASs in the field of SE is garnering increasing attention. However, there is no dedicated study that systematically explores the design of LLM-based MASs, including the Quality Attributes (QAs) on which designers mainly focus, the design patterns used by designers, and the rationale guiding the design of LLM-based MASs for SE tasks. To this end, we conducted a study to identify the QAs that LLM-based MASs for SE tasks focus on, the design patterns used in the MASs, and the design rationale for the MASs. We collected 94 papers on LLM-based MASs for SE tasks as the source. Our study shows that: (1) Code Generation is the most common SE task solved by LLM-based MASs among ten identified SE tasks, (2) Functional Suitability is the QA on which designers of LLM-based MASs pay the most attention, (3) Role-Based Cooperation is the design pattern most frequently employed among 16 patterns used to construct LLM-based MASs, and (4) Improving the Quality of Generated Code is the most common rationale behind the design of LLM-based MASs. Based on the study results, we presented the implications for the design of LLM-based MASs to support SE tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the complexity of Software Engineering (SE) tasks continues to escalate, Multi-Agent Systems (MASs) have emerged as a focal point of research and practice due to their autonomy and scalability. Furthermore, through leveraging the reasoning and planning capabilities of Large Language Models (LLMs), the application of LLM-based MASs in the field of SE is garnering increasing attention. However, there is no dedicated study that systematically explores the design of LLM-based MASs, including the Quality Attributes (QAs) on which designers mainly focus, the design patterns used by designers, and the rationale guiding the design of LLM-based MASs for SE tasks. To this end, we conducted a study to identify the QAs that LLM-based MASs for SE tasks focus on, the design patterns used in the MASs, and the design rationale for the MASs. We collected 94 papers on LLM-based MASs for SE tasks as the source. Our study shows that: (1) Code Generation is the most common SE task solved by LLM-based MASs among ten identified SE tasks, (2) Functional Suitability is the QA on which designers of LLM-based MASs pay the most attention, (3) Role-Based Cooperation is the design pattern most frequently employed among 16 patterns used to construct LLM-based MASs, and (4) Improving the Quality of Generated Code is the most common rationale behind the design of LLM-based MASs. Based on the study results, we presented the implications for the design of LLM-based MASs to support SE tasks."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-11T17:19:30Z",
                "published_parsed": [
                    2025,
                    11,
                    11,
                    17,
                    19,
                    30,
                    1,
                    315,
                    0
                ],
                "arxiv_comment": "35 pages, 4 images, 7 tables, Manuscript submitted to a Journal (2025)",
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Yangxiao Cai"
                    },
                    {
                        "name": "Ruiyin Li"
                    },
                    {
                        "name": "Peng Liang"
                    },
                    {
                        "name": "Mojtaba Shahin"
                    },
                    {
                        "name": "Zengyang Li"
                    }
                ],
                "author_detail": {
                    "name": "Zengyang Li"
                },
                "author": "Zengyang Li"
            },
            {
                "id": "http://arxiv.org/abs/2510.11062v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.11062v3",
                "title": "Stronger-MAS: Multi-Agent Reinforcement Learning for Collaborative LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stronger-MAS: Multi-Agent Reinforcement Learning for Collaborative LLMs"
                },
                "updated": "2025-12-05T05:59:28Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    5,
                    59,
                    28,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.11062v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.11062v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Multi-agent systems (MAS) and reinforcement learning (RL) are widely used to enhance the agentic capabilities of large language models (LLMs). MAS improves task performance through role-based orchestration, while RL uses environmental rewards to learn stronger policies, such as GRPO-style optimization. However, applying on-policy RL to MAS remains underexplored and presents unique challenges. Algorithmically, standard GRPO grouping assumptions break down because prompts vary by role and by turn. System-wise, the training stack must support MAS-workflow rollouts and on-policy updates for both single-policy and multi-policy models.\n  We propose AT-GRPO, which includes (i) an agent- and turn-wise grouped RL algorithm tailored to MAS and (ii) a training system that supports both single- and multi-policy regimes. Across game, planning, coding, and math tasks, AT-GRPO delivers substantial gains. On long-horizon planning, it increases accuracy from a 14.0 to 47.0 percent single-agent RL baseline to 96.0 to 99.5 percent. It also improves reasoning performance, with average gains of 3.87 to 7.62 percent on coding tasks and 9.0 to 17.93 percent on math. Code and environments are available at: https://github.com/pettingllms-ai/PettingLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-agent systems (MAS) and reinforcement learning (RL) are widely used to enhance the agentic capabilities of large language models (LLMs). MAS improves task performance through role-based orchestration, while RL uses environmental rewards to learn stronger policies, such as GRPO-style optimization. However, applying on-policy RL to MAS remains underexplored and presents unique challenges. Algorithmically, standard GRPO grouping assumptions break down because prompts vary by role and by turn. System-wise, the training stack must support MAS-workflow rollouts and on-policy updates for both single-policy and multi-policy models.\n  We propose AT-GRPO, which includes (i) an agent- and turn-wise grouped RL algorithm tailored to MAS and (ii) a training system that supports both single- and multi-policy regimes. Across game, planning, coding, and math tasks, AT-GRPO delivers substantial gains. On long-horizon planning, it increases accuracy from a 14.0 to 47.0 percent single-agent RL baseline to 96.0 to 99.5 percent. It also improves reasoning performance, with average gains of 3.87 to 7.62 percent on coding tasks and 9.0 to 17.93 percent on math. Code and environments are available at: https://github.com/pettingllms-ai/PettingLLMs."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-13T06:55:09Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    6,
                    55,
                    9,
                    0,
                    286,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Yujie Zhao"
                    },
                    {
                        "name": "Lanxiang Hu"
                    },
                    {
                        "name": "Yang Wang"
                    },
                    {
                        "name": "Minmin Hou"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Ke Ding"
                    },
                    {
                        "name": "Jishen Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Jishen Zhao"
                },
                "author": "Jishen Zhao"
            },
            {
                "id": "http://arxiv.org/abs/2505.15436v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2505.15436v3",
                "title": "Adaptive Chain-of-Focus Reasoning via Dynamic Visual Search and Zooming for Efficient VLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Chain-of-Focus Reasoning via Dynamic Visual Search and Zooming for Efficient VLMs"
                },
                "updated": "2025-12-05T05:44:39Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    5,
                    44,
                    39,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2505.15436v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2505.15436v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Vision language models (VLMs) have achieved impressive performance across a variety of computer vision tasks. However, the multimodal reasoning capability has not been fully explored in existing models. In this paper, we propose a Chain-of-Focus (CoF) method that allows VLMs to perform adaptive focusing and zooming in on key image regions based on obtained visual cues and the given questions, achieving efficient multimodal reasoning. To enable this CoF capability, we present a two-stage training pipeline, including supervised fine-tuning (SFT) and reinforcement learning (RL). In the SFT stage, we construct the MM-CoF dataset, comprising 3K samples derived from a visual agent designed to adaptively identify key regions to solve visual tasks with different image resolutions and questions. We use MM-CoF to fine-tune the Qwen2.5-VL model for cold start. In the RL stage, we leverage the outcome accuracies and formats as rewards to update the Qwen2.5-VL model, enabling further refining the search and reasoning strategy of models without human priors. Our model achieves significant improvements on multiple benchmarks. On the V* benchmark that requires strong visual reasoning capability, our model outperforms existing VLMs by 5% among 8 image resolutions ranging from 224 to 4K, demonstrating the effectiveness of the proposed CoF method and facilitating the more efficient deployment of VLMs in practical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision language models (VLMs) have achieved impressive performance across a variety of computer vision tasks. However, the multimodal reasoning capability has not been fully explored in existing models. In this paper, we propose a Chain-of-Focus (CoF) method that allows VLMs to perform adaptive focusing and zooming in on key image regions based on obtained visual cues and the given questions, achieving efficient multimodal reasoning. To enable this CoF capability, we present a two-stage training pipeline, including supervised fine-tuning (SFT) and reinforcement learning (RL). In the SFT stage, we construct the MM-CoF dataset, comprising 3K samples derived from a visual agent designed to adaptively identify key regions to solve visual tasks with different image resolutions and questions. We use MM-CoF to fine-tune the Qwen2.5-VL model for cold start. In the RL stage, we leverage the outcome accuracies and formats as rewards to update the Qwen2.5-VL model, enabling further refining the search and reasoning strategy of models without human priors. Our model achieves significant improvements on multiple benchmarks. On the V* benchmark that requires strong visual reasoning capability, our model outperforms existing VLMs by 5% among 8 image resolutions ranging from 224 to 4K, demonstrating the effectiveness of the proposed CoF method and facilitating the more efficient deployment of VLMs in practical applications."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-05-21T12:18:15Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    12,
                    18,
                    15,
                    2,
                    141,
                    0
                ],
                "arxiv_comment": "https://github.com/xtong-zhang/Chain-of-Focus",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Xintong Zhang"
                    },
                    {
                        "name": "Zhi Gao"
                    },
                    {
                        "name": "Bofei Zhang"
                    },
                    {
                        "name": "Pengxiang Li"
                    },
                    {
                        "name": "Xiaowen Zhang"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Tao Yuan"
                    },
                    {
                        "name": "Yuwei Wu"
                    },
                    {
                        "name": "Yunde Jia"
                    },
                    {
                        "name": "Song-Chun Zhu"
                    },
                    {
                        "name": "Qing Li"
                    }
                ],
                "author_detail": {
                    "name": "Qing Li"
                },
                "author": "Qing Li"
            },
            {
                "id": "http://arxiv.org/abs/2508.11222v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.11222v2",
                "title": "ORFuzz: Fuzzing the \"Other Side\" of LLM Safety -- Testing Over-Refusal",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ORFuzz: Fuzzing the \"Other Side\" of LLM Safety -- Testing Over-Refusal"
                },
                "updated": "2025-12-05T05:36:55Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    5,
                    36,
                    55,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.11222v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.11222v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) increasingly exhibit over-refusal - erroneously rejecting benign queries due to overly conservative safety measures - a critical functional flaw that undermines their reliability and usability. Current methods for testing this behavior are demonstrably inadequate, suffering from flawed benchmarks and limited test generation capabilities, as highlighted by our empirical user study. To the best of our knowledge, this paper introduces the first evolutionary testing framework, ORFuzz, for the systematic detection and analysis of LLM over-refusals. ORFuzz uniquely integrates three core components: (1) safety category-aware seed selection for comprehensive test coverage, (2) adaptive mutator optimization using reasoning LLMs to generate effective test cases, and (3) OR-Judge, a human-aligned judge model validated to accurately reflect user perception of toxicity and refusal. Our extensive evaluations demonstrate that ORFuzz generates diverse, validated over-refusal instances at a rate (6.98% average) more than double that of leading baselines, effectively uncovering vulnerabilities. Furthermore, ORFuzz's outputs form the basis of ORFuzzSet, a new benchmark of 1,855 highly transferable test cases that achieves a superior 63.56% average over-refusal rate across 10 diverse LLMs, significantly outperforming existing datasets. ORFuzz and ORFuzzSet provide a robust automated testing framework and a valuable community resource, paving the way for developing more reliable and trustworthy LLM-based software systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) increasingly exhibit over-refusal - erroneously rejecting benign queries due to overly conservative safety measures - a critical functional flaw that undermines their reliability and usability. Current methods for testing this behavior are demonstrably inadequate, suffering from flawed benchmarks and limited test generation capabilities, as highlighted by our empirical user study. To the best of our knowledge, this paper introduces the first evolutionary testing framework, ORFuzz, for the systematic detection and analysis of LLM over-refusals. ORFuzz uniquely integrates three core components: (1) safety category-aware seed selection for comprehensive test coverage, (2) adaptive mutator optimization using reasoning LLMs to generate effective test cases, and (3) OR-Judge, a human-aligned judge model validated to accurately reflect user perception of toxicity and refusal. Our extensive evaluations demonstrate that ORFuzz generates diverse, validated over-refusal instances at a rate (6.98% average) more than double that of leading baselines, effectively uncovering vulnerabilities. Furthermore, ORFuzz's outputs form the basis of ORFuzzSet, a new benchmark of 1,855 highly transferable test cases that achieves a superior 63.56% average over-refusal rate across 10 diverse LLMs, significantly outperforming existing datasets. ORFuzz and ORFuzzSet provide a robust automated testing framework and a valuable community resource, paving the way for developing more reliable and trustworthy LLM-based software systems."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-15T05:03:26Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    5,
                    3,
                    26,
                    4,
                    227,
                    0
                ],
                "arxiv_comment": "Accepted by ASE 2025",
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Haonan Zhang"
                    },
                    {
                        "name": "Dongxia Wang"
                    },
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Kexin Chen"
                    },
                    {
                        "name": "Jiashui Wang"
                    },
                    {
                        "name": "Xinlei Ying"
                    },
                    {
                        "name": "Long Liu"
                    },
                    {
                        "name": "Wenhai Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wenhai Wang"
                },
                "author": "Wenhai Wang"
            },
            {
                "id": "http://arxiv.org/abs/2512.05439v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05439v1",
                "title": "BEAVER: An Efficient Deterministic LLM Verifier",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BEAVER: An Efficient Deterministic LLM Verifier"
                },
                "updated": "2025-12-05T05:34:06Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    5,
                    34,
                    6,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05439v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05439v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "As large language models (LLMs) transition from research prototypes to production systems, practitioners often need reliable methods to verify that model outputs satisfy required constraints. While sampling-based estimates provide an intuition of model behavior, they offer no sound guarantees. We present BEAVER, the first practical framework for computing deterministic, sound probability bounds on LLM constraint satisfaction. Given any prefix-closed semantic constraint, BEAVER systematically explores the generation space using novel token trie and frontier data structures, maintaining provably sound bounds at every iteration. We formalize the verification problem, prove soundness of our approach, and evaluate BEAVER on correctness verification, privacy verification and secure code generation tasks across multiple state of the art LLMs. BEAVER achieves 6 to 8 times tighter probability bounds and identifies 3 to 4 times more high risk instances compared to baseline methods under identical computational budgets, enabling precise characterization and risk assessment that loose bounds or empirical evaluation cannot provide.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) transition from research prototypes to production systems, practitioners often need reliable methods to verify that model outputs satisfy required constraints. While sampling-based estimates provide an intuition of model behavior, they offer no sound guarantees. We present BEAVER, the first practical framework for computing deterministic, sound probability bounds on LLM constraint satisfaction. Given any prefix-closed semantic constraint, BEAVER systematically explores the generation space using novel token trie and frontier data structures, maintaining provably sound bounds at every iteration. We formalize the verification problem, prove soundness of our approach, and evaluate BEAVER on correctness verification, privacy verification and secure code generation tasks across multiple state of the art LLMs. BEAVER achieves 6 to 8 times tighter probability bounds and identifies 3 to 4 times more high risk instances compared to baseline methods under identical computational budgets, enabling precise characterization and risk assessment that loose bounds or empirical evaluation cannot provide."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.FL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T05:34:06Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    5,
                    34,
                    6,
                    4,
                    339,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Tarun Suresh"
                    },
                    {
                        "name": "Nalin Wadhwa"
                    },
                    {
                        "name": "Debangshu Banerjee"
                    },
                    {
                        "name": "Gagandeep Singh"
                    }
                ],
                "author_detail": {
                    "name": "Gagandeep Singh"
                },
                "author": "Gagandeep Singh"
            },
            {
                "id": "http://arxiv.org/abs/2512.05430v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05430v1",
                "title": "ArtistMus: A Globally Diverse, Artist-Centric Benchmark for Retrieval-Augmented Music Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ArtistMus: A Globally Diverse, Artist-Centric Benchmark for Retrieval-Augmented Music Question Answering"
                },
                "updated": "2025-12-05T05:09:30Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    5,
                    9,
                    30,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05430v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05430v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recent advances in large language models (LLMs) have transformed open-domain question answering, yet their effectiveness in music-related reasoning remains limited due to sparse music knowledge in pretraining data. While music information retrieval and computational musicology have explored structured and multimodal understanding, few resources support factual and contextual music question answering (MQA) grounded in artist metadata or historical context. We introduce MusWikiDB, a vector database of 3.2M passages from 144K music-related Wikipedia pages, and ArtistMus, a benchmark of 1,000 questions on 500 diverse artists with metadata such as genre, debut year, and topic. These resources enable systematic evaluation of retrieval-augmented generation (RAG) for MQA. Experiments show that RAG markedly improves factual accuracy; open-source models gain up to +56.8 percentage points (for example, Qwen3 8B improves from 35.0 to 91.8), approaching proprietary model performance. RAG-style fine-tuning further boosts both factual recall and contextual reasoning, improving results on both in-domain and out-of-domain benchmarks. MusWikiDB also yields approximately 6 percentage points higher accuracy and 40% faster retrieval than a general-purpose Wikipedia corpus. We release MusWikiDB and ArtistMus to advance research in music information retrieval and domain-specific question answering, establishing a foundation for retrieval-augmented reasoning in culturally rich domains such as music.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have transformed open-domain question answering, yet their effectiveness in music-related reasoning remains limited due to sparse music knowledge in pretraining data. While music information retrieval and computational musicology have explored structured and multimodal understanding, few resources support factual and contextual music question answering (MQA) grounded in artist metadata or historical context. We introduce MusWikiDB, a vector database of 3.2M passages from 144K music-related Wikipedia pages, and ArtistMus, a benchmark of 1,000 questions on 500 diverse artists with metadata such as genre, debut year, and topic. These resources enable systematic evaluation of retrieval-augmented generation (RAG) for MQA. Experiments show that RAG markedly improves factual accuracy; open-source models gain up to +56.8 percentage points (for example, Qwen3 8B improves from 35.0 to 91.8), approaching proprietary model performance. RAG-style fine-tuning further boosts both factual recall and contextual reasoning, improving results on both in-domain and out-of-domain benchmarks. MusWikiDB also yields approximately 6 percentage points higher accuracy and 40% faster retrieval than a general-purpose Wikipedia corpus. We release MusWikiDB and ArtistMus to advance research in music information retrieval and domain-specific question answering, establishing a foundation for retrieval-augmented reasoning in culturally rich domains such as music."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T05:09:30Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    5,
                    9,
                    30,
                    4,
                    339,
                    0
                ],
                "arxiv_comment": "Submitted to LREC 2026. This work is an evolution of our earlier preprint arXiv:2507.23334",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Daeyong Kwon"
                    },
                    {
                        "name": "SeungHeon Doh"
                    },
                    {
                        "name": "Juhan Nam"
                    }
                ],
                "author_detail": {
                    "name": "Juhan Nam"
                },
                "author": "Juhan Nam"
            },
            {
                "id": "http://arxiv.org/abs/2512.05428v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05428v1",
                "title": "Bita: A Conversational Assistant for Fairness Testing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bita: A Conversational Assistant for Fairness Testing"
                },
                "updated": "2025-12-05T05:04:59Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    5,
                    4,
                    59,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05428v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05428v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Bias in AI systems can lead to unfair and discriminatory outcomes, especially when left untested before deployment. Although fairness testing aims to identify and mitigate such bias, existing tools are often difficult to use, requiring advanced expertise and offering limited support for real-world workflows. To address this, we introduce Bita, a conversational assistant designed to help software testers detect potential sources of bias, evaluate test plans through a fairness lens, and generate fairness-oriented exploratory testing charters. Bita integrates a large language model with retrieval-augmented generation, grounding its responses in curated fairness literature. Our validation demonstrates how Bita supports fairness testing tasks on real-world AI systems, providing structured, reproducible evidence of its utility. In summary, our work contributes a practical tool that operationalizes fairness testing in a way that is accessible, systematic, and directly applicable to industrial practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bias in AI systems can lead to unfair and discriminatory outcomes, especially when left untested before deployment. Although fairness testing aims to identify and mitigate such bias, existing tools are often difficult to use, requiring advanced expertise and offering limited support for real-world workflows. To address this, we introduce Bita, a conversational assistant designed to help software testers detect potential sources of bias, evaluate test plans through a fairness lens, and generate fairness-oriented exploratory testing charters. Bita integrates a large language model with retrieval-augmented generation, grounding its responses in curated fairness literature. Our validation demonstrates how Bita supports fairness testing tasks on real-world AI systems, providing structured, reproducible evidence of its utility. In summary, our work contributes a practical tool that operationalizes fairness testing in a way that is accessible, systematic, and directly applicable to industrial practice."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T05:04:59Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    5,
                    4,
                    59,
                    4,
                    339,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Keeryn Johnson"
                    },
                    {
                        "name": "Cleyton Magalhaes"
                    },
                    {
                        "name": "Ronnie de Souza Santos"
                    }
                ],
                "author_detail": {
                    "name": "Ronnie de Souza Santos"
                },
                "author": "Ronnie de Souza Santos"
            }
        ]
    }
]